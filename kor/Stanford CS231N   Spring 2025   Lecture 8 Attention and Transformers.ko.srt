1
00:00:04,760 --> 00:00:07,440
좋습니다, 여러분 모두 강의 8에 다시 오신 것을 환영합니다.

2
00:00:07,440 --> 00:00:09,940
오늘은 attention과 transformers에 대해 이야기할 겁니다.

3
00:00:09,940 --> 00:00:12,200
이 주제는 정말 재미있다고 생각합니다.

4
00:00:12,200 --> 00:00:14,005
간단히 복습하자면, 지난 시간에는 recurrent

5
00:00:14,005 --> 00:00:15,880
neural networks에 대해 이야기했었죠.

6
00:00:15,880 --> 00:00:18,297
recurrent neural networks는

7
00:00:18,297 --> 00:00:20,460
시퀀스를 처리하기 위한 새로운 신경망 구조였습니다.

8
00:00:20,460 --> 00:00:23,200
특히, 시퀀스를 처리함으로써 convolutional

9
00:00:23,200 --> 00:00:26,120
networks로는 다룰 수 없었던 새로운

10
00:00:26,120 --> 00:00:29,120
문제들을 해결할 수 있게 되었다는 점을 보았습니다.

11
00:00:29,120 --> 00:00:31,073
보통 우리는 하나의 입력,

12
00:00:31,073 --> 00:00:33,240
예를 들어 이미지 하나를

13
00:00:33,240 --> 00:00:36,560
넣고 그 이미지에 대한 분류 결과 하나를 출력하는

14
00:00:36,560 --> 00:00:38,780
일대일 문제를 생각했었죠.

15
00:00:38,780 --> 00:00:41,080
하지만 이미지에서 벗어나 시퀀스

16
00:00:41,080 --> 00:00:42,870
데이터를 다룰 수 있게 되면서,

17
00:00:42,870 --> 00:00:44,870
이미지 캡셔닝 같은 일대다

18
00:00:44,870 --> 00:00:47,218
문제도 해결할 수 있게 되었습니다.

19
00:00:47,218 --> 00:00:49,760
예를 들어 이미지를 입력으로 넣고, 그

20
00:00:49,760 --> 00:00:52,680
이미지에 대한 텍스트 설명, 즉 단어들의 시퀀스를

21
00:00:52,680 --> 00:00:55,880
출력하는 문제죠. 또는 다대일 문제로, 여러 프레임

22
00:00:55,880 --> 00:00:58,000
시퀀스를 입력으로 넣고 그 프레임들에

23
00:00:58,000 --> 00:01:00,500
대한 분류를 출력하는 문제도 있습니다.

24
00:01:00,500 --> 00:01:03,860
이처럼 더 복잡한 신경망 구조로 나아가면서,

25
00:01:03,860 --> 00:01:05,860
전통적인 feedforward

26
00:01:05,860 --> 00:01:07,720
neural

27
00:01:07,720 --> 00:01:09,340
networks로는 해결할

28
00:01:09,340 --> 00:01:12,540
수 없던 새로운 문제들을 다룰 수

29
00:01:12,540 --> 00:01:13,637
있게 되었습니다.

30
00:01:13,637 --> 00:01:15,220
오늘은 그 위에 더해서

31
00:01:15,220 --> 00:01:18,140
두 가지 새로운 주제를 다룰 겁니다.

32
00:01:18,140 --> 00:01:20,022
첫 번째는 attention으로,

33
00:01:20,022 --> 00:01:21,980
벡터 집합을 근본적으로

34
00:01:21,980 --> 00:01:25,175
다루는 새로운 신경망 원시 연산입니다.

35
00:01:25,175 --> 00:01:27,300
두 번째는 transformer입니다.

36
00:01:27,300 --> 00:01:28,480
두 번째는 transformer입니다.

37
00:01:28,480 --> 00:01:30,580
transformer는

38
00:01:30,580 --> 00:01:33,780
self-attention을 핵심으로 하는 새로운 신경망 구조입니다.

39
00:01:33,780 --> 00:01:37,780
미리 말씀드리자면, 오늘날 딥러닝에서 거의 모든 문제에

40
00:01:37,780 --> 00:01:40,220
쓰이는 아키텍처가 바로

41
00:01:40,220 --> 00:01:41,620
transformer입니다.

42
00:01:41,620 --> 00:01:43,620
현재 야외에서 볼 수

43
00:01:43,620 --> 00:01:45,300
있는 가장 큰

44
00:01:45,300 --> 00:01:48,900
규모의 응용들, 이미지 분류, 이미지

45
00:01:48,900 --> 00:01:51,940
생성, 텍스트 생성, 텍스트

46
00:01:51,940 --> 00:01:55,160
분류, 오디오 처리 등 거의

47
00:01:55,160 --> 00:01:57,700
모든 대규모 신경망은

48
00:01:57,700 --> 00:01:59,560
오늘날 transformer

49
00:01:59,560 --> 00:02:01,600
기반입니다.

50
00:02:01,600 --> 00:02:03,958
그래서 최신이자 최고의

51
00:02:03,958 --> 00:02:06,000
아키텍처를 여러분께 소개할

52
00:02:06,000 --> 00:02:08,840
수 있어서 정말 흥미롭습니다.

53
00:02:08,840 --> 00:02:11,400
하지만 transformer가 오늘날

54
00:02:11,400 --> 00:02:14,020
모두가 쓰는 최첨단 아키텍처임에도

55
00:02:14,020 --> 00:02:16,480
불구하고, 그 역사는 꽤 오래되었습니다.

56
00:02:16,480 --> 00:02:19,240
이 분야가 발전하는 모습을 지켜보는

57
00:02:19,240 --> 00:02:20,538
것은 흥미롭습니다.

58
00:02:20,538 --> 00:02:23,080
transformer가 처음 등장했을 때,

59
00:02:23,080 --> 00:02:24,480
마치 큰 전환점,

60
00:02:24,480 --> 00:02:26,160
새로운 혁신처럼 느껴질

61
00:02:26,160 --> 00:02:28,243
법한 순간이었지만, 실제로는 그렇지

62
00:02:28,243 --> 00:02:29,140
않았습니다.

63
00:02:29,140 --> 00:02:30,697
transformer 아키텍처가 탄생한 한 순간이 있었지만,
self-attention과 attention을 다양한 방식으로 사용하는
아이디어들은 이미 몇 년 전부터 존재했습니다.

64
00:02:30,697 --> 00:02:33,280
특히, attention과

65
00:02:33,280 --> 00:02:36,080
self-attention 아이디어는

66
00:02:36,080 --> 00:02:38,858
recurrent neural

67
00:02:38,858 --> 00:02:41,400
networks에서 발전해 온

68
00:02:41,400 --> 00:02:42,340
것입니다.

69
00:02:42,340 --> 00:02:45,740
그래서 먼저 그 부분부터 시작해서

70
00:02:45,740 --> 00:02:48,180
문제를 동기부여할 겁니다.

71
00:02:48,180 --> 00:02:50,120
이것은 이 아이디어들의 역사적 발전 과정을 어느 정도

72
00:02:50,120 --> 00:02:50,782
반영하는 방식입니다.

73
00:02:50,782 --> 00:02:52,240
그래서 transformer를 소개하기 위해,

74
00:02:52,240 --> 00:02:55,800
지난 강의에서 본 recurrent neural networks에 대해 조금
되돌아가 복습할 겁니다.

75
00:02:55,800 --> 00:02:58,352
동기부여 문제로서,

76
00:02:58,352 --> 00:03:00,060
번역이라는

77
00:03:00,060 --> 00:03:01,780
시퀀스-투-시퀀스

78
00:03:01,780 --> 00:03:03,613
문제를

79
00:03:03,613 --> 00:03:05,220
생각해 봅시다.

80
00:03:05,220 --> 00:03:08,580
하나의 시퀀스, 즉 영어 단어들의

81
00:03:08,580 --> 00:03:10,740
시퀀스를 입력으로 넣고,

82
00:03:10,740 --> 00:03:12,260
다른 시퀀스, 즉 이탈리아어

83
00:03:12,260 --> 00:03:14,920
단어들의 시퀀스를 출력으로 내놓고자 합니다.

84
00:03:14,920 --> 00:03:16,620
영어 단어와

85
00:03:16,620 --> 00:03:18,287
이탈리아어 단어

86
00:03:18,287 --> 00:03:20,040
사이에 직접적인

87
00:03:20,040 --> 00:03:23,420
대응 관계가 있다고 가정할 수

88
00:03:23,420 --> 00:03:24,680
없습니다.

89
00:03:24,680 --> 00:03:26,588
영어 문장의 단어 수와

90
00:03:26,588 --> 00:03:28,380
이탈리아어 문장의 단어 수가

91
00:03:28,380 --> 00:03:29,427
다를 수 있고,

92
00:03:29,427 --> 00:03:31,760
단어들의 순서도 완전히 다를 수 있습니다.

93
00:03:31,760 --> 00:03:33,780
이것은 우리가 지난 강의에서 본

94
00:03:33,780 --> 00:03:35,660
recurrent neural

95
00:03:35,660 --> 00:03:37,380
networks의 시퀀스 처리

96
00:03:37,380 --> 00:03:39,180
알고리즘에 딱 맞는 응용입니다.

97
00:03:39,180 --> 00:03:42,020
실제로, sequence to sequence 문제를

98
00:03:42,020 --> 00:03:44,500
recurrent neural networks로

99
00:03:44,500 --> 00:03:48,160
처리하는 이 아이디어는 2014년, 심지어 그 이전부터 시작되었습니다.

100
00:03:48,160 --> 00:03:49,900
하지만 사람들은 이미 10년

101
00:03:49,900 --> 00:03:53,420
넘게 recurrent neural networks로

102
00:03:53,420 --> 00:03:54,900
시퀀스를 처리해왔습니다.

103
00:03:54,900 --> 00:03:57,680
그래서 sequence to sequence 문제를

104
00:03:57,680 --> 00:03:59,960
recurrent neural networks로 처리하는

105
00:03:59,960 --> 00:04:02,220
기본 구조는 보통 하나의 인코더로 시작합니다.

106
00:04:02,220 --> 00:04:04,520
인코더는 recurrent neural network입니다.

107
00:04:04,520 --> 00:04:06,000
recurrent neural

108
00:04:06,000 --> 00:04:08,480
network는 두 입력에 재귀적으로

109
00:04:08,480 --> 00:04:09,820
적용되는 함수입니다.

110
00:04:09,820 --> 00:04:12,520
하나는 현재 시점의 입력인 xt이고,

111
00:04:12,520 --> 00:04:14,380
다른 하나는 이전 시점의

112
00:04:14,380 --> 00:04:16,940
hidden state인 ht-1입니다.

113
00:04:16,940 --> 00:04:18,523
그리고 recurrent

114
00:04:18,523 --> 00:04:20,680
neural network 유닛은 다음

115
00:04:20,680 --> 00:04:23,200
시점의 hidden state를 출력합니다.

116
00:04:23,200 --> 00:04:26,160
그 다음 같은 recurrent neural network

117
00:04:26,160 --> 00:04:28,920
유닛을 시간에 따라 반복 적용해서 가변

118
00:04:28,920 --> 00:04:30,948
길이의 시퀀스를 처리할 수 있습니다.

119
00:04:30,948 --> 00:04:33,240
이 경우, 영어 입력 시퀀스를 받는

120
00:04:33,240 --> 00:04:36,560
recurrent neural network 인코더를 사용합니다.

121
00:04:36,560 --> 00:04:40,000
입력 시퀀스는 슬라이드에 맞추고 모든 박스가 보이도록

122
00:04:40,000 --> 00:04:42,340
상대적으로 짧은 문장을 사용해야 합니다.

123
00:04:42,340 --> 00:04:44,460
그래서 "We see the sky."

124
00:04:44,460 --> 00:04:46,160
같은 짧고 간단한 문장을 사용합니다.

125
00:04:46,160 --> 00:04:48,320
문장 내 각 단어는 recurrent

126
00:04:48,320 --> 00:04:53,040
neural network의 한 타임스텝으로 처리됩니다.

127
00:04:53,040 --> 00:04:56,500
이 인코더 recurrent neural

128
00:04:56,500 --> 00:04:59,500
network의 목적은 입력 시퀀스의 모든 단어를

129
00:04:59,500 --> 00:05:02,340
처리해서 입력 문장의 내용을 요약하는

130
00:05:02,340 --> 00:05:05,460
것입니다. 그래야 다른, 즉 출력 목표 언어로

131
00:05:05,460 --> 00:05:07,060
번역할 수 있습니다.

132
00:05:07,060 --> 00:05:10,180
좀 더 구체적으로 말하면,

133
00:05:10,180 --> 00:05:14,080
입력 시퀀스의 모든 단어를 처리한 후,

134
00:05:14,080 --> 00:05:17,300
전체 내용을 하나의 벡터인

135
00:05:17,300 --> 00:05:21,077
context vector로 요약합니다.

136
00:05:21,077 --> 00:05:22,660
그리고 보통 recurrent neural

137
00:05:22,660 --> 00:05:24,160
networks에서는 이 과정을

138
00:05:24,160 --> 00:05:25,523
수행하는 몇 가지 방법이 있습니다.

139
00:05:25,523 --> 00:05:27,440
세부 사항은 그다지 흥미롭지 않은 것 같습니다.

140
00:05:27,440 --> 00:05:30,340
그래서 context vector는 기본적으로

141
00:05:30,340 --> 00:05:33,300
인코더 순환 신경망의 마지막 은닉 상태라고

142
00:05:33,300 --> 00:05:34,260
생각하시면 됩니다.

143
00:05:34,260 --> 00:05:36,900
그리고 이제 아이디어는 순환 신경망의

144
00:05:36,900 --> 00:05:39,280
순환 구조 덕분에 마지막 은닉

145
00:05:39,280 --> 00:05:41,740
상태가 전체 입력 시퀀스의 정보를

146
00:05:41,740 --> 00:05:43,320
포함한다는 겁니다.

147
00:05:43,320 --> 00:05:45,260
그래서 그 마지막 은닉 상태를 전체

148
00:05:45,260 --> 00:05:47,820
입력 시퀀스의 모든 정보를 요약하거나

149
00:05:47,820 --> 00:05:49,660
인코딩한 것으로 생각할 수 있습니다.

150
00:05:49,660 --> 00:05:52,220
그럼 그 벡터 하나가 전체 입력 시퀀스를

151
00:05:52,220 --> 00:05:55,240
요약해서 우리가 원하는 어떤 작업을 할

152
00:05:55,240 --> 00:05:56,600
수 있게 되는 거죠.

153
00:05:56,600 --> 00:05:58,360
그리고 이 경우에는 그걸

154
00:05:58,360 --> 00:06:00,520
이용해 입력 시퀀스를 다른 언어의

155
00:06:00,520 --> 00:06:02,582
출력 시퀀스로 번역하는 겁니다.

156
00:06:02,582 --> 00:06:05,040
그래서 이를 위해 두 번째 순환

157
00:06:05,040 --> 00:06:07,760
신경망인 디코더를 사용할 텐데, 보통

158
00:06:07,760 --> 00:06:10,520
같은 구조지만 가중치 행렬이나 학습된

159
00:06:10,520 --> 00:06:12,800
파라미터는 다를 수 있습니다.

160
00:06:12,800 --> 00:06:14,800
이 디코더 gu는 다른 학습

161
00:06:14,800 --> 00:06:17,640
가능한 가중치 u를 가진 또 다른 순환

162
00:06:17,640 --> 00:06:20,200
신경망이지만 기본 아이디어는 같습니다.

163
00:06:20,200 --> 00:06:22,840
이제 순환 신경망 유닛이 매

164
00:06:22,840 --> 00:06:25,300
시점마다 세 가지 입력을

165
00:06:25,300 --> 00:06:27,520
받는다면, 이전 시점 출력

166
00:06:27,520 --> 00:06:30,240
시퀀스의 토큰 yt-1을

167
00:06:30,240 --> 00:06:30,900
받습니다.

168
00:06:30,900 --> 00:06:34,400
그리고 이전 출력 시퀀스의 은닉 상태 st-1과

169
00:06:34,400 --> 00:06:36,480
전체 입력 시퀀스를

170
00:06:36,480 --> 00:06:39,440
요약한 context vector

171
00:06:39,440 --> 00:06:40,400
C를 받습니다.

172
00:06:40,400 --> 00:06:42,520
그리고 나서 지난 강의에서

173
00:06:42,520 --> 00:06:45,680
봤듯이 출력 시퀀스를 펼쳐서 한 번에 한

174
00:06:45,680 --> 00:06:47,178
단어씩 생성합니다.

175
00:06:47,178 --> 00:06:48,720
저는 이탈리아어를 못해서

176
00:06:48,720 --> 00:06:50,980
발음하려고 시도하지는 않겠습니다.

177
00:06:50,980 --> 00:06:54,460
하지만 화면에 이탈리아어 단어들이 보이실 겁니다.

178
00:06:54,460 --> 00:06:58,780
그리고 그게 실제로 '우리는 하늘을 봅니다'로

179
00:06:58,780 --> 00:07:00,380
번역된다고 가정합니다.

180
00:07:00,380 --> 00:07:01,882
아마 맞을 거라고 생각합니다.

181
00:07:01,882 --> 00:07:03,340
하지만 아이디어는 이 순환

182
00:07:03,340 --> 00:07:05,382
신경망을 한 틱씩 작동시키는 겁니다.

183
00:07:05,382 --> 00:07:07,200
단어를 한 번에 하나씩 출력할 겁니다.

184
00:07:07,200 --> 00:07:09,837
이것은 기본적으로 지난 강의에서 본 내용을 요약한 것입니다.

185
00:07:09,837 --> 00:07:11,420
그래서 이전 강의를

186
00:07:11,420 --> 00:07:15,140
고려하면, 이것이 너무 놀라운 일은 아닙니다.

187
00:07:15,140 --> 00:07:16,920
하지만 여기에는 잠재적인 문제가 있습니다.

188
00:07:16,920 --> 00:07:19,740
입력 시퀀스와 출력 시퀀스

189
00:07:19,740 --> 00:07:23,460
사이에 통신 병목 현상이 있습니다.

190
00:07:23,460 --> 00:07:26,660
입력 시퀀스가 출력 시퀀스와 소통하는 유일한

191
00:07:26,660 --> 00:07:30,020
방법은 그 컨텍스트 벡터 C를 통해서입니다.

192
00:07:30,020 --> 00:07:31,740
그리고 그 C는 고정 길이 벡터가

193
00:07:31,740 --> 00:07:33,740
될 것입니다. 왜냐하면 그 벡터의

194
00:07:33,740 --> 00:07:37,300
크기는 우리가 순환 신경망의 크기를 설정할 때 고정되기 때문입니다.

195
00:07:37,300 --> 00:07:38,760
아마 괜찮을 수도 있습니다.

196
00:07:38,760 --> 00:07:42,420
그래서 C는 128개 또는 1,024개의 float로 이루어진 고정

197
00:07:42,420 --> 00:07:43,880
길이 벡터일 수 있습니다.

198
00:07:43,880 --> 00:07:46,660
하지만 입력 벡터의 크기는 입력과 출력 시퀀스

199
00:07:46,660 --> 00:07:49,300
크기가 커지거나 작아져도 변하지 않습니다.

200
00:07:49,300 --> 00:07:51,080
이것이 잠재적인 문제입니다.

201
00:07:51,080 --> 00:07:53,933
짧은 시퀀스, 예를 들어 하늘을 본다면, 그

202
00:07:53,933 --> 00:07:56,600
1,024개의 float로 이루어진 고정

203
00:07:56,600 --> 00:07:58,880
벡터에 그 시퀀스에 대해 알아야 할

204
00:07:58,880 --> 00:08:00,320
모든 것을 요약할 수

205
00:08:00,320 --> 00:08:01,780
있을 것 같아 보입니다.

206
00:08:01,780 --> 00:08:04,180
하지만 네 단어를 번역하는 것이 아니라면 어떨까요?

207
00:08:04,180 --> 00:08:05,555
한 문단 전체, 한

208
00:08:05,555 --> 00:08:07,360
권의 책, 또는 전체 데이터

209
00:08:07,360 --> 00:08:09,815
집합을 번역하려고 한다면 어떨까요?

210
00:08:09,815 --> 00:08:12,440
그 경우에는 입력 시퀀스가

211
00:08:12,440 --> 00:08:15,140
커질수록 네트워크가 전체 입력

212
00:08:15,140 --> 00:08:18,080
시퀀스를 단일 고정 길이 벡터로

213
00:08:18,080 --> 00:08:20,320
요약하도록 하는 것이

214
00:08:20,320 --> 00:08:22,820
합리적이지 않게 됩니다.

215
00:08:22,820 --> 00:08:25,040
그래서 그것이 문제로 작용할 것입니다.

216
00:08:25,040 --> 00:08:28,160
여기서 해결책은, 사실 하나의

217
00:08:28,160 --> 00:08:33,400
고정 길이 벡터로 네트워크를 병목시키지 않는 겁니다.

218
00:08:33,400 --> 00:08:35,600
대신, 순환 신경망의 아키텍처를

219
00:08:35,600 --> 00:08:37,100
바꾸는 거죠.

220
00:08:37,100 --> 00:08:40,600
직관적으로, 우리가 원하는 것은 입력과 출력 사이에 고정 길이

221
00:08:40,600 --> 00:08:43,020
벡터로 병목 현상을 강요하지 않는 것입니다.

222
00:08:43,020 --> 00:08:45,830
대신, 출력 시퀀스를 처리하면서 모델이

223
00:08:45,830 --> 00:08:48,080
입력 시퀀스를 다시 볼 수 있는

224
00:08:48,080 --> 00:08:49,480
능력을 주는 겁니다.

225
00:08:49,480 --> 00:08:52,080
그리고 이제, 매번 출력 벡터를 생성할 때마다

226
00:08:52,080 --> 00:08:54,860
네트워크가 전체 입력 시퀀스를 다시 볼 수 있는

227
00:08:54,860 --> 00:08:56,192
기회를 주고 싶습니다.

228
00:08:56,192 --> 00:08:58,400
이렇게 하면 병목 현상이 사라집니다.

229
00:08:58,400 --> 00:09:00,238
더 긴 시퀀스에도 확장할 수 있죠.

230
00:09:00,238 --> 00:09:01,780
그리고 모델 아키텍처가 훨씬 더 잘

231
00:09:01,780 --> 00:09:03,060
작동할 거라고 기대합니다.

232
00:09:03,060 --> 00:09:04,900
이것이 바로 오늘날 우리가 보는

233
00:09:04,900 --> 00:09:06,980
attention과 transformer 같은 훌륭한

234
00:09:06,980 --> 00:09:08,560
기술들의 동기가 된 아이디어입니다.

235
00:09:08,560 --> 00:09:10,713
이 모든 것은 순환 신경망의

236
00:09:10,713 --> 00:09:13,380
병목 문제를 해결하려는 시도에서

237
00:09:13,380 --> 00:09:15,620
시작되었다고 이야기할 수 있습니다.

238
00:09:15,620 --> 00:09:18,892
그럼 이 직관을 실제로 어떻게 구현해서 순환

239
00:09:18,892 --> 00:09:21,100
신경망이 매 타임스텝마다 입력 시퀀스를

240
00:09:21,100 --> 00:09:23,980
다시 볼 수 있게 할지 살펴보겠습니다.

241
00:09:23,980 --> 00:09:26,540
여기서는 같은 것부터 시작할 겁니다.

242
00:09:26,540 --> 00:09:28,920
인코더 신경망은 그대로 유지하고,

243
00:09:28,920 --> 00:09:29,960
변경하지 않습니다.

244
00:09:29,960 --> 00:09:32,300
출력 시퀀스의 초기 은닉

245
00:09:32,300 --> 00:09:34,980
상태를 설정해야 합니다.

246
00:09:34,980 --> 00:09:38,660
그래서 초기 디코더 상태 s0를 어떤 방식으로든

247
00:09:38,660 --> 00:09:39,860
설정해야 하죠.

248
00:09:39,860 --> 00:09:42,160
하지만 이제 그 디코더 은닉 상태를 얻으면,

249
00:09:42,160 --> 00:09:45,005
입력 시퀀스를 다시 보는 작업을 할 겁니다.

250
00:09:45,005 --> 00:09:46,380
그래서 우리가

251
00:09:46,380 --> 00:09:50,560
할 방법은 정렬 점수를 계산하는 건데,

252
00:09:50,560 --> 00:09:53,920
기본적으로 입력 시퀀스의 각 단계마다

253
00:09:53,920 --> 00:09:57,760
스칼라 값을 계산해서 초기 디코더

254
00:09:57,760 --> 00:09:59,760
상태 s0가 입력

255
00:09:59,760 --> 00:10:03,680
시퀀스의 각 토큰과 얼마나 일치하는지

256
00:10:03,680 --> 00:10:04,900
평가하는 겁니다.

257
00:10:04,900 --> 00:10:07,623
이 경우에는 입력 시퀀스에 네 개의 토큰이 있었습니다.

258
00:10:07,623 --> 00:10:10,040
그래서 네 개의 정렬 점수를

259
00:10:10,040 --> 00:10:12,200
계산하고 싶은데,

260
00:10:12,200 --> 00:10:16,940
각 점수는 단일 숫자로서 입력 시퀀스의 토큰과

261
00:10:16,940 --> 00:10:21,880
초기 디코더 상태 s0 사이의 유사도가 얼마인지를

262
00:10:21,880 --> 00:10:22,973
나타냅니다.

263
00:10:22,973 --> 00:10:24,640
정렬 점수를 구현하는 방법은

264
00:10:24,640 --> 00:10:26,100
여러 가지가 있습니다.

265
00:10:26,100 --> 00:10:28,520
하지만 간단한 방법은 f sub att라고 부르는

266
00:10:28,520 --> 00:10:30,400
간단한 선형 계층을 사용하는 겁니다.

267
00:10:30,400 --> 00:10:32,280
이 선형 계층은 디코더

268
00:10:32,280 --> 00:10:35,560
은닉 상태 s와 인코더 은닉 상태 h 중 하나를

269
00:10:35,560 --> 00:10:37,240
연결(concatenate)해서

270
00:10:37,240 --> 00:10:39,500
벡터로 만들고, 그

271
00:10:39,500 --> 00:10:42,360
벡터에 선형 변환을 적용해 스칼라 값으로

272
00:10:42,360 --> 00:10:43,345
압축합니다.

273
00:10:43,345 --> 00:10:44,720
이것은 계산 그래프에

274
00:10:44,720 --> 00:10:46,540
넣을 수 있는 선형 연산자로,

275
00:10:46,540 --> 00:10:49,140
네트워크의 다른 모든 파라미터를 학습하는 것처럼

276
00:10:49,140 --> 00:10:51,660
경사 하강법으로 함께 학습할 수 있습니다.

277
00:10:51,660 --> 00:10:55,020
이제 이 시점에서 입력 시퀀스의 각 단계마다

278
00:10:55,020 --> 00:10:57,860
스칼라 정렬 점수를 얻은 상태입니다.

279
00:10:57,860 --> 00:11:00,420
이제, 소프트맥스 함수를 적용하려고 합니다.

280
00:11:00,420 --> 00:11:03,820
이 스칼라 정렬 점수들은 완전히 제한이 없습니다.

281
00:11:03,820 --> 00:11:06,520
이 점수들은 마이너스 무한대부터 무한대까지 임의의 실수 값입니다.

282
00:11:06,520 --> 00:11:08,900
이 값들이 폭주하는 것을 막기 위해서

283
00:11:08,900 --> 00:11:10,460
어떤 구조를 씌우고 싶습니다.

284
00:11:10,460 --> 00:11:13,040
그래서 우리가 하는 한 가지 방법은 소프트맥스 함수를 적용하는 것입니다.

285
00:11:13,040 --> 00:11:15,700
그래서 우리는 디코더 히든 상태와 각 인코더

286
00:11:15,700 --> 00:11:18,020
히든 상태 간의 정렬을 나타내는

287
00:11:18,020 --> 00:11:20,460
네 개의 스칼라 값을 가지고 있습니다.

288
00:11:20,460 --> 00:11:22,900
이제, 이 네 값에 대해

289
00:11:22,900 --> 00:11:27,125
소프트맥스를 적용해서 네 값에 대한 분포를 만듭니다.

290
00:11:27,125 --> 00:11:28,500
기억하세요, 몇 강

291
00:11:28,500 --> 00:11:30,660
전에 본 소프트맥스 함수는

292
00:11:30,660 --> 00:11:32,700
임의의 점수 벡터를 받아서

293
00:11:32,700 --> 00:11:34,722
확률 분포로 변환합니다.

294
00:11:34,722 --> 00:11:36,180
즉, 출력 소프트맥스

295
00:11:36,180 --> 00:11:39,380
확률의 각 항목이 0과 1 사이에 있고,

296
00:11:39,380 --> 00:11:42,900
합이 1이 되는 성질을 갖는다는 뜻입니다.

297
00:11:42,900 --> 00:11:44,460
그래서 벡터를 소프트맥스에

298
00:11:44,460 --> 00:11:46,780
통과시킬 때마다, 우리가

299
00:11:46,780 --> 00:11:48,560
얻는 것은 확률 분포, 좀

300
00:11:48,560 --> 00:11:50,560
더 정확히 말하면 입력 점수들에

301
00:11:50,560 --> 00:11:52,240
대한 이산 확률

302
00:11:52,240 --> 00:11:54,000
분포라고 생각할 수 있습니다.

303
00:11:54,000 --> 00:11:55,260
이 경우에는,

304
00:11:55,260 --> 00:11:57,840
정렬 점수들을 소프트맥스에

305
00:11:57,840 --> 00:12:01,000
통과시킨 후, 본질적으로 디코더

306
00:12:01,000 --> 00:12:04,000
은닉 상태를 기준으로 입력

307
00:12:04,000 --> 00:12:07,880
토큰들에 대한 분포를 예측한 것입니다.

308
00:12:07,880 --> 00:12:10,160
이제 우리가 할 일은 입력

309
00:12:10,160 --> 00:12:13,000
토큰들에 대한 그 분포를

310
00:12:13,000 --> 00:12:18,080
사용해서 인코더의 정보를 요약하는 벡터를 계산하는 것입니다.

311
00:12:18,080 --> 00:12:21,560
그 방법은, 주의(attention) 점수들, 즉

312
00:12:21,560 --> 00:12:26,880
a11, a12, a13, a14 같은 숫자들을 사용하는 건데, 이 숫자들은
모두

313
00:12:26,880 --> 00:12:28,220
0과 1 사이입니다.

314
00:12:28,220 --> 00:12:29,007
이 숫자들의 합은 1입니다.

315
00:12:29,007 --> 00:12:30,840
이제 인코더 은닉

316
00:12:30,840 --> 00:12:34,880
상태들 h1, h2, h3, h4에

317
00:12:34,880 --> 00:12:38,040
주의 점수들을 가중치로 하여

318
00:12:38,040 --> 00:12:40,400
선형 결합을 할 겁니다.

319
00:12:40,400 --> 00:12:42,840
이렇게 하면 보라색으로

320
00:12:42,840 --> 00:12:45,420
표시된 c1이라는 컨텍스트 벡터를

321
00:12:45,420 --> 00:12:47,940
얻는데, 이 벡터는 주의

322
00:12:47,940 --> 00:12:52,700
가중치에 의해 조절된 방식으로 인코더 시퀀스의

323
00:12:52,700 --> 00:12:54,260
정보를 요약합니다.

324
00:12:54,260 --> 00:12:57,140
이 시점에서 c1은 기본적으로 입력

325
00:12:57,140 --> 00:13:01,460
인코더 상태 h1부터 h4까지의 선형 결합입니다.

326
00:13:01,460 --> 00:13:03,180
비주의(attention)

327
00:13:03,180 --> 00:13:04,980
경우와 거의 같은 모습입니다.

328
00:13:04,980 --> 00:13:06,860
컨텍스트 벡터를 가지고 있습니다.

329
00:13:06,860 --> 00:13:10,180
이 벡터를 출력 시퀀스의 첫 번째

330
00:13:10,180 --> 00:13:14,200
토큰 y0와 연결(concatenate)한

331
00:13:14,200 --> 00:13:19,100
뒤, 이를 순환 유닛에 통과시켜서 디코더 RNN의

332
00:13:19,100 --> 00:13:22,140
다음 은닉 상태와 첫 번째

333
00:13:22,140 --> 00:13:24,020
출력 토큰을 얻습니다.

334
00:13:24,020 --> 00:13:27,580
즉, 디코더 RNN의 구조는 사실상

335
00:13:27,580 --> 00:13:29,020
변하지 않았습니다.

336
00:13:29,020 --> 00:13:32,180
단지 컨텍스트 벡터를 계산하는 방식을

337
00:13:32,180 --> 00:13:34,940
이 주의 선형 결합 메커니즘으로

338
00:13:34,940 --> 00:13:36,780
바꿨을 뿐입니다.

339
00:13:36,780 --> 00:13:40,180
하지만 여기서 중요한 점은, 이 컨텍스트

340
00:13:40,180 --> 00:13:42,160
벡터가 기본적으로 출력

341
00:13:42,160 --> 00:13:45,840
RNN이 이 순간에 보고 싶어 하는 입력 시퀀스의

342
00:13:45,840 --> 00:13:49,260
서로 다른 부분들을 주의(attend)하거나

343
00:13:49,260 --> 00:13:51,120
바라본다는 직관입니다.

344
00:13:51,120 --> 00:13:54,920
예를 들어, 입력 시퀀스의 일부에는

345
00:13:54,920 --> 00:13:59,040
우리가 보는 이 두 단어가 포함되어

346
00:13:59,040 --> 00:13:59,920
있습니다.

347
00:13:59,920 --> 00:14:02,840
그래서 우리가 'we see'에 해당하는 이탈리아어

348
00:14:02,840 --> 00:14:05,640
단어 하나를 생성하려고 할 때, 네트워크는

349
00:14:05,640 --> 00:14:08,120
아마도 출력 단어를 결정하기 위해 입력

350
00:14:08,120 --> 00:14:11,280
시퀀스의 그 두 단어를 다시 살펴보고 싶어할 것입니다.

351
00:14:11,280 --> 00:14:13,220
그래서 우리는 직관적으로

352
00:14:13,220 --> 00:14:16,360
'vediamo'라는 단어를 생성하려고

353
00:14:16,360 --> 00:14:20,320
할 때, 네트워크가 'we see'라는 단어를

354
00:14:20,320 --> 00:14:23,840
다시 보고 그 부분에 더 높은 어텐션 가중치를

355
00:14:23,840 --> 00:14:25,900
두길 기대할 수 있습니다.

356
00:14:25,900 --> 00:14:27,860
그리고 하늘(sky)에 대해서는 신경

357
00:14:27,860 --> 00:14:30,840
쓰지 않는데, 그 단어들은 'vediamo'라는 출력을

358
00:14:30,840 --> 00:14:32,865
생성하는 데 필요하지 않기 때문입니다.

359
00:14:32,865 --> 00:14:34,240
이런 게 바로 우리가 네트워크에 주는 직관입니다.

360
00:14:34,240 --> 00:14:35,520
네트워크가 지금

361
00:14:35,520 --> 00:14:37,360
예측하려는 단어에 대해

362
00:14:37,360 --> 00:14:39,402
입력 시퀀스의 관련 부분을

363
00:14:39,402 --> 00:14:42,220
다시 볼 수 있는 능력을 주는 거죠.

364
00:14:42,220 --> 00:14:43,780
그리고 또 한 가지 기억할 점은

365
00:14:43,780 --> 00:14:45,620
이 모든 과정이 미분 가능하다는 겁니다.

366
00:14:45,620 --> 00:14:47,320
우리는 네트워크를 감독할 필요가 없습니다.

367
00:14:47,320 --> 00:14:50,020
출력의 각 단어에 필요한 입력 단어가

368
00:14:50,020 --> 00:14:52,040
무엇인지 알려줄 필요도 없습니다.

369
00:14:52,040 --> 00:14:54,580
대신, 이것은 미분 가능한 연산들로 구성된

370
00:14:54,580 --> 00:14:56,580
큰 계산 그래프일 뿐입니다.

371
00:14:56,580 --> 00:14:59,198
이 모든 것은 경사 하강법을 통해 엔드 투 엔드로 학습할 수 있습니다.

372
00:14:59,198 --> 00:15:00,740
결국에는 네트워크가

373
00:15:00,740 --> 00:15:03,525
출력 시퀀스의 토큰을 예측하려고 하는 크로스

374
00:15:03,525 --> 00:15:05,900
엔트로피 소프트맥스 손실 함수가

375
00:15:05,900 --> 00:15:06,680
있습니다.

376
00:15:06,680 --> 00:15:09,060
그리고 올바른 출력 토큰을 예측하는

377
00:15:09,060 --> 00:15:10,920
과정에서 네트워크는

378
00:15:10,920 --> 00:15:12,940
스스로 입력 시퀀스의 다양한

379
00:15:12,940 --> 00:15:16,140
부분에 어떻게 어텐션을 둘지 학습하게 됩니다.

380
00:15:16,140 --> 00:15:17,540
이 점이 정말 중요합니다.

381
00:15:17,540 --> 00:15:19,980
만약 우리가 직접 감독해서 두 시퀀스 간의

382
00:15:19,980 --> 00:15:21,540
정렬을 알려줘야 한다면, 이런

383
00:15:21,540 --> 00:15:23,373
종류의 학습 데이터를 얻는 것이

384
00:15:23,373 --> 00:15:24,460
매우 어려웠을 겁니다.

385
00:15:24,460 --> 00:15:26,550
그럼 질문은, 디코더를 어떻게 초기화하느냐입니다.

386
00:15:26,550 --> 00:15:28,800
사실 여기서 'initialize'라는 단어를

387
00:15:28,800 --> 00:15:30,175
조금 주의해서 써야 하는데,

388
00:15:30,175 --> 00:15:31,560
약간 과하게 쓰이고 있습니다.

389
00:15:31,560 --> 00:15:34,660
그래서 한 가지 질문은, 디코더 자체가 가중치를 가진

390
00:15:34,660 --> 00:15:35,500
신경망이라는 점입니다.

391
00:15:35,500 --> 00:15:37,000
그 네트워크를 훈련시키기 시작할 때,

392
00:15:37,000 --> 00:15:39,260
우리는 그 가중치들을 어떤 방식으로든 초기화해야 합니다.

393
00:15:39,260 --> 00:15:40,800
그래서 보통 디코더의

394
00:15:40,800 --> 00:15:43,652
가중치를 무작위로 초기화한 다음, 다른

395
00:15:43,652 --> 00:15:46,360
신경망 가중치들처럼 경사 하강법으로

396
00:15:46,360 --> 00:15:47,320
최적화합니다.

397
00:15:47,320 --> 00:15:49,380
하지만 초기화라는 개념이 두 가지가 있습니다.

398
00:15:49,380 --> 00:15:51,680
네트워크가 시퀀스를 처리할

399
00:15:51,680 --> 00:15:55,620
때, 현재 가중치 값이 무엇이든 출력 시퀀스

400
00:15:55,620 --> 00:15:58,600
처리를 시작할 때 초기 은닉

401
00:15:58,600 --> 00:16:01,320
상태를 설정할 방법이 필요합니다.

402
00:16:01,320 --> 00:16:05,200
그럴 때는 디코더 출력 시퀀스의 초기 은닉

403
00:16:05,200 --> 00:16:08,757
상태를 설정하는 규칙이나 방법이 필요합니다.

404
00:16:08,757 --> 00:16:10,840
이것에는 몇 가지 다른 메커니즘이 있습니다.

405
00:16:10,840 --> 00:16:12,320
가끔은 인코더의

406
00:16:12,320 --> 00:16:15,520
마지막 은닉 상태를 초기값으로 사용하는

407
00:16:15,520 --> 00:16:16,980
경우가 있습니다.

408
00:16:16,980 --> 00:16:20,120
또는 마지막 인코더 상태에서 첫

409
00:16:20,120 --> 00:16:22,520
번째 디코더 상태로 학습된

410
00:16:22,520 --> 00:16:23,340
선형

411
00:16:23,340 --> 00:16:26,240
변환을 적용하는 경우도 있습니다.

412
00:16:26,240 --> 00:16:28,360
또는 때로는 디코더의 첫

413
00:16:28,360 --> 00:16:31,907
은닉 상태를 모두 0으로 초기화하기도 합니다.

414
00:16:31,907 --> 00:16:34,240
이 중 어떤 방법이든 네트워크가 그런 입력을

415
00:16:34,240 --> 00:16:36,240
기대하도록 훈련하면 작동합니다.

416
00:16:36,240 --> 00:16:37,880
그럼 질문은, 부정과 XOR

417
00:16:37,880 --> 00:16:39,160
연산이 문제를 일으킬까요?

418
00:16:39,160 --> 00:16:39,660
아마도 그렇습니다.

419
00:16:39,660 --> 00:16:40,822
이건 어려운 문제입니다.

420
00:16:40,822 --> 00:16:42,780
하지만 네트워크가 이를 분리해내길

421
00:16:42,780 --> 00:16:45,460
바라려면 많은 데이터와 많은 연산량이 필요합니다.

422
00:16:45,460 --> 00:16:49,080
기본적으로 순환 유닛은 세 가지를 입력으로 받습니다.

423
00:16:49,080 --> 00:16:50,567
디코더가 입력으로 받습니다.

424
00:16:50,567 --> 00:16:52,900
이전 은닉 상태, 이전 디코더 은닉

425
00:16:52,900 --> 00:16:53,800
상태를 받습니다.

426
00:16:53,800 --> 00:16:55,340
현재 컨텍스트 벡터를 받습니다.

427
00:16:55,340 --> 00:16:59,020
그리고 출력 시퀀스의 현재 토큰을 받습니다.

428
00:16:59,020 --> 00:17:01,620
그다음에 거기서 다음 은닉 상태를 만듭니다.

429
00:17:01,620 --> 00:17:03,120
다음 은닉 상태에서

430
00:17:03,120 --> 00:17:04,885
출력 토큰을 예측하죠.

431
00:17:04,885 --> 00:17:06,260
이것은 사실 비어텐션

432
00:17:06,260 --> 00:17:08,380
경우와 같은 설정입니다.

433
00:17:08,380 --> 00:17:10,060
s0에서 s1로

434
00:17:10,060 --> 00:17:13,780
연결이 있는데, 그걸 그리지 않았네요.

435
00:17:13,780 --> 00:17:17,060
사실 s0에서 s1로 가는 화살표가 하나 더 있어야 합니다.

436
00:17:17,060 --> 00:17:20,391
s0 화살표를 빠뜨렸네요, 죄송합니다.

437
00:17:20,391 --> 00:17:22,099
네트워크가 스스로

438
00:17:22,099 --> 00:17:24,980
입력 시퀀스의 어떤 부분이 과제에

439
00:17:24,980 --> 00:17:28,800
관련 있을지 찾아보도록 하는 겁니다.

440
00:17:28,800 --> 00:17:31,860
이 메커니즘이 타당하고 네트워크에 도움이

441
00:17:31,860 --> 00:17:33,500
될 거라 생각하는

442
00:17:33,500 --> 00:17:36,160
이유는, 언어 과제에서는 출력 단어와

443
00:17:36,160 --> 00:17:37,920
입력 단어 사이에

444
00:17:37,920 --> 00:17:40,620
대응 관계가 자주 있기 때문입니다.

445
00:17:40,620 --> 00:17:43,440
네트워크가 뒤돌아보며 출력의 이 부분을 만들기

446
00:17:43,440 --> 00:17:45,720
위해 입력의 어떤 부분이 중요한지

447
00:17:45,720 --> 00:17:47,025
골라내도록 하고 싶습니다.

448
00:17:47,025 --> 00:17:48,900
하지만 직접적으로 감독하지는 않습니다.

449
00:17:48,900 --> 00:17:51,340
어텐션 점수를 어떻게 사용할지 알려주지 않습니다.

450
00:17:51,340 --> 00:17:52,960
그냥 이 메커니즘을 주면

451
00:17:52,960 --> 00:17:55,320
네트워크가 그렇게 할 가능성이 있다고

452
00:17:55,320 --> 00:17:56,960
직관적으로 생각하는 겁니다.

453
00:17:56,960 --> 00:18:00,520
좋습니다, 출력의 한 타임스텝이 끝났습니다.

454
00:18:00,520 --> 00:18:02,500
이제 다시 같은 과정을 반복합니다.

455
00:18:02,500 --> 00:18:04,560
디코더 RNN을 한 타임스텝 돌릴 때마다

456
00:18:04,560 --> 00:18:06,245
이 전체 과정을 반복합니다.

457
00:18:06,245 --> 00:18:08,120
우리가 해결하려던 문제는,

458
00:18:08,120 --> 00:18:10,360
이전에는 디코더가 하나의 벡터로 병목 현상을

459
00:18:10,360 --> 00:18:11,680
겪었다는 점입니다.

460
00:18:11,680 --> 00:18:12,938
이제는 하나의 벡터로

461
00:18:12,938 --> 00:18:14,980
병목 현상을 겪는 대신,

462
00:18:14,980 --> 00:18:17,397
이 과정을 반복해서 두 번째 디코더

463
00:18:17,397 --> 00:18:20,360
타임스텝을 위한 새로운 컨텍스트 벡터를

464
00:18:20,360 --> 00:18:22,920
계산하고, 다시 입력 시퀀스 전체를

465
00:18:22,920 --> 00:18:23,880
살펴보게 합니다.

466
00:18:23,880 --> 00:18:27,120
이제, 첫 번째 디코더 은닉

467
00:18:27,120 --> 00:18:31,120
상태 s1이 주어졌으니, 다시

468
00:18:31,120 --> 00:18:32,400
돌아갑니다.

469
00:18:32,400 --> 00:18:35,460
s1을 가지고 돌아가서 비교를 하고,

470
00:18:35,460 --> 00:18:38,540
어텐션 메커니즘을 사용해 s1과

471
00:18:38,540 --> 00:18:41,820
인코더의 모든 은닉 상태 사이의 유사도 점수를

472
00:18:41,820 --> 00:18:43,020
계산합니다.

473
00:18:43,020 --> 00:18:44,900
첫 타임스텝에서

474
00:18:44,900 --> 00:18:48,340
썼던 동일한

475
00:18:48,340 --> 00:18:51,740
FATT, 같은 선형 투영을

476
00:18:51,740 --> 00:18:55,100
사용해 정렬 점수를

477
00:18:55,100 --> 00:18:58,120
계산하고,

478
00:18:58,120 --> 00:19:00,260
소프트맥스를 통해

479
00:19:00,260 --> 00:19:02,420
입력 시퀀스에

480
00:19:02,420 --> 00:19:05,700
대한 새로운 분포를

481
00:19:05,700 --> 00:19:07,100
얻습니다.

482
00:19:07,100 --> 00:19:12,620
그리고 두 번째 디코더 타임스텝에서 계산한

483
00:19:12,620 --> 00:19:15,660
이 새로운 분포로 인코더

484
00:19:15,660 --> 00:19:18,700
은닉 상태들의 선형

485
00:19:18,700 --> 00:19:21,500
결합을 다시 계산합니다.

486
00:19:21,500 --> 00:19:23,960
이렇게 해서 새로운 컨텍스트 벡터 c2가 만들어집니다. 이 벡터는 입력
시퀀스를 다르게 요약한 결과입니다.

487
00:19:23,960 --> 00:19:25,320
그리고 이 과정이 반복됩니다.

488
00:19:25,320 --> 00:19:29,340
새로운 컨텍스트

489
00:19:29,340 --> 00:19:32,260
벡터가

490
00:19:32,260 --> 00:19:34,620
생기고,

491
00:19:34,620 --> 00:19:37,140
그걸 사용해 디코더

492
00:19:37,140 --> 00:19:39,500
RNN을 한 타임스텝

493
00:19:39,500 --> 00:19:42,220
더 돌립니다. 이번에는

494
00:19:42,220 --> 00:19:45,580
이전 타임스텝에 없던 그

495
00:19:45,580 --> 00:19:48,960
신비한 화살표도 포함됩니다.

496
00:19:48,960 --> 00:19:49,500
새로운 컨텍스트 벡터, 다음 출력 토큰, 그리고 디코더의 s1 은닉 상태가
주어지면, 새로운 디코더 상태 s2를 계산하고, 거기서 또 다른 출력 토큰을
만듭니다.

497
00:19:49,500 --> 00:19:52,860
다시

498
00:19:52,860 --> 00:19:55,260
말해,

499
00:19:55,260 --> 00:19:56,560
이 경우에는 슬라이드에 따르면 'il'을 출력하는데, 아마 'the'일
겁니다. 그랬으면 좋겠네요.

500
00:19:56,560 --> 00:19:58,887
그리고 이 경우에는, 네트워크가 이

501
00:19:58,887 --> 00:20:00,720
시퀀스에 대해 생성하려는 단어와

502
00:20:00,720 --> 00:20:03,195
출력의 단어 중 하나, 그리고 입력의 단어

503
00:20:03,195 --> 00:20:05,820
중 하나가 일대일 대응 관계일 수도 있습니다.

504
00:20:05,820 --> 00:20:07,720
그래서 네트워크가 입력 시퀀스의 단어

505
00:20:07,720 --> 00:20:09,680
중 단 하나에 상대적으로 높은

506
00:20:09,680 --> 00:20:12,060
attention 가중치를 두고, 나머지 단어들에는

507
00:20:12,060 --> 00:20:14,580
상대적으로 낮은 attention 가중치를 두는 게

508
00:20:14,580 --> 00:20:15,620
예상될 수 있습니다.

509
00:20:15,620 --> 00:20:16,900
하지만 다시 말하지만, 우리는 이것을 감독하지 않습니다.

510
00:20:16,900 --> 00:20:18,320
네트워크가 이 메커니즘을 어떻게

511
00:20:18,320 --> 00:20:20,200
활용할지 스스로 결정하며, 모두 우리의

512
00:20:20,200 --> 00:20:23,000
훈련 과제에 대한 그래디언트 하강법에 의해 이루어집니다.

513
00:20:23,000 --> 00:20:24,680
그리고 이 전체

514
00:20:24,680 --> 00:20:27,040
과정은 디코더 RNN의 매

515
00:20:27,040 --> 00:20:29,600
타임스텝마다 반복됩니다.

516
00:20:29,600 --> 00:20:32,480
이제 이 메커니즘이 기본적으로 우리의 문제를 해결했습니다.

517
00:20:32,480 --> 00:20:34,860
우리는 더 이상 입력 시퀀스를 고정 길이 벡터

518
00:20:34,860 --> 00:20:36,520
하나로 병목 처리하지 않습니다.

519
00:20:36,520 --> 00:20:38,180
대신, 디코더의 매

520
00:20:38,180 --> 00:20:40,383
타임스텝마다 네트워크가 입력

521
00:20:40,383 --> 00:20:42,550
시퀀스 전체를 다시 살펴보고,

522
00:20:42,550 --> 00:20:47,060
입력 시퀀스를 재요약해 그 타임스텝에 맞는 새로운 컨텍스트

523
00:20:47,060 --> 00:20:49,920
벡터를 즉석에서 생성한 뒤, 이를

524
00:20:49,920 --> 00:20:51,900
사용해 출력을 만듭니다.

525
00:20:51,900 --> 00:20:53,980
이것은 꽤 멋진 메커니즘입니다.

526
00:20:53,980 --> 00:20:56,380
그리고 이것을 attention이라고

527
00:20:56,380 --> 00:20:59,260
부르는데, 네트워크가 매 순간 출력할 때 입력

528
00:20:59,260 --> 00:21:03,108
시퀀스의 다른 부분에 주의를 기울이거나 바라본다는 의미입니다.

529
00:21:03,108 --> 00:21:04,900
우리는 이 attention 가중치에 대해 이야기했죠.

530
00:21:04,900 --> 00:21:06,680
그리고 이 가중치들은

531
00:21:06,680 --> 00:21:08,500
네트워크가 훈련 데이터와

532
00:21:08,500 --> 00:21:10,980
훈련 과제에 기반해 스스로 학습해

533
00:21:10,980 --> 00:21:12,900
설정한다는 점을 말했습니다.

534
00:21:12,900 --> 00:21:14,820
attention의 또 다른 멋진

535
00:21:14,820 --> 00:21:17,900
점은, 네트워크가 문제를 해결하려 할 때 어디를 보고

536
00:21:17,900 --> 00:21:19,580
있는지 내부를 들여다볼 수 있는

537
00:21:19,580 --> 00:21:21,020
방법을 제공한다는 겁니다.

538
00:21:21,020 --> 00:21:25,100
우리는 입력 시퀀스와 출력 시퀀스 간의 정렬이 무엇인지

539
00:21:25,100 --> 00:21:27,435
네트워크에 알려주지 않았습니다.

540
00:21:27,435 --> 00:21:29,060
하지만 네트워크가 이 과제를

541
00:21:29,060 --> 00:21:32,220
해결하려 할 때 예측하는 attention 가중치를

542
00:21:32,220 --> 00:21:35,160
보면, 네트워크가 문제를 해결하는 동안 어디를

543
00:21:35,160 --> 00:21:36,840
보고 있었는지 알 수 있습니다.

544
00:21:36,840 --> 00:21:38,360
이것은 신경망의 처리를

545
00:21:38,360 --> 00:21:41,120
어느 정도 해석할 수 있는 방법을 제공합니다.

546
00:21:41,120 --> 00:21:44,640
그래서 우리가 할 수 있는 한 가지는 특정

547
00:21:44,640 --> 00:21:47,680
시퀀스를 처리하는 과정에서 네트워크가 이

548
00:21:47,680 --> 00:21:50,000
작업을 수행할 때 예측한

549
00:21:50,000 --> 00:21:51,920
attention 가중치가 무엇인지

550
00:21:51,920 --> 00:21:53,540
살펴보는 것입니다.

551
00:21:53,540 --> 00:21:56,240
그리고 이를 2차원 격자 형태로 시각화할 수 있습니다.

552
00:21:56,240 --> 00:21:58,440
여기서는 영어에서 프랑스어로

553
00:21:58,440 --> 00:22:01,400
번역하는 예를 보고 있습니다.

554
00:22:01,400 --> 00:22:05,780
위쪽에는 입력 시퀀스가 있습니다.

555
00:22:05,780 --> 00:22:07,720
유럽 경제 지역 협정은

556
00:22:07,720 --> 00:22:09,680
1992년 8월에 체결되었습니다.

557
00:22:09,680 --> 00:22:13,460
그리고 행을 따라 내려가면 출력 시퀀스가 있는데, 프랑스어로

558
00:22:13,460 --> 00:22:16,748
되어 있어서 제가 발음하려고 시도하지는 않겠습니다.

559
00:22:16,748 --> 00:22:19,040
하지만 기본적으로 이 attention

560
00:22:19,040 --> 00:22:20,580
메커니즘을 통해,

561
00:22:20,580 --> 00:22:22,760
기억하시죠, 이 attention

562
00:22:22,760 --> 00:22:25,520
메커니즘은 네트워크가 출력 시퀀스의 각

563
00:22:25,520 --> 00:22:28,040
단어를 생성할 때마다 전체 입력 시퀀스에

564
00:22:28,040 --> 00:22:30,740
대한 확률 분포를 예측한다는 것입니다.

565
00:22:30,740 --> 00:22:33,540
그래서 첫 번째 행에서 그걸 시각화합니다.

566
00:22:33,540 --> 00:22:35,640
이 행렬의 첫 번째 행을 보면,

567
00:22:35,640 --> 00:22:38,540
전체 입력 영어 문장에 대한 예측된

568
00:22:38,540 --> 00:22:40,840
확률 분포를 시각화한 것입니다.

569
00:22:40,840 --> 00:22:44,580
프랑스어 문장의 첫 단어인 "le"를 예측할 때,

570
00:22:44,580 --> 00:22:48,100
영어 단어 "the"에 많은 확률 질량을 할당하고

571
00:22:48,100 --> 00:22:51,580
다른 단어에는 거의 확률 질량을 주지 않는 것을

572
00:22:51,580 --> 00:22:52,938
볼 수 있습니다.

573
00:22:52,938 --> 00:22:55,480
그다음 출력 시퀀스의 두 번째 단어를 예측할

574
00:22:55,480 --> 00:22:57,730
때, 다시 전체 입력 시퀀스에

575
00:22:57,730 --> 00:22:59,480
대한 새로운 분포를 예측합니다.

576
00:22:59,480 --> 00:23:01,880
이것이 이 행렬의 두 번째 행이 됩니다.

577
00:23:01,880 --> 00:23:05,500
보시면 "agreement"에 많은 확률 질량을

578
00:23:05,500 --> 00:23:07,620
할당하고 다른 곳에는 확률 질량이

579
00:23:07,620 --> 00:23:09,920
거의 없는 것을 알 수 있습니다.

580
00:23:09,920 --> 00:23:12,900
이로써 네트워크가 번역 작업을 수행할 때

581
00:23:12,900 --> 00:23:15,220
입력 단어와 출력 단어 간의

582
00:23:15,220 --> 00:23:18,620
정렬을 실제로 파악했다는 것을 알 수 있습니다.

583
00:23:18,620 --> 00:23:21,660
여기서 흥미로운 패턴들이 나타납니다.

584
00:23:21,660 --> 00:23:23,940
이 attention 행렬에서

585
00:23:23,940 --> 00:23:25,820
대각선 구조를 보면, 입력

586
00:23:25,820 --> 00:23:28,760
시퀀스와 출력 시퀀스의 단어들이 순서대로

587
00:23:28,760 --> 00:23:31,000
일대일 대응 관계에 있다는

588
00:23:31,000 --> 00:23:31,720
의미입니다.

589
00:23:31,720 --> 00:23:33,960
특히, 입력 시퀀스의 처음 네

590
00:23:33,960 --> 00:23:36,840
단어인 "the"에 대한 일치가 어텐션

591
00:23:36,840 --> 00:23:39,840
매트릭스의 대각선 구조에 해당한다는 것을 볼 수

592
00:23:39,840 --> 00:23:40,540
있습니다.

593
00:23:40,540 --> 00:23:43,080
즉, 네트워크가 스스로 입력

594
00:23:43,080 --> 00:23:45,840
시퀀스의 처음 네 단어가 서로

595
00:23:45,840 --> 00:23:49,200
정렬되거나 일치하거나 대응된다고 결정했다는

596
00:23:49,200 --> 00:23:51,280
의미이고, 마지막 몇

597
00:23:51,280 --> 00:23:53,053
단어도 마찬가지입니다.

598
00:23:53,053 --> 00:23:54,720
다시 말해, 시퀀스 끝부분에서

599
00:23:54,720 --> 00:23:58,000
대각선 구조를 볼 수 있는데, 이는 1992년 8월

600
00:23:58,000 --> 00:24:02,160
또는 1992년 8월에 해당하는 프랑스어 시퀀스의 마지막 몇 단어와

601
00:24:02,160 --> 00:24:03,580
대응된다는 뜻입니다.

602
00:24:03,580 --> 00:24:05,622
그리고 출력 단어와 입력 단어 사이에

603
00:24:05,622 --> 00:24:08,472
일대일 대응이 있다는 것을 다시 확인할 수 있습니다.

604
00:24:08,472 --> 00:24:10,680
하지만 중간 부분에서는 다른 흥미로운 점들도 볼

605
00:24:10,680 --> 00:24:11,260
수 있습니다.

606
00:24:11,260 --> 00:24:14,660
중간 부분에서는 European economic area가 보입니다.

607
00:24:14,660 --> 00:24:17,040
하지만 프랑스어에서는 약간 다른

608
00:24:17,040 --> 00:24:19,977
순서로 보이는 비슷한 단어들이 있습니다.

609
00:24:19,977 --> 00:24:22,060
좋은 질문입니다, 문법을 어떻게 알아내는 걸까요?

610
00:24:22,060 --> 00:24:24,172
그것이 바로 딥러닝의 신비입니다.

611
00:24:24,172 --> 00:24:26,880
기본적으로 우리는 네트워크에 문법에 대해 아무것도

612
00:24:26,880 --> 00:24:27,677
알려주지 않았습니다.

613
00:24:27,677 --> 00:24:29,260
우리는 네트워크에 많은 입력-출력

614
00:24:29,260 --> 00:24:30,840
쌍으로 감독 학습을 시켰습니다.

615
00:24:30,840 --> 00:24:33,360
영어로 된 입력 시퀀스가 여기 있습니다.

616
00:24:33,360 --> 00:24:34,960
프랑스어로 된 출력 시퀀스가 여기 있습니다.

617
00:24:34,960 --> 00:24:37,060
이것을 처리하는 메커니즘이

618
00:24:37,060 --> 00:24:40,580
있고, 경사 하강법을 통해 이 아키텍처의 가중치를

619
00:24:40,580 --> 00:24:43,180
조정하여 이 입력으로부터 이

620
00:24:43,180 --> 00:24:45,460
출력을 만들어내도록 학습합니다.

621
00:24:45,460 --> 00:24:47,700
우리는 문법에 대해 아무것도 알려준 적이 없습니다.

622
00:24:47,700 --> 00:24:50,420
하지만 인간 설계자로서 일부 단어들

623
00:24:50,420 --> 00:24:52,540
사이에 어떤 대응이 있어야

624
00:24:52,540 --> 00:24:54,832
한다는 직관이 있어서, 이

625
00:24:54,832 --> 00:24:57,220
문제를 해결하는 데 도움이 될

626
00:24:57,220 --> 00:24:59,740
것 같은 메커니즘을 설계에

627
00:24:59,740 --> 00:25:00,475
포함시켰습니다.

628
00:25:00,475 --> 00:25:02,100
그리고 네트워크는 엔드 투

629
00:25:02,100 --> 00:25:04,380
엔드 작업을 수행하는 과정에서 그 메커니즘을

630
00:25:04,380 --> 00:25:08,380
어떻게 활용해 우리가 설정한 문제를 해결할지 스스로 알아냅니다.

631
00:25:08,380 --> 00:25:10,050
그리고 그것이 작동한다는 것은 정말 놀라운 일입니다.

632
00:25:12,700 --> 00:25:16,320
하지만 이 경우에는 네트워크가 스스로 일부 문법을 알아냈습니다.

633
00:25:16,320 --> 00:25:18,860
그래서 여기 어텐션 매트릭스에서 비대각선,

634
00:25:18,860 --> 00:25:21,320
일종의 역대각선을 볼 수 있습니다.

635
00:25:21,320 --> 00:25:23,112
이것은 네트워크가 영어

636
00:25:23,112 --> 00:25:26,960
단어와 프랑스어 단어 사이의 다른 단어 순서를 스스로

637
00:25:26,960 --> 00:25:28,960
알아냈다는 의미입니다.

638
00:25:28,960 --> 00:25:32,920
중간에는 2x2 격자가 조금 보이는데요.

639
00:25:32,920 --> 00:25:33,500
중간에는 2x2 격자가 조금 보이는데요.

640
00:25:33,500 --> 00:25:35,978
이것은 영어 단어와 프랑스어 단어

641
00:25:35,978 --> 00:25:37,520
사이에 일대일 대응이

642
00:25:37,520 --> 00:25:39,420
없었던 상황에 해당합니다.

643
00:25:39,420 --> 00:25:41,400
두 개의 프랑스어 단어가 두 개의 영어

644
00:25:41,400 --> 00:25:43,317
단어에 대응했지만 완벽하게 분리되지

645
00:25:43,317 --> 00:25:44,627
않았을 수도 있다는 뜻입니다.

646
00:25:44,627 --> 00:25:46,960
즉, 네트워크는 많은 데이터로 학습하고

647
00:25:46,960 --> 00:25:49,280
많은 계산을 거치면서 이 모든

648
00:25:49,280 --> 00:25:51,080
것을 스스로 알아내는 겁니다.

649
00:25:51,080 --> 00:25:53,360
그리고 그 점이 정말 멋집니다.

650
00:25:53,360 --> 00:25:56,240
좋습니다, 사실 여기에는—이것이

651
00:25:56,240 --> 00:25:58,760
기계 학습에서 어텐션이 처음 사용된

652
00:25:58,760 --> 00:25:59,920
사례입니다.

653
00:25:59,920 --> 00:26:03,320
이것은 실제로 기계 번역 문제에서 시작되었습니다.

654
00:26:03,320 --> 00:26:06,600
2015년에 발표된 'Neural Machine Translation

655
00:26:06,600 --> 00:26:10,720
by Jointly Learning to Align and
Translate'라는

656
00:26:10,720 --> 00:26:11,840
논문에서 나온 것입니다.

657
00:26:11,840 --> 00:26:14,840
이 논문은 ICLR 2025에서 타임 어워드

658
00:26:14,840 --> 00:26:16,720
준우승을 차지하기도 했습니다.

659
00:26:16,720 --> 00:26:19,400
정말 멋진 일이죠.

660
00:26:19,400 --> 00:26:22,440
이 논문은 시간이 지나면서 매우 영향력 있는 논문이 되었습니다.

661
00:26:22,440 --> 00:26:24,160
하지만 사실 여기에는 더

662
00:26:24,160 --> 00:26:27,380
일반적인 아이디어와 더 일반적인 연산자가 숨어 있다는 것이

663
00:26:27,380 --> 00:26:28,140
밝혀졌습니다.

664
00:26:28,140 --> 00:26:30,300
우리는 이 문제에 대해 순환

665
00:26:30,300 --> 00:26:33,040
신경망을 고치려는 관점에서 접근합니다.

666
00:26:33,040 --> 00:26:34,860
하지만 순환 신경망을 고치기

667
00:26:34,860 --> 00:26:37,380
위해 사용한 메커니즘이 사실은 일반적이고

668
00:26:37,380 --> 00:26:39,180
흥미로우며 그 자체로

669
00:26:39,180 --> 00:26:41,300
매우 강력한 것임이 밝혀졌습니다.

670
00:26:41,300 --> 00:26:43,860
그래서 이제 우리는 이 주의(attention)라는

671
00:26:43,860 --> 00:26:47,300
아이디어를 꺼내서, 주의 개념을 순환

672
00:26:47,300 --> 00:26:49,280
신경망과 분리하려고 합니다.

673
00:26:49,280 --> 00:26:51,100
그리고 주의는 순환

674
00:26:51,100 --> 00:26:53,580
신경망 부분을 떼어내고 주의만

675
00:26:53,580 --> 00:26:56,520
남겨도, 그 자체로 신경망에서

676
00:26:56,520 --> 00:26:59,820
매우 유용하고 강력한 계산 원시

677
00:26:59,820 --> 00:27:01,780
연산자가 될 수 있다는

678
00:27:01,780 --> 00:27:03,600
것이 밝혀졌습니다.

679
00:27:03,600 --> 00:27:06,380
그리고 바로 그 방향으로 나아가고 있습니다.

680
00:27:06,380 --> 00:27:09,400
그래서 이제 우리가 할 일은, 순환

681
00:27:09,400 --> 00:27:11,240
신경망에서 봤던 주의 개념을

682
00:27:11,240 --> 00:27:13,300
일반화하고, 독립적으로

683
00:27:13,300 --> 00:27:16,700
사용할 수 있는 연산자로 분리하는 것입니다.

684
00:27:16,700 --> 00:27:19,940
그럼 이 주의 메커니즘이 무엇을 하고 있었는지 생각해 봅시다.

685
00:27:19,940 --> 00:27:22,540
기본적으로 이 주의 메커니즘은 여러

686
00:27:22,540 --> 00:27:24,900
개의 쿼리 벡터가 있었습니다.

687
00:27:24,900 --> 00:27:26,995
음, 아마도 이걸 반대 순서로 이야기하는

688
00:27:26,995 --> 00:27:28,620
게 더 이해가 될 겁니다.

689
00:27:28,620 --> 00:27:30,640
데이터 벡터들이 있는데, 이들은 우리가

690
00:27:30,640 --> 00:27:32,100
요약하려는 데이터와 같습니다.

691
00:27:32,100 --> 00:27:35,660
이들은 인코더 RNN의 인코더 상태들이죠.

692
00:27:35,660 --> 00:27:37,180
입력 시퀀스가 있고,

693
00:27:37,180 --> 00:27:39,640
그걸 벡터 시퀀스로 요약했습니다.

694
00:27:39,640 --> 00:27:41,372
이 벡터 시퀀스는 우리가

695
00:27:41,372 --> 00:27:43,080
해결하려는 문제에 관련 있다고

696
00:27:43,080 --> 00:27:44,680
생각하는 데이터입니다.

697
00:27:44,680 --> 00:27:48,060
이제 그 데이터를 활용하는 과정에서 여러

698
00:27:48,060 --> 00:27:50,060
출력을 만들어내려고 합니다.

699
00:27:50,060 --> 00:27:52,420
각 출력마다 쿼리 벡터가 있습니다.

700
00:27:52,420 --> 00:27:54,240
쿼리 벡터란 어떤 출력을

701
00:27:54,240 --> 00:27:56,000
만들어내기 위해

702
00:27:56,000 --> 00:27:57,760
사용하는 벡터입니다.

703
00:27:57,760 --> 00:27:59,640
이 경우 쿼리 벡터는

704
00:27:59,640 --> 00:28:03,480
디코더 RNN의 은닉 상태입니다.

705
00:28:03,480 --> 00:28:07,000
그리고 각 쿼리 벡터마다

706
00:28:07,000 --> 00:28:09,460
데이터 벡터를

707
00:28:09,460 --> 00:28:12,360
다시 보고, 데이터

708
00:28:12,360 --> 00:28:16,760
벡터의 정보를 요약해 컨텍스트

709
00:28:16,760 --> 00:28:19,452
벡터를 만들고자

710
00:28:19,452 --> 00:28:20,660
합니다.

711
00:28:20,660 --> 00:28:23,100
주의 연산자의 출력은 바로 우리가

712
00:28:23,100 --> 00:28:25,900
방금 말한 RNN의 컨텍스트 벡터입니다.

713
00:28:25,900 --> 00:28:27,820
그래서 주의 연산자가 하는

714
00:28:27,820 --> 00:28:30,100
일이 무엇인지 생각해 보면,

715
00:28:30,100 --> 00:28:33,060
주의 연산자의 출력은 RNN에 입력되는

716
00:28:33,060 --> 00:28:34,540
컨텍스트 벡터입니다.

717
00:28:34,540 --> 00:28:36,800
그렇다면 주의 연산자는 무엇을 하는 걸까요?

718
00:28:36,800 --> 00:28:39,740
주의 연산자는 쿼리 벡터를 받아서 입력

719
00:28:39,740 --> 00:28:42,340
데이터 벡터를 다시 보고, 데이터를 새로운

720
00:28:42,340 --> 00:28:45,500
방식으로 요약해 출력 벡터를 만들어냅니다.

721
00:28:45,500 --> 00:28:48,560
바로 그것이 주의 연산자가 하는 일입니다.

722
00:28:48,560 --> 00:28:51,420
이것이 우리가 방금 본 주의 메커니즘을

723
00:28:51,420 --> 00:28:53,712
일반화한 개념으로 이해가 되시나요?

724
00:28:53,712 --> 00:28:55,442
[잘 안 들림] 네, 다시 한 번

725
00:28:55,442 --> 00:28:57,400
설명하겠습니다. 이게 좀 까다롭거든요.

726
00:28:57,400 --> 00:28:59,240
여기서 여러 가지가 복잡하게 얽혀 있고, 상자들이 많이 등장합니다.

727
00:28:59,240 --> 00:29:00,865
그리고 상자를 정의하는

728
00:29:00,865 --> 00:29:02,240
단어들도 바뀌고 있죠.

729
00:29:02,240 --> 00:29:03,820
그래서 이해가 되실 겁니다, 여러 가지가 일어나고 있으니까요.

730
00:29:03,820 --> 00:29:06,180
주의 연산자가 하는 일은,

731
00:29:06,180 --> 00:29:07,980
여러 데이터 벡터(인코더

732
00:29:07,980 --> 00:29:09,740
은닉 상태들)가 있고,

733
00:29:09,740 --> 00:29:11,740
그다음에 여러 쿼리 벡터가

734
00:29:11,740 --> 00:29:15,580
있는데, 이 쿼리 벡터들은 우리가 출력하려는 대상들입니다.

735
00:29:15,580 --> 00:29:18,083
이제 쿼리 벡터를 처리하는 과정에서,

736
00:29:18,083 --> 00:29:19,500
각 쿼리 벡터마다

737
00:29:19,500 --> 00:29:22,920
데이터 벡터로 돌아가서 데이터를 새로운 맞춤

738
00:29:22,920 --> 00:29:27,040
방식으로 요약하고, 그 결과로 출력 벡터를 만들어냅니다.

739
00:29:27,040 --> 00:29:29,520
이 출력 벡터가 다음 RNN

740
00:29:29,520 --> 00:29:31,800
단계에 입력되는 컨텍스트입니다.

741
00:29:31,800 --> 00:29:34,140
그래서 우리의 쿼리 벡터는 초록색으로 표시된 이 벡터들입니다.

742
00:29:34,140 --> 00:29:36,540
각 쿼리 벡터마다 데이터 벡터로 돌아가

743
00:29:36,540 --> 00:29:38,480
데이터를 요약하고, 새로운 출력 벡터를

744
00:29:38,480 --> 00:29:40,920
만들어냅니다. 이 출력 벡터는 네트워크의

745
00:29:40,920 --> 00:29:43,640
나머지 부분에 입력되는 컨텍스트 중 하나입니다.

746
00:29:43,640 --> 00:29:45,920
이 부분이 까다로운데, 왜냐하면

747
00:29:45,920 --> 00:29:49,640
이 아키텍처에서 어텐션 부분만 조심스럽게 잘라내서

748
00:29:49,640 --> 00:29:52,640
RNN에서 분리하려고 하기 때문입니다.

749
00:29:52,640 --> 00:29:55,320
그래서 이번에는 어텐션 연산자

750
00:29:55,320 --> 00:29:57,832
관점에서 다시 한 번 살펴보겠습니다.

751
00:29:57,832 --> 00:30:00,663
어텐션 연산자 관점에서 보면,

752
00:30:00,663 --> 00:30:02,080
처음에는 RNN의

753
00:30:02,080 --> 00:30:06,920
상태 중 하나인 단일 쿼리 벡터부터 시작합니다.

754
00:30:06,920 --> 00:30:08,700
그리고 RNN의 인코더

755
00:30:08,700 --> 00:30:12,160
숨겨진 상태인 여러 데이터 벡터들도 있습니다.

756
00:30:12,160 --> 00:30:14,120
우리가 수행하려는 계산은 먼저

757
00:30:14,120 --> 00:30:17,520
그 쿼리 벡터와 모든 데이터 벡터 간의 유사도를

758
00:30:17,520 --> 00:30:18,740
계산하는 것입니다.

759
00:30:18,740 --> 00:30:20,020
이것은 방금 본 것과 정확히

760
00:30:20,020 --> 00:30:21,820
같은 내용인데, 다르게 표현한 것뿐입니다.

761
00:30:21,820 --> 00:30:26,660
그래서 FATT 함수를 사용해 각 데이터

762
00:30:26,660 --> 00:30:30,820
벡터와 쿼리 벡터 간의 유사도 점수를

763
00:30:30,820 --> 00:30:32,100
계산합니다.

764
00:30:32,100 --> 00:30:33,760
그 다음, 이 유사도

765
00:30:33,760 --> 00:30:35,660
점수들을 소프트맥스에 통과시켜 어텐션

766
00:30:35,660 --> 00:30:37,080
가중치를 얻습니다.

767
00:30:37,080 --> 00:30:39,300
이 가중치는 이 한 쿼리 벡터에

768
00:30:39,300 --> 00:30:43,420
대해 즉석에서 계산된 데이터 벡터들에 대한 분포입니다.

769
00:30:43,420 --> 00:30:46,520
그 다음 우리가 하고자 하는 것은 출력 벡터를 만드는 것입니다.

770
00:30:46,520 --> 00:30:50,160
이 출력 벡터는 데이터 벡터들의 선형 결합이며,

771
00:30:50,160 --> 00:30:53,060
이때 선형 결합 가중치는 방금 계산한 어텐션

772
00:30:53,060 --> 00:30:54,200
점수들입니다.

773
00:30:54,200 --> 00:30:56,620
이것이 어텐션 레이어의 출력입니다.

774
00:30:56,620 --> 00:30:59,340
그리고 우리가 본 더 큰 RNN 맥락에서,

775
00:30:59,340 --> 00:31:02,300
어텐션 레이어 또는 어텐션 연산자의

776
00:31:02,300 --> 00:31:05,940
출력은 디코더 RNN의 다음 단계 입력이 됩니다.

777
00:31:05,940 --> 00:31:07,720
하지만 우리는 RNN을 점차 사용하지 않으려 하기 때문에 그

778
00:31:07,720 --> 00:31:08,760
부분에 대해서는 이야기하지 않겠습니다.

779
00:31:08,760 --> 00:31:10,020
우리는 단지 attention에

780
00:31:10,020 --> 00:31:12,500
대해서만 이야기하고, attention 레이어 내부에서 일어나는

781
00:31:12,500 --> 00:31:13,620
계산에 집중하려고 합니다.

782
00:31:13,620 --> 00:31:18,160
이것이 기본적으로 RNN에서 봤던 연산자입니다.

783
00:31:18,160 --> 00:31:20,000
우리는 이 과정을 반복했죠.

784
00:31:20,000 --> 00:31:21,440
쿼리 벡터를 가져와서

785
00:31:21,440 --> 00:31:23,320
유사도 점수를 계산하고,

786
00:31:23,320 --> 00:31:26,060
attention 가중치를 얻고, 출력 벡터를

787
00:31:26,060 --> 00:31:27,128
얻는 과정을요.

788
00:31:27,128 --> 00:31:28,420
그다음 새로운 쿼리 벡터를 얻었습니다.

789
00:31:28,420 --> 00:31:30,003
그 쿼리 벡터는 어디서 나온 걸까요?

790
00:31:30,003 --> 00:31:31,500
Attention 연산자는 그 부분에 신경 쓰지 않습니다.

791
00:31:31,500 --> 00:31:34,380
새로운 쿼리 벡터를 받아서 다시 돌아가 데이터 벡터들을 요약하고,

792
00:31:34,380 --> 00:31:35,680
새로운 출력 벡터를 얻습니다.

793
00:31:35,680 --> 00:31:38,120
이것이 attention 연산자의 핵심입니다.

794
00:31:38,120 --> 00:31:39,640
이제 이것을 일반화해서

795
00:31:39,640 --> 00:31:43,000
더 강력한 계산 원시 연산자로 만들어 보겠습니다.

796
00:31:43,000 --> 00:31:46,117
네, 원칙적으로 이 FATT는 어떤

797
00:31:46,117 --> 00:31:47,700
함수여도 상관없습니다.

798
00:31:47,700 --> 00:31:49,533
두 벡터를 입력으로 받아 스칼라를 출력하는 어떤

799
00:31:49,533 --> 00:31:50,898
함수도 될 수 있습니다, 원칙적으로는요.

800
00:31:50,898 --> 00:31:52,440
하지만 실제로는 몇

801
00:31:52,440 --> 00:31:55,022
슬라이드 후에 더 간단하게 만들 예정입니다.

802
00:31:55,022 --> 00:31:57,480
하지만 원칙적으로는 원하는 어떤

803
00:31:57,480 --> 00:32:00,310
함수든 그 자리에 넣을 수 있습니다.

804
00:32:00,310 --> 00:32:02,560
좋습니다, 우리가 할 첫 번째 일반화는

805
00:32:02,560 --> 00:32:04,640
사실 방금 제안한 것과 반대로,

806
00:32:04,640 --> 00:32:07,800
그 유사도 함수를 더 단순하게 만드는 것입니다.

807
00:32:07,800 --> 00:32:09,520
우리는 원칙적으로 두 벡터를 받아

808
00:32:09,520 --> 00:32:11,040
유사도 점수를 내는 어떤

809
00:32:11,040 --> 00:32:12,640
함수도 될 수 있다고 했습니다.

810
00:32:12,640 --> 00:32:15,520
두 벡터를 입력받아 스칼라 유사도 점수를 출력하는

811
00:32:15,520 --> 00:32:17,620
가장 간단한 함수는 무엇일까요?

812
00:32:17,620 --> 00:32:18,800
바로 내적(dot product)입니다.

813
00:32:18,800 --> 00:32:21,420
우리는 가능한 한 단순하게 하면서도 동시에

814
00:32:21,420 --> 00:32:23,122
일반화할 수 있기를 원합니다.

815
00:32:23,122 --> 00:32:24,580
내적이 이 목적에

816
00:32:24,580 --> 00:32:26,380
충분히 좋은 유사도 점수라는

817
00:32:26,380 --> 00:32:27,960
것이 밝혀졌습니다.

818
00:32:27,960 --> 00:32:29,460
그래서 첫 번째로 할

819
00:32:29,460 --> 00:32:34,340
일은, 실제로 오직 내적만을 사용해 유사도를 계산하는 것입니다.

820
00:32:34,340 --> 00:32:36,180
하지만 내적에는 약간의

821
00:32:36,180 --> 00:32:37,460
문제가 있습니다.

822
00:32:37,460 --> 00:32:39,700
이 문제는 내적과 소프트맥스(softmax)

823
00:32:39,700 --> 00:32:42,980
사이의 미묘한 상호작용 때문입니다.

824
00:32:42,980 --> 00:32:45,580
이것은 벡터 차원이 커지거나

825
00:32:45,580 --> 00:32:48,960
작아질 때 발생하는 현상과 관련이 있습니다.

826
00:32:48,960 --> 00:32:50,900
예를 들어, 차원이

827
00:32:50,900 --> 00:32:52,820
10인 모든

828
00:32:52,820 --> 00:32:55,100
원소가 1인 벡터와

829
00:32:55,100 --> 00:32:57,020
차원이 100인

830
00:32:57,020 --> 00:33:00,300
모든 원소가 1인 벡터를

831
00:33:00,300 --> 00:33:03,660
생각해보면, 차원이 커질수록

832
00:33:03,660 --> 00:33:05,860
소프트맥스 내부의 합을

833
00:33:05,860 --> 00:33:08,260
계산할 때 더 큰

834
00:33:08,260 --> 00:33:10,280
수로 나누게 됩니다.

835
00:33:10,280 --> 00:33:13,180
그래서 차원이 커질수록 확률 점수가

836
00:33:13,180 --> 00:33:15,430
더 압축되어 표현됩니다.

837
00:33:15,430 --> 00:33:17,010
이로 인해 이전 강의에서 본

838
00:33:17,010 --> 00:33:18,593
것처럼 그래디언트 소실이 발생하고

839
00:33:18,593 --> 00:33:20,470
전체 학습이 방해받을 수 있습니다.

840
00:33:20,470 --> 00:33:26,230
그래서 이를 방지하고 이 아키텍처가 다양한 차원의 벡터에

841
00:33:26,230 --> 00:33:28,390
대해 더 일반화되고

842
00:33:28,390 --> 00:33:30,990
확장 가능하도록 하기 위해,

843
00:33:30,990 --> 00:33:33,370
순수 내적 대신 벡터

844
00:33:33,370 --> 00:33:35,990
차원의 제곱근으로 내적 값을

845
00:33:35,990 --> 00:33:38,702
나누는 방식을 사용합니다.

846
00:33:38,702 --> 00:33:40,910
이 방법은 그래디언트 소실을

847
00:33:40,910 --> 00:33:44,110
막고 소프트맥스를 통한 그래디언트 흐름을 더

848
00:33:44,110 --> 00:33:45,805
원활하게 만들어 줍니다.

849
00:33:45,805 --> 00:33:47,430
이것은 매우 중요한데, 시간이

850
00:33:47,430 --> 00:33:49,830
지남에 따라 네트워크가 점점 커질수록 더

851
00:33:49,830 --> 00:33:52,590
높은 차원의 벡터를 사용하고 싶기 때문입니다. 이는

852
00:33:52,590 --> 00:33:54,692
더 많은 계산량과 용량을 의미합니다.

853
00:33:54,692 --> 00:33:57,150
그래서 아키텍처의 각 부분이

854
00:33:57,150 --> 00:33:59,790
점점 커질 때 어떻게 확장할지

855
00:33:59,790 --> 00:34:01,870
항상 고민해야 합니다.

856
00:34:01,870 --> 00:34:04,390
따라서 이 스케일된 내적(scaled dot product)은

857
00:34:04,390 --> 00:34:07,065
그래디언트 소실을 방지하는 데 정말 중요합니다.

858
00:34:07,065 --> 00:34:09,190
네, 질문은 데이터와 쿼리 벡터가 같은

859
00:34:09,190 --> 00:34:12,670
크기로 제한된다는 것이었는데, 사실 그 부분을 해결할 겁니다.

860
00:34:12,670 --> 00:34:15,010
그래서 첫 번째 일반화는 실제로 scaled

861
00:34:15,010 --> 00:34:19,210
dot product similarity를 유사도 측정으로 사용하는
것이었습니다.

862
00:34:19,210 --> 00:34:21,830
이제 다시 이들의 형태를 보면, 쿼리

863
00:34:21,830 --> 00:34:24,010
벡터 하나가 차원 Dq를 가집니다.

864
00:34:24,010 --> 00:34:25,730
데이터 벡터는 Nx 곱하기

865
00:34:25,730 --> 00:34:30,250
Dq 차원인데, dot product를 하기 때문에 크기가 맞아야 합니다.

866
00:34:30,250 --> 00:34:32,850
하지만 다음 일반화는 여러

867
00:34:32,850 --> 00:34:36,070
개의 쿼리 벡터를 사용하는 것입니다.

868
00:34:36,070 --> 00:34:39,590
한 번에 하나의 쿼리 벡터만 처리하는 것이 아니라, 여러

869
00:34:39,590 --> 00:34:41,770
쿼리 벡터를 한꺼번에 처리할 수 있는

870
00:34:41,770 --> 00:34:43,850
능력을 갖추고 싶을 수 있습니다.

871
00:34:43,850 --> 00:34:45,570
이것은 RNN에서 실제로 일어납니다.

872
00:34:45,570 --> 00:34:47,540
우리는 결국 여러 개의 쿼리 벡터를 갖게 되었죠.

873
00:34:47,540 --> 00:34:49,290
그리고 attention

874
00:34:49,290 --> 00:34:52,190
연산자가 한 번에 하나가

875
00:34:52,190 --> 00:34:55,710
아니라 여러 쿼리 벡터 집합을 병렬로

876
00:34:55,710 --> 00:34:58,330
처리하고, 같은 계산을 병렬로

877
00:34:58,330 --> 00:35:00,570
수행하는 것이 유용합니다.

878
00:35:00,570 --> 00:35:02,950
그래서 이 경우, 이제 N으로 일반화했습니다.

879
00:35:02,950 --> 00:35:06,290
그래서 Q는 이제 Nq 곱하기 Dq 형태의 행렬입니다.

880
00:35:06,290 --> 00:35:08,190
Nq개의 쿼리 벡터가 있습니다.

881
00:35:08,190 --> 00:35:10,450
각 쿼리 벡터는 Dq 차원을 가집니다.

882
00:35:10,450 --> 00:35:14,670
데이터 벡터는 Nx 곱하기 Dq 크기의 행렬입니다.

883
00:35:14,670 --> 00:35:18,150
이제 계산이 조금 바뀌는데, 정렬

884
00:35:18,150 --> 00:35:21,550
점수, 즉 유사도를 계산할 때,

885
00:35:21,550 --> 00:35:23,390
모든 입력 데이터

886
00:35:23,390 --> 00:35:27,350
벡터와 모든 입력 쿼리 벡터 간의

887
00:35:27,350 --> 00:35:31,410
모든 쌍의 유사도를 계산하려고 합니다.

888
00:35:31,410 --> 00:35:33,510
그리고 각 유사도는 dot product

889
00:35:33,510 --> 00:35:36,170
또는 scaled dot product입니다.

890
00:35:36,170 --> 00:35:38,750
그럼 두 세트의 입력 벡터 사이의 내적을

891
00:35:38,750 --> 00:35:41,430
계산하는 아주 효율적이고 쉽고 자연스러운

892
00:35:41,430 --> 00:35:43,050
방법은 무엇일까요?

893
00:35:43,050 --> 00:35:45,368
그것은 바로 행렬 곱셈입니다.

894
00:35:45,368 --> 00:35:47,410
왜냐하면 행렬 곱셈을 할 때

895
00:35:47,410 --> 00:35:50,390
출력 행렬의 각 원소는 한 행렬의 열 벡터와

896
00:35:50,390 --> 00:35:51,990
다른 행렬의 행 벡터의

897
00:35:51,990 --> 00:35:53,570
내적이기 때문입니다.

898
00:35:53,570 --> 00:35:56,230
즉, 행렬 곱셈의 출력에 있는 각

899
00:35:56,230 --> 00:35:59,070
원소는 행과 열 벡터 사이의

900
00:35:59,070 --> 00:36:00,990
정확한 내적이라는 겁니다.

901
00:36:00,990 --> 00:36:03,470
그래서 쿼리 벡터 Q와 데이터

902
00:36:03,470 --> 00:36:07,283
벡터 X 사이에 행렬 곱셈을 계산하는데,

903
00:36:07,283 --> 00:36:08,950
행과 열이

904
00:36:08,950 --> 00:36:11,650
올바르게 맞도록 전치(transpose)를

905
00:36:11,650 --> 00:36:12,970
해주면,

906
00:36:12,970 --> 00:36:15,850
이 방법으로 모든 데이터

907
00:36:15,850 --> 00:36:17,490
벡터와 모든 쿼리

908
00:36:17,490 --> 00:36:21,940
벡터 간의 유사도를 한 번에 계산할 수 있습니다.

909
00:36:21,940 --> 00:36:24,190
이제 우리는 여전히 이 어텐션 가중치들을 계산해야 합니다.

910
00:36:24,190 --> 00:36:25,648
기억하세요, 어텐션 가중치는

911
00:36:25,648 --> 00:36:27,570
각 쿼리 벡터마다 계산해야 합니다.

912
00:36:27,570 --> 00:36:29,530
각 쿼리 벡터에 대해 데이터 벡터들에 대한

913
00:36:29,530 --> 00:36:30,350
분포를 계산하는 거죠.

914
00:36:30,350 --> 00:36:31,585
지금 우리는 이미

915
00:36:31,585 --> 00:36:33,210
유사도 점수들을 가지고 있는데,

916
00:36:33,210 --> 00:36:34,970
이 점수들은 단일 벡터가

917
00:36:34,970 --> 00:36:37,470
아니라 모든 유사도를 담은 행렬입니다.

918
00:36:37,470 --> 00:36:40,170
하지만 여전히 각 쿼리 벡터마다 독립적으로

919
00:36:40,170 --> 00:36:42,907
데이터 벡터에 대한 분포를 계산해야 합니다.

920
00:36:42,907 --> 00:36:44,490
그래서 이제는 유사도 점수

921
00:36:44,490 --> 00:36:48,370
행렬의 한 축에 대해서만 소프트맥스(softmax)를 계산해야

922
00:36:48,370 --> 00:36:49,070
합니다.

923
00:36:49,070 --> 00:36:50,610
이것은 기본적으로 우리가 방금 본 계산과

924
00:36:50,610 --> 00:36:51,430
정확히 같은 것입니다.

925
00:36:51,430 --> 00:36:54,050
단지 여러 쿼리 벡터에 대해 동시에

926
00:36:54,050 --> 00:36:55,610
병렬로 수행하는 거죠.

927
00:36:55,610 --> 00:36:57,870
이제 출력 벡터들을 계산해야 합니다.

928
00:36:57,870 --> 00:36:59,930
그리고 기억하세요,

929
00:36:59,930 --> 00:37:05,350
출력 벡터는 소프트맥스 값인 가중치로 데이터

930
00:37:05,350 --> 00:37:08,890
벡터들을 가중 합한 결과입니다.

931
00:37:08,890 --> 00:37:10,990
이것 역시 행렬 곱셈으로 할 수

932
00:37:10,990 --> 00:37:12,710
있다는 것이 밝혀졌습니다.

933
00:37:12,710 --> 00:37:14,830
행렬 곱셈을 생각하는 또 다른

934
00:37:14,830 --> 00:37:17,930
방법은, 두 행렬의 곱셈을 할 때, 행렬

935
00:37:17,930 --> 00:37:20,630
곱셈을 다르게 보면, 선형 결합을 취하는

936
00:37:20,630 --> 00:37:22,395
거죠, 아, 행과 열을

937
00:37:22,395 --> 00:37:24,770
제대로 맞출 수 있을지 모르겠네요.

938
00:37:24,770 --> 00:37:26,710
하지만 한 입력 행렬의

939
00:37:26,710 --> 00:37:29,390
열들을 다른 입력 행렬의 값들로 가중치를

940
00:37:29,390 --> 00:37:33,010
둔 선형 결합으로 얻는다고 생각하시면 됩니다.

941
00:37:33,010 --> 00:37:34,470
이것이 행렬 곱셈에

942
00:37:34,470 --> 00:37:36,010
대한 또 다른 해석입니다.

943
00:37:36,010 --> 00:37:38,000
인덱스를 하나하나

944
00:37:38,000 --> 00:37:39,750
살펴보고 작은 그림을

945
00:37:39,750 --> 00:37:41,810
그려서 스스로 증명해보면,

946
00:37:41,810 --> 00:37:45,950
이제 우리가 하고 싶은 것은 데이터 벡터들의

947
00:37:45,950 --> 00:37:48,510
여러 선형 결합을 계산하는

948
00:37:48,510 --> 00:37:51,150
것인데, 각 선형 결합은 어텐션

949
00:37:51,150 --> 00:37:54,750
행렬의 한 행에 있는 확률들로 주어진다는

950
00:37:54,750 --> 00:37:55,790
겁니다.

951
00:37:55,790 --> 00:37:59,230
그래서 어텐션 행렬 A와 데이터 벡터 X 사이의 또

952
00:37:59,230 --> 00:38:03,130
다른 행렬 곱셈으로 한꺼번에 모두 계산할 수 있습니다.

953
00:38:03,130 --> 00:38:05,630
그리고 이걸 제대로 하려면 전치 행렬을 올바른

954
00:38:05,630 --> 00:38:07,002
순서로 맞춰야 합니다.

955
00:38:07,002 --> 00:38:09,710
기본적으로는 우리가 방금 본 것과 정확히 같은 연산인데,

956
00:38:09,710 --> 00:38:12,590
이제는 쿼리 벡터 집합 전체에 대해 한꺼번에 수행하는 겁니다.

957
00:38:12,590 --> 00:38:15,330
그리고 실제로 몇 번의 행렬 곱셈만으로

958
00:38:15,330 --> 00:38:17,730
한꺼번에 할 수 있다는 것이 밝혀졌습니다.

959
00:38:17,730 --> 00:38:19,850
다음으로 일반화할 방법은,

960
00:38:19,850 --> 00:38:23,650
이 식에서 데이터 벡터 X가 계산에

961
00:38:23,650 --> 00:38:26,610
두 군데에서 들어간다는 점을

962
00:38:26,610 --> 00:38:28,222
주목하는 겁니다.

963
00:38:28,222 --> 00:38:29,930
첫 번째는 쿼리

964
00:38:29,930 --> 00:38:32,170
벡터와의 유사도를 계산할 때

965
00:38:32,170 --> 00:38:35,710
데이터 벡터 X를 사용하는 부분입니다.

966
00:38:35,710 --> 00:38:38,250
즉, 데이터 벡터가 각

967
00:38:38,250 --> 00:38:40,428
쿼리 벡터와 내적을 통해

968
00:38:40,428 --> 00:38:41,970
얼마나 일치하는지

969
00:38:41,970 --> 00:38:43,610
측정하는 거죠.

970
00:38:43,610 --> 00:38:46,090
하지만 또 다시 데이터 벡터를 출력 벡터를

971
00:38:46,090 --> 00:38:47,750
계산하는 데 사용합니다.

972
00:38:47,750 --> 00:38:51,010
출력 벡터는 이제 어텐션 가중치로

973
00:38:51,010 --> 00:38:54,370
가중된 데이터 벡터들의 선형 결합입니다.

974
00:38:54,370 --> 00:38:57,330
이렇게 데이터 벡터를 두 가지 다른 맥락에서 재사용하는

975
00:38:57,330 --> 00:38:59,650
것이 조금 이상하게 느껴질 수도 있습니다.

976
00:38:59,650 --> 00:39:03,330
그래서 이제는 데이터 벡터의 두 가지

977
00:39:03,330 --> 00:39:07,390
사용을 분리하고, 네트워크가 스스로 두 가지

978
00:39:07,390 --> 00:39:09,990
다른 방식으로 데이터 벡터를

979
00:39:09,990 --> 00:39:12,830
활용하도록 하려는 겁니다.

980
00:39:12,830 --> 00:39:16,773
이를 위해 키(keys)와 쿼리(queries)라는 개념을 도입할 것입니다.

981
00:39:16,773 --> 00:39:18,190
자, 이제 우리가

982
00:39:18,190 --> 00:39:21,710
할 일은, 데이터 벡터 집합이 있었는데,

983
00:39:21,710 --> 00:39:24,430
각 데이터 벡터를 두 개의

984
00:39:24,430 --> 00:39:26,250
벡터로 투영하는 겁니다.

985
00:39:26,250 --> 00:39:27,530
하나는 key 벡터입니다.

986
00:39:27,530 --> 00:39:29,390
하나는 value 벡터입니다.

987
00:39:29,390 --> 00:39:32,630
key 벡터의 아이디어는 query 벡터와

988
00:39:32,630 --> 00:39:34,830
비교해서 정렬 점수를 계산하는

989
00:39:34,830 --> 00:39:36,410
데 사용된다는 겁니다.

990
00:39:36,410 --> 00:39:38,190
그리고 value 벡터는

991
00:39:38,190 --> 00:39:40,870
이 층의 출력을 계산하기

992
00:39:40,870 --> 00:39:43,510
위해 선형 결합을 할 대상입니다.

993
00:39:43,510 --> 00:39:46,430
그리고 이렇게 구현하는 방법은, 학습 가능한

994
00:39:46,430 --> 00:39:49,470
두 개의 가중치 행렬, key 행렬과

995
00:39:49,470 --> 00:39:51,070
value 행렬을 추가하는

996
00:39:51,070 --> 00:39:54,510
겁니다. 이들은 입력, 즉 데이터 벡터를 key

997
00:39:54,510 --> 00:39:57,310
벡터와 value 벡터로 선형 투영하는

998
00:39:57,310 --> 00:39:58,170
역할을 합니다.

999
00:39:58,170 --> 00:40:00,430
데이터 벡터는 N개가 있고,

1000
00:40:00,430 --> 00:40:02,990
각각 차원이 Dx라는 것을 기억하세요.

1001
00:40:02,990 --> 00:40:06,570
key 행렬은 Dx에서 Dq로 투영하는

1002
00:40:06,570 --> 00:40:10,090
선형 변환입니다. 왜냐하면 key 벡터와

1003
00:40:10,090 --> 00:40:12,090
query 벡터를 비교해야

1004
00:40:12,090 --> 00:40:13,148
하니까요.

1005
00:40:13,148 --> 00:40:15,690
그래서 query 벡터와 같은 차원을 가져야 합니다.

1006
00:40:15,690 --> 00:40:17,090
따라서 행렬

1007
00:40:17,090 --> 00:40:19,850
곱셈 K = XWk를 적용하면

1008
00:40:19,850 --> 00:40:24,170
각 데이터 벡터가 차원 Dq의 key 벡터로

1009
00:40:24,170 --> 00:40:25,690
투영됩니다.

1010
00:40:25,690 --> 00:40:28,370
그리고 별도로 Dx에서 Dv로 투영하는

1011
00:40:28,370 --> 00:40:31,210
또 다른 가중치 행렬이 있는데, Dv는

1012
00:40:31,210 --> 00:40:33,490
value 벡터의 차원입니다.

1013
00:40:33,490 --> 00:40:36,970
원칙적으로 query 벡터 차원과 다를 수 있습니다.

1014
00:40:36,970 --> 00:40:40,130
그리고 다시 행렬 곱셈 연산으로 각

1015
00:40:40,130 --> 00:40:44,690
데이터 벡터를 value 벡터로 별도로 투영합니다.

1016
00:40:44,690 --> 00:40:48,210
여기서 직관은, 검색 엔진에서 찾고자

1017
00:40:48,210 --> 00:40:51,210
하는 것과 그에 대한 답변을

1018
00:40:51,210 --> 00:40:52,970
분리하는 것과 같습니다.

1019
00:40:52,970 --> 00:40:56,410
예를 들어 구글이나 요즘은 ChatGPT에 '세계에서 가장

1020
00:40:56,410 --> 00:40:58,790
좋은 학교가 어디인가요?'라고 입력하는 거죠.

1021
00:40:58,790 --> 00:40:59,790
그게 query입니다.

1022
00:40:59,790 --> 00:41:01,380
그리고 얻는 값은—음, 그

1023
00:41:01,380 --> 00:41:03,130
query가 백엔드에서 key와

1024
00:41:03,130 --> 00:41:04,390
결합되어야 한다는 겁니다.

1025
00:41:04,390 --> 00:41:06,070
하지만 쿼리에서 얻고자

1026
00:41:06,070 --> 00:41:07,750
하는 값, 즉 데이터는

1027
00:41:07,750 --> 00:41:09,870
실제로 입력한 쿼리와 다릅니다.

1028
00:41:09,870 --> 00:41:13,350
그래서 쿼리를 입력하는 것과, 예를 들어 '세계에서 가장 좋은 학교는
어디인가요?

1029
00:41:13,350 --> 00:41:14,770
'라는 질문을 분리하고 싶습니다.

1030
00:41:14,770 --> 00:41:17,270
그 쿼리는 인터넷에 있는 다양한 문자열과

1031
00:41:17,270 --> 00:41:18,445
매칭되어야 합니다.

1032
00:41:18,445 --> 00:41:20,070
그리고 그 쿼리에서

1033
00:41:20,070 --> 00:41:23,098
얻고자 하는 값은 Stanford인데,

1034
00:41:23,098 --> 00:41:25,390
이 값은 입력한 쿼리와 다른

1035
00:41:25,390 --> 00:41:26,330
값입니다.

1036
00:41:26,330 --> 00:41:27,530
이것이 직관적인 이해입니다.

1037
00:41:27,530 --> 00:41:29,310
키, 쿼리, 값의 개념을

1038
00:41:29,310 --> 00:41:32,470
이렇게 분리하는 또 다른 직관은, 쿼리는 내가

1039
00:41:32,470 --> 00:41:33,870
찾고자 하는 것이고,

1040
00:41:33,870 --> 00:41:37,670
키는 백엔드에 있는 데이터 벡터들에

1041
00:41:37,670 --> 00:41:39,570
대한 어떤 기록입니다.

1042
00:41:39,570 --> 00:41:42,357
하지만 쿼리를 할 때는 데이터 벡터의

1043
00:41:42,357 --> 00:41:44,190
일부, 잠재적으로 일부만

1044
00:41:44,190 --> 00:41:45,172
매칭하고 싶습니다.

1045
00:41:45,172 --> 00:41:47,630
그리고 데이터 벡터에서 되돌려 받고자

1046
00:41:47,630 --> 00:41:48,870
하는 것이 값입니다.

1047
00:41:48,870 --> 00:41:51,230
그래서 데이터 벡터의 사용을

1048
00:41:51,230 --> 00:41:55,070
키와 값이라는 두 가지 개념으로 분리하는 겁니다.

1049
00:41:55,070 --> 00:41:57,470
이것을 다른 방식으로 시각화할 수 있습니다.

1050
00:41:57,470 --> 00:42:00,250
이제 드디어 RNN을 버리고,

1051
00:42:00,250 --> 00:42:03,070
어텐션을 독립적인 연산자로 봅니다.

1052
00:42:03,070 --> 00:42:05,087
그래서 이 연산을 다시 단계별로 살펴볼 수 있습니다.

1053
00:42:05,087 --> 00:42:06,670
쿼리 벡터가 들어오고,

1054
00:42:06,670 --> 00:42:08,650
데이터 벡터가 들어옵니다.

1055
00:42:08,650 --> 00:42:11,030
이제 데이터 벡터

1056
00:42:11,030 --> 00:42:15,250
각각을 키와 값으로 투영할 겁니다.

1057
00:42:15,250 --> 00:42:18,490
그다음에 각 키와 각 쿼리를

1058
00:42:18,490 --> 00:42:21,388
비교해서 유사도 점수를 구합니다.

1059
00:42:21,388 --> 00:42:23,930
이것은 유사도 행렬입니다-- 각 키와

1060
00:42:23,930 --> 00:42:27,290
각 쿼리 간의 유사도를 나타내는 스칼라 행렬입니다.

1061
00:42:27,290 --> 00:42:30,050
이 유사도 점수 행렬을 얻으면,

1062
00:42:30,050 --> 00:42:33,490
각 쿼리에 대해 데이터 벡터에

1063
00:42:33,490 --> 00:42:36,450
대한 분포를 계산하고 싶습니다.

1064
00:42:36,450 --> 00:42:38,570
즉, 이 정렬 점수

1065
00:42:38,570 --> 00:42:41,690
행렬의 각 행에 대해

1066
00:42:41,690 --> 00:42:44,610
소프트맥스를 실행해야 합니다.

1067
00:42:44,610 --> 00:42:49,370
그다음에, 소프트맥스에서 나온 어텐션 점수로

1068
00:42:49,370 --> 00:42:52,625
값 벡터를 재가중치합니다.

1069
00:42:52,625 --> 00:42:54,250
아, 사실 아니네요,

1070
00:42:54,250 --> 00:42:56,930
각-- 각 열이 분포가 되어야

1071
00:42:56,930 --> 00:42:57,730
합니다.

1072
00:42:57,730 --> 00:43:02,677
왜냐하면 각 쿼리에 대해 키에 대한 분포가 필요하기 때문입니다.

1073
00:43:02,677 --> 00:43:04,510
즉, 열에 대해 소프트맥스를 해야

1074
00:43:04,510 --> 00:43:07,190
하는데, 열에 맞춰 정렬되어야 하기 때문입니다.

1075
00:43:07,190 --> 00:43:10,130
그래서 이제 이 쿼리 1이 있습니다.

1076
00:43:10,130 --> 00:43:12,230
이 계산에서 모든

1077
00:43:12,230 --> 00:43:15,692
키에 대한 분포를 예측했습니다.

1078
00:43:15,692 --> 00:43:18,150
그다음 이 어텐션 가중치로

1079
00:43:18,150 --> 00:43:19,822
값 벡터들의 선형

1080
00:43:19,822 --> 00:43:22,030
조합을 구해서 첫 번째

1081
00:43:22,030 --> 00:43:24,670
출력 벡터 Y1을 만듭니다.

1082
00:43:24,670 --> 00:43:26,490
그리고 같은 과정이 여기서도 일어납니다.

1083
00:43:26,490 --> 00:43:28,650
두 번째 쿼리가 모든 키와 비교되었습니다.

1084
00:43:28,650 --> 00:43:31,030
그 정렬 점수에 대해 분포를

1085
00:43:31,030 --> 00:43:32,910
계산해서 두

1086
00:43:32,910 --> 00:43:35,890
번째 쿼리에 대한 키 분포를 얻고,

1087
00:43:35,890 --> 00:43:39,910
그걸로 값을 선형 조합해서 출력 벡터를

1088
00:43:39,910 --> 00:43:40,950
만듭니다.

1089
00:43:40,950 --> 00:43:43,710
이제 이것이 순환 신경망과 분리된

1090
00:43:43,710 --> 00:43:46,467
독립적인 어텐션 연산자입니다.

1091
00:43:46,467 --> 00:43:48,550
질문은, 데이터 벡터를 어떻게 키와 값으로

1092
00:43:48,550 --> 00:43:49,610
나누느냐 하는 것입니다.

1093
00:43:49,610 --> 00:43:53,470
멋진 점은 우리가 어떻게 하는지 말할 필요가 없다는 겁니다.

1094
00:43:53,470 --> 00:43:56,830
우리는 단지 신경망에 키와 값을 따로

1095
00:43:56,830 --> 00:43:59,470
투영하는 메커니즘을 줌으로써

1096
00:43:59,470 --> 00:44:02,910
스스로 분리할 수 있는 능력을 부여합니다.

1097
00:44:02,910 --> 00:44:05,238
하지만 어떻게 하는지는 알려주지 않습니다.

1098
00:44:05,238 --> 00:44:07,030
키 행렬과 값 행렬은 모델의

1099
00:44:07,030 --> 00:44:08,210
학습 가능한

1100
00:44:08,210 --> 00:44:09,930
파라미터일 뿐이며, 다른

1101
00:44:09,930 --> 00:44:12,347
모든 것과 함께 경사 하강법으로

1102
00:44:12,347 --> 00:44:12,870
학습됩니다.

1103
00:44:12,870 --> 00:44:14,850
영어와 프랑스어 문장을 어떻게

1104
00:44:14,850 --> 00:44:17,330
정렬할지 알려주지 않았던 것처럼,

1105
00:44:17,330 --> 00:44:19,110
모든 것은 경사

1106
00:44:19,110 --> 00:44:22,130
하강법으로 학습되었고, 모델은 문제 해결에

1107
00:44:22,130 --> 00:44:24,170
도움이 되는 방식으로 키와

1108
00:44:24,170 --> 00:44:25,370
값을 따로 투영하는

1109
00:44:25,370 --> 00:44:27,575
법을 스스로 배우게 됩니다.

1110
00:44:27,575 --> 00:44:29,450
키와 값은 일종의 필터라고 생각할

1111
00:44:29,450 --> 00:44:30,470
수 있습니다.

1112
00:44:30,470 --> 00:44:32,873
데이터 벡터에는 많은 정보가 들어 있을 수 있죠.

1113
00:44:32,873 --> 00:44:34,290
하지만 당면한 작업을 위해

1114
00:44:34,290 --> 00:44:36,250
데이터 벡터를 여러 방식으로 필터링하고,

1115
00:44:36,250 --> 00:44:38,670
쿼리를 그 일부에만 맞추고 싶을 수 있습니다.

1116
00:44:38,670 --> 00:44:40,290
그리고 우리는 그 일부의 다른 정보를

1117
00:44:40,290 --> 00:44:41,390
가져오는 것에만 관심이 있습니다.

1118
00:44:41,390 --> 00:44:42,932
그래서 그것들을 데이터

1119
00:44:42,932 --> 00:44:44,570
벡터의 정보를 두 가지

1120
00:44:44,570 --> 00:44:47,770
다른 방식으로 필터링하는 것으로 생각할 수 있습니다.

1121
00:44:47,770 --> 00:44:50,710
자, 이것이 기본적으로 우리의 어텐션 연산자입니다.

1122
00:44:50,710 --> 00:44:52,632
그리고 이제 여기에는 RNN이 없습니다.

1123
00:44:52,632 --> 00:44:54,090
이것은 독립적으로 존재할

1124
00:44:54,090 --> 00:44:56,030
수 있는 신경망 층일 뿐입니다.

1125
00:44:56,030 --> 00:44:58,690
두 개의 입력, 쿼리 벡터와 데이터

1126
00:44:58,690 --> 00:44:59,850
벡터를 받습니다.

1127
00:44:59,850 --> 00:45:01,690
학습 가능한 두 개의 가중치

1128
00:45:01,690 --> 00:45:03,830
파라미터, 키 행렬과 값 행렬이 있습니다.

1129
00:45:03,830 --> 00:45:06,710
두 개의 벡터 시퀀스를 입력받아 벡터

1130
00:45:06,710 --> 00:45:08,250
시퀀스를 출력합니다.

1131
00:45:08,250 --> 00:45:10,342
이것은 자체적으로 신경망

1132
00:45:10,342 --> 00:45:12,550
레이어로, 다양한 신경망 아키텍처에

1133
00:45:12,550 --> 00:45:14,830
연결해서 사용할 수 있습니다.

1134
00:45:14,830 --> 00:45:17,230
이것을 크로스 어텐션 레이어라고 부르기도

1135
00:45:17,230 --> 00:45:20,010
하는데, 두 세트의 입력이 들어오기 때문입니다.

1136
00:45:20,010 --> 00:45:23,450
아이디어는 데이터 벡터와 쿼리 벡터가 있다는 겁니다.

1137
00:45:23,450 --> 00:45:25,870
이들은 서로 다른 두 출처에서 올 수 있습니다.

1138
00:45:25,870 --> 00:45:27,510
이것이 때때로 유용합니다.

1139
00:45:27,510 --> 00:45:29,470
제가 쿼리 집합을 가지고 있다고 합시다.

1140
00:45:29,470 --> 00:45:31,870
각 쿼리에 대해, 데이터에서 정보를

1141
00:45:31,870 --> 00:45:34,038
요약하고 싶은데, 데이터는 쿼리 벡터와

1142
00:45:34,038 --> 00:45:35,830
다르거나 개수가 다르거나

1143
00:45:35,830 --> 00:45:37,270
완전히 다를 수 있습니다.

1144
00:45:37,270 --> 00:45:39,950
그래서 두 다른 집합 사이를

1145
00:45:39,950 --> 00:45:41,910
교차해서 어텐션하기 때문에

1146
00:45:41,910 --> 00:45:44,732
크로스 어텐션 레이어라고 부릅니다.

1147
00:45:44,732 --> 00:45:46,190
하지만 더 흔히 발생하는

1148
00:45:46,190 --> 00:45:48,790
또 다른 버전이 있는데, 그것이 바로

1149
00:45:48,790 --> 00:45:50,350
셀프 어텐션 레이어입니다.

1150
00:45:50,350 --> 00:45:52,430
여기서는 하나의

1151
00:45:52,430 --> 00:45:53,970
집합만 있습니다.

1152
00:45:53,970 --> 00:45:55,890
하나의 입력 시퀀스만 있습니다.

1153
00:45:55,890 --> 00:45:58,650
처리하는 벡터 집합, 시퀀스가

1154
00:45:58,650 --> 00:45:59,970
하나뿐입니다.

1155
00:45:59,970 --> 00:46:02,290
그래서 이제 데이터 벡터와 쿼리

1156
00:46:02,290 --> 00:46:04,310
벡터의 구분이 없습니다.

1157
00:46:04,310 --> 00:46:06,690
처리하고 싶은 입력 벡터 집합

1158
00:46:06,690 --> 00:46:08,170
하나만 있습니다.

1159
00:46:08,170 --> 00:46:10,450
셀프 어텐션 레이어에서는

1160
00:46:10,450 --> 00:46:13,110
입력 벡터 집합 하나를 받아서

1161
00:46:13,110 --> 00:46:15,470
출력 벡터 집합을 만듭니다.

1162
00:46:15,470 --> 00:46:18,290
입력 벡터 x 집합을 넣으면, 입력과

1163
00:46:18,290 --> 00:46:21,670
같은 개수의 출력 벡터 y 집합을 얻고자 합니다.

1164
00:46:21,670 --> 00:46:23,650
기본적으로 방금 본 것과

1165
00:46:23,650 --> 00:46:26,050
같은 어텐션 메커니즘을 사용합니다.

1166
00:46:26,050 --> 00:46:28,147
하지만 이제, 이전처럼 데이터

1167
00:46:28,147 --> 00:46:30,230
벡터를 키와 쿼리로 투영하는 대신,

1168
00:46:30,230 --> 00:46:32,370
각 입력

1169
00:46:32,370 --> 00:46:35,050
벡터를 세

1170
00:46:35,050 --> 00:46:37,810
가지로

1171
00:46:37,810 --> 00:46:40,730
투영합니다.

1172
00:46:40,730 --> 00:46:42,410
각 입력 벡터에서

1173
00:46:42,410 --> 00:46:46,370
쿼리, 키, 밸류로 각각 투영합니다.

1174
00:46:46,370 --> 00:46:49,730
수식은 약간 바뀌지만 그림은

1175
00:46:49,730 --> 00:46:51,530
크게 달라지지

1176
00:46:51,530 --> 00:46:52,690
않습니다.

1177
00:46:52,690 --> 00:46:54,770
각 입력 벡터마다

1178
00:46:54,770 --> 00:46:58,390
쿼리, 키, 밸류로 따로 투영합니다.

1179
00:46:58,390 --> 00:47:01,530
그리고 나서 완전히 같은 계산을 합니다.

1180
00:47:01,530 --> 00:47:03,910
쿼리, 키, 밸류가 생겼습니다.

1181
00:47:03,910 --> 00:47:06,250
위에서 일어나는 모든 관점에서 보면

1182
00:47:06,250 --> 00:47:07,090
동일합니다.

1183
00:47:07,090 --> 00:47:09,790
키, 쿼리, 밸류를 모두

1184
00:47:09,790 --> 00:47:12,030
같은 입력 벡터에서

1185
00:47:12,030 --> 00:47:14,950
다른 선형 투영으로 계산했을

1186
00:47:14,950 --> 00:47:15,730
뿐입니다.

1187
00:47:15,730 --> 00:47:18,430
하지만 나머지 계산은 모두 공유됩니다.

1188
00:47:18,430 --> 00:47:21,250
질문이 있는데, D in과 D out은 무엇인가요?

1189
00:47:21,250 --> 00:47:22,060
이들은 어떻게 크기가 정해지나요?

1190
00:47:22,060 --> 00:47:24,310
이것들은 레이어의 아키텍처

1191
00:47:24,310 --> 00:47:25,490
하이퍼파라미터입니다.

1192
00:47:25,490 --> 00:47:28,390
모델에서 학습 가능한 선형 레이어가 있을 때, 선형

1193
00:47:28,390 --> 00:47:30,865
레이어는 D in에서 D out으로 투영하는데,

1194
00:47:30,865 --> 00:47:32,990
이것들이 레이어의 아키텍처

1195
00:47:32,990 --> 00:47:33,913
하이퍼파라미터입니다.

1196
00:47:33,913 --> 00:47:36,330
셀프 어텐션 레이어도 마찬가지로 D

1197
00:47:36,330 --> 00:47:38,372
in과 D out은 아키텍처

1198
00:47:38,372 --> 00:47:39,350
하이퍼파라미터입니다.

1199
00:47:39,350 --> 00:47:41,830
원칙적으로는 서로 다를 수 있습니다.

1200
00:47:41,830 --> 00:47:44,530
이 아키텍처에는 충분한 유연성이 있어서,

1201
00:47:44,530 --> 00:47:48,370
원칙적으로 D in과 D out이 다를 수 있습니다.

1202
00:47:48,370 --> 00:47:50,550
하지만 저는 거의 그런 경우를 본 적이 없습니다.

1203
00:47:50,550 --> 00:47:52,750
실제로는 거의 항상 같습니다.

1204
00:47:52,750 --> 00:47:55,250
그래서 여기 표기법에서는 조금

1205
00:47:55,250 --> 00:47:57,890
더 일반적으로 표현했습니다.

1206
00:47:57,890 --> 00:47:59,650
음, 꼭 이 부분을 다 살펴볼 필요는

1207
00:47:59,650 --> 00:48:00,750
없을 것 같습니다.

1208
00:48:00,750 --> 00:48:02,500
아, 사실 중요한 한 가지가 있습니다.

1209
00:48:02,500 --> 00:48:05,530
입력을 쿼리, 키, 값으로 각각

1210
00:48:05,530 --> 00:48:07,530
투영한다고 말씀드렸죠.

1211
00:48:07,530 --> 00:48:10,450
이것은 세 개의 학습 가능한 가중치 행렬과의 행렬 곱셈

1212
00:48:10,450 --> 00:48:11,870
세 번을 통해 이루어집니다.

1213
00:48:11,870 --> 00:48:14,390
키, 값, 쿼리 각각에 대해 학습 가능한 세

1214
00:48:14,390 --> 00:48:16,210
개의 가중치 행렬이 있습니다.

1215
00:48:16,210 --> 00:48:19,370
그리고 입력 벡터 X를 키, 쿼리,

1216
00:48:19,370 --> 00:48:21,450
값으로 각각 투영합니다.

1217
00:48:21,450 --> 00:48:23,850
하지만 실제로는 보통 한 번에

1218
00:48:23,850 --> 00:48:26,428
하나의 행렬 곱셈으로 계산하는 경우가

1219
00:48:26,428 --> 00:48:28,470
많습니다. 하드웨어 상에서

1220
00:48:28,470 --> 00:48:32,250
작은 행렬 곱셈 여러 번보다 큰 행렬 곱셈 하나가 더

1221
00:48:32,250 --> 00:48:33,790
효율적이기 때문입니다.

1222
00:48:33,790 --> 00:48:36,130
실제로 흔히 쓰이는 트릭은 이

1223
00:48:36,130 --> 00:48:40,170
세 행렬을 차원에 따라 이어붙여서, 모든 입력

1224
00:48:40,170 --> 00:48:42,388
벡터에 대해 키, 쿼리, 값을

1225
00:48:42,388 --> 00:48:43,930
한꺼번에 큰 행렬

1226
00:48:43,930 --> 00:48:46,130
곱셈으로 계산하는 것입니다.

1227
00:48:46,130 --> 00:48:47,870
트랜스포머를 읽어보셨다면,

1228
00:48:47,870 --> 00:48:50,610
인코더와 디코더 트랜스포머 또는 인코더-디코더

1229
00:48:50,610 --> 00:48:53,270
어텐션을 구분하는 경우가 있습니다.

1230
00:48:53,270 --> 00:48:56,870
그 경우, 이것은 트랜스포머 논문에서 디코더

1231
00:48:56,870 --> 00:48:59,150
전용 어텐션에 해당합니다.

1232
00:48:59,150 --> 00:49:01,390
그리고 이는 수업 초반의

1233
00:49:01,390 --> 00:49:03,670
RNN 디코더에서 사용된

1234
00:49:03,670 --> 00:49:05,430
방식과 일치합니다.

1235
00:49:05,430 --> 00:49:08,360
하지만 이 메커니즘은 사실 요즘 가장

1236
00:49:08,360 --> 00:49:10,110
흔히 쓰이는 어텐션

1237
00:49:10,110 --> 00:49:12,030
형태로, 이른바 디코더

1238
00:49:12,030 --> 00:49:13,590
전용 어텐션입니다.

1239
00:49:13,590 --> 00:49:16,730
우리는 이제 RNN과는 꽤 분리된 상태입니다.

1240
00:49:16,730 --> 00:49:18,670
이 형태는 수업 초반에

1241
00:49:18,670 --> 00:49:20,630
본 RNN에서는 사실

1242
00:49:20,630 --> 00:49:22,088
잘 맞지 않습니다.

1243
00:49:22,088 --> 00:49:24,630
우리는 여기서 약간의 트릭을

1244
00:49:24,630 --> 00:49:26,910
썼는데, 기계 번역 시퀀스 투

1245
00:49:26,910 --> 00:49:29,830
시퀀스라는 구체적인 경우를 위해 이

1246
00:49:29,830 --> 00:49:31,610
아키텍처를 도입했지만,

1247
00:49:31,610 --> 00:49:33,670
이제는 완전히 다른 연산자로

1248
00:49:33,670 --> 00:49:36,370
일반화되어 독립적으로 사용할 수 있습니다.

1249
00:49:36,370 --> 00:49:38,030
특히 이 자기 어텐션으로의

1250
00:49:38,030 --> 00:49:40,030
일반화에서는 더 이상 RNN

1251
00:49:40,030 --> 00:49:42,350
디코더에서 사용할 수 없습니다.

1252
00:49:42,350 --> 00:49:44,110
하지만 이것은 매우 유용한 기본

1253
00:49:44,110 --> 00:49:46,462
연산으로, 여러 다른 곳에서 사용됩니다.

1254
00:49:46,462 --> 00:49:48,670
자, 질문은 자기 어텐션과 교차 어텐션의

1255
00:49:48,670 --> 00:49:50,950
차이점이나 이점이 무엇인가 하는 것입니다.

1256
00:49:50,950 --> 00:49:52,730
이들은 서로 다른 상황에서 사용됩니다.

1257
00:49:52,730 --> 00:49:55,330
어떤 경우에는 비교하고자 하는 두 가지

1258
00:49:55,330 --> 00:49:58,427
다른 종류의 데이터가 자연스럽게 존재합니다.

1259
00:49:58,427 --> 00:50:01,010
예를 들어 기계 번역 상황에서 보았죠.

1260
00:50:01,010 --> 00:50:02,225
입력 문장이 있고,

1261
00:50:02,225 --> 00:50:03,350
출력 문장이 있습니다.

1262
00:50:03,350 --> 00:50:05,250
문제에 자연스러운 구조가 있어서,

1263
00:50:05,250 --> 00:50:07,625
비교하고자 하는 두 가지 다른 집합이

1264
00:50:07,625 --> 00:50:08,890
있다고 생각합니다.

1265
00:50:08,890 --> 00:50:11,230
이것은 이미지 캡셔닝 같은 경우에도 마찬가지입니다.

1266
00:50:11,230 --> 00:50:13,027
입력 이미지가 있고, 출력 문장을

1267
00:50:13,027 --> 00:50:14,610
생성하려고 할 때, 비교하고자

1268
00:50:14,610 --> 00:50:16,693
하는 두 가지 다른 종류의 대상이

1269
00:50:16,693 --> 00:50:18,730
있습니다. 이미지의 부분과 생성하는

1270
00:50:18,730 --> 00:50:19,790
단어의 토큰이죠.

1271
00:50:19,790 --> 00:50:22,210
그래서 어떤 문제에서는 두 가지 다른

1272
00:50:22,210 --> 00:50:25,250
종류의 대상이 자연스럽게 존재하는 구조가 있습니다.

1273
00:50:25,250 --> 00:50:27,790
하지만 다른 문제들에서는 두 가지 종류의 것이 없습니다.

1274
00:50:27,790 --> 00:50:29,090
하나의 것만 있습니다.

1275
00:50:29,090 --> 00:50:30,905
예를 들어 이미지 분류를 한다면,

1276
00:50:30,905 --> 00:50:32,030
이미지 하나만 있는 거죠.

1277
00:50:32,030 --> 00:50:33,430
우리는 단지 이미지를 처리하고 싶습니다.

1278
00:50:33,430 --> 00:50:35,847
그래서 그런 경우에는 이미지의 일부를 자기 자신과 비교하고

1279
00:50:35,847 --> 00:50:36,350
싶습니다.

1280
00:50:36,350 --> 00:50:38,600
바로 그럴 때 self-attention 레이어를 사용합니다.

1281
00:50:38,600 --> 00:50:41,913
그래서 이것들은 서로 다른 종류의 문제에 사용됩니다.

1282
00:50:41,913 --> 00:50:43,330
하지만 중요한 것은,

1283
00:50:43,330 --> 00:50:45,530
기본적으로 같은 메커니즘과

1284
00:50:45,530 --> 00:50:48,110
같은 계산 원시 연산을 다양한 문제에

1285
00:50:48,110 --> 00:50:50,110
재사용하고 싶다는 겁니다.

1286
00:50:50,110 --> 00:50:52,310
그게 정말로 유익합니다.

1287
00:50:52,310 --> 00:50:54,750
attention에 대해 제가 말씀드리고 싶은 흥미로운

1288
00:50:54,750 --> 00:50:55,950
점이 몇 가지 있습니다.

1289
00:50:55,950 --> 00:50:57,990
첫 번째는 입력을 순서를 바꾸면 어떻게

1290
00:50:57,990 --> 00:50:59,810
되는지 생각해보자는 겁니다.

1291
00:50:59,810 --> 00:51:01,268
입력 벡터 집합이 있는데,

1292
00:51:01,268 --> 00:51:03,102
그것들을 섞어서 다른 순서로 처리하면

1293
00:51:03,102 --> 00:51:03,990
어떻게 될까요?

1294
00:51:03,990 --> 00:51:06,770
사실 여기서 흥미로운 일이 많이 일어납니다.

1295
00:51:06,770 --> 00:51:08,630
키, 쿼리, 값은 모두

1296
00:51:08,630 --> 00:51:10,630
입력의 선형 투영으로 계산되기

1297
00:51:10,630 --> 00:51:12,970
때문에 결국 똑같이 나옵니다.

1298
00:51:12,970 --> 00:51:15,650
그래서 같은 키, 쿼리, 값이 나오게 됩니다.

1299
00:51:15,650 --> 00:51:18,233
다만 입력이 섞인 것과 같은 방식으로 순서만

1300
00:51:18,233 --> 00:51:19,230
바뀌게 되는 거죠.

1301
00:51:19,230 --> 00:51:21,230
그리고 이제, 우리의 유사도 점수가

1302
00:51:21,230 --> 00:51:23,950
단순히 내적이었기 때문에, 입력을 섞는

1303
00:51:23,950 --> 00:51:25,550
방식에 따라 유사도 점수도

1304
00:51:25,550 --> 00:51:27,390
똑같이 섞인 채로 나오게 됩니다.

1305
00:51:27,390 --> 00:51:28,730
소프트맥스도 마찬가지입니다.

1306
00:51:28,730 --> 00:51:31,650
소프트맥스는 입력의 순서에 실제로 신경 쓰지 않습니다.

1307
00:51:31,650 --> 00:51:34,810
그래서 소프트맥스는 같은 벡터에 대해 작동하지만, 순서가 섞여 있는
상태입니다.

1308
00:51:34,810 --> 00:51:37,030
그래서 어텐션 가중치의 각 열도

1309
00:51:37,030 --> 00:51:39,110
이전과 똑같이 나오지만, 순서만

1310
00:51:39,110 --> 00:51:41,890
섞여 있고, 선형 결합도 마찬가지입니다.

1311
00:51:41,890 --> 00:51:45,430
그래서 출력 Y도 사실 이전과 똑같은

1312
00:51:45,430 --> 00:51:46,530
출력이 됩니다.

1313
00:51:46,530 --> 00:51:47,835
단지 모두 섞여 있을 뿐입니다.

1314
00:51:47,835 --> 00:51:50,210
이것은 순열 등변성(permutation

1315
00:51:50,210 --> 00:51:53,390
equivariance)이라는 매우 흥미로운 구조를 의미합니다.

1316
00:51:53,390 --> 00:51:57,615
기억하시죠, 몇 강 전에 컨볼루션에서 이걸 봤습니다.

1317
00:51:57,615 --> 00:51:59,490
이제는 이 자기어텐션 레이어에서

1318
00:51:59,490 --> 00:52:01,630
다른 등변성 속성을 보는 겁니다.

1319
00:52:01,630 --> 00:52:03,772
즉, 입력을 섞으면

1320
00:52:03,772 --> 00:52:05,730
출력도 똑같이 섞인

1321
00:52:05,730 --> 00:52:09,170
상태로 같은 결과를 얻는다는 겁니다.

1322
00:52:09,170 --> 00:52:11,270
이것은 자기어텐션이 입력

1323
00:52:11,270 --> 00:52:13,530
순서에 실제로 신경 쓰지

1324
00:52:13,530 --> 00:52:15,568
않는다는 의미입니다.

1325
00:52:15,568 --> 00:52:17,110
입력 순서를 바꿔도 출력은

1326
00:52:17,110 --> 00:52:19,650
같은데, 입력이 섞인 방식대로 섞여서 나옵니다.

1327
00:52:19,650 --> 00:52:21,330
레이어의 계산은 입력을

1328
00:52:21,330 --> 00:52:24,010
제시하는 순서에 의존하지 않습니다.

1329
00:52:24,010 --> 00:52:27,090
그래서 자기어텐션을 벡터 시퀀스에 작동하는

1330
00:52:27,090 --> 00:52:29,590
것으로 생각하지 않아도 됩니다.

1331
00:52:29,590 --> 00:52:32,632
단지 행렬의 순서가 있는 시퀀스에 벡터들이 담겨 있을 뿐입니다.

1332
00:52:32,632 --> 00:52:34,090
하지만 우리는 실제로 이것을

1333
00:52:34,090 --> 00:52:36,750
순서가 없는 벡터 집합에 대해 작동하는

1334
00:52:36,750 --> 00:52:40,050
것으로 생각합니다. 왜냐하면 우리가 얻는 출력은 입력 행렬에

1335
00:52:40,050 --> 00:52:43,010
벡터를 어떤 순서로 넣었는지에 실제로 의존하지

1336
00:52:43,010 --> 00:52:43,850
않기 때문입니다.

1337
00:52:43,850 --> 00:52:45,677
그래서 우리는 이것을 벡터의

1338
00:52:45,677 --> 00:52:48,010
시퀀스가 아니라 벡터 집합에 근본적으로

1339
00:52:48,010 --> 00:52:50,430
작동하는 일종의 다른 신경망 원시

1340
00:52:50,430 --> 00:52:51,553
연산으로 생각합니다.

1341
00:52:51,553 --> 00:52:52,970
하지만 이것이 때때로 문제이기도 합니다.

1342
00:52:52,970 --> 00:52:55,150
때때로 신경망에게 시퀀스의 순서, 즉 항목들의

1343
00:52:55,150 --> 00:52:57,590
순서를 알려주는 것이 유용할 때가 있습니다.

1344
00:52:57,590 --> 00:52:59,350
그래서 이를 빠르게 해결하기 위해,

1345
00:52:59,350 --> 00:53:01,550
우리는 때때로 각 입력 벡터에 위치

1346
00:53:01,550 --> 00:53:04,650
임베딩이라고 하는 추가 데이터를 덧붙입니다. 이 데이터는

1347
00:53:04,650 --> 00:53:06,758
신경망에게 이 벡터가 인덱스 1에 있고,

1348
00:53:06,758 --> 00:53:09,050
저 벡터가 인덱스 2에 있고, 또 다른 벡터가

1349
00:53:09,050 --> 00:53:11,030
인덱스 3에 있다는 정보를 알려주는

1350
00:53:11,030 --> 00:53:11,988
역할을 합니다.

1351
00:53:11,988 --> 00:53:14,230
그리고 이를 위한 여러 가지 다른 메커니즘이 있습니다.

1352
00:53:14,230 --> 00:53:18,652
질문은, 이것이 같은 결과로 학습될 것인가 하는 점입니다.

1353
00:53:18,652 --> 00:53:20,610
여기서는 학습에 대해 이야기하는 것이 아닙니다.

1354
00:53:20,610 --> 00:53:23,030
가중치 행렬을 고정하고 레이어의

1355
00:53:23,030 --> 00:53:25,070
계산만 고려한다면,

1356
00:53:25,070 --> 00:53:27,050
입력을 섞어도 같은 출력을

1357
00:53:27,050 --> 00:53:29,670
받지만 출력도 입력이 섞인 것과

1358
00:53:29,670 --> 00:53:32,130
같은 방식으로 섞이게 됩니다.

1359
00:53:32,130 --> 00:53:36,070
그래서 출력에서 어떤 벡터를 계산하는지는

1360
00:53:36,070 --> 00:53:40,787
입력 벡터의 순서에 의존하지 않습니다.

1361
00:53:40,787 --> 00:53:42,870
하지만 출력에서 벡터들의 순서는

1362
00:53:42,870 --> 00:53:44,287
입력에서 제시된

1363
00:53:44,287 --> 00:53:45,652
순서에 의존합니다.

1364
00:53:45,652 --> 00:53:47,110
셀프 어텐션에 대해 할 수

1365
00:53:47,110 --> 00:53:48,735
있는 몇 가지 다른 트릭이 더 있지만,

1366
00:53:48,735 --> 00:53:50,810
이것들은 좀 더 빠르게 설명하겠습니다.

1367
00:53:50,810 --> 00:53:53,950
때때로 전체 셀프 어텐션 레이어에서는 입력의

1368
00:53:53,950 --> 00:53:55,730
모든 부분이 다른 모든 부분을

1369
00:53:55,730 --> 00:53:57,790
볼 수 있도록 허용했습니다.

1370
00:53:57,790 --> 00:53:59,490
하지만 어떤 문제에서는 이

1371
00:53:59,490 --> 00:54:02,050
계산에 구조를 부여해서 특정 입력 부분만

1372
00:54:02,050 --> 00:54:03,717
특정 다른 부분을 볼 수

1373
00:54:03,717 --> 00:54:05,717
있도록 제한하고 싶을 때가 있습니다.

1374
00:54:05,717 --> 00:54:07,330
즉, 모든 것이 모든

1375
00:54:07,330 --> 00:54:09,550
것을 보는 것이 아니라 제한하는 거죠.

1376
00:54:09,550 --> 00:54:11,210
이것은 마스크드 셀프 어텐션이라는

1377
00:54:11,210 --> 00:54:13,010
개념으로 구현할 수 있습니다.

1378
00:54:13,010 --> 00:54:15,090
우리가 할 것은 정렬 점수

1379
00:54:15,090 --> 00:54:17,370
E를 계산한 후, 어텐션을

1380
00:54:17,370 --> 00:54:19,330
차단하고 싶은 위치에 음의

1381
00:54:19,330 --> 00:54:20,930
무한대를 넣어 정렬

1382
00:54:20,930 --> 00:54:22,663
점수를 덮어쓰는 것입니다.

1383
00:54:22,663 --> 00:54:24,330
그리고 이제, alignment

1384
00:54:24,330 --> 00:54:26,832
점수에 음의 무한대가 있다면, softmax를

1385
00:54:26,832 --> 00:54:29,290
수행한 후에는 softmax 계산을

1386
00:54:29,290 --> 00:54:30,370
거치면서 0이 됩니다.

1387
00:54:30,370 --> 00:54:32,090
즉, alignment

1388
00:54:32,090 --> 00:54:33,590
점수에 음의 무한대가

1389
00:54:33,590 --> 00:54:35,810
있을 때마다 softmax

1390
00:54:35,810 --> 00:54:38,350
점수에서는 0이 된다는 뜻입니다.

1391
00:54:38,350 --> 00:54:40,170
이는 출력 Y가 해당 인덱스에서

1392
00:54:40,170 --> 00:54:43,170
계산된 value 벡터 값에 의존하지 않는다는 의미입니다.

1393
00:54:43,170 --> 00:54:46,630
이것은 계산 과정에서 어떤 입력들이 서로 상호작용할

1394
00:54:46,630 --> 00:54:50,340
수 있는지를 제어할 수 있는 메커니즘입니다.

1395
00:54:50,340 --> 00:54:52,590
이제 언어 모델링에 이 방식을 적용할

1396
00:54:52,590 --> 00:54:54,990
수 있는데, 이 연산자를 일반화해서 RNN이

1397
00:54:54,990 --> 00:54:57,210
전혀 필요 없게 되었기 때문입니다.

1398
00:54:57,210 --> 00:54:59,390
우리는 이전에 RNN으로 풀던 같은 문제에

1399
00:54:59,390 --> 00:55:01,270
이 방식을 사용할 수 있습니다.

1400
00:55:01,270 --> 00:55:03,950
그래서 이제 단어 시퀀스를 처리할 때, 예를 들어

1401
00:55:03,950 --> 00:55:07,950
attention이 very이고 출력이 very cool인 경우에 쓸 수
있습니다.

1402
00:55:07,950 --> 00:55:10,630
이 경우에는 지난 강의에서 RNN으로 했던 같은

1403
00:55:10,630 --> 00:55:12,770
언어 모델링 작업을 수행하는 겁니다.

1404
00:55:12,770 --> 00:55:14,310
하지만 이제는 이 self-attention

1405
00:55:14,310 --> 00:55:15,790
블록만으로도 자연스럽게 할 수 있습니다.

1406
00:55:15,790 --> 00:55:19,230
하지만 여기서는 첫 번째 출력인 "is"가 첫 번째 단어에만

1407
00:55:19,230 --> 00:55:20,770
의존하도록 하고 싶습니다.

1408
00:55:20,770 --> 00:55:23,510
두 번째 출력인 "very"는 첫 두 단어에만 의존하도록 허용하고

1409
00:55:23,510 --> 00:55:24,050
싶습니다.

1410
00:55:24,050 --> 00:55:26,550
네트워크가 시퀀스의 앞을 미리 보면서 부정행위를 하지 못하게

1411
00:55:26,550 --> 00:55:27,330
하고 싶기 때문입니다.

1412
00:55:27,330 --> 00:55:30,535
이럴 때 마스킹을 사용합니다.

1413
00:55:30,535 --> 00:55:32,910
self-attention에서 가끔 사용하는

1414
00:55:32,910 --> 00:55:34,930
또 다른 기법은 multi-headed

1415
00:55:34,930 --> 00:55:38,550
self-attention인데, self-attention을 n개

1416
00:55:38,550 --> 00:55:40,325
독립적으로 병렬 실행하는 겁니다.

1417
00:55:40,325 --> 00:55:41,450
왜 이렇게 하려고 할까요?

1418
00:55:41,450 --> 00:55:43,330
계산량이 더 많고, 플롭스도 더 많고, 파라미터도

1419
00:55:43,330 --> 00:55:44,205
더 많기 때문입니다.

1420
00:55:44,205 --> 00:55:46,250
딥러닝에서는 항상 더 크고 더 많은 것을 원합니다.

1421
00:55:46,250 --> 00:55:49,930
이것은 이 레이어를 더 크고 더 강력하게 만드는

1422
00:55:49,930 --> 00:55:51,550
또 다른 방법입니다.

1423
00:55:51,550 --> 00:55:53,850
그래서 우리는 입력 X를 받아서 H개의

1424
00:55:53,850 --> 00:55:56,170
독립된 self-attention

1425
00:55:56,170 --> 00:55:58,102
레이어 복사본으로 보낼 것입니다.

1426
00:55:58,102 --> 00:55:59,810
각각은 자신의 출력

1427
00:55:59,810 --> 00:56:04,450
Y를 생성하고, 이 출력들이 쌓여서 합쳐진 후, 각 독립

1428
00:56:04,450 --> 00:56:06,410
self-attention

1429
00:56:06,410 --> 00:56:08,890
레이어의 출력 데이터를 융합하기

1430
00:56:08,890 --> 00:56:11,810
위해 또 다른 선형 투영을 거칩니다.

1431
00:56:11,810 --> 00:56:14,410
이 경우를 멀티헤드 self-attention이라고

1432
00:56:14,410 --> 00:56:16,250
부릅니다.

1433
00:56:16,250 --> 00:56:19,510
이것이 사실상 우리가 실무에서 항상 보는 형식입니다.

1434
00:56:19,510 --> 00:56:22,870
즉, 요즘 self-attention을 볼

1435
00:56:22,870 --> 00:56:25,290
때 거의 항상 이 멀티헤드

1436
00:56:25,290 --> 00:56:27,770
self-attention 버전입니다.

1437
00:56:27,770 --> 00:56:30,250
실제로는 이 모든 것을 행렬

1438
00:56:30,250 --> 00:56:32,790
곱셈으로 계산할 수 있습니다.

1439
00:56:32,790 --> 00:56:34,810
따라서 for 루프를 돌릴 필요가 없습니다.

1440
00:56:34,810 --> 00:56:37,930
똑똑하게 배치 행렬 곱셈을 적절한 위치에 사용하면

1441
00:56:37,930 --> 00:56:41,050
H개의 self-attention 복사본을 모두

1442
00:56:41,050 --> 00:56:42,850
병렬로 계산할 수 있습니다.

1443
00:56:42,850 --> 00:56:45,990
사실 이 self-attention 연산자는

1444
00:56:45,990 --> 00:56:47,570
복잡해 보이지만,

1445
00:56:47,570 --> 00:56:50,390
기본적으로 네 번의 행렬 곱셈일 뿐입니다.

1446
00:56:50,390 --> 00:56:52,990
하나는 입력을 받아 쿼리, 키,

1447
00:56:52,990 --> 00:56:56,150
값으로 투영하는 행렬 곱셈입니다.

1448
00:56:56,150 --> 00:56:59,610
또 다른 하나는 Q와 K의 유사도를 계산하는 행렬 곱셈입니다.

1449
00:56:59,610 --> 00:57:03,070
각 Q에 대해 모든 K와의 유사도를 계산하는 거죠.

1450
00:57:03,070 --> 00:57:05,270
멀티헤드 경우에는 이것이 하나의

1451
00:57:05,270 --> 00:57:07,170
큰 배치 행렬 곱셈입니다.

1452
00:57:07,170 --> 00:57:09,150
또 다른 하나는 V 가중치 계산인데,

1453
00:57:09,150 --> 00:57:10,900
소프트맥스 결과로

1454
00:57:10,900 --> 00:57:13,770
가중치가 부여된 값들의 선형 조합을 구하는 겁니다.

1455
00:57:13,770 --> 00:57:16,790
그리고 그것은 또 다른 큰 배치 행렬 곱셈으로 수행할 수 있습니다.

1456
00:57:16,790 --> 00:57:18,750
마지막으로, 우리는 self-attention의

1457
00:57:18,750 --> 00:57:21,910
여러 헤드 간 정보를 섞기 위한 출력

1458
00:57:21,910 --> 00:57:22,827
프로젝션이 있습니다.

1459
00:57:22,827 --> 00:57:24,577
그래서 많은 수식과 벡터가

1460
00:57:24,577 --> 00:57:26,210
있지만, 이 전체

1461
00:57:26,210 --> 00:57:27,627
self-attention 연산자는

1462
00:57:27,627 --> 00:57:30,870
기본적으로 네 번의 큰 배치 행렬 곱셈일 뿐입니다.

1463
00:57:30,870 --> 00:57:33,150
이것이 좋은 이유는 행렬

1464
00:57:33,150 --> 00:57:35,510
곱셈이 매우 확장 가능하고 강력한

1465
00:57:35,510 --> 00:57:38,110
기본 연산으로, 분산 처리,

1466
00:57:38,110 --> 00:57:40,950
최적화, 그리고 매우 병렬적이고

1467
00:57:40,950 --> 00:57:43,810
효율적으로 만들 수 있기 때문입니다.

1468
00:57:43,810 --> 00:57:45,410
네, 질문은 x1,

1469
00:57:45,410 --> 00:57:48,566
x2, x3가 정확히 같은지에 관한 거죠.

1470
00:57:48,566 --> 00:57:51,490
네, 하지만 기본적으로 self-attention

1471
00:57:51,490 --> 00:57:54,470
레이어의 별도 복사본을 갖게 됩니다.

1472
00:57:54,470 --> 00:57:55,652
모두 랜덤일 겁니다.

1473
00:57:55,652 --> 00:57:57,610
중요하게도 모두 다른 가중치를 갖습니다.

1474
00:57:57,610 --> 00:58:00,270
그리고 그 가중치들은 초기화 시점에 랜덤하게

1475
00:58:00,270 --> 00:58:01,503
다르게 초기화됩니다.

1476
00:58:01,503 --> 00:58:03,170
그래서 결국 약간씩 다르게 처리하는

1477
00:58:03,170 --> 00:58:04,350
법을 학습하게 됩니다.

1478
00:58:04,350 --> 00:58:07,460
이것은 레이어에 추가 용량을 부여하는 방법일 뿐입니다.

1479
00:58:07,460 --> 00:58:09,210
아, 네, 다른 헤드 간에 다른

1480
00:58:09,210 --> 00:58:10,310
점은 가중치뿐입니다.

1481
00:58:10,310 --> 00:58:12,227
그래서 아키텍처와 계산은

1482
00:58:12,227 --> 00:58:13,770
정확히 같지만,

1483
00:58:13,770 --> 00:58:14,830
가중치가 다릅니다.

1484
00:58:14,830 --> 00:58:17,370
그리고 그 가중치는 초기화 시점에

1485
00:58:17,370 --> 00:58:18,570
다르게 초기화됩니다.

1486
00:58:18,570 --> 00:58:21,110
그 외에는 모두 정확히 같습니다.

1487
00:58:21,110 --> 00:58:23,590
좋습니다, 여기서 건너뛸 부분이 좀 있네요.

1488
00:58:23,590 --> 00:58:26,490
하지만 이제, 기본적으로 우리는 이 수업에서

1489
00:58:26,490 --> 00:58:28,970
본 시퀀스를 처리하는 세 가지 매우

1490
00:58:28,970 --> 00:58:30,890
흥미로운 방법에 도달했습니다.

1491
00:58:30,890 --> 00:58:32,710
첫 번째는 순환 신경망입니다.

1492
00:58:32,710 --> 00:58:35,570
순환 신경망은 기본적으로 1차원 순서가 있는 시퀀스에서

1493
00:58:35,570 --> 00:58:37,130
작동한다는 것을 보았습니다.

1494
00:58:37,130 --> 00:58:38,350
그리고 정말 멋집니다.

1495
00:58:38,350 --> 00:58:39,350
정말 강력합니다.

1496
00:58:39,350 --> 00:58:40,850
사람들이 오랫동안 좋아해왔습니다.

1497
00:58:40,850 --> 00:58:43,130
하지만 근본적으로는 병렬화가 잘 안 됩니다.

1498
00:58:43,130 --> 00:58:44,970
각 은닉 상태가 이전 은닉

1499
00:58:44,970 --> 00:58:47,877
상태에 의존하는 이 동시 구조 때문에,

1500
00:58:47,877 --> 00:58:49,710
근본적으로 순차적인

1501
00:58:49,710 --> 00:58:50,450
알고리즘입니다.

1502
00:58:50,450 --> 00:58:53,270
시퀀스 전체에 걸쳐 병렬화할 방법이 없습니다.

1503
00:58:53,270 --> 00:58:55,130
그래서 확장하기도, 매우

1504
00:58:55,130 --> 00:58:57,310
크게 만들기도 매우 어렵습니다.

1505
00:58:57,310 --> 00:58:59,810
우리가 본 또 다른 기본 요소는 컨볼루션입니다.

1506
00:58:59,810 --> 00:59:01,470
컨볼루션은 기본적으로

1507
00:59:01,470 --> 00:59:03,190
다차원 격자에서 작동합니다.

1508
00:59:03,190 --> 00:59:05,790
이미지의 경우 2차원 격자에서 본 적이 있습니다.

1509
00:59:05,790 --> 00:59:08,930
1차원, 3차원, 4차원 격자에서도 실행할 수 있습니다.

1510
00:59:08,930 --> 00:59:10,630
컨볼루션은 기본적으로

1511
00:59:10,630 --> 00:59:14,670
N차원 격자에서 국소적으로 정보를 섞는 것입니다.

1512
00:59:14,670 --> 00:59:15,410
이것은 아주 훌륭합니다.

1513
00:59:15,410 --> 00:59:16,830
이것은 매우 병렬화할 수 있습니다.

1514
00:59:16,830 --> 00:59:20,170
커널을 격자 위에서 슬라이딩하는 개념 덕분에,

1515
00:59:20,170 --> 00:59:22,470
커널을 놓고 싶은 각 위치는 원칙적으로

1516
00:59:22,470 --> 00:59:24,690
병렬로 계산할 수 있습니다.

1517
00:59:24,690 --> 00:59:26,990
그래서 이것은 매우 병렬화 가능한 기본 연산입니다.

1518
00:59:26,990 --> 00:59:30,630
하지만 큰 수용 영역을 구축하는 데는 어려움이 있습니다.

1519
00:59:30,630 --> 00:59:33,990
매우 긴 입력 시퀀스 전체나 매우 큰

1520
00:59:33,990 --> 00:59:37,170
이미지를 합성곱으로 요약하려면, 매우 큰

1521
00:59:37,170 --> 00:59:39,530
합성곱 커널을 사용하거나, 합성곱

1522
00:59:39,530 --> 00:59:42,190
층을 아주 많이 쌓아야 합니다.

1523
00:59:42,190 --> 00:59:45,010
그래서 여전히 큰 데이터를 처리하는

1524
00:59:45,010 --> 00:59:48,410
방식에는 근본적인 순차성이 도입됩니다.

1525
00:59:48,410 --> 00:59:50,050
그리고 이제 self-attention은

1526
00:59:50,050 --> 00:59:51,810
기본적으로 벡터

1527
00:59:51,810 --> 00:59:54,290
집합에 작용하는 별도의 기본 연산입니다.

1528
00:59:54,290 --> 00:59:56,647
이는 자연스럽게 긴 시퀀스에도 일반화됩니다.

1529
00:59:56,647 --> 00:59:58,730
순환 신경망에서처럼

1530
00:59:58,730 --> 01:00:00,710
병목 현상이 없습니다.

1531
01:00:00,710 --> 01:00:02,570
모든 벡터가 서로를 볼

1532
01:00:02,570 --> 01:00:05,570
수 있게 하기 위해 많은 층을 쌓을 필요도

1533
01:00:05,570 --> 01:00:06,570
없습니다.

1534
01:00:06,570 --> 01:00:08,650
자기 주의의 한 층에서는 모든 벡터가

1535
01:00:08,650 --> 01:00:10,030
다른 모든 벡터를 봅니다.

1536
01:00:10,030 --> 01:00:12,290
그래서 한 층만으로도 요약할 수 있고,

1537
01:00:12,290 --> 01:00:14,290
많은 계산을 할 수 있습니다.

1538
01:00:14,290 --> 01:00:15,910
또한 매우 병렬화가 가능합니다.

1539
01:00:15,910 --> 01:00:18,170
보셨듯이 전체 연산은 단지 네 번의 큰

1540
01:00:18,170 --> 01:00:19,058
행렬 곱셈입니다.

1541
01:00:19,058 --> 01:00:20,850
행렬 곱셈은 분산 처리하기 좋은

1542
01:00:20,850 --> 01:00:21,950
기본 연산입니다.

1543
01:00:21,950 --> 01:00:23,070
GPU에서 실행할 수 있습니다.

1544
01:00:23,070 --> 01:00:26,170
우리는 매우 확장 가능하고 분산된 방식으로 실행할 수 있습니다.

1545
01:00:26,170 --> 01:00:29,390
Attention의 유일한 단점은 비용이 많이 든다는 점입니다.

1546
01:00:29,390 --> 01:00:31,050
길이가 n인 시퀀스에

1547
01:00:31,050 --> 01:00:33,830
대해 결국 n 제곱의 연산과 n 제곱,

1548
01:00:33,830 --> 01:00:36,970
또는 나중에는 n의 메모리가 필요합니다.

1549
01:00:36,970 --> 01:00:40,950
만약 n이 10만, 100만, 1,000만이 된다면 n

1550
01:00:40,950 --> 01:00:42,690
제곱은 매우 비싸집니다.

1551
01:00:42,690 --> 01:00:45,670
하지만 더 많은 GPU를 구매해서 해결할 수 있습니다.

1552
01:00:45,670 --> 01:00:47,510
그래서 이것이 사람들이 여기서

1553
01:00:47,510 --> 01:00:48,970
생각해낸 기본적인 해결책입니다.

1554
01:00:48,970 --> 01:00:50,790
즉, attention은

1555
01:00:50,790 --> 01:00:52,910
매우 임의적인 데이터 조각을

1556
01:00:52,910 --> 01:00:56,990
처리하는 데 강력한, 정말 멋진 기본 연산이 되었습니다.

1557
01:00:56,990 --> 01:01:00,550
어떤 것을 사용해야 할지 궁금할 수 있습니다.

1558
01:01:00,550 --> 01:01:01,770
Attention만 있으면 됩니다.

1559
01:01:01,770 --> 01:01:03,590
세 가지 중에서 오직 attention만으로도

1560
01:01:03,590 --> 01:01:06,390
꽤 멀리 갈 수 있다는 것이 밝혀졌습니다.

1561
01:01:06,390 --> 01:01:08,130
이제 질문은 병렬화가 가능한데, 그

1562
01:01:08,130 --> 01:01:09,470
장점이 무엇인가 하는 겁니다.

1563
01:01:09,470 --> 01:01:13,730
장점은 컴퓨팅 역사에서 프로세서를 더 빠르게 만드는 것이

1564
01:01:13,730 --> 01:01:15,930
점점 어려워졌다는 점입니다.

1565
01:01:15,930 --> 01:01:18,630
하드웨어의 근본적인 한계에

1566
01:01:18,630 --> 01:01:19,490
부딪혔습니다.

1567
01:01:19,490 --> 01:01:22,150
개별 프로세서를 더 빠르게 만드는 것이 매우

1568
01:01:22,150 --> 01:01:22,930
어려워졌습니다.

1569
01:01:22,930 --> 01:01:26,050
하지만 우리가 쉽게 할 수 있는 것은 많은 프로세서를 확보하는 것입니다.

1570
01:01:26,050 --> 01:01:30,870
지난 20년간 더 많은 계산을 할 수 있었던 방법은 하나의

1571
01:01:30,870 --> 01:01:32,870
매우 빠른 프로세서에서

1572
01:01:32,870 --> 01:01:35,390
실행할 필요가 없는 알고리즘을

1573
01:01:35,390 --> 01:01:37,073
찾는 것이었습니다.

1574
01:01:37,073 --> 01:01:39,490
대신 10개, 100개, 1,000개,

1575
01:01:39,490 --> 01:01:43,090
백만 개의 프로세서를 사용할 수 있는 알고리즘을

1576
01:01:43,090 --> 01:01:46,190
찾는 것이죠. 저는 스탠포드 캠퍼스 전체에

1577
01:01:46,190 --> 01:01:48,690
프로세서를 깔고 모두 함께 큰

1578
01:01:48,690 --> 01:01:50,790
작업을 처리하게 하고 싶습니다.

1579
01:01:50,790 --> 01:01:52,630
그런 알고리즘을 찾으면

1580
01:01:52,630 --> 01:01:54,922
확장해서 정말 크고 강력한 계산을 할

1581
01:01:54,922 --> 01:01:55,890
수 있습니다.

1582
01:01:55,890 --> 01:01:57,690
그래서 병렬화의 이점은 더

1583
01:01:57,690 --> 01:02:00,250
많은 프로세서를 쉽게 사용할 수 있는

1584
01:02:00,250 --> 01:02:02,570
알고리즘이 있다면, 개별 프로세서가 더

1585
01:02:02,570 --> 01:02:04,763
빨라지길 기다리지 않고도 알고리즘을

1586
01:02:04,763 --> 01:02:06,930
확장할 수 있다는 점입니다. 그

1587
01:02:06,930 --> 01:02:09,513
프로세서가 더 빨라지지 않을 수도 있지만요.

1588
01:02:09,513 --> 01:02:11,430
네, n 제곱과 관련된 트레이드오프가 있나요?

1589
01:02:11,430 --> 01:02:13,610
사실 n 제곱은 좋은 점이라고 생각합니다.

1590
01:02:13,610 --> 01:02:14,995
나쁘게 보일 수 있지만요.

1591
01:02:14,995 --> 01:02:16,370
컴퓨터 과학에서는

1592
01:02:16,370 --> 01:02:20,390
n 안의 파라미터가 나쁘다고 배우지만,

1593
01:02:20,390 --> 01:02:22,630
신경망의 경우 계산량이 많다는

1594
01:02:22,630 --> 01:02:25,370
것은 네트워크가 더 많은 계산을

1595
01:02:25,370 --> 01:02:27,150
한다는 뜻이고, 더 많이

1596
01:02:27,150 --> 01:02:30,230
생각하고 처리할 수 있다는 의미입니다.

1597
01:02:30,230 --> 01:02:32,010
그래서 네트워크가 입력 시퀀스에

1598
01:02:32,010 --> 01:02:34,260
대해 더 많은 계산을 할수록 더

1599
01:02:34,260 --> 01:02:36,050
좋은 답을 낼 수도 있습니다.

1600
01:02:36,050 --> 01:02:38,090
즉, 비용이 더 들지만

1601
01:02:38,090 --> 01:02:40,330
반드시 나쁜 것은 아닙니다.

1602
01:02:40,330 --> 01:02:41,830
기본적으로 트랜스포머는

1603
01:02:41,830 --> 01:02:44,710
self-attention을 핵심으로 둔

1604
01:02:44,710 --> 01:02:46,110
신경망 아키텍처입니다.

1605
01:02:46,110 --> 01:02:49,430
입력은 벡터 집합 X가 될 것입니다. 그 다음 모든 벡터를
self-attention에

1606
01:02:49,430 --> 01:02:51,975
통과시키는데, 앞서 말했듯이 이

1607
01:02:51,975 --> 01:02:54,350
놀라운 기본 연산은 모든 벡터가

1608
01:02:54,350 --> 01:02:56,230
서로 소통할 수 있게 합니다.

1609
01:02:56,230 --> 01:02:58,230
그 후에는 residual connection으로

1610
01:02:58,230 --> 01:03:00,550
self-attention을 감쌉니다.

1611
01:03:00,550 --> 01:03:03,070
이는 몇 강 전 ResNet에서 residual

1612
01:03:03,070 --> 01:03:05,030
connection을 쓴 이유와 같습니다.

1613
01:03:05,030 --> 01:03:08,107
그 다음 residual connection의 출력을 layer

1614
01:03:08,107 --> 01:03:09,690
normalization에 통과시킵니다.

1615
01:03:09,690 --> 01:03:12,130
ResNet과 CNN에서 보았듯이,

1616
01:03:12,130 --> 01:03:14,110
아키텍처 내에 정규화를 추가하면

1617
01:03:14,110 --> 01:03:16,470
학습이 더 안정적이기 때문입니다.

1618
01:03:16,470 --> 01:03:18,905
하지만 이제 흥미로운 점이 있습니다.

1619
01:03:18,905 --> 01:03:21,030
셀프 어텐션은 기본적으로 모든

1620
01:03:21,030 --> 01:03:23,472
벡터를 서로 비교하는 역할을 합니다.

1621
01:03:23,472 --> 01:03:24,930
이것은 매우 유용한 기본 연산입니다.

1622
01:03:24,930 --> 01:03:26,570
아주 강력한 작업이죠.

1623
01:03:26,570 --> 01:03:28,790
하지만 우리는 이 네트워크가

1624
01:03:28,790 --> 01:03:32,990
벡터들을 하나씩 독립적으로 처리할 수 있는 능력도 갖추길 원합니다.

1625
01:03:32,990 --> 01:03:34,410
그래서 트랜스포머 안에는 두

1626
01:03:34,410 --> 01:03:37,910
번째 기본 연산인 멀티레이어 퍼셉트론, MLP, 또는 FFN이라고도

1627
01:03:37,910 --> 01:03:39,077
불리는 것이 있습니다.

1628
01:03:39,077 --> 01:03:41,410
기본적으로 이것은 각 벡터에

1629
01:03:41,410 --> 01:03:43,810
독립적으로 작동하는

1630
01:03:43,810 --> 01:03:45,930
작은 2층 신경망입니다.

1631
01:03:45,930 --> 01:03:48,750
이것은 셀프 어텐션과 함께 작동하는데,

1632
01:03:48,750 --> 01:03:51,250
셀프 어텐션은 모든 벡터가 서로

1633
01:03:51,250 --> 01:03:53,210
소통하고 비교할 수 있게 해주고,

1634
01:03:53,210 --> 01:03:56,370
FFN 또는 MLP는 각 벡터를 독립적으로

1635
01:03:56,370 --> 01:03:58,490
계산할 수 있게 해줍니다.

1636
01:03:58,490 --> 01:04:01,130
우리는 또한 MLP를 잔차 연결로 감싸고, 레이어

1637
01:04:01,130 --> 01:04:03,850
정규화를 적용하며, 전체를 하나의 박스로

1638
01:04:03,850 --> 01:04:05,610
묶어 신경망 블록이라고 부릅니다.

1639
01:04:05,610 --> 01:04:08,230
이것이 바로 우리의 트랜스포머 블록입니다.

1640
01:04:08,230 --> 01:04:11,730
트랜스포머는 이런 트랜스포머 블록들의 연속입니다.

1641
01:04:11,730 --> 01:04:14,650
이 구조는 시간이 지나면서 훨씬 커졌습니다.

1642
01:04:14,650 --> 01:04:17,530
2017년에 처음 소개된 이후로 아키텍처는

1643
01:04:17,530 --> 01:04:19,010
크게 변하지 않았습니다.

1644
01:04:19,010 --> 01:04:21,870
원래 트랜스포머는 약 12개의 블록과 2억

1645
01:04:21,870 --> 01:04:23,770
개의 파라미터였지만, 지금은

1646
01:04:23,770 --> 01:04:27,290
수백 개의 블록과 수조 개의 파라미터를 가진

1647
01:04:27,290 --> 01:04:29,150
트랜스포머가 훈련되고 있습니다.

1648
01:04:29,150 --> 01:04:30,730
이 같은 아키텍처는

1649
01:04:30,730 --> 01:04:33,190
지난 8년간 컴퓨팅 파워와 크기,

1650
01:04:33,190 --> 01:04:36,238
파라미터 수 면에서 여러 단계로 확장되었습니다.

1651
01:04:36,238 --> 01:04:38,030
이들은 이미 본 것처럼 언어

1652
01:04:38,030 --> 01:04:39,750
모델링에 사용할 수 있습니다.

1653
01:04:39,750 --> 01:04:43,050
또한 이미지에도 사용할 수 있습니다.

1654
01:04:43,050 --> 01:04:45,275
여기서 적용 방법은 꽤 직관적입니다.

1655
01:04:45,275 --> 01:04:47,150
이미지를 주면, 이미지를

1656
01:04:47,150 --> 01:04:49,750
여러 패치로 나누고, 각 패치를

1657
01:04:49,750 --> 01:04:51,830
별도로 벡터로 투영합니다.

1658
01:04:51,830 --> 01:04:55,430
그 벡터들이 트랜스포머에 입력으로 들어갑니다.

1659
01:04:55,430 --> 01:04:59,070
출력은 입력의 각 패치마다

1660
01:04:59,070 --> 01:05:00,888
하나씩 나옵니다.

1661
01:05:00,888 --> 01:05:03,430
분류 점수 같은 작업을 하려면,

1662
01:05:03,430 --> 01:05:05,190
트랜스포머에서 나오는

1663
01:05:05,190 --> 01:05:07,432
모든 벡터에 대해 풀링 연산을

1664
01:05:07,432 --> 01:05:09,390
하고, 선형층을 통해

1665
01:05:09,390 --> 01:05:11,390
클래스 점수를 예측합니다.

1666
01:05:11,390 --> 01:05:14,670
그래서 이 동일한 트랜스포머 아키텍처는

1667
01:05:14,670 --> 01:05:17,190
언어뿐 아니라 이미지, 그리고

1668
01:05:17,190 --> 01:05:20,627
다른 많은 분야에도 적용할 수 있습니다.

1669
01:05:20,627 --> 01:05:22,710
트랜스포머가 처음 소개된 이후

1670
01:05:22,710 --> 01:05:25,000
몇 가지 사소한 수정이 있었습니다.

1671
01:05:25,000 --> 01:05:26,750
하지만 시간이 부족하니 그 부분은

1672
01:05:26,750 --> 01:05:28,390
추가 읽기로 남기겠습니다.

1673
01:05:28,390 --> 01:05:31,450
이 강의의 마지막 요약은,

1674
01:05:31,450 --> 01:05:34,410
처음에 약속한 두 가지입니다.

1675
01:05:34,410 --> 01:05:36,830
첫째는 벡터 집합을 다룰 수 있는

1676
01:05:36,830 --> 01:05:38,650
새로운 기본 연산인

1677
01:05:38,650 --> 01:05:40,550
어텐션을 소개했다는 점입니다.

1678
01:05:40,550 --> 01:05:41,910
이는 매우 병렬화가 가능하고,

1679
01:05:41,910 --> 01:05:44,150
사실상 몇 번의 행렬 곱셈으로 이루어집니다.

1680
01:05:44,150 --> 01:05:47,230
그래서 매우 확장 가능하고, 병렬화가 잘 되며, 유연합니다.

1681
01:05:47,230 --> 01:05:49,490
다양한 상황에 적용할 수 있습니다.

1682
01:05:49,490 --> 01:05:51,690
그리고 트랜스포머는 이제 셀프

1683
01:05:51,690 --> 01:05:53,890
어텐션을 주요 계산 기본

1684
01:05:53,890 --> 01:05:56,570
연산으로 사용하는 신경망 아키텍처입니다.

1685
01:05:56,570 --> 01:05:59,370
트랜스포머는 기본적으로 요즘 딥러닝의

1686
01:05:59,370 --> 01:06:02,990
모든 응용 분야에서 사용하는 신경망 아키텍처입니다.

1687
01:06:02,990 --> 01:06:07,058
그래서 매우 강력하고, 매우 흥미롭고, 매우 신나는 기술입니다.

1688
01:06:07,058 --> 01:06:09,350
트랜스포머는 벌써 8년째 우리와 함께하고 있습니다.

1689
01:06:09,350 --> 01:06:12,090
그리고 당분간 사라질 것 같지 않습니다.

1690
01:06:12,090 --> 01:06:14,690
그래서 정말 흥미로운 기술이죠.

1691
01:06:14,690 --> 01:06:16,970
오늘 강의는 여기까지입니다.

1692
01:06:16,970 --> 01:06:18,490
다음 시간에는 다시 와서

1693
01:06:18,490 --> 01:06:20,330
새로운 과제들—탐지, 분할,

1694
01:06:20,330 --> 01:06:22,090
시각화에 대해 이야기하고, 이

1695
01:06:22,090 --> 01:06:24,370
아키텍처들을 어떻게 활용해 새로운 멋진

1696
01:06:24,370 --> 01:06:26,400
일들을 할 수 있는지 살펴보겠습니다.
