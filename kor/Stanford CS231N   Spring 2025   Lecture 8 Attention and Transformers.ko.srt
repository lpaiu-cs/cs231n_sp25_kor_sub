1
00:00:04,760 --> 00:00:07,440
좋습니다, 여러분, 8강에 다시 오신 것을 환영합니다.

2
00:00:07,440 --> 00:00:09,940
오늘은 주의(attention)와 변환기(transformers)에 대해
이야기할 것입니다.

3
00:00:09,940 --> 00:00:12,200
정말 재미있는 주제라고 생각합니다.

4
00:00:12,200 --> 00:00:14,005
간단히 요약하자면, 지난 시간에는 순환 신경망(recurrent

5
00:00:14,005 --> 00:00:15,880
neural networks)에 대해 이야기했습니다.

6
00:00:15,880 --> 00:00:18,297
순환 신경망은 시퀀스를 처리하기 위해

7
00:00:18,297 --> 00:00:20,460
설계된 새로운 신경망 아키텍처입니다.

8
00:00:20,460 --> 00:00:23,200
특히, 우리는 신경망이 시퀀스를 처리함으로써 이전의 합성곱

9
00:00:23,200 --> 00:00:26,120
신경망(convolutional networks)으로는 해결할 수

10
00:00:26,120 --> 00:00:29,120
없었던 새로운 문제들을 해결할 수 있게 해준다는 것을 보았습니다.

11
00:00:29,120 --> 00:00:31,073
특히, 우리는 일반적으로

12
00:00:31,073 --> 00:00:33,240
이미지와 같은 하나의 입력을 넣고

13
00:00:33,240 --> 00:00:36,560
그 이미지에 대한 분류와 같은 하나의 출력을

14
00:00:36,560 --> 00:00:38,780
내는 일대일 문제를 생각해왔습니다.

15
00:00:38,780 --> 00:00:41,080
하지만 이미지에서 벗어나 데이터 시퀀스로

16
00:00:41,080 --> 00:00:42,870
나아갈 수 있는 능력을 가지게

17
00:00:42,870 --> 00:00:44,870
되면서, 우리는 이미지 캡셔닝과 같은

18
00:00:44,870 --> 00:00:47,218
일대다 문제를 해결할 수 있게 되었습니다.

19
00:00:47,218 --> 00:00:49,760
예를 들어, 이미지를 입력하고 그 이미지에 대한

20
00:00:49,760 --> 00:00:52,680
텍스트 설명을 출력하는 것, 즉 단어의 시퀀스가 될

21
00:00:52,680 --> 00:00:55,880
수 있습니다. 또는 여러 개의 프레임을 입력하고 그

22
00:00:55,880 --> 00:00:58,000
프레임에 대한 분류를 출력하는 다대일 문제와

23
00:00:58,000 --> 00:01:00,500
같은 여러 가지 문제를 해결할 수 있습니다.

24
00:01:00,500 --> 00:01:03,860
따라서 이제 우리는 이러한 더 정교한 신경망 아키텍처로

25
00:01:03,860 --> 00:01:05,860
나아가는 것이 구조적으로 더

26
00:01:05,860 --> 00:01:07,720
흥미롭고, 또한 전통적인 피드포워드

27
00:01:07,720 --> 00:01:09,340
신경망으로는 해결할 수

28
00:01:09,340 --> 00:01:12,540
없었던 새로운 문제를 해결할 수 있게 해준다는

29
00:01:12,540 --> 00:01:13,637
것을 보고 있습니다.

30
00:01:13,637 --> 00:01:15,220
그래서 오늘은 그 기반 위에서

31
00:01:15,220 --> 00:01:18,140
두 가지 새로운 주제에 대해 이야기할 것입니다.

32
00:01:18,140 --> 00:01:20,022
첫 번째는 주의(attention)로,

33
00:01:20,022 --> 00:01:21,980
이는 벡터 집합에서

34
00:01:21,980 --> 00:01:25,175
기본적으로 작동하는 새로운 신경망 원시 요소입니다.

35
00:01:25,175 --> 00:01:27,300
두 번째는 변환기(transformer)에

36
00:01:27,300 --> 00:01:28,480
대해 이야기할 것입니다.

37
00:01:28,480 --> 00:01:30,580
변환기는 자기 주의(self-attention)를

38
00:01:30,580 --> 00:01:33,780
핵심으로 하는 다른 신경망 아키텍처입니다.

39
00:01:33,780 --> 00:01:37,780
스포일러 알림은 변환기가 오늘날 딥러닝의 거의

40
00:01:37,780 --> 00:01:40,220
모든 문제에 사용되는

41
00:01:40,220 --> 00:01:41,620
아키텍처라는 것입니다.

42
00:01:41,620 --> 00:01:43,620
따라서 오늘날 여러분이

43
00:01:43,620 --> 00:01:45,300
보게 될 가장 큰

44
00:01:45,300 --> 00:01:48,900
응용 프로그램들, 이미지 분류, 이미지 생성,

45
00:01:48,900 --> 00:01:51,940
텍스트 생성, 텍스트 분류, 오디오

46
00:01:51,940 --> 00:01:55,160
작업 등, 기본적으로 오늘날 대규모로

47
00:01:55,160 --> 00:01:57,700
훈련되고 대기업에 의해 배포된

48
00:01:57,700 --> 00:01:59,560
모든 최신 신경망은

49
00:01:59,560 --> 00:02:01,600
거의 모두 변환기입니다.

50
00:02:01,600 --> 00:02:03,958
그래서 사람들이 지금 사용하는 최신

51
00:02:03,958 --> 00:02:06,000
아키텍처에 대한 정보를 제공할

52
00:02:06,000 --> 00:02:08,840
수 있다는 것은 정말 흥미로운 일입니다.

53
00:02:08,840 --> 00:02:11,400
하지만 변환기가 오늘날 모든 것이 사용하는

54
00:02:11,400 --> 00:02:14,020
최첨단 아키텍처임에도 불구하고, 그들은

55
00:02:14,020 --> 00:02:16,480
비교적 긴 역사를 가지고 있습니다.

56
00:02:16,480 --> 00:02:19,240
그리고 그들은 처음에-- 이 분야가 발전하는 모습을

57
00:02:19,240 --> 00:02:20,538
보는 것은 흥미롭습니다.

58
00:02:20,538 --> 00:02:23,080
왜냐하면 변환기가 등장한 순간을

59
00:02:23,080 --> 00:02:24,480
되돌아보면, 그것은

60
00:02:24,480 --> 00:02:26,160
큰 변화가 있었던

61
00:02:26,160 --> 00:02:28,243
순간처럼 느껴져야 했기

62
00:02:28,243 --> 00:02:29,140
때문입니다.

63
00:02:29,140 --> 00:02:30,697
하지만 실제로는 그렇게 느껴지지 않았습니다.

64
00:02:30,697 --> 00:02:33,280
왜냐하면 변환기 아키텍처가 태어난

65
00:02:33,280 --> 00:02:36,080
순간이 있었지만, 자기 주의와 다양한

66
00:02:36,080 --> 00:02:38,858
방식으로 주의를 사용하는 아이디어는 그

67
00:02:38,858 --> 00:02:41,400
당시 몇 년 동안 이 분야에 존재했기

68
00:02:41,400 --> 00:02:42,340
때문입니다.

69
00:02:42,340 --> 00:02:45,740
특히, 자기 주의에 대한 이러한 아이디어는

70
00:02:45,740 --> 00:02:48,180
순환 신경망에서 발전했습니다.

71
00:02:48,180 --> 00:02:50,120
그래서 우리는 여기서 시작하여 이러한 문제를 이야기하고 동기를

72
00:02:50,120 --> 00:02:50,782
부여할 것입니다.

73
00:02:50,782 --> 00:02:52,240
이것은 이러한

74
00:02:52,240 --> 00:02:55,800
아이디어의 역사적 발전을 약간 반영하는 것입니다.

75
00:02:55,800 --> 00:02:58,352
그래서 그런 이유로, 변환기를 소개하기

76
00:02:58,352 --> 00:03:00,060
위해 우리는 실제로

77
00:03:00,060 --> 00:03:01,780
지난 강의에서 보았던 순환

78
00:03:01,780 --> 00:03:03,613
신경망에 대한 아이디어를

79
00:03:03,613 --> 00:03:05,220
다시 살펴볼 것입니다.

80
00:03:05,220 --> 00:03:08,580
자극 문제로, 번역의 시퀀스-시퀀스

81
00:03:08,580 --> 00:03:10,740
문제를 생각해 봅시다.

82
00:03:10,740 --> 00:03:12,260
하나의 시퀀스, 즉

83
00:03:12,260 --> 00:03:14,920
영어 단어의 시퀀스를 입력하고 싶습니다.

84
00:03:14,920 --> 00:03:16,620
그런 다음 다른 언어,

85
00:03:16,620 --> 00:03:18,287
이탈리아어의 단어 시퀀스를

86
00:03:18,287 --> 00:03:20,040
출력하고 싶습니다. 그리고

87
00:03:20,040 --> 00:03:23,420
우리는 그 단어들 사이에 어떤 대응 관계가 있다고

88
00:03:23,420 --> 00:03:24,680
가정할 수 없습니다.

89
00:03:24,680 --> 00:03:26,588
영어 문장의 단어 수는 이탈리아어

90
00:03:26,588 --> 00:03:28,380
문장의 단어 수와

91
00:03:28,380 --> 00:03:29,427
다를 수 있습니다.

92
00:03:29,427 --> 00:03:31,760
그리고 그 단어들의 순서도 완전히 다를 수 있습니다.

93
00:03:31,760 --> 00:03:33,780
그래서 이것은 우리가

94
00:03:33,780 --> 00:03:35,660
순환 신경망에서 보았던

95
00:03:35,660 --> 00:03:37,380
시퀀스 처리 알고리즘의

96
00:03:37,380 --> 00:03:39,180
완벽한 응용입니다.

97
00:03:39,180 --> 00:03:42,020
실제로, 순환 신경망으로 이러한 시퀀스 투 시퀀스

98
00:03:42,020 --> 00:03:44,500
문제를 처리하는 아이디어는 2014년으로

99
00:03:44,500 --> 00:03:48,160
거슬러 올라가며, 그보다 조금 더 이전에도 존재했습니다.

100
00:03:48,160 --> 00:03:49,900
하지만 현재까지 10년

101
00:03:49,900 --> 00:03:53,420
이상 사람들이 순환 신경망으로 시퀀스를

102
00:03:53,420 --> 00:03:54,900
처리해왔습니다.

103
00:03:54,900 --> 00:03:57,680
따라서 순환 신경망으로 시퀀스 투 시퀀스 문제를

104
00:03:57,680 --> 00:03:59,960
처리하기 위한 기본 아키텍처는

105
00:03:59,960 --> 00:04:02,220
일반적으로 하나의 인코더로 시작합니다.

106
00:04:02,220 --> 00:04:04,520
당신의 인코더는 순환 신경망입니다.

107
00:04:04,520 --> 00:04:06,000
순환 신경망의 기억은

108
00:04:06,000 --> 00:04:08,480
두 입력에 대해 재귀적으로 적용되는

109
00:04:08,480 --> 00:04:09,820
이 함수입니다.

110
00:04:09,820 --> 00:04:12,520
하나는 현재 시간 단계에서의 입력인 xt입니다.

111
00:04:12,520 --> 00:04:14,380
다른 하나는 이전 시간

112
00:04:14,380 --> 00:04:16,940
단계에서의 은닉 상태인 ht-1입니다.

113
00:04:16,940 --> 00:04:18,523
그리고 당신의 순환 신경망

114
00:04:18,523 --> 00:04:20,680
유닛은 다음 시간 단계에서의 다음

115
00:04:20,680 --> 00:04:23,200
은닉 유닛, 즉 다음 은닉 상태를 출력합니다.

116
00:04:23,200 --> 00:04:26,160
그런 다음 우리는 시간이 지남에 따라 동일한 순환

117
00:04:26,160 --> 00:04:28,920
신경망 유닛을 적용하여 잠재적으로 가변 길이의

118
00:04:28,920 --> 00:04:30,948
시퀀스를 처리할 수 있습니다.

119
00:04:30,948 --> 00:04:33,240
이 경우, 우리는 입력 시퀀스를 영어로

120
00:04:33,240 --> 00:04:36,560
입력하는 순환 신경망 인코더를 사용하고 있습니다.

121
00:04:36,560 --> 00:04:40,000
입력 시퀀스 - 슬라이드에 맞추기 위해 상대적으로 짧은 문장을

122
00:04:40,000 --> 00:04:42,340
사용해야 하며 모든 상자가 표시되도록 해야 합니다.

123
00:04:42,340 --> 00:04:44,460
그래서 우리는 "우리는 하늘을 봅니다."라는

124
00:04:44,460 --> 00:04:46,160
짧고 어리석은 문장을 사용하고 있습니다.

125
00:04:46,160 --> 00:04:48,320
이 문장의 각 단어는

126
00:04:48,320 --> 00:04:53,040
순환 신경망의 한 틱을 통해 처리됩니다.

127
00:04:53,040 --> 00:04:56,500
그리고 이 인코더 순환 신경망의 아이디어는

128
00:04:56,500 --> 00:04:59,500
입력 시퀀스의 모든 단어를

129
00:04:59,500 --> 00:05:02,340
처리하고 그 입력 문장의 내용을

130
00:05:02,340 --> 00:05:05,460
요약하여 다른 언어로 번역할 수 있도록

131
00:05:05,460 --> 00:05:07,060
하는 것입니다.

132
00:05:07,060 --> 00:05:10,180
이것이 구체적으로 발생하는 방법은

133
00:05:10,180 --> 00:05:14,080
입력 시퀀스의 모든 단어를 처리한 후, 그

134
00:05:14,080 --> 00:05:17,300
입력 시퀀스의 전체 내용을 하나의

135
00:05:17,300 --> 00:05:21,077
벡터인 컨텍스트 벡터로 요약하는 것입니다.

136
00:05:21,077 --> 00:05:22,660
사람들이 일반적으로 순환 신경망에서

137
00:05:22,660 --> 00:05:24,160
이러한 작업을 수행하는

138
00:05:24,160 --> 00:05:25,523
몇 가지 방법이 있습니다.

139
00:05:25,523 --> 00:05:27,440
세부 사항이 그리 흥미롭지 않다고 생각합니다.

140
00:05:27,440 --> 00:05:30,340
그래서 컨텍스트 벡터는 기본적으로 인코더

141
00:05:30,340 --> 00:05:33,300
순환 신경망의 마지막 은닉 상태라고 생각할

142
00:05:33,300 --> 00:05:34,260
수 있습니다.

143
00:05:34,260 --> 00:05:36,900
이제 순환 신경망의 순환

144
00:05:36,900 --> 00:05:39,280
구조 때문에 마지막 은닉

145
00:05:39,280 --> 00:05:41,740
상태는 전체 입력 시퀀스의

146
00:05:41,740 --> 00:05:43,320
정보를 포함합니다.

147
00:05:43,320 --> 00:05:45,260
따라서 우리는 마지막 은닉 상태가

148
00:05:45,260 --> 00:05:47,820
전체 입력 시퀀스의 모든 정보를 요약하거나

149
00:05:47,820 --> 00:05:49,660
인코딩한다고 생각할 수 있습니다.

150
00:05:49,660 --> 00:05:52,220
그래서 그것은 우리가 원하는 대로

151
00:05:52,220 --> 00:05:55,240
사용할 전체 입력 시퀀스를 요약하는

152
00:05:55,240 --> 00:05:56,600
하나의 벡터입니다.

153
00:05:56,600 --> 00:05:58,360
이 경우, 우리가 그것으로 하고

154
00:05:58,360 --> 00:06:00,520
싶은 것은 입력 시퀀스를 다른 언어의

155
00:06:00,520 --> 00:06:02,582
출력 시퀀스로 번역하는 것입니다.

156
00:06:02,582 --> 00:06:05,040
이를 위해 우리는 디코더라고 불리는 두 번째

157
00:06:05,040 --> 00:06:07,760
순환 신경망을 사용할 것이며, 일반적으로 동일한

158
00:06:07,760 --> 00:06:10,520
아키텍처를 가지지만 잠재적으로 다른 가중치

159
00:06:10,520 --> 00:06:12,800
행렬과 학습된 매개변수 집합을 가집니다.

160
00:06:12,800 --> 00:06:14,800
이 디코더 gu는 다른 학습

161
00:06:14,800 --> 00:06:17,640
가능한 가중치 u를 가진 다른 순환

162
00:06:17,640 --> 00:06:20,200
신경망이지만 기본 아이디어는 동일합니다.

163
00:06:20,200 --> 00:06:22,840
이제 우리의 순환 신경망 유닛이 매

164
00:06:22,840 --> 00:06:25,300
시간 단계에서 세 개의 입력을 받을

165
00:06:25,300 --> 00:06:27,520
것이라면, 이전 시간 단계의

166
00:06:27,520 --> 00:06:30,240
출력 시퀀스에서의 토큰 yt-1을 받을

167
00:06:30,240 --> 00:06:30,900
것입니다.

168
00:06:30,900 --> 00:06:34,400
출력 시퀀스의 이전 은닉 상태인 st-1을

169
00:06:34,400 --> 00:06:36,480
받을 것이고, 전체 입력

170
00:06:36,480 --> 00:06:39,440
시퀀스를 요약하는 컨텍스트 벡터 C를

171
00:06:39,440 --> 00:06:40,400
받을 것입니다.

172
00:06:40,400 --> 00:06:42,520
그런 다음 우리는 마지막 강의에서 본

173
00:06:42,520 --> 00:06:45,680
것처럼 출력 시퀀스를 펼치고 출력 시퀀스에서 단어를 한

174
00:06:45,680 --> 00:06:47,178
번에 하나씩 생성합니다.

175
00:06:47,178 --> 00:06:48,720
저는 이탈리아어를 하지 않기

176
00:06:48,720 --> 00:06:50,980
때문에 발음하려고 시도하지 않을 것입니다.

177
00:06:50,980 --> 00:06:54,460
하지만 화면에 이탈리아어 단어가 몇 개 보입니다.

178
00:06:54,460 --> 00:06:58,780
그리고 저는 그것이 실제로 "우리는 하늘을 봅니다."로 번역된다고

179
00:06:58,780 --> 00:07:00,380
가정하고 있습니다.

180
00:07:00,380 --> 00:07:01,882
희망적으로, 그것이 맞기를 바랍니다.

181
00:07:01,882 --> 00:07:03,340
하지만 아이디어는 이 순환 신경망을

182
00:07:03,340 --> 00:07:05,382
한 번에 한 틱씩 진행할 것이라는 것입니다.

183
00:07:05,382 --> 00:07:07,200
하나씩 단어를 출력할 것입니다.

184
00:07:07,200 --> 00:07:09,837
그리고 이것은 우리가 지난 강의에서 본 내용을 요약한 것입니다.

185
00:07:09,837 --> 00:07:11,420
따라서 이전 강의를

186
00:07:11,420 --> 00:07:15,140
고려할 때, 이것은 그리 놀라운 일이 아닐 것입니다.

187
00:07:15,140 --> 00:07:16,920
하지만 여기에는 잠재적인 문제가 있습니다.

188
00:07:16,920 --> 00:07:19,740
입력 시퀀스와 출력 시퀀스

189
00:07:19,740 --> 00:07:23,460
사이에 통신 병목 현상이 있습니다.

190
00:07:23,460 --> 00:07:26,660
입력 시퀀스가 출력 시퀀스와 통신하는 유일한

191
00:07:26,660 --> 00:07:30,020
방법은 그 컨텍스트 벡터 C를 통해서입니다.

192
00:07:30,020 --> 00:07:31,740
그리고 그 C는 고정 길이

193
00:07:31,740 --> 00:07:33,740
벡터가 될 것이며, 그 벡터의

194
00:07:33,740 --> 00:07:37,300
크기는 순환 신경망의 크기를 설정할 때 고정됩니다.

195
00:07:37,300 --> 00:07:38,760
그리고 아마도 그게 괜찮을 것입니다.

196
00:07:38,760 --> 00:07:42,420
그래서 C는 128개의 부동 소수점 또는 1,024개의 부동 소수점으로
이루어진

197
00:07:42,420 --> 00:07:43,880
고정 길이 벡터일 수 있습니다.

198
00:07:43,880 --> 00:07:46,660
하지만 입력 벡터의 크기는 입력 및 출력 시퀀스의

199
00:07:46,660 --> 00:07:49,300
크기가 커지거나 작아져도 변하지 않을 것입니다.

200
00:07:49,300 --> 00:07:51,080
그리고 그것이 잠재적인 문제입니다.

201
00:07:51,080 --> 00:07:53,933
그래서 짧은 시퀀스를 처리할 때, 예를 들어 '하늘을

202
00:07:53,933 --> 00:07:56,600
봅니다'와 같은 경우, 그 고정 벡터인 1,

203
00:07:56,600 --> 00:07:58,880
024개의 부동 소수점으로 그 시퀀스에 대해

204
00:07:58,880 --> 00:08:00,320
알아야 할 모든 것을

205
00:08:00,320 --> 00:08:01,780
요약할 수 있을 것 같습니다.

206
00:08:01,780 --> 00:08:04,180
하지만 네 개의 단어를 번역하려고 하는 것이 아니라면 어떻게 될까요?

207
00:08:04,180 --> 00:08:05,555
전체 단락이나

208
00:08:05,555 --> 00:08:07,360
전체 책, 또는 전체

209
00:08:07,360 --> 00:08:09,815
데이터 집합을 번역하려고 한다면?

210
00:08:09,815 --> 00:08:12,440
그 경우에는 입력 시퀀스를

211
00:08:12,440 --> 00:08:15,140
확장할 때, 네트워크에 전체 입력

212
00:08:15,140 --> 00:08:18,080
시퀀스를 단일 고정 길이 벡터로

213
00:08:18,080 --> 00:08:20,320
요약하라고 요청하는 것이

214
00:08:20,320 --> 00:08:22,820
합리적이지 않을 것입니다.

215
00:08:22,820 --> 00:08:25,040
그래서 그것이 문제가 될 것입니다.

216
00:08:25,040 --> 00:08:28,160
해결책은 사실, 네트워크를 하나의

217
00:08:28,160 --> 00:08:33,400
고정 길이 벡터로 병목 현상을 만들지 않도록 하자는 것입니다.

218
00:08:33,400 --> 00:08:35,600
대신, 순환 신경망의

219
00:08:35,600 --> 00:08:37,100
아키텍처를 변경합시다.

220
00:08:37,100 --> 00:08:40,600
직관적으로 우리가 하고 싶은 것은 입력과 출력 사이에 고정 길이

221
00:08:40,600 --> 00:08:43,020
벡터로 병목 현상을 강요하지 않는 것입니다.

222
00:08:43,020 --> 00:08:45,830
대신, 출력 시퀀스를 처리할 때 모델이

223
00:08:45,830 --> 00:08:48,080
입력 시퀀스를 되돌아볼 수 있는 능력을

224
00:08:48,080 --> 00:08:49,480
부여할 것입니다.

225
00:08:49,480 --> 00:08:52,080
이제 매번 출력 벡터를 생성할 때,

226
00:08:52,080 --> 00:08:54,860
네트워크가 전체 입력 시퀀스를 되돌아볼

227
00:08:54,860 --> 00:08:56,192
기회를 주고 싶습니다.

228
00:08:56,192 --> 00:08:58,400
이렇게 하면 병목 현상이 없을 것입니다.

229
00:08:58,400 --> 00:09:00,238
더 긴 시퀀스에 확장될 것입니다.

230
00:09:00,238 --> 00:09:01,780
그리고 희망적으로 모델 아키텍처가

231
00:09:01,780 --> 00:09:03,060
훨씬 더 잘 작동할 것입니다.

232
00:09:03,060 --> 00:09:04,900
그래서 이것이 주의(attention)와

233
00:09:04,900 --> 00:09:06,980
변환기(transformers) 및 오늘날 우리가 보는 모든 훌륭한

234
00:09:06,980 --> 00:09:08,560
것들을 이끌어낸 동기 부여 아이디어입니다.

235
00:09:08,560 --> 00:09:10,713
이 모든 것은 순환 신경망의

236
00:09:10,713 --> 00:09:13,380
병목 문제를 해결하려고 시도한

237
00:09:13,380 --> 00:09:15,620
것에서 비롯된 이야기입니다.

238
00:09:15,620 --> 00:09:18,892
이제 이 직관을 실제로 구현하고 매 시간 단계마다

239
00:09:18,892 --> 00:09:21,100
입력 시퀀스를 되돌아볼 수 있는 능력을

240
00:09:21,100 --> 00:09:23,980
순환 신경망에 부여하는 방법을 살펴보겠습니다.

241
00:09:23,980 --> 00:09:26,540
여기서 우리는 같은 것으로 시작할 것입니다.

242
00:09:26,540 --> 00:09:28,920
우리의 인코더 신경망은 동일하게 유지되며,

243
00:09:28,920 --> 00:09:29,960
변경 사항은 없습니다.

244
00:09:29,960 --> 00:09:32,300
출력 시퀀스를 위한 초기

245
00:09:32,300 --> 00:09:34,980
은닉 상태를 설정해야 합니다.

246
00:09:34,980 --> 00:09:38,660
따라서 초기 디코더 상태 s0를 어떤 방식으로든

247
00:09:38,660 --> 00:09:39,860
설정해야 합니다.

248
00:09:39,860 --> 00:09:42,160
하지만 이제 그 디코더 은닉 상태를 얻은 후,

249
00:09:42,160 --> 00:09:45,005
우리가 할 일은 입력 시퀀스를 되돌아보는 것입니다.

250
00:09:45,005 --> 00:09:46,380
우리가 이를

251
00:09:46,380 --> 00:09:50,560
수행하는 방법은 정렬 점수를 계산하는 것입니다.

252
00:09:50,560 --> 00:09:53,920
기본적으로 입력 시퀀스의 각

253
00:09:53,920 --> 00:09:57,760
단계에 대해 스칼라 값을 계산하여 초기

254
00:09:57,760 --> 00:09:59,760
디코더 상태 s0가 입력

255
00:09:59,760 --> 00:10:03,680
시퀀스의 각 토큰과 얼마나 일치하는지를

256
00:10:03,680 --> 00:10:04,900
나타냅니다.

257
00:10:04,900 --> 00:10:07,623
이 경우 입력 시퀀스에는 네 개의 토큰이 있었습니다.

258
00:10:07,623 --> 00:10:10,040
따라서 우리는 네 개의

259
00:10:10,040 --> 00:10:12,200
정렬 점수를 계산하고자

260
00:10:12,200 --> 00:10:16,940
하며, 각 점수는 입력 시퀀스의 토큰과 이 초기

261
00:10:16,940 --> 00:10:21,880
디코더 상태 s0 간의 유사성을 나타내는 단일

262
00:10:21,880 --> 00:10:22,973
숫자입니다.

263
00:10:22,973 --> 00:10:24,640
정렬 점수를 구현하는 방법은

264
00:10:24,640 --> 00:10:26,100
여러 가지가 있습니다.

265
00:10:26,100 --> 00:10:28,520
하지만 간단한 방법은 f sub att라고 부르는

266
00:10:28,520 --> 00:10:30,400
간단한 선형 레이어를 사용하는 것입니다.

267
00:10:30,400 --> 00:10:32,280
이 선형 레이어는 디코더

268
00:10:32,280 --> 00:10:35,560
은닉 상태 s와 인코더 은닉 상태

269
00:10:35,560 --> 00:10:37,240
h 중 하나를 연결하여

270
00:10:37,240 --> 00:10:39,500
벡터로 만든 다음,

271
00:10:39,500 --> 00:10:42,360
이를 스칼라로 압축하는 선형 변환을

272
00:10:42,360 --> 00:10:43,345
적용합니다.

273
00:10:43,345 --> 00:10:44,720
이는 계산 그래프에 넣을

274
00:10:44,720 --> 00:10:46,540
수 있는 선형 연산자로, 네트워크의

275
00:10:46,540 --> 00:10:49,140
다른 모든 매개변수를 학습하는 방식으로 경량

276
00:10:49,140 --> 00:10:51,660
하강법을 통해 공동으로 학습할 수 있습니다.

277
00:10:51,660 --> 00:10:55,020
이제 이 시점에서 입력 시퀀스의 각 단계에

278
00:10:55,020 --> 00:10:57,860
대한 스칼라 정렬 점수를 얻었습니다.

279
00:10:57,860 --> 00:11:00,420
이제 소프트맥스 함수를 적용하고자 합니다.

280
00:11:00,420 --> 00:11:03,820
이 스칼라 정렬 점수는 완전히 무한합니다.

281
00:11:03,820 --> 00:11:06,520
이들은 마이너스 무한대에서 플러스 무한대까지의 임의의 실수 값입니다.

282
00:11:06,520 --> 00:11:08,900
우리는 이것이 폭발하는 것을 방지하기 위해

283
00:11:08,900 --> 00:11:10,460
구조를 부여하고자 합니다.

284
00:11:10,460 --> 00:11:13,040
그래서 우리가 이를 수행하는 한 가지 방법은 소프트맥스 함수를 적용하는
것입니다.

285
00:11:13,040 --> 00:11:15,700
우리는 디코더 은닉 상태와 각 인코더

286
00:11:15,700 --> 00:11:18,020
은닉 상태의 정렬을 알려주는 네

287
00:11:18,020 --> 00:11:20,460
개의 스칼라 값을 가지고 있습니다.

288
00:11:20,460 --> 00:11:22,900
이제 이 네 개의 값에 대해

289
00:11:22,900 --> 00:11:27,125
소프트맥스를 적용하여 이 네 개의 값에 대한 분포를 제공합니다.

290
00:11:27,125 --> 00:11:28,500
몇 강의 전에 본

291
00:11:28,500 --> 00:11:30,660
소프트맥스 함수는 임의의

292
00:11:30,660 --> 00:11:32,700
점수 벡터를 받아

293
00:11:32,700 --> 00:11:34,722
확률 분포로 변환합니다.

294
00:11:34,722 --> 00:11:36,180
즉, 출력 소프트맥스

295
00:11:36,180 --> 00:11:39,380
확률의 각 항목이 0과 1 사이의 값을

296
00:11:39,380 --> 00:11:42,900
가지며, 이들의 합이 1이 된다는 것입니다.

297
00:11:42,900 --> 00:11:44,460
따라서 벡터를

298
00:11:44,460 --> 00:11:46,780
소프트맥스를 통과시킬 때마다,

299
00:11:46,780 --> 00:11:48,560
우리가 얻는 것을 확률

300
00:11:48,560 --> 00:11:50,560
분포, 즉 입력 점수에

301
00:11:50,560 --> 00:11:52,240
대한 이산 확률 분포로

302
00:11:52,240 --> 00:11:54,000
생각할 수 있습니다.

303
00:11:54,000 --> 00:11:55,260
따라서 이 경우,

304
00:11:55,260 --> 00:11:57,840
정렬 점수를 소프트맥스를

305
00:11:57,840 --> 00:12:01,000
통과시킨 후, 본질적으로 입력 토큰에

306
00:12:01,000 --> 00:12:04,000
대한 분포를 예측한 것입니다.

307
00:12:04,000 --> 00:12:07,880
이는 디코더의 은닉 상태를 고려한 것입니다.

308
00:12:07,880 --> 00:12:10,160
이제 우리가 하고자 하는

309
00:12:10,160 --> 00:12:13,000
것은 입력 토큰에 대한 그 분포를

310
00:12:13,000 --> 00:12:18,080
가져와서 인코더의 정보를 요약하는 벡터를 계산하는 것입니다.

311
00:12:18,080 --> 00:12:21,560
우리가 그렇게 하는 방법은 주의 점수를 가져오는

312
00:12:21,560 --> 00:12:26,880
것입니다. 이 점수는 a11, a12, a13, a14와 같은 숫자로, 모두

313
00:12:26,880 --> 00:12:28,220
0과 1 사이입니다.

314
00:12:28,220 --> 00:12:29,007
이들은 1로 합산됩니다.

315
00:12:29,007 --> 00:12:30,840
이제 인코더의 은닉 상태

316
00:12:30,840 --> 00:12:34,880
h1, h2, h3, h4의 선형 조합을 취하고,

317
00:12:34,880 --> 00:12:38,040
주의 점수로 가중치를 부여한 인코더 은닉 상태의

318
00:12:38,040 --> 00:12:40,400
선형 조합을 취할 것입니다.

319
00:12:40,400 --> 00:12:42,840
이렇게 하면 주의

320
00:12:42,840 --> 00:12:45,420
가중치에 의해 조정된

321
00:12:45,420 --> 00:12:47,940
인코더 시퀀스의

322
00:12:47,940 --> 00:12:52,700
정보를 요약하는 컨텍스트 벡터 c1을

323
00:12:52,700 --> 00:12:54,260
얻게 됩니다.

324
00:12:54,260 --> 00:12:57,140
이 시점에서 c1은 기본적으로 입력

325
00:12:57,140 --> 00:13:01,460
인코더 상태 h1에서 h4까지의 선형 조합입니다.

326
00:13:01,460 --> 00:13:03,180
모든 것이 비주얼 주의가 없는

327
00:13:03,180 --> 00:13:04,980
경우와 기본적으로 동일하게 보입니다.

328
00:13:04,980 --> 00:13:06,860
우리는 컨텍스트 벡터를 가지고 있습니다.

329
00:13:06,860 --> 00:13:10,180
이를 출력 시퀀스의 첫 번째

330
00:13:10,180 --> 00:13:14,200
토큰 y0와 연결하고, 이를 순환

331
00:13:14,200 --> 00:13:19,100
유닛에 전달하여 디코더 순환 신경망의 다음

332
00:13:19,100 --> 00:13:22,140
은닉 상태와 첫 번째 출력

333
00:13:22,140 --> 00:13:24,020
토큰을 얻습니다.

334
00:13:24,020 --> 00:13:27,580
기본적으로 디코더 RNN의 구조는 크게

335
00:13:27,580 --> 00:13:29,020
변경되지 않았습니다.

336
00:13:29,020 --> 00:13:32,180
우리가 한 것은 단지 주의 선형 조합 메커니즘을

337
00:13:32,180 --> 00:13:34,940
사용하여 컨텍스트 벡터를 다른

338
00:13:34,940 --> 00:13:36,780
방식으로 계산한 것입니다.

339
00:13:36,780 --> 00:13:40,180
하지만 이제 중요한 것은 이 컨텍스트 벡터가

340
00:13:40,180 --> 00:13:42,160
본질적으로 입력

341
00:13:42,160 --> 00:13:45,840
시퀀스의 다양한 부분을 주의 깊게 살펴보며, 이는

342
00:13:45,840 --> 00:13:49,260
출력 RNN이 현재 시점에서 보고자 하는

343
00:13:49,260 --> 00:13:51,120
것에 의해 조정됩니다.

344
00:13:51,120 --> 00:13:54,920
예를 들어, 입력 시퀀스의 일부에는

345
00:13:54,920 --> 00:13:59,040
우리가 보는 이 두 단어가 포함되어

346
00:13:59,040 --> 00:13:59,920
있습니다.

347
00:13:59,920 --> 00:14:02,840
그러면 우리가 'we see'에 해당하는 이탈리아어

348
00:14:02,840 --> 00:14:05,640
단어를 생성하려고 할 때, 네트워크는

349
00:14:05,640 --> 00:14:08,120
아마도 출력 단어를 생성하기 위해 입력

350
00:14:08,120 --> 00:14:11,280
시퀀스의 두 단어를 다시 살펴보기를 원할 것입니다.

351
00:14:11,280 --> 00:14:13,220
그래서 우리는 직관적으로

352
00:14:13,220 --> 00:14:16,360
'vediamo'라는 단어를 생성하려고 할

353
00:14:16,360 --> 00:14:20,320
때 네트워크가 'we see'라는 단어를 다시 보고

354
00:14:20,320 --> 00:14:23,840
그 단어에 더 높은 주의 가중치를 두기를 원할

355
00:14:23,840 --> 00:14:25,900
것이라고 기대할 수 있습니다.

356
00:14:25,900 --> 00:14:27,860
그리고 그것은 'vediamo'

357
00:14:27,860 --> 00:14:30,840
출력을 생성하는 데 필요하지 않기 때문에 하늘에

358
00:14:30,840 --> 00:14:32,865
대해서는 신경 쓰지 않습니다.

359
00:14:32,865 --> 00:14:34,240
이것이 바로 직관입니다.

360
00:14:34,240 --> 00:14:35,520
우리는 네트워크가

361
00:14:35,520 --> 00:14:37,360
현재 예측하려는 단어에 대해

362
00:14:37,360 --> 00:14:39,402
입력 시퀀스의 관련 부분을 다시

363
00:14:39,402 --> 00:14:42,220
살펴볼 수 있는 능력을 부여하고 있습니다.

364
00:14:42,220 --> 00:14:43,780
그리고 염두에 두어야 할 또 다른

365
00:14:43,780 --> 00:14:45,620
점은 이것이 모두 미분 가능하다는 것입니다.

366
00:14:45,620 --> 00:14:47,320
우리는 네트워크를 감독할 필요가 없습니다.

367
00:14:47,320 --> 00:14:50,020
우리는 출력의 각 단어에 대해 입력 시퀀스에서 어떤

368
00:14:50,020 --> 00:14:52,040
단어가 필요한지 알려줄 필요가 없습니다.

369
00:14:52,040 --> 00:14:54,580
대신, 이것은 미분 가능한 연산으로

370
00:14:54,580 --> 00:14:56,580
구성된 큰 계산 그래프입니다.

371
00:14:56,580 --> 00:14:59,198
이 모든 것은 경량 하강법을 통해 엔드 투 엔드로 학습될 수 있습니다.

372
00:14:59,198 --> 00:15:00,740
결국 우리는 여전히

373
00:15:00,740 --> 00:15:03,525
네트워크가 출력 시퀀스의 토큰을 예측하려고 하는

374
00:15:03,525 --> 00:15:05,900
교차 엔트로피 소프트맥스 손실을

375
00:15:05,900 --> 00:15:06,680
가질 것입니다.

376
00:15:06,680 --> 00:15:09,060
출력 시퀀스에서 올바른 토큰을 예측하려는

377
00:15:09,060 --> 00:15:10,920
과정에서 네트워크는

378
00:15:10,920 --> 00:15:12,940
입력 시퀀스의 다양한 부분에 주의를

379
00:15:12,940 --> 00:15:16,140
기울이는 방법을 스스로 학습하게 될 것입니다.

380
00:15:16,140 --> 00:15:17,540
그래서 이것은 정말 중요합니다.

381
00:15:17,540 --> 00:15:19,980
우리가 네트워크에 두 개의 정렬을 감독하고

382
00:15:19,980 --> 00:15:21,540
알려줘야 한다면, 이런 종류의

383
00:15:21,540 --> 00:15:23,373
훈련 데이터를 얻는 것은

384
00:15:23,373 --> 00:15:24,460
매우 어려울 것입니다.

385
00:15:24,460 --> 00:15:26,550
질문은 디코더를 어떻게 초기화하느냐입니다.

386
00:15:26,550 --> 00:15:28,800
우리는 실제로 '초기화'라는 단어를

387
00:15:28,800 --> 00:15:30,175
약간 과부하된 상태로

388
00:15:30,175 --> 00:15:31,560
사용하고 있습니다.

389
00:15:31,560 --> 00:15:34,660
그래서 한 가지 질문은 디코더 자체가 가중치를 가진

390
00:15:34,660 --> 00:15:35,500
신경망이라는 것입니다.

391
00:15:35,500 --> 00:15:37,000
우리가 그 네트워크를 훈련하기 시작할 때,

392
00:15:37,000 --> 00:15:39,260
우리는 어떤 방식으로든 그 가중치를 초기화해야 합니다.

393
00:15:39,260 --> 00:15:40,800
그래서 우리는 일반적으로

394
00:15:40,800 --> 00:15:43,652
디코더의 가중치를 무작위로 초기화한 다음, 다른

395
00:15:43,652 --> 00:15:46,360
신경망 가중치와 마찬가지로 경량 하강법을

396
00:15:46,360 --> 00:15:47,320
통해 최적화합니다.

397
00:15:47,320 --> 00:15:49,380
하지만 초기화의 두 번째 개념이 있습니다.

398
00:15:49,380 --> 00:15:51,680
네트워크가 시퀀스를 처리할

399
00:15:51,680 --> 00:15:55,620
때, 현재 가중치의 값이 무엇이든, 출력 시퀀스를

400
00:15:55,620 --> 00:15:58,600
처리하기 시작할 때 초기 은닉

401
00:15:58,600 --> 00:16:01,320
상태를 설정할 방법이 필요합니다.

402
00:16:01,320 --> 00:16:05,200
그 경우, 우리는 디코더 출력 시퀀스의 초기 은닉

403
00:16:05,200 --> 00:16:08,757
상태를 설정할 규칙이나 방법이 필요합니다.

404
00:16:08,757 --> 00:16:10,840
이를 위한 몇 가지 다른 메커니즘이 있습니다.

405
00:16:10,840 --> 00:16:12,320
때때로, 당신은

406
00:16:12,320 --> 00:16:15,520
마지막 인코더의 은닉 상태로 초기화할

407
00:16:15,520 --> 00:16:16,980
수 있습니다.

408
00:16:16,980 --> 00:16:20,120
마지막 디코더 상태에서 첫 번째

409
00:16:20,120 --> 00:16:22,520
디코더 상태로의 학습된 투영을

410
00:16:22,520 --> 00:16:23,340
가진

411
00:16:23,340 --> 00:16:26,240
선형 변환이 있을 수 있습니다.

412
00:16:26,240 --> 00:16:28,360
또는 때때로 사람들은 디코더의 첫

413
00:16:28,360 --> 00:16:31,907
번째 은닉 상태를 모두 0으로 초기화하기도 합니다.

414
00:16:31,907 --> 00:16:34,240
그 중 어떤 것이든 네트워크가 그런 종류의 입력을

415
00:16:34,240 --> 00:16:36,240
기대하도록 훈련한다면 작동할 것입니다.

416
00:16:36,240 --> 00:16:37,880
그래서 질문은 부정과 XOR가

417
00:16:37,880 --> 00:16:39,160
문제를 일으킬까요?

418
00:16:39,160 --> 00:16:39,660
아마도.

419
00:16:39,660 --> 00:16:40,822
이것은 어려운 문제입니다.

420
00:16:40,822 --> 00:16:42,780
하지만 그런 경우에는 많은

421
00:16:42,780 --> 00:16:45,460
데이터와 많은 연산이 필요합니다.

422
00:16:45,460 --> 00:16:49,080
기본적으로 순환 유닛은 세 가지를 입력으로 받습니다.

423
00:16:49,080 --> 00:16:50,567
디코더를 입력으로 받습니다.

424
00:16:50,567 --> 00:16:52,900
이전의 은닉 상태와 이전의 디코더 은닉

425
00:16:52,900 --> 00:16:53,800
상태를 사용합니다.

426
00:16:53,800 --> 00:16:55,340
현재의 컨텍스트 벡터를 사용합니다.

427
00:16:55,340 --> 00:16:59,020
그리고 출력 시퀀스의 현재 토큰을 사용합니다.

428
00:16:59,020 --> 00:17:01,620
그로부터 다음 은닉 상태를 생성합니다.

429
00:17:01,620 --> 00:17:03,120
그리고 다음 은닉 상태에서

430
00:17:03,120 --> 00:17:04,885
출력 토큰을 예측합니다.

431
00:17:04,885 --> 00:17:06,260
그래서 사실 비주목

432
00:17:06,260 --> 00:17:08,380
경우와 동일한 설정입니다.

433
00:17:08,380 --> 00:17:10,060
s0에서 s1로의

434
00:17:10,060 --> 00:17:13,780
연결이 있지만 우리는 그려지지 않았습니다.

435
00:17:13,780 --> 00:17:17,060
s0에서 s1로의 또 다른 화살표가 있어야 했습니다.

436
00:17:17,060 --> 00:17:20,391
s0 화살표를 놓친 것 같아서 죄송합니다.

437
00:17:20,391 --> 00:17:22,099
기본적으로 네트워크가 현재

438
00:17:22,099 --> 00:17:24,980
작업에 관련이 있을 것이라고 생각하는 입력

439
00:17:24,980 --> 00:17:28,800
시퀀스의 어떤 부분을 되돌아볼지 스스로 결정하도록 하고 있습니다.

440
00:17:28,800 --> 00:17:31,860
우리가 이 메커니즘이 그럴듯하고 네트워크에

441
00:17:31,860 --> 00:17:33,500
도움이 될 것이라고

442
00:17:33,500 --> 00:17:36,160
생각하는 이유는 언어 작업에서 출력의 단어와

443
00:17:36,160 --> 00:17:37,920
입력의 단어 사이에 종종

444
00:17:37,920 --> 00:17:40,620
어떤 종류의 대응이 있기 때문입니다.

445
00:17:40,620 --> 00:17:43,440
우리는 네트워크가 되돌아보고 이 출력의 일부를

446
00:17:43,440 --> 00:17:45,720
생성하는 데 관련된 입력의 부분을

447
00:17:45,720 --> 00:17:47,025
선택하도록 하고 싶습니다.

448
00:17:47,025 --> 00:17:48,900
하지만 다시 말하지만, 우리는 그것을 직접 감독하지 않습니다.

449
00:17:48,900 --> 00:17:51,340
우리는 이 주목 점수를 어떻게 사용할지 알려주지 않습니다.

450
00:17:51,340 --> 00:17:52,960
하지만 우리의 직관은 이

451
00:17:52,960 --> 00:17:55,320
메커니즘을 고려할 때 그것이 선택할 수 있는

452
00:17:55,320 --> 00:17:56,960
그럴듯한 일이라고 생각합니다.

453
00:17:56,960 --> 00:18:00,520
좋아요, 그래서 출력의 한 번의 틱입니다.

454
00:18:00,520 --> 00:18:02,500
이제 기본적으로, 우리는 다시 합니다.

455
00:18:02,500 --> 00:18:04,560
디코더 RNN을 틱할 때마다 이

456
00:18:04,560 --> 00:18:06,245
전체 과정을 다시 수행합니다.

457
00:18:06,245 --> 00:18:08,120
우리가 해결하려고 했던 문제는 이전에

458
00:18:08,120 --> 00:18:10,360
디코더가 단일 벡터를 통해 병목 현상을

459
00:18:10,360 --> 00:18:11,680
일으켰다는 것입니다.

460
00:18:11,680 --> 00:18:12,938
이제 우리는 단일 벡터를

461
00:18:12,938 --> 00:18:14,980
통해 병목 현상을 일으키는

462
00:18:14,980 --> 00:18:17,397
대신, 이 전체 과정을 다시 반복하고

463
00:18:17,397 --> 00:18:20,360
디코더의 두 번째 시간 단계에 대한 새로운 컨텍스트

464
00:18:20,360 --> 00:18:22,920
벡터를 계산하며, 전체 입력 시퀀스를 다시

465
00:18:22,920 --> 00:18:23,880
살펴보게 합니다.

466
00:18:23,880 --> 00:18:27,120
그래서 이제 기본적으로, 디코더에서

467
00:18:27,120 --> 00:18:31,120
계산된 첫 번째 은닉 상태인 s1을 가지고

468
00:18:31,120 --> 00:18:32,400
돌아갑니다.

469
00:18:32,400 --> 00:18:35,460
s1을 가져와서 돌아가서 비교를 계산하고,

470
00:18:35,460 --> 00:18:38,540
우리의 주의 메커니즘을 사용하여 s1과

471
00:18:38,540 --> 00:18:41,820
인코더의 모든 은닉 상태 간의 유사성

472
00:18:41,820 --> 00:18:43,020
점수를 계산합니다.

473
00:18:43,020 --> 00:18:44,900
이것은 우리가 첫 번째 시간

474
00:18:44,900 --> 00:18:48,340
단계에서 사용했던 동일한 FATT, 동일한 선형

475
00:18:48,340 --> 00:18:51,740
프로젝션을 사용하여 유사성 점수를 계산하고,

476
00:18:51,740 --> 00:18:55,100
다시 소프트맥스를 통해 입력 시퀀스에 대한 새로운

477
00:18:55,100 --> 00:18:58,120
분포를 얻기 위해 정렬 점수를 계산하며,

478
00:18:58,120 --> 00:19:00,260
이제 두 번째 시간 단계에서

479
00:19:00,260 --> 00:19:02,420
계산한 이 새로운 분포에 의해

480
00:19:02,420 --> 00:19:05,700
가중치가 부여된 인코더 은닉 상태의 새로운 선형

481
00:19:05,700 --> 00:19:07,100
조합을 계산합니다.

482
00:19:07,100 --> 00:19:12,620
이것은 기본적으로 이제 입력 시퀀스의 다른 요약인 새로운

483
00:19:12,620 --> 00:19:15,660
컨텍스트 벡터 c2를 제공합니다. 이는

484
00:19:15,660 --> 00:19:18,700
이제 입력 인코더 은닉 상태의

485
00:19:18,700 --> 00:19:21,500
새로운 선형 조합으로 계산됩니다.

486
00:19:21,500 --> 00:19:23,960
그리고 전체 과정이 반복됩니다.

487
00:19:23,960 --> 00:19:25,320
우리는 새로운 컨텍스트 벡터를 가지고 있습니다.

488
00:19:25,320 --> 00:19:29,340
우리는 그것을 사용하여 이제 이전 시간 단계에 없었던

489
00:19:29,340 --> 00:19:32,260
신비로운 누락된 화살표를 포함하는 디코더 RNN

490
00:19:32,260 --> 00:19:34,620
유닛의 또 다른 틱을 실행합니다.

491
00:19:34,620 --> 00:19:37,140
그래서 새로운 컨텍스트 벡터,

492
00:19:37,140 --> 00:19:39,500
출력 시퀀스의 다음 토큰, 그리고

493
00:19:39,500 --> 00:19:42,220
디코더의 s1 은닉 상태를 고려하여

494
00:19:42,220 --> 00:19:45,580
새로운 디코더 상태 s2를 계산하고, 그로부터

495
00:19:45,580 --> 00:19:48,960
출력 시퀀스의 또 다른 토큰을 계산합니다.

496
00:19:48,960 --> 00:19:49,500
그리고 다시.

497
00:19:49,500 --> 00:19:52,860
이 경우, 슬라이드에 따르면 il을 생성하고 있으며,

498
00:19:52,860 --> 00:19:55,260
이는 아마도 "the"일 것입니다.

499
00:19:55,260 --> 00:19:56,560
그게 사실이길 바랍니다.

500
00:19:56,560 --> 00:19:58,887
이 경우, 네트워크가 이 시퀀스를

501
00:19:58,887 --> 00:20:00,720
위해 생성하려고 하는

502
00:20:00,720 --> 00:20:03,195
단어와 출력의 단어 중 하나 사이에

503
00:20:03,195 --> 00:20:05,820
일대일 대응이 있을 수 있습니다.

504
00:20:05,820 --> 00:20:07,720
따라서 네트워크는 입력 시퀀스의

505
00:20:07,720 --> 00:20:09,680
단어 중 하나에 상대적으로 높은

506
00:20:09,680 --> 00:20:12,060
주의 가중치를 두고, 나머지 단어들에는

507
00:20:12,060 --> 00:20:14,580
상대적으로 낮은 주의 가중치를 두어야 한다고

508
00:20:14,580 --> 00:20:15,620
예상할 수 있습니다.

509
00:20:15,620 --> 00:20:16,900
하지만 다시 말하지만, 우리는 이를 감독하지 않습니다.

510
00:20:16,900 --> 00:20:18,320
네트워크는 우리의 훈련

511
00:20:18,320 --> 00:20:20,200
작업에 대한 경량 하강법에 의해

512
00:20:20,200 --> 00:20:23,000
이 메커니즘을 어떻게 활용할지 스스로 결정합니다.

513
00:20:23,000 --> 00:20:24,680
이 전체 과정은

514
00:20:24,680 --> 00:20:27,040
디코더 RNN의 각

515
00:20:27,040 --> 00:20:29,600
틱에 대해 반복할 것입니다.

516
00:20:29,600 --> 00:20:32,480
그래서 이제, 이것은 기본적으로 우리의 문제를 해결했습니다.

517
00:20:32,480 --> 00:20:34,860
우리는 더 이상 입력 시퀀스를 단일 고정

518
00:20:34,860 --> 00:20:36,520
길이 벡터로 병목시키지 않습니다.

519
00:20:36,520 --> 00:20:38,180
대신, 우리는 디코더의

520
00:20:38,180 --> 00:20:40,383
각 시간 단계에서 네트워크가

521
00:20:40,383 --> 00:20:42,550
전체 입력 시퀀스를

522
00:20:42,550 --> 00:20:47,060
되돌아보고, 이 디코더의 한 시간 단계에 대해 새로운 컨텍스트

523
00:20:47,060 --> 00:20:49,920
벡터를 즉석에서 생성하는 새로운

524
00:20:49,920 --> 00:20:51,900
메커니즘을 갖게 되었습니다.

525
00:20:51,900 --> 00:20:53,980
그래서 이것은 꽤 멋진 메커니즘입니다.

526
00:20:53,980 --> 00:20:56,380
이것은 네트워크가 출력의 매 순간마다

527
00:20:56,380 --> 00:20:59,260
입력 시퀀스의 다양한 부분을 주목하거나

528
00:20:59,260 --> 00:21:03,108
보고 있기 때문에 주의(attention)라고 불립니다.

529
00:21:03,108 --> 00:21:04,900
우리는 이러한 주의 가중치에 대해 이야기했습니다.

530
00:21:04,900 --> 00:21:06,680
그리고 우리는 네트워크가 훈련

531
00:21:06,680 --> 00:21:08,500
데이터와 훈련 작업에 기반하여

532
00:21:08,500 --> 00:21:10,980
이러한 주의 가중치를 설정하는 방법을

533
00:21:10,980 --> 00:21:12,900
스스로 학습하고 있다고 말했습니다.

534
00:21:12,900 --> 00:21:14,820
주의의 또 다른 정말 멋진 점은

535
00:21:14,820 --> 00:21:17,900
네트워크가 이 문제를 해결하려고 할 때 무엇을 보고 있는지

536
00:21:17,900 --> 00:21:19,580
introspect하고 볼 수 있는

537
00:21:19,580 --> 00:21:21,020
방법을 제공한다는 것입니다.

538
00:21:21,020 --> 00:21:25,100
우리는 입력 시퀀스와 출력 시퀀스 간의 정렬이 무엇인지

539
00:21:25,100 --> 00:21:27,435
결코 알려주지 않았습니다.

540
00:21:27,435 --> 00:21:29,060
하지만 이 작업을 해결하려고

541
00:21:29,060 --> 00:21:32,220
할 때 네트워크가 예측하는 주의 가중치를 보면,

542
00:21:32,220 --> 00:21:35,160
네트워크가 문제를 해결하려고 할 때 무엇을 보고

543
00:21:35,160 --> 00:21:36,840
있었는지 감을 잡을 수 있습니다.

544
00:21:36,840 --> 00:21:38,360
그래서 이것은 신경망의 처리를

545
00:21:38,360 --> 00:21:41,120
어떤 방식으로 해석할 수 있는 방법을 제공합니다.

546
00:21:41,120 --> 00:21:44,640
그래서 우리가 할 수 있는 한 가지는 특정

547
00:21:44,640 --> 00:21:47,680
시퀀스를 처리하는 과정에서, 이 작업을

548
00:21:47,680 --> 00:21:50,000
수행하려고 할 때 네트워크가

549
00:21:50,000 --> 00:21:51,920
예측한 주의 가중치를

550
00:21:51,920 --> 00:21:53,540
살펴보는 것입니다.

551
00:21:53,540 --> 00:21:56,240
그리고 우리는 이를 2차원 그리드로 시각화할 수 있습니다.

552
00:21:56,240 --> 00:21:58,440
여기서 우리는 영어에서

553
00:21:58,440 --> 00:22:01,400
프랑스어 번역의 예를 보고 있습니다.

554
00:22:01,400 --> 00:22:05,780
위쪽에는 입력 시퀀스가 있습니다.

555
00:22:05,780 --> 00:22:07,720
유럽 경제 지역에 대한 합의는

556
00:22:07,720 --> 00:22:09,680
1992년 8월에 서명되었습니다.

557
00:22:09,680 --> 00:22:13,460
그리고 아래로 내려가는 행은 프랑스어로 된 출력

558
00:22:13,460 --> 00:22:16,748
시퀀스입니다. 발음은 시도하지 않겠습니다.

559
00:22:16,748 --> 00:22:19,040
하지만 기본적으로 이 주의

560
00:22:19,040 --> 00:22:20,580
메커니즘을 통해,

561
00:22:20,580 --> 00:22:22,760
매번 네트워크가 출력

562
00:22:22,760 --> 00:22:25,520
시퀀스의 단어 중 하나를 생성할

563
00:22:25,520 --> 00:22:28,040
때마다 전체 입력 시퀀스에

564
00:22:28,040 --> 00:22:30,740
대한 확률 분포를 예측했습니다.

565
00:22:30,740 --> 00:22:33,540
그래서 우리는 첫 번째 행에서 이를 시각화합니다.

566
00:22:33,540 --> 00:22:35,640
이 행렬의 첫 번째 행을 보면,

567
00:22:35,640 --> 00:22:38,540
전체 입력 영어 문장에 대한 예측된

568
00:22:38,540 --> 00:22:40,840
확률 분포를 시각화하고 있습니다.

569
00:22:40,840 --> 00:22:44,580
그리고 프랑스어 문장의 첫 번째 단어인 "le"를 예측할

570
00:22:44,580 --> 00:22:48,100
때, 영어 단어 "the"에 많은 확률 질량을

571
00:22:48,100 --> 00:22:51,580
두고 다른 단어들에는 기본적으로 확률 질량을 두지

572
00:22:51,580 --> 00:22:52,938
않는 것을 봅니다.

573
00:22:52,938 --> 00:22:55,480
그런 다음 출력 시퀀스의 두 번째 단어를 예측할

574
00:22:55,480 --> 00:22:57,730
때, 다시 돌아가서 전체 입력 시퀀스에

575
00:22:57,730 --> 00:22:59,480
대한 새로운 분포를 예측합니다.

576
00:22:59,480 --> 00:23:01,880
그리고 그것이 이 행렬의 두 번째 행이 될 것입니다.

577
00:23:01,880 --> 00:23:05,500
그래서 코드에서, 합의에 많은 확률 질량을 두고

578
00:23:05,500 --> 00:23:07,620
다른 곳에는 확률 질량을

579
00:23:07,620 --> 00:23:09,920
두지 않는 것을 볼 수 있습니다.

580
00:23:09,920 --> 00:23:12,900
그래서 이것은 네트워크가 이 번역 작업을

581
00:23:12,900 --> 00:23:15,220
수행할 때 입력 단어와 출력 단어

582
00:23:15,220 --> 00:23:18,620
간의 정렬을 실제로 파악했다는 감각을 줍니다.

583
00:23:18,620 --> 00:23:21,660
여기에는 흥미로운 패턴이 나타납니다.

584
00:23:21,660 --> 00:23:23,940
우리가 이 주의 행렬에서 대각선

585
00:23:23,940 --> 00:23:25,820
구조를 볼 때, 이는 입력

586
00:23:25,820 --> 00:23:28,760
시퀀스와 출력 시퀀스 간의 단어가 순서대로

587
00:23:28,760 --> 00:23:31,000
일대일 대응이 있었다는 것을

588
00:23:31,000 --> 00:23:31,720
의미합니다.

589
00:23:31,720 --> 00:23:33,960
특히, 우리는 입력 시퀀스의

590
00:23:33,960 --> 00:23:36,840
첫 네 단어인 "the"에 대한 일치가

591
00:23:36,840 --> 00:23:39,840
주의 행렬의 이 대각선 구조에 해당함을

592
00:23:39,840 --> 00:23:40,540
봅니다.

593
00:23:40,540 --> 00:23:43,080
즉, 네트워크가 입력 시퀀스의

594
00:23:43,080 --> 00:23:45,840
첫 네 단어가 정렬되거나

595
00:23:45,840 --> 00:23:49,200
일치하거나 대응한다고 스스로 결정했다는

596
00:23:49,200 --> 00:23:51,280
의미입니다. 마지막 몇

597
00:23:51,280 --> 00:23:53,053
단어도 마찬가지입니다.

598
00:23:53,053 --> 00:23:54,720
다시 말해, 우리는 시퀀스의

599
00:23:54,720 --> 00:23:58,000
끝에서 이 대각선 구조를 보고 있으며, 이는 1992년

600
00:23:58,000 --> 00:24:02,160
8월 또는 1992년 8월이 프랑스어 시퀀스의 마지막 몇 단어에

601
00:24:02,160 --> 00:24:03,580
해당함을 의미합니다.

602
00:24:03,580 --> 00:24:05,622
그리고 다시, 출력의 단어와

603
00:24:05,622 --> 00:24:08,472
입력의 단어 사이에는 일대일 대응이 있습니다.

604
00:24:08,472 --> 00:24:10,680
하지만 여기 중간에서 흥미로운 다른 것들도

605
00:24:10,680 --> 00:24:11,260
보입니다.

606
00:24:11,260 --> 00:24:14,660
중간에는 유럽 경제 지역이 보입니다.

607
00:24:14,660 --> 00:24:17,040
하지만 프랑스어에서는 약간

608
00:24:17,040 --> 00:24:19,977
다른 순서로 보이는 단어들이 있습니다.

609
00:24:19,977 --> 00:24:22,060
좋은 질문입니다. 어떻게 문법을 파악하나요?

610
00:24:22,060 --> 00:24:24,172
그것이 딥러닝의 신비입니다.

611
00:24:24,172 --> 00:24:26,880
기본적으로 우리는 네트워크에 문법에 대한 아무것도

612
00:24:26,880 --> 00:24:27,677
말하지 않았습니다.

613
00:24:27,677 --> 00:24:29,260
우리는 네트워크에 많은

614
00:24:29,260 --> 00:24:30,840
입력-출력 쌍으로 감독했습니다.

615
00:24:30,840 --> 00:24:33,360
우리는 네트워크에 영어로 된 입력 시퀀스를 제공했습니다.

616
00:24:33,360 --> 00:24:34,960
프랑스어로 된 출력 시퀀스를 제공했습니다.

617
00:24:34,960 --> 00:24:37,060
이 입력으로부터 이 출력을

618
00:24:37,060 --> 00:24:40,580
생성하기 위해 이 아키텍처의 가중치를 설정하는 방법을

619
00:24:40,580 --> 00:24:43,180
학습하도록 경량 하강법을 통해

620
00:24:43,180 --> 00:24:45,460
처리하는 메커니즘을 제공했습니다.

621
00:24:45,460 --> 00:24:47,700
우리는 문법에 대한 아무것도 말하지 않았습니다.

622
00:24:47,700 --> 00:24:50,420
하지만 우리는 인간 설계자로서 일부

623
00:24:50,420 --> 00:24:52,540
단어 간에 어떤 대응이 있어야

624
00:24:52,540 --> 00:24:54,832
한다는 직관이 있기 때문에,

625
00:24:54,832 --> 00:24:57,220
이 문제를 해결하는 데 도움이 될

626
00:24:57,220 --> 00:24:59,740
것이라고 생각하는 메커니즘을

627
00:24:59,740 --> 00:25:00,475
내장했습니다.

628
00:25:00,475 --> 00:25:02,100
그리고 네트워크는 엔드 투

629
00:25:02,100 --> 00:25:04,380
엔드 작업을 수행하는 과정에서 그 메커니즘을

630
00:25:04,380 --> 00:25:08,380
활용하여 우리가 설정한 문제를 해결하는 방법을 스스로 알아냅니다.

631
00:25:08,380 --> 00:25:10,050
그리고 그것이 작동한다는 것이 정말 놀랍습니다.

632
00:25:12,700 --> 00:25:16,320
하지만 이 경우, 네트워크는 스스로 일부 문법을 파악했습니다.

633
00:25:16,320 --> 00:25:18,860
그래서 우리는 주의 행렬에서 비대각선,

634
00:25:18,860 --> 00:25:21,320
즉 역대각선을 보고 있습니다.

635
00:25:21,320 --> 00:25:23,112
그것은 네트워크가 영어

636
00:25:23,112 --> 00:25:26,960
단어와 프랑스어 단어 간의 다른 단어 순서를 스스로

637
00:25:26,960 --> 00:25:28,960
알아냈다는 것을 의미합니다.

638
00:25:28,960 --> 00:25:32,920
또는 중간에 여기 작은 2x2 격자가

639
00:25:32,920 --> 00:25:33,500
있습니다.

640
00:25:33,500 --> 00:25:35,978
그리고 그것은 영어 단어와 프랑스어

641
00:25:35,978 --> 00:25:37,520
단어 간에 일대일 대응이

642
00:25:37,520 --> 00:25:39,420
없었던 상황에 해당합니다.

643
00:25:39,420 --> 00:25:41,400
두 개의 프랑스어 단어가 두 개의 영어

644
00:25:41,400 --> 00:25:43,317
단어에 대응했을 수 있으며, 완벽하게

645
00:25:43,317 --> 00:25:44,627
분리되지 않았을 수 있습니다.

646
00:25:44,627 --> 00:25:46,960
즉, 네트워크는 많은 데이터에

647
00:25:46,960 --> 00:25:49,280
대한 훈련 과정에서 스스로 이

648
00:25:49,280 --> 00:25:51,080
모든 것을 알아냅니다.

649
00:25:51,080 --> 00:25:53,360
그리고 그것은 정말 멋집니다.

650
00:25:53,360 --> 00:25:56,240
좋습니다, 실제로-- 그리고 이것이

651
00:25:56,240 --> 00:25:58,760
실제로 기계 학습에서 주의의 초기

652
00:25:58,760 --> 00:25:59,920
사용이었습니다.

653
00:25:59,920 --> 00:26:03,320
사실 이것은 이러한 기계 번역 문제에서 비롯되었습니다.

654
00:26:03,320 --> 00:26:06,600
이것은 2015년의 논문

655
00:26:06,600 --> 00:26:10,720
'신경 기계 번역'에서 나온

656
00:26:10,720 --> 00:26:11,840
것입니다.

657
00:26:11,840 --> 00:26:14,840
이 논문은 ICLR 2025에서 타임 어워드

658
00:26:14,840 --> 00:26:16,720
준우승을 차지했습니다.

659
00:26:16,720 --> 00:26:19,400
그래서 정말 멋집니다.

660
00:26:19,400 --> 00:26:22,440
이 논문은 시간이 지나면서 정말 영향력 있는 논문이었습니다.

661
00:26:22,440 --> 00:26:24,160
하지만 실제로 여기에는 더

662
00:26:24,160 --> 00:26:27,380
일반적인 아이디어와 더 일반적인 연산자가 숨겨져

663
00:26:27,380 --> 00:26:28,140
있습니다.

664
00:26:28,140 --> 00:26:30,300
우리는 순환 신경망을 수정하려는

665
00:26:30,300 --> 00:26:33,040
관점에서 이 문제에 접근합니다.

666
00:26:33,040 --> 00:26:34,860
하지만 우리가 순환 신경망을

667
00:26:34,860 --> 00:26:37,380
수정하기 위해 사용한 메커니즘은

668
00:26:37,380 --> 00:26:39,180
실제로 일반적이고 흥미롭고 그

669
00:26:39,180 --> 00:26:41,300
자체로 매우 강력한 것입니다.

670
00:26:41,300 --> 00:26:43,860
그래서 이제 우리는 그 아이디어, 즉

671
00:26:43,860 --> 00:26:47,300
주의(attention) 개념을 끌어내고 순환

672
00:26:47,300 --> 00:26:49,280
신경망과 분리하고자 합니다.

673
00:26:49,280 --> 00:26:51,100
그리고 주의는 그

674
00:26:51,100 --> 00:26:53,580
자체로 신경망에 매우 유용하고

675
00:26:53,580 --> 00:26:56,520
강력한 계산 원시가 될 것이며,

676
00:26:56,520 --> 00:26:59,820
순환 신경망 부분을 제거하고 주의만을

677
00:26:59,820 --> 00:27:01,780
아키텍처의 핵심 원시로

678
00:27:01,780 --> 00:27:03,600
남길 수 있습니다.

679
00:27:03,600 --> 00:27:06,380
그리고 그것이 우리가 나아가고자 하는 방향입니다.

680
00:27:06,380 --> 00:27:09,400
그래서 이제 우리가 하고자 하는 것은 순환

681
00:27:09,400 --> 00:27:11,240
신경망에서 보았던 주의

682
00:27:11,240 --> 00:27:13,300
개념을 일반화하고 독립적으로

683
00:27:13,300 --> 00:27:16,700
사용할 수 있는 연산자를 조각내는 것입니다.

684
00:27:16,700 --> 00:27:19,940
그럼 이 주의 메커니즘이 무엇을 하고 있었는지 생각해 봅시다.

685
00:27:19,940 --> 00:27:22,540
기본적으로 이 주의 메커니즘은 여러

686
00:27:22,540 --> 00:27:24,900
개의 쿼리 벡터가 있었습니다.

687
00:27:24,900 --> 00:27:26,995
이들은-- 아마도 다른 순서로 이야기하는

688
00:27:26,995 --> 00:27:28,620
것이 더 의미가 있을 것입니다.

689
00:27:28,620 --> 00:27:30,640
그래서 데이터 벡터가 있습니다. 이는 우리가

690
00:27:30,640 --> 00:27:32,100
요약하고자 하는 데이터와 같습니다.

691
00:27:32,100 --> 00:27:35,660
이들은 인코더 RNN의 인코더 상태입니다.

692
00:27:35,660 --> 00:27:37,180
우리는 이 입력 시퀀스를 가지고 있습니다.

693
00:27:37,180 --> 00:27:39,640
그리고 우리는 그것을 벡터 시퀀스로 요약했습니다.

694
00:27:39,640 --> 00:27:41,372
이 벡터 시퀀스는 우리가

695
00:27:41,372 --> 00:27:43,080
해결하고자 하는 문제와 관련이

696
00:27:43,080 --> 00:27:44,680
있다고 생각하는 데이터입니다.

697
00:27:44,680 --> 00:27:48,060
이제 그 데이터를 활용하는 과정에서 우리는 여러

698
00:27:48,060 --> 00:27:50,060
개의 출력을 생성하고자 합니다.

699
00:27:50,060 --> 00:27:52,420
각 출력에 대해 쿼리 벡터가 있습니다.

700
00:27:52,420 --> 00:27:54,240
쿼리 벡터는 우리가 어떤

701
00:27:54,240 --> 00:27:56,000
출력을 생성하기 위해

702
00:27:56,000 --> 00:27:57,760
사용하려는 벡터입니다.

703
00:27:57,760 --> 00:27:59,640
이 경우 쿼리 벡터는

704
00:27:59,640 --> 00:28:03,480
디코더 RNN의 은닉 상태입니다.

705
00:28:03,480 --> 00:28:07,000
각 쿼리 벡터에 대해 데이터를 다시

706
00:28:07,000 --> 00:28:09,460
살펴보고 데이터 벡터의

707
00:28:09,460 --> 00:28:12,360
정보를 요약하여 각 컨텍스트

708
00:28:12,360 --> 00:28:16,760
벡터를 생성하고자 합니다. 주의 메커니즘의

709
00:28:16,760 --> 00:28:19,452
목적에서 보면, 이건 조금

710
00:28:19,452 --> 00:28:20,660
이상해집니다.

711
00:28:20,660 --> 00:28:23,100
주의 연산자의 출력은 우리가 방금

712
00:28:23,100 --> 00:28:25,900
이야기한 RNN의 컨텍스트 벡터입니다.

713
00:28:25,900 --> 00:28:27,820
주의 연산자가 하는

714
00:28:27,820 --> 00:28:30,100
일을 생각해보면, 주의

715
00:28:30,100 --> 00:28:33,060
연산자의 출력은 RNN에 공급하는

716
00:28:33,060 --> 00:28:34,540
컨텍스트 벡터입니다.

717
00:28:34,540 --> 00:28:36,800
그렇다면 주의 연산자는 무엇을 하고 있나요?

718
00:28:36,800 --> 00:28:39,740
주의 연산자는 쿼리 벡터를 사용하여 입력

719
00:28:39,740 --> 00:28:42,340
데이터 벡터로 돌아가고, 데이터를 새로운

720
00:28:42,340 --> 00:28:45,500
방식으로 요약하여 출력 벡터를 생성합니다.

721
00:28:45,500 --> 00:28:48,560
그게 바로 주의 연산자가 하는 일입니다.

722
00:28:48,560 --> 00:28:51,420
우리가 방금 본 주의 메커니즘의

723
00:28:51,420 --> 00:28:53,712
일반화로서 이해가 되나요?

724
00:28:53,712 --> 00:28:55,442
[듣기 어려움] 네, 다시

725
00:28:55,442 --> 00:28:57,400
반복할게요. 이건 복잡합니다.

726
00:28:57,400 --> 00:28:59,240
여기에는 많은 것들이 오가고 있고, 많은 박스들이 있습니다.

727
00:28:59,240 --> 00:29:00,865
우리가 박스를 정의하는 데 사용하는

728
00:29:00,865 --> 00:29:02,240
단어들이 바뀌고 있습니다.

729
00:29:02,240 --> 00:29:03,820
그래서 이해합니다. 많은 일이 일어나고 있습니다.

730
00:29:03,820 --> 00:29:06,180
주의 연산자가 하는 일은

731
00:29:06,180 --> 00:29:07,980
인코더의 은닉 상태인 데이터

732
00:29:07,980 --> 00:29:09,740
벡터들이 있습니다.

733
00:29:09,740 --> 00:29:11,740
그런 다음 우리는 출력을

734
00:29:11,740 --> 00:29:15,580
생성하려고 하는 쿼리 벡터들이 있습니다.

735
00:29:15,580 --> 00:29:18,083
이제 쿼리 벡터를 처리하는

736
00:29:18,083 --> 00:29:19,500
과정에서 데이터 벡터로

737
00:29:19,500 --> 00:29:22,920
돌아가 각 쿼리 벡터에 대해 데이터를

738
00:29:22,920 --> 00:29:27,040
새로운 방식으로 요약하고, 그 결과로 출력 벡터를

739
00:29:27,040 --> 00:29:29,520
생성하여 RNN의 다음 단계에

740
00:29:29,520 --> 00:29:31,800
공급할 컨텍스트를 만듭니다.

741
00:29:31,800 --> 00:29:34,140
그래서 우리의 쿼리 벡터는 이 초록색 친구들입니다.

742
00:29:34,140 --> 00:29:36,540
각 쿼리 벡터에 대해 데이터 벡터로

743
00:29:36,540 --> 00:29:38,480
돌아가 데이터를 요약한 다음,

744
00:29:38,480 --> 00:29:40,920
나머지 네트워크에 공급할 컨텍스트

745
00:29:40,920 --> 00:29:43,640
중 하나인 새로운 출력 벡터를 생성합니다.

746
00:29:43,640 --> 00:29:45,920
이 아키텍처에 들어가서

747
00:29:45,920 --> 00:29:49,640
주의 부분을 조심스럽게 잘라내는 것이기

748
00:29:49,640 --> 00:29:52,640
때문에 이것은 까다롭습니다.

749
00:29:52,640 --> 00:29:55,320
그래서 우리는 주의 연산자 관점에서

750
00:29:55,320 --> 00:29:57,832
다시 이 과정을 살펴보려고 합니다.

751
00:29:57,832 --> 00:30:00,663
주의 연산자 관점에서 처음에는

752
00:30:00,663 --> 00:30:02,080
RNN의

753
00:30:02,080 --> 00:30:06,920
상태 중 하나인 쿼리 벡터 하나로 시작합니다.

754
00:30:06,920 --> 00:30:08,700
우리는 또한 RNN의 인코더

755
00:30:08,700 --> 00:30:12,160
은닉 상태인 여러 데이터 벡터를 가지고 있습니다.

756
00:30:12,160 --> 00:30:14,120
이제 우리가 수행하고자 하는

757
00:30:14,120 --> 00:30:17,520
계산은 먼저 쿼리 벡터와 모든 데이터 벡터 간의 유사성을

758
00:30:17,520 --> 00:30:18,740
계산하는 것입니다.

759
00:30:18,740 --> 00:30:20,020
이것은 우리가 방금 본 것과 정확히

760
00:30:20,020 --> 00:30:21,820
동일한 내용으로, 다른 방식으로 작성된 것입니다.

761
00:30:21,820 --> 00:30:26,660
우리는 이 FATT 함수를 사용하여 각 데이터 벡터와 우리의

762
00:30:26,660 --> 00:30:30,820
쿼리 벡터 간의 유사성을 계산하기 위한 유사성

763
00:30:30,820 --> 00:30:32,100
점수를 계산합니다.

764
00:30:32,100 --> 00:30:33,760
그런 다음 이러한 유사성을

765
00:30:33,760 --> 00:30:35,660
얻으면 소프트맥스를 통해 압축하여

766
00:30:35,660 --> 00:30:37,080
주의 가중치를 얻습니다.

767
00:30:37,080 --> 00:30:39,300
이것은 이 쿼리 벡터에 대해

768
00:30:39,300 --> 00:30:43,420
즉석에서 계산된 데이터 벡터에 대한 분포입니다.

769
00:30:43,420 --> 00:30:46,520
그런 다음 우리는 출력 벡터를 생성하고자 합니다.

770
00:30:46,520 --> 00:30:50,160
이 출력 벡터는 우리가 방금 계산한 주의 점수인

771
00:30:50,160 --> 00:30:53,060
선형 조합 가중치를 가진 데이터 벡터의

772
00:30:53,060 --> 00:30:54,200
선형 조합입니다.

773
00:30:54,200 --> 00:30:56,620
이것이 주의 레이어의 출력입니다.

774
00:30:56,620 --> 00:30:59,340
그리고 우리가 본 더 큰 RNN의

775
00:30:59,340 --> 00:31:02,300
맥락에서 주의 레이어의 출력 또는 주의 연산자는

776
00:31:02,300 --> 00:31:05,940
디코더 RNN의 다음 단계에 대한 입력이 됩니다.

777
00:31:05,940 --> 00:31:07,720
하지만 우리는 RNN을 사용하지 않으려 하므로 그에

778
00:31:07,720 --> 00:31:08,760
대해 이야기하고 싶지 않습니다.

779
00:31:08,760 --> 00:31:10,020
우리는 단지 주의(attention)에

780
00:31:10,020 --> 00:31:12,500
대해 이야기하고 주의 레이어 내부에서 발생하는

781
00:31:12,500 --> 00:31:13,620
계산에 집중하고 싶습니다.

782
00:31:13,620 --> 00:31:18,160
그래서 이것은 기본적으로 우리가 RNN에서 본 연산자입니다.

783
00:31:18,160 --> 00:31:20,000
우리는 쿼리 벡터를 가져와

784
00:31:20,000 --> 00:31:21,440
유사도 점수를

785
00:31:21,440 --> 00:31:23,320
계산하고, 주의 가중치를

786
00:31:23,320 --> 00:31:26,060
얻고, 출력 벡터를 얻는 과정을

787
00:31:26,060 --> 00:31:27,128
반복했습니다.

788
00:31:27,128 --> 00:31:28,420
그런 다음 새로운 쿼리 벡터를 얻었습니다.

789
00:31:28,420 --> 00:31:30,003
그 쿼리 벡터는 어디서 왔나요?

790
00:31:30,003 --> 00:31:31,500
주의 연산자는 신경 쓰지 않습니다.

791
00:31:31,500 --> 00:31:34,380
새로운 쿼리 벡터를 가져와서 데이터를 요약하고 새로운

792
00:31:34,380 --> 00:31:35,680
출력 벡터를 얻으세요.

793
00:31:35,680 --> 00:31:38,120
그리고 그것이 주의 연산자의 핵심입니다.

794
00:31:38,120 --> 00:31:39,640
이제 이것을 일반화하여

795
00:31:39,640 --> 00:31:43,000
더 강력한 계산 원시로 만들어 보겠습니다.

796
00:31:43,000 --> 00:31:46,117
네, 원칙적으로 이 FATT는 반드시 특정해야 할 필요는 없으며,

797
00:31:46,117 --> 00:31:47,700
어떤 함수일 수도 있습니다.

798
00:31:47,700 --> 00:31:49,533
원칙적으로 두 벡터의 함수로

799
00:31:49,533 --> 00:31:50,898
스칼라를 출력할 수 있습니다.

800
00:31:50,898 --> 00:31:52,440
하지만 실제로는 몇

801
00:31:52,440 --> 00:31:55,022
슬라이드 후에 더 간단하게 만들 것입니다.

802
00:31:55,022 --> 00:31:57,480
하지만 원칙적으로는, 원하는

803
00:31:57,480 --> 00:32:00,310
어떤 함수도 그곳에 넣을 수 있습니다.

804
00:32:00,310 --> 00:32:02,560
좋습니다, 우리가 할 첫 번째 일반화는,

805
00:32:02,560 --> 00:32:04,640
사실, 당신이 방금 제안한 것의

806
00:32:04,640 --> 00:32:07,800
반대이며 그 유사도 함수를 더 간단하게 만드는 것입니다.

807
00:32:07,800 --> 00:32:09,520
그래서 우리는 원칙적으로 두 벡터를

808
00:32:09,520 --> 00:32:11,040
가져와 유사도 점수를 주는

809
00:32:11,040 --> 00:32:12,640
어떤 함수일 수 있다고 말했습니다.

810
00:32:12,640 --> 00:32:15,520
두 개의 벡터를 입력받아 스칼라 유사도 점수를

811
00:32:15,520 --> 00:32:17,620
주는 가장 간단한 함수는 무엇인가요?

812
00:32:17,620 --> 00:32:18,800
점곱입니다.

813
00:32:18,800 --> 00:32:21,420
그래서 우리는 사물을 더 간단하게 만들고

814
00:32:21,420 --> 00:32:23,122
동시에 일반화하려고 합니다.

815
00:32:23,122 --> 00:32:24,580
점곱이 이 목적에

816
00:32:24,580 --> 00:32:26,380
사용하기에 충분한 유사도

817
00:32:26,380 --> 00:32:27,960
점수라는 것이 밝혀졌습니다.

818
00:32:27,960 --> 00:32:29,460
그래서 우리가 할 첫

819
00:32:29,460 --> 00:32:34,340
번째 일은 사실 점곱만 사용하여 유사도를 계산하는 것입니다.

820
00:32:34,340 --> 00:32:36,180
하지만 점곱에는 약간의

821
00:32:36,180 --> 00:32:37,460
문제가 있습니다.

822
00:32:37,460 --> 00:32:39,700
이 문제는 점곱과 소프트맥스

823
00:32:39,700 --> 00:32:42,980
간의 이상한 상호작용 때문에 미묘합니다.

824
00:32:42,980 --> 00:32:45,580
이것은 벡터의 차원이 증가하거나

825
00:32:45,580 --> 00:32:48,960
감소할 때 발생하는 일과 관련이 있습니다.

826
00:32:48,960 --> 00:32:50,900
예를 들어, 차원이

827
00:32:50,900 --> 00:32:52,820
10인 모든 1로

828
00:32:52,820 --> 00:32:55,100
이루어진 상수 벡터와

829
00:32:55,100 --> 00:32:57,020
차원이 100인 모든

830
00:32:57,020 --> 00:33:00,300
1로 이루어진 상수 벡터를

831
00:33:00,300 --> 00:33:03,660
비교하면, 차원이 더 높은 벡터로 갈수록

832
00:33:03,660 --> 00:33:05,860
소프트맥스 내부의

833
00:33:05,860 --> 00:33:08,260
합을 계산할 때 더 큰

834
00:33:08,260 --> 00:33:10,280
수로 나누게 됩니다.

835
00:33:10,280 --> 00:33:13,180
그래서 차원이 더 높은 벡터로 갈수록 더

836
00:33:13,180 --> 00:33:15,430
압축된 확률 점수를 얻게 됩니다.

837
00:33:15,430 --> 00:33:17,010
이것은 이전 강의에서 본 것처럼

838
00:33:17,010 --> 00:33:18,593
기울기 소실로 이어질 수 있으며,

839
00:33:18,593 --> 00:33:20,470
이 전체 학습을 방해할 수 있습니다.

840
00:33:20,470 --> 00:33:26,230
그래서 이를 방지하고 이 아키텍처를 다양한 차원 벡터에 대해 더 일반화

841
00:33:26,230 --> 00:33:28,390
가능하고 확장 가능하게

842
00:33:28,390 --> 00:33:30,990
만들기 위해, 우리는 실제로

843
00:33:30,990 --> 00:33:33,370
순수한 점곱을 사용하지 않고

844
00:33:33,370 --> 00:33:35,990
우리가 보고 있는 벡터의 차원의

845
00:33:35,990 --> 00:33:38,702
제곱근으로 점곱을 축소할 것입니다.

846
00:33:38,702 --> 00:33:40,910
이것은 기울기 소실을 방지하고 더 넓은

847
00:33:40,910 --> 00:33:44,110
범위의 벡터 차원에 대해 소프트맥스를 통해 더 나은 기울기

848
00:33:44,110 --> 00:33:45,805
흐름을 제공하는 방법입니다.

849
00:33:45,805 --> 00:33:47,430
이것은 매우 중요합니다. 왜냐하면

850
00:33:47,430 --> 00:33:49,830
시간이 지남에 따라 이러한 네트워크를 점점 더 크게

851
00:33:49,830 --> 00:33:52,590
만들 때, 더 높은 차원의 벡터를 얻고 싶기 때문입니다. 이는

852
00:33:52,590 --> 00:33:54,692
더 많은 계산과 더 많은 용량을 제공합니다.

853
00:33:54,692 --> 00:33:57,150
그래서 우리는 항상 아키텍처가 어떻게

854
00:33:57,150 --> 00:33:59,790
확장될지를 생각해야 합니다. 아키텍처의

855
00:33:59,790 --> 00:34:01,870
부분이 점점 더 커질수록 말이죠.

856
00:34:01,870 --> 00:34:04,390
그래서 이 축소된 점곱은 기울기

857
00:34:04,390 --> 00:34:07,065
소실을 방지하는 데 정말 중요합니다.

858
00:34:07,065 --> 00:34:09,190
질문은 데이터와 쿼리 벡터의 크기가 같아야

859
00:34:09,190 --> 00:34:12,670
한다는 제한이 있지만, 우리는 이를 실제로 수정할 것입니다.

860
00:34:12,670 --> 00:34:15,010
그래서 우리의 첫 번째 일반화는 실제로

861
00:34:15,010 --> 00:34:19,210
축소된 점곱 유사도를 유사도 측정으로 사용하는 것이었습니다.

862
00:34:19,210 --> 00:34:21,830
이제 이 것들의 형태를 다시 살펴보면, 우리는 차원

863
00:34:21,830 --> 00:34:24,010
Dq의 쿼리 벡터 하나를 가지고 있습니다.

864
00:34:24,010 --> 00:34:25,730
데이터 벡터는 차원 Nx

865
00:34:25,730 --> 00:34:30,250
by Dq를 가지고 있습니다. 점곱이기 때문에 일치해야 합니다.

866
00:34:30,250 --> 00:34:32,850
하지만 우리가 할 다음 일반화는

867
00:34:32,850 --> 00:34:36,070
여러 개의 쿼리 벡터를 갖는 것입니다.

868
00:34:36,070 --> 00:34:39,590
우리는 한 번에 하나의 쿼리 벡터만 처리하고 싶지 않을 수도

869
00:34:39,590 --> 00:34:41,770
있으며, 모든 쿼리 벡터 집합을 한

870
00:34:41,770 --> 00:34:43,850
번에 처리할 수 있는 능력을 원합니다.

871
00:34:43,850 --> 00:34:45,570
이것은 RNN에서 발생합니다.

872
00:34:45,570 --> 00:34:47,540
우리는 여러 개의 쿼리 벡터를 갖게 되었습니다.

873
00:34:47,540 --> 00:34:49,290
그리고 주의 연산자가

874
00:34:49,290 --> 00:34:52,190
한 번에 하나의 쿼리 벡터가 아니라,

875
00:34:52,190 --> 00:34:55,710
기본적으로 모든 쿼리 벡터 집합을

876
00:34:55,710 --> 00:34:58,330
병렬로 처리하고 동일한 계산을

877
00:34:58,330 --> 00:35:00,570
수행하는 것이 유용합니다.

878
00:35:00,570 --> 00:35:02,950
그래서 이 경우, 우리는 이제 N을 일반화했습니다.

879
00:35:02,950 --> 00:35:06,290
Q는 이제 Nq by Dq 형태의 행렬입니다.

880
00:35:06,290 --> 00:35:08,190
그래서 우리는 Nq 쿼리 벡터를 가지고 있습니다.

881
00:35:08,190 --> 00:35:10,450
각 쿼리 벡터는 차원 Dq를 가지고 있습니다.

882
00:35:10,450 --> 00:35:14,670
우리의 데이터 벡터는 Nx by Dq 크기의 행렬입니다.

883
00:35:14,670 --> 00:35:18,150
이제 계산이 약간 변경됩니다. 왜냐하면

884
00:35:18,150 --> 00:35:21,550
이제 이러한 정렬 점수를 계산할

885
00:35:21,550 --> 00:35:23,390
때, 모든 입력 데이터

886
00:35:23,390 --> 00:35:27,350
벡터와 모든 입력 쿼리 벡터 간의

887
00:35:27,350 --> 00:35:31,410
모든 쌍의 유사도를 계산하고 싶기 때문입니다.

888
00:35:31,410 --> 00:35:33,510
각 유사도는 점곱

889
00:35:33,510 --> 00:35:36,170
또는 축소된 점곱입니다.

890
00:35:36,170 --> 00:35:38,750
그렇다면 두 개의 입력 벡터 집합 간의

891
00:35:38,750 --> 00:35:41,430
내적을 계산하는 매우 효율적이고 쉽고

892
00:35:41,430 --> 00:35:43,050
자연스러운 방법은 무엇일까요?

893
00:35:43,050 --> 00:35:45,368
그것은 행렬 곱셈입니다.

894
00:35:45,368 --> 00:35:47,410
행렬 곱셈을 할 때, 출력

895
00:35:47,410 --> 00:35:50,390
행렬의 각 항목은 하나의 행렬의 열과

896
00:35:50,390 --> 00:35:51,990
다른 행렬의 행의 내적이라는

897
00:35:51,990 --> 00:35:53,570
것을 기억하세요.

898
00:35:53,570 --> 00:35:56,230
그래서 행렬 곱셈의 출력에서

899
00:35:56,230 --> 00:35:59,070
각 항목은 출력의 행과 열

900
00:35:59,070 --> 00:36:00,990
간의 내적입니다.

901
00:36:00,990 --> 00:36:03,470
쿼리 벡터 Q와 데이터 벡터

902
00:36:03,470 --> 00:36:07,283
X 간의 행렬 곱셈을 계산하고, 행과 열이

903
00:36:07,283 --> 00:36:08,950
올바르게 맞도록 전치

904
00:36:08,950 --> 00:36:11,650
행렬을 사용해야 합니다. 이것은

905
00:36:11,650 --> 00:36:12,970
기본적으로

906
00:36:12,970 --> 00:36:15,850
모든 데이터 벡터와 모든 쿼리 벡터

907
00:36:15,850 --> 00:36:17,490
간의 유사성을 한

908
00:36:17,490 --> 00:36:21,940
번의 간단한 행렬 곱셈으로 계산할 수 있게 해줍니다.

909
00:36:21,940 --> 00:36:24,190
이제 우리는 여전히 이러한 주의 가중치를 계산해야 합니다.

910
00:36:24,190 --> 00:36:25,648
주의 가중치는 각 쿼리

911
00:36:25,648 --> 00:36:27,570
벡터에 대해 계산하고자 합니다.

912
00:36:27,570 --> 00:36:29,530
데이터 벡터에 대한 분포를 계산하고자

913
00:36:29,530 --> 00:36:30,350
합니다.

914
00:36:30,350 --> 00:36:31,585
우리는 이미 이러한 유사성

915
00:36:31,585 --> 00:36:33,210
점수를 가지고 있습니다. 이제

916
00:36:33,210 --> 00:36:34,970
우리의 유사성 점수는 단일 점수 벡터가

917
00:36:34,970 --> 00:36:37,470
아니라 모든 유사성을 제공하는 점수 행렬입니다.

918
00:36:37,470 --> 00:36:40,170
하지만 우리는 여전히 각 쿼리 벡터에 대해 데이터

919
00:36:40,170 --> 00:36:42,907
벡터에 대한 분포를 독립적으로 계산하고자 합니다.

920
00:36:42,907 --> 00:36:44,490
따라서 이제 유사성

921
00:36:44,490 --> 00:36:48,370
점수 행렬의 한 축에 대해 소프트맥스를 계산해야

922
00:36:48,370 --> 00:36:49,070
합니다.

923
00:36:49,070 --> 00:36:50,610
이것은 우리가 방금 본 것과 정확히

924
00:36:50,610 --> 00:36:51,430
같은 계산입니다.

925
00:36:51,430 --> 00:36:54,050
우리는 쿼리 벡터 집합에 대해 동시에

926
00:36:54,050 --> 00:36:55,610
병렬로 수행하고 있습니다.

927
00:36:55,610 --> 00:36:57,870
이제 출력 벡터를 계산해야 합니다.

928
00:36:57,870 --> 00:36:59,930
출력 벡터는 데이터

929
00:36:59,930 --> 00:37:05,350
벡터의 가중 조합이 될 것이며, 그

930
00:37:05,350 --> 00:37:08,890
가중치는 소프트맥스의 값입니다.

931
00:37:08,890 --> 00:37:10,990
그리고 이것도 행렬 곱셈이

932
00:37:10,990 --> 00:37:12,710
수행하는 것입니다.

933
00:37:12,710 --> 00:37:14,830
행렬 곱셈을 생각하는 또 다른

934
00:37:14,830 --> 00:37:17,930
방법은 두 행렬의 곱셈을 할 때, 행렬 곱셈을

935
00:37:17,930 --> 00:37:20,630
보는 다른 방법은 선형 조합을 취하는

936
00:37:20,630 --> 00:37:22,395
것인데, 아, 내가

937
00:37:22,395 --> 00:37:24,770
행과 열을 올바르게 맞출 수 있을까?

938
00:37:24,770 --> 00:37:26,710
하지만 나는 입력 행렬 중

939
00:37:26,710 --> 00:37:29,390
하나의 열의 선형 조합을 다른 입력

940
00:37:29,390 --> 00:37:33,010
행렬의 값으로 가중치를 두어 얻는다고 생각해.

941
00:37:33,010 --> 00:37:34,470
그래서 이것은 행렬 곱셈에

942
00:37:34,470 --> 00:37:36,010
대한 또 다른 해석이다.

943
00:37:36,010 --> 00:37:38,000
그럼 인덱스를 통해 작업하고

944
00:37:38,000 --> 00:37:39,750
스스로 무언가가

945
00:37:39,750 --> 00:37:41,810
어떻게 진행되는지 증명하기

946
00:37:41,810 --> 00:37:45,950
위해 작은 그림을 그리면, 이제 우리가 하고자 하는

947
00:37:45,950 --> 00:37:48,510
것은 데이터 벡터의 많은 선형

948
00:37:48,510 --> 00:37:51,150
조합을 계산하는 것인데, 각 선형

949
00:37:51,150 --> 00:37:54,750
조합은 주의 행렬의 한 행에 있는 확률에

950
00:37:54,750 --> 00:37:55,790
의해 주어진다.

951
00:37:55,790 --> 00:37:59,230
그래서 우리는 주의 행렬 A와 데이터 벡터 X 간의

952
00:37:59,230 --> 00:38:03,130
또 다른 행렬 곱셈으로 모두 한 번에 계산할 수 있다.

953
00:38:03,130 --> 00:38:05,630
그리고 다시, 이것이 잘 작동하도록 전치 행렬을

954
00:38:05,630 --> 00:38:07,002
올바른 순서로 맞춰야 한다.

955
00:38:07,002 --> 00:38:09,710
하지만 기본적으로, 이것은 우리가 방금 본 것과 정확히 동일한

956
00:38:09,710 --> 00:38:12,590
작업인데, 이제 우리는 쿼리 벡터 집합에 대해 한 번에 수행하고 있다.

957
00:38:12,590 --> 00:38:15,330
그리고 우리는 단지 몇 개의 행렬 곱셈으로 모두

958
00:38:15,330 --> 00:38:17,730
한 번에 수행할 수 있다는 것이 밝혀졌다.

959
00:38:17,730 --> 00:38:19,850
우리가 이를 일반화할 다음

960
00:38:19,850 --> 00:38:23,650
방법은 이 방정식에서 데이터 벡터 X가 실제로 이

961
00:38:23,650 --> 00:38:26,610
계산에서 두 가지 다른 위치에 들어간다는

962
00:38:26,610 --> 00:38:28,222
것을 주목하는 것이다.

963
00:38:28,222 --> 00:38:29,930
우리가 데이터 벡터 X를

964
00:38:29,930 --> 00:38:32,170
사용하는 첫 번째 위치는 유사성

965
00:38:32,170 --> 00:38:35,710
계산에서 쿼리 벡터와의 유사성을 계산하는 것이다.

966
00:38:35,710 --> 00:38:38,250
그런 의미에서, 우리가 하려는 것은

967
00:38:38,250 --> 00:38:40,428
데이터 벡터가 각 쿼리 벡터와

968
00:38:40,428 --> 00:38:41,970
얼마나 일치하는지를 내적에

969
00:38:41,970 --> 00:38:43,610
의해 측정하는 것이다.

970
00:38:43,610 --> 00:38:46,090
하지만 우리는 데이터 벡터를 다시 사용하여

971
00:38:46,090 --> 00:38:47,750
출력 벡터를 계산하고 있다.

972
00:38:47,750 --> 00:38:51,010
그래서 출력 벡터는 이제 우리의 주의 가중치로

973
00:38:51,010 --> 00:38:54,370
가중치가 부여된 데이터 벡터의 선형 조합이다.

974
00:38:54,370 --> 00:38:57,330
그래서 두 가지 다른 맥락에서 데이터 벡터를 재사용하는

975
00:38:57,330 --> 00:38:59,650
것이 조금 이상하게 보일 수 있다.

976
00:38:59,650 --> 00:39:03,330
그래서 이제 우리가 하려는 것은 데이터 벡터의 두

977
00:39:03,330 --> 00:39:07,390
가지 사용을 분리하고 네트워크가 그 두 가지 맥락에서

978
00:39:07,390 --> 00:39:09,990
데이터 벡터를 사용하는 두 가지

979
00:39:09,990 --> 00:39:12,830
방법을 스스로 알아내도록 하는 것이다.

980
00:39:12,830 --> 00:39:16,773
그래서 이를 위해 우리는 키와 쿼리라는 개념을 도입할 것이다.

981
00:39:16,773 --> 00:39:18,190
이제 우리가 할

982
00:39:18,190 --> 00:39:21,710
일은 데이터 벡터 집합이 있었고, 각

983
00:39:21,710 --> 00:39:24,430
데이터 벡터를 두 개의 벡터로

984
00:39:24,430 --> 00:39:26,250
투영하는 것입니다.

985
00:39:26,250 --> 00:39:27,530
하나는 키 벡터입니다.

986
00:39:27,530 --> 00:39:29,390
하나는 값 벡터입니다.

987
00:39:29,390 --> 00:39:32,630
키 벡터의 아이디어는 키 벡터가 쿼리

988
00:39:32,630 --> 00:39:34,830
벡터와 비교되어 정렬 점수를

989
00:39:34,830 --> 00:39:36,410
계산하는 것입니다.

990
00:39:36,410 --> 00:39:38,190
값 벡터는 레이어에서

991
00:39:38,190 --> 00:39:40,870
출력을 계산하기 위해

992
00:39:40,870 --> 00:39:43,510
선형 조합을 계산할 것입니다.

993
00:39:43,510 --> 00:39:46,430
이것은 또한-- 그래서 우리가 이를

994
00:39:46,430 --> 00:39:49,470
구현하는 방법은 두 개의 학습 가능한 가중치

995
00:39:49,470 --> 00:39:51,070
행렬, 키 행렬과

996
00:39:51,070 --> 00:39:54,510
값 행렬을 추가하는 것입니다. 이들은 입력을

997
00:39:54,510 --> 00:39:57,310
키 벡터와 값 벡터로 투영하는 선형

998
00:39:57,310 --> 00:39:58,170
투영입니다.

999
00:39:58,170 --> 00:40:00,430
이제 데이터 벡터를 기억하세요, 우리는 N개의

1000
00:40:00,430 --> 00:40:02,990
데이터 벡터가 있고, 각 벡터의 차원은 Dx입니다.

1001
00:40:02,990 --> 00:40:06,570
이제 키 행렬은 Dx에서 Dq로 투영하는

1002
00:40:06,570 --> 00:40:10,090
선형 변환입니다. 왜냐하면 우리는 키 벡터를

1003
00:40:10,090 --> 00:40:12,090
쿼리 벡터와 비교할 것이기

1004
00:40:12,090 --> 00:40:13,148
때문입니다.

1005
00:40:13,148 --> 00:40:15,690
그래서 쿼리 벡터와 같은 차원을 가져야 합니다.

1006
00:40:15,690 --> 00:40:17,090
그래서 K =

1007
00:40:17,090 --> 00:40:19,850
XWk의 행렬 곱셈을

1008
00:40:19,850 --> 00:40:24,170
적용하면 각 데이터 벡터를 차원 Dq의 키 벡터로

1009
00:40:24,170 --> 00:40:25,690
투영합니다.

1010
00:40:25,690 --> 00:40:28,370
그런 다음 Dx에서 Dv로 투영하는

1011
00:40:28,370 --> 00:40:31,210
또 다른 가중치 행렬이 따로 있을 것이며,

1012
00:40:31,210 --> 00:40:33,490
이는 값 벡터의 차원으로,

1013
00:40:33,490 --> 00:40:36,970
원칙적으로 쿼리 벡터의 차원과 다를 수 있습니다.

1014
00:40:36,970 --> 00:40:40,130
그리고 각 데이터 벡터를 값 벡터로 다시

1015
00:40:40,130 --> 00:40:44,690
투영할 것입니다. 여기서도 행렬 곱셈 연산자를 사용합니다.

1016
00:40:44,690 --> 00:40:48,210
여기서의 직관은 검색 엔진에서 내가 찾고 있는 것과

1017
00:40:48,210 --> 00:40:51,210
그 쿼리에 대한 응답으로 원하는 답변을

1018
00:40:51,210 --> 00:40:52,970
분리하고자 하는 것입니다.

1019
00:40:52,970 --> 00:40:56,410
그래서 구글에 가거나 요즘은 ChatGPT에 가서 '세계에서

1020
00:40:56,410 --> 00:40:58,790
가장 좋은 학교는 무엇인가?'라고 입력합니다.

1021
00:40:58,790 --> 00:40:59,790
그게 당신의 쿼리입니다.

1022
00:40:59,790 --> 00:41:01,380
그리고 당신이 받는 값은--

1023
00:41:01,380 --> 00:41:03,130
그건 백엔드에서 키와

1024
00:41:03,130 --> 00:41:04,390
결합해야 하는 쿼리입니다.

1025
00:41:04,390 --> 00:41:06,070
하지만 그 값, 즉 그 쿼리에서

1026
00:41:06,070 --> 00:41:07,750
얻고자 하는 데이터는

1027
00:41:07,750 --> 00:41:09,870
실제로 당신이 입력한 쿼리와 다릅니다.

1028
00:41:09,870 --> 00:41:13,350
그래서 우리는 '세계에서 가장 좋은 학교는 무엇인가?'라는 쿼리를 입력하는
이

1029
00:41:13,350 --> 00:41:14,770
아이디어를 분리하고자 합니다.

1030
00:41:14,770 --> 00:41:17,270
그 쿼리는 인터넷의 다양한 문자열과

1031
00:41:17,270 --> 00:41:18,445
일치해야 합니다.

1032
00:41:18,445 --> 00:41:20,070
그리고 그 쿼리에서

1033
00:41:20,070 --> 00:41:23,098
얻고자 하는 값은 스탠포드로,

1034
00:41:23,098 --> 00:41:25,390
이는 입력한 쿼리와는 다른

1035
00:41:25,390 --> 00:41:26,330
값입니다.

1036
00:41:26,330 --> 00:41:27,530
그래서 그게 직관입니다.

1037
00:41:27,530 --> 00:41:29,310
키와 쿼리, 값을 이렇게

1038
00:41:29,310 --> 00:41:32,470
분리하는 또 다른 직관은 쿼리가 내가

1039
00:41:32,470 --> 00:41:33,870
찾고 있는 것입니다.

1040
00:41:33,870 --> 00:41:37,670
키는-- 백엔드에서 데이터 벡터에 있는 모든

1041
00:41:37,670 --> 00:41:39,570
데이터의 기록이 있습니다.

1042
00:41:39,570 --> 00:41:42,357
하지만 우리가 쿼리할 때, 우리는 데이터

1043
00:41:42,357 --> 00:41:44,190
벡터의 일부와 일치시키고자

1044
00:41:44,190 --> 00:41:45,172
합니다.

1045
00:41:45,172 --> 00:41:47,630
그리고 데이터 벡터에서 얻고자 하는

1046
00:41:47,630 --> 00:41:48,870
것은 값입니다.

1047
00:41:48,870 --> 00:41:51,230
그래서 우리는 데이터 벡터의 사용을

1048
00:41:51,230 --> 00:41:55,070
키와 값이라는 두 가지 다른 개념으로 분리하고 있습니다.

1049
00:41:55,070 --> 00:41:57,470
그런 다음 우리는 이를 다른 방식으로 시각화할 수 있습니다.

1050
00:41:57,470 --> 00:42:00,250
이제 우리는 마침내 RNN을 버리고

1051
00:42:00,250 --> 00:42:03,070
주의를 단독 연산자로 보고 있습니다.

1052
00:42:03,070 --> 00:42:05,087
그래서 우리는 이 작업을 다시 단계별로 진행할 수 있습니다.

1053
00:42:05,087 --> 00:42:06,670
쿼리 벡터가 들어오고 있습니다.

1054
00:42:06,670 --> 00:42:08,650
데이터 벡터가 들어오고 있습니다.

1055
00:42:08,650 --> 00:42:11,030
이제 우리는 데이터 벡터에서

1056
00:42:11,030 --> 00:42:15,250
각 데이터 벡터를 키와 값으로 투영할 것입니다.

1057
00:42:15,250 --> 00:42:18,490
그럼 각 키를 각 쿼리와 비교하여

1058
00:42:18,490 --> 00:42:21,388
유사도 점수를 얻겠습니다.

1059
00:42:21,388 --> 00:42:23,930
이것은 유사도 행렬로, 각

1060
00:42:23,930 --> 00:42:27,290
키와 각 쿼리 간의 유사도를 제공합니다.

1061
00:42:27,290 --> 00:42:30,050
이 유사도 점수 행렬을 얻은

1062
00:42:30,050 --> 00:42:33,490
후, 각 쿼리에 대한 데이터 벡터의

1063
00:42:33,490 --> 00:42:36,450
분포를 계산하고자 합니다.

1064
00:42:36,450 --> 00:42:38,570
즉, 이 정렬 점수 행렬에

1065
00:42:38,570 --> 00:42:41,690
대해 소프트맥스를 실행해야 하며, 각

1066
00:42:41,690 --> 00:42:44,610
행에 대해 소프트맥스를 계산합니다.

1067
00:42:44,610 --> 00:42:49,370
그 다음, 소프트맥스의 주의 점수로 값 벡터의

1068
00:42:49,370 --> 00:42:52,625
가중치를 조정하고자 합니다.

1069
00:42:52,625 --> 00:42:54,250
아, 사실, 죄송합니다.

1070
00:42:54,250 --> 00:42:56,930
우리는 각 열이 분포가 되기를

1071
00:42:56,930 --> 00:42:57,730
원합니다.

1072
00:42:57,730 --> 00:43:02,677
각 쿼리에 대해 키에 대한 분포를 원하기 때문입니다.

1073
00:43:02,677 --> 00:43:04,510
즉, 열에 대해 소프트맥스를

1074
00:43:04,510 --> 00:43:07,190
원하며, 열에 맞춰 정렬되기를 원합니다.

1075
00:43:07,190 --> 00:43:10,130
그럼 이제 쿼리 하나가 있습니다.

1076
00:43:10,130 --> 00:43:12,230
이 계산에서 모든

1077
00:43:12,230 --> 00:43:15,692
키에 대한 분포를 예측했습니다.

1078
00:43:15,692 --> 00:43:18,150
그런 다음, 이 주의 가중치로

1079
00:43:18,150 --> 00:43:19,822
가중치가 조정된 값의

1080
00:43:19,822 --> 00:43:22,030
선형 조합을 취하여 첫

1081
00:43:22,030 --> 00:43:24,670
번째 출력 벡터 Y1을 생성합니다.

1082
00:43:24,670 --> 00:43:26,490
그리고 여기에서도 같은 일이 발생합니다.

1083
00:43:26,490 --> 00:43:28,650
두 번째 쿼리가 모든 키와 비교되었습니다.

1084
00:43:28,650 --> 00:43:31,030
정렬 점수에 대한 분포를

1085
00:43:31,030 --> 00:43:32,910
계산하여 두 번째 쿼리에

1086
00:43:32,910 --> 00:43:35,890
대한 키의 분포를 얻었고, 이를

1087
00:43:35,890 --> 00:43:39,910
선형 결합하여 값을 선형 결합하여 출력 벡터를

1088
00:43:39,910 --> 00:43:40,950
생성합니다.

1089
00:43:40,950 --> 00:43:43,710
이제 이것은 순환 신경망과 분리된 상태에서

1090
00:43:43,710 --> 00:43:46,467
독립적으로 서 있는 주의 연산자입니다.

1091
00:43:46,467 --> 00:43:48,550
질문은 데이터 벡터를 키와 값으로 어떻게

1092
00:43:48,550 --> 00:43:49,610
나누는가입니다.

1093
00:43:49,610 --> 00:43:53,470
아름다운 점은 우리가 방법을 말할 필요가 없다는 것입니다.

1094
00:43:53,470 --> 00:43:56,830
우리는 신경망이 스스로 분할할

1095
00:43:56,830 --> 00:43:59,470
수 있는 능력을 주기 위해

1096
00:43:59,470 --> 00:44:02,910
이 메커니즘을 제공할 뿐입니다.

1097
00:44:02,910 --> 00:44:05,238
하지만 우리는 그것이 어떻게 해야 하는지 말하지 않을 것입니다.

1098
00:44:05,238 --> 00:44:07,030
키 행렬과 값 행렬은 모두

1099
00:44:07,030 --> 00:44:08,210
다른 것들과

1100
00:44:08,210 --> 00:44:09,930
함께 경량 하강법을 통해

1101
00:44:09,930 --> 00:44:12,347
학습될 모델의 학습 가능한 매개변수입니다.

1102
00:44:12,347 --> 00:44:12,870
학습될 모델의 학습 가능한 매개변수입니다.

1103
00:44:12,870 --> 00:44:14,850
영어와 프랑스어 문장을 정렬하는

1104
00:44:14,850 --> 00:44:17,330
방법을 말하지 않았던 것처럼,

1105
00:44:17,330 --> 00:44:19,110
모든 것은 경량 하강법을

1106
00:44:19,110 --> 00:44:22,130
통해 학습되었고, 모델은 문제를 해결하는 데

1107
00:44:22,130 --> 00:44:24,170
유용한 방식으로 키와 값으로

1108
00:44:24,170 --> 00:44:25,370
별도로 프로젝션하는

1109
00:44:25,370 --> 00:44:27,575
방법을 스스로 학습할 것입니다.

1110
00:44:27,575 --> 00:44:29,450
키와 값은 일종의 필터로 생각할

1111
00:44:29,450 --> 00:44:30,470
수 있습니다.

1112
00:44:30,470 --> 00:44:32,873
데이터 벡터에는 많은 정보가 포함될 수 있습니다.

1113
00:44:32,873 --> 00:44:34,290
하지만 현재 작업을 위해

1114
00:44:34,290 --> 00:44:36,250
데이터 벡터를 다양한 방식으로 필터링하고

1115
00:44:36,250 --> 00:44:38,670
쿼리의 일부와만 일치시키고자 할 수 있습니다.

1116
00:44:38,670 --> 00:44:40,290
우리는 다른 부분의 정보를 검색하는

1117
00:44:40,290 --> 00:44:41,390
것에만 관심이 있습니다.

1118
00:44:41,390 --> 00:44:42,932
따라서 이는 데이터 벡터의

1119
00:44:42,932 --> 00:44:44,570
정보를 두 가지 다른

1120
00:44:44,570 --> 00:44:47,770
방식으로 필터링하는 것으로 생각할 수 있습니다.

1121
00:44:47,770 --> 00:44:50,710
좋습니다, 이것이 기본적으로 우리의 주의 연산자입니다.

1122
00:44:50,710 --> 00:44:52,632
이제 여기에 RNN은 없습니다.

1123
00:44:52,632 --> 00:44:54,090
이것은 독립적으로 서

1124
00:44:54,090 --> 00:44:56,030
있을 수 있는 신경망 레이어입니다.

1125
00:44:56,030 --> 00:44:58,690
쿼리 벡터와 데이터 벡터의 두

1126
00:44:58,690 --> 00:44:59,850
입력을 받습니다.

1127
00:44:59,850 --> 00:45:01,690
학습 가능한 매개변수인 두 개의

1128
00:45:01,690 --> 00:45:03,830
가중치, 즉 키 행렬과 값 행렬이 있습니다.

1129
00:45:03,830 --> 00:45:06,710
두 개의 벡터 시퀀스를 입력하고 벡터

1130
00:45:06,710 --> 00:45:08,250
시퀀스를 출력합니다.

1131
00:45:08,250 --> 00:45:10,342
그래서 이것은 신경망

1132
00:45:10,342 --> 00:45:12,550
아키텍처의 여러 곳에 연결할 수

1133
00:45:12,550 --> 00:45:14,830
있는 신경망 레이어입니다.

1134
00:45:14,830 --> 00:45:17,230
이것은 두 세트의 입력이 들어오기

1135
00:45:17,230 --> 00:45:20,010
때문에 때때로 교차 주의 레이어라고 불립니다.

1136
00:45:20,010 --> 00:45:23,450
아이디어는 데이터 벡터와 쿼리 벡터를 모두 가지고 있다는 것입니다.

1137
00:45:23,450 --> 00:45:25,870
이들은 잠재적으로 두 개의 다른 출처에서 올 수 있습니다.

1138
00:45:25,870 --> 00:45:27,510
그리고 이것은 때때로 유용합니다.

1139
00:45:27,510 --> 00:45:29,470
그래서 나는 쿼리 집합을 가지고 있습니다.

1140
00:45:29,470 --> 00:45:31,870
각 쿼리에 대해, 나는 내 데이터에서 정보를

1141
00:45:31,870 --> 00:45:34,038
요약하고 싶습니다. 이는 잠재적으로 다르거나,

1142
00:45:34,038 --> 00:45:35,830
다른 수치이거나, 내 쿼리 벡터와

1143
00:45:35,830 --> 00:45:37,270
완전히 다를 수 있습니다.

1144
00:45:37,270 --> 00:45:39,950
그래서 이것은 두 개의 서로 다른 것들

1145
00:45:39,950 --> 00:45:41,910
사이에서 교차 주의를 하기 때문에

1146
00:45:41,910 --> 00:45:44,732
때때로 교차 주의 레이어라고 불립니다.

1147
00:45:44,732 --> 00:45:46,190
하지만 더 일반적으로

1148
00:45:46,190 --> 00:45:48,790
발생하는 또 다른 버전은 자기

1149
00:45:48,790 --> 00:45:50,350
주의 레이어입니다.

1150
00:45:50,350 --> 00:45:52,430
여기서 우리가 할 것은, 우리는 단 하나의

1151
00:45:52,430 --> 00:45:53,970
세트의 것만 가지고 있습니다.

1152
00:45:53,970 --> 00:45:55,890
우리는 단 하나의 입력 시퀀스만 가지고 있습니다.

1153
00:45:55,890 --> 00:45:58,650
우리는 처리할 하나의 벡터 집합, 하나의 벡터

1154
00:45:58,650 --> 00:45:59,970
시퀀스를 가지고 있습니다.

1155
00:45:59,970 --> 00:46:02,290
그리고 이제 우리는 데이터 벡터와 쿼리

1156
00:46:02,290 --> 00:46:04,310
벡터 간의 분리가 더 이상 없습니다.

1157
00:46:04,310 --> 00:46:06,690
우리는 처리하고 싶은 하나의 입력 벡터

1158
00:46:06,690 --> 00:46:08,170
집합만 가지고 있습니다.

1159
00:46:08,170 --> 00:46:10,450
그래서 자기 주의 레이어에서는,

1160
00:46:10,450 --> 00:46:13,110
우리는 하나의 입력 벡터 집합을 가지고,

1161
00:46:13,110 --> 00:46:15,470
출력 벡터 집합을 생성할 것입니다.

1162
00:46:15,470 --> 00:46:18,290
우리는 벡터 x 집합을 입력하고, 입력

1163
00:46:18,290 --> 00:46:21,670
벡터와 같은 수의 벡터 y 집합을 출력하고자 합니다.

1164
00:46:21,670 --> 00:46:23,650
하지만 이제 이 메커니즘은 기본적으로

1165
00:46:23,650 --> 00:46:26,050
우리가 방금 본 동일한 주의 메커니즘입니다.

1166
00:46:26,050 --> 00:46:28,147
하지만 이제는 투영하는 대신, 여전히

1167
00:46:28,147 --> 00:46:30,230
필터링 개념을 사용할 것입니다.

1168
00:46:30,230 --> 00:46:32,370
하지만 이제는 이전에 했던

1169
00:46:32,370 --> 00:46:35,050
것처럼 데이터 벡터를 키와 쿼리로

1170
00:46:35,050 --> 00:46:37,810
투영하는 대신, 각 입력 벡터를 세

1171
00:46:37,810 --> 00:46:40,730
가지 다른 것으로 투영할 것입니다.

1172
00:46:40,730 --> 00:46:42,410
각 입력 벡터에서

1173
00:46:42,410 --> 00:46:46,370
우리는 쿼리, 키 및 값으로 투영할 것입니다.

1174
00:46:46,370 --> 00:46:49,730
그리고 방정식은 약간 변경되지만, 여기

1175
00:46:49,730 --> 00:46:51,530
그림은 실제로 크게

1176
00:46:51,530 --> 00:46:52,690
변하지 않습니다.

1177
00:46:52,690 --> 00:46:54,770
각 입력 벡터에 대해 우리는

1178
00:46:54,770 --> 00:46:58,390
쿼리, 키 및 값으로 별도로 투영합니다.

1179
00:46:58,390 --> 00:47:01,530
그리고 이제 우리는 정확히 동일한 계산을 가지고 있습니다.

1180
00:47:01,530 --> 00:47:03,910
이제 우리는 쿼리, 키, 값을 가지고 있습니다.

1181
00:47:03,910 --> 00:47:06,250
여기서 일어나는 모든 것의 관점에서, 모든

1182
00:47:06,250 --> 00:47:07,090
것이 동일합니다.

1183
00:47:07,090 --> 00:47:09,790
우리가 동일한 입력 벡터의

1184
00:47:09,790 --> 00:47:12,030
서로 다른 선형 투영에서

1185
00:47:12,030 --> 00:47:14,950
키와 쿼리 및 값을 계산했을

1186
00:47:14,950 --> 00:47:15,730
뿐입니다.

1187
00:47:15,730 --> 00:47:18,430
하지만 모든 계산은 다른 방식으로 공유됩니다.

1188
00:47:18,430 --> 00:47:21,250
네, 질문은 D in과 D out이 무엇인가요?

1189
00:47:21,250 --> 00:47:22,060
그들은 어떻게 크기가 조정되나요?

1190
00:47:22,060 --> 00:47:24,310
그래서 이것들은 레이어의 아키텍처

1191
00:47:24,310 --> 00:47:25,490
하이퍼파라미터가 될 것입니다.

1192
00:47:25,490 --> 00:47:28,390
모델에 학습 가능한 선형 레이어가 있을 때, 선형 레이어는

1193
00:47:28,390 --> 00:47:30,865
기본적으로 D in에서 D out으로 투영합니다.

1194
00:47:30,865 --> 00:47:32,990
이것들은 레이어의 아키텍처 하이퍼파라미터가

1195
00:47:32,990 --> 00:47:33,913
될 것입니다.

1196
00:47:33,913 --> 00:47:36,330
자기 주의 레이어에서도 마찬가지로 D in과 D

1197
00:47:36,330 --> 00:47:38,372
out은 레이어의 아키텍처 하이퍼파라미터가

1198
00:47:38,372 --> 00:47:39,350
될 것입니다.

1199
00:47:39,350 --> 00:47:41,830
원칙적으로, 그들은 다를 수 있습니다.

1200
00:47:41,830 --> 00:47:44,530
이 아키텍처에는 충분한 유연성이 있어서,

1201
00:47:44,530 --> 00:47:48,370
원칙적으로 D in과 D out이 다를 수 있습니다.

1202
00:47:48,370 --> 00:47:50,550
하지만 제가 거의 본 적이 없다고 생각합니다.

1203
00:47:50,550 --> 00:47:52,750
실제로는 거의 항상 동일합니다.

1204
00:47:52,750 --> 00:47:55,250
그래서 여기서 표기법을 조금

1205
00:47:55,250 --> 00:47:57,890
더 일반적으로 사용했습니다.

1206
00:47:57,890 --> 00:47:59,650
좋아요, 그래서 우리가 반드시 이 내용을 자세히

1207
00:47:59,650 --> 00:48:00,750
살펴볼 필요는 없다고 생각합니다.

1208
00:48:00,750 --> 00:48:02,500
사실, 중요한 점이 하나 있습니다.

1209
00:48:02,500 --> 00:48:05,530
그래서 우리는 입력을 쿼리, 키, 값으로 별도로

1210
00:48:05,530 --> 00:48:07,530
프로젝션한다고 말했습니다.

1211
00:48:07,530 --> 00:48:10,450
이는 세 개의 학습 가능한 가중치 행렬을 사용하여 세 번의

1212
00:48:10,450 --> 00:48:11,870
행렬 곱셈을 통해 이루어집니다.

1213
00:48:11,870 --> 00:48:14,390
이제 우리는 키, 값, 쿼리 각각에 대해 하나씩의

1214
00:48:14,390 --> 00:48:16,210
학습 가능한 가중치 행렬이 있습니다.

1215
00:48:16,210 --> 00:48:19,370
그리고 입력 벡터 X를 키, 쿼리, 값으로

1216
00:48:19,370 --> 00:48:21,450
별도로 프로젝션합니다.

1217
00:48:21,450 --> 00:48:23,850
하지만 실제로는 일반적으로

1218
00:48:23,850 --> 00:48:26,428
하나의 행렬 곱셈으로 모두 계산할

1219
00:48:26,428 --> 00:48:28,470
수 있습니다. 하드웨어에서

1220
00:48:28,470 --> 00:48:32,250
더 큰 행렬 곱셈을 적게 하는 것이 더

1221
00:48:32,250 --> 00:48:33,790
효율적이기 때문입니다.

1222
00:48:33,790 --> 00:48:36,130
그래서 실제로 꽤 일반적인 트릭은

1223
00:48:36,130 --> 00:48:40,170
이 세 개의 행렬을 차원에 따라 연결하여 모든 입력

1224
00:48:40,170 --> 00:48:42,388
벡터에 대한 키, 쿼리,

1225
00:48:42,388 --> 00:48:43,930
값을 한 번에 큰 행렬

1226
00:48:43,930 --> 00:48:46,130
곱셈으로 계산하는 것입니다.

1227
00:48:46,130 --> 00:48:47,870
이전에 변환기를 읽어본 적이

1228
00:48:47,870 --> 00:48:50,610
있다면, 때때로 인코더와 디코더 변환기 또는

1229
00:48:50,610 --> 00:48:53,270
인코더-디코더 주의(attention)를 구분합니다.

1230
00:48:53,270 --> 00:48:56,870
그래서 그 경우, 이것은 변환기 논문을 읽어본 적이 있다면

1231
00:48:56,870 --> 00:48:59,150
디코더 전용 주의가 될 것입니다.

1232
00:48:59,150 --> 00:49:01,390
그리고 이는 수업 시작 부분의

1233
00:49:01,390 --> 00:49:03,670
RNN 디코더에서 사용되는

1234
00:49:03,670 --> 00:49:05,430
방식에 해당합니다.

1235
00:49:05,430 --> 00:49:08,360
하지만 이 메커니즘은 실제로 현재 가장

1236
00:49:08,360 --> 00:49:10,110
일반적으로 사용되는

1237
00:49:10,110 --> 00:49:12,030
주의의 형태는 이 소위 디코더

1238
00:49:12,030 --> 00:49:13,590
전용 주의입니다.

1239
00:49:13,590 --> 00:49:16,730
그래서 우리는 이제 RNN과 상당히 분리되고 있습니다.

1240
00:49:16,730 --> 00:49:18,670
이 형태는 수업 시작 부분에서

1241
00:49:18,670 --> 00:49:20,630
본 RNN에서 사용되기에는

1242
00:49:20,630 --> 00:49:22,088
그다지 의미가 없습니다.

1243
00:49:22,088 --> 00:49:24,630
그래서 우리는 기본적으로 여기서 약간의

1244
00:49:24,630 --> 00:49:26,910
속임수를 사용하고 있습니다.

1245
00:49:26,910 --> 00:49:29,830
이 아키텍처를 RNN의 특정 사례인 기계

1246
00:49:29,830 --> 00:49:31,610
번역을 위해 도입했습니다.

1247
00:49:31,610 --> 00:49:33,670
하지만 이제 우리는 이를 완전히 독립적으로

1248
00:49:33,670 --> 00:49:36,370
사용할 수 있는 전혀 다른 연산자로 일반화했습니다.

1249
00:49:36,370 --> 00:49:38,030
그리고 이 자기

1250
00:49:38,030 --> 00:49:40,030
주의(self-attention)로의 특정 일반화에서는 더

1251
00:49:40,030 --> 00:49:42,350
이상 RNN의 디코더에서 사용할 수 없습니다.

1252
00:49:42,350 --> 00:49:44,110
하지만 이는 실제로 다른 많은

1253
00:49:44,110 --> 00:49:46,462
곳에서 사용되는 매우 유용한 원시 요소입니다.

1254
00:49:46,462 --> 00:49:48,670
좋아요, 질문은 자기 주의와 교차

1255
00:49:48,670 --> 00:49:50,950
주의의 이점이나 차이가 무엇인가입니다.

1256
00:49:50,950 --> 00:49:52,730
그들은 서로 다른 맥락에서 사용됩니다.

1257
00:49:52,730 --> 00:49:55,330
어떤 상황에서는 비교하고 싶은 두 가지

1258
00:49:55,330 --> 00:49:58,427
다른 종류의 데이터가 자연스럽게 존재합니다.

1259
00:49:58,427 --> 00:50:01,010
예를 들어 기계 번역 설정에서 보았습니다.

1260
00:50:01,010 --> 00:50:02,225
우리는 입력 문장이 있습니다.

1261
00:50:02,225 --> 00:50:03,350
우리는 출력 문장이 있습니다.

1262
00:50:03,350 --> 00:50:05,250
문제에 자연스러운 구조가 있다고

1263
00:50:05,250 --> 00:50:07,625
믿습니다. 비교하고 싶은 두 가지

1264
00:50:07,625 --> 00:50:08,890
다른 집합이 있습니다.

1265
00:50:08,890 --> 00:50:11,230
이는 이미지 캡셔닝에서도 발생할 수 있습니다.

1266
00:50:11,230 --> 00:50:13,027
예를 들어, 입력 이미지가 있고

1267
00:50:13,027 --> 00:50:14,610
출력 문장을 생성하고 싶다면,

1268
00:50:14,610 --> 00:50:16,693
비교하고 싶은 두 가지 다른 종류의 것이

1269
00:50:16,693 --> 00:50:18,730
있습니다. 이미지의 조각과 우리가 생성하는

1270
00:50:18,730 --> 00:50:19,790
단어의 토큰입니다.

1271
00:50:19,790 --> 00:50:22,210
그래서 어떤 문제에서는 두 가지 다른

1272
00:50:22,210 --> 00:50:25,250
종류의 것이 떠다니는 자연스러운 구조가 있습니다.

1273
00:50:25,250 --> 00:50:27,790
하지만 다른 문제의 경우, 두 가지 종류의 것이 없습니다.

1274
00:50:27,790 --> 00:50:29,090
그냥 하나의 것만 있습니다.

1275
00:50:29,090 --> 00:50:30,905
예를 들어, 이미지 분류를 하고 있다면,

1276
00:50:30,905 --> 00:50:32,030
오직 이미지만 있습니다.

1277
00:50:32,030 --> 00:50:33,430
우리는 이미지를 처리하고 싶습니다.

1278
00:50:33,430 --> 00:50:35,847
따라서 그런 경우, 우리는 이미지의 부분을 자기 자신과 비교하고

1279
00:50:35,847 --> 00:50:36,350
싶습니다.

1280
00:50:36,350 --> 00:50:38,600
그때 자기 주의 레이어를 사용하게 됩니다.

1281
00:50:38,600 --> 00:50:41,913
그들은 단지 다양한 종류의 문제에 사용됩니다.

1282
00:50:41,913 --> 00:50:43,330
하지만 우리는 --

1283
00:50:43,330 --> 00:50:45,530
하지만 중요한 것은, 우리는 기본적으로

1284
00:50:45,530 --> 00:50:48,110
같은 기계와 같은 계산 원리를 다양한

1285
00:50:48,110 --> 00:50:50,110
문제에 재사용하고 싶습니다.

1286
00:50:50,110 --> 00:50:52,310
그리고 그것은 정말 유익합니다.

1287
00:50:52,310 --> 00:50:54,750
주의에 대해 흥미로운 몇

1288
00:50:54,750 --> 00:50:55,950
가지가 있습니다.

1289
00:50:55,950 --> 00:50:57,990
하나의 경우는 입력을 섞으면 어떤

1290
00:50:57,990 --> 00:50:59,810
일이 발생하는지 고려해 보겠습니다.

1291
00:50:59,810 --> 00:51:01,268
우리는 입력 벡터 집합이 있었고,

1292
00:51:01,268 --> 00:51:03,102
그것들을 섞어서 다른 순서로 처리하면

1293
00:51:03,102 --> 00:51:03,990
어떻게 될까요?

1294
00:51:03,990 --> 00:51:06,770
실제로 많은 흥미로운 일이 발생합니다.

1295
00:51:06,770 --> 00:51:08,630
키, 쿼리 및 값은 모두

1296
00:51:08,630 --> 00:51:10,630
입력의 선형 투영으로 계산되기

1297
00:51:10,630 --> 00:51:12,970
때문에 결국 동일하게 됩니다.

1298
00:51:12,970 --> 00:51:15,650
그래서 우리는 같은 키, 쿼리 및 값을 얻게 됩니다.

1299
00:51:15,650 --> 00:51:18,233
그들은 단지 다른 순서로, 입력이 섞인 것과 같은

1300
00:51:18,233 --> 00:51:19,230
방식으로 섞일 것입니다.

1301
00:51:19,230 --> 00:51:21,230
이제 우리의 유사도 점수가 단순한

1302
00:51:21,230 --> 00:51:23,950
내적이었기 때문에, 입력을 섞는 방식에

1303
00:51:23,950 --> 00:51:25,550
따라 다시 섞인 동일한

1304
00:51:25,550 --> 00:51:27,390
유사도 점수를 얻게 됩니다.

1305
00:51:27,390 --> 00:51:28,730
소프트맥스도 마찬가지입니다.

1306
00:51:28,730 --> 00:51:31,650
소프트맥스는 실제로 입력의 순서에 신경 쓰지 않습니다.

1307
00:51:31,650 --> 00:51:34,810
그래서 소프트맥스는 이제 같은 벡터에서 작동하지만 섞여 있습니다.

1308
00:51:34,810 --> 00:51:37,030
따라서 우리의 주의 가중치의 각

1309
00:51:37,030 --> 00:51:39,110
열은 이전과 동일하게, 단지 섞여서

1310
00:51:39,110 --> 00:51:41,890
나타납니다. 선형 조합도 마찬가지입니다.

1311
00:51:41,890 --> 00:51:45,430
따라서 우리의 출력 Y는 실제로 이전과 동일한

1312
00:51:45,430 --> 00:51:46,530
출력을 유지합니다.

1313
00:51:46,530 --> 00:51:47,835
모두 섞여 있을 뿐입니다.

1314
00:51:47,835 --> 00:51:50,210
이는 순열 동등성이라는 매우

1315
00:51:50,210 --> 00:51:53,390
흥미로운 구조가 있음을 의미합니다.

1316
00:51:53,390 --> 00:51:57,615
몇 강의 전에 컨볼루션에서 이것을 보았던 것을 기억하세요.

1317
00:51:57,615 --> 00:51:59,490
이제 우리는 이러한 자기 주의

1318
00:51:59,490 --> 00:52:01,630
레이어의 다른 동등성 속성을 봅니다.

1319
00:52:01,630 --> 00:52:03,772
입력을 섞으면

1320
00:52:03,772 --> 00:52:05,730
출력도 동일하게

1321
00:52:05,730 --> 00:52:09,170
섞인 출력을 얻게 됩니다.

1322
00:52:09,170 --> 00:52:11,270
이는 이 경우 자기 주의가

1323
00:52:11,270 --> 00:52:13,530
실제로 입력의 순서에 신경 쓰지

1324
00:52:13,530 --> 00:52:15,568
않는다는 것을 의미합니다.

1325
00:52:15,568 --> 00:52:17,110
입력의 순서를 변경하면 동일한

1326
00:52:17,110 --> 00:52:19,650
출력을 얻게 되며, 단지 같은 방식으로 섞입니다.

1327
00:52:19,650 --> 00:52:21,330
레이어의 계산은 입력을

1328
00:52:21,330 --> 00:52:24,010
제시하는 순서에 의존하지 않습니다.

1329
00:52:24,010 --> 00:52:27,090
따라서 우리는 자기 주의를 실제로 벡터의 시퀀스에서

1330
00:52:27,090 --> 00:52:29,590
작동하는 것으로 생각하지 않을 수 있습니다.

1331
00:52:29,590 --> 00:52:32,632
그들은 우연히 행렬의 정렬된 시퀀스로 포장되어 있습니다.

1332
00:52:32,632 --> 00:52:34,090
하지만 우리는 실제로

1333
00:52:34,090 --> 00:52:36,750
그것을 무작위 벡터 집합에서 작동하는 것으로

1334
00:52:36,750 --> 00:52:40,050
생각합니다. 왜냐하면 우리가 얻는 출력은 입력 행렬에

1335
00:52:40,050 --> 00:52:43,010
벡터를 어떤 순서로 배치했는지에 따라 달라지지

1336
00:52:43,010 --> 00:52:43,850
않기 때문입니다.

1337
00:52:43,850 --> 00:52:45,677
그래서 우리는 이것을 벡터의

1338
00:52:45,677 --> 00:52:48,010
시퀀스가 아닌 벡터 집합에서 근본적으로

1339
00:52:48,010 --> 00:52:50,430
작동하는 일종의 다른 신경망

1340
00:52:50,430 --> 00:52:51,553
원시로 생각합니다.

1341
00:52:51,553 --> 00:52:52,970
하지만 이것은 때때로 문제가 됩니다.

1342
00:52:52,970 --> 00:52:55,150
때때로 신경망에 시퀀스의 항목

1343
00:52:55,150 --> 00:52:57,590
순서를 알려주는 것이 유용합니다.

1344
00:52:57,590 --> 00:52:59,350
그래서 이를 빠르게

1345
00:52:59,350 --> 00:53:01,550
해결하기 위해, 우리는 때때로 각

1346
00:53:01,550 --> 00:53:04,650
입력 벡터에 위치 임베딩이라고 불리는

1347
00:53:04,650 --> 00:53:06,758
추가 데이터를 연결합니다.

1348
00:53:06,758 --> 00:53:09,050
이는 신경망에 이 벡터가 인덱스

1349
00:53:09,050 --> 00:53:11,030
1에 있다는 것을 알려주는

1350
00:53:11,030 --> 00:53:11,988
데이터입니다.

1351
00:53:11,988 --> 00:53:14,230
그리고 이를 위한 다양한 메커니즘이 있습니다.

1352
00:53:14,230 --> 00:53:18,652
질문은 동일한 결과로 훈련될 것인가입니다.

1353
00:53:18,652 --> 00:53:20,610
여기서 훈련에 대해 이야기하는 것은 아닙니다.

1354
00:53:20,610 --> 00:53:23,030
나는 가중치 행렬을 고정하고

1355
00:53:23,030 --> 00:53:25,070
레이어의 계산만 고려할 때,

1356
00:53:25,070 --> 00:53:27,050
입력을 섞으면 동일한 출력을

1357
00:53:27,050 --> 00:53:29,670
받지만 입력이 섞인 방식으로

1358
00:53:29,670 --> 00:53:32,130
섞인다는 것을 이야기하고 있습니다.

1359
00:53:32,130 --> 00:53:36,070
출력에서 어떤 벡터를 계산하는지는

1360
00:53:36,070 --> 00:53:40,787
입력의 벡터 순서에 의존하지 않습니다.

1361
00:53:40,787 --> 00:53:42,870
하지만 출력에서 얻는 벡터의

1362
00:53:42,870 --> 00:53:44,287
순서는 입력에서 제시된

1363
00:53:44,287 --> 00:53:45,652
순서에 의존합니다.

1364
00:53:45,652 --> 00:53:47,110
자기 주의와 관련하여 우리가 할

1365
00:53:47,110 --> 00:53:48,735
수 있는 몇 가지 다른 트릭이

1366
00:53:48,735 --> 00:53:50,810
있지만, 이를 조금 더 빠르게 진행하겠습니다.

1367
00:53:50,810 --> 00:53:53,950
때때로 전체 자기 주의 레이어에서는 입력의 모든

1368
00:53:53,950 --> 00:53:55,730
조각이 입력의 다른 모든

1369
00:53:55,730 --> 00:53:57,790
조각을 볼 수 있도록 허용했습니다.

1370
00:53:57,790 --> 00:53:59,490
하지만 일부 문제에서는

1371
00:53:59,490 --> 00:54:02,050
이 계산에 구조를 부여하고

1372
00:54:02,050 --> 00:54:03,717
특정 입력 조각이

1373
00:54:03,717 --> 00:54:05,717
특정 다른 입력 조각만

1374
00:54:05,717 --> 00:54:07,330
볼 수 있도록

1375
00:54:07,330 --> 00:54:09,550
하고 싶을 수 있습니다.

1376
00:54:09,550 --> 00:54:11,210
그리고 우리는 이를 마스크된 자기

1377
00:54:11,210 --> 00:54:13,010
주의라는 개념을 통해 구현할 수 있습니다.

1378
00:54:13,010 --> 00:54:15,090
우리가 할 것은 이러한 정렬

1379
00:54:15,090 --> 00:54:17,370
점수 E를 계산한 후, 주의를

1380
00:54:17,370 --> 00:54:19,330
차단하고 싶은 곳에

1381
00:54:19,330 --> 00:54:20,930
음의 무한대로 정렬 점수를

1382
00:54:20,930 --> 00:54:22,663
덮어쓰는 것입니다.

1383
00:54:22,663 --> 00:54:24,330
이제 정렬 점수에 음의

1384
00:54:24,330 --> 00:54:26,832
무한대가 있으면 소프트맥스를

1385
00:54:26,832 --> 00:54:29,290
수행한 후 소프트맥스 계산을 통해

1386
00:54:29,290 --> 00:54:30,370
0이 됩니다.

1387
00:54:30,370 --> 00:54:32,090
즉, 정렬 점수에

1388
00:54:32,090 --> 00:54:33,590
음의 무한대가

1389
00:54:33,590 --> 00:54:35,810
있을 때마다 소프트맥스

1390
00:54:35,810 --> 00:54:38,350
점수에서 0이 됩니다.

1391
00:54:38,350 --> 00:54:40,170
이는 출력 Y가 해당 인덱스에서

1392
00:54:40,170 --> 00:54:43,170
계산된 값 벡터에 의존하지 않음을 의미합니다.

1393
00:54:43,170 --> 00:54:46,630
따라서 이는 우리가 계산 과정에서 어떤 입력이 서로

1394
00:54:46,630 --> 00:54:50,340
상호작용할 수 있는지를 제어할 수 있게 해주는 메커니즘입니다.

1395
00:54:50,340 --> 00:54:52,590
우리는 이제 언어 모델링을 위해 이를 수행하고 싶을

1396
00:54:52,590 --> 00:54:54,990
수 있습니다. 왜냐하면 이제 우리는 이 연산자를 RNN이

1397
00:54:54,990 --> 00:54:57,210
전혀 필요 없는 지점까지 일반화했기 때문입니다.

1398
00:54:57,210 --> 00:54:59,390
우리는 이제 RNN에서 사용하던 동일한

1399
00:54:59,390 --> 00:55:01,270
문제를 위해 이를 사용할 수 있습니다.

1400
00:55:01,270 --> 00:55:03,950
따라서 이제 우리는 단어 시퀀스를 처리하는 데 사용할 수 있습니다.

1401
00:55:03,950 --> 00:55:07,950
예를 들어, 'attention is very'와 'output is
very cool'입니다.

1402
00:55:07,950 --> 00:55:10,630
따라서 이 경우 우리는 RNN으로 지난 강의에서 본 것과

1403
00:55:10,630 --> 00:55:12,770
동일한 언어 모델링 작업을 수행하고 있습니다.

1404
00:55:12,770 --> 00:55:14,310
하지만 이제 우리는 이 자기 주의 블록으로

1405
00:55:14,310 --> 00:55:15,790
이를 본격적으로 수행할 수 있습니다.

1406
00:55:15,790 --> 00:55:19,230
하지만 이 경우 첫 번째 출력 'is'는 첫 번째 단어에만

1407
00:55:19,230 --> 00:55:20,770
의존하도록 하고 싶습니다.

1408
00:55:20,770 --> 00:55:23,510
두 번째 출력 'very'는 첫 두 단어에만 의존하도록

1409
00:55:23,510 --> 00:55:24,050
허용합니다.

1410
00:55:24,050 --> 00:55:26,550
우리는 네트워크가 시퀀스에서 미리 보거나 속이는 것을

1411
00:55:26,550 --> 00:55:27,330
원하지 않습니다.

1412
00:55:27,330 --> 00:55:30,535
그래서 여기서 우리는 마스킹을 사용할 것입니다.

1413
00:55:30,535 --> 00:55:32,910
자기 주의와 관련하여 우리가 때때로

1414
00:55:32,910 --> 00:55:34,930
하는 또 다른 것은 다중 헤드 자기

1415
00:55:34,930 --> 00:55:38,550
주의라고 하며, 이는 n개의 독립적인 자기 주의 복사본을

1416
00:55:38,550 --> 00:55:40,325
병렬로 실행하는 것입니다.

1417
00:55:40,325 --> 00:55:41,450
왜 이렇게 하기를 원합니까?

1418
00:55:41,450 --> 00:55:43,330
더 많은 계산이 필요하고, 더 많은 플롭이 필요하며, 더 많은

1419
00:55:43,330 --> 00:55:44,205
매개변수가 필요하기 때문입니다.

1420
00:55:44,205 --> 00:55:46,250
딥러닝에서는 항상 더 크고 더 많은 것을 원합니다.

1421
00:55:46,250 --> 00:55:49,930
그리고 이것은 이 레이어를 더 크고 강력하게 만드는

1422
00:55:49,930 --> 00:55:51,550
또 다른 방법입니다.

1423
00:55:51,550 --> 00:55:53,850
우리가 할 것은 입력 X를

1424
00:55:53,850 --> 00:55:56,170
H개의 독립적인 자기 주의

1425
00:55:56,170 --> 00:55:58,102
레이어로 라우팅하는 것입니다.

1426
00:55:58,102 --> 00:55:59,810
각각은 자신의 출력

1427
00:55:59,810 --> 00:56:04,450
Y를 생성하고, 그 출력은 쌓여서 융합되고, 각 독립적인

1428
00:56:04,450 --> 00:56:06,410
자기 주의 레이어의 출력

1429
00:56:06,410 --> 00:56:08,890
데이터를 융합하기 위해

1430
00:56:08,890 --> 00:56:11,810
출력에서 또 다른 선형 투영을 가집니다.

1431
00:56:11,810 --> 00:56:14,410
이 경우 이것을 다중 헤드

1432
00:56:14,410 --> 00:56:16,250
자기 주의라고 합니다.

1433
00:56:16,250 --> 00:56:19,510
그리고 이것은 우리가 항상 실제에서 보는 형식입니다.

1434
00:56:19,510 --> 00:56:22,870
요즘 자기 주의가 사용될 때마다

1435
00:56:22,870 --> 00:56:25,290
거의 항상 이 다중 헤드

1436
00:56:25,290 --> 00:56:27,770
자기 주의 버전입니다.

1437
00:56:27,770 --> 00:56:30,250
실제로는 이 모든 것을 행렬

1438
00:56:30,250 --> 00:56:32,790
곱셈으로 계산할 수 있습니다.

1439
00:56:32,790 --> 00:56:34,810
그래서 for 루프를 실행할 필요가 없습니다.

1440
00:56:34,810 --> 00:56:37,930
당신이 똑똑하고 적절한 곳에서 배치 행렬 곱셈을

1441
00:56:37,930 --> 00:56:41,050
사용하면 이 H개의 자기 주의 복사본을 모두

1442
00:56:41,050 --> 00:56:42,850
병렬로 계산할 수 있습니다.

1443
00:56:42,850 --> 00:56:45,990
사실, 이 전체 자기 주의 연산자는 많은 것이 일어나는

1444
00:56:45,990 --> 00:56:47,570
것처럼 보이지만,

1445
00:56:47,570 --> 00:56:50,390
기본적으로는 네 개의 행렬 곱셈으로 이루어져 있습니다.

1446
00:56:50,390 --> 00:56:52,990
우리는 입력을 쿼리, 키 및 값으로

1447
00:56:52,990 --> 00:56:56,150
투영하는 하나의 행렬 곱셈이 있습니다.

1448
00:56:56,150 --> 00:56:59,610
우리는 Qk 유사성을 계산하는 또 다른 행렬 곱셈이 있습니다.

1449
00:56:59,610 --> 00:57:03,070
각 Q에 대해 모든 K와의 유사성을 계산합니다.

1450
00:57:03,070 --> 00:57:05,270
그리고 이것은 다중 헤드 경우의

1451
00:57:05,270 --> 00:57:07,170
큰 배치 행렬 곱셈입니다.

1452
00:57:07,170 --> 00:57:09,150
우리는 V 가중치라고 불리는 또 다른

1453
00:57:09,150 --> 00:57:10,900
것을 가지고 있으며, 소프트맥스 항목으로

1454
00:57:10,900 --> 00:57:13,770
가중치가 부여된 모든 값의 선형 조합을 취하고자 합니다.

1455
00:57:13,770 --> 00:57:16,790
그리고 이것은 또 다른 큰 배치 행렬 곱셈으로 수행될 수 있습니다.

1456
00:57:16,790 --> 00:57:18,750
마지막으로, 우리는 자기 주의의

1457
00:57:18,750 --> 00:57:21,910
서로 다른 헤드 간의 정보를 혼합하기 위한 출력

1458
00:57:21,910 --> 00:57:22,827
투영이 있습니다.

1459
00:57:22,827 --> 00:57:24,577
많은 방정식과 많은 벡터가

1460
00:57:24,577 --> 00:57:26,210
오가지만, 이 전체 자기

1461
00:57:26,210 --> 00:57:27,627
주의 연산자는 기본적으로

1462
00:57:27,627 --> 00:57:30,870
네 개의 큰 배치 행렬 곱셈으로 이루어져 있습니다.

1463
00:57:30,870 --> 00:57:33,150
행렬 곱셈은 정말 확장 가능하고

1464
00:57:33,150 --> 00:57:35,510
강력한 원시 연산이기 때문에,

1465
00:57:35,510 --> 00:57:38,110
우리는 이를 분산하고 최적화할 수

1466
00:57:38,110 --> 00:57:40,950
있으며, 이 작업을 매우 병렬화하고 확장

1467
00:57:40,950 --> 00:57:43,810
가능하며 효율적으로 만들 수 있습니다.

1468
00:57:43,810 --> 00:57:45,410
네, 질문은 x1,

1469
00:57:45,410 --> 00:57:48,566
x2, x3가 정확히 동일하다는 것입니다.

1470
00:57:48,566 --> 00:57:51,490
네, 하지만 우리는 기본적으로 자기 주의

1471
00:57:51,490 --> 00:57:54,470
레이어의 별도의 복사본을 가질 것입니다.

1472
00:57:54,470 --> 00:57:55,652
모두 무작위일 것입니다.

1473
00:57:55,652 --> 00:57:57,610
모두 비판적으로 다른 가중치를 가질 것입니다.

1474
00:57:57,610 --> 00:58:00,270
그 가중치는 무작위로 초기화되며,

1475
00:58:00,270 --> 00:58:01,503
초기화 시 다릅니다.

1476
00:58:01,503 --> 00:58:03,170
그래서 약간 다른 방식으로 처리하는

1477
00:58:03,170 --> 00:58:04,350
것을 배우게 될 것입니다.

1478
00:58:04,350 --> 00:58:07,460
이것은 레이어에 추가 용량을 제공하는 방법입니다.

1479
00:58:07,460 --> 00:58:09,210
아, 네, 서로 다른 헤드 간의 유일한

1480
00:58:09,210 --> 00:58:10,310
차이는 가중치입니다.

1481
00:58:10,310 --> 00:58:12,227
구조는 정확히 동일하지만, 계산도 정확히

1482
00:58:12,227 --> 00:58:13,770
동일하지만, 서로 다른

1483
00:58:13,770 --> 00:58:14,830
가중치를 가질 것입니다.

1484
00:58:14,830 --> 00:58:17,370
그 가중치는 초기화 시 서로 다른

1485
00:58:17,370 --> 00:58:18,570
값으로 초기화됩니다.

1486
00:58:18,570 --> 00:58:21,110
하지만 그 외에는 모두 정확히 동일합니다.

1487
00:58:21,110 --> 00:58:23,590
좋아요, 우리가 건너뛸 수 있는 내용이 있습니다.

1488
00:58:23,590 --> 00:58:26,490
하지만 이제 우리는 이 수업에서 본 세 가지

1489
00:58:26,490 --> 00:58:28,970
서로 다른 방식으로 시퀀스를 처리하는 정말

1490
00:58:28,970 --> 00:58:30,890
흥미로운 지점에 도달했습니다.

1491
00:58:30,890 --> 00:58:32,710
첫 번째는 순환 신경망입니다.

1492
00:58:32,710 --> 00:58:35,570
우리는 순환 신경망이 기본적으로 1D 순서형 시퀀스에서

1493
00:58:35,570 --> 00:58:37,130
작동한다는 것을 보았습니다.

1494
00:58:37,130 --> 00:58:38,350
정말 멋집니다.

1495
00:58:38,350 --> 00:58:39,350
정말 강력합니다.

1496
00:58:39,350 --> 00:58:40,850
사람들은 오랫동안 그것들을 좋아했습니다.

1497
00:58:40,850 --> 00:58:43,130
하지만 본질적으로 병렬화가 잘 되지 않습니다.

1498
00:58:43,130 --> 00:58:44,970
각 은닉 상태가 이전 은닉

1499
00:58:44,970 --> 00:58:47,877
상태에 의존하는 이 동시 구조 때문에,

1500
00:58:47,877 --> 00:58:49,710
본질적으로 순차적인

1501
00:58:49,710 --> 00:58:50,450
알고리즘입니다.

1502
00:58:50,450 --> 00:58:53,270
시퀀스 전반에 걸쳐 병렬화할 방법이 없습니다.

1503
00:58:53,270 --> 00:58:55,130
그래서 확장하기가 매우 어렵고,

1504
00:58:55,130 --> 00:58:57,310
매우 크게 만들기가 어렵습니다.

1505
00:58:57,310 --> 00:58:59,810
우리가 본 또 다른 원시적인 것은 합성곱입니다.

1506
00:58:59,810 --> 00:59:01,470
합성곱은 기본적으로 다차원

1507
00:59:01,470 --> 00:59:03,190
그리드에서 작동합니다.

1508
00:59:03,190 --> 00:59:05,790
우리는 이미지의 경우 2차원 그리드에서 그것을 보았습니다.

1509
00:59:05,790 --> 00:59:08,930
1D 그리드, 3D 그리드, 4D 그리드에서도 실행할 수 있습니다.

1510
00:59:08,930 --> 00:59:10,630
합성곱은 기본적으로

1511
00:59:10,630 --> 00:59:14,670
N차원 그리드에서 정보를 지역적으로 혼합하는 것입니다.

1512
00:59:14,670 --> 00:59:15,410
이것은 훌륭합니다.

1513
00:59:15,410 --> 00:59:16,830
매우 병렬화가 가능합니다.

1514
00:59:16,830 --> 00:59:20,170
그리드에서 커널을 슬라이딩하는 개념 덕분에, 커널을

1515
00:59:20,170 --> 00:59:22,470
배치할 수 있는 각 위치는 원칙적으로

1516
00:59:22,470 --> 00:59:24,690
병렬로 계산될 수 있습니다.

1517
00:59:24,690 --> 00:59:26,990
그래서 이것은 매우 병렬화 가능한 원시입니다.

1518
00:59:26,990 --> 00:59:30,630
하지만 큰 수용 필드를 구축하는 데 어려움이 있습니다.

1519
00:59:30,630 --> 00:59:33,990
전체 매우 긴 입력 시퀀스나 전체 매우 큰

1520
00:59:33,990 --> 00:59:37,170
이미지를 합성곱으로 요약하려면, 매우

1521
00:59:37,170 --> 00:59:39,530
큰 합성곱 커널이 필요하거나

1522
00:59:39,530 --> 00:59:42,190
많은 합성곱 층을 쌓아야 합니다.

1523
00:59:42,190 --> 00:59:45,010
그래서 여전히 대량의 데이터를 처리하는

1524
00:59:45,010 --> 00:59:48,410
방식에서 일부 본질적인 순차성이 도입됩니다.

1525
00:59:48,410 --> 00:59:50,050
이제 자기 주의는

1526
00:59:50,050 --> 00:59:51,810
기본적으로 벡터 집합에서

1527
00:59:51,810 --> 00:59:54,290
작동하는 별도의 원시입니다.

1528
00:59:54,290 --> 00:59:56,647
자연스럽게 긴 시퀀스로 일반화됩니다.

1529
00:59:56,647 --> 00:59:58,730
순환 신경망에서처럼

1530
00:59:58,730 --> 01:00:00,710
병목 현상이 없습니다.

1531
01:00:00,710 --> 01:00:02,570
모든 벡터가 서로를 볼

1532
01:00:02,570 --> 01:00:05,570
수 있도록 많은 층을 쌓을 필요도

1533
01:00:05,570 --> 01:00:06,570
없습니다.

1534
01:00:06,570 --> 01:00:08,650
자기 주의의 한 층에서 모든 벡터는

1535
01:00:08,650 --> 01:00:10,030
다른 모든 벡터를 봅니다.

1536
01:00:10,030 --> 01:00:12,290
그래서 단 한 층으로도 많은

1537
01:00:12,290 --> 01:00:14,290
계산을 수행할 수 있습니다.

1538
01:00:14,290 --> 01:00:15,910
그리고 또한 매우 병렬화 가능합니다.

1539
01:00:15,910 --> 01:00:18,170
우리가 보았듯이, 전체 작업은 단지 네 개의 큰

1540
01:00:18,170 --> 01:00:19,058
행렬 곱셈입니다.

1541
01:00:19,058 --> 01:00:20,850
행렬 곱셈은 우리가 분산할 수 있는

1542
01:00:20,850 --> 01:00:21,950
훌륭한 원시입니다.

1543
01:00:21,950 --> 01:00:23,070
우리는 GPU에서 실행할 수 있습니다.

1544
01:00:23,070 --> 01:00:26,170
우리는 매우 확장 가능하고 분산된 방식으로 실행할 수 있습니다.

1545
01:00:26,170 --> 01:00:29,390
주의의 유일한 단점은 비용이 많이 든다는 것입니다.

1546
01:00:29,390 --> 01:00:31,050
길이 n인 시퀀스에 대해 n

1547
01:00:31,050 --> 01:00:33,830
제곱의 계산이 필요하고, n 제곱 또는 나중에

1548
01:00:33,830 --> 01:00:36,970
n, 길이 n인 시퀀스에 대해 n 메모리가 필요합니다.

1549
01:00:36,970 --> 01:00:40,950
그리고 만약 n이 100,000, 1백만, 1천만이 된다면,

1550
01:00:40,950 --> 01:00:42,690
n 제곱은 매우 비쌉니다.

1551
01:00:42,690 --> 01:00:45,670
하지만 더 많은 GPU를 구매함으로써 이를 해결할 수 있습니다.

1552
01:00:45,670 --> 01:00:47,510
그래서 사람들이 여기서 생각해낸

1553
01:00:47,510 --> 01:00:48,970
기본적인 해결책입니다.

1554
01:00:48,970 --> 01:00:50,790
기본적으로 주의는 매우

1555
01:00:50,790 --> 01:00:52,910
임의의 데이터 조각을 처리하는

1556
01:00:52,910 --> 01:00:56,990
데 매우 강력한 슈퍼 멋진 원시 요소가 되었습니다.

1557
01:00:56,990 --> 01:01:00,550
어떤 것을 사용해야 할지 궁금할 수 있습니다.

1558
01:01:00,550 --> 01:01:01,770
주의만 있으면 됩니다.

1559
01:01:01,770 --> 01:01:03,590
세 가지 중에서 주의만으로도

1560
01:01:03,590 --> 01:01:06,390
상당한 성과를 낼 수 있습니다.

1561
01:01:06,390 --> 01:01:08,130
이제 질문은 병렬화 가능성입니다.

1562
01:01:08,130 --> 01:01:09,470
그 장점은 무엇인가요?

1563
01:01:09,470 --> 01:01:13,730
그 장점은 컴퓨팅 역사에서 프로세서를 더 빠르게 만드는

1564
01:01:13,730 --> 01:01:15,930
것이 어려워진다는 것입니다.

1565
01:01:15,930 --> 01:01:18,630
우리는 하드웨어의 근본적인 한계로 인해 이 한계에

1566
01:01:18,630 --> 01:01:19,490
부딪혔습니다.

1567
01:01:19,490 --> 01:01:22,150
개별 프로세스를 더 빠르게 만드는 것이 매우

1568
01:01:22,150 --> 01:01:22,930
어려워졌습니다.

1569
01:01:22,930 --> 01:01:26,050
하지만 우리가 매우 쉽게 할 수 있는 것은 많은 프로세서를 확보하는
것입니다.

1570
01:01:26,050 --> 01:01:30,870
지난 20년 동안 더 많은 계산을 조정할 수 있었던 방법은

1571
01:01:30,870 --> 01:01:32,870
하나의 매우 빠른

1572
01:01:32,870 --> 01:01:35,390
프로세서에서 실행할 필요가 없는 알고리즘을

1573
01:01:35,390 --> 01:01:37,073
찾는 것입니다.

1574
01:01:37,073 --> 01:01:39,490
대신, 10개의 프로세서, 100개의 프로세서,

1575
01:01:39,490 --> 01:01:43,090
1,000개의 프로세서, 또는 백만 개의 프로세서를 활용할

1576
01:01:43,090 --> 01:01:46,190
수 있는 알고리즘이 있다면, 저는 스탠포드 캠퍼스 전체를

1577
01:01:46,190 --> 01:01:48,690
프로세서로 덮고 이 큰 작업을 처리하기

1578
01:01:48,690 --> 01:01:50,790
위해 모두 함께 작업하게 하고 싶습니다.

1579
01:01:50,790 --> 01:01:52,630
그런 알고리즘을 찾을 수 있다면, 그것이

1580
01:01:52,630 --> 01:01:54,922
우리가 확장하고 정말 크고 강력한 계산을 할

1581
01:01:54,922 --> 01:01:55,890
수 있는 방법입니다.

1582
01:01:55,890 --> 01:01:57,690
따라서 병렬화의 이점은 더 많은

1583
01:01:57,690 --> 01:02:00,250
프로세서를 병렬로 쉽게 활용할 수 있는

1584
01:02:00,250 --> 01:02:02,570
알고리즘이 있다면, 개별 프로세서가 더

1585
01:02:02,570 --> 01:02:04,763
빨라지기를 기다리지 않고도 이러한

1586
01:02:04,763 --> 01:02:06,930
알고리즘을 확장할 수 있다는 것입니다.

1587
01:02:06,930 --> 01:02:09,513
그들은 결코 빨라지지 않을 수도 있습니다.

1588
01:02:09,513 --> 01:02:11,430
네, n 제곱과의 트레이드오프가 있나요?

1589
01:02:11,430 --> 01:02:13,610
저는 n 제곱이 실제로 좋은 것이라고 생각합니다.

1590
01:02:13,610 --> 01:02:14,995
그래서 나쁘게 보입니다.

1591
01:02:14,995 --> 01:02:16,370
컴퓨터 과학에서는

1592
01:02:16,370 --> 01:02:20,390
n 안의 매개변수가 나쁘다고 배웁니다.

1593
01:02:20,390 --> 01:02:22,630
하지만 신경망의 경우, 계산 측면에서

1594
01:02:22,630 --> 01:02:25,370
더 많은 계산은 네트워크가 더 많은 계산을 수행하고,

1595
01:02:25,370 --> 01:02:27,150
더 많은 사고 능력과 처리

1596
01:02:27,150 --> 01:02:30,230
능력을 갖게 되므로 실제로 좋은 것일 수 있습니다.

1597
01:02:30,230 --> 01:02:32,010
그래서 실제로 네트워크가 입력

1598
01:02:32,010 --> 01:02:34,260
시퀀스에서 더 많은 계산을 할수록, 더 나은

1599
01:02:34,260 --> 01:02:36,050
답변에 도달할 수 있을지도 모릅니다.

1600
01:02:36,050 --> 01:02:38,090
그래서 더 비싸다는 의미지만,

1601
01:02:38,090 --> 01:02:40,330
그것이 반드시 나쁜 것은 아닙니다.

1602
01:02:40,330 --> 01:02:41,830
기본적으로, 트랜스포머는

1603
01:02:41,830 --> 01:02:44,710
이제 모든 것의 핵심에 자기 주의를 두는

1604
01:02:44,710 --> 01:02:46,110
신경망 아키텍처입니다.

1605
01:02:46,110 --> 01:02:49,430
우리의 입력은 벡터 집합 X가 될 것입니다. 그런 다음 우리는 방금 말한
대로 모든

1606
01:02:49,430 --> 01:02:51,975
벡터를 서로 대화할 수 있게 해주는

1607
01:02:51,975 --> 01:02:54,350
놀라운 원시인 자기 주의를

1608
01:02:54,350 --> 01:02:56,230
통해 실행할 것입니다.

1609
01:02:56,230 --> 01:02:58,230
그 후, 우리는 몇 강의 전

1610
01:02:58,230 --> 01:03:00,550
ResNet에서 잔여 연결을 사용하고자

1611
01:03:00,550 --> 01:03:03,070
했던 것과 같은 이유로 그 자기 주의를

1612
01:03:03,070 --> 01:03:05,030
잔여 연결로 감쌀 것입니다.

1613
01:03:05,030 --> 01:03:08,107
그런 다음 그 잔여 연결의 출력을 가져와서 레이어

1614
01:03:08,107 --> 01:03:09,690
정규화를 통과시킬 것입니다.

1615
01:03:09,690 --> 01:03:12,130
왜냐하면 ResNet과 CNN에서 보았듯이

1616
01:03:12,130 --> 01:03:14,110
아키텍처 내부에 정규화를

1617
01:03:14,110 --> 01:03:16,470
추가하면 더 안정적으로 훈련되기 때문입니다.

1618
01:03:16,470 --> 01:03:18,905
하지만 지금 흥미로운 것이 있습니다.

1619
01:03:18,905 --> 01:03:21,030
자기 주의(attention)는

1620
01:03:21,030 --> 01:03:23,472
기본적으로 모든 벡터를 서로 비교하는 것입니다.

1621
01:03:23,472 --> 01:03:24,930
그리고 이것은 매우 유용한 원시 연산입니다.

1622
01:03:24,930 --> 01:03:26,570
이것은 매우 강력한 작업입니다.

1623
01:03:26,570 --> 01:03:28,790
하지만 우리는 이 네트워크가 벡터를

1624
01:03:28,790 --> 01:03:32,990
독립적으로 하나씩 처리할 수 있는 능력을 갖추기를 원합니다.

1625
01:03:32,990 --> 01:03:34,410
그래서 변환기 내부에 두

1626
01:03:34,410 --> 01:03:37,910
번째 원시 연산이 있습니다. 그것은 다층 퍼셉트론(MLP), 또는

1627
01:03:37,910 --> 01:03:39,077
FFN이라고도 불립니다.

1628
01:03:39,077 --> 01:03:41,410
기본적으로 이것은 내부의 각

1629
01:03:41,410 --> 01:03:43,810
벡터에 독립적으로 작동하는 두

1630
01:03:43,810 --> 01:03:45,930
개의 층을 가진 신경망입니다.

1631
01:03:45,930 --> 01:03:48,750
그래서 이것은 자기 주의와 함께 작동하여,

1632
01:03:48,750 --> 01:03:51,250
자기 주의가 모든 벡터가 서로

1633
01:03:51,250 --> 01:03:53,210
대화하고 비교할 수 있게 하고,

1634
01:03:53,210 --> 01:03:56,370
FFN 또는 MLP가 각 벡터에 독립적으로

1635
01:03:56,370 --> 01:03:58,490
계산을 수행할 수 있게 합니다.

1636
01:03:58,490 --> 01:04:01,130
우리는 또한 MLP를 잔여 연결로 감싸고,

1637
01:04:01,130 --> 01:04:03,850
층 정규화를 추가하고, 전체를 박스로 감싸서

1638
01:04:03,850 --> 01:04:05,610
신경망 블록이라고 부릅니다.

1639
01:04:05,610 --> 01:04:08,230
이것이 우리의 변환기 블록입니다.

1640
01:04:08,230 --> 01:04:11,730
변환기는 변환기 블록의 연속입니다.

1641
01:04:11,730 --> 01:04:14,650
이것들은 시간이 지남에 따라 훨씬 더 커졌습니다.

1642
01:04:14,650 --> 01:04:17,530
아키텍처는 2017년 이 기술이 도입된 이후로

1643
01:04:17,530 --> 01:04:19,010
크게 변하지 않았습니다.

1644
01:04:19,010 --> 01:04:21,870
원래의 변환기는 약 12개의 블록과 2억 개의

1645
01:04:21,870 --> 01:04:23,770
매개변수를 가지고 있었고,

1646
01:04:23,770 --> 01:04:27,290
이제는 사람들이 수백 개의 블록과 수조 개의 매개변수를

1647
01:04:27,290 --> 01:04:29,150
가진 변환기를 훈련하고 있습니다.

1648
01:04:29,150 --> 01:04:30,730
이 동일한 아키텍처는

1649
01:04:30,730 --> 01:04:33,190
지난 8년 동안 계산, 크기 및

1650
01:04:33,190 --> 01:04:36,238
매개변수에서 여러 배수로 확장되었습니다.

1651
01:04:36,238 --> 01:04:38,030
우리는 이미 본 것처럼 언어

1652
01:04:38,030 --> 01:04:39,750
모델링에도 사용할 수 있습니다.

1653
01:04:39,750 --> 01:04:43,050
이미지에도 사용할 수 있습니다.

1654
01:04:43,050 --> 01:04:45,275
여기서 응용은 꽤 간단합니다.

1655
01:04:45,275 --> 01:04:47,150
이미지를 주면, 기본적으로

1656
01:04:47,150 --> 01:04:49,750
이미지를 패치로 나누고 각 패치를

1657
01:04:49,750 --> 01:04:51,830
개별적으로 벡터로 변환합니다.

1658
01:04:51,830 --> 01:04:55,430
그 벡터들은 우리의 변환기로 입력됩니다.

1659
01:04:55,430 --> 01:04:59,070
그런 다음 출력은 입력의 각 패치에 대해 변환기로부터

1660
01:04:59,070 --> 01:05:00,888
하나의 출력을 제공합니다.

1661
01:05:00,888 --> 01:05:03,430
이제 분류 점수와 같은 작업을 하거나 분류

1662
01:05:03,430 --> 01:05:05,190
문제를 해결하려면, 변환기로부터

1663
01:05:05,190 --> 01:05:07,432
나오는 모든 벡터에 대해 풀링

1664
01:05:07,432 --> 01:05:09,390
작업을 수행하고 클래스 점수를

1665
01:05:09,390 --> 01:05:11,390
예측하는 선형 레이어를 사용합니다.

1666
01:05:11,390 --> 01:05:14,670
따라서 이 변환기의 동일한 구조는

1667
01:05:14,670 --> 01:05:17,190
언어와 이미지, 그리고 많은

1668
01:05:17,190 --> 01:05:20,627
다른 것들에도 적용될 수 있습니다.

1669
01:05:20,627 --> 01:05:22,710
변환기가 처음 도입된 이후 몇 가지

1670
01:05:22,710 --> 01:05:25,000
사소한 수정이 있었다고 언급했습니다.

1671
01:05:25,000 --> 01:05:26,750
하지만 시간이 부족하므로 그 부분은

1672
01:05:26,750 --> 01:05:28,390
추가 읽기로 남겨두겠습니다.

1673
01:05:28,390 --> 01:05:31,450
이 강의의 마지막에서 우리가 도달한 요약은,

1674
01:05:31,450 --> 01:05:34,410
기본적으로 제가 처음에 약속한 두 가지입니다.

1675
01:05:34,410 --> 01:05:36,830
하나는 벡터 집합에서 작업할 수

1676
01:05:36,830 --> 01:05:38,650
있게 해주는 새로운 원시

1677
01:05:38,650 --> 01:05:40,550
연산인 주의를 도입했습니다.

1678
01:05:40,550 --> 01:05:41,910
고도로 병렬화 가능합니다.

1679
01:05:41,910 --> 01:05:44,150
기본적으로 몇 개의 행렬 곱셈에 불과합니다.

1680
01:05:44,150 --> 01:05:47,230
따라서 고도로 확장 가능하고, 고도로 병렬화 가능하며, 고도로 유연합니다.

1681
01:05:47,230 --> 01:05:49,490
많은 다양한 상황에 적용될 수 있습니다.

1682
01:05:49,490 --> 01:05:51,690
그리고 변환기는 이제 자기

1683
01:05:51,690 --> 01:05:53,890
주의를 주요 계산 원시로

1684
01:05:53,890 --> 01:05:56,570
사용하는 신경망 아키텍처입니다.

1685
01:05:56,570 --> 01:05:59,370
트랜스포머는 현재 딥러닝의 모든

1686
01:05:59,370 --> 01:06:02,990
응용 프로그램이 사용하는 신경망 아키텍처입니다.

1687
01:06:02,990 --> 01:06:07,058
그래서 정말 강력하고, 흥미롭고, 신나는 기술입니다.

1688
01:06:07,058 --> 01:06:09,350
트랜스포머는 이제 8년 동안 우리와 함께 해왔습니다.

1689
01:06:09,350 --> 01:06:12,090
그리고 당분간 사라질 것 같지는 않습니다.

1690
01:06:12,090 --> 01:06:14,690
그래서 꽤 흥미롭습니다.

1691
01:06:14,690 --> 01:06:16,970
오늘 강의는 여기까지입니다.

1692
01:06:16,970 --> 01:06:18,490
다음 시간에는 새로운 작업인

1693
01:06:18,490 --> 01:06:20,330
탐지, 분할, 시각화에

1694
01:06:20,330 --> 01:06:22,090
대해 이야기하고, 이러한 아키텍처를

1695
01:06:22,090 --> 01:06:24,370
사용하여 새로운 멋진 일을 어떻게

1696
01:06:24,370 --> 01:06:26,400
할 수 있는지 살펴보겠습니다.
