1
00:00:05,440 --> 00:00:08,000
모두 와주셔서 감사합니다.

2
00:00:08,000 --> 00:00:10,000
또 다른 게스트 강연이 있습니다.

3
00:00:10,000 --> 00:00:12,160
오늘은 Ranjay Krishna가 있습니다.

4
00:00:12,160 --> 00:00:14,160
Ranjay Krishna는

5
00:00:14,160 --> 00:00:16,600
워싱턴 대학교 컴퓨터 과학 및

6
00:00:16,600 --> 00:00:21,040
공학부의 조교수이며, RAIVN 연구소의 공동 디렉터입니다.

7
00:00:21,040 --> 00:00:24,200
그는 2020년과 2021년에 CS231N의

8
00:00:24,200 --> 00:00:27,160
이전 버전을 가르쳤으며, 그의

9
00:00:27,160 --> 00:00:30,220
연구는 컴퓨터 비전, 자연어 처리, 로봇

10
00:00:30,220 --> 00:00:33,280
공학 및 인간-컴퓨터 상호작용의 교차점에

11
00:00:33,280 --> 00:00:34,240
있습니다.

12
00:00:34,240 --> 00:00:37,000
오늘 강의에서는 다중 모달 기초 모델에 대해 논의할

13
00:00:37,000 --> 00:00:37,500
것입니다.

14
00:00:37,500 --> 00:00:38,000
그리고.

15
00:00:38,000 --> 00:00:39,200
Ranjay, 발언권은 당신에게 있습니다.

16
00:00:39,200 --> 00:00:40,103
감사합니다.

17
00:00:40,103 --> 00:00:41,020
다시 돌아오게 되어 기쁩니다.

18
00:00:41,020 --> 00:00:43,080
제가 스탠포드에서 이 과정을

19
00:00:43,080 --> 00:00:47,520
처음 가르쳤던 것은 2020년이었고, 그때 우리는 모든

20
00:00:47,520 --> 00:00:51,560
자료를 온라인으로 옮기는 데 약 3주가 필요했습니다.

21
00:00:51,560 --> 00:00:55,320
그 이후 매년 가르치는 것이 훨씬 쉬워졌습니다.

22
00:00:55,320 --> 00:00:56,300
다시 돌아오게 되어 기쁩니다.

23
00:00:56,300 --> 00:00:58,675
그래서 오늘은 다중 모달 기초 모델에 대해

24
00:00:58,675 --> 00:00:59,530
이야기할 것입니다.

25
00:00:59,530 --> 00:01:01,610
지금까지 이 수업의 많은

26
00:01:01,610 --> 00:01:05,930
강의는 개별 작업을 위한 개별 모델 구축에 초점을

27
00:01:05,930 --> 00:01:07,410
맞추었습니다.

28
00:01:07,410 --> 00:01:09,968
이러한 모델은 강의에서 여러 번 보았던

29
00:01:09,968 --> 00:01:12,010
몇 가지 단계를 따릅니다.

30
00:01:12,010 --> 00:01:13,930
데이터 세트를 수집하고, 일반적으로 훈련

31
00:01:13,930 --> 00:01:16,650
세트와 테스트 세트를 준비한 다음, 그 목적에 맞는

32
00:01:16,650 --> 00:01:18,118
매우 전문화된 모델을 훈련합니다.

33
00:01:18,118 --> 00:01:20,410
그것은 이미지 분류 모델이나 이미지 캡션

34
00:01:20,410 --> 00:01:22,810
모델일 수 있으며, 여러분의 과제에서도 보았던

35
00:01:22,810 --> 00:01:23,510
것들입니다.

36
00:01:23,510 --> 00:01:27,490
그리고 마지막으로 그 모델들을 테스트 세트에서 평가합니다.

37
00:01:27,490 --> 00:01:29,850
지난 몇 년 동안 이

38
00:01:29,850 --> 00:01:32,170
분야에서 달라진 점은 이러한

39
00:01:32,170 --> 00:01:34,770
개별 모델에서 더 기초

40
00:01:34,770 --> 00:01:38,370
모델을 구축하는 방향으로의 전환입니다.

41
00:01:38,370 --> 00:01:40,210
기초 모델을 생각하는

42
00:01:40,210 --> 00:01:43,290
방법은 다양한 기술과 다양한

43
00:01:43,290 --> 00:01:46,790
작업에 대해 모델을 사전 훈련하고, 이후

44
00:01:46,790 --> 00:01:50,090
필요에 따라 개별 작업에 맞게

45
00:01:50,090 --> 00:01:51,570
조정하는 것입니다.

46
00:01:51,570 --> 00:01:54,690
예를 들어, 여러분이 아마도 어떤 형태로든

47
00:01:54,690 --> 00:01:58,580
사용할 매우 일반적인 기초 모델은 GPT이며, GPT는

48
00:01:58,580 --> 00:02:01,660
인터넷의 많은 Common Crawl 데이터로

49
00:02:01,660 --> 00:02:02,600
훈련되었습니다.

50
00:02:02,600 --> 00:02:04,438
그런 다음 그 모델을 가져와서

51
00:02:04,438 --> 00:02:06,480
다양한 목적에 맞게 미세 조정합니다.

52
00:02:06,480 --> 00:02:09,020
예를 들어, 수학 문제나 기호 추론

53
00:02:09,020 --> 00:02:11,520
또는 퀴즈 질문에 맞게 미세 조정합니다.

54
00:02:11,520 --> 00:02:13,140
이 모든 것은 이 모델이 빠르게

55
00:02:13,140 --> 00:02:14,845
적응할 수 있는 개별 작업입니다.

56
00:02:14,845 --> 00:02:16,220
기초 모델의 장점은 새로운

57
00:02:16,220 --> 00:02:17,980
작업에 대한 업데이트 단계,

58
00:02:17,980 --> 00:02:20,940
즉 적응을 매우 최소한의 데이터로 수행할 수

59
00:02:20,940 --> 00:02:22,590
있다는 것입니다. 즉, 많은

60
00:02:22,590 --> 00:02:24,840
훈련 데이터를 수집할 필요가 없습니다.

61
00:02:24,840 --> 00:02:27,085
보통 아주 적은 양으로도 충분합니다.

62
00:02:27,085 --> 00:02:28,460
종종 훈련 데이터를

63
00:02:28,460 --> 00:02:31,780
전혀 수집하지 않고도 가능할 수 있습니다.

64
00:02:31,780 --> 00:02:34,120
따라서 기초 모델에 대해 생각할 때,

65
00:02:34,120 --> 00:02:36,580
여러분이 관심을 가질 수 있는 다양한

66
00:02:36,580 --> 00:02:38,260
기초 모델 클래스가 있습니다.

67
00:02:38,260 --> 00:02:40,980
언어에서는 ELMo와 BERT가 이

68
00:02:40,980 --> 00:02:42,820
전체 혁명을 시작했습니다.

69
00:02:42,820 --> 00:02:46,833
그리고 이제 우리는 GPT와 T5 및 이러한 모델의 변형이 있습니다.

70
00:02:46,833 --> 00:02:48,500
이것들은 우리가 주로 다중 모달

71
00:02:48,500 --> 00:02:50,958
모델에 대해 이야기할 것이기 때문에 이 수업에서는

72
00:02:50,958 --> 00:02:52,400
다루지 않을 것입니다.

73
00:02:52,400 --> 00:02:54,140
우리가 이야기할 것은 이미지

74
00:02:54,140 --> 00:02:57,570
분류를 위한 동일한 기초 모델을 어떻게 구축하는지입니다.

75
00:02:57,570 --> 00:03:02,110
오늘은 CLIP과 CoCa와 같은 예제를 살펴보겠습니다.

76
00:03:02,110 --> 00:03:05,390
또한 수업에서 이미 본 언어 모델과 이러한

77
00:03:05,390 --> 00:03:08,830
비전 기초 모델을 결합하여 다양한 작업을

78
00:03:08,830 --> 00:03:12,590
해결할 수 있는 다중 모달 기초 모델을

79
00:03:12,590 --> 00:03:15,665
만드는 방법에 대해서도 이야기하겠습니다.

80
00:03:15,665 --> 00:03:17,790
물론, 우리는 언어 작업을 해결하는 것

81
00:03:17,790 --> 00:03:19,710
이상으로 많은 것을 할 수 있습니다.

82
00:03:19,710 --> 00:03:21,590
텍스트뿐만 아니라 생성하고자

83
00:03:21,590 --> 00:03:25,270
하는 마스크나 이미지를 출력할 수 있는 모델을

84
00:03:25,270 --> 00:03:28,735
구축하는 방법에 대해서도 이야기하겠습니다.

85
00:03:28,735 --> 00:03:31,110
마지막으로, 여러 기초 모델을

86
00:03:31,110 --> 00:03:33,150
결합하여 새로운 작업을

87
00:03:33,150 --> 00:03:37,030
수행하는 체인 아이디어에 대해 이야기하겠습니다.

88
00:03:37,030 --> 00:03:38,970
기초 모델에 대해 이야기할

89
00:03:38,970 --> 00:03:41,950
때, 분류하는 방법은 여러 가지가 있습니다.

90
00:03:41,950 --> 00:03:43,910
정의에 대한 의견이 종종

91
00:03:43,910 --> 00:03:46,130
다르기 때문에 어렵습니다.

92
00:03:46,130 --> 00:03:48,590
하지만 기초 모델에서 일반적으로 볼 수

93
00:03:48,590 --> 00:03:52,450
있는 것은 많은 다양한 작업에 대해 강력하고 일반적이라는 것입니다.

94
00:03:52,450 --> 00:03:55,220
따라서 모든 다양한 사용 사례에 대해 동일한 모델을 적용할

95
00:03:55,220 --> 00:03:57,608
수 있으며, 오늘 많은 사용 사례를 보여드리겠습니다.

96
00:03:57,608 --> 00:03:59,400
또한 이러한 많은 기초 모델에서

97
00:03:59,400 --> 00:04:01,025
공통적으로 나타나는 것은 많은

98
00:04:01,025 --> 00:04:02,700
매개변수를 가지고 있다는 것입니다.

99
00:04:02,700 --> 00:04:05,340
매우 많은 수의 매개변수와 대량의

100
00:04:05,340 --> 00:04:07,800
훈련 데이터를 가지고 있으며,

101
00:04:07,800 --> 00:04:10,800
일반적으로 일부 자기 지도 목표로 훈련됩니다.

102
00:04:10,800 --> 00:04:13,500
따라서 우리는 언어 관련 내용에 대해서는 이야기하지 않겠지만,

103
00:04:13,500 --> 00:04:16,120
오늘은 초록색으로 표시된 것들에 대해 이야기할 것입니다.

104
00:04:16,120 --> 00:04:18,800
그럼 이미지 분류부터 시작해 보겠습니다.

105
00:04:18,800 --> 00:04:22,520
어떻게 하면 우리가 관심 있는 데이터 세트에 대해 이미지

106
00:04:22,520 --> 00:04:25,200
분류를 해결할 수 있는 기초 모델을

107
00:04:25,200 --> 00:04:26,960
실제로 구축할 수 있을까요?

108
00:04:26,960 --> 00:04:29,300
몇 강의 전을 기억하신다면, 우리는

109
00:04:29,300 --> 00:04:31,840
자기 지도 학습에 대해 이야기했습니다.

110
00:04:31,840 --> 00:04:33,560
자기 지도 학습에서

111
00:04:33,560 --> 00:04:35,400
보았던 방법 중 하나는

112
00:04:35,400 --> 00:04:38,720
SimCLR로, 이는 유사하지

113
00:04:38,720 --> 00:04:42,200
않은 이미지와 대조하여 변형된 동일한

114
00:04:42,200 --> 00:04:45,360
이미지의 표현을 더 가깝게 끌어오는

115
00:04:45,360 --> 00:04:47,760
대조 목표를 가지고 있습니다.

116
00:04:47,760 --> 00:04:51,000
이 아이디어는 유사한 개념을 함께 끌어오는 것으로 생각할

117
00:04:51,000 --> 00:04:51,780
수 있습니다.

118
00:04:51,780 --> 00:04:54,170
고양이의 다양한 증강은

119
00:04:54,170 --> 00:04:57,310
서로 유사한 표현을 생성해야 하지만,

120
00:04:57,310 --> 00:04:59,570
예를 들어 개와 같은

121
00:04:59,570 --> 00:05:03,090
다른 카테고리의 표현은 멀어져야 합니다.

122
00:05:03,090 --> 00:05:06,010
이러한 자기 지도 학습 목표로 훈련할

123
00:05:06,010 --> 00:05:08,850
때의 희망은 이러한 표현이 충분히

124
00:05:08,850 --> 00:05:11,330
일반적이어서 새로운 것을 보았을

125
00:05:11,330 --> 00:05:14,490
때, 예를 들어 고양이나 개의 스케치가

126
00:05:14,490 --> 00:05:17,090
여전히 그 공간에 임베드되어

127
00:05:17,090 --> 00:05:20,250
정확히 무엇인지 분류하기 쉽다는 것입니다.

128
00:05:20,250 --> 00:05:21,910
이제 다중 모달로 넘어가겠습니다.

129
00:05:21,910 --> 00:05:25,090
이 동일한 아이디어와 목표를 가지고 텍스트를 그

130
00:05:25,090 --> 00:05:27,690
표현 공간에 추가하면 어떤 일이

131
00:05:27,690 --> 00:05:29,690
발생할지 생각해 볼 수 있습니다.

132
00:05:29,690 --> 00:05:31,410
예를 들어, '귀여운

133
00:05:31,410 --> 00:05:36,090
복슬복슬한 고양이'라는 텍스트의 표현을 임베드하고 그것이

134
00:05:36,090 --> 00:05:38,792
고양이 표현과 가까워지면

135
00:05:38,792 --> 00:05:40,250
좋습니다. 이제 우리는

136
00:05:40,250 --> 00:05:44,370
이미지와 텍스트 모두에서 쿼리할 수 있습니다.

137
00:05:44,370 --> 00:05:47,730
마찬가지로 '내가 가장 좋아하는 개는 골든

138
00:05:47,730 --> 00:05:49,710
리트리버'라는 문구를 임베드할 수

139
00:05:49,710 --> 00:05:51,380
있다면, 이상적으로 그

140
00:05:51,380 --> 00:05:55,700
표현은 다른 종류의 개보다 골든 리트리버에 더 가까워야 합니다.

141
00:05:55,700 --> 00:05:58,185
그래서 지금까지 수업에서 이야기한

142
00:05:58,185 --> 00:06:00,060
자기 지도 학습 목표를

143
00:06:00,060 --> 00:06:03,340
텍스트 및 기타 다중 모달 입력을 통합하도록

144
00:06:03,340 --> 00:06:06,140
조정하는 일반적인 아이디어입니다.

145
00:06:06,140 --> 00:06:08,700
SimCLR에서 기억하신다면,

146
00:06:08,700 --> 00:06:13,180
주요 목표는 동일한 이미지의 변형을 다시 끌어오는

147
00:06:13,180 --> 00:06:14,240
것입니다.

148
00:06:14,240 --> 00:06:18,060
고양이는 다른 고양이 증강에 가장 가까워야 합니다.

149
00:06:18,060 --> 00:06:20,500
따라서 저 초록색 화살표는 함께 끌어와야 하는

150
00:06:20,500 --> 00:06:21,792
두 가지를 나타냅니다.

151
00:06:21,792 --> 00:06:23,375
그리고 모든 다른 증강에서 더

152
00:06:23,375 --> 00:06:24,440
멀어져야 합니다.

153
00:06:24,440 --> 00:06:27,440
따라서 개나 원숭이의 다른 이미지는

154
00:06:27,440 --> 00:06:29,980
그 표현이 멀어지기를 원합니다.

155
00:06:29,980 --> 00:06:32,500
이제 우리는 그 동일한 아이디어를 사용하여 CLIP

156
00:06:32,500 --> 00:06:34,820
모델 훈련에 대해 생각해 볼 수 있습니다.

157
00:06:34,820 --> 00:06:37,700
CLIP에서는 왼쪽에 있는 동일한

158
00:06:37,700 --> 00:06:40,080
이미지 인코더를 유지하지만,

159
00:06:40,080 --> 00:06:43,100
오른쪽에는 이제 텍스트 인코더가 있습니다.

160
00:06:43,100 --> 00:06:46,020
이 텍스트 인코더는 개별 이미지에 대한

161
00:06:46,020 --> 00:06:47,980
설명을 임베딩하고 있습니다.

162
00:06:47,980 --> 00:06:51,310
따라서 당신의 개 이미지가 이제는 '내가 가장

163
00:06:51,310 --> 00:06:54,270
좋아하는 개는 골든 리트리버다'라는

164
00:06:54,270 --> 00:06:56,270
텍스트 표현에 더 가까워지고

165
00:06:56,270 --> 00:06:59,310
다른 모든 표현에서 멀어지기를 바랍니다.

166
00:06:59,310 --> 00:07:01,470
그리고 이것은 SimCLR에서 본

167
00:07:01,470 --> 00:07:03,390
것과 동일한 공식이기 때문에,

168
00:07:03,390 --> 00:07:05,470
이러한 모델을 훈련하는 데 사용하는

169
00:07:05,470 --> 00:07:08,550
목표는 많은 이미지-텍스트 쌍을 수집하는 것입니다.

170
00:07:08,550 --> 00:07:10,090
그런 다음 이러한

171
00:07:10,090 --> 00:07:12,070
쌍을 미니 배치로 모델에

172
00:07:12,070 --> 00:07:15,390
입력하고, SimCLR에 사용하는 대조 목표가

173
00:07:15,390 --> 00:07:16,810
있는지 확인합니다.

174
00:07:16,810 --> 00:07:19,970
하지만 이제 우리는 이미지와 텍스트에 걸쳐 이를 적용하고 있습니다.

175
00:07:19,970 --> 00:07:22,270
여기 분자에서는 유사한

176
00:07:22,270 --> 00:07:24,790
것들의 표현을 모으고, 분모에서는

177
00:07:24,790 --> 00:07:26,710
다른 모든 것들의

178
00:07:26,710 --> 00:07:29,110
표현을 분리하고 있습니다.

179
00:07:29,110 --> 00:07:30,550
물론 우리는 그 이미지가

180
00:07:30,550 --> 00:07:33,030
해당 텍스트에 가장 가까워지고 다른

181
00:07:33,030 --> 00:07:35,370
모든 텍스트에서 멀어지기를 원합니다.

182
00:07:35,370 --> 00:07:37,730
하지만 그 반대도 사실이기를 원합니다.

183
00:07:37,730 --> 00:07:39,350
따라서 모든 텍스트는

184
00:07:39,350 --> 00:07:42,710
해당 이미지에 가장 가까워지고 다른 모든 이미지

185
00:07:42,710 --> 00:07:45,970
설명에서 멀어져야 한다는 두 번째 목표도 있습니다.

186
00:07:45,970 --> 00:07:48,910
따라서 이 학습 목표에 입력하는

187
00:07:48,910 --> 00:07:51,550
두 가지 서로 다른 유형의

188
00:07:51,550 --> 00:07:55,470
모달리티 간에 보완적인 대칭 손실이 있습니다.

189
00:07:55,470 --> 00:07:58,470
물론 CLIP과 같은 모델의 정말 좋은

190
00:07:58,470 --> 00:08:02,070
점은 이미지와 텍스트의 연관성만으로 훈련할

191
00:08:02,070 --> 00:08:03,650
수 있다는 것입니다.

192
00:08:03,650 --> 00:08:06,090
인터넷에는 이러한 데이터가 많이 있습니다.

193
00:08:06,090 --> 00:08:08,950
따라서 인터넷에서 가져올 수 있는 해당 이미지와

194
00:08:08,950 --> 00:08:10,650
텍스트의 데이터가 많습니다.

195
00:08:10,650 --> 00:08:13,030
다운로드할 수 있으며, 이제 매우 대규모로

196
00:08:13,030 --> 00:08:14,850
이 모델을 훈련할 수 있습니다.

197
00:08:14,850 --> 00:08:16,590
그리고 이것이 OpenAI가

198
00:08:16,590 --> 00:08:18,550
2021년에 CLIP

199
00:08:18,550 --> 00:08:20,910
모델을 출시할 때 한 일입니다.

200
00:08:20,910 --> 00:08:22,650
그들은 많은 데이터를 수집한

201
00:08:22,650 --> 00:08:25,530
다음, 인터넷에서 찾은 모든 이미지-텍스트

202
00:08:25,530 --> 00:08:27,230
쌍을 사용하여 이 대조 목표를

203
00:08:27,230 --> 00:08:28,650
사용하여 훈련했습니다.

204
00:08:28,650 --> 00:08:30,590
그리고 훈련이 끝나면, 자가

205
00:08:30,590 --> 00:08:33,309
지도 학습 수업에서 본 것과 동일한

206
00:08:33,309 --> 00:08:35,630
2단계 파이프라인을 따릅니다.

207
00:08:35,630 --> 00:08:38,169
1단계에서는 사전 훈련을 하고,

208
00:08:38,169 --> 00:08:40,890
2단계에서는 그 이미지 인코더를 가져와

209
00:08:40,890 --> 00:08:43,470
새로운 작업에 적응시킬 수 있습니다.

210
00:08:43,470 --> 00:08:46,140
따라서 이 사전 훈련된 이미지 인코더를

211
00:08:46,140 --> 00:08:47,620
얻으면, 그것을 가져와

212
00:08:47,620 --> 00:08:50,760
가중치를 사용하고, 이미지 분류 작업이나

213
00:08:50,760 --> 00:08:52,600
탐지 작업에 적응시키기 위해

214
00:08:52,600 --> 00:08:54,680
추가적인 선형 레이어를 태그할

215
00:08:54,680 --> 00:08:58,000
수 있습니다. 또는 디코더를 넣고 의미론적

216
00:08:58,000 --> 00:09:00,120
분할 맵을 디코딩할 수 있습니다.

217
00:09:00,120 --> 00:09:02,520
따라서 이 사전 훈련된 목표에서

218
00:09:02,520 --> 00:09:07,120
모델을 초기화함으로써 가능한 다양한 작업이 많아집니다.

219
00:09:07,120 --> 00:09:09,200
이 논문이 발표되었을 때 정말

220
00:09:09,200 --> 00:09:13,400
흥미로웠던 점은 이 CLIP 인코더 위에 하나의 선형

221
00:09:13,400 --> 00:09:17,160
분류기를 선형으로 추가하는 것이 성능에서 큰 개선을 가져왔다는

222
00:09:17,160 --> 00:09:18,160
것입니다.

223
00:09:18,160 --> 00:09:20,760
그래프에서 여러 이미지 분류 데이터

224
00:09:20,760 --> 00:09:23,400
세트에 대한 평균 성능을 보여주고

225
00:09:23,400 --> 00:09:24,360
있습니다.

226
00:09:24,360 --> 00:09:26,780
CLIP 모델, 빨간색으로 표시된 모델들은

227
00:09:26,780 --> 00:09:28,280
모두 맨 위에 있습니다.

228
00:09:28,280 --> 00:09:32,200
더 많은 이미지를 훈련할수록 성능이 점점 더

229
00:09:32,200 --> 00:09:35,000
좋아지는 것을 볼 수 있습니다.

230
00:09:35,000 --> 00:09:36,560
그래서 정말 흥미로웠던 것은

231
00:09:36,560 --> 00:09:38,440
우리가 잠금 해제할 수 있었던

232
00:09:38,440 --> 00:09:41,420
정말 좋은 사전 훈련 목표가 있다는 것을 나타내는 것

233
00:09:41,420 --> 00:09:43,650
같았고, 인터넷에는 이미지 텍스트 데이터가

234
00:09:43,650 --> 00:09:46,330
풍부하다는 것을 의미합니다. 이는 우리가 이

235
00:09:46,330 --> 00:09:50,410
모델들을 매우 크고 성능이 뛰어나게 훈련할 수 있다는 것을 의미합니다.

236
00:09:50,410 --> 00:09:52,310
물론, 이것이 이야기의 끝은 아닙니다.

237
00:09:52,310 --> 00:09:54,450
우리가 이상적으로 하고 싶은 것은 이러한 기능을

238
00:09:54,450 --> 00:09:56,837
새로운 것에 맞게 조정할 필요가 없도록 하는 것입니다.

239
00:09:56,837 --> 00:09:59,170
우리는 이상적으로 CLIP 모델을 즉시

240
00:09:59,170 --> 00:10:00,730
사용할 수 있기를 원합니다.

241
00:10:00,730 --> 00:10:03,610
예를 들어 언어 모델에서는 일반적으로 자동 완성을

242
00:10:03,610 --> 00:10:05,310
위해 모델을 훈련합니다.

243
00:10:05,310 --> 00:10:07,430
이 자동 완성은 이렇게 작동합니다.

244
00:10:07,430 --> 00:10:09,410
당신은 '나는 사랑한다'라는 구문을 가지고

245
00:10:09,410 --> 00:10:13,250
있고, 그 다음 모델이 다음 단어를 채웁니다. 예를 들어, 케이크입니다.

246
00:10:13,250 --> 00:10:15,810
그리고 이 사전 훈련 목표로 훈련합니다.

247
00:10:15,810 --> 00:10:18,170
그리고 두 번째 단계에서 하고 싶은

248
00:10:18,170 --> 00:10:20,670
것은 기본적으로 동일한 모델을 가져와 새로운

249
00:10:20,670 --> 00:10:22,490
작업에 적응시키는 것입니다.

250
00:10:22,490 --> 00:10:25,610
언어 모델의 경우, 해당 모델을 재훈련할 필요가 없습니다.

251
00:10:25,610 --> 00:10:28,170
새로운 다운스트림 작업에 대해 재훈련할 필요가 없습니다.

252
00:10:28,170 --> 00:10:30,810
모든 작업은 언어 작업이며, 모든

253
00:10:30,810 --> 00:10:34,970
작업은 이 자동 완성 프로세스로 처리될 수 있습니다.

254
00:10:34,970 --> 00:10:37,290
하지만 CLIP의 경우, 문제는 자동

255
00:10:37,290 --> 00:10:39,410
완성 프로세스가 없다는 것입니다.

256
00:10:39,410 --> 00:10:42,240
우리는 이 대조 목표로 모델을 훈련했지만,

257
00:10:42,240 --> 00:10:45,760
새로운 작업에 적응하려면 여전히 훈련 데이터가

258
00:10:45,760 --> 00:10:48,900
필요하고, 새로운 작업에 적응하기 위해

259
00:10:48,900 --> 00:10:51,442
훈련해야 할 선형 레이어가 필요합니다.

260
00:10:51,442 --> 00:10:52,900
그래서 많은 사람들이

261
00:10:52,900 --> 00:10:59,060
이 모델을 상자에서 직접 사용할 수 있도록 적응시키기 위해 무엇을 할 수
있을지 고민하기

262
00:10:59,060 --> 00:11:00,100
시작했습니다.

263
00:11:00,100 --> 00:11:02,800
사람들이 생각해낸 이 기발한 트릭이 있습니다.

264
00:11:02,800 --> 00:11:06,900
이 기발한 트릭은 기본적으로 텍스트 인코더를

265
00:11:06,900 --> 00:11:11,020
사용하여 모델이 모든 다운스트림 분류 작업에

266
00:11:11,020 --> 00:11:13,480
일반화하도록 안내하는 것입니다.

267
00:11:13,480 --> 00:11:14,613
작동 방식은 이렇습니다.

268
00:11:14,613 --> 00:11:16,780
예를 들어, CLIP 모델을 사용하여 이

269
00:11:16,780 --> 00:11:20,260
이미지를 분류하고 싶지만, 이 모델을 재훈련하거나 다운스트림 작업에

270
00:11:20,260 --> 00:11:22,600
적응하고 싶지 않다고 가정해 보겠습니다.

271
00:11:22,600 --> 00:11:26,660
할 수 있는 것은 텍스트 인코더를 사용하여

272
00:11:26,660 --> 00:11:29,380
단어를 입력하고 텍스트 벡터를

273
00:11:29,380 --> 00:11:33,740
생성한 다음, 최근접 이웃을 사용하여 올바른 분류를

274
00:11:33,740 --> 00:11:35,460
찾는 것입니다.

275
00:11:35,460 --> 00:11:38,300
이 방식은 새로운 데이터 세트의 모든 카테고리를

276
00:11:38,300 --> 00:11:39,405
가져오는 것입니다.

277
00:11:39,405 --> 00:11:41,030
예를 들어, 새로운 데이터 세트에

278
00:11:41,030 --> 00:11:44,430
평범한, 개, 새라는 카테고리가 포함되어 있다고 가정해 보겠습니다.

279
00:11:44,430 --> 00:11:47,230
이 모든 것을 텍스트 공간에

280
00:11:47,230 --> 00:11:50,350
임베드하여 평범한 벡터, 개 벡터, 새

281
00:11:50,350 --> 00:11:52,030
벡터를 얻습니다.

282
00:11:52,030 --> 00:11:54,870
이제 새로운 이미지가 들어오면, 그 이미지를

283
00:11:54,870 --> 00:11:57,870
이미지 인코더를 사용하여 임베드한 다음 가장

284
00:11:57,870 --> 00:11:59,990
가까운 이웃을 찾기만 하면 됩니다.

285
00:11:59,990 --> 00:12:02,630
이 경우, 이 이미지가 올바른 클래스와

286
00:12:02,630 --> 00:12:06,870
가장 높은 유사성을 가진다는 것을 알 수 있어야 합니다.

287
00:12:06,870 --> 00:12:09,325
이 경우, 개 벡터여야 합니다.

288
00:12:09,325 --> 00:12:11,950
개 벡터가 가장 높은 유사성 점수를 가지고 있다는 것을 볼 수

289
00:12:11,950 --> 00:12:12,410
있습니다.

290
00:12:12,410 --> 00:12:13,868
그래서 그로 인해

291
00:12:13,868 --> 00:12:17,780
이제 그 이미지를 개로 분류할 수 있습니다.

292
00:12:17,780 --> 00:12:19,870
이 전체 프로세스를 본질적으로

293
00:12:19,870 --> 00:12:23,930
하나의 이웃 알고리즘을 구축하는 것으로 생각할 수 있습니다.

294
00:12:23,930 --> 00:12:27,510
당신은 기술 공간에서 생성한 여러 중심 또는

295
00:12:27,510 --> 00:12:30,090
임베딩을 가지고 있으며, 이제 그것들을

296
00:12:30,090 --> 00:12:33,288
클래스 카테고리 레이블로 사용할 수

297
00:12:33,288 --> 00:12:34,830
있고, 새로운 이미지가

298
00:12:34,830 --> 00:12:38,680
들어올 때 최적의 분류를 찾기 위해 하나의 최근접

299
00:12:38,680 --> 00:12:40,640
이웃을 수행하고 있습니다.

300
00:12:40,640 --> 00:12:43,960
물론, 단어 하나만으로는 정말 좋은 단어

301
00:12:43,960 --> 00:12:47,040
벡터를 얻기에 충분하지 않을 수 있습니다.

302
00:12:47,040 --> 00:12:49,693
대신, 구문을 사용하는 것이 좋습니다.

303
00:12:49,693 --> 00:12:51,360
이렇게 해야 하는

304
00:12:51,360 --> 00:12:54,040
이유는 많은 인터넷 데이터가 보통

305
00:12:54,040 --> 00:12:57,100
혼자 발생하는 단어가 없기 때문입니다.

306
00:12:57,100 --> 00:13:00,200
CLIP은 인터넷에서 다운로드한 구문으로

307
00:13:00,200 --> 00:13:01,573
훈련되었습니다.

308
00:13:01,573 --> 00:13:04,240
그래서 이상적으로는 최상의 표현을 제공하는

309
00:13:04,240 --> 00:13:06,680
올바른 구문을 선택하는 것이 좋습니다.

310
00:13:06,680 --> 00:13:10,180
그래서 단순히 평범한, 개, 새 카테고리만 있는

311
00:13:10,180 --> 00:13:12,320
대신, 비행기 사진, 개 사진을

312
00:13:12,320 --> 00:13:15,720
나타내는 벡터를 임베드하고 싶을 수 있습니다.

313
00:13:15,720 --> 00:13:19,040
이 작은 변화를 주면

314
00:13:19,040 --> 00:13:21,640
ImageNet에서 약 1.

315
00:13:21,640 --> 00:13:24,800
3%의 개선을 보게 됩니다.

316
00:13:24,800 --> 00:13:26,640
물론, 올바른 구문을 선택하는

317
00:13:26,640 --> 00:13:28,820
것도 매우 어려운 일입니다.

318
00:13:28,820 --> 00:13:30,903
그래서 사람들이 일반적으로 하는 것은 단일 구문을

319
00:13:30,903 --> 00:13:33,400
선택하는 것이 아니라 여러 다른 구문을 선택하는 것입니다.

320
00:13:33,400 --> 00:13:35,970
개 사진, 개 그림 또는 다양한

321
00:13:35,970 --> 00:13:39,290
구문에 대한 여러 아이디어를 선택하고, 생각할

322
00:13:39,290 --> 00:13:41,610
수 있는 모든 다양한 구문에

323
00:13:41,610 --> 00:13:44,730
대해 여러 다른 벡터를 생성하고 싶습니다.

324
00:13:44,730 --> 00:13:48,130
결국, 각 카테고리에 대해 모든 구문을

325
00:13:48,130 --> 00:13:52,330
통해 평균 벡터 표현을 취하고, 이를

326
00:13:52,330 --> 00:13:55,970
평균 개 벡터, 평균 비행기 벡터 및

327
00:13:55,970 --> 00:13:58,113
평균 벡터로 사용합니다.

328
00:13:58,113 --> 00:14:00,030
그리고 이제 당신은 시작했던

329
00:14:00,030 --> 00:14:03,930
곳으로 돌아왔고, 여기서 같은 최근접 이웃 알고리즘을 적용할

330
00:14:03,930 --> 00:14:05,090
수 있습니다.

331
00:14:05,090 --> 00:14:08,370
아마도 ImageNet에서 훈련되었을 것입니다.

332
00:14:08,370 --> 00:14:10,690
이것은 새로운 작업에 적응할 수 있다는

333
00:14:10,690 --> 00:14:12,630
점을 보여주는 것이라고 생각합니다.

334
00:14:12,630 --> 00:14:14,680
하지만 제가 훈련되지 않은

335
00:14:14,680 --> 00:14:16,430
데이터 세트의 다른 예를

336
00:14:16,430 --> 00:14:19,450
보여드릴 것이고, 그것도 잘 적응합니다.

337
00:14:19,450 --> 00:14:21,442
그래서 단일 벡터가 출력되며,

338
00:14:21,442 --> 00:14:23,150
사용하는 아키텍처에 따라 다릅니다.

339
00:14:23,150 --> 00:14:26,530
ResNet을 사용하는 경우, 최종 벡터 표현을 가져옵니다.

340
00:14:26,530 --> 00:14:29,950
텍스트 인코더가 VIT 또는 트랜스포머라고

341
00:14:29,950 --> 00:14:35,980
가정하면, 일반적으로 트랜스포머의 CLS 토큰을 가져옵니다.

342
00:14:35,980 --> 00:14:38,160
그래서 CLIP에 대한 내용은 여기까지입니다.

343
00:14:38,160 --> 00:14:41,540
기본적으로 다양한 새로운 이미지 분류 작업에

344
00:14:41,540 --> 00:14:42,880
적응할 수 있습니다.

345
00:14:42,880 --> 00:14:45,480
현재 질문에 대해, ImageNet에서

346
00:14:45,480 --> 00:14:48,180
잘 수행된다는 것이 그리 큰 문제가

347
00:14:48,180 --> 00:14:51,340
아니지만, ImageNet에서 잘 수행한다는

348
00:14:51,340 --> 00:14:53,540
것이 여전히 흥미롭습니다.

349
00:14:53,540 --> 00:14:57,420
더 흥미로운 점은 CLIP이 출시된 이후에 수집된

350
00:14:57,420 --> 00:15:00,520
다른 데이터 세트를 살펴보면 알 수 있습니다.

351
00:15:00,520 --> 00:15:02,900
예를 들어, 사람들이 매우 이상한 장소에서

352
00:15:02,900 --> 00:15:06,400
찍은 물체를 포함하는 ObjectNet과 같은 데이터 세트입니다.

353
00:15:06,400 --> 00:15:08,960
그래서 그들은 바나나를 바닥에 놓고

354
00:15:08,960 --> 00:15:10,940
사진을 찍거나, 정말 썩은

355
00:15:10,940 --> 00:15:12,340
바나나를 찍었습니다.

356
00:15:12,340 --> 00:15:14,660
즉, 일반적이지 않은 것들입니다.

357
00:15:14,660 --> 00:15:18,440
이 데이터 세트에서 ImageNet으로 훈련하면 잘 되지 않는데,

358
00:15:18,440 --> 00:15:21,120
ImageNet은 다시 말해 이러한 카테고리의

359
00:15:21,120 --> 00:15:22,700
대부분을 가장 전형적인

360
00:15:22,700 --> 00:15:24,780
형태로 포함하고 있기 때문입니다.

361
00:15:24,780 --> 00:15:28,740
하지만 CLIP 모델을 사용하면 동일한 성능을 발휘합니다.

362
00:15:28,740 --> 00:15:31,420
그리고 이것은 많은 사람들에게 정말

363
00:15:31,420 --> 00:15:33,030
흥미로웠습니다. 이전에 본

364
00:15:33,030 --> 00:15:35,030
적이 없는 완전히 새로운

365
00:15:35,030 --> 00:15:38,150
데이터 세트에 일반화할 수 있는 능력은 정말

366
00:15:38,150 --> 00:15:38,858
대단했습니다.

367
00:15:38,858 --> 00:15:40,025
그래서 왜 이렇게 생각하나요?

368
00:15:40,025 --> 00:15:42,110
왜 CLIP이 ImageNet에서 훈련하는 것보다

369
00:15:42,110 --> 00:15:43,870
훨씬 더 잘 일반화된다고 생각하나요?

370
00:15:43,870 --> 00:15:45,670
당신의 응답을 바꾸어 말하자면,

371
00:15:45,670 --> 00:15:49,510
제가 생각하기에 그것이 올바른 응답입니다. 인터넷에서 다운로드한

372
00:15:49,510 --> 00:15:51,090
텍스트는 카테고리 레이블보다

373
00:15:51,090 --> 00:15:53,090
훨씬 더 많은 것을 포함합니다.

374
00:15:53,090 --> 00:15:55,050
형태, 색상에 대한

375
00:15:55,050 --> 00:15:56,670
정보 등 구조적 정보가

376
00:15:56,670 --> 00:15:59,790
훨씬 더 많이 포함되어 있으며,

377
00:15:59,790 --> 00:16:02,810
이 모든 것이 표현에 추가됩니다.

378
00:16:02,810 --> 00:16:05,790
그래서 이러한 모델은 약간의 분포에서 벗어나거나

379
00:16:05,790 --> 00:16:08,790
약간 다른 모습의 객체에 훨씬 더 잘 적응할

380
00:16:08,790 --> 00:16:10,430
수 있습니다. 왜냐하면

381
00:16:10,430 --> 00:16:13,470
그들이 찾고 있는 다른 것들도 있기 때문입니다.

382
00:16:13,470 --> 00:16:15,670
따라서 이러한 추가적인 감독은 정말

383
00:16:15,670 --> 00:16:17,150
많은 도움이 됩니다.

384
00:16:17,150 --> 00:16:20,290
또 다른 이유는 데이터의 규모입니다.

385
00:16:20,290 --> 00:16:23,610
ImageNet은 약 130만 개의 이미지에 불과하지만,

386
00:16:23,610 --> 00:16:26,288
인터넷에는 수백만, 수십억 개가 있습니다.

387
00:16:26,288 --> 00:16:27,830
현재 수십억 개의 이미지-텍스트

388
00:16:27,830 --> 00:16:30,250
쌍을 매우 쉽게 다운로드할 수 있습니다.

389
00:16:30,250 --> 00:16:33,840
그래서 이러한 모델은 훨씬 더 많은 데이터를 보았기

390
00:16:33,840 --> 00:16:36,605
때문에 이 적응이 훨씬 쉬워집니다.

391
00:16:36,605 --> 00:16:38,480
그래서 사람들은 다양한 일반화

392
00:16:38,480 --> 00:16:41,115
작업에 대한 실험을 시작했습니다.

393
00:16:41,115 --> 00:16:43,240
그들은 이러한 모델이 자연 이미지뿐만

394
00:16:43,240 --> 00:16:46,800
아니라 스케치에서도 일반화할 수 있음을 보여주었고, 적대적 데이터

395
00:16:46,800 --> 00:16:49,860
세트에서도 이 작업을 수행할 수 있음을 보여주었습니다.

396
00:16:49,860 --> 00:16:51,760
전반적으로 성능은 이러한

397
00:16:51,760 --> 00:16:53,860
모델이 정말로 뛰어나고 다양한

398
00:16:53,860 --> 00:16:57,777
응용 프로그램에 강건하다는 것을 나타내는 것 같습니다.

399
00:16:57,777 --> 00:17:00,360
그리고 여기서 저는 제로 샷과 선형 프로브의 차이를

400
00:17:00,360 --> 00:17:01,275
보여주고 있습니다.

401
00:17:01,275 --> 00:17:03,400
물론 선형 프로브는 추가적인

402
00:17:03,400 --> 00:17:05,800
선형 분류기를 추가하고 약간 훈련하여

403
00:17:05,800 --> 00:17:07,760
적응시키면 대부분의 데이터

404
00:17:07,760 --> 00:17:09,960
세트에서 성능이 향상됩니다.

405
00:17:09,960 --> 00:17:13,079
녹색으로 표시된 데이터 세트에서 그렇지만 항상

406
00:17:13,079 --> 00:17:14,460
그런 것은 아닙니다.

407
00:17:14,460 --> 00:17:16,880
경우에 따라 CLIP 제로샷은

408
00:17:16,880 --> 00:17:19,720
바로 사용해도 정말 잘 작동합니다.

409
00:17:19,720 --> 00:17:21,920
그래서 우리는 이미지 인코더를

410
00:17:21,920 --> 00:17:23,880
다양한 다운스트림 작업에

411
00:17:23,880 --> 00:17:27,599
적응할 수 있는 능력을 드디어 열었다는 것을

412
00:17:27,599 --> 00:17:29,025
나타내는 것 같았습니다.

413
00:17:29,025 --> 00:17:30,650
그래서 많은 사람들이

414
00:17:30,650 --> 00:17:35,050
CLIP을 이미지의 첫 번째 기초 모델로 이야기하는 이유입니다.

415
00:17:35,050 --> 00:17:37,610
그럼 CLIP이 잘 작동하는 이유에 대해 이야기해봅시다.

416
00:17:37,610 --> 00:17:40,370
물론 CLIP에는 실제 레이블이 없습니다.

417
00:17:40,370 --> 00:17:43,430
우리는 이미지와 관련된 텍스트를 다운로드하고 있습니다.

418
00:17:43,430 --> 00:17:45,583
CLIP이 잘 작동하는

419
00:17:45,583 --> 00:17:47,750
이유는 처음 훈련될 때

420
00:17:47,750 --> 00:17:51,010
포함된 것이라고 제가 말했던 것이며,

421
00:17:51,010 --> 00:17:53,270
매개변수가 거대했습니다.

422
00:17:53,270 --> 00:17:55,090
모델을 확장하고 아키텍처를

423
00:17:55,090 --> 00:17:58,290
ResNet에서 VIT로 변경했습니다.

424
00:17:58,290 --> 00:18:00,370
그래서 3억 7천만 개의

425
00:18:00,370 --> 00:18:05,510
매개변수를 가진 이 변환기 아키텍처로 모델을 훈련했습니다.

426
00:18:05,510 --> 00:18:08,230
두 번째로 도움이 된 것은 데이터의 양입니다.

427
00:18:08,230 --> 00:18:11,230
ImageNet의 120만 개 이미지

428
00:18:11,230 --> 00:18:14,090
대신, 인터넷에서 다운로드한 약

429
00:18:14,090 --> 00:18:15,810
4억 개의 이미지-텍스트

430
00:18:15,810 --> 00:18:17,270
쌍이 있었습니다.

431
00:18:17,270 --> 00:18:19,850
모델 크기와 데이터 양

432
00:18:19,850 --> 00:18:23,050
모두에서 이 규모가 성능을 크게

433
00:18:23,050 --> 00:18:24,210
향상시켰습니다.

434
00:18:24,210 --> 00:18:26,410
CLIP이 출시된 직후, 사람들은

435
00:18:26,410 --> 00:18:28,680
이 목표로 실험을 시작했습니다.

436
00:18:28,680 --> 00:18:31,540
수년 동안 다양한 CLIP

437
00:18:31,540 --> 00:18:33,040
변형이 나왔습니다.

438
00:18:33,040 --> 00:18:35,460
하지만 특히 두드러진 하나는

439
00:18:35,460 --> 00:18:38,220
2022년에 나온 CoCa입니다.

440
00:18:38,220 --> 00:18:40,140
CoCa는 CLIP 모델을 사용했습니다.

441
00:18:40,140 --> 00:18:42,322
여기에서 동일한 목표를 볼 수 있습니다.

442
00:18:42,322 --> 00:18:44,280
한쪽에서는 이미지가 인코딩되고 있습니다.

443
00:18:44,280 --> 00:18:46,240
다른 쪽에서는 텍스트가 인코딩되고 있습니다.

444
00:18:46,240 --> 00:18:47,865
그리고 두 가지 사이에 대조

445
00:18:47,865 --> 00:18:48,820
손실이 있습니다.

446
00:18:48,820 --> 00:18:50,820
하지만 그들은 하나의 추가 요소를 추가했습니다.

447
00:18:50,820 --> 00:18:53,660
그들은 이미지 인코더에서 이미지

448
00:18:53,660 --> 00:18:56,540
특징을 가져오는 디코더를 추가했습니다.

449
00:18:56,540 --> 00:18:59,420
그리고 그것을 교차 주의를 통해

450
00:18:59,420 --> 00:19:01,620
입력하여 이미지를 캡션했습니다.

451
00:19:01,620 --> 00:19:04,180
이 캡션 프로세스가 모델이 많은 풍부한

452
00:19:04,180 --> 00:19:07,560
정보를 학습하는 데 도움이 된다는 것이 밝혀졌습니다.

453
00:19:07,560 --> 00:19:09,020
여기서 일반적인 동기는

454
00:19:09,020 --> 00:19:11,500
단순히 이것이 고양이의 이미지인지 개의

455
00:19:11,500 --> 00:19:14,360
이미지인지 말하는 것으로는 충분하지 않으며, 그

456
00:19:14,360 --> 00:19:16,460
이미지를 텍스트로 설명하려면 모델이

457
00:19:16,460 --> 00:19:19,760
학습해야 할 정보가 더 많이 필요하다는 것입니다.

458
00:19:19,760 --> 00:19:22,660
그래서 가설은 이것이 더 강력한 학습 목표라는

459
00:19:22,660 --> 00:19:23,580
것입니다.

460
00:19:23,580 --> 00:19:26,830
그로 인해 더 나은 특징을 학습합니다.

461
00:19:26,830 --> 00:19:29,350
전반적으로 이것이 사실임을 발견했습니다.

462
00:19:29,350 --> 00:19:32,310
CoCa와 CLIP을 비교할 때, 모든

463
00:19:32,310 --> 00:19:36,230
다양한 ImageNet 변형에서 성능이 크게 향상됩니다.

464
00:19:36,230 --> 00:19:38,790
전반적으로 모든 데이터 세트에서

465
00:19:38,790 --> 00:19:41,350
성능이 10% 향상되었습니다.

466
00:19:41,350 --> 00:19:44,950
이것이 기초 모델들이 감독 학습에서 훈련한

467
00:19:44,950 --> 00:19:48,590
모든 모델을 실제로 이긴 첫 번째

468
00:19:48,590 --> 00:19:50,030
사례라고 생각합니다.

469
00:19:50,030 --> 00:19:52,070
이 시점에서 사람들은 온라인

470
00:19:52,070 --> 00:19:55,190
리더보드에 다양한 모델을 올리고 있었습니다.

471
00:19:55,190 --> 00:19:58,550
그리고 그 수년 간의 리더보드에서 모델들이 점점

472
00:19:58,550 --> 00:20:00,910
더 잘 수행하는 경향이 상승하고

473
00:20:00,910 --> 00:20:02,770
있음을 볼 수 있습니다.

474
00:20:02,770 --> 00:20:04,395
그리고 이것이 제가 생각하기에

475
00:20:04,395 --> 00:20:07,270
사람들이 이미지 인코더에 대한 감독

476
00:20:07,270 --> 00:20:09,750
학습 목표를 포기하고 대신 인터넷 데이터의

477
00:20:09,750 --> 00:20:11,950
자기 감독 학습 방법을 사용하여

478
00:20:11,950 --> 00:20:15,630
사전 훈련 목표에만 집중하게 된 전환점이라고 생각합니다.

479
00:20:15,630 --> 00:20:17,810
그럼 CLIP의 몇 가지 장점에 대해 이야기해봅시다.

480
00:20:17,810 --> 00:20:20,510
CLIP에는 정말 재미있는 것들이 많이 있습니다.

481
00:20:20,510 --> 00:20:24,000
훈련이 매우 간단한 대조 학습 목표이기 때문에

482
00:20:24,000 --> 00:20:25,080
매우 쉽습니다.

483
00:20:25,080 --> 00:20:27,640
추론 측면에서도 정말 빠릅니다.

484
00:20:27,640 --> 00:20:32,300
전체 데이터 세트를 어떤 표현으로 임베드할 수 있고,

485
00:20:32,300 --> 00:20:34,240
분류를 위해서는 그

486
00:20:34,240 --> 00:20:38,560
임베드된 데이터 세트에서 검색만 하면 됩니다.

487
00:20:38,560 --> 00:20:40,280
따라서 CLIP의 표현을

488
00:20:40,280 --> 00:20:41,960
사용하면 사전

489
00:20:41,960 --> 00:20:44,320
작업뿐만 아니라 검색 및 검색

490
00:20:44,320 --> 00:20:47,918
작업에도 매우 유용하게 사용할 수 있습니다.

491
00:20:47,918 --> 00:20:49,960
사람들이 CLIP에 대해 정말 좋아했던 또 다른

492
00:20:49,960 --> 00:20:51,220
점은 개방형 어휘라는 것입니다.

493
00:20:51,220 --> 00:20:53,145
어떤 텍스트 설명도 입력할 수 있으며,

494
00:20:53,145 --> 00:20:54,520
그러면 올바른

495
00:20:54,520 --> 00:20:56,200
이미지를 검색할 수 있어야 합니다.

496
00:20:56,200 --> 00:20:58,920
그래서 이것은 다양한 도메인에서의

497
00:20:58,920 --> 00:21:01,213
적용 가능성을 허용합니다.

498
00:21:01,213 --> 00:21:03,380
물론, 우리는 나중에 이에 대해 이야기할 것입니다.

499
00:21:03,380 --> 00:21:07,380
CLIP은 다른 모델과 연결하기에 정말 적합합니다.

500
00:21:07,380 --> 00:21:10,025
그리고 이 연결 아이디어는 정말 인기를 끌기 시작했습니다.

501
00:21:10,025 --> 00:21:10,900
하지만 그건 잠시 미뤄두세요.

502
00:21:10,900 --> 00:21:13,982
몇 분 후에 그에 대해 이야기하겠습니다.

503
00:21:13,982 --> 00:21:15,940
물론, 저는 모든 좋은 것들에 대해 이야기하고 있습니다.

504
00:21:15,940 --> 00:21:18,520
알고 보니 나쁜 점도 많습니다.

505
00:21:18,520 --> 00:21:20,520
불행히도 CLIP은 이 두

506
00:21:20,520 --> 00:21:22,200
이미지를 구별할 수 없습니다.

507
00:21:22,200 --> 00:21:25,380
그래서 잔디 위의 머그컵 이미지와

508
00:21:25,380 --> 00:21:27,720
머그컵 안의 잔디가

509
00:21:27,720 --> 00:21:30,880
있는데, CLIP은 이 두 가지의

510
00:21:30,880 --> 00:21:33,070
차이를 알지 못합니다.

511
00:21:33,070 --> 00:21:37,200
그것이 알지 못하는 이유는 CLIP의 학습 목표가

512
00:21:37,200 --> 00:21:40,720
배치 크기에 정말 의존하기 때문입니다.

513
00:21:40,720 --> 00:21:43,400
배치 크기가 충분히 크지 않으면

514
00:21:43,400 --> 00:21:45,240
다른 배치 요소들이

515
00:21:45,240 --> 00:21:48,760
모델에 유용한 감독을 제공할 가능성이

516
00:21:48,760 --> 00:21:49,720
낮습니다.

517
00:21:49,720 --> 00:21:52,380
항상 고양이와 트럭을 비교하고 있다면,

518
00:21:52,380 --> 00:21:56,160
고양이에 대한 표현을 배우지 못할 것입니다.

519
00:21:56,160 --> 00:21:59,280
대신, 얻는 것은 어느 정도 높은 수준에서 괜찮은

520
00:21:59,280 --> 00:22:02,157
표현이지만, 배치 크기를 늘리면 고양이와

521
00:22:02,157 --> 00:22:04,240
유사한 다른 동물들을 더 많이 접하게

522
00:22:04,240 --> 00:22:05,800
되어 훨씬 더 나은

523
00:22:05,800 --> 00:22:07,393
표현을 배울 수 있습니다.

524
00:22:07,393 --> 00:22:09,560
그리고 물론, 배치 크기를

525
00:22:09,560 --> 00:22:15,000
32,000으로 늘리고 많은 GPU에서 훈련하면, 갑자기 정말

526
00:22:15,000 --> 00:22:17,860
좋은 표현을 배우기 시작합니다.

527
00:22:17,860 --> 00:22:19,770
실제로 웰시 코기와 다른 코기를

528
00:22:19,770 --> 00:22:21,530
구별하기 시작할 수 있습니다.

529
00:22:21,530 --> 00:22:24,670
그리고 이것은 거대한 배치 크기가 있을

530
00:22:24,670 --> 00:22:27,330
때만 가능하며, 이는 모델이 학습하도록

531
00:22:27,330 --> 00:22:29,490
강제하는 충분히 가까운

532
00:22:29,490 --> 00:22:33,350
어려운 부정 예제가 배치에 있어야 하기 때문입니다.

533
00:22:33,350 --> 00:22:34,850
그래서 이것은 이러한 모델이

534
00:22:34,850 --> 00:22:37,250
잘 작동하도록 하는 데 매우 중요합니다.

535
00:22:37,250 --> 00:22:39,450
하지만 불행히도 사람들이 아무리

536
00:22:39,450 --> 00:22:41,690
노력해도 배치 크기를 늘린다고 해서

537
00:22:41,690 --> 00:22:45,090
모델이 사물에 대한 좋은 표현을 배울 것이라는

538
00:22:45,090 --> 00:22:46,030
보장은 없습니다.

539
00:22:46,030 --> 00:22:49,730
그래서 당신은 훈련 데이터의 무작위성에

540
00:22:49,730 --> 00:22:51,090
의존하게 됩니다.

541
00:22:51,090 --> 00:22:52,650
배치 크기를 늘리는 것은

542
00:22:52,650 --> 00:22:54,790
세밀한 개념에 어느 정도 도움이 됩니다.

543
00:22:54,790 --> 00:22:56,570
하지만 물론 여전히 한계가 있습니다.

544
00:22:56,570 --> 00:23:01,930
32,000의 배치 크기로 훈련하는 것은 대부분의

545
00:23:01,930 --> 00:23:06,330
연구실에서 고려하기에는 너무 큽니다.

546
00:23:06,330 --> 00:23:08,410
사람들은 여러 다른 벤치마크에서

547
00:23:08,410 --> 00:23:11,690
이 오류를 확인하고 CLIP이 조합성

548
00:23:11,690 --> 00:23:14,810
개념을 가지고 있지 않다는 것을 지적했습니다.

549
00:23:14,810 --> 00:23:17,570
컵과 잔디, 잔디가 컵 안에 있는 것에

550
00:23:17,570 --> 00:23:18,960
대한 아이디어입니다.

551
00:23:18,960 --> 00:23:21,240
컵과 잔디와 같은 서로

552
00:23:21,240 --> 00:23:24,380
다른 개념을 조합하는 것이며, 이러한

553
00:23:24,380 --> 00:23:26,900
개별 구성 요소 간의

554
00:23:26,900 --> 00:23:31,303
관계가 CLIP 표현에서 잘 조합되지 않았습니다.

555
00:23:31,303 --> 00:23:33,220
Winoground, CREPE,

556
00:23:33,220 --> 00:23:36,180
ARO와 같은 다양한 벤치마크가 있었습니다.

557
00:23:36,180 --> 00:23:39,620
이러한 벤치마크의 많은 부분이 제 연구실에서 나왔습니다.

558
00:23:39,620 --> 00:23:42,260
그들은 CLIP이 많은 한계를 가지고 있으며,

559
00:23:42,260 --> 00:23:44,900
그들이 할 수 없는 것들이 많다는

560
00:23:44,900 --> 00:23:46,980
것을 반복해서 발견하고 있습니다.

561
00:23:46,980 --> 00:23:49,620
물론 이에 대한 반응으로, 커뮤니티는 어려운

562
00:23:49,620 --> 00:23:53,900
부정 사례를 포함하도록 배치를 수작업으로 만드는 방법에 대해

563
00:23:53,900 --> 00:23:57,620
즉시 생각하기 시작했습니다. 예를 들어, 한 종류의 코기와

564
00:23:57,620 --> 00:24:00,400
함께 다른 종류의 코기를 포함해야 합니다.

565
00:24:00,400 --> 00:24:03,940
그래서 모델은 좋은 표현을 배우도록 강요받습니다.

566
00:24:03,940 --> 00:24:06,660
어려운 부정 사례로 훈련하는 아이디어는

567
00:24:06,660 --> 00:24:10,100
커뮤니티에서 1년 동안 매우 인기를 끌었습니다.

568
00:24:10,100 --> 00:24:12,940
우리가 어려운 부정 사례로

569
00:24:12,940 --> 00:24:14,760
훈련하면 의미론에 대한 많은

570
00:24:14,760 --> 00:24:18,720
것을 잊게 된다는 후속 논문을 발표할 때까지였습니다.

571
00:24:18,720 --> 00:24:20,470
어떤 이유에서인지, 그리고

572
00:24:20,470 --> 00:24:22,550
이것은 우리가 아직 이론적으로

573
00:24:22,550 --> 00:24:25,470
이해하지 못하는 부분인데, 우리는 다양한 환경과

574
00:24:25,470 --> 00:24:28,630
다양한 데이터 세트에서 일반화 성능이 훨씬

575
00:24:28,630 --> 00:24:30,510
나빠지는 결과를 얻게 됩니다.

576
00:24:30,510 --> 00:24:32,790
따라서 데이터 세트를 구성하는

577
00:24:32,790 --> 00:24:35,855
올바른 방법, 배치와 훈련 신호를 구성하는 올바른

578
00:24:35,855 --> 00:24:38,230
방법을 찾는 데 여전히 많은

579
00:24:38,230 --> 00:24:39,590
작업이 남아 있습니다.

580
00:24:39,590 --> 00:24:41,370
그래서 우리는 아직 그 목표에서 많이 멀리 있습니다.

581
00:24:41,370 --> 00:24:44,510
하지만 그럼에도 불구하고 사람들은 CLIP에 대해 여전히

582
00:24:44,510 --> 00:24:48,190
매우 흥미를 느끼고 있습니다. 왜냐하면 그것은 어느 정도의

583
00:24:48,190 --> 00:24:49,830
감독을 제공하기 때문입니다.

584
00:24:49,830 --> 00:24:54,630
물론 다시 말하지만, 이미지 수준의 캡션은 여전히 부족합니다.

585
00:24:54,630 --> 00:24:57,030
이상적으로 우리가 원하는 것은 그 이상입니다.

586
00:24:57,030 --> 00:24:59,310
우리는 단순히 사람이 길을 건너고 있다는 것을

587
00:24:59,310 --> 00:25:01,670
식별하는 것이 아니라, 그 사람이 이 위치에 있고,

588
00:25:01,670 --> 00:25:04,910
차가 여기 있으며, 길이 여기 있다는 것을 식별할 수 있기를 원합니다.

589
00:25:04,910 --> 00:25:07,110
이 모든 정보, 즉 기초 정보가

590
00:25:07,110 --> 00:25:08,990
CLIP에서 완전히 누락되어 있습니다.

591
00:25:08,990 --> 00:25:11,990
따라서 이상적으로는 데이터 세트가 이러한

592
00:25:11,990 --> 00:25:14,453
종류의 정보를 포함하고, 모델이

593
00:25:14,453 --> 00:25:16,120
이러한 종류의 정보를

594
00:25:16,120 --> 00:25:18,160
추론할 수 있기를 원합니다.

595
00:25:18,160 --> 00:25:21,840
또한 CLIP의 큰 단점 중 하나는 데이터 세트의

596
00:25:21,840 --> 00:25:24,300
크기와 관계없이, 예를 들어

597
00:25:24,300 --> 00:25:27,900
50억 개의 이미지를 수집하더라도, 당신이 중요하게

598
00:25:27,900 --> 00:25:29,880
생각할 수 있는 모든

599
00:25:29,880 --> 00:25:33,260
것을 포착하기에는 여전히 부족하다는 것입니다.

600
00:25:33,260 --> 00:25:34,760
그래서 우리는 데이터 필터링에

601
00:25:34,760 --> 00:25:36,740
많은 노력을 기울이고 있습니다.

602
00:25:36,740 --> 00:25:38,200
인터넷을 필터링하여 이러한

603
00:25:38,200 --> 00:25:41,660
CLIP 모델을 훈련하기 위한 최상의 훈련 데이터를 찾는 방법은 무엇인가요?

604
00:25:41,660 --> 00:25:43,840
오늘은 그 부분에 대해 이야기하지

605
00:25:43,840 --> 00:25:47,400
않겠지만, 사람들이 탐색하고 있는 모든

606
00:25:47,400 --> 00:25:49,680
메커니즘이 현재 이 분야의

607
00:25:49,680 --> 00:25:51,480
연구 최전선이 되었습니다.

608
00:25:51,480 --> 00:25:54,400
좋습니다, 우리가 이야기한 첫 번째 기초 모델의

609
00:25:54,400 --> 00:25:55,300
가지입니다.

610
00:25:55,300 --> 00:25:57,440
이는 분류를 다양한 작업으로

611
00:25:57,440 --> 00:25:59,260
일반화하는 것입니다.

612
00:25:59,260 --> 00:26:02,860
이제 비전 및 언어 모델에 대해 이야기해 보겠습니다.

613
00:26:02,860 --> 00:26:05,780
최근 2년, 2년 반 동안 인기를 끌고

614
00:26:05,780 --> 00:26:08,820
있는 새로운 기초 모델 클래스가 있으며,

615
00:26:08,820 --> 00:26:12,930
우리는 이를 다중 모달 언어 모델이라고 부릅니다.

616
00:26:12,930 --> 00:26:15,050
이 논의를 LLaVA에 초점을

617
00:26:15,050 --> 00:26:17,690
맞추어 시작하겠습니다. 이는

618
00:26:17,690 --> 00:26:20,250
매우 인기가 있었던 다중 모달

619
00:26:20,250 --> 00:26:22,210
언어 모델 중 하나입니다.

620
00:26:22,210 --> 00:26:24,970
여기서의 동기는 언어 모델이 다음

621
00:26:24,970 --> 00:26:28,830
토큰 예측, 즉 자동 완성 과정을 수행하는 것이며,

622
00:26:28,830 --> 00:26:31,930
이 과정은 많은 새로운 작업에 적응하는 데

623
00:26:31,930 --> 00:26:33,350
매우 유용합니다.

624
00:26:33,350 --> 00:26:37,830
그렇다면 이미지 모델도 같은 방식으로 생각할

625
00:26:37,830 --> 00:26:39,110
수 있을까요?

626
00:26:39,110 --> 00:26:41,010
이미지를 제공하고 이

627
00:26:41,010 --> 00:26:43,730
자기 회귀 과정과 유사한 다양한

628
00:26:43,730 --> 00:26:46,370
종류의 추론을 시작할 수 있을까요?

629
00:26:46,370 --> 00:26:49,210
이것이 비전 언어 모델 또는 다중 모달 모델이라는

630
00:26:49,210 --> 00:26:51,250
모델 클래스의 출현을 가져왔습니다.

631
00:26:51,250 --> 00:26:54,590
물론, 역사적으로 정확하게 말하자면, 이

632
00:26:54,590 --> 00:26:58,530
아이디어는 2022년에 완전히 새로운 것은 아니었습니다.

633
00:26:58,530 --> 00:27:02,730
2019년에 ViLBERT가 실제로 이 아이디어를 도입했습니다.

634
00:27:02,730 --> 00:27:05,250
2019년에 ViLBERT라는

635
00:27:05,250 --> 00:27:08,010
논문이 이러한 이미지

636
00:27:08,010 --> 00:27:12,620
모델과 언어 모델을 결합하여 다양한 작업에 대한

637
00:27:12,620 --> 00:27:15,340
일반화를 달성했지만, 모두

638
00:27:15,340 --> 00:27:20,580
변환기 이전에 훈련되었고 대부분 LSTM을 사용했습니다.

639
00:27:20,580 --> 00:27:23,220
그리고 이러한 모든 것의 재탄생이

640
00:27:23,220 --> 00:27:26,780
지금 LLaVA와 함께 일어나고 있으며,

641
00:27:26,780 --> 00:27:30,080
많은 모델이 더 나은 아키텍처로 전환하고

642
00:27:30,080 --> 00:27:33,160
더 나은 목표 세트로 전환하여 이제는

643
00:27:33,160 --> 00:27:35,640
개별 작업이 아니라 다양한 작업의

644
00:27:35,640 --> 00:27:39,600
기초 위에서 훈련하고 있으며, 인터넷에서 일부

645
00:27:39,600 --> 00:27:42,620
사전 훈련 목표를 사용하고 있습니다.

646
00:27:42,620 --> 00:27:43,560
그럼 이것은 어떻게 작동하나요?

647
00:27:43,560 --> 00:27:45,500
LLaVA에 대해 어떻게 생각하나요?

648
00:27:45,500 --> 00:27:49,260
LLaVA에 대해 이야기하기 위해, 한 걸음 물러서서

649
00:27:49,260 --> 00:27:53,340
변환기 모델 또는 특히 자기 주의(attention)에

650
00:27:53,340 --> 00:27:54,540
대해 생각해 봅시다.

651
00:27:54,540 --> 00:27:58,180
언어 모델에 대해 생각할 때, 그들이 하는 것은 과거에

652
00:27:58,180 --> 00:28:00,155
주의를 기울이는 것입니다.

653
00:28:00,155 --> 00:28:02,280
단어의 시퀀스가 들어오고 있습니다.

654
00:28:02,280 --> 00:28:04,980
예를 들어, '고양이는 너무'라고 하면,

655
00:28:04,980 --> 00:28:07,060
모델은 다음 단어를 생성하기

656
00:28:07,060 --> 00:28:09,830
위해 그 역사적 맥락에 주의를 기울이고

657
00:28:09,830 --> 00:28:12,470
다음 단어가 무엇이어야 하는지 생성합니다.

658
00:28:12,470 --> 00:28:14,230
그래서 '고양이는 너무 귀엽다'라는

659
00:28:14,230 --> 00:28:16,687
구문이 되어야 한다고 생각할 수 있습니다.

660
00:28:16,687 --> 00:28:18,270
그리고 동일한 목표를 표현하는

661
00:28:18,270 --> 00:28:19,770
또 다른 방법이 있습니다.

662
00:28:19,770 --> 00:28:22,090
입력 텍스트가 하단에서 들어오고,

663
00:28:22,090 --> 00:28:25,590
모델은 다음 단어인 '귀엽다'를 생성합니다.

664
00:28:25,590 --> 00:28:28,750
비전 언어 모델에 대해 생각할 때,

665
00:28:28,750 --> 00:28:32,710
사람들이 일반적으로 언급하는 것은 우리가 관심

666
00:28:32,710 --> 00:28:34,550
있는 이미지로 대화를

667
00:28:34,550 --> 00:28:37,910
기반으로 추가적인 맥락을 추가하는 것입니다.

668
00:28:37,910 --> 00:28:41,310
그래서 우리는 이미지를 어떤 식으로든

669
00:28:41,310 --> 00:28:44,070
토큰화하고, 그 토큰을 고양이와 같은

670
00:28:44,070 --> 00:28:46,890
역사적 맥락과 함께 언어 모델에

671
00:28:46,890 --> 00:28:49,350
입력하여 나머지 설명을 자동

672
00:28:49,350 --> 00:28:51,670
완성하는 데 사용할 수 있습니다.

673
00:28:51,670 --> 00:28:54,030
그래서 LLaVA의 기본 아이디어는

674
00:28:54,030 --> 00:28:57,270
이러한 이미지 토큰을 입력하고 생성되는 단어와

675
00:28:57,270 --> 00:28:59,470
함께 지속적으로 그 이미지에

676
00:28:59,470 --> 00:29:02,070
대한 더 많은 단어를 생성하는 것입니다.

677
00:29:02,070 --> 00:29:03,810
물론, 질문이 생깁니다.

678
00:29:03,810 --> 00:29:05,890
이 토큰들을 어떻게 정의하나요?

679
00:29:05,890 --> 00:29:08,600
이 토큰들은 원래 무엇이어야 하나요?

680
00:29:08,600 --> 00:29:12,960
LLaVA의 해결책은 CLIP 이미지 인코더를 사용하는 것이었습니다.

681
00:29:12,960 --> 00:29:16,200
그래서 그들은 CLIP 모델을 가져와서

682
00:29:16,200 --> 00:29:19,440
이미지 인코더를 사용하고, 그 인코더에서

683
00:29:19,440 --> 00:29:21,283
토큰을 추출했습니다.

684
00:29:21,283 --> 00:29:23,200
가장 먼저 생각할 수 있는 것은

685
00:29:23,200 --> 00:29:25,080
CLS 토큰을 사용하는 것입니다.

686
00:29:25,080 --> 00:29:26,520
여기에서 이미지가 들어오고

687
00:29:26,520 --> 00:29:28,800
패치가 생성되는 것을 볼 수 있습니다.

688
00:29:28,800 --> 00:29:29,300
오, 작동하네요.

689
00:29:29,300 --> 00:29:31,940
여기에서 이미지가 들어오고

690
00:29:31,940 --> 00:29:33,760
패치가 생성됩니다.

691
00:29:33,760 --> 00:29:35,840
각 패치는 CLIP의 변환기

692
00:29:35,840 --> 00:29:39,440
아키텍처에 공급되는 표현으로 변환됩니다.

693
00:29:39,440 --> 00:29:41,720
여러 처리 레이어를 거칩니다.

694
00:29:41,720 --> 00:29:44,080
그리고 마지막에 각 패치에

695
00:29:44,080 --> 00:29:47,000
대한 다양한 토큰과 CLS 토큰에 대한

696
00:29:47,000 --> 00:29:48,560
표현을 얻습니다.

697
00:29:48,560 --> 00:29:51,500
지금까지 우리는 CLS 토큰만 고려해왔습니다.

698
00:29:51,500 --> 00:29:53,720
우리는 어떤 분류 작업을 수행하기

699
00:29:53,720 --> 00:29:56,060
위해 CLS 토큰만 사용해왔습니다.

700
00:29:56,060 --> 00:29:59,400
하지만 그 안에는 다른 모든 토큰들도 있습니다.

701
00:29:59,400 --> 00:30:01,160
이 다른 토큰들의 문제는

702
00:30:01,160 --> 00:30:03,160
결코 감독되지 않는다는 것입니다.

703
00:30:03,160 --> 00:30:06,090
CLS 토큰은 텍스트와의 대조적 목표로

704
00:30:06,090 --> 00:30:10,490
감독되지만, 다른 토큰들은 어떤 목적에도 사용되지 않습니다.

705
00:30:10,490 --> 00:30:13,070
그래서 실제로 유용한 정보를 포함하고 있지 않을 수도 있습니다.

706
00:30:13,070 --> 00:30:16,170
그리고 경험적으로 사람들은 이러한 특징들이 그리

707
00:30:16,170 --> 00:30:18,450
유용하지 않다는 것을 보여주었지만,

708
00:30:18,450 --> 00:30:22,290
그들이 보여준 것은 CLIP 인코더의 마지막에서 두 번째

709
00:30:22,290 --> 00:30:26,370
층으로 한 층 더 거슬러 올라가면 이러한 특징들이 실제로 매우

710
00:30:26,370 --> 00:30:27,770
유용하다는 것입니다.

711
00:30:27,770 --> 00:30:30,450
그래서 이러한 특징들은 마지막 층에서 최종

712
00:30:30,450 --> 00:30:32,610
CLIP 임베딩을 생성하는 데 사용됩니다.

713
00:30:32,610 --> 00:30:35,370
그리고 이들은 전체 이미지에서 객체가 어디에

714
00:30:35,370 --> 00:30:39,050
있는지에 대한 많은 공간 정보를 포함하고 있습니다.

715
00:30:39,050 --> 00:30:40,890
그래서 이것이

716
00:30:40,890 --> 00:30:49,010
사람들이 CLIP 인코더와 변환기 LLM 기반 모델을 결합할 때 일반적으로
사용하는

717
00:30:49,010 --> 00:30:49,890
것입니다.

718
00:30:49,890 --> 00:30:52,630
이것이 전체 LLaVA 아키텍처의 모습입니다.

719
00:30:52,630 --> 00:30:56,090
이미지를 CLIP-- 사전 훈련된 CLIP

720
00:30:56,090 --> 00:30:58,650
인코더를 통해 전달하고,

721
00:30:58,650 --> 00:31:01,690
그로부터 많은 특징을 추출한 다음, 그

722
00:31:01,690 --> 00:31:05,800
특징들을 훈련해야 하는 선형 층을 통과시킵니다.

723
00:31:05,800 --> 00:31:07,740
그리고 이 선형 층이 훈련할

724
00:31:07,740 --> 00:31:11,420
것은 CLIP 표현을 LLM이 이해하고 의미를

725
00:31:11,420 --> 00:31:14,460
부여할 수 있는 것으로 변환하는 것입니다.

726
00:31:14,460 --> 00:31:16,580
이 토큰들을 얻으면

727
00:31:16,580 --> 00:31:20,240
이제 모든 토큰을 언어 모델에 전달합니다.

728
00:31:20,240 --> 00:31:22,940
그리고 이제 그 이미지를

729
00:31:22,940 --> 00:31:26,500
가지고 대화를 생성할 수 있습니다.

730
00:31:26,500 --> 00:31:28,780
LLaVA는 그 당시 가장 인기 있는 모델

731
00:31:28,780 --> 00:31:29,840
중 하나였습니다.

732
00:31:29,840 --> 00:31:33,460
그리고 이어서 구글은 빠르게 Flamingo를

733
00:31:33,460 --> 00:31:36,620
출시했으며, Flamingo는 비전

734
00:31:36,620 --> 00:31:40,300
인코더의 특징과 대형 언어 모델을 결합할 수

735
00:31:40,300 --> 00:31:42,560
있는 LLaVA 설정을 따랐지만,

736
00:31:42,560 --> 00:31:44,260
그들이 혁신한 부분은

737
00:31:44,260 --> 00:31:48,100
이러한 다양한 특징들을 융합하는 방법입니다.

738
00:31:48,100 --> 00:31:51,460
그래서 LLaVA에서는 특징들이 선형 층을

739
00:31:51,460 --> 00:31:54,500
통해 들어오고 입력의 일부로 제공되었습니다.

740
00:31:54,500 --> 00:31:56,620
Flamingo에서는

741
00:31:56,620 --> 00:31:59,260
대신 비전 인코더에서 나오는

742
00:31:59,260 --> 00:32:01,180
모든 특징을 LLM의

743
00:32:01,180 --> 00:32:03,750
모든 층에 전달했습니다.

744
00:32:03,750 --> 00:32:04,250
좋습니다.

745
00:32:04,250 --> 00:32:07,510
그래서 그들은 LLM 아키텍처 자체에 몇 가지

746
00:32:07,510 --> 00:32:08,790
변경을 해야 했습니다.

747
00:32:08,790 --> 00:32:10,890
그리고 이것이 그들이 변경한 방법입니다.

748
00:32:10,890 --> 00:32:13,190
여기 Flamingo의 훈련 데이터가 어떻게 생겼는지에

749
00:32:13,190 --> 00:32:14,210
대한 예가 있습니다.

750
00:32:14,210 --> 00:32:15,810
이미지가 인코딩되어 있습니다.

751
00:32:15,810 --> 00:32:18,250
그래서 이 개와 이 고양이가 있고,

752
00:32:18,250 --> 00:32:19,630
둘 다 임베딩됩니다.

753
00:32:19,630 --> 00:32:22,430
그리고 둘 다 LLM의 모든

754
00:32:22,430 --> 00:32:24,190
층에 제공될 것입니다.

755
00:32:24,190 --> 00:32:26,350
그리고 아래에는 이미지로

756
00:32:26,350 --> 00:32:30,070
시작하여 그 이미지를 설명하는 데이터가 있으며,

757
00:32:30,070 --> 00:32:32,790
다음 이미지는 다음 이미지를

758
00:32:32,790 --> 00:32:35,230
설명하고, 계속해서 그렇게 됩니다.

759
00:32:35,230 --> 00:32:38,030
그리고 이들은 LLM에 입력으로 제공됩니다.

760
00:32:38,030 --> 00:32:43,030
그리고 출력은 마지막 이미지를 자동 완성하는 것입니다.

761
00:32:43,030 --> 00:32:44,710
그래서 하나의 이미지가 있고,

762
00:32:44,710 --> 00:32:47,037
그 뒤에 개에 대한 설명이 있으며,

763
00:32:47,037 --> 00:32:48,870
두 번째 이미지가 있고 설명을

764
00:32:48,870 --> 00:32:51,550
시작하면 모델이 두 번째 이미지에 대한 설명을

765
00:32:51,550 --> 00:32:53,350
자동 완성하도록 훈련됩니다.

766
00:32:53,350 --> 00:32:54,170
그들은 무엇을 했습니까?

767
00:32:54,170 --> 00:32:56,330
모델 자체에 무엇을 변경했습니까?

768
00:32:56,330 --> 00:33:01,000
그들은 LLM의 모든 층에 이 GATED X

769
00:33:01,000 --> 00:33:03,760
교차 주의 모듈을 추가했습니다.

770
00:33:03,760 --> 00:33:05,580
그리고 또 다른 변경을 했습니다.

771
00:33:05,580 --> 00:33:08,080
그들은 또한 여기에서 이미지

772
00:33:08,080 --> 00:33:12,720
표현을 샘플링하고 다운샘플링하는 이 인식기 샘플러를

773
00:33:12,720 --> 00:33:14,300
추가했습니다.

774
00:33:14,300 --> 00:33:17,400
그래서 모든 층에 대해 더 작은 차원과 고정된

775
00:33:17,400 --> 00:33:18,680
수의 토큰이 있습니다.

776
00:33:18,680 --> 00:33:22,640
자, 그들이 어떻게 생겼는지에 대한 세부 사항을 살펴보겠습니다.

777
00:33:22,640 --> 00:33:24,840
그래서 이것이 전체 아키텍처입니다.

778
00:33:24,840 --> 00:33:26,500
대부분의 구성 요소는 고정되어

779
00:33:26,500 --> 00:33:28,660
있으며, 모든 언어 모델 가중치와

780
00:33:28,660 --> 00:33:30,860
모든 비전 모델 부분이 고정되어 있습니다.

781
00:33:30,860 --> 00:33:32,400
훈련되는 유일한

782
00:33:32,400 --> 00:33:34,500
부분은 이 퍼시버 샘플러

783
00:33:34,500 --> 00:33:36,680
구성 요소와 LLM의 모든

784
00:33:36,680 --> 00:33:41,000
레이어에 추가된 이 크로스 어텐션 레이어입니다.

785
00:33:41,000 --> 00:33:44,040
이 크로스 어텐션 모듈이 어떻게 생겼는지 이야기해

786
00:33:44,040 --> 00:33:44,840
보겠습니다.

787
00:33:44,840 --> 00:33:47,900
이것은 제가 그 크로스 어텐션 모듈을 확대하는 모습입니다.

788
00:33:47,900 --> 00:33:51,520
모든 LLM 레이어에서 LLM 레이어 바로 앞에 이 크로스

789
00:33:51,520 --> 00:33:53,660
어텐션 구성 요소가 있습니다.

790
00:33:53,660 --> 00:33:56,840
그 목적은 이미지 특징을 보고 어떤

791
00:33:56,840 --> 00:33:58,930
부분을 유지할지,

792
00:33:58,930 --> 00:34:01,690
언어 모델이 알아야 할 유용한

793
00:34:01,690 --> 00:34:04,810
부분은 무엇인지 결정하는 것입니다.

794
00:34:04,810 --> 00:34:08,210
그들은 지금까지 본 구성 요소

795
00:34:08,210 --> 00:34:10,170
세트로 설계했습니다.

796
00:34:10,170 --> 00:34:12,530
이미지 특징을 크로스 어텐션

797
00:34:12,530 --> 00:34:15,210
레이어를 사용하여 주목한 다음, 그 크로스

798
00:34:15,210 --> 00:34:19,268
어텐션 뒤에 이 tanh 비선형 활성화를 추가했습니다.

799
00:34:19,268 --> 00:34:21,810
기본적으로 이 구성 요소의 어떤

800
00:34:21,810 --> 00:34:25,130
부분을 유지할지, 어떤 부분의 이미지를

801
00:34:25,130 --> 00:34:26,929
잊을지 결정하는 것입니다.

802
00:34:26,929 --> 00:34:29,449
그런 다음 완전 연결 레이어를

803
00:34:29,449 --> 00:34:32,190
통과하여 그 표현을 약간 조정하고,

804
00:34:32,190 --> 00:34:35,409
다시 tanh 비선형성을 통해

805
00:34:35,409 --> 00:34:38,062
어떤 부분을 유지할지 결정합니다.

806
00:34:38,062 --> 00:34:39,770
이 두 구성 요소를

807
00:34:39,770 --> 00:34:43,489
통과한 후 각각 잔여 연결이 있으며, 그

808
00:34:43,489 --> 00:34:46,570
다음 일반 언어 모델 처리로 넘어가서

809
00:34:46,570 --> 00:34:49,449
필요한 단어를 계속 생성합니다.

810
00:34:49,449 --> 00:34:52,409
따라서 추가 레이어가 추가되어

811
00:34:52,409 --> 00:34:56,409
언어가 모든 레이어에서 시각적 특징을 통합하고

812
00:34:56,409 --> 00:34:59,290
주목할 수 있는 방법이 됩니다.

813
00:34:59,290 --> 00:34:59,790
좋습니다.

814
00:34:59,790 --> 00:35:01,382
실제 수정 사항은

815
00:35:01,382 --> 00:35:03,590
코드에서 어떻게 생겼는지

816
00:35:03,590 --> 00:35:06,730
궁금하다면, 크로스 어텐션 레이어와 그

817
00:35:06,730 --> 00:35:09,570
사이에 tanh 비선형성을 추가한

818
00:35:09,570 --> 00:35:11,625
두세 줄의 코드입니다.

819
00:35:11,625 --> 00:35:12,750
그게 전부입니다.

820
00:35:12,750 --> 00:35:15,170
코드 측면에서 매우 최소한의 변경입니다.

821
00:35:15,170 --> 00:35:17,610
모델에 있어서는 매우 거대한 변화입니다.

822
00:35:17,610 --> 00:35:20,010
이제는 처리의 모든 레이어에서

823
00:35:20,010 --> 00:35:23,470
어떤 이미지 부분에 주목할지를 선택할 수 있습니다.

824
00:35:23,470 --> 00:35:25,570
모델에게 시각적 특징에 대해

825
00:35:25,570 --> 00:35:30,330
언제 어떻게 주목할지를 결정할 수 있는 많은 능력을 부여합니다.

826
00:35:30,330 --> 00:35:31,050
좋습니다.

827
00:35:31,050 --> 00:35:33,570
Flamingo는 매우 흥미로웠지만,

828
00:35:33,570 --> 00:35:35,890
훈련하기가 매우 어려웠습니다.

829
00:35:35,890 --> 00:35:38,170
그들은 모델이 다양한 작업에 적응할 수

830
00:35:38,170 --> 00:35:41,570
있도록 하는 매우 기발한 훈련 방법을 가지고 있었습니다.

831
00:35:41,570 --> 00:35:45,970
훈련 방법은 여러 이미지를 함께

832
00:35:45,970 --> 00:35:48,750
연결하는 방식이었습니다.

833
00:35:48,750 --> 00:35:51,430
하나의 이미지와 하나의 설명만 있는 것이 아닙니다.

834
00:35:51,430 --> 00:35:55,140
처음에 '여기 내 애완동물의 귀여운 사진들이

835
00:35:55,140 --> 00:35:57,180
있습니다'라는 설명과 함께

836
00:35:57,180 --> 00:35:59,600
이미지 시작 문장이 있고, 첫

837
00:35:59,600 --> 00:36:01,620
번째 이미지에 대한 설명과

838
00:36:01,620 --> 00:36:06,420
첫 번째 구성 요소, 두 번째 이미지와 두 번째 이미지에

839
00:36:06,420 --> 00:36:08,160
대한 설명이 있었습니다.

840
00:36:08,160 --> 00:36:10,380
훈련 세트는 이미지

841
00:36:10,380 --> 00:36:14,180
텍스트가 교차된 긴 시퀀스처럼

842
00:36:14,180 --> 00:36:16,500
보이도록 설정되었습니다.

843
00:36:16,500 --> 00:36:19,120
물론, 단일 이미지를 설명할 때

844
00:36:19,120 --> 00:36:22,480
모델이 전체 맥락을 보지 않기를 원합니다.

845
00:36:22,480 --> 00:36:24,760
특정 이미지만 보기를 원합니다.

846
00:36:24,760 --> 00:36:27,220
그래서 그들은 마스킹 방식을 만들어서,

847
00:36:27,220 --> 00:36:31,260
생성할 때마다 각 이미지는 특정 이미지 특징만

848
00:36:31,260 --> 00:36:34,100
보고 다른 이미지는 보지 않도록 했습니다.

849
00:36:34,100 --> 00:36:35,660
즉, '내 강아지가

850
00:36:35,660 --> 00:36:38,320
풀에 앉아 있다'는 설명을

851
00:36:38,320 --> 00:36:40,940
생성할 때, 강아지에 해당하는

852
00:36:40,940 --> 00:36:42,820
특징만 보고 그 단어를

853
00:36:42,820 --> 00:36:44,120
생성합니다.

854
00:36:44,120 --> 00:36:47,100
마찬가지로 고양이에 대한 설명을 생성할 때는

855
00:36:47,100 --> 00:36:50,780
고양이 이미지만 보고 다른 이미지는 보지 않습니다.

856
00:36:50,780 --> 00:36:53,150
그래서 그들은 설명이

857
00:36:53,150 --> 00:36:55,790
항상 특정 이미지만 따르고

858
00:36:55,790 --> 00:36:58,750
보도록 하기 위해 수작업으로

859
00:36:58,750 --> 00:37:02,030
마스킹 시트를 만들었습니다.

860
00:37:02,030 --> 00:37:04,070
하지만 훈련할 때 모델은

861
00:37:04,070 --> 00:37:08,750
생성하는 모든 것의 전체 맥락을 볼 수 있습니다.

862
00:37:08,750 --> 00:37:09,890
그렇다면 왜 이게 도움이 될까요?

863
00:37:09,890 --> 00:37:11,910
이 모든 것을 함께 볼 수 있는

864
00:37:11,910 --> 00:37:13,810
이 과정이 왜 도움이 될까요?

865
00:37:13,810 --> 00:37:15,830
그것은 이러한 종류의 응용 프로그램을

866
00:37:15,830 --> 00:37:17,550
가능하게 해주기 때문에 유용합니다.

867
00:37:17,550 --> 00:37:19,950
여기 Flamingo가 보여줄 수 있었던 세

868
00:37:19,950 --> 00:37:21,750
가지 다른 응용 프로그램이 있습니다.

869
00:37:21,750 --> 00:37:25,390
모두 여러 대화나 여러 이미지를 다루는 것에

870
00:37:25,390 --> 00:37:27,270
중심을 두고 있습니다.

871
00:37:27,270 --> 00:37:30,710
첫 번째 경우에는 이미지가 입력되고 Flamingo

872
00:37:30,710 --> 00:37:33,110
모델이 이 이미지를 설명하는데,

873
00:37:33,110 --> 00:37:37,150
두 개의 곰인형이 달에 있는 사진이라고 말합니다.

874
00:37:37,150 --> 00:37:38,950
그 다음 사람들은 다른 질문을 할

875
00:37:38,950 --> 00:37:40,950
수 있습니다. 사람들이 그들이 무엇을

876
00:37:40,950 --> 00:37:42,630
하고 있는지 물어볼 수 있습니다.

877
00:37:42,630 --> 00:37:45,430
이미 기존의 대형 언어 모델을 사용하여

878
00:37:45,430 --> 00:37:48,150
훈련되고 있기 때문에, 그 대형 언어

879
00:37:48,150 --> 00:37:50,950
모델의 추론 능력이 상속되어 이제 우리는

880
00:37:50,950 --> 00:37:53,360
이 특정 질문에 대해 추론하고

881
00:37:53,360 --> 00:37:54,760
대답할 수 있습니다.

882
00:37:54,760 --> 00:37:56,920
이제 대답할 수 있으며, 곰인형들이 대화를

883
00:37:56,920 --> 00:37:58,340
나누고 있다고 말할 수 있습니다.

884
00:37:58,340 --> 00:38:01,182
그리고 사용자가 그들이 어떤 물체를 사용하고 있는지 물어볼 수 있습니다.

885
00:38:01,182 --> 00:38:02,640
다시 말해, Flamingo는

886
00:38:02,640 --> 00:38:04,900
그것이 컴퓨터처럼 보인다고 말할 수 있습니다.

887
00:38:04,900 --> 00:38:07,480
따라서 두 가지 작업을 수행함으로써

888
00:38:07,480 --> 00:38:10,820
이미지에 대한 다중 턴 대화를 가능하게 할 수 있습니다.

889
00:38:10,820 --> 00:38:14,000
먼저 언어 모델을 사전 훈련하고 그 언어 모델을

890
00:38:14,000 --> 00:38:16,540
Flamingo에 통합하여 훈련합니다.

891
00:38:16,540 --> 00:38:18,200
둘째로, 모델이 훈련

892
00:38:18,200 --> 00:38:20,680
데이터 전반에 걸쳐 여러 다른 이미지와

893
00:38:20,680 --> 00:38:23,040
여러 다른 턴을 볼 수 있도록

894
00:38:23,040 --> 00:38:26,600
하여 더 긴 텍스트 시퀀스에 적응할 수 있게 합니다.

895
00:38:26,600 --> 00:38:28,480
여러 이미지를 제공하고 이 이미지들에

896
00:38:28,480 --> 00:38:31,300
대한 공통된 점이 무엇인지 물어볼 수도 있습니다.

897
00:38:31,300 --> 00:38:32,800
이제 Flamingo 모델은

898
00:38:32,800 --> 00:38:35,200
각기 다른 구성 요소를 살펴보고 모두

899
00:38:35,200 --> 00:38:37,440
Flamingo라고 추론할 것입니다.

900
00:38:37,440 --> 00:38:40,540
따라서 이러한 멋진 응용 프로그램을 많이 시작할 수 있습니다.

901
00:38:40,540 --> 00:38:43,512
사람들은 또한 맥락 내 학습을 시작할 수 있음을 보여주었습니다.

902
00:38:43,512 --> 00:38:45,720
이것이 언어 모델에서 이미 본 것인지

903
00:38:45,720 --> 00:38:48,040
모르겠지만, GPT와 함께 맥락 내 학습을

904
00:38:48,040 --> 00:38:51,690
사용한 적이 있을 것입니다. GPT에게 내가 원하는 것의

905
00:38:51,690 --> 00:38:55,090
예를 보여주고, 이와 같은 더 많은 것을 달라고 요청합니다.

906
00:38:55,090 --> 00:38:56,970
Flamingo에서도 같은

907
00:38:56,970 --> 00:38:59,970
작업을 수행할 수 있으며, 이미지와 설명을 전달할

908
00:38:59,970 --> 00:39:01,030
수 있습니다.

909
00:39:01,030 --> 00:39:02,710
이제 새로운 이미지를

910
00:39:02,710 --> 00:39:04,410
전달하면 설명을 제공합니다.

911
00:39:04,410 --> 00:39:07,990
또는, 여기 이미지가 있고 질문과

912
00:39:07,990 --> 00:39:09,350
답변이 있습니다.

913
00:39:09,350 --> 00:39:11,130
여기 이미지 질문과 답변이 있습니다.

914
00:39:11,130 --> 00:39:13,930
그런 다음 새로운 이미지를 전달하고 질문만

915
00:39:13,930 --> 00:39:15,485
하면 답변을 제공합니다.

916
00:39:15,485 --> 00:39:18,110
따라서 이러한 다양한 작업을

917
00:39:18,110 --> 00:39:21,250
수행하도록 훈련하는 것이 아니라, 가져야

918
00:39:21,250 --> 00:39:22,890
할 행동의 예를

919
00:39:22,890 --> 00:39:25,490
제공하고, 새로운 행동에

920
00:39:25,490 --> 00:39:26,790
일반화해야 합니다.

921
00:39:26,790 --> 00:39:29,570
유사하게, 단순한 분류에 관심이 있을 수 있습니다.

922
00:39:29,570 --> 00:39:32,250
Flamingo를 사용하여 분류를 수행할 수도 있습니다.

923
00:39:32,250 --> 00:39:35,390
따라서 이미지를 제공하고 이것은 지하, 이것은

924
00:39:35,390 --> 00:39:37,010
의회라고 말할 수 있습니다.

925
00:39:37,010 --> 00:39:39,050
그런 다음 이것이 무엇인지 물어볼 수 있습니다.

926
00:39:39,050 --> 00:39:43,330
또한 이미지를 제공하고 이것이 2 더하기 1은 3에 해당해야

927
00:39:43,330 --> 00:39:45,930
한다고 말하여 OCR과 수학을 수행하도록

928
00:39:45,930 --> 00:39:47,370
가르칠 수도 있습니다.

929
00:39:47,370 --> 00:39:49,760
결국 새로운 이미지를 제공하면

930
00:39:49,760 --> 00:39:51,980
3 곱하기 6을 자동

931
00:39:51,980 --> 00:39:55,740
완성하고 이 전체 과정을 통해 결과를 제공할

932
00:39:55,740 --> 00:39:57,240
수 있어야 합니다.

933
00:39:57,240 --> 00:39:57,740
응.

934
00:39:57,740 --> 00:39:59,740
이것은 몇 가지 예시를

935
00:39:59,740 --> 00:40:02,500
주고 새로운 것이 무엇인지

936
00:40:02,500 --> 00:40:06,060
물어보는 몇 샷 학습의 예시입니다.

937
00:40:06,060 --> 00:40:09,040
모든 인컨텍스트 예시를 버린다면, 그것은 제로

938
00:40:09,040 --> 00:40:10,700
샷 학습이 될 것입니다.

939
00:40:10,700 --> 00:40:12,860
그래서 우리는 그것들을 연결하지 않습니다.

940
00:40:12,860 --> 00:40:15,340
우리는 기술적으로 이미지 토큰을 이

941
00:40:15,340 --> 00:40:17,860
퍼시버 샘플러를 통해 LLM의 모든

942
00:40:17,860 --> 00:40:19,460
레이어로 전달하고 있습니다.

943
00:40:19,460 --> 00:40:22,820
그래서 오직 텍스트만이 연결되어 플라밍고 모델에

944
00:40:22,820 --> 00:40:25,060
입력으로 제공되고, 모델은 이미지의

945
00:40:25,060 --> 00:40:28,100
어떤 부분에 주의를 기울일지 선택합니다.

946
00:40:28,100 --> 00:40:29,500
한 번 제공하면 됩니다.

947
00:40:29,500 --> 00:40:31,400
하지만 배경에서는 이것이

948
00:40:31,400 --> 00:40:33,320
인터페이스입니다-- 웹 인터페이스.

949
00:40:33,320 --> 00:40:35,300
하지만 배경에서 그들이 실제로 하는

950
00:40:35,300 --> 00:40:37,780
것은 사용자가 계속 대화할 것이라고 가정하고

951
00:40:37,780 --> 00:40:39,380
모델을 캐시하는 것입니다.

952
00:40:39,380 --> 00:40:42,740
그래서 모델은 캐시되어 더 많은 토큰을 받을 준비가 되어 있습니다.

953
00:40:42,740 --> 00:40:44,800
응, 하지만 만약 그들이 캐시하지 않았다면,

954
00:40:44,800 --> 00:40:47,890
그렇다면 이 전체 대화를 입력으로 전달했을 것입니다.

955
00:40:47,890 --> 00:40:49,950
응.

956
00:40:49,950 --> 00:40:50,550
좋아요.

957
00:40:50,550 --> 00:40:52,990
플라밍고는 그들의 논문에

958
00:40:52,990 --> 00:40:54,790
정말 큰 표가 있어서

959
00:40:54,790 --> 00:40:56,270
정말 멋졌습니다.

960
00:40:56,270 --> 00:40:57,870
하지만 정말 멋진 점은

961
00:40:57,870 --> 00:41:01,050
매우 어려운 모든 작업들이 있었고,

962
00:41:01,050 --> 00:41:03,550
CLIP을 적응시켜야 했다는 것입니다.

963
00:41:03,550 --> 00:41:06,990
하지만 플라밍고는 제로 샷 또는 몇 샷으로 그것을 할 수 있었습니다.

964
00:41:06,990 --> 00:41:09,310
그리고 여러 다른 벤치마크에서

965
00:41:09,310 --> 00:41:11,510
엄청난 개선을 보기 시작했습니다.

966
00:41:11,510 --> 00:41:13,070
이때, 저는 이 분야가 몇

967
00:41:13,070 --> 00:41:15,950
가지 분류 벤치마크를 보고하는 것에서 모든 이해

968
00:41:15,950 --> 00:41:18,810
작업을 보고하는 것으로 전환되었다고 생각합니다.

969
00:41:18,810 --> 00:41:21,690
질문 응답 프로세스로 프레임할 수 있는 한,

970
00:41:21,690 --> 00:41:25,810
다양한 기술에 대한 벤치마크를 구축할 수 있으며, 우리는

971
00:41:25,810 --> 00:41:28,750
지난 2년 동안 컴퓨터 비전 분야에서 그것이

972
00:41:28,750 --> 00:41:31,470
표준이 되는 것을 보기 시작했습니다.

973
00:41:31,470 --> 00:41:35,270
그래서 저는 작년 어느 시점에서 우리가 있었던 곳이라고

974
00:41:35,270 --> 00:41:38,390
생각하며, LLaVA의 성공을 보면서 많은

975
00:41:38,390 --> 00:41:41,890
회사들이 이러한 모델에 상당히 많은 투자를 시작했습니다.

976
00:41:41,890 --> 00:41:44,920
그래서 GPT-40, GPT-4V,

977
00:41:44,920 --> 00:41:52,360
제미니 1.5 프로, 제미니 1.5 플래시와 같은 많은 API 모델들을 보기
시작했습니다.

978
00:41:52,360 --> 00:41:54,980
이러한 모델들이 많이 출시되었고,

979
00:41:54,980 --> 00:41:58,760
심지어 앤트로픽도 클로드 3 오푸스와 함께 등장했습니다.

980
00:41:58,760 --> 00:42:01,880
그리고 이제 물론 클로드 4 오푸스도 나왔습니다.

981
00:42:01,880 --> 00:42:03,980
그래서 많은 모델들이 출시되었고,

982
00:42:03,980 --> 00:42:05,760
이들 벤치마크에서 훨씬 더

983
00:42:05,760 --> 00:42:07,240
나은 성능을 보였습니다.

984
00:42:07,240 --> 00:42:10,120
여기서 저는 이 분야에서 11개의 더 인기

985
00:42:10,120 --> 00:42:13,280
있는 시각적 이해 벤치마크에서 평균 성능을

986
00:42:13,280 --> 00:42:14,340
보여주고 있습니다.

987
00:42:14,340 --> 00:42:16,360
그리고 이 거대한 차이가 있습니다.

988
00:42:16,360 --> 00:42:19,920
LLaVA와 우리가 이야기한 모델

989
00:42:19,920 --> 00:42:24,040
간의 차이는 평균 약 43% 정확도입니다.

990
00:42:24,040 --> 00:42:27,240
한편 GPT와 이 다른 모델들은 약

991
00:42:27,240 --> 00:42:32,780
80% 또는 70% 후반대에서 훨씬 더 나은 성능을 보이고 있습니다.

992
00:42:32,780 --> 00:42:34,400
그래서 이 두 종류의

993
00:42:34,400 --> 00:42:37,080
모델 간의 성능 차이가 큽니다.

994
00:42:37,080 --> 00:42:39,900
물론 이 차이를 즉시 보고,

995
00:42:39,900 --> 00:42:43,370
사람들은 GPT와 제미니를 증류 변형으로

996
00:42:43,370 --> 00:42:48,130
증류하기 시작하고 그 모델들을 출시하려고 했습니다.

997
00:42:48,130 --> 00:42:50,150
중국의 알리바바라는 회사가

998
00:42:50,150 --> 00:42:52,490
QWEN이라는 모델을 출시했습니다.

999
00:42:52,490 --> 00:42:54,970
그리고 인턴 VL, 피가 있습니다.

1000
00:42:54,970 --> 00:42:57,470
다양한 모델들이 나오고 있습니다.

1001
00:42:57,470 --> 00:43:00,530
모든 모델은 GPT에서 파생되었습니다.

1002
00:43:00,530 --> 00:43:02,810
GPT가 아니라면, Gemini입니다.

1003
00:43:02,810 --> 00:43:05,890
이로 인해 분야에 큰 문제가 발생했습니다.

1004
00:43:05,890 --> 00:43:09,090
이는 제 연구 의제의 중요한 부분이

1005
00:43:09,090 --> 00:43:12,170
되었는데, 우리는 연구 커뮤니티로서

1006
00:43:12,170 --> 00:43:14,210
실제로 성능이 뛰어난 비전

1007
00:43:14,210 --> 00:43:17,730
언어 모델을 구축하는 방법을 알지 못합니다.

1008
00:43:17,730 --> 00:43:20,270
이 모델을 구축하는 방법에

1009
00:43:20,270 --> 00:43:23,130
대한 비결은 OpenAI와 Google의

1010
00:43:23,130 --> 00:43:26,390
Gemini 팀만 알고 있습니다.

1011
00:43:26,390 --> 00:43:28,950
하지만 오픈 소스 커뮤니티는 여기 있습니다.

1012
00:43:28,950 --> 00:43:29,870
여기 있습니다.

1013
00:43:29,870 --> 00:43:33,010
작년 기준으로 연구 커뮤니티는 이곳에 있었습니다.

1014
00:43:33,010 --> 00:43:36,110
물론, 이 모델들이 정말 좋은 오픈 모델이라고 주장할 수 있지만,

1015
00:43:36,110 --> 00:43:37,650
실제로는 그렇게 열려 있지 않습니다.

1016
00:43:37,650 --> 00:43:39,650
왜냐하면 이 모델들은 증류되어,

1017
00:43:39,650 --> 00:43:42,980
우리는 이 모델들을 재현하는 방법을 알지 못하기 때문입니다.

1018
00:43:42,980 --> 00:43:44,640
우리는 단지 이 모델들을 생성할 수 있을 뿐입니다.

1019
00:43:44,640 --> 00:43:46,160
하지만 GPT가 존재한다면.

1020
00:43:46,160 --> 00:43:48,300
GPT가 존재하지 않는다면, 우리는 이러한 다른

1021
00:43:48,300 --> 00:43:50,020
모델들을 만드는 방법을 알지 못합니다.

1022
00:43:50,020 --> 00:43:51,717
그래서 지난 몇 년 동안 제

1023
00:43:51,717 --> 00:43:53,800
연구 의제가 집중했던 것은 이

1024
00:43:53,800 --> 00:43:55,640
격차를 어떻게 좁힐 수 있을지,

1025
00:43:55,640 --> 00:43:58,980
어떻게 정말 좋은 다중 모달 언어 모델을 구축하고

1026
00:43:58,980 --> 00:44:01,340
그 이해를 전체 커뮤니티에 전파할 수

1027
00:44:01,340 --> 00:44:03,060
있을지를 알아내는 것입니다.

1028
00:44:03,060 --> 00:44:08,860
그래서 지난 6개월 또는 약 1년 동안 우리가 한

1029
00:44:08,860 --> 00:44:12,980
일은 Molmo라는 모델 클래스를

1030
00:44:12,980 --> 00:44:14,340
만든 것입니다.

1031
00:44:14,340 --> 00:44:17,540
그리고 저는 Molmo의 성능을 위에 보여주고 있습니다.

1032
00:44:17,540 --> 00:44:20,180
Molmo가 다른 모든 모델들과 차별화되는

1033
00:44:20,180 --> 00:44:23,000
점은 완전히 오픈 소스라는 것입니다.

1034
00:44:23,000 --> 00:44:25,160
즉, 가중치가 공개되어 있습니다.

1035
00:44:25,160 --> 00:44:26,560
그래서 모델을 다운로드할 수 있습니다.

1036
00:44:26,560 --> 00:44:29,060
데이터도 공개되어 있어, 훈련 세트와 평가 세트를

1037
00:44:29,060 --> 00:44:30,223
다운로드할 수 있습니다.

1038
00:44:30,223 --> 00:44:32,140
코드도 공개되어 있어,

1039
00:44:32,140 --> 00:44:34,360
충분한 GPU가 있다면 집에서

1040
00:44:34,360 --> 00:44:37,700
자신의 Molmo를 훈련할 수 있으며, 새로운

1041
00:44:37,700 --> 00:44:39,810
평가를 추가하고 이 모델을

1042
00:44:39,810 --> 00:44:42,930
다양한 새로운 것들에 맞게 조정할 수

1043
00:44:42,930 --> 00:44:46,430
있고, 물론 다양한 맥락에서 사용할 수 있습니다.

1044
00:44:46,430 --> 00:44:49,270
물론, 학술 벤치마크만으로는 충분하지 않습니다.

1045
00:44:49,270 --> 00:44:51,790
왜냐하면 결국 우리가 중요하게 생각하는 것은 사람들이

1046
00:44:51,790 --> 00:44:53,610
이 모델들을 사용할 것인가입니다.

1047
00:44:53,610 --> 00:44:56,830
사람들이 GPT보다 이 모델들을 사용하고 싶어할까요?

1048
00:44:56,830 --> 00:45:00,050
그래서 우리가 그 평가를 제대로 수행했는지

1049
00:45:00,050 --> 00:45:02,690
확인하기 위해, Molmo와 함께

1050
00:45:02,690 --> 00:45:05,030
놀이터를 출시했고, 실제로 우리

1051
00:45:05,030 --> 00:45:07,510
모델의 출력과 다른 모든 모델의 출력을

1052
00:45:07,510 --> 00:45:10,990
비교하는 대규모 사용자 연구를 진행했습니다.

1053
00:45:10,990 --> 00:45:14,710
우리 모델은 GPT와 동일한 Elo 등급을 가지고 있습니다.

1054
00:45:14,710 --> 00:45:17,910
GPT-40과의 Elo 등급 차이는 1로

1055
00:45:17,910 --> 00:45:19,790
두 번째로 나타났습니다.

1056
00:45:19,790 --> 00:45:22,670
이 그래프는 제가 몇 가지 예를 보여주기

1057
00:45:22,670 --> 00:45:24,390
위해 회전한 것입니다.

1058
00:45:24,390 --> 00:45:26,770
이것은 대규모 평가였습니다.

1059
00:45:26,770 --> 00:45:31,010
약 870명의 사용자에게 이 모델 출력을 보여주었습니다.

1060
00:45:31,010 --> 00:45:34,030
우리는 약 325,000개의 쌍 비교를 진행했으며,

1061
00:45:34,030 --> 00:45:36,800
사람들에게 어떤 모델의 출력을 선호하는지 물었습니다.

1062
00:45:36,800 --> 00:45:39,340
우리 모델, Moima 모델은 다시 말해 두

1063
00:45:39,340 --> 00:45:41,303
번째로 순위가 매겨졌으며, 사람들은

1064
00:45:41,303 --> 00:45:42,720
GPT와 우리 모델 사이에서

1065
00:45:42,720 --> 00:45:44,960
거의 동전 던지기와 같은 선택을 했습니다.

1066
00:45:44,960 --> 00:45:49,520
하지만 이미 Gemini 1.5 Pro와 Claude 3.5를 초과했습니다.

1067
00:45:49,520 --> 00:45:52,640
이제 큰 차이점은 우리가 작은 연구실이라는

1068
00:45:52,640 --> 00:45:56,080
것이고, 구글의 제미니에 대한 수십억 달러의 투자와

1069
00:45:56,080 --> 00:45:58,160
앤트로픽의 수십억 달러의 투자를

1070
00:45:58,160 --> 00:46:00,280
제치고 있다는 것입니다. 이미

1071
00:46:00,280 --> 00:46:02,080
GPT와 맞먹고 있습니다.

1072
00:46:02,080 --> 00:46:05,000
그래서 우리는 이 전체 과정에 대해 매우 흥미로웠습니다.

1073
00:46:05,000 --> 00:46:08,240
하지만 우리는 그 큰 모델들 바로 뒤에 오는

1074
00:46:08,240 --> 00:46:10,140
70억 모델도 개발했습니다.

1075
00:46:10,140 --> 00:46:12,680
이 70억 모델은 단일 GPU에서 실행할

1076
00:46:12,680 --> 00:46:15,240
수 있기 때문에 정말 흥미롭습니다.

1077
00:46:15,240 --> 00:46:17,360
이제 이 모델은 단일 GPU에서

1078
00:46:17,360 --> 00:46:19,680
작동하는 다양한 비전 작업을

1079
00:46:19,680 --> 00:46:23,160
수행할 수 있어 많은 사람들이 이를 사용하고 모든

1080
00:46:23,160 --> 00:46:25,920
종류의 작업에 맞게 조정할 수 있습니다.

1081
00:46:25,920 --> 00:46:28,200
우리는 이 모델을 9월 25일에 출시했으며,

1082
00:46:28,200 --> 00:46:30,720
커뮤니티는 이에 대해 매우 흥미로워했습니다.

1083
00:46:30,720 --> 00:46:34,440
이것은 매우 성능이 뛰어난 다중 모달 비전 언어 모델이 처음으로

1084
00:46:34,440 --> 00:46:36,610
출시된 것이며, 많은 사람들이 이를

1085
00:46:36,610 --> 00:46:39,530
사용하고 싶어하는 방법에 대해 이야기하고 글을 쓰기

1086
00:46:39,530 --> 00:46:40,358
시작했습니다.

1087
00:46:40,358 --> 00:46:42,650
반복적으로 등장한 사용 사례 중

1088
00:46:42,650 --> 00:46:46,310
하나는 Molmo를 로봇 응용 프로그램에

1089
00:46:46,310 --> 00:46:48,010
사용하겠다는 아이디어였습니다.

1090
00:46:48,010 --> 00:46:50,250
오늘은 로봇에 대해 이야기하지

1091
00:46:50,250 --> 00:46:53,210
않겠지만, 다음 수업에서 배울 것입니다.

1092
00:46:53,210 --> 00:46:55,330
하지만 로봇과 관련하여 사람들이

1093
00:46:55,330 --> 00:46:59,050
흥미로워했던 몇 가지 예를 보여주고 싶습니다.

1094
00:46:59,050 --> 00:47:02,790
많은 사람들이, 심지어 NVIDIA의 사람들도, 비공식적으로

1095
00:47:02,790 --> 00:47:05,512
모델 개발을 얼마나 많이 하든지 간에 오픈

1096
00:47:05,512 --> 00:47:06,970
소스에 대해 배팅해서는

1097
00:47:06,970 --> 00:47:09,270
안 된다고 이야기하기 시작했습니다.

1098
00:47:09,270 --> 00:47:11,750
결국 오픈 소스 커뮤니티는 따라잡을 것이고,

1099
00:47:11,750 --> 00:47:14,610
그 시점에서 우리는 따라잡고 있었습니다.

1100
00:47:14,610 --> 00:47:17,490
그래서 우리의 모델이 출시되자, 메타는

1101
00:47:17,490 --> 00:47:22,170
신속하게 LLaVA 3.2 모델을 출시했고, 많은 사람들이 Molmo와

1102
00:47:22,170 --> 00:47:26,110
메타의 LLaVA 모델을 비교하는 평가를 했습니다.

1103
00:47:26,110 --> 00:47:29,050
다시 말하지만, 우리는 LLaVA보다 우위를 점하게

1104
00:47:29,050 --> 00:47:30,130
되어 매우 기쁩니다.

1105
00:47:30,130 --> 00:47:32,730
그래서 Molmo가 왜 이렇게 잘 작동하는지 보여드리겠습니다.

1106
00:47:32,730 --> 00:47:36,180
이 모델들이 잘 작동하도록 하는 비결은 무엇이었나요?

1107
00:47:36,180 --> 00:47:38,860
비결은 의사 결정을 픽셀 자체에

1108
00:47:38,860 --> 00:47:41,260
기반을 두는 것이었습니다.

1109
00:47:41,260 --> 00:47:44,260
보통 모델에 '배가 몇 대인지 세어보라'는

1110
00:47:44,260 --> 00:47:47,000
질문을 주면, 어떤 숫자를 제시하지만,

1111
00:47:47,000 --> 00:47:48,740
종종 환각을 일으킵니다.

1112
00:47:48,740 --> 00:47:50,380
하지만 우리 모델을 차별화하는

1113
00:47:50,380 --> 00:47:52,700
것은 실제로 세고 있는 모든 것들을

1114
00:47:52,700 --> 00:47:53,740
가리킨다는 것입니다.

1115
00:47:53,740 --> 00:47:56,180
그래서 모든 배를 가리키고

1116
00:47:56,180 --> 00:47:58,620
최종 숫자를 출력합니다.

1117
00:47:58,620 --> 00:48:02,300
그래서 의사 결정은 픽셀 자체에 기반을 두고 있습니다.

1118
00:48:02,300 --> 00:48:05,260
이것은 메타의 LLaVA와 달리 약 60억

1119
00:48:05,260 --> 00:48:08,260
개의 이미지 텍스트 쌍으로 훈련된 모델과는

1120
00:48:08,260 --> 00:48:12,380
달리, 우리 모델은 단 70만 개의 이미지 텍스트 쌍으로

1121
00:48:12,380 --> 00:48:13,580
훈련되었습니다.

1122
00:48:13,580 --> 00:48:17,580
큰 차이점은 우리가 70만 개의 이미지 텍스트 쌍을

1123
00:48:17,580 --> 00:48:20,940
수작업으로 선별했다는 것이며, 이것이 우리가 할

1124
00:48:20,940 --> 00:48:23,780
수 있었던 것과 이들 회사들이 구축한

1125
00:48:23,780 --> 00:48:26,460
모델들 간의 가장 큰 차이점이었습니다.

1126
00:48:26,460 --> 00:48:30,700
현재 많은 사람들이 인터넷에서 이러한 이미지 텍스트 쌍을

1127
00:48:30,700 --> 00:48:32,228
다운로드하려고 하고 있습니다.

1128
00:48:32,228 --> 00:48:34,270
이것은 많은 사람들이 이러한 비전 언어

1129
00:48:34,270 --> 00:48:36,010
모델을 훈련하는 기반이 되어왔습니다.

1130
00:48:36,010 --> 00:48:39,070
많은 인터넷 데이터를 수집하여 이미지와 관련된

1131
00:48:39,070 --> 00:48:40,450
텍스트를 수집합니다.

1132
00:48:40,450 --> 00:48:43,110
하지만 인터넷 데이터의 문제는 우연적이라는 것입니다.

1133
00:48:43,110 --> 00:48:45,470
이미지와 관련된 텍스트는 종종

1134
00:48:45,470 --> 00:48:47,670
주관적인 것 또는 업로더가

1135
00:48:47,670 --> 00:48:50,310
이미지에 대해 느낀 것을 설명합니다.

1136
00:48:50,310 --> 00:48:52,550
이미지 자체의 내용에 대해 이야기하는

1137
00:48:52,550 --> 00:48:54,070
경우는 드뭅니다.

1138
00:48:54,070 --> 00:48:57,230
한편, 이것이 우리의 데이터 모습입니다.

1139
00:48:57,230 --> 00:49:01,110
단일 이미지에 대해 우리는 그 이미지의 실제 내용에

1140
00:49:01,110 --> 00:49:03,650
대한 밀집된 설명을 가지고 있습니다.

1141
00:49:03,650 --> 00:49:05,150
그리고 우리는 사람들이 인터넷에서 절대

1142
00:49:05,150 --> 00:49:06,690
이야기하지 않는 것들을 가지고 있습니다.

1143
00:49:06,690 --> 00:49:09,550
우리가 절대 언급하지 않는 시각 세계에 대한

1144
00:49:09,550 --> 00:49:11,370
수많은 작업과 지식이 있습니다.

1145
00:49:11,370 --> 00:49:12,870
무언가가 다른 것의 왼쪽에

1146
00:49:12,870 --> 00:49:14,410
있다고 말하지 않을 것입니다.

1147
00:49:14,410 --> 00:49:16,930
왜냐하면 그것은 우리에게 자연스럽지 않기 때문입니다.

1148
00:49:16,930 --> 00:49:18,470
무언가가 다른 것의 왼쪽에 있다는

1149
00:49:18,470 --> 00:49:19,730
것은 너무나 명백합니다.

1150
00:49:19,730 --> 00:49:21,850
왜 그런 정보를 전달해야 할까요?

1151
00:49:21,850 --> 00:49:23,225
그래서 우리는 사람들에게서

1152
00:49:23,225 --> 00:49:24,950
그런 정보를 끌어내기 시작했습니다.

1153
00:49:24,950 --> 00:49:27,117
우리는 사람들에게 물체의 크기나 모양에

1154
00:49:27,117 --> 00:49:29,470
대해 이야기하게 했습니다. 예를 들어,

1155
00:49:29,470 --> 00:49:31,710
크고 직사각형 모양 같은 것들입니다.

1156
00:49:31,710 --> 00:49:34,550
우리는 매끄럽고 풍부한 재료와 이미지에서의

1157
00:49:34,550 --> 00:49:36,630
위치에 대해 이야기했습니다.

1158
00:49:36,630 --> 00:49:40,350
예를 들어, 이미지의 수평면을 가로지르는 것처럼요.

1159
00:49:40,350 --> 00:49:42,350
이 모든 정보가 이러한 모델을

1160
00:49:42,350 --> 00:49:44,650
더 성능 좋게 만드는 것입니다.

1161
00:49:44,650 --> 00:49:46,750
데이터 세트에서 또 다른 예시입니다.

1162
00:49:46,750 --> 00:49:50,110
여기 매우 간단한 전화기 화면 또는 태블릿 화면의 이미지가 있습니다.

1163
00:49:50,110 --> 00:49:53,750
그리고 여기에는 사람들이 유용하다고 생각할

1164
00:49:53,750 --> 00:49:56,990
만한 정보가 다시 완전히 누락되어 있습니다.

1165
00:49:56,990 --> 00:49:58,730
예를 들어, 이것은 태블릿 장치입니다.

1166
00:49:58,730 --> 00:50:02,270
현재 시간은 이렇고, 장치에 남아 있는 전력량은

1167
00:50:02,270 --> 00:50:03,000
이렇습니다.

1168
00:50:03,000 --> 00:50:04,750
이것은 사람들이 이러한 모델을

1169
00:50:04,750 --> 00:50:06,398
사용하는 데 도움이 될 정보입니다.

1170
00:50:06,398 --> 00:50:08,190
하지만 이것은 우리가 인터넷에서

1171
00:50:08,190 --> 00:50:10,630
절대 언급하지 않는 정보입니다.

1172
00:50:10,630 --> 00:50:12,370
그래서 이런 정보를 얻기 위해

1173
00:50:12,370 --> 00:50:14,370
우리는 다양한 질문을 설계했습니다.

1174
00:50:14,370 --> 00:50:17,030
우리는 인터넷에서 누락된 올바른 정보나 요소가

1175
00:50:17,030 --> 00:50:19,710
무엇인지 파악하기 위해 다양한 유형의 유도

1176
00:50:19,710 --> 00:50:22,210
연구를 두 년 동안 진행했습니다. 그리고

1177
00:50:22,210 --> 00:50:25,710
그것들을 가능한 한 효과적으로 유도하는 방법을 찾았습니다.

1178
00:50:25,710 --> 00:50:27,480
매우 중요한 한 가지는

1179
00:50:27,480 --> 00:50:31,860
모든 주석자가 설명을 입력하는 것이 아니라

1180
00:50:31,860 --> 00:50:34,720
설명에 대해 이야기하도록 했다는

1181
00:50:34,720 --> 00:50:39,760
것입니다. 대화는 많은 고정관념을 자동으로 깨뜨립니다. 격언.

1182
00:50:39,760 --> 00:50:41,808
사람들이 이야기하도록 함으로써,

1183
00:50:41,808 --> 00:50:43,600
우리는 그들이 보통 입력하지

1184
00:50:43,600 --> 00:50:46,250
않을 것들에 대해 이야기하게 만들었습니다.

1185
00:50:46,250 --> 00:50:48,500
모델 자체는 LLaVA와 다르게 보이지 않았습니다.

1186
00:50:48,500 --> 00:50:51,900
우리는 CLIP와 코딩이 들어오는 동일한 설정을 가지고 있었습니다.

1187
00:50:51,900 --> 00:50:54,320
연결자는 단순한 선형 계층이었고,

1188
00:50:54,320 --> 00:50:56,080
그런 다음 모든 토큰을 받아들이고

1189
00:50:56,080 --> 00:50:59,320
당신이 관심 있는 것을 출력하는 대형 언어

1190
00:50:59,320 --> 00:51:00,500
모델이 있었습니다.

1191
00:51:00,500 --> 00:51:03,660
그래서 모델 자체는 기존 모델과 매우 유사하게 보였습니다.

1192
00:51:03,660 --> 00:51:05,480
가장 큰 차이는 데이터와

1193
00:51:05,480 --> 00:51:08,400
데이터 자체의 품질 및 밀도에 있었습니다.

1194
00:51:08,400 --> 00:51:11,220
모델이 이미지 자체에서 의사 결정을 내리는

1195
00:51:11,220 --> 00:51:12,720
기반 능력 덕분에,

1196
00:51:12,720 --> 00:51:14,920
Malmo는 다른 모델로는 할 수

1197
00:51:14,920 --> 00:51:16,560
없는 작업을 수행할 수

1198
00:51:16,560 --> 00:51:19,080
있었습니다; 예를 들어 메뉴를 가리키는

1199
00:51:19,080 --> 00:51:20,500
것과 같은 작업입니다.

1200
00:51:20,500 --> 00:51:22,960
실제로 그 메뉴 항목이 어디에 있는지

1201
00:51:22,960 --> 00:51:26,370
알려주거나, 검색 옵션을 설정할 수 있는 위치를 가리키도록

1202
00:51:26,370 --> 00:51:27,970
요청하면, OK, 이곳에서

1203
00:51:27,970 --> 00:51:31,490
옵션을 설정할 수 있거나 중간 크기의 데이터 세트가 있는

1204
00:51:31,490 --> 00:51:33,530
위치를 가리키도록 요청하면, 어떤

1205
00:51:33,530 --> 00:51:35,670
옵션을 이동해야 하는지 알려줍니다.

1206
00:51:35,670 --> 00:51:36,170
그리고

1207
00:51:36,170 --> 00:51:38,550
이미 카운트를 가리킬 수 있다는 것을 보여주었지만,

1208
00:51:38,550 --> 00:51:41,870
정말 세밀한 작업을 수행할 수 있도록 요청할 수도 있습니다.

1209
00:51:41,870 --> 00:51:45,410
예를 들어, 이 버스의 노선 번호가 무엇인지 물어보는 것입니다.

1210
00:51:45,410 --> 00:51:47,550
Molmo는 단순히 답변을 제공하는 것이

1211
00:51:47,550 --> 00:51:49,810
아니라, 이미지에서 어디에 있는지를 가리킵니다.

1212
00:51:49,810 --> 00:51:51,690
이 경우, 버스 번호가

1213
00:51:51,690 --> 00:51:54,170
포함된 이 영역이 있으며, 그

1214
00:51:54,170 --> 00:51:56,050
버스 번호를 반환합니다.

1215
00:51:56,050 --> 00:51:57,970
왼쪽에 있는 자동차 수와 오른쪽에 있는

1216
00:51:57,970 --> 00:52:00,630
자동차 수에 대해 추론하도록 요청할 수 있습니다.

1217
00:52:00,630 --> 00:52:04,070
깊이 이미지나 오버헤드 이미지, 또는 정말 혼잡한

1218
00:52:04,070 --> 00:52:08,445
장면과 스포츠 지역에 대해 추론하도록 요청할 수 있습니다.

1219
00:52:08,445 --> 00:52:10,070
또한 정말 흥미로운 점은,

1220
00:52:10,070 --> 00:52:11,695
다시 말하지만 몇 분 후에 이

1221
00:52:11,695 --> 00:52:13,650
이야기를 할 것이며, 오늘날 다중 모달

1222
00:52:13,650 --> 00:52:16,670
모델 전반에 걸쳐 계속 등장하는 체인 아이디어입니다.

1223
00:52:16,670 --> 00:52:20,250
Molmo를 다른 모델에 연결하는 아이디어로, Molmo의

1224
00:52:20,250 --> 00:52:22,930
출력을 SAM 2와 같은 다른 모델의

1225
00:52:22,930 --> 00:52:25,500
입력으로 연결할 수 있으며, Molmo에게

1226
00:52:25,500 --> 00:52:28,280
크리켓 배트를 가리키도록 요청할 수 있습니다.

1227
00:52:28,280 --> 00:52:31,580
이제 그 지점을 가져와서 세분화 작업을 수행하는 SAM 2와

1228
00:52:31,580 --> 00:52:33,700
같은 모델에 입력으로 제공하면, 시간이

1229
00:52:33,700 --> 00:52:36,700
지남에 따라 그 크리켓 배트의 세분화를 수행할 수 있습니다.

1230
00:52:36,700 --> 00:52:39,620
따라서 모든 종류의 새로운 응용 프로그램을 가능하게 할 수 있습니다.

1231
00:52:39,620 --> 00:52:43,318
여기 사무실에서 실험해본 하나의 예가 있으며,

1232
00:52:43,318 --> 00:52:44,860
다음 강의에서

1233
00:52:44,860 --> 00:52:48,700
로봇 공학에 대해 들을 때 배우게 될 것입니다.

1234
00:52:48,700 --> 00:52:52,160
하지만 우리는 Molmo에게 물병이 어디에 있는지 가리키도록 요청했습니다.

1235
00:52:52,160 --> 00:52:55,380
그리고 간단한 동작 계획기를 사용하여 로봇을 그

1236
00:52:55,380 --> 00:52:56,700
물병으로 이동시켰습니다.

1237
00:52:56,700 --> 00:52:59,420
다음으로 우리는 그 물병을 더러운 접시가 있는

1238
00:52:59,420 --> 00:53:01,140
곳으로 옮기도록 요청했습니다.

1239
00:53:01,140 --> 00:53:02,460
그것은 싱크대-- 죄송합니다--

1240
00:53:02,460 --> 00:53:05,460
싱크대를 가리키고, 그곳으로 로봇을 이동시킵니다.

1241
00:53:05,460 --> 00:53:07,300
그리고 우리는 그것에게 싱크대의

1242
00:53:07,300 --> 00:53:09,540
빈 공간이 어디인지 가리키고 그

1243
00:53:09,540 --> 00:53:11,460
위치에 물병을 놓도록 요청합니다.

1244
00:53:11,460 --> 00:53:15,100
따라서 이제 이러한 모든 기능을 결합하고 체인으로

1245
00:53:15,100 --> 00:53:19,460
연결하여 많은 로봇 공학 응용 프로그램을 자동화할 수 있습니다.

1246
00:53:19,460 --> 00:53:21,660
현재 제 그룹에서는 이러한 시각적

1247
00:53:21,660 --> 00:53:24,070
언어 모델을 조정하고 실제 물리적

1248
00:53:24,070 --> 00:53:26,150
영역에서 많은 일반화를 가능하게

1249
00:53:26,150 --> 00:53:28,750
하는 데 많은 초점을 맞추고 있습니다.

1250
00:53:28,750 --> 00:53:31,590
따라서 질문은 이러한 모델이

1251
00:53:31,590 --> 00:53:34,950
이미지의 해상도를 항상 고정 해상도로 변경할

1252
00:53:34,950 --> 00:53:38,830
경우 가리킬 수 있을지에 관한 것입니다.

1253
00:53:38,830 --> 00:53:41,030
실제로 이러한 모델을 현재 어떤

1254
00:53:41,030 --> 00:53:43,750
해상도로도 조정할 수 있다는 것이 밝혀졌습니다.

1255
00:53:43,750 --> 00:53:47,910
변수 크기 이미지 입력을 허용하는

1256
00:53:47,910 --> 00:53:52,350
방법을 도입한 메커니즘이 있으며, 새로운

1257
00:53:52,350 --> 00:53:57,230
공간에서 가리키도록 조정할 수 있습니다.

1258
00:53:57,230 --> 00:53:59,430
따라서 모델의 위치

1259
00:53:59,430 --> 00:54:03,710
임베딩은 이미지 크기에 따라 변경되며, 일반적으로

1260
00:54:03,710 --> 00:54:07,230
잘 일반화되는 경향이 있습니다.

1261
00:54:07,230 --> 00:54:10,510
그래서 비전과 다중 모달 모델을 결합하는

1262
00:54:10,510 --> 00:54:12,270
것에 대한 대화였습니다.

1263
00:54:12,270 --> 00:54:14,467
남은 20분 동안, 이미지

1264
00:54:14,467 --> 00:54:16,550
분류와 텍스트뿐만 아니라 사용자가

1265
00:54:16,550 --> 00:54:19,690
관심 있는 모든 출력 공간에 일반화할

1266
00:54:19,690 --> 00:54:22,120
수 있는 이러한 기초 모델에 대해

1267
00:54:22,120 --> 00:54:23,720
이야기하고 싶습니다.

1268
00:54:23,720 --> 00:54:25,320
이 분야에서 정말 인기를 끌고

1269
00:54:25,320 --> 00:54:29,440
있는 모델 중 하나가 바로 Segment Anything Model입니다.

1270
00:54:29,440 --> 00:54:31,480
Segment Anything

1271
00:54:31,480 --> 00:54:34,320
Model, 줄여서

1272
00:54:34,320 --> 00:54:37,000
SAM은 모든 종류의 세분화 작업을

1273
00:54:37,000 --> 00:54:40,360
위한 기초 모델을 구축하려고 합니다.

1274
00:54:40,360 --> 00:54:41,920
그들이 정말로 하려는

1275
00:54:41,920 --> 00:54:45,720
것은 누구나 이미지에서 관심 있는 것을 가리킬

1276
00:54:45,720 --> 00:54:48,080
수 있도록 하고, 그 대상이

1277
00:54:48,080 --> 00:54:53,360
모델이 마스크를 출력할 수 있는 것이 되기를 희망하는 것입니다.

1278
00:54:53,360 --> 00:54:56,000
예를 들어, 고정된 수의 카테고리를

1279
00:54:56,000 --> 00:54:58,560
넘어 사용자가 관심 있는 모든 카테고리에

1280
00:54:58,560 --> 00:55:00,760
일반화되는 모델을 원합니다.

1281
00:55:00,760 --> 00:55:03,080
이러한 출력은 사용자가 관심 있는

1282
00:55:03,080 --> 00:55:07,220
모든 카테고리에 대한 마스크가 되기를 이상적으로 원합니다.

1283
00:55:07,220 --> 00:55:09,080
따라서 우리는 모든 카테고리, 즉

1284
00:55:09,080 --> 00:55:11,720
매우 많은 카테고리에 일반화하고자 하는 두 가지

1285
00:55:11,720 --> 00:55:12,700
목표가 있습니다.

1286
00:55:12,700 --> 00:55:15,800
그리고 우리는 사용자가 정말로 관심 있는 것을 매우

1287
00:55:15,800 --> 00:55:18,300
구체적으로 출력할 수 있기를 원합니다.

1288
00:55:18,300 --> 00:55:19,510
따라서 두 가지 모두 도전 과제입니다.

1289
00:55:19,510 --> 00:55:20,770
다양한 카테고리를

1290
00:55:20,770 --> 00:55:22,770
아우르는 대량의 데이터를

1291
00:55:22,770 --> 00:55:24,870
수집하는 방법과 사용자가 정말로

1292
00:55:24,870 --> 00:55:27,330
관심 있는 것을 정확히 파악하는

1293
00:55:27,330 --> 00:55:31,050
아키텍처를 설계하는 방법 모두 도전 과제입니다.

1294
00:55:31,050 --> 00:55:33,930
이제 두 번째 질문부터 시작해 보겠습니다.

1295
00:55:33,930 --> 00:55:38,910
무언가에 대한 마스크를 원할 때 정말 모호합니다.

1296
00:55:38,910 --> 00:55:42,850
예를 들어, 이미지에 두 마리의 고양이가 있고 사용자가

1297
00:55:42,850 --> 00:55:44,570
'고양이에 대한 세분화를

1298
00:55:44,570 --> 00:55:47,450
원한다'고 말할 때, 어떤 고양이에 대한

1299
00:55:47,450 --> 00:55:50,090
세분화를 원하는지 명확하지 않습니다.

1300
00:55:50,090 --> 00:55:52,750
이상적으로는 더 많은 마스크 포인팅 기능이 있다면, 실제로

1301
00:55:52,750 --> 00:55:54,990
어떤 고양이에 관심이 있는지를 가리킬 수 있습니다.

1302
00:55:54,990 --> 00:55:56,490
그리고 그 지점에

1303
00:55:56,490 --> 00:55:59,690
따라 중요한 마스크를 만들 수 있습니다.

1304
00:55:59,690 --> 00:56:01,755
물론, 이것들은 그리 좋은 마스크가 아닙니다.

1305
00:56:01,755 --> 00:56:03,130
이상적으로는

1306
00:56:03,130 --> 00:56:06,290
이러한 마스크가 이미지 편집이나 다른

1307
00:56:06,290 --> 00:56:09,170
다양한 하위 응용 프로그램을

1308
00:56:09,170 --> 00:56:11,770
지원할 수 있을 만큼 품질이

1309
00:56:11,770 --> 00:56:13,930
매우 뛰어나야 합니다.

1310
00:56:13,930 --> 00:56:16,650
사용자가 자신이 중요하게 생각하는

1311
00:56:16,650 --> 00:56:19,280
것을 정확히 지정할 수 있도록 이

1312
00:56:19,280 --> 00:56:22,700
아키텍처를 구축하려면 단순히 텍스트를 입력하는 것

1313
00:56:22,700 --> 00:56:24,620
이상으로 나아가야 합니다.

1314
00:56:24,620 --> 00:56:28,538
SAM 아키텍처는 두 개 또는 세 개의 구성 요소를 가지고

1315
00:56:28,538 --> 00:56:29,080
있습니다.

1316
00:56:29,080 --> 00:56:31,760
이미지 인코더가 있으며, 이는 다시

1317
00:56:31,760 --> 00:56:33,780
클립 인코더일 수 있습니다.

1318
00:56:33,780 --> 00:56:36,720
그리고 특별한 프롬프트 인코더가 있습니다.

1319
00:56:36,720 --> 00:56:40,580
이 프롬프트 인코더는 사용자가 중요하게 생각하는 것을

1320
00:56:40,580 --> 00:56:43,180
지정할 수 있는 텍스트, 포인트, 바운딩

1321
00:56:43,180 --> 00:56:45,300
박스 등을 인코딩하려고 합니다.

1322
00:56:45,300 --> 00:56:47,220
그리고 이 두 가지를 바탕으로

1323
00:56:47,220 --> 00:56:49,460
매우 가벼운 디코더를 통해

1324
00:56:49,460 --> 00:56:50,640
마스크를 출력합니다.

1325
00:56:50,640 --> 00:56:53,900
디코더는 이 과정에서 이미 본 세분화

1326
00:56:53,900 --> 00:56:56,900
디코더와 매우 유사하게 보입니다.

1327
00:56:56,900 --> 00:56:59,540
전반적으로 모델은 이렇게 생겼습니다.

1328
00:56:59,540 --> 00:57:03,580
이미지를 주면 이미지 인코더를 사용하여 그 이미지를 인코딩합니다.

1329
00:57:03,580 --> 00:57:06,100
그리고 다양한 프롬프트가

1330
00:57:06,100 --> 00:57:09,340
디코더를 통해 이미지

1331
00:57:09,340 --> 00:57:12,040
인코딩과 상호작용하며

1332
00:57:12,040 --> 00:57:13,980
마스크를 출력합니다.

1333
00:57:13,980 --> 00:57:16,790
이것이 전체 아키텍처 디자인입니다.

1334
00:57:16,790 --> 00:57:20,330
이제 세분화와 관련된 큰 문제가 하나 있습니다.

1335
00:57:20,330 --> 00:57:23,150
사용자가 특정 위치를 가리키고 '이 위치에 대한

1336
00:57:23,150 --> 00:57:25,270
세분화 마스크를 원한다'고 말한다고

1337
00:57:25,270 --> 00:57:26,477
가정해 보겠습니다.

1338
00:57:26,477 --> 00:57:28,310
그 세분화 마스크의 문제는

1339
00:57:28,310 --> 00:57:30,530
여전히 모호하다는 것입니다.

1340
00:57:30,530 --> 00:57:32,830
포인트가 있어도 충분하지 않습니다.

1341
00:57:32,830 --> 00:57:36,710
그 포인트가 전체 가위를 가리킬 수도 있습니다.

1342
00:57:36,710 --> 00:57:39,790
그것은 당신이 잡을 수 있는 부분만을 가리키고 있을 수도

1343
00:57:39,790 --> 00:57:42,870
있고, 또는 당신이 잡을 수 있는 부분 중 하나를

1344
00:57:42,870 --> 00:57:43,950
가리킬 수도 있습니다.

1345
00:57:43,950 --> 00:57:47,550
이러한 모호성은 해결하기 매우 어렵습니다.

1346
00:57:47,550 --> 00:57:49,270
그리고 잘못된 것을 선택했다고

1347
00:57:49,270 --> 00:57:50,830
모델을 처벌하고 싶지 않습니다.

1348
00:57:50,830 --> 00:57:52,590
SAM 아키텍처는 하나의

1349
00:57:52,590 --> 00:57:55,050
세분화 마스크를 출력하는 대신 서로

1350
00:57:55,050 --> 00:57:57,150
다른 세분화 수준에서 세 개의

1351
00:57:57,150 --> 00:57:59,070
세분화 마스크를 출력합니다.

1352
00:57:59,070 --> 00:58:01,350
그런 다음 실제 값과 가장 잘

1353
00:58:01,350 --> 00:58:02,990
일치하는 것을 선택하고

1354
00:58:02,990 --> 00:58:06,110
이를 사용하여 손실을 계산하여 다른 것들을

1355
00:58:06,110 --> 00:58:07,190
처벌하지 않습니다.

1356
00:58:07,190 --> 00:58:09,870
따라서 시간이 지남에 따라 이 모델이 다양한

1357
00:58:09,870 --> 00:58:12,130
종류의 마스크를 출력하도록 학습할 것이라는

1358
00:58:12,130 --> 00:58:14,120
희망이 있습니다. 사용자는 자신의

1359
00:58:14,120 --> 00:58:17,280
사용 사례에 가장 적합한 것을 선택할 수 있습니다.

1360
00:58:17,280 --> 00:58:19,860
이 모든 것을 종합하면,

1361
00:58:19,860 --> 00:58:22,860
이제 필요한 것은 데이터입니다.

1362
00:58:22,860 --> 00:58:25,720
이 모델을 가능하게 하려면 다양한

1363
00:58:25,720 --> 00:58:28,000
카테고리에서 많은 데이터가 필요합니다.

1364
00:58:28,000 --> 00:58:30,880
데이터의 문제는 2023년에 이 모델이

1365
00:58:30,880 --> 00:58:34,960
출시되기 전까지 약 1년 반, 어쩌면 2년 전이었습니다.

1366
00:58:34,960 --> 00:58:38,120
이 모델이 출시되었을 때 대부분의 세분화 데이터

1367
00:58:38,120 --> 00:58:39,840
세트는 매우 작았습니다.

1368
00:58:39,840 --> 00:58:43,365
이 논문의 저자들은 세분화 데이터

1369
00:58:43,365 --> 00:58:45,740
세트의 양을 약

1370
00:58:45,740 --> 00:58:49,000
6배, 이미지의 양을 약

1371
00:58:49,000 --> 00:58:51,040
400배로 늘렸습니다.

1372
00:58:51,040 --> 00:58:55,080
그래서 이 모델이 가능한 한 성능이 뛰어나도록 많은

1373
00:58:55,080 --> 00:58:57,560
마스크를 수집하고 성장시켰습니다.

1374
00:58:57,560 --> 00:58:59,800
다시 말해, 메시지는 Flamingo와

1375
00:58:59,800 --> 00:59:02,280
Molmo에서의 메시지와 매우 유사합니다.

1376
00:59:02,280 --> 00:59:04,460
메시지는 이러한 모델이 가능한 한

1377
00:59:04,460 --> 00:59:07,240
성능이 뛰어나도록 정말로 좋은 고품질

1378
00:59:07,240 --> 00:59:08,860
데이터가 필요하다는 것입니다.

1379
00:59:08,860 --> 00:59:12,050
많은 비전 작업의 경우 데이터가 인터넷에서

1380
00:59:12,050 --> 00:59:15,210
완전히 누락되어 있으며, 이러한 모델이 잘

1381
00:59:15,210 --> 00:59:18,330
작동하도록 데이터를 찾아 수집해야 합니다.

1382
00:59:18,330 --> 00:59:21,250
그래서 이 데이터를 만들기 위해,

1383
00:59:21,250 --> 00:59:23,450
그들은 처음에 일부 데이터에

1384
00:59:23,450 --> 00:59:27,010
주석을 달아 루프 프로세스를 만들었습니다.

1385
00:59:27,010 --> 00:59:28,490
그 주석에서 그들은 훈련 데이터

1386
00:59:28,490 --> 00:59:30,430
세트를 만들고, 모델을 훈련시킨 후, 그

1387
00:59:30,430 --> 00:59:32,730
모델을 사용하여 더 많은 데이터에 주석을 달았습니다.

1388
00:59:32,730 --> 00:59:34,690
그리고 그들은 사용자와 함께

1389
00:59:34,690 --> 00:59:37,650
모델이 생성한 세그먼트를 반복적으로 개선하며

1390
00:59:37,650 --> 00:59:39,450
이 과정을 계속했습니다.

1391
00:59:39,450 --> 00:59:41,730
그래서 그들은 인간과 모델이 함께하는

1392
00:59:41,730 --> 00:59:45,210
루프 프로세스를 가지고, 세그먼트를 제안하고 인간

1393
00:59:45,210 --> 00:59:47,890
주석자를 사용하여 세그먼트를 수정했습니다.

1394
00:59:47,890 --> 00:59:50,750
이것은 그들의 데이터 세트에서의 예시 이미지입니다.

1395
00:59:50,750 --> 00:59:52,590
카테고리가 꽤 많습니다.

1396
00:59:52,590 --> 00:59:57,130
각 개별 채소는 고유한 마스크로 주석이 달려 있습니다.

1397
00:59:57,130 --> 00:59:59,570
그래서 수집하는 데 꽤 비쌉니다.

1398
00:59:59,570 --> 01:00:04,770
그리고 그들은 수백만 개의 이미지에서 이 작업을 수행했습니다.

1399
01:00:04,770 --> 01:00:07,850
여기 또 다른 예가 있으며, 다시 모든 단일 우산에

1400
01:00:07,850 --> 01:00:09,290
주석이 달려 있습니다.

1401
01:00:09,290 --> 01:00:11,560
다시, 여기 수중에서의 또 다른 예가 있습니다.

1402
01:00:11,560 --> 01:00:14,080
그리고 물론, 그림도 있습니다.

1403
01:00:14,080 --> 01:00:16,380
그들은 그림의 세그멘테이션을 가지고 있습니다.

1404
01:00:16,380 --> 01:00:18,140
그래서 이 모든 것이

1405
01:00:18,140 --> 01:00:20,820
세그멘테이션을 위한 기초 모델을

1406
01:00:20,820 --> 01:00:24,028
만드는 데 정말로 기초가 되었습니다.

1407
01:00:24,028 --> 01:00:25,320
그래서 이것이 Segment Anything에 대한 것입니다.

1408
01:00:25,320 --> 01:00:27,153
오늘 남은 몇 분을

1409
01:00:27,153 --> 01:00:29,380
사용하여 멀티모달

1410
01:00:29,380 --> 01:00:34,100
언어 모델의 마지막 부분인 체인에 정말 집중하고

1411
01:00:34,100 --> 01:00:35,100
싶습니다.

1412
01:00:35,100 --> 01:00:38,880
체인의 아이디어는 여러분이 이미 본 것입니다.

1413
01:00:38,880 --> 01:00:41,370
이 강의 내내 이미 힌트를 드렸습니다.

1414
01:00:41,370 --> 01:00:43,620
아이디어는 서로 다른 모델을 결합하여

1415
01:00:43,620 --> 01:00:47,420
단일 모델이 혼자서는 할 수 없는 작업을 가능하게 하는 것입니다.

1416
01:00:47,420 --> 01:00:50,660
여기 우리가 수업으로 할 수 있는 재미있는 작은 연습이 있습니다.

1417
01:00:50,660 --> 01:00:53,740
그래서 저는 여러분에게 네 개의 이미지를 주고

1418
01:00:53,740 --> 01:00:55,860
네 개의 카테고리를 제공합니다.

1419
01:00:55,860 --> 01:00:59,020
이것들은 여러분 중 일부가 본 적이 없는

1420
01:00:59,020 --> 01:01:00,540
카테고리일 수 있습니다.

1421
01:01:00,540 --> 01:01:03,460
그리고 CLIP이 본 적이 없는 카테고리이기도 합니다.

1422
01:01:03,460 --> 01:01:05,540
그래서 CLIP은 어떤 것이 어떤 것과

1423
01:01:05,540 --> 01:01:07,123
관련이 있는지 전혀 알지 못하기

1424
01:01:07,123 --> 01:01:09,110
때문에 이 카테고리에서 실패합니다.

1425
01:01:09,110 --> 01:01:12,190
여기서 누가 어떤 것인지 아는 사람이 있나요?

1426
01:01:12,190 --> 01:01:13,930
마림바, 네, 두 번째 것은 마림바입니다.

1427
01:01:13,930 --> 01:01:14,472
맞습니다.

1428
01:01:14,472 --> 01:01:16,250
네, 하나는 조금 쉽습니다.

1429
01:01:16,250 --> 01:01:17,750
고가도로.

1430
01:01:17,750 --> 01:01:21,630
네, 많은 분들이 그게 무엇인지 아는 것 같습니다.

1431
01:01:21,630 --> 01:01:25,390
하지만, 어떤 것이 개이고 어떤 것이 새인가요?

1432
01:01:25,390 --> 01:01:28,430
이 대신에 제가 여러분에게 이것을 드린다면 어떻게 될까요?

1433
01:01:28,430 --> 01:01:31,322
이제 제가 이러한 것들에 대한 설명을

1434
01:01:31,322 --> 01:01:33,030
드리면, 각 항목을 올바른

1435
01:01:33,030 --> 01:01:36,276
범주와 쉽게 연관 지을 수 있게 됩니다.

1436
01:01:36,276 --> 01:01:38,830
이것이 체인의 기본 아이디어입니다.

1437
01:01:38,830 --> 01:01:41,290
CLIP이 이러한 이미지를 본 적이

1438
01:01:41,290 --> 01:01:44,150
없더라도, 이러한 개념들이 인터넷에서 어느

1439
01:01:44,150 --> 01:01:46,030
정도 언급되었을 가능성이

1440
01:01:46,030 --> 01:01:49,870
높고, GPT가 이를 설명할 수 있을 가능성이 큽니다.

1441
01:01:49,870 --> 01:01:52,350
GPT가 이러한 설명을 생성할 수 있다면,

1442
01:01:52,350 --> 01:01:54,750
이제 그 설명은 정확히 어떤 범주가

1443
01:01:54,750 --> 01:01:58,150
어떤 것인지 분류하는 데 매우 좋은 방법이 됩니다.

1444
01:01:58,150 --> 01:01:59,730
체인의 아이디어는 한

1445
01:01:59,730 --> 01:02:01,690
모델의 강점을 활용하고

1446
01:02:01,690 --> 01:02:03,890
다른 모델의 기능과 결합하여

1447
01:02:03,890 --> 01:02:06,230
이전에는 없었던 다양한 새로운

1448
01:02:06,230 --> 01:02:07,887
기능을 얻는 것입니다.

1449
01:02:07,887 --> 01:02:09,720
따라서 CLIP에 훈련 데이터가

1450
01:02:09,720 --> 01:02:12,240
없는 많은 범주를 가져올 수 있습니다.

1451
01:02:12,240 --> 01:02:14,080
하지만 모든 것을 설명할 수 있다면,

1452
01:02:14,080 --> 01:02:16,500
CLIP은 많은 설명을 본 적이 있기 때문에

1453
01:02:16,500 --> 01:02:19,080
이제 모든 것을 매우 잘 분류할 수 있습니다.

1454
01:02:19,080 --> 01:02:21,600
개별 꽃이나 개별 자동차, 개별

1455
01:02:21,600 --> 01:02:24,360
공간 또는 다양한 종류의 애완동물에

1456
01:02:24,360 --> 01:02:28,280
대한 분류를 생성하도록 CLIP을 시작할 수 있습니다.

1457
01:02:28,280 --> 01:02:30,080
그리고 더 세분화된 전문

1458
01:02:30,080 --> 01:02:32,000
범주에 대한 다양한 데이터

1459
01:02:32,000 --> 01:02:35,258
세트에서 이러한 개선을 보기 시작합니다.

1460
01:02:35,258 --> 01:02:36,800
이것이 가능했던 유일한

1461
01:02:36,800 --> 01:02:39,680
방법은 GPT가 이러한 것들을 설명할

1462
01:02:39,680 --> 01:02:42,600
수 있는 능력을 흡수했기 때문입니다.

1463
01:02:42,600 --> 01:02:45,600
새로운 기능을 일반화할 수 있는 이

1464
01:02:45,600 --> 01:02:49,180
아이디어는 작년에 매우 인기가 있었고,

1465
01:02:49,180 --> 01:02:52,520
올해도 여전히 매우 인기가 있습니다.

1466
01:02:52,520 --> 01:02:56,280
모든 질문에 대해 체인의 아이디어를

1467
01:02:56,280 --> 01:02:58,960
통해 가능합니다.

1468
01:02:58,960 --> 01:03:01,640
예를 들어, 제가 당신에게 보트에

1469
01:03:01,640 --> 01:03:04,120
세 사람이 있는지 물어본다면?

1470
01:03:04,120 --> 01:03:07,480
이 질문에 답하기 위해 다중 모달 언어

1471
01:03:07,480 --> 01:03:09,940
모델에 다시 질문하거나,

1472
01:03:09,940 --> 01:03:13,360
지난 몇 십 년 동안 개발해온 수백

1473
01:03:13,360 --> 01:03:15,440
개의 전문 비전 모델을

1474
01:03:15,440 --> 01:03:17,062
사용할 수 있습니다.

1475
01:03:17,062 --> 01:03:19,520
수업에서 배운 객체 탐지 모델이

1476
01:03:19,520 --> 01:03:20,300
있습니다.

1477
01:03:20,300 --> 01:03:21,800
객체 탐지기를 사용하면 세

1478
01:03:21,800 --> 01:03:24,133
사람 각각에 대한 탐지를 얻을 수 있습니다.

1479
01:03:24,133 --> 01:03:25,800
그런 다음, 탐지가 세 개이므로

1480
01:03:25,800 --> 01:03:27,640
'세 사람입니다'라고 말할 수 있습니다.

1481
01:03:27,640 --> 01:03:32,800
이것이 일반적인 아이디어입니다. 다른 모델의 출력을 함께 훈련시켜

1482
01:03:32,800 --> 01:03:35,720
새로운 기능을 수행할 수 있습니다.

1483
01:03:35,720 --> 01:03:37,320
다른 예를 들어보겠습니다.

1484
01:03:37,320 --> 01:03:39,200
제가 당신에게 이 두 보트에 총

1485
01:03:39,200 --> 01:03:42,518
몇 명의 사람이 있는지 물어본다면, 여섯 명인가요?

1486
01:03:42,518 --> 01:03:44,060
다시 같은 방식으로 할 수 있습니다.

1487
01:03:44,060 --> 01:03:47,120
첫 번째 이미지에서 객체 탐지를 수행하는 프로그램을 작성한

1488
01:03:47,120 --> 01:03:49,000
다음, 두 번째 이미지에서 객체

1489
01:03:49,000 --> 01:03:52,260
탐지를 수행하고, 그 모든 구성 요소를 더할 수 있습니다.

1490
01:03:52,260 --> 01:03:57,933
이것이 우리가 이제 체인이라고 부르는 기본 아이디어입니다.

1491
01:03:57,933 --> 01:03:59,600
이 아이디어는 작년에

1492
01:03:59,600 --> 01:04:03,370
Best Paper 상을 수상한 VisProg라는 논문에 의해

1493
01:04:03,370 --> 01:04:06,930
대중화되었습니다. 이 VisProg 논문, 즉 시각

1494
01:04:06,930 --> 01:04:10,810
프로그래밍 논문에서는 어떤 이미지나 질문을 가져와 프로그램을

1495
01:04:10,810 --> 01:04:13,090
생성하는 아이디어가 있었습니다.

1496
01:04:13,090 --> 01:04:16,530
이미지 1에 대해 무언가를 답하고, 이미지 2에

1497
01:04:16,530 --> 01:04:19,290
대해 무언가를 답한 다음, 그 답변을

1498
01:04:19,290 --> 01:04:22,890
결합하여 최종 답변을 제공하는 프로그램을 생성합니다.

1499
01:04:22,890 --> 01:04:25,230
Python에서 함수를 작성하고,

1500
01:04:25,230 --> 01:04:27,230
그 Python 함수 내에서

1501
01:04:27,230 --> 01:04:30,170
우리가 이미 훈련에서 본 다른 모델에

1502
01:04:30,170 --> 01:04:32,290
대한 개별 호출을 포함합니다.

1503
01:04:32,290 --> 01:04:35,890
예를 들어, 이 특정 질문을 할 때, 이 진술이 참인지

1504
01:04:35,890 --> 01:04:37,742
여부를 결정하는 경우, 왼쪽

1505
01:04:37,742 --> 01:04:39,450
및 오른쪽 이미지에는 총

1506
01:04:39,450 --> 01:04:41,610
6명이 있고 2개의 보트가 있습니다.

1507
01:04:41,610 --> 01:04:44,210
GPT에게 이 질문에 답하려고

1508
01:04:44,210 --> 01:04:47,030
하는 프로그램을 실제로 생성하도록 요청할

1509
01:04:47,030 --> 01:04:51,610
수 있으며, 그 프로그램의 답변을 가져올 수 있습니다.

1510
01:04:51,610 --> 01:04:55,530
또한 GPT에게 다른 함수들을 사용하여 생성할 수 있는 프로그램의

1511
01:04:55,530 --> 01:04:57,450
예를 제공하여 인-context

1512
01:04:57,450 --> 01:04:59,670
예제를 수행하도록 할 수 있습니다.

1513
01:04:59,670 --> 01:05:02,900
우리는 그것이 새로운 질문에 일반화되고, 사용할

1514
01:05:02,900 --> 01:05:05,460
수 있는 모든 기능을 사용하기 시작하는

1515
01:05:05,460 --> 01:05:06,835
것을 볼 수 있습니다.

1516
01:05:06,835 --> 01:05:08,460
물론, 당신이 해야 할 한

1517
01:05:08,460 --> 01:05:10,680
가지는 함수 자체를 제공하는 것입니다.

1518
01:05:10,680 --> 01:05:12,260
다른 모델에서 사용할 수

1519
01:05:12,260 --> 01:05:15,280
있는 이러한 기능이 있다는 것을 알려줘야 합니다.

1520
01:05:15,280 --> 01:05:17,600
객체 탐지기를 사용하여 사물을 로컬라이즈할 수 있습니다.

1521
01:05:17,600 --> 01:05:21,380
얼굴 탐지기를 사용하여 얼굴을 로컬라이즈할 수 있습니다.

1522
01:05:21,380 --> 01:05:24,340
그리고 사람들이 만든 다양한 다른

1523
01:05:24,340 --> 01:05:27,760
모델에서 이러한 다양한 기능을 가질 수 있습니다.

1524
01:05:27,760 --> 01:05:30,800
그리고 이들을 연결하여 다양한 작업을 수행할 수 있습니다.

1525
01:05:30,800 --> 01:05:32,800
네, 그래서 두 가지 방법이 있습니다.

1526
01:05:32,800 --> 01:05:34,500
하나는 가능한 한 다양한

1527
01:05:34,500 --> 01:05:36,860
예제를 제공하여 일반화되기를

1528
01:05:36,860 --> 01:05:38,680
바라는 정적 방법입니다.

1529
01:05:38,680 --> 01:05:40,840
다른 하나는 이 질문에 대해 어떤

1530
01:05:40,840 --> 01:05:43,860
최상의 맥락 예제를 사용해야 하는지를 동적으로

1531
01:05:43,860 --> 01:05:44,900
선택하는 것입니다.

1532
01:05:44,900 --> 01:05:48,420
그래서 이를 또 다른 검색 프로세스로 간주할 수 있으며,

1533
01:05:48,420 --> 01:05:50,780
최상의 예제를 검색한 다음

1534
01:05:50,780 --> 01:05:52,540
프로그램 생성을 요청합니다.

1535
01:05:52,540 --> 01:05:54,640
그리고 이는 훨씬 더 잘 수행되는

1536
01:05:54,640 --> 01:05:58,620
경향이 있지만, 좋은 검색 시스템이 있을 때만 그렇습니다.

1537
01:05:58,620 --> 01:06:00,690
네, 많은 계산이 필요할 것입니다.

1538
01:06:00,690 --> 01:06:04,030
그래서 GPT를 호출하는 데 필요한 계산이 있으며, 이는 API를

1539
01:06:04,030 --> 01:06:05,335
통해 수행해야 합니다.

1540
01:06:05,335 --> 01:06:07,710
그리고 이러한 개별 모델을 각각

1541
01:06:07,710 --> 01:06:10,790
메모리에 로드한 다음 순차적으로 실행해야 합니다.

1542
01:06:10,790 --> 01:06:13,630
그래서 실제로 비용이 더 많이 들 수 있습니다.

1543
01:06:13,630 --> 01:06:15,770
그래서 사람들이 하려는 것은 이러한 기능을

1544
01:06:15,770 --> 01:06:17,150
단일 모델로 증류할 수

1545
01:06:17,150 --> 01:06:18,670
있는지를 알아내는 것입니다.

1546
01:06:18,670 --> 01:06:20,430
그리고 그것이 2025년

1547
01:06:20,430 --> 01:06:22,550
오늘날 연구의 큰 부분입니다.

1548
01:06:22,550 --> 01:06:24,570
하지만 물론 사람들은 이러한 것들을

1549
01:06:24,570 --> 01:06:27,070
효과적으로 잘 연결하는 방법을 여전히 알아내려고

1550
01:06:27,070 --> 01:06:28,150
하고 있습니다.

1551
01:06:28,150 --> 01:06:29,810
네, 이를 에이전트로 생각할 수 있습니다.

1552
01:06:29,810 --> 01:06:30,310
네.

1553
01:06:30,310 --> 01:06:32,570
그래서 이 질문을 고려할 때, 어떤

1554
01:06:32,570 --> 01:06:35,030
다른 모델에서 도움을 받아야 하는지

1555
01:06:35,030 --> 01:06:39,282
결정하고 이를 함께 연결하여 새로운 기능을 수행하는 에이전트가 있습니다.

1556
01:06:39,282 --> 01:06:40,490
그래서 그것이 어떻게 보이는지입니다.

1557
01:06:40,490 --> 01:06:41,590
네.

1558
01:06:41,590 --> 01:06:44,550
여기 이미지 편집을 하고 싶을 때의 또 다른 예가 있습니다.

1559
01:06:44,550 --> 01:06:47,030
사막의 모래를 푸른 풀로 교체하는

1560
01:06:47,030 --> 01:06:48,990
것에 관심이 있을 수 있습니다.

1561
01:06:48,990 --> 01:06:51,923
물론 이미지 편집 모델은 아직 초기 단계입니다.

1562
01:06:51,923 --> 01:06:53,590
그래서 대신 세분화

1563
01:06:53,590 --> 01:06:56,850
모델을 호출하여 사막을 식별하고,

1564
01:06:56,850 --> 01:07:01,520
사막 부분, 즉 픽셀을 풀로 교체한 다음, 이를

1565
01:07:01,520 --> 01:07:05,000
합성하여 새로운 이미지를 만들 수 있습니다.

1566
01:07:05,000 --> 01:07:08,440
이것이 제가 다양한 기능에 대해 이야기하고

1567
01:07:08,440 --> 01:07:10,200
싶었던 모든 것입니다.

1568
01:07:10,200 --> 01:07:13,520
그래서 여기 기초 모델에 대해 생각하는

1569
01:07:13,520 --> 01:07:16,440
방법에 대한 몇 가지 기능이 있습니다.

1570
01:07:16,440 --> 01:07:18,560
결국 단일 작업을 위해

1571
01:07:18,560 --> 01:07:22,120
모델을 훈련시키고, 그 단일

1572
01:07:22,120 --> 01:07:25,640
작업에서 많은 다양한 하위 응용

1573
01:07:25,640 --> 01:07:28,560
프로그램으로 일반화하는 능력입니다.

1574
01:07:28,560 --> 01:07:30,520
그리고 우리는 분류에서

1575
01:07:30,520 --> 01:07:33,360
인터넷에서 많은 이미지-텍스트 쌍을 가져와

1576
01:07:33,360 --> 01:07:36,040
서로 훈련시켜 다양한 작업을 수행하는

1577
01:07:36,040 --> 01:07:37,920
방법에 대해 이야기했습니다.

1578
01:07:37,920 --> 01:07:39,400
그리고 이는 실제 세계에

1579
01:07:39,400 --> 01:07:41,800
존재하지 않거나 실제 세계에서

1580
01:07:41,800 --> 01:07:44,080
레이블이 없는 새로운 데이터 세트로

1581
01:07:44,080 --> 01:07:45,720
일반화할 수 있게 해줍니다.

1582
01:07:45,720 --> 01:07:48,040
또한 언어 모델과 결합하여 캡션

1583
01:07:48,040 --> 01:07:51,640
작성, 카운팅 또는 OCR과 같은 맥락 예제를 수행하도록

1584
01:07:51,640 --> 01:07:53,360
훈련할 수 있습니다.

1585
01:07:53,360 --> 01:07:55,280
그리고 이는 다시 많은 다양한 응용

1586
01:07:55,280 --> 01:07:57,010
프로그램을 가능하게 하는 기능입니다.

1587
01:07:57,010 --> 01:07:58,890
그리고 물론 출력이 항상 언어나

1588
01:07:58,890 --> 01:08:00,510
범주일 필요는 없습니다.

1589
01:08:00,510 --> 01:08:02,610
그들은 또한 서로 다른 사용자 입력에

1590
01:08:02,610 --> 01:08:04,810
따라 다양한 종류의 마스크를 사용할

1591
01:08:04,810 --> 01:08:06,850
수 있는 분할 마스크일 수 있습니다.

1592
01:08:06,850 --> 01:08:09,650
그리고 이러한 여러 기초 모델이나 더 작은

1593
01:08:09,650 --> 01:08:11,850
모델들을 프로그램을 통해 결합하여

1594
01:08:11,850 --> 01:08:16,450
더 일반화할 수 있으며, 새로운 다양한 작업을 수행할 수 있습니다.

1595
01:08:16,450 --> 01:08:20,490
그래서 환각은 여전히 모든 곳에서 발생합니다.

1596
01:08:20,490 --> 01:08:22,450
우리가 보여주는 것은 포인팅이

1597
01:08:22,450 --> 01:08:25,090
생성물에 대한 증거를 찾아야 하기

1598
01:08:25,090 --> 01:08:27,569
때문에 환각을 상당히 줄이는 것처럼

1599
01:08:27,569 --> 01:08:29,021
보인다는 것입니다.

1600
01:08:29,021 --> 01:08:30,729
하지만 그렇다고 해서 반드시

1601
01:08:30,729 --> 01:08:33,649
올바른 것을 가리킬 것이라는 보장은 없습니다.

1602
01:08:33,649 --> 01:08:35,790
그래서 이를 해결하는 방법은 여러 가지가 있습니다.

1603
01:08:35,790 --> 01:08:38,850
하나는 물론, 당신이 하고자 하는 추론과 관련된

1604
01:08:38,850 --> 01:08:41,490
더 많은 데이터를 수집하는 것입니다.

1605
01:08:41,490 --> 01:08:45,130
하지만 더 나은 방법은 포인트를 기반으로 출력이 신뢰할

1606
01:08:45,130 --> 01:08:48,410
수 있는 것인지 아닌지를 검증하는 검증 방법을

1607
01:08:48,410 --> 01:08:49,545
갖는 것입니다.

1608
01:08:49,545 --> 01:08:51,670
더 큰 모델과 큰 회사들은

1609
01:08:51,670 --> 01:08:55,660
일반적으로 그들의 모델을 사용할 때 단일

1610
01:08:55,660 --> 01:08:58,240
모델이 출력을 생성하지 않습니다.

1611
01:08:58,240 --> 01:09:01,540
보통 그 출력을 사용자가 보기 전에 다른

1612
01:09:01,540 --> 01:09:03,700
검증기를 통해 전달합니다.

1613
01:09:03,700 --> 01:09:06,060
그리고 그것은 이러한 문제들을 완화합니다.

1614
01:09:06,060 --> 01:09:08,680
하지만 현재 이것은 활발한 연구 주제입니다.

1615
01:09:08,680 --> 01:09:11,700
어떻게 환각을 줄이고 이러한 모델의 실제

1616
01:09:11,700 --> 01:09:13,420
정확성을 향상시킬 수 있을까요?

1617
01:09:13,420 --> 01:09:14,380
네.

1618
01:09:14,380 --> 01:09:17,380
당신의 질문을 반복하자면, 이러한

1619
01:09:17,380 --> 01:09:21,220
모델이 도구가 필요할 때 새로운 도구를

1620
01:09:21,220 --> 01:09:22,859
구축할 수 있을까요?

1621
01:09:22,859 --> 01:09:24,580
네, 가능합니다.

1622
01:09:24,580 --> 01:09:28,300
우리는 그 방향으로 몇 가지 초기 실험을 진행했으며, 모델에게

1623
01:09:28,300 --> 01:09:29,840
'내가 원하는 능력은

1624
01:09:29,840 --> 01:09:31,843
이것이다'라고 말할 수 있습니다.

1625
01:09:31,843 --> 01:09:33,260
그리고 당신이 구축할 수

1626
01:09:33,260 --> 01:09:35,979
있는 것은 특정 사용 사례를 위한 훈련

1627
01:09:35,979 --> 01:09:39,260
데이터를 자동으로 수집하고 도구를 만드는 시스템입니다.

1628
01:09:39,260 --> 01:09:42,180
하지만 그 작업은 여전히 초기 단계에 있습니다.

1629
01:09:42,180 --> 01:09:44,300
우리가 적극적으로 작업하고 있는 분야입니다.

1630
01:09:44,300 --> 01:09:48,829
하지만 많은 사람들이 그 방향에 대해 기대하고 있습니다.
