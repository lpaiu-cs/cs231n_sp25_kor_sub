1
00:00:05,440 --> 00:00:08,000
모두 와주셔서 감사합니다.

2
00:00:08,000 --> 00:00:10,000
오늘은 특별 강연이 있습니다.

3
00:00:10,000 --> 00:00:12,160
오늘은 Ranjay Krishna 교수님을 모셨습니다.

4
00:00:12,160 --> 00:00:14,160
Ranjay Krishna

5
00:00:14,160 --> 00:00:16,600
교수님은 워싱턴 대학교 컴퓨터 과학 및

6
00:00:16,600 --> 00:00:21,040
공학부 조교수이며, RAIVN 연구실을 공동으로 이끌고 있습니다.

7
00:00:21,040 --> 00:00:24,200
그는 2020년과 2021년에 CS231N

8
00:00:24,200 --> 00:00:27,160
강의를 맡았으며, 그의 연구는

9
00:00:27,160 --> 00:00:30,220
컴퓨터 비전, 자연어 처리, 로보틱스,

10
00:00:30,220 --> 00:00:33,280
인간-컴퓨터 상호작용의 교차점에

11
00:00:33,280 --> 00:00:34,240
있습니다.

12
00:00:34,240 --> 00:00:37,000
오늘 강의에서는 멀티모달 파운데이션 모델에 대해 이야기할

13
00:00:37,000 --> 00:00:37,500
것입니다.

14
00:00:37,500 --> 00:00:38,000
그리고.

15
00:00:38,000 --> 00:00:39,200
Ranjay 교수님, 발표를 부탁드립니다.

16
00:00:39,200 --> 00:00:40,103
감사합니다.

17
00:00:40,103 --> 00:00:41,020
다시 와서 정말 반갑습니다.

18
00:00:41,020 --> 00:00:43,080
제가 처음 이 강의를

19
00:00:43,080 --> 00:00:47,520
스탠포드에서 맡았던 게 2020년이었는데, 그때 약 3주

20
00:00:47,520 --> 00:00:51,560
동안 모든 강의 자료를 온라인으로 옮겨야 했습니다.

21
00:00:51,560 --> 00:00:55,320
그 이후로는 매년 강의가 훨씬 수월해졌습니다.

22
00:00:55,320 --> 00:00:56,300
다시 와서 정말 반갑습니다.

23
00:00:56,300 --> 00:00:58,675
오늘은 멀티모달 파운데이션 모델에 대해

24
00:00:58,675 --> 00:00:59,530
이야기하겠습니다.

25
00:00:59,530 --> 00:01:01,610
지금까지 이 수업의 많은

26
00:01:01,610 --> 00:01:05,930
강의는 개별 과제를 위한 개별 모델을 만드는 데

27
00:01:05,930 --> 00:01:07,410
집중해 왔습니다.

28
00:01:07,410 --> 00:01:09,968
이 모델들은 보통 강의에서 여러 번

29
00:01:09,968 --> 00:01:12,010
보셨던 몇 가지 단계를 따릅니다.

30
00:01:12,010 --> 00:01:13,930
데이터 세트를 수집합니다. 보통은 훈련

31
00:01:13,930 --> 00:01:16,650
세트와 테스트 세트를 모두 수집한 다음, 그 목적에 맞게

32
00:01:16,650 --> 00:01:18,118
매우 특화된 모델을 훈련시킵니다.

33
00:01:18,118 --> 00:01:20,410
예를 들어 이미지 분류 모델이나 이미지 캡셔닝

34
00:01:20,410 --> 00:01:22,810
모델이 될 수 있는데, 여러분이 과제에서 본 것들과

35
00:01:22,810 --> 00:01:23,510
비슷합니다.

36
00:01:23,510 --> 00:01:27,490
그리고 나서 최종적으로 그 모델들을 테스트 세트에서 평가합니다.

37
00:01:27,490 --> 00:01:29,850
최근 몇 년간 이 분야에서 달라진

38
00:01:29,850 --> 00:01:32,170
점은 개별 모델에서 벗어나 더

39
00:01:32,170 --> 00:01:34,770
큰 기반 모델(foundation

40
00:01:34,770 --> 00:01:38,370
models)을 구축하는 방향으로 전환된 것입니다.

41
00:01:38,370 --> 00:01:40,210
기반 모델을 생각하는

42
00:01:40,210 --> 00:01:43,290
방법은 다양한 기술과 여러 가지 과제에

43
00:01:43,290 --> 00:01:46,790
대해 사전 훈련(pre-train)을 시키고,

44
00:01:46,790 --> 00:01:50,090
나중에 필요에 따라 개별 과제에 맞게

45
00:01:50,090 --> 00:01:51,570
적응시키는 것입니다.

46
00:01:51,570 --> 00:01:54,690
예를 들어, 여러분 모두가 어떤 형태로든 사용해봤을

47
00:01:54,690 --> 00:01:58,580
GPT가 매우 흔한 기반 모델 중 하나입니다. GPT는

48
00:01:58,580 --> 00:02:01,660
인터넷의 Common Crawl 데이터로 많이

49
00:02:01,660 --> 00:02:02,600
훈련되었습니다.

50
00:02:02,600 --> 00:02:04,438
그런 다음 그 모델을 가져와서 다양한 목적에

51
00:02:04,438 --> 00:02:06,480
맞게 미세 조정(fine tune)합니다.

52
00:02:06,480 --> 00:02:09,020
수학 문제, 기호 추론, 퀴즈 질문 등

53
00:02:09,020 --> 00:02:11,520
다양한 과제에 맞게 미세 조정하는 거죠.

54
00:02:11,520 --> 00:02:13,140
이 모든 것은 모델이 빠르게

55
00:02:13,140 --> 00:02:14,845
적응할 수 있는 개별 과제들입니다.

56
00:02:14,845 --> 00:02:16,220
기반 모델의 좋은 점은

57
00:02:16,220 --> 00:02:17,980
새로운 과제에 적응하는

58
00:02:17,980 --> 00:02:20,940
업데이트 단계를 매우 적은 데이터로 할 수 있다는

59
00:02:20,940 --> 00:02:22,590
겁니다. 즉, 많은 훈련

60
00:02:22,590 --> 00:02:24,840
데이터를 수집할 필요가 없습니다.

61
00:02:24,840 --> 00:02:27,085
보통 아주 적은 데이터로도 충분합니다.

62
00:02:27,085 --> 00:02:28,460
때로는 훈련 데이터를

63
00:02:28,460 --> 00:02:31,780
전혀 수집하지 않아도 되는 경우도 있습니다.

64
00:02:31,780 --> 00:02:34,120
기반 모델을 생각할 때,

65
00:02:34,120 --> 00:02:36,580
관심 가질 만한 여러 종류의

66
00:02:36,580 --> 00:02:38,260
기반 모델이 있습니다.

67
00:02:38,260 --> 00:02:40,980
언어 분야에서는 ELMo와 BERT가

68
00:02:40,980 --> 00:02:42,820
이 혁명을 시작했습니다.

69
00:02:42,820 --> 00:02:46,833
그리고 지금은 GPT, T5, 그리고 이 모델들의 변형들이 있습니다.

70
00:02:46,833 --> 00:02:48,500
이 수업에서는 주로 멀티모달 모델에

71
00:02:48,500 --> 00:02:50,958
대해 다룰 것이기 때문에, 이 모델들에 대해서는

72
00:02:50,958 --> 00:02:52,400
자세히 다루지 않을 겁니다.

73
00:02:52,400 --> 00:02:54,140
오늘 다룰 내용은 이미지 분류를

74
00:02:54,140 --> 00:02:57,570
위한 동일한 foundation 모델을 어떻게 구축하는지입니다.

75
00:02:57,570 --> 00:03:02,110
그리고 오늘은 CLIP과 CoCa 같은 예시를 살펴보겠습니다.

76
00:03:02,110 --> 00:03:05,390
또한, 이미 수업에서 보셨을 법한 언어 모델과

77
00:03:05,390 --> 00:03:08,830
이 비전 foundation 모델을 결합해 다양한

78
00:03:08,830 --> 00:03:12,590
작업을 해결할 수 있는 새로운 멀티모달 foundation

79
00:03:12,590 --> 00:03:15,665
모델을 만드는 방법도 이야기할 겁니다.

80
00:03:15,665 --> 00:03:17,790
물론, 언어 작업을 푸는 것

81
00:03:17,790 --> 00:03:19,710
이상도 할 수 있습니다.

82
00:03:19,710 --> 00:03:21,590
텍스트뿐만 아니라

83
00:03:21,590 --> 00:03:25,270
생성하고 싶은 마스크나 이미지도 출력할 수 있는

84
00:03:25,270 --> 00:03:28,735
모델을 어떻게 구축하는지 이야기할 겁니다.

85
00:03:28,735 --> 00:03:31,110
마지막으로, 여러 foundation

86
00:03:31,110 --> 00:03:33,150
모델을 연결해 함께 새로운

87
00:03:33,150 --> 00:03:37,030
일을 할 수 있게 하는 체이닝 개념도 다루겠습니다.

88
00:03:37,030 --> 00:03:38,970
foundation 모델에 대해

89
00:03:38,970 --> 00:03:41,950
이야기할 때는 분류하는 방법이 여러 가지 있습니다.

90
00:03:41,950 --> 00:03:43,910
그리고 정의가 자주

91
00:03:43,910 --> 00:03:46,130
의견이 갈려서 어렵습니다.

92
00:03:46,130 --> 00:03:48,590
하지만 일반적으로 foundation

93
00:03:48,590 --> 00:03:52,450
모델은 여러 작업에 대해 강건하고 일반화가 잘 됩니다.

94
00:03:52,450 --> 00:03:55,220
그래서 같은 모델을 다양한 용도에 적용할 수

95
00:03:55,220 --> 00:03:57,608
있고, 오늘 많은 사례를 보여드리겠습니다.

96
00:03:57,608 --> 00:03:59,400
또한, 이런 foundation

97
00:03:59,400 --> 00:04:01,025
모델에는 공통적으로

98
00:04:01,025 --> 00:04:02,700
많은 파라미터가 있습니다.

99
00:04:02,700 --> 00:04:05,340
많은 파라미터 수, 방대한

100
00:04:05,340 --> 00:04:07,800
학습 데이터, 그리고 보통은

101
00:04:07,800 --> 00:04:10,800
자기지도 학습 목표로 훈련됩니다.

102
00:04:10,800 --> 00:04:13,500
물론 언어 관련 내용은 다루지 않지만, 오늘은

103
00:04:13,500 --> 00:04:16,120
초록색으로 표시된 부분을 다룰 겁니다.

104
00:04:16,120 --> 00:04:18,800
그럼 이미지 분류부터 시작하겠습니다.

105
00:04:18,800 --> 00:04:22,520
어떻게 하면 어떤 데이터셋이든 이미지 분류를 해결할

106
00:04:22,520 --> 00:04:25,200
수 있는 foundation 모델을

107
00:04:25,200 --> 00:04:26,960
실제로 구축할 수 있을까요?

108
00:04:26,960 --> 00:04:29,300
몇 강 전을 기억하신다면,

109
00:04:29,300 --> 00:04:31,840
자기지도 학습에 대해 이야기했었습니다.

110
00:04:31,840 --> 00:04:33,560
그리고 self-supervised

111
00:04:33,560 --> 00:04:35,400
learning에서 보셨던

112
00:04:35,400 --> 00:04:38,720
방법 중 하나가 SimCLR인데, 여기서는

113
00:04:38,720 --> 00:04:42,200
contrastive objective를 사용해서 서로 다른 이미지와는

114
00:04:42,200 --> 00:04:45,360
거리를 두고, 어떤 식으로든 변형된 같은 이미지의

115
00:04:45,360 --> 00:04:47,760
표현은 더 가깝게 당기는 방식입니다.

116
00:04:47,760 --> 00:04:51,000
이 아이디어는 비슷한 개념들을 서로 끌어당기는 것으로 생각할

117
00:04:51,000 --> 00:04:51,780
수 있습니다.

118
00:04:51,780 --> 00:04:54,170
그래서 고양이의 다양한

119
00:04:54,170 --> 00:04:57,310
augmentation은 서로 비슷한 표현을 만들어내야

120
00:04:57,310 --> 00:04:59,570
하지만, 예를 들어 개 같은

121
00:04:59,570 --> 00:05:03,090
다른 카테고리의 표현과는 거리를 두어야 합니다.

122
00:05:03,090 --> 00:05:06,010
self-supervised learning

123
00:05:06,010 --> 00:05:08,850
목표로 훈련할 때의 기대는, 이런

124
00:05:08,850 --> 00:05:11,330
표현들이 충분히 일반화되어서 새로운

125
00:05:11,330 --> 00:05:14,490
것을 봤을 때, 예를 들어 고양이나 개의

126
00:05:14,490 --> 00:05:17,090
스케치 같은 것도 그 공간에 잘 임베딩되어

127
00:05:17,090 --> 00:05:20,250
쉽게 정확히 분류할 수 있다는 겁니다.

128
00:05:20,250 --> 00:05:21,910
이제 멀티모달로 넘어가겠습니다.

129
00:05:21,910 --> 00:05:25,090
우리는 같은 아이디어와 같은 목표를 가지고,

130
00:05:25,090 --> 00:05:27,690
여기에 텍스트를 표현 공간에 추가하면 어떻게

131
00:05:27,690 --> 00:05:29,690
될지 생각할 수 있습니다.

132
00:05:29,690 --> 00:05:31,410
예를 들어, 'a

133
00:05:31,410 --> 00:05:36,090
cute fluffy cat'라는 텍스트 표현도 고양이 표현과

134
00:05:36,090 --> 00:05:38,792
가깝게 임베딩할 수 있다면 좋겠죠,

135
00:05:38,792 --> 00:05:40,250
왜냐하면 이제

136
00:05:40,250 --> 00:05:44,370
이미지와 텍스트 둘 다로 쿼리할 수 있기 때문입니다.

137
00:05:44,370 --> 00:05:47,730
마찬가지로 'my favorite dog is a golden

138
00:05:47,730 --> 00:05:49,710
retriever'라는 문장도

139
00:05:49,710 --> 00:05:51,380
임베딩해서, 이상적으로는 그

140
00:05:51,380 --> 00:05:55,700
표현이 다른 개 종류보다 골든 리트리버에 더 가깝게 위치하는 것이 좋습니다.

141
00:05:55,700 --> 00:05:58,185
이것이 지금까지 수업에서 다룬

142
00:05:58,185 --> 00:06:00,060
self-supervised

143
00:06:00,060 --> 00:06:03,340
learning 목표를 텍스트와 다른 멀티모달

144
00:06:03,340 --> 00:06:06,140
입력에 적용하는 일반적인 아이디어입니다.

145
00:06:06,140 --> 00:06:08,700
SimCLR에서 기억하시면,

146
00:06:08,700 --> 00:06:13,180
주요 목표는 같은 이미지의 변형들을 서로 끌어당기는

147
00:06:13,180 --> 00:06:14,240
것이었습니다.

148
00:06:14,240 --> 00:06:18,060
그래서 고양이는 다른 고양이 augmentation과 가장 가까워야 합니다.

149
00:06:18,060 --> 00:06:20,500
저 초록색 화살표는 서로 끌어당겨야 하는

150
00:06:20,500 --> 00:06:21,792
두 가지를 나타냅니다.

151
00:06:21,792 --> 00:06:23,375
그리고 다른 모든 augmentation들과는

152
00:06:23,375 --> 00:06:24,440
더 멀리 떨어져 있어야 합니다.

153
00:06:24,440 --> 00:06:27,440
그래서 개나 원숭이 같은 다른 이미지들은

154
00:06:27,440 --> 00:06:29,980
표현이 멀리 떨어져 있어야 합니다.

155
00:06:29,980 --> 00:06:32,500
이제 같은 아이디어를 사용해서 CLIP

156
00:06:32,500 --> 00:06:34,820
모델 훈련을 생각해볼 수 있습니다.

157
00:06:34,820 --> 00:06:37,700
CLIP에서는 왼쪽에 이미지 인코더가

158
00:06:37,700 --> 00:06:40,080
그대로 있고, 오른쪽에는

159
00:06:40,080 --> 00:06:43,100
이제 텍스트 인코더가 추가되어 있습니다.

160
00:06:43,100 --> 00:06:46,020
이 텍스트 인코더는 개별 이미지들의

161
00:06:46,020 --> 00:06:47,980
설명을 임베딩하고 있습니다.

162
00:06:47,980 --> 00:06:51,310
그래서 여러분의 강아지 이미지는 이제 '내가 가장

163
00:06:51,310 --> 00:06:54,270
좋아하는 강아지는 골든 리트리버다'라는 텍스트

164
00:06:54,270 --> 00:06:56,270
표현과 더 가까워지고, 다른

165
00:06:56,270 --> 00:06:59,310
모든 표현과는 멀어지도록 학습할 수 있습니다.

166
00:06:59,310 --> 00:07:01,470
그리고 이것은 SimCLR에서 보았던

167
00:07:01,470 --> 00:07:03,390
것과 같은 공식이기 때문에,

168
00:07:03,390 --> 00:07:05,470
이런 모델을 학습시키는 목적

169
00:07:05,470 --> 00:07:08,550
함수는 많은 이미지-텍스트 쌍을 모으는 것입니다.

170
00:07:08,550 --> 00:07:10,090
그런 다음 그 쌍들을

171
00:07:10,090 --> 00:07:12,070
미니배치로 모델에 넣고,

172
00:07:12,070 --> 00:07:15,390
SimCLR에서 사용하는 대조적 목적 함수를

173
00:07:15,390 --> 00:07:16,810
적용하는 겁니다.

174
00:07:16,810 --> 00:07:19,970
하지만 이제는 이미지와 텍스트 간에 적용하는 거죠.

175
00:07:19,970 --> 00:07:22,270
분자에서는 비슷한 것들의

176
00:07:22,270 --> 00:07:24,790
표현을 끌어당기고, 분모에서는

177
00:07:24,790 --> 00:07:26,710
나머지 모든 것들의

178
00:07:26,710 --> 00:07:29,110
표현을 멀리하는 겁니다.

179
00:07:29,110 --> 00:07:30,550
물론, 그 이미지는

180
00:07:30,550 --> 00:07:33,030
해당 텍스트와 가장 가까워야 하고,

181
00:07:33,030 --> 00:07:35,370
다른 모든 텍스트와는 멀어야 합니다.

182
00:07:35,370 --> 00:07:37,730
하지만 반대도 성립해야 합니다.

183
00:07:37,730 --> 00:07:39,350
그래서 두 번째 목적 함수도

184
00:07:39,350 --> 00:07:42,710
있는데, 모든 텍스트는 자기 이미지와 가장 가까워야

185
00:07:42,710 --> 00:07:45,970
하고, 다른 모든 이미지 설명과는 멀어야 한다는 겁니다.

186
00:07:45,970 --> 00:07:48,910
즉, 서로 다른 두 종류의

187
00:07:48,910 --> 00:07:51,550
모달리티 간에 상호 보완적이고

188
00:07:51,550 --> 00:07:55,470
대칭적인 손실이 존재하는 거죠.

189
00:07:55,470 --> 00:07:58,470
CLIP 같은 모델의 좋은 점은

190
00:07:58,470 --> 00:08:02,070
이미지와 텍스트의 연관성만으로 학습할 수

191
00:08:02,070 --> 00:08:03,650
있다는 것입니다.

192
00:08:03,650 --> 00:08:06,090
그리고 인터넷에는 이런 데이터가 엄청나게 많습니다.

193
00:08:06,090 --> 00:08:08,950
인터넷에서 대응하는 이미지와 텍스트 데이터를

194
00:08:08,950 --> 00:08:10,650
많이 구할 수 있습니다.

195
00:08:10,650 --> 00:08:13,030
다운로드해서 아주 대규모로 이

196
00:08:13,030 --> 00:08:14,850
모델을 학습시킬 수 있죠.

197
00:08:14,850 --> 00:08:16,590
이것이 바로 OpenAI가

198
00:08:16,590 --> 00:08:18,550
2021년에 CLIP

199
00:08:18,550 --> 00:08:20,910
모델을 발표할 때 한 일입니다.

200
00:08:20,910 --> 00:08:22,650
그들은 많은 데이터를 수집하고,

201
00:08:22,650 --> 00:08:25,530
인터넷에서 찾은 모든 이미지-텍스트

202
00:08:25,530 --> 00:08:27,230
쌍을 이용해 이 대조적 목적

203
00:08:27,230 --> 00:08:28,650
함수로 학습시켰습니다.

204
00:08:28,650 --> 00:08:30,590
그리고 그 훈련이 끝나면,

205
00:08:30,590 --> 00:08:33,309
self-supervised learning

206
00:08:33,309 --> 00:08:35,630
수업에서 봤던 것과 같은 2단계 파이프라인을

207
00:08:35,630 --> 00:08:38,169
따릅니다. 1단계에서는 사전 훈련을

208
00:08:38,169 --> 00:08:40,890
하고, 2단계에서는 그 이미지 인코더를

209
00:08:40,890 --> 00:08:43,470
가져와서 새로운 작업에 적응시킬 수 있습니다.

210
00:08:43,470 --> 00:08:46,140
사전 훈련된 이미지 인코더를 얻으면,

211
00:08:46,140 --> 00:08:47,620
그 가중치를

212
00:08:47,620 --> 00:08:50,760
가져와서 이미지 분류 작업이나 객체

213
00:08:50,760 --> 00:08:52,600
검출 작업에 맞게

214
00:08:52,600 --> 00:08:54,680
추가적인 선형 레이어를 덧붙이거나,

215
00:08:54,680 --> 00:08:58,000
디코더를 넣어 의미론적 분할 맵을

216
00:08:58,000 --> 00:09:00,120
디코딩할 수도 있습니다.

217
00:09:00,120 --> 00:09:02,520
이렇게 사전 훈련된 목표에서

218
00:09:02,520 --> 00:09:07,120
모델을 초기화하는 것만으로도 다양한 작업이 가능해집니다.

219
00:09:07,120 --> 00:09:09,200
이 논문이 발표되었을 때 정말

220
00:09:09,200 --> 00:09:13,400
흥미로웠던 점은, 이 CLIP 인코더 위에 단 하나의

221
00:09:13,400 --> 00:09:17,160
선형 분류기를 추가하는 것만으로도 성능이 크게

222
00:09:17,160 --> 00:09:18,160
향상되었다는 겁니다.

223
00:09:18,160 --> 00:09:20,760
이 그래프에서는 여러 이미지 분류

224
00:09:20,760 --> 00:09:23,400
데이터셋에서의 평균 성능을 보여드리고

225
00:09:23,400 --> 00:09:24,360
있습니다.

226
00:09:24,360 --> 00:09:26,780
빨간색으로 표시된 CLIP 모델들이

227
00:09:26,780 --> 00:09:28,280
가장 상위에 위치해 있죠.

228
00:09:28,280 --> 00:09:32,200
더 많은 이미지를 학습할수록 점점 더 좋은

229
00:09:32,200 --> 00:09:35,000
성능을 얻는 것을 볼 수 있습니다.

230
00:09:35,000 --> 00:09:36,560
이것이 매우 흥미로웠던

231
00:09:36,560 --> 00:09:38,440
이유는, 우리가 잠금 해제할

232
00:09:38,440 --> 00:09:41,420
수 있었던 정말 좋은 사전 훈련 목표가 있다는

233
00:09:41,420 --> 00:09:43,650
것을 시사했고, 인터넷에는 풍부한

234
00:09:43,650 --> 00:09:46,330
이미지-텍스트 데이터가 있어서 이 모델들을

235
00:09:46,330 --> 00:09:50,410
매우 크고 성능 좋게 훈련할 수 있다는 뜻이기 때문입니다.

236
00:09:50,410 --> 00:09:52,310
물론, 이것이 이야기의 끝은 아닙니다.

237
00:09:52,310 --> 00:09:54,450
이상적으로는, 새로운 작업에 맞게 이

238
00:09:54,450 --> 00:09:56,837
특징들을 적응시킬 필요가 없었으면 좋겠습니다.

239
00:09:56,837 --> 00:09:59,170
이상적으로는 CLIP 모델을 바로

240
00:09:59,170 --> 00:10:00,730
사용할 수 있으면 좋겠죠.

241
00:10:00,730 --> 00:10:03,610
예를 들어 언어 모델에서는 보통 자동완성 작업을

242
00:10:03,610 --> 00:10:05,310
위해 모델을 훈련합니다.

243
00:10:05,310 --> 00:10:07,430
자동완성은 이렇게 작동합니다.

244
00:10:07,430 --> 00:10:09,410
예를 들어 'I love'라는 구절이

245
00:10:09,410 --> 00:10:13,250
있으면, 모델이 다음 단어인 'cake'를 채워 넣는 식이죠.

246
00:10:13,250 --> 00:10:15,810
그리고 이런 사전 훈련 목표로 훈련합니다.

247
00:10:15,810 --> 00:10:18,170
두 번째 단계에서는 기본적으로

248
00:10:18,170 --> 00:10:20,670
같은 모델을 가져와서 새로운 작업에

249
00:10:20,670 --> 00:10:22,490
적응시키는 것이 목표입니다.

250
00:10:22,490 --> 00:10:25,610
언어 모델의 경우, 그 모델을 다시 훈련할 필요가 전혀 없습니다.

251
00:10:25,610 --> 00:10:28,170
새로운 다운스트림 작업에 대해 다시 훈련할 필요가 없습니다.

252
00:10:28,170 --> 00:10:30,810
모든 작업이 언어 작업이기 때문에,

253
00:10:30,810 --> 00:10:34,970
모든 작업을 이 자동완성 과정으로 처리할 수 있습니다.

254
00:10:34,970 --> 00:10:37,290
하지만 CLIP의 문제는 자동완성

255
00:10:37,290 --> 00:10:39,410
과정이 없다는 점입니다.

256
00:10:39,410 --> 00:10:42,240
그래서 우리는 이 모델을 대조적 학습 목표로

257
00:10:42,240 --> 00:10:45,760
훈련했지만, 새로운 작업에 적응시키려면 여전히 훈련

258
00:10:45,760 --> 00:10:48,900
데이터가 필요하고, 새로운 작업에 적응시키기 위해

259
00:10:48,900 --> 00:10:51,442
훈련해야 하는 선형 계층이 필요합니다.

260
00:10:51,442 --> 00:10:52,900
그래서 많은

261
00:10:52,900 --> 00:10:59,060
사람들이 이 모델을 바로 사용할 수 있도록 적응시키는 방법에 대해 고민하기

262
00:10:59,060 --> 00:11:00,100
시작했습니다.

263
00:11:00,100 --> 00:11:02,800
그리고 사람들이 생각해낸 영리한 트릭이 있습니다.

264
00:11:02,800 --> 00:11:06,900
이 영리한 트릭은 텍스트 인코더를 사용해 모델이

265
00:11:06,900 --> 00:11:11,020
어떤 다운스트림 분류 작업에도 일반화할 수

266
00:11:11,020 --> 00:11:13,480
있도록 안내하는 방법입니다.

267
00:11:13,480 --> 00:11:14,613
작동 방식은 이렇습니다.

268
00:11:14,613 --> 00:11:16,780
예를 들어, CLIP 모델을 사용해 이

269
00:11:16,780 --> 00:11:20,260
이미지가 무엇인지 분류하고 싶은데, 모델을 다시 훈련하거나 다운스트림

270
00:11:20,260 --> 00:11:22,600
작업에 맞게 적응시키고 싶지 않다고 합시다.

271
00:11:22,600 --> 00:11:26,660
그럴 때 텍스트 인코더에 단어를 넣어

272
00:11:26,660 --> 00:11:29,380
텍스트 벡터를 만들고,

273
00:11:29,380 --> 00:11:33,740
최근접 이웃을 사용해 올바른 분류를

274
00:11:33,740 --> 00:11:35,460
찾을 수 있습니다.

275
00:11:35,460 --> 00:11:38,300
이 방법은 새 데이터셋의 모든 카테고리를 텍스트 공간에

276
00:11:38,300 --> 00:11:39,405
임베딩하는 것입니다.

277
00:11:39,405 --> 00:11:41,030
예를 들어, 새 데이터셋에

278
00:11:41,030 --> 00:11:44,430
평면(plane), 개(dog), 새(bird)라는 카테고리가 있다고
합시다.

279
00:11:44,430 --> 00:11:47,230
이 카테고리들을 텍스트 공간에

280
00:11:47,230 --> 00:11:50,350
임베딩해서 평면 벡터, 개 벡터, 새

281
00:11:50,350 --> 00:11:52,030
벡터를 얻습니다.

282
00:11:52,030 --> 00:11:54,870
그리고 새로운 이미지가 들어오면, 이미지

283
00:11:54,870 --> 00:11:57,870
인코더로 그 이미지를 임베딩한 뒤 가장

284
00:11:57,870 --> 00:11:59,990
가까운 이웃을 찾으면 됩니다.

285
00:11:59,990 --> 00:12:02,630
이 경우, 이 이미지가 가장

286
00:12:02,630 --> 00:12:06,870
높은 유사도를 가진 클래스는 개 벡터일 것입니다.

287
00:12:06,870 --> 00:12:09,325
실제로 개 벡터가 가장 높은 유사도 점수를 가지고 있음을 알 수 있습니다.

288
00:12:09,325 --> 00:12:11,950
그래서 이 이미지를 개로 분류할 수

289
00:12:11,950 --> 00:12:12,410
있습니다.

290
00:12:12,410 --> 00:12:13,868
이 전체 과정을 본질적으로

291
00:12:13,868 --> 00:12:17,780
1-최근접 이웃 알고리즘을 구축하는 것으로 생각할 수 있습니다.

292
00:12:17,780 --> 00:12:19,870
텍스트 공간에 생성한 여러 중심점 또는 임베딩을 클래스

293
00:12:19,870 --> 00:12:23,930
레이블로 사용하고, 새 이미지가 들어올 때 최적의 분류를 찾기 위해
1-최근접 이웃을 수행하는 겁니다.

294
00:12:23,930 --> 00:12:27,510
물론, 단일

295
00:12:27,510 --> 00:12:30,090
단어만으로는 좋은

296
00:12:30,090 --> 00:12:33,288
단어 벡터를

297
00:12:33,288 --> 00:12:34,830
얻기에

298
00:12:34,830 --> 00:12:38,680
충분하지 않을 수

299
00:12:38,680 --> 00:12:40,640
있습니다.

300
00:12:40,640 --> 00:12:43,960
대신 구문(phrase)을

301
00:12:43,960 --> 00:12:47,040
사용하는 것이 좋습니다.

302
00:12:47,040 --> 00:12:49,693
그 이유는 인터넷 데이터 대부분이 단어가 단독으로 등장하지 않기 때문입니다.

303
00:12:49,693 --> 00:12:51,360
CLIP은

304
00:12:51,360 --> 00:12:54,040
인터넷에서 다운로드한

305
00:12:54,040 --> 00:12:57,100
구문들로만 훈련되었습니다.

306
00:12:57,100 --> 00:13:00,200
그래서 이상적으로는 가장 좋은 표현을 주는 적절한 구문을

307
00:13:00,200 --> 00:13:01,573
선택하는 것이 중요합니다.

308
00:13:01,573 --> 00:13:04,240
그래서 평면, 개, 새라는 카테고리 대신에 'a photo of a
plane',

309
00:13:04,240 --> 00:13:06,680
'a photo of a dog' 같은 구문을 임베딩하는 것이 좋습니다.

310
00:13:06,680 --> 00:13:10,180
이 작은 변화를 하면 ImageNet에서

311
00:13:10,180 --> 00:13:12,320
약 1.3% 향상되는

312
00:13:12,320 --> 00:13:15,720
큰 성능 향상을 얻을 수 있습니다.

313
00:13:15,720 --> 00:13:19,040
물론, 적절한 구문을

314
00:13:19,040 --> 00:13:21,640
고르는 것도

315
00:13:21,640 --> 00:13:24,800
매우 어렵습니다.

316
00:13:24,800 --> 00:13:26,640
그래서 보통 사람들은 단일 구문만

317
00:13:26,640 --> 00:13:28,820
고르지 않고 여러 구문을 선택합니다.

318
00:13:28,820 --> 00:13:30,903
'a photo of a dog', 'a drawing

319
00:13:30,903 --> 00:13:33,400
of a dog' 같은 다양한 구문을 여러 개 선택하는 것이죠.

320
00:13:33,400 --> 00:13:35,970
그리고 각

321
00:13:35,970 --> 00:13:39,290
구문에 대해

322
00:13:39,290 --> 00:13:41,610
여러 벡터를

323
00:13:41,610 --> 00:13:44,730
생성합니다.

324
00:13:44,730 --> 00:13:48,130
마지막으로 각 카테고리에 대해 모든

325
00:13:48,130 --> 00:13:52,330
구문의 벡터 표현의 평균 벡터를 구해서, 이를

326
00:13:52,330 --> 00:13:55,970
평균 개 벡터, 평균 평면 벡터, 평균

327
00:13:55,970 --> 00:13:58,113
새 벡터로 사용합니다.

328
00:13:58,113 --> 00:14:00,030
그리고 이제 다시 시작했던

329
00:14:00,030 --> 00:14:03,930
곳으로 돌아왔고, 여기서 같은 1-최근접 이웃 알고리즘을

330
00:14:03,930 --> 00:14:05,090
적용할 수 있습니다.

331
00:14:05,090 --> 00:14:08,370
아마도 ImageNet으로 학습되었을 겁니다.

332
00:14:08,370 --> 00:14:10,690
이것은 새로운 작업에 적응할 수 있다는

333
00:14:10,690 --> 00:14:12,630
점을 보여주기 위한 것 같습니다.

334
00:14:12,630 --> 00:14:14,680
하지만 확실히 학습되지 않은

335
00:14:14,680 --> 00:14:16,430
다른 데이터셋 예시도

336
00:14:16,430 --> 00:14:19,450
보여드릴 텐데, 그 경우에도 잘 적응합니다.

337
00:14:19,450 --> 00:14:21,442
그래서 하나의 벡터가 나오고,

338
00:14:21,442 --> 00:14:23,150
사용하는 아키텍처에 따라 다릅니다.

339
00:14:23,150 --> 00:14:26,530
ResNet을 사용한다면, 최종 벡터 표현을 취합니다.

340
00:14:26,530 --> 00:14:29,950
텍스트 인코더가 예를 들어 VIT나

341
00:14:29,950 --> 00:14:35,980
트랜스포머라면, 보통 트랜스포머의 CLS 토큰을 취합니다.

342
00:14:35,980 --> 00:14:38,160
이것이 CLIP에 대한 설명의 전부입니다.

343
00:14:38,160 --> 00:14:41,540
기본적으로 다양한 새로운 이미지 분류 작업에 이걸

344
00:14:41,540 --> 00:14:42,880
적응시킬 수 있습니다.

345
00:14:42,880 --> 00:14:45,480
지금 질문하신 것처럼, ImageNet에서

346
00:14:45,480 --> 00:14:48,180
성능이 비슷하다고 해서 큰 문제는

347
00:14:48,180 --> 00:14:51,340
아니지만, 그래도 ImageNet에서 잘

348
00:14:51,340 --> 00:14:53,540
작동한다는 점은 흥미롭습니다.

349
00:14:53,540 --> 00:14:57,420
더 흥미로운 점은 CLIP이 나온 이후에 수집된

350
00:14:57,420 --> 00:15:00,520
다른 데이터셋들을 보면 알 수 있습니다.

351
00:15:00,520 --> 00:15:02,900
예를 들어 ObjectNet 같은 데이터셋인데,

352
00:15:02,900 --> 00:15:06,400
사람들이 아주 이상한 장소에서 찍은 물체 사진들을 포함합니다.

353
00:15:06,400 --> 00:15:08,960
예를 들어 바나나를 땅에 놓고 사진을

354
00:15:08,960 --> 00:15:10,940
찍거나, 아주 썩은 바나나를

355
00:15:10,940 --> 00:15:12,340
찍은 경우입니다.

356
00:15:12,340 --> 00:15:14,660
즉, 흔하지 않은 것들이죠.

357
00:15:14,660 --> 00:15:18,440
이 데이터셋에서는 ImageNet으로 학습하면 성능이

358
00:15:18,440 --> 00:15:21,120
좋지 않은데, ImageNet은 대부분의

359
00:15:21,120 --> 00:15:22,700
범주를 가장 전형적인

360
00:15:22,700 --> 00:15:24,780
형태로 포함하고 있기 때문입니다.

361
00:15:24,780 --> 00:15:28,740
하지만 CLIP 모델을 사용하면 똑같이 좋은 성능을 냅니다.

362
00:15:28,740 --> 00:15:31,420
이것은 정말 많은 사람들에게 매우 흥미로웠습니다.

363
00:15:31,420 --> 00:15:33,030
왜냐하면 이전에 본 적 없는

364
00:15:33,030 --> 00:15:35,030
완전히 새로운 데이터 세트, 어느 정도

365
00:15:35,030 --> 00:15:38,150
도메인 밖인 데이터에도 일반화할 수 있는 능력이 정말 대단했기

366
00:15:38,150 --> 00:15:38,858
때문입니다.

367
00:15:38,858 --> 00:15:40,025
그렇다면 왜 그렇다고 생각하십니까?

368
00:15:40,025 --> 00:15:42,110
왜 CLIP이 ImageNet에서 훈련한 것보다

369
00:15:42,110 --> 00:15:43,870
훨씬 더 잘 일반화한다고 생각하십니까?

370
00:15:43,870 --> 00:15:45,670
당신의 답변을 바꾸어 말하자면,

371
00:15:45,670 --> 00:15:49,510
제가 생각하기에 맞는 답변인데, 인터넷에서 다운로드한 텍스트는 단순한

372
00:15:49,510 --> 00:15:51,090
카테고리 라벨보다 훨씬 더

373
00:15:51,090 --> 00:15:53,090
많은 정보를 담고 있기 때문입니다.

374
00:15:53,090 --> 00:15:55,050
텍스트는 훨씬 더 많은

375
00:15:55,050 --> 00:15:56,670
구조적 정보를 포함하고

376
00:15:56,670 --> 00:15:59,790
있고, 형태나 색상에 대한 정보도 포함되어

377
00:15:59,790 --> 00:16:02,810
있어 이 모든 것이 표현에 더해집니다.

378
00:16:02,810 --> 00:16:05,790
그래서 이 모델들은 약간 분포에서 벗어난 데이터나

379
00:16:05,790 --> 00:16:08,790
조금 다르게 보이는 객체에도 훨씬 더 잘 적응할

380
00:16:08,790 --> 00:16:10,430
수 있습니다. 왜냐하면 이들이

381
00:16:10,430 --> 00:16:13,470
찾는 다른 여러 정보도 가지고 있기 때문입니다.

382
00:16:13,470 --> 00:16:15,670
그래서 그 추가적인 감독 신호가

383
00:16:15,670 --> 00:16:17,150
정말 큰 도움이 됩니다.

384
00:16:17,150 --> 00:16:20,290
또 다른 큰 도움이 되는 이유는 데이터의 규모입니다.

385
00:16:20,290 --> 00:16:23,610
ImageNet은 약 130만 장의 이미지에 불과하지만,

386
00:16:23,610 --> 00:16:26,288
인터넷에는 수백만, 수십억 장의 이미지가 있습니다.

387
00:16:26,288 --> 00:16:27,830
현재는 수십억 개의 이미지-텍스트

388
00:16:27,830 --> 00:16:30,250
쌍을 아주 쉽게 다운로드할 수 있습니다.

389
00:16:30,250 --> 00:16:33,840
그래서 이 모델들은 훨씬 더 많은 데이터를

390
00:16:33,840 --> 00:16:36,605
봤기 때문에 적응이 훨씬 쉬워집니다.

391
00:16:36,605 --> 00:16:38,480
그래서 사람들은 다양한 일반화

392
00:16:38,480 --> 00:16:41,115
작업에 대해 이런 실험들을 시작했습니다.

393
00:16:41,115 --> 00:16:43,240
자연 이미지뿐만 아니라 스케치,

394
00:16:43,240 --> 00:16:46,800
그리고 적대적 데이터 세트에서도 이

395
00:16:46,800 --> 00:16:49,860
모델들이 일반화할 수 있음을 보여주었습니다.

396
00:16:49,860 --> 00:16:51,760
그리고 전반적인 성능은

397
00:16:51,760 --> 00:16:53,860
이 모델들이 정말 매우

398
00:16:53,860 --> 00:16:57,777
뛰어나고 다양한 응용에 강인하다는 것을 나타냅니다.

399
00:16:57,777 --> 00:17:00,360
여기서는 zero shot과 linear probe의 차이를

400
00:17:00,360 --> 00:17:01,275
보여드리고 있습니다.

401
00:17:01,275 --> 00:17:03,400
보시다시피, 물론 linear

402
00:17:03,400 --> 00:17:05,800
probe는 추가적인 선형 분류기를

403
00:17:05,800 --> 00:17:07,760
더해 훈련하고 약간 적응시키면

404
00:17:07,760 --> 00:17:09,960
대부분의 데이터 세트,

405
00:17:09,960 --> 00:17:13,079
초록색으로 표시된 곳에서 성능이 향상되지만 항상

406
00:17:13,079 --> 00:17:14,460
그런 것은 아닙니다.

407
00:17:14,460 --> 00:17:16,880
어떤 경우에는 CLIP의 제로샷이

408
00:17:16,880 --> 00:17:19,720
바로 매우 잘 작동하기도 합니다.

409
00:17:19,720 --> 00:17:21,920
그래서 이것은 우리가 마침내

410
00:17:21,920 --> 00:17:23,880
이미지 인코더를 다양한 다운스트림

411
00:17:23,880 --> 00:17:27,599
작업에 적응시킬 수 있는 능력을 열었다는 것을

412
00:17:27,599 --> 00:17:29,025
보여주는 것 같았습니다.

413
00:17:29,025 --> 00:17:30,650
이것이 많은 사람들이

414
00:17:30,650 --> 00:17:35,050
CLIP을 이미지용 최초의 파운데이션 모델이라고 부르는 이유입니다.

415
00:17:35,050 --> 00:17:37,610
그럼 CLIP이 왜 이렇게 잘 작동하는지 이야기해 보겠습니다.

416
00:17:37,610 --> 00:17:40,370
물론 CLIP에는 진짜 레이블이 따로 있는 것은 아닙니다.

417
00:17:40,370 --> 00:17:43,430
우리는 단지 이미지와 관련된 텍스트를 다운로드하는 것뿐입니다.

418
00:17:43,430 --> 00:17:45,583
CLIP이 잘 작동하는

419
00:17:45,583 --> 00:17:47,750
이유는 처음 훈련될 때

420
00:17:47,750 --> 00:17:51,010
포함된 내용 때문인데, 훈련에 사용된

421
00:17:51,010 --> 00:17:53,270
파라미터 수가 엄청났습니다.

422
00:17:53,270 --> 00:17:55,090
모델을 확장하고 아키텍처를

423
00:17:55,090 --> 00:17:58,290
ResNet에서 VIT로 변경했습니다.

424
00:17:58,290 --> 00:18:00,370
그래서 3억 700만 개의

425
00:18:00,370 --> 00:18:05,510
파라미터를 가진 트랜스포머 아키텍처가 이 모델 훈련에 사용되었습니다.

426
00:18:05,510 --> 00:18:08,230
두 번째로 도움이 된 것은 데이터 양이었습니다.

427
00:18:08,230 --> 00:18:11,230
ImageNet의 120만 장 이미지

428
00:18:11,230 --> 00:18:14,090
대신 인터넷에서 다운로드한 약 4억

429
00:18:14,090 --> 00:18:15,810
개의 이미지-텍스트

430
00:18:15,810 --> 00:18:17,270
쌍을 사용했습니다.

431
00:18:17,270 --> 00:18:19,850
모델 크기와 데이터 양

432
00:18:19,850 --> 00:18:23,050
모두에서 이 규모가 성능 향상에 크게

433
00:18:23,050 --> 00:18:24,210
기여했습니다.

434
00:18:24,210 --> 00:18:26,410
CLIP이 나온 직후 사람들은 이

435
00:18:26,410 --> 00:18:28,680
목적 함수를 가지고 실험을 시작했습니다.

436
00:18:28,680 --> 00:18:31,540
그리고 수년간 다양한 CLIP

437
00:18:31,540 --> 00:18:33,040
변형들이 나왔습니다.

438
00:18:33,040 --> 00:18:35,460
그중 특히 눈에 띄는 것이

439
00:18:35,460 --> 00:18:38,220
2022년에 나온 CoCa라는 모델입니다.

440
00:18:38,220 --> 00:18:40,140
CoCa는 CLIP 모델을 가져갔습니다.

441
00:18:40,140 --> 00:18:42,322
여기서 보시면 같은 목적 함수라는 것을 알 수 있습니다.

442
00:18:42,322 --> 00:18:44,280
한쪽에서는 이미지가 인코딩되고 있습니다.

443
00:18:44,280 --> 00:18:46,240
다른 한쪽에서는 텍스트가 인코딩되고 있습니다.

444
00:18:46,240 --> 00:18:47,865
그리고 두 가지 사이에 대조 손실(contrastive

445
00:18:47,865 --> 00:18:48,820
loss)이 있습니다.

446
00:18:48,820 --> 00:18:50,820
하지만 그들은 한 가지를 더 추가했습니다.

447
00:18:50,820 --> 00:18:53,660
이미지 인코더에서 이미지 특징을

448
00:18:53,660 --> 00:18:56,540
가져가는 디코더도 추가했습니다.

449
00:18:56,540 --> 00:18:59,420
그 후 크로스 어텐션을 통해 그것을

450
00:18:59,420 --> 00:19:01,620
입력받아 이미지를 캡션하는 거죠.

451
00:19:01,620 --> 00:19:04,180
이 캡션 생성 과정이 모델이 풍부한 정보를

452
00:19:04,180 --> 00:19:07,560
학습하는 데도 크게 도움이 된다는 것이 밝혀졌습니다.

453
00:19:07,560 --> 00:19:09,020
일반적인 동기는 단순히

454
00:19:09,020 --> 00:19:11,500
이 이미지가 고양이인지 개인지

455
00:19:11,500 --> 00:19:14,360
구분하는 것만으로는 충분하지 않고,

456
00:19:14,360 --> 00:19:16,460
텍스트로 이미지를 설명하려면 훨씬

457
00:19:16,460 --> 00:19:19,760
더 많은 정보를 모델이 배워야 한다는 겁니다.

458
00:19:19,760 --> 00:19:22,660
그래서 이것이 더 강력한 학습 목표라는

459
00:19:22,660 --> 00:19:23,580
가설입니다.

460
00:19:23,580 --> 00:19:26,830
그 결과 더 좋은 특징을 학습하게 됩니다.

461
00:19:26,830 --> 00:19:29,350
전체적으로 이것이 사실임을 확인했습니다.

462
00:19:29,350 --> 00:19:32,310
CoCa와 CLIP을 비교하면, 모든

463
00:19:32,310 --> 00:19:36,230
ImageNet 변형 데이터셋에서 성능이 크게 향상되었습니다.

464
00:19:36,230 --> 00:19:38,790
전체적으로 모든 데이터셋에서 약

465
00:19:38,790 --> 00:19:41,350
10% 성능 향상이 있었습니다.

466
00:19:41,350 --> 00:19:44,950
이것이 아마도 기초 모델들이 감독 학습으로

467
00:19:44,950 --> 00:19:48,590
훈련한 모든 모델을 처음으로 능가한

468
00:19:48,590 --> 00:19:50,030
사례라고 생각합니다.

469
00:19:50,030 --> 00:19:52,070
이 시점에서 사람들은 온라인

470
00:19:52,070 --> 00:19:55,190
리더보드에 다양한 모델들을 많이 올리고 있었습니다.

471
00:19:55,190 --> 00:19:58,550
그리고 수년간의 리더보드에서 모델

472
00:19:58,550 --> 00:20:00,910
성능이 점점 좋아지는 추세를

473
00:20:00,910 --> 00:20:02,770
볼 수 있습니다.

474
00:20:02,770 --> 00:20:04,395
이것이 이미지 인코더에

475
00:20:04,395 --> 00:20:07,270
대한 감독 학습 목표를 포기하고

476
00:20:07,270 --> 00:20:09,750
인터넷 데이터의 자기지도 학습

477
00:20:09,750 --> 00:20:11,950
방법을 이용한 사전 학습

478
00:20:11,950 --> 00:20:15,630
목표에만 집중하기 시작한 전환점이라고 생각합니다.

479
00:20:15,630 --> 00:20:17,810
그럼 CLIP의 장점 몇 가지에 대해 이야기해 보겠습니다.

480
00:20:17,810 --> 00:20:20,510
CLIP은 정말 재미있는 기능이 많습니다.

481
00:20:20,510 --> 00:20:24,000
단순한 대조 학습 목표라서 훈련이 매우

482
00:20:24,000 --> 00:20:25,080
쉽습니다.

483
00:20:25,080 --> 00:20:27,640
추론 속도도 매우 빠릅니다.

484
00:20:27,640 --> 00:20:32,300
전체 데이터셋을 어떤 표현으로 임베딩한 뒤,

485
00:20:32,300 --> 00:20:34,240
분류할 때는 그

486
00:20:34,240 --> 00:20:38,560
임베딩된 데이터셋에서 검색만 하면 됩니다.

487
00:20:38,560 --> 00:20:40,280
그래서 CLIP의

488
00:20:40,280 --> 00:20:41,960
표현을 이용하면

489
00:20:41,960 --> 00:20:44,320
분류뿐 아니라 검색과

490
00:20:44,320 --> 00:20:47,918
검색 작업에도 매우 유용합니다.

491
00:20:47,918 --> 00:20:49,960
사람들이 CLIP을 좋아하는 또 다른 이유는

492
00:20:49,960 --> 00:20:51,220
오픈 보캐뷸러리라는 점입니다.

493
00:20:51,220 --> 00:20:53,145
어떤 텍스트 설명도 입력할 수

494
00:20:53,145 --> 00:20:54,520
있고, 그에 맞는

495
00:20:54,520 --> 00:20:56,200
이미지를 찾아낼 수 있습니다.

496
00:20:56,200 --> 00:20:58,920
그래서 다양한 도메인에 적용할

497
00:20:58,920 --> 00:21:01,213
수 있다는 장점도 있습니다.

498
00:21:01,213 --> 00:21:03,380
물론, 이 부분은 나중에 더 이야기할 겁니다.

499
00:21:03,380 --> 00:21:07,380
CLIP은 다른 모델과 연결해서 사용하기에 정말 적합합니다.

500
00:21:07,380 --> 00:21:10,025
이 연결하는 아이디어가 점점 인기를 얻기 시작했습니다.

501
00:21:10,025 --> 00:21:10,900
하지만 잠시만요.

502
00:21:10,900 --> 00:21:13,982
몇 분 후에 자세히 다루겠습니다.

503
00:21:13,982 --> 00:21:15,940
물론, 좋은 점들만 말씀드리고 있습니다.

504
00:21:15,940 --> 00:21:18,520
하지만 사실 나쁜 점들도 많다는 것이 밝혀졌습니다.

505
00:21:18,520 --> 00:21:20,520
안타깝게도 CLIP은 이 두

506
00:21:20,520 --> 00:21:22,200
이미지를 구분할 수 없습니다.

507
00:21:22,200 --> 00:21:25,380
잔디 위에 머그잔이 있는 이미지와

508
00:21:25,380 --> 00:21:27,720
머그잔 안에 잔디가 있는

509
00:21:27,720 --> 00:21:30,880
이미지가 있는데, CLIP은 이 둘의

510
00:21:30,880 --> 00:21:33,070
차이를 알지 못합니다.

511
00:21:33,070 --> 00:21:37,200
이유는 CLIP의 학습 목표가 배치

512
00:21:37,200 --> 00:21:40,720
크기에 크게 의존하기 때문입니다.

513
00:21:40,720 --> 00:21:43,400
배치 크기가 충분히 크지 않으면,

514
00:21:43,400 --> 00:21:45,240
다른 배치 요소들이

515
00:21:45,240 --> 00:21:48,760
모델에 유용한 감독 신호를 제공하지 못할

516
00:21:48,760 --> 00:21:49,720
가능성이 큽니다.

517
00:21:49,720 --> 00:21:52,380
항상 고양이와 트럭을 비교한다면,

518
00:21:52,380 --> 00:21:56,160
고양이에 대한 표현을 제대로 배우지 못합니다.

519
00:21:56,160 --> 00:21:59,280
대신 어느 정도 수준에서 괜찮은 표현을 얻지만,

520
00:21:59,280 --> 00:22:02,157
배치 크기를 늘리면 고양이와 비슷한

521
00:22:02,157 --> 00:22:04,240
다른 동물들을 만날 확률이

522
00:22:04,240 --> 00:22:05,800
높아져 훨씬 더 좋은

523
00:22:05,800 --> 00:22:07,393
표현을 배우게 됩니다.

524
00:22:07,393 --> 00:22:09,560
그리고 물론 배치 크기를

525
00:22:09,560 --> 00:22:15,000
32,000 정도로 늘리고 여러 GPU에서 학습시키면, 정말

526
00:22:15,000 --> 00:22:17,860
좋은 표현을 배우기 시작합니다.

527
00:22:17,860 --> 00:22:19,770
실제로 웰시 코기와 다른 코기를

528
00:22:19,770 --> 00:22:21,530
구분할 수 있게 되는 거죠.

529
00:22:21,530 --> 00:22:24,670
이것은 거대한 배치 크기가 있어야만

530
00:22:24,670 --> 00:22:27,330
가능한데, 그 이유는 배치 내에 충분히

531
00:22:27,330 --> 00:22:29,490
가까운, 어려운 부정

532
00:22:29,490 --> 00:22:33,350
예제가 있어야 모델이 학습할 수 있기 때문입니다.

533
00:22:33,350 --> 00:22:34,850
그래서 이런 모델을 잘

534
00:22:34,850 --> 00:22:37,250
작동시키려면 이 점이 매우 중요합니다.

535
00:22:37,250 --> 00:22:39,450
하지만 불행히도, 사람들이 아무리

536
00:22:39,450 --> 00:22:41,690
노력해도 배치 크기를

537
00:22:41,690 --> 00:22:45,090
늘린다고 해서 모델이 좋은 표현을 반드시 배우는

538
00:22:45,090 --> 00:22:46,030
것은 아닙니다.

539
00:22:46,030 --> 00:22:49,730
그래서 학습 데이터의 무작위성에 의존할

540
00:22:49,730 --> 00:22:51,090
수밖에 없습니다.

541
00:22:51,090 --> 00:22:52,650
배치 크기를 늘리는 것이 어느

542
00:22:52,650 --> 00:22:54,790
정도 미세한 개념 학습에 도움이 되긴 하지만,

543
00:22:54,790 --> 00:22:56,570
물론 여전히 한계가 있습니다.

544
00:22:56,570 --> 00:23:01,930
그리고 배치 사이즈 32,000으로 훈련하는 것은 대부분의

545
00:23:01,930 --> 00:23:06,330
연구실에서 감히 시도하기에도 너무 큽니다.

546
00:23:06,330 --> 00:23:08,410
사람들은 여러 벤치마크에서 이 오류를

547
00:23:08,410 --> 00:23:11,690
발견했고, CLIP이 합성성(compositionality)

548
00:23:11,690 --> 00:23:14,810
개념을 갖고 있지 않다는 것을 확인했습니다.

549
00:23:14,810 --> 00:23:17,570
즉, 머그컵과 풀, 혹은 풀과 머그컵의

550
00:23:17,570 --> 00:23:18,960
차이 같은 개념입니다.

551
00:23:18,960 --> 00:23:21,240
이것은 머그컵과 풀 같은

552
00:23:21,240 --> 00:23:24,380
서로 다른 개념들을 조합하는

553
00:23:24,380 --> 00:23:26,900
문제인데, CLIP 표현에서는

554
00:23:26,900 --> 00:23:31,303
이 개별 요소들의 관계가 잘 조합되지 않습니다.

555
00:23:31,303 --> 00:23:33,220
Winoground, CREPE,

556
00:23:33,220 --> 00:23:36,180
ARO 같은 다양한 벤치마크가 많이 있습니다.

557
00:23:36,180 --> 00:23:39,620
이 벤치마크들 중 상당수가 제 연구실에서 나왔습니다.

558
00:23:39,620 --> 00:23:42,260
이들은 반복해서 CLIP이 많은

559
00:23:42,260 --> 00:23:44,900
한계가 있고, 할 수 없는 일이

560
00:23:44,900 --> 00:23:46,980
많다는 것을 발견했습니다.

561
00:23:46,980 --> 00:23:49,620
물론 이에 대한 반응으로, 커뮤니티는 즉시 배치를

562
00:23:49,620 --> 00:23:53,900
수작업으로 구성해 어려운 네거티브(hard negatives)를 포함시키는

563
00:23:53,900 --> 00:23:57,620
방법을 고민하기 시작했습니다. 예를 들어, 한 종류의 코기를

564
00:23:57,620 --> 00:24:00,400
갖고 있다면 다른 종류의 코기도 포함시키는 식이죠.

565
00:24:00,400 --> 00:24:03,940
그래서 모델이 좋은 표현을 배우도록 강제하는 겁니다.

566
00:24:03,940 --> 00:24:06,660
이렇게 어려운 네거티브로 훈련하는

567
00:24:06,660 --> 00:24:10,100
아이디어는 1년 동안 커뮤니티에서 매우 인기를

568
00:24:10,100 --> 00:24:12,940
끌었습니다. 그러다 후속 논문에서 어려운

569
00:24:12,940 --> 00:24:14,760
네거티브로 훈련하면

570
00:24:14,760 --> 00:24:18,720
의미론에 관한 많은 것을 오히려 잊게 된다고 발표했습니다.

571
00:24:18,720 --> 00:24:20,470
왜 그런지는 아직

572
00:24:20,470 --> 00:24:22,550
이론적으로 완전히 이해하지

573
00:24:22,550 --> 00:24:25,470
못했지만, 다양한 환경과

574
00:24:25,470 --> 00:24:28,630
데이터셋에서 일반화 성능이 오히려 나빠지는

575
00:24:28,630 --> 00:24:30,510
결과가 나왔습니다.

576
00:24:30,510 --> 00:24:32,790
그래서 데이터셋 구성, 배치

577
00:24:32,790 --> 00:24:35,855
구성, 훈련 신호 설계에 관한 올바른

578
00:24:35,855 --> 00:24:38,230
방법을 찾는 데는 아직 할

579
00:24:38,230 --> 00:24:39,590
일이 많습니다.

580
00:24:39,590 --> 00:24:41,370
아직 갈 길이 멀다는 뜻입니다.

581
00:24:41,370 --> 00:24:44,510
하지만 어쨌든, 사람들은 CLIP에 대해 여전히

582
00:24:44,510 --> 00:24:48,190
매우 기대하고 있습니다. 왜냐하면 어느 정도의 감독

583
00:24:48,190 --> 00:24:49,830
신호는 제공하기 때문입니다.

584
00:24:49,830 --> 00:24:54,630
물론 다시 말하지만, 이미지 수준 캡션만으로는 충분하지 않습니다.

585
00:24:54,630 --> 00:24:57,030
이상적으로는 그 이상을 원합니다.

586
00:24:57,030 --> 00:24:59,310
단순히 사람이 길을 건넌다는 것뿐 아니라, 그

587
00:24:59,310 --> 00:25:01,670
사람이 이 위치에 있고, 차는 여기 있고,

588
00:25:01,670 --> 00:25:04,910
길은 여기 있다는 식으로 위치 정보까지 파악할 수 있어야 합니다.

589
00:25:04,910 --> 00:25:07,110
그런 모든 정보, 즉 그라운딩 정보가

590
00:25:07,110 --> 00:25:08,990
CLIP에는 완전히 빠져 있습니다.

591
00:25:08,990 --> 00:25:11,990
그래서 이상적으로는 데이터셋에 이런

592
00:25:11,990 --> 00:25:14,453
정보가 포함되어야 하고,

593
00:25:14,453 --> 00:25:16,120
모델도 그런 정보를

594
00:25:16,120 --> 00:25:18,160
추론할 수 있어야 합니다.

595
00:25:18,160 --> 00:25:21,840
또한 CLIP의 큰 단점 중 하나는 데이터셋이

596
00:25:21,840 --> 00:25:24,300
아무리 커도, 예를 들어

597
00:25:24,300 --> 00:25:27,900
50억 장 이상의 이미지를 모아도, 당신이

598
00:25:27,900 --> 00:25:29,880
중요하게 생각하는 모든

599
00:25:29,880 --> 00:25:33,260
것을 담기에는 여전히 부족하다는 점입니다.

600
00:25:33,260 --> 00:25:34,760
그래서 우리는 데이터 필터링에

601
00:25:34,760 --> 00:25:36,740
많은 노력을 기울이고 있습니다.

602
00:25:36,740 --> 00:25:38,200
인터넷에서 어떻게 최적의

603
00:25:38,200 --> 00:25:41,660
훈련 데이터를 걸러내 CLIP 모델 훈련에 사용할지 말이죠.

604
00:25:41,660 --> 00:25:43,840
오늘은 자세히 다루지

605
00:25:43,840 --> 00:25:47,400
않겠지만, 이런 여러 방법들이

606
00:25:47,400 --> 00:25:49,680
현재 이 분야 연구의

607
00:25:49,680 --> 00:25:51,480
최전선이 되었습니다.

608
00:25:51,480 --> 00:25:54,400
좋습니다, 이것이 우리가 이야기한 첫 번째 종류의 파운데이션

609
00:25:54,400 --> 00:25:55,300
모델입니다.

610
00:25:55,300 --> 00:25:57,440
분류를 다양한 작업으로

611
00:25:57,440 --> 00:25:59,260
일반화하는 것이죠.

612
00:25:59,260 --> 00:26:02,860
이제 비전과 언어 모델에 대해 이야기해 보겠습니다.

613
00:26:02,860 --> 00:26:05,780
최근 2년, 2년 반 사이에 인기를 끈

614
00:26:05,780 --> 00:26:08,820
새로운 종류의 파운데이션 모델이 있는데,

615
00:26:08,820 --> 00:26:12,930
우리는 이를 멀티모달 언어 모델이라고 부릅니다.

616
00:26:12,930 --> 00:26:15,050
먼저 LLaVA에 대해 이야기하면서

617
00:26:15,050 --> 00:26:17,690
시작하겠습니다. LLaVA는 아마도

618
00:26:17,690 --> 00:26:20,250
가장 처음으로 매우 인기를 끈 멀티모달

619
00:26:20,250 --> 00:26:22,210
언어 모델 중 하나입니다.

620
00:26:22,210 --> 00:26:24,970
여기서 동기는, 다음 토큰 예측,

621
00:26:24,970 --> 00:26:28,830
즉 자동완성 과정을 하는 언어 모델이 많은

622
00:26:28,830 --> 00:26:31,930
새로운 작업에 적응하는 데 매우

623
00:26:31,930 --> 00:26:33,350
유용하다는 점입니다.

624
00:26:33,350 --> 00:26:37,830
그렇다면 이미지 모델도 똑같이 할 수

625
00:26:37,830 --> 00:26:39,110
있을까요?

626
00:26:39,110 --> 00:26:41,010
이미지를 입력받아 이

627
00:26:41,010 --> 00:26:43,730
자동회귀 과정과 비슷한 다양한

628
00:26:43,730 --> 00:26:46,370
추론을 할 수 있을까요?

629
00:26:46,370 --> 00:26:49,210
이런 생각에서 비전 언어 모델, 즉 멀티모달

630
00:26:49,210 --> 00:26:51,250
모델이라는 모델 종류가 등장했습니다.

631
00:26:51,250 --> 00:26:54,590
하지만 물론, 역사적으로 정확히 말하자면, 이

632
00:26:54,590 --> 00:26:58,530
아이디어가 2022년에 완전히 새로 나온 것은 아닙니다.

633
00:26:58,530 --> 00:27:02,730
2019년에 ViLBERT가 실제로 이 아이디어를 도입했습니다.

634
00:27:02,730 --> 00:27:05,250
2019년 ViLBERT라는

635
00:27:05,250 --> 00:27:08,010
논문이 있는데, 이 논문은 이미지

636
00:27:08,010 --> 00:27:12,620
모델과 언어 모델을 모두 결합해 다양한 작업에 걸친

637
00:27:12,620 --> 00:27:15,340
일반화를 달성하려 했지만, 모두

638
00:27:15,340 --> 00:27:20,580
트랜스포머 이전에 훈련되었고 대부분 LSTM을 사용했습니다.

639
00:27:20,580 --> 00:27:23,220
그래서 이 모든 것이 다시 부활한

640
00:27:23,220 --> 00:27:26,780
것이 바로 지금 LLaVA에서 일어나고 있는

641
00:27:26,780 --> 00:27:30,080
일입니다. 많은 모델들이 더 나은 아키텍처와

642
00:27:30,080 --> 00:27:33,160
더 나은 목표 함수로 전환했고, 이제는

643
00:27:33,160 --> 00:27:35,640
개별 작업만 훈련하는 것이 아니라

644
00:27:35,640 --> 00:27:39,600
인터넷에서 가져온 사전 훈련 목표를 사용해 다양한

645
00:27:39,600 --> 00:27:42,620
작업의 기반 위에서 훈련하고 있습니다.

646
00:27:42,620 --> 00:27:43,560
그럼 이게 어떻게 작동할까요?

647
00:27:43,560 --> 00:27:45,500
LLaVA를 어떻게 생각해야 할까요?

648
00:27:45,500 --> 00:27:49,260
LLaVA를 설명하기 위해 한 걸음 물러나서

649
00:27:49,260 --> 00:27:53,340
트랜스포머 모델, 특히 셀프 어텐션에 대해

650
00:27:53,340 --> 00:27:54,540
생각해 봅시다.

651
00:27:54,540 --> 00:27:58,180
언어 모델을 생각할 때, 이들은 과거에

652
00:27:58,180 --> 00:28:00,155
집중(attend)하는 역할을 합니다.

653
00:28:00,155 --> 00:28:02,280
즉, 단어 시퀀스가 들어오면,

654
00:28:02,280 --> 00:28:04,980
예를 들어, 'cats are

655
00:28:04,980 --> 00:28:07,060
so'라는 문장이 있을 때,

656
00:28:07,060 --> 00:28:09,830
모델은 다음 단어를 생성하기

657
00:28:09,830 --> 00:28:12,470
위해 그 과거 문맥에 집중합니다.

658
00:28:12,470 --> 00:28:14,230
그래서 모델은 다음 단어가

659
00:28:14,230 --> 00:28:16,687
'cute'일 거라고 생각할 수 있습니다.

660
00:28:16,687 --> 00:28:18,270
이것을 다른

661
00:28:18,270 --> 00:28:19,770
방식으로 표현하면,

662
00:28:19,770 --> 00:28:22,090
입력 텍스트가 아래에서 들어오고,

663
00:28:22,090 --> 00:28:25,590
모델이 다음 단어인 'cute'를 생성하는 겁니다.

664
00:28:25,590 --> 00:28:28,750
비전-언어 모델을 생각할 때,

665
00:28:28,750 --> 00:28:32,710
사람들이 보통 말하는 것은 우리가 관심 있는

666
00:28:32,710 --> 00:28:34,550
이미지와 그 대화를

667
00:28:34,550 --> 00:28:37,910
연결해 추가적인 문맥을 넣는 것입니다.

668
00:28:37,910 --> 00:28:41,310
그래서 이미지를 어떤 식으로든 토크나이즈(tokenize)해서,

669
00:28:41,310 --> 00:28:44,070
그 토큰들을 언어

670
00:28:44,070 --> 00:28:46,890
모델에 과거 문맥인 'cats are

671
00:28:46,890 --> 00:28:49,350
so'와 함께 넣고, 그걸 바탕으로

672
00:28:49,350 --> 00:28:51,670
나머지 설명을 자동완성하는 거죠.

673
00:28:51,670 --> 00:28:54,030
이것이 LLaVA의 기본 아이디어입니다.

674
00:28:54,030 --> 00:28:57,270
이미지 토큰과 생성되는 단어들을 함께

675
00:28:57,270 --> 00:28:59,470
넣어 그 이미지에 대해 계속해서

676
00:28:59,470 --> 00:29:02,070
더 많은 단어를 생성하는 겁니다.

677
00:29:02,070 --> 00:29:03,810
물론 여기서 질문이 생깁니다.

678
00:29:03,810 --> 00:29:05,890
이 토큰들을 어떻게 정의할까요?

679
00:29:05,890 --> 00:29:08,600
토큰이 처음부터 무엇이어야 할까요?

680
00:29:08,600 --> 00:29:12,960
LLaVA의 해결책은 CLIP 이미지 인코더를 사용하는 것이었습니다.

681
00:29:12,960 --> 00:29:16,200
CLIP 모델에서 이미지 인코더를

682
00:29:16,200 --> 00:29:19,440
가져와서, 그 인코더에서 토큰을

683
00:29:19,440 --> 00:29:21,283
추출한 겁니다.

684
00:29:21,283 --> 00:29:23,200
처음에 생각할 수 있는 방법은

685
00:29:23,200 --> 00:29:25,080
CLS 토큰만 사용하는 것입니다.

686
00:29:25,080 --> 00:29:26,520
여기 보시면—마우스가

687
00:29:26,520 --> 00:29:28,800
작동하는지 볼게요.

688
00:29:28,800 --> 00:29:29,300
아, 작동하네요.

689
00:29:29,300 --> 00:29:31,940
이미지가 여기로 들어오고,

690
00:29:31,940 --> 00:29:33,760
패치로 나뉩니다.

691
00:29:33,760 --> 00:29:35,840
각 패치는 CLIP의 트랜스포머

692
00:29:35,840 --> 00:29:39,440
아키텍처에 입력되는 표현으로 변환됩니다.

693
00:29:39,440 --> 00:29:41,720
여러 층의 처리를 거치고,

694
00:29:41,720 --> 00:29:44,080
마지막에는 각 패치마다

695
00:29:44,080 --> 00:29:47,000
여러 토큰과 CLS 토큰의

696
00:29:47,000 --> 00:29:48,560
표현을 얻습니다.

697
00:29:48,560 --> 00:29:51,500
지금까지는 CLS 토큰만 고려했습니다.

698
00:29:51,500 --> 00:29:53,720
분류 작업을 할 때

699
00:29:53,720 --> 00:29:56,060
CLS 토큰만 사용했죠.

700
00:29:56,060 --> 00:29:59,400
하지만 그 외에도 많은 다른 토큰들이 있습니다.

701
00:29:59,400 --> 00:30:01,160
문제는 이 다른 토큰들은

702
00:30:01,160 --> 00:30:03,160
감독(supervision)을 받지 않는다는 점입니다.

703
00:30:03,160 --> 00:30:06,090
CLS 토큰은 텍스트와의 대조적 목표(contrastive

704
00:30:06,090 --> 00:30:10,490
objective)로 감독을 받지만, 다른 토큰들은 어떤 용도로도 사용되지
않았습니다.

705
00:30:10,490 --> 00:30:13,070
그래서 실제로 유용한 정보를 포함하지 않을 수도 있습니다.

706
00:30:13,070 --> 00:30:16,170
그리고 경험적으로, 사람들은 이 특징들이 그리

707
00:30:16,170 --> 00:30:18,450
유용하지 않다는 것을 보여주었지만, 한

708
00:30:18,450 --> 00:30:22,290
층 더 뒤로 가서 CLIP 인코더의 끝에서 두 번째 층,

709
00:30:22,290 --> 00:30:26,370
즉 펜얼티메이트 레이어를 보면 이 특징들이 실제로 매우 유용하다는

710
00:30:26,370 --> 00:30:27,770
것을 보여주었습니다.

711
00:30:27,770 --> 00:30:30,450
그래서 이 특징들은 최종 층에서 최종 CLIP

712
00:30:30,450 --> 00:30:32,610
임베딩을 생성하는 데 사용됩니다.

713
00:30:32,610 --> 00:30:35,370
그리고 이 특징들은 이미지 전체에서 객체가

714
00:30:35,370 --> 00:30:39,050
어디에 있는지에 대한 많은 공간 정보를 포함하고 있습니다.

715
00:30:39,050 --> 00:30:40,890
그래서 이것이 보통

716
00:30:40,890 --> 00:30:49,010
CLIP 인코더와 트랜스포머 기반 LLM 모델을 결합할 때 사용하는

717
00:30:49,010 --> 00:30:49,890
특징들입니다.

718
00:30:49,890 --> 00:30:52,630
이것이 전체 LLaVA 아키텍처의 모습입니다.

719
00:30:52,630 --> 00:30:56,090
이미지를 사전 학습된 CLIP

720
00:30:56,090 --> 00:30:58,650
인코더에 넣고, 거기서 여러

721
00:30:58,650 --> 00:31:01,690
특징을 추출한 다음, 그 특징들을

722
00:31:01,690 --> 00:31:05,800
학습해야 하는 선형 계층에 통과시킵니다.

723
00:31:05,800 --> 00:31:07,740
이 선형 계층은 CLIP

724
00:31:07,740 --> 00:31:11,420
표현을 LLM이 이해하고 해석할 수

725
00:31:11,420 --> 00:31:14,460
있는 형태로 변환하도록 학습됩니다.

726
00:31:14,460 --> 00:31:16,580
이 토큰들을 얻으면 이제

727
00:31:16,580 --> 00:31:20,240
모든 토큰을 언어 모델에 입력할 수 있습니다.

728
00:31:20,240 --> 00:31:22,940
그리고 이제 그 이미지 자체에

729
00:31:22,940 --> 00:31:26,500
대한 대화를 생성할 수 있습니다.

730
00:31:26,500 --> 00:31:28,780
LLaVA는 가장 처음에 인기를 끌었던

731
00:31:28,780 --> 00:31:29,840
모델 중 하나였습니다.

732
00:31:29,840 --> 00:31:33,460
그 뒤를 이어 구글은 빠르게 Flamingo를

733
00:31:33,460 --> 00:31:36,620
발표했는데, Flamingo는 LLaVA의

734
00:31:36,620 --> 00:31:40,300
비전 인코더 특징과 대형 언어 모델을

735
00:31:40,300 --> 00:31:42,560
결합하는 전체 구조를 많이 따랐지만,

736
00:31:42,560 --> 00:31:44,260
차별화된 점은

737
00:31:44,260 --> 00:31:48,100
서로 다른 특징들을 어떻게 융합하느냐에 있었습니다.

738
00:31:48,100 --> 00:31:51,460
LLaVA에서는 특징들을 선형 계층을

739
00:31:51,460 --> 00:31:54,500
통해 입력의 일부로 넣었지만,

740
00:31:54,500 --> 00:31:56,620
Flamingo에서는

741
00:31:56,620 --> 00:31:59,260
비전 인코더에서 나오는 모든

742
00:31:59,260 --> 00:32:01,180
특징들을 LLM의 모든

743
00:32:01,180 --> 00:32:03,750
층에 입력으로 넣었습니다.

744
00:32:03,750 --> 00:32:04,250
네.

745
00:32:04,250 --> 00:32:07,510
그래서 LLM 아키텍처 자체에 약간의 변경을

746
00:32:07,510 --> 00:32:08,790
해야 했습니다.

747
00:32:08,790 --> 00:32:10,890
이렇게 해서 그들이 변화를 만든 겁니다.

748
00:32:10,890 --> 00:32:13,190
여기 Flamingo의 학습 데이터

749
00:32:13,190 --> 00:32:14,210
예시가 있습니다.

750
00:32:14,210 --> 00:32:15,810
이미지들이 인코딩되어 있죠.

751
00:32:15,810 --> 00:32:18,250
이 개와 고양이가 각각

752
00:32:18,250 --> 00:32:19,630
임베딩됩니다.

753
00:32:19,630 --> 00:32:22,430
그리고 이 둘은 LLM의 모든

754
00:32:22,430 --> 00:32:24,190
레이어에 입력됩니다.

755
00:32:24,190 --> 00:32:26,350
아래에는 이미지로

756
00:32:26,350 --> 00:32:30,070
시작해서 그 이미지를 설명하는 데이터가

757
00:32:30,070 --> 00:32:32,790
있고, 다음 이미지도 설명하고,

758
00:32:32,790 --> 00:32:35,230
계속해서 이어집니다.

759
00:32:35,230 --> 00:32:38,030
이것들이 LLM의 입력으로 들어가는 거죠.

760
00:32:38,030 --> 00:32:43,030
출력은 마지막 이미지를 자동 완성하는 겁니다.

761
00:32:43,030 --> 00:32:44,710
한 이미지가 있고, 개에

762
00:32:44,710 --> 00:32:47,037
대한 설명이 이어지고, 두 번째

763
00:32:47,037 --> 00:32:48,870
이미지가 나오면 설명을 시작하고,

764
00:32:48,870 --> 00:32:51,550
모델은 그 두 번째 이미지 설명을

765
00:32:51,550 --> 00:32:53,350
자동 완성하도록 학습됩니다.

766
00:32:53,350 --> 00:32:54,170
그럼 그들은 무엇을 했을까요?

767
00:32:54,170 --> 00:32:56,330
모델 자체에 어떤 변화를 줬을까요?

768
00:32:56,330 --> 00:33:01,000
LLM의 모든 레이어에 GATED X 크로스-어텐션

769
00:33:01,000 --> 00:33:03,760
모듈을 추가했습니다.

770
00:33:03,760 --> 00:33:05,580
그리고 또 한 가지 변화를 줬습니다.

771
00:33:05,580 --> 00:33:08,080
여기 perceiver

772
00:33:08,080 --> 00:33:12,720
sampler를 추가해서 이미지 표현을 샘플링하고

773
00:33:12,720 --> 00:33:14,300
다운샘플링합니다.

774
00:33:14,300 --> 00:33:17,400
그래서 모든 레이어에 대해 더 작은 차원과 고정된 토큰

775
00:33:17,400 --> 00:33:18,680
수를 가지게 됩니다.

776
00:33:18,680 --> 00:33:22,640
자, 그 모듈들이 어떻게 생겼는지 좀 더 자세히 설명하겠습니다.

777
00:33:22,640 --> 00:33:24,840
이것이 전체 아키텍처입니다, 전체적으로요.

778
00:33:24,840 --> 00:33:26,500
대부분의 구성 요소는 고정되어

779
00:33:26,500 --> 00:33:28,660
있습니다. 모든 언어 모델 가중치가 고정되어 있고,

780
00:33:28,660 --> 00:33:30,860
모든 비전 모델 부분도 고정되어 있습니다.

781
00:33:30,860 --> 00:33:32,400
훈련되는 유일한 부분은

782
00:33:32,400 --> 00:33:34,500
이 perceiver sampler

783
00:33:34,500 --> 00:33:36,680
구성 요소들과 LLM의 모든

784
00:33:36,680 --> 00:33:41,000
레이어에 추가된 이 cross-attention 레이어입니다.

785
00:33:41,000 --> 00:33:44,040
그럼 이 cross-attention 모듈이 어떻게 생겼는지

786
00:33:44,040 --> 00:33:44,840
이야기해 보겠습니다.

787
00:33:44,840 --> 00:33:47,900
이것은 제가 그 cross-attention 모듈을 확대해서 보여드리는
겁니다.

788
00:33:47,900 --> 00:33:51,520
모든 LLM 레이어 바로 전에 이 cross-attention

789
00:33:51,520 --> 00:33:53,660
구성 요소가 있습니다.

790
00:33:53,660 --> 00:33:56,840
이것의 목적은 이미지 특징을 보고

791
00:33:56,840 --> 00:33:58,930
어떤 부분을 유지할지,

792
00:33:58,930 --> 00:34:01,690
언어 모델이 알면 유용할

793
00:34:01,690 --> 00:34:04,810
부분이 무엇인지 결정하는 겁니다.

794
00:34:04,810 --> 00:34:08,210
그리고 이들은 지금까지 보신 구성 요소들로

795
00:34:08,210 --> 00:34:10,170
이걸 설계했습니다.

796
00:34:10,170 --> 00:34:12,530
그래서 cross-attention

797
00:34:12,530 --> 00:34:15,210
레이어를 사용해 이미지 특징에 주의를

798
00:34:15,210 --> 00:34:19,268
기울이고, 그 다음에 이 tanh 비선형 활성화를 추가했습니다.

799
00:34:19,268 --> 00:34:21,810
이것은 기본적으로 어떤 부분을

800
00:34:21,810 --> 00:34:25,130
유지할지, 어떤 이미지 부분을 잊을지

801
00:34:25,130 --> 00:34:26,929
결정하는 역할을 합니다.

802
00:34:26,929 --> 00:34:29,449
그 다음 완전 연결층을 거쳐서

803
00:34:29,449 --> 00:34:32,190
표현을 약간 조정하고,

804
00:34:32,190 --> 00:34:35,409
다시 tanh 비선형성을 통해 어떤

805
00:34:35,409 --> 00:34:38,062
부분을 유지할지 결정합니다.

806
00:34:38,062 --> 00:34:39,770
이 두 구성 요소를

807
00:34:39,770 --> 00:34:43,489
거치고 각각 잔차 연결이 있으며, 그 후에

808
00:34:43,489 --> 00:34:46,570
정상적인 언어 모델 처리로 넘어가

809
00:34:46,570 --> 00:34:49,449
필요한 단어를 계속 생성합니다.

810
00:34:49,449 --> 00:34:52,409
즉, 매 레이어마다 시각적

811
00:34:52,409 --> 00:34:56,409
특징을 통합하고 주의를 기울이기 위한

812
00:34:56,409 --> 00:34:59,290
추가 레이어가 더해진 겁니다.

813
00:34:59,290 --> 00:34:59,790
네.

814
00:34:59,790 --> 00:35:01,382
실제로 코드에서 이

815
00:35:01,382 --> 00:35:03,590
수정은 관심 있으시면,

816
00:35:03,590 --> 00:35:06,730
cross-attention 레이어와 그

817
00:35:06,730 --> 00:35:09,570
사이에 tanh 비선형성을 추가하는

818
00:35:09,570 --> 00:35:11,625
두세 줄 정도의 코드입니다.

819
00:35:11,625 --> 00:35:12,750
그게 전부입니다.

820
00:35:12,750 --> 00:35:15,170
그래서 코드 측면에서는 아주 최소한의 변화입니다.

821
00:35:15,170 --> 00:35:17,610
하지만 모델 입장에서는 아주 거대한

822
00:35:17,610 --> 00:35:20,010
변화인데, 이제는 처리의 각 레이어마다

823
00:35:20,010 --> 00:35:23,470
이미지의 어떤 부분에 주목할지 선택할 수 있기 때문입니다.

824
00:35:23,470 --> 00:35:25,570
즉, 모델에게 비전 특징에

825
00:35:25,570 --> 00:35:30,330
언제 어떻게 주목할지 결정할 수 있는 많은 능력을 부여하는 겁니다.

826
00:35:30,330 --> 00:35:31,050
좋습니다.

827
00:35:31,050 --> 00:35:33,570
Flamingo는 매우, 매우 흥미로웠지만,

828
00:35:33,570 --> 00:35:35,890
훈련시키는 것은 매우 어려웠습니다.

829
00:35:35,890 --> 00:35:38,170
그리고 그들은 모델이 다양한 작업에 적응할

830
00:35:38,170 --> 00:35:41,570
수 있도록 하는 정말 기발한 훈련 방식을 가지고 있었습니다.

831
00:35:41,570 --> 00:35:45,970
그들이 훈련한 방식은 여러 이미지를 하나로

832
00:35:45,970 --> 00:35:48,750
연결(concatenation)하는 방식이었습니다.

833
00:35:48,750 --> 00:35:51,430
그래서 단일 이미지와 단일 설명만 있는 게 아니라,

834
00:35:51,430 --> 00:35:55,140
처음에 '여기 내 귀여운 반려동물 사진들이

835
00:35:55,140 --> 00:35:57,180
있습니다'라는 설명과 이미지

836
00:35:57,180 --> 00:35:59,600
시작 문장이 있고, 그 다음 첫

837
00:35:59,600 --> 00:36:01,620
번째 이미지와 그에 대한

838
00:36:01,620 --> 00:36:06,420
설명, 그리고 두 번째 이미지와 두 번째 이미지에 대한 설명이

839
00:36:06,420 --> 00:36:08,160
이어지는 식이었습니다.

840
00:36:08,160 --> 00:36:10,380
그래서 훈련 데이터는

841
00:36:10,380 --> 00:36:14,180
이미지-텍스트, 이미지-텍스트가 교차된 긴 시퀀스처럼

842
00:36:14,180 --> 00:36:16,500
보이도록 구성되었습니다.

843
00:36:16,500 --> 00:36:19,120
물론, 어떤 단일 이미지를 설명할

844
00:36:19,120 --> 00:36:22,480
때는 모델이 전체 문맥을 보는 게 아니라,

845
00:36:22,480 --> 00:36:24,760
그 특정 이미지 하나만 보도록 해야 합니다.

846
00:36:24,760 --> 00:36:27,220
그래서 그들은 마스킹 방식을

847
00:36:27,220 --> 00:36:31,260
만들어서, 각 이미지가 생성될 때 그 이미지 특징만 보고

848
00:36:31,260 --> 00:36:34,100
다른 이미지는 보지 않도록 했습니다.

849
00:36:34,100 --> 00:36:35,660
즉, '내 강아지가

850
00:36:35,660 --> 00:36:38,320
잔디에 앉아 있다'는 설명을

851
00:36:38,320 --> 00:36:40,940
생성할 때는 강아지에 해당하는 특징만

852
00:36:40,940 --> 00:36:42,820
보고 그 단어들을

853
00:36:42,820 --> 00:36:44,120
생성하는 겁니다.

854
00:36:44,120 --> 00:36:47,100
마찬가지로 고양이 설명을 생성할 때는 고양이

855
00:36:47,100 --> 00:36:50,780
이미지 특징만 보고 다른 이미지는 보지 않습니다.

856
00:36:50,780 --> 00:36:53,150
그래서 이처럼 그들은

857
00:36:53,150 --> 00:36:55,790
수작업으로 마스킹 규칙을

858
00:36:55,790 --> 00:36:58,750
만들어서 설명이 항상 그 특정

859
00:36:58,750 --> 00:37:02,030
이미지만 보고 따르도록 했습니다.

860
00:37:02,030 --> 00:37:04,070
하지만 훈련될 때, 모델은

861
00:37:04,070 --> 00:37:08,750
생성하는 모든 것의 전체 문맥을 볼 수 있습니다.

862
00:37:08,750 --> 00:37:09,890
그게 왜 도움이 될까요?

863
00:37:09,890 --> 00:37:11,910
이 모든 것을 함께 볼 수 있는 이

864
00:37:11,910 --> 00:37:13,810
전체 과정이 왜 도움이 될까요?

865
00:37:13,810 --> 00:37:15,830
이것이 도움이 되는 이유는 이런 종류의

866
00:37:15,830 --> 00:37:17,550
응용을 할 수 있게 해주기 때문입니다.

867
00:37:17,550 --> 00:37:19,950
여기 Flamingo가 보여준 세 가지

868
00:37:19,950 --> 00:37:21,750
다른 응용 사례가 있습니다.

869
00:37:21,750 --> 00:37:25,390
모두 여러 대화나 여러 이미지를 다루는 데

870
00:37:25,390 --> 00:37:27,270
중점을 두고 있습니다.

871
00:37:27,270 --> 00:37:30,710
첫 번째 경우에는 이미지가 입력되고

872
00:37:30,710 --> 00:37:33,110
Flamingo 모델이 이 이미지가 달

873
00:37:33,110 --> 00:37:37,150
위에 있는 두 개의 테디베어 사진이라고 설명합니다.

874
00:37:37,150 --> 00:37:38,950
그리고 나서 사람들이 또 다른 질문을 할

875
00:37:38,950 --> 00:37:40,950
수 있게 해줍니다. 예를 들어, 그들은 무엇을

876
00:37:40,950 --> 00:37:42,630
하고 있나요? 라고 물을 수 있죠.

877
00:37:42,630 --> 00:37:45,430
이미 기존의 대형 언어 모델을 사용해

878
00:37:45,430 --> 00:37:48,150
훈련되고 있기 때문에, 그 대형

879
00:37:48,150 --> 00:37:50,950
언어 모델의 추론 능력을 물려받아 이제

880
00:37:50,950 --> 00:37:53,360
이 특정 질문에 대해 추론하고

881
00:37:53,360 --> 00:37:54,760
답할 수 있습니다.

882
00:37:54,760 --> 00:37:56,920
이제 답변할 수 있는데, 테디베어들이 대화를

883
00:37:56,920 --> 00:37:58,340
나누고 있다고 말할 수 있습니다.

884
00:37:58,340 --> 00:38:01,182
그리고 사용자가 어떤 물건을 사용하고 있나요? 라고 물을 수도 있습니다.

885
00:38:01,182 --> 00:38:02,640
다시 Flamingo는 컴퓨터처럼

886
00:38:02,640 --> 00:38:04,900
보인다고 답할 수 있고, 이런 식으로 계속됩니다.

887
00:38:04,900 --> 00:38:07,480
이미지에 대해 이런 다중 턴 대화를

888
00:38:07,480 --> 00:38:10,820
가능하게 하려면 두 가지를 하면 됩니다.

889
00:38:10,820 --> 00:38:14,000
먼저 언어 모델을 사전 훈련하고 그 언어 모델을

890
00:38:14,000 --> 00:38:16,540
Flamingo에 통합해서 훈련합니다.

891
00:38:16,540 --> 00:38:18,200
둘째, 모델이 훈련

892
00:38:18,200 --> 00:38:20,680
데이터 전반에 걸쳐 여러 이미지와

893
00:38:20,680 --> 00:38:23,040
여러 턴을 보게 하여 더

894
00:38:23,040 --> 00:38:26,600
긴 텍스트 시퀀스에 적응할 수 있게 합니다.

895
00:38:26,600 --> 00:38:28,480
또한 여러 이미지를 주고 이

896
00:38:28,480 --> 00:38:31,300
이미지들에 공통된 점이 무엇인지 물어볼 수도 있습니다.

897
00:38:31,300 --> 00:38:32,800
이제 Flamingo 모델은

898
00:38:32,800 --> 00:38:35,200
각각의 다른 구성 요소들을 보고 모두

899
00:38:35,200 --> 00:38:37,440
Flamingo라고 판단할 수 있습니다.

900
00:38:37,440 --> 00:38:40,540
그래서 정말 멋진 여러 응용을 시작할 수 있습니다.

901
00:38:40,540 --> 00:38:43,512
사람들은 또한 in-context learning을 할 수 있다는 것을
보여주었습니다.

902
00:38:43,512 --> 00:38:45,720
이게 언어 모델에서 이미 보셨는지

903
00:38:45,720 --> 00:38:48,040
모르겠지만, GPT를 사용할 때 in-context

904
00:38:48,040 --> 00:38:51,690
learning을 사용해본 적이 분명 있을 겁니다.

905
00:38:51,690 --> 00:38:55,090
여기 예시를 주면 비슷한 걸 더 만들어 달라고 하는 방식이죠.

906
00:38:55,090 --> 00:38:56,970
Flamingo도

907
00:38:56,970 --> 00:38:59,970
마찬가지로 이미지와 설명을 함께 넣어줄

908
00:38:59,970 --> 00:39:01,030
수 있습니다.

909
00:39:01,030 --> 00:39:02,710
그리고 이제 새로운 이미지를

910
00:39:02,710 --> 00:39:04,410
넣으면 설명을 제공합니다.

911
00:39:04,410 --> 00:39:07,990
또는 이렇게 할 수도 있습니다. 이미지와 질문,

912
00:39:07,990 --> 00:39:09,350
답변을 주는 거죠.

913
00:39:09,350 --> 00:39:11,130
이미지, 질문, 답변을 주는 겁니다.

914
00:39:11,130 --> 00:39:13,930
그 다음 새로운 이미지를 넣고 질문만

915
00:39:13,930 --> 00:39:15,485
하면 답변을 줍니다.

916
00:39:15,485 --> 00:39:18,110
즉, 이런 다양한 작업을 위해

917
00:39:18,110 --> 00:39:21,250
따로 학습시키는 게 아니라, 원하는 행동의

918
00:39:21,250 --> 00:39:22,890
예시를 보여주면

919
00:39:22,890 --> 00:39:25,490
새로운 행동에도 일반화할 수 있게

920
00:39:25,490 --> 00:39:26,790
만드는 겁니다.

921
00:39:26,790 --> 00:39:29,570
마찬가지로 단순 분류 작업에도 관심이 있을 수 있습니다.

922
00:39:29,570 --> 00:39:32,250
Flamingo를 이용해 분류도 할 수 있습니다.

923
00:39:32,250 --> 00:39:35,390
이미지를 주고 이것은 지하철, 이것은 의회라고

924
00:39:35,390 --> 00:39:37,010
알려줄 수 있습니다.

925
00:39:37,010 --> 00:39:39,050
그 다음에 이게 무엇인지 물어볼 수 있죠.

926
00:39:39,050 --> 00:39:43,330
심지어 OCR과 수학 문제도 가르칠 수 있습니다. 이미지와

927
00:39:43,330 --> 00:39:45,930
함께 '이건 2 더하기 1은 3이다'라고

928
00:39:45,930 --> 00:39:47,370
알려주는 식으로요.

929
00:39:47,370 --> 00:39:49,760
결국 새로운 이미지를 주면

930
00:39:49,760 --> 00:39:51,980
3 곱하기 6을 자동 완성하고,

931
00:39:51,980 --> 00:39:55,740
이 전체 과정을 추론해서 결과도 내줄

932
00:39:55,740 --> 00:39:57,240
수 있어야 합니다.

933
00:39:57,240 --> 00:39:57,740
네.

934
00:39:57,740 --> 00:39:59,740
이것은 몇 가지 예시를

935
00:39:59,740 --> 00:40:02,500
주고 새로운 것이 무엇인지 묻는

936
00:40:02,500 --> 00:40:06,060
few shot learning의 예입니다.

937
00:40:06,060 --> 00:40:09,040
만약 모든 문맥 내 예시를 버린다면, 그것이 zero

938
00:40:09,040 --> 00:40:10,700
shot learning입니다.

939
00:40:10,700 --> 00:40:12,860
그래서 우리는 그것들을 연결(concatenate)하지 않습니다.

940
00:40:12,860 --> 00:40:15,340
기술적으로는 이미지 토큰을 perceiver

941
00:40:15,340 --> 00:40:17,860
sampler를 통해 LLM의

942
00:40:17,860 --> 00:40:19,460
모든 레이어에 전달합니다.

943
00:40:19,460 --> 00:40:22,820
그래서 텍스트만이 Flamingo 모델에 입력으로

944
00:40:22,820 --> 00:40:25,060
연결되어 주어지고, 모델이

945
00:40:25,060 --> 00:40:28,100
이미지의 어느 부분에 주의를 기울일지 선택합니다.

946
00:40:28,100 --> 00:40:29,500
한 번만 줍니다.

947
00:40:29,500 --> 00:40:31,400
하지만 실제로는 이것이

948
00:40:31,400 --> 00:40:33,320
웹 인터페이스입니다.

949
00:40:33,320 --> 00:40:35,300
하지만 실제로는 사용자가

950
00:40:35,300 --> 00:40:37,780
대화를 계속할 것이라 가정하고

951
00:40:37,780 --> 00:40:39,380
모델을 캐시합니다.

952
00:40:39,380 --> 00:40:42,740
그래서 모델이 캐시되어 더 많은 토큰을 받을 준비가 되어 있습니다.

953
00:40:42,740 --> 00:40:44,800
네, 만약 캐시하지 않았다면,

954
00:40:44,800 --> 00:40:47,890
전체 대화를 입력으로 전달했을 겁니다.

955
00:40:47,890 --> 00:40:49,950
네.

956
00:40:49,950 --> 00:40:50,550
알겠습니다.

957
00:40:50,550 --> 00:40:52,990
Flamingo는 정말 멋졌는데,

958
00:40:52,990 --> 00:40:54,790
논문에 큰 표들이 있어서

959
00:40:54,790 --> 00:40:56,270
확인할 수 있습니다.

960
00:40:56,270 --> 00:40:57,870
정말 멋진 점은

961
00:40:57,870 --> 00:41:01,050
어려운 여러 작업들이 있었고,

962
00:41:01,050 --> 00:41:03,550
CLIP을 적응시켜야 했는데,

963
00:41:03,550 --> 00:41:06,990
Flamingo는 zero shot이나 few shot으로 그냥 할 수
있었다는 겁니다.

964
00:41:06,990 --> 00:41:09,310
그리고 여러 벤치마크에서 엄청난

965
00:41:09,310 --> 00:41:11,510
성능 향상이 보이기 시작했습니다.

966
00:41:11,510 --> 00:41:13,070
이때부터 분야가 몇

967
00:41:13,070 --> 00:41:15,950
가지 분류 벤치마크 보고에서 모든

968
00:41:15,950 --> 00:41:18,810
이해 작업 보고로 전환된 것 같습니다.

969
00:41:18,810 --> 00:41:21,690
질문 답변 과정으로 프레임할 수

970
00:41:21,690 --> 00:41:25,810
있다면 다양한 기술에 대한 벤치마크를 만들 수 있고,

971
00:41:25,810 --> 00:41:28,750
지난 2년간 컴퓨터 비전 분야에서

972
00:41:28,750 --> 00:41:31,470
이것이 표준이 되기 시작했습니다.

973
00:41:31,470 --> 00:41:35,270
그래서 작년쯤 우리는 여기까지 왔고,

974
00:41:35,270 --> 00:41:38,390
LLaVA의 성공을 보고 많은 회사들이

975
00:41:38,390 --> 00:41:41,890
이 모델들에 크게 투자하기 시작했습니다.

976
00:41:41,890 --> 00:41:44,920
그래서 GPT-4, GPT-4V,

977
00:41:44,920 --> 00:41:52,360
Gemini 1.5 Pro, Gemini 1.5 Flash 같은 API
모델들이 많이 나오기 시작했고,

978
00:41:52,360 --> 00:41:54,980
Anthropic도

979
00:41:54,980 --> 00:41:58,760
Claude 3 Opus로 등장했고,

980
00:41:58,760 --> 00:42:01,880
지금은 Claude 4 Opus도 나왔습니다.

981
00:42:01,880 --> 00:42:03,980
많은 모델들이 나오면서 여러

982
00:42:03,980 --> 00:42:05,760
벤치마크에서 훨씬 더 좋은

983
00:42:05,760 --> 00:42:07,240
성능을 보였습니다.

984
00:42:07,240 --> 00:42:10,120
여기서는 분야에서 인기 있는

985
00:42:10,120 --> 00:42:13,280
11개의 시각 이해 벤치마크 평균 성능을

986
00:42:13,280 --> 00:42:14,340
보여줍니다.

987
00:42:14,340 --> 00:42:16,360
엄청난 차이가 있습니다.

988
00:42:16,360 --> 00:42:19,920
LLaVA는 평균 약

989
00:42:19,920 --> 00:42:24,040
43% 정확도를 보이고,

990
00:42:24,040 --> 00:42:27,240
반면 GPT와 다른 모델들은

991
00:42:27,240 --> 00:42:32,780
70~80%대의 훨씬 좋은 성능을 냅니다.

992
00:42:32,780 --> 00:42:34,400
두 종류 모델

993
00:42:34,400 --> 00:42:37,080
간에 큰 성능 차이가 있습니다.

994
00:42:37,080 --> 00:42:39,900
이 차이를 보고 바로

995
00:42:39,900 --> 00:42:43,370
GPT와 Gemini를 증류한 변형

996
00:42:43,370 --> 00:42:48,130
모델을 만들고 출시하려는 시도가 있었습니다.

997
00:42:48,130 --> 00:42:50,150
중국 회사인 Alibaba는

998
00:42:50,150 --> 00:42:52,490
QWEN이라는 모델을 출시했고,

999
00:42:52,490 --> 00:42:54,970
그리고 Intern VL, Phi도 있습니다.

1000
00:42:54,970 --> 00:42:57,470
여러 가지 다른 모델들이 계속해서 나오고 있습니다.

1001
00:42:57,470 --> 00:43:00,530
그리고 이 모든 모델들은 GPT에서 증류된 것입니다.

1002
00:43:00,530 --> 00:43:02,810
GPT가 아니라면 Gemini에서 나온 것이죠.

1003
00:43:02,810 --> 00:43:05,890
이로 인해 분야에 큰 문제가 생겼는데,

1004
00:43:05,890 --> 00:43:09,090
이 문제는 제 연구 주제의 중요한 부분이기도

1005
00:43:09,090 --> 00:43:12,170
합니다. 즉, 연구 커뮤니티가 정말

1006
00:43:12,170 --> 00:43:14,210
성능 좋은 비전-언어 모델을

1007
00:43:14,210 --> 00:43:17,730
어떻게 만드는지 아직 잘 모른다는 겁니다.

1008
00:43:17,730 --> 00:43:20,270
이 모델들을 만드는 비결은

1009
00:43:20,270 --> 00:43:23,130
오직 OpenAI와 Google의

1010
00:43:23,130 --> 00:43:26,390
Gemini 팀만 알고 있습니다.

1011
00:43:26,390 --> 00:43:28,950
하지만 오픈 소스 커뮤니티는 여기 아래에 있습니다.

1012
00:43:28,950 --> 00:43:29,870
여기 아래에 있죠.

1013
00:43:29,870 --> 00:43:33,010
작년까지 연구 커뮤니티가 바로 이 위치에 있었습니다.

1014
00:43:33,010 --> 00:43:36,110
물론, 이 모델들이 정말 좋은 오픈 모델이라고 주장할 수 있지만,

1015
00:43:36,110 --> 00:43:37,650
사실 완전히 오픈된 것은 아닙니다.

1016
00:43:37,650 --> 00:43:39,650
왜냐하면 증류된 모델이라서, 실제로

1017
00:43:39,650 --> 00:43:42,980
이 모델들을 재현하는 방법을 알지 못하기 때문입니다.

1018
00:43:42,980 --> 00:43:44,640
우리는 단지 모델을 생산할 수 있을 뿐입니다.

1019
00:43:44,640 --> 00:43:46,160
하지만 GPT가 존재한다면 말이죠.

1020
00:43:46,160 --> 00:43:48,300
GPT가 없다면, 다른 모델들을

1021
00:43:48,300 --> 00:43:50,020
만드는 방법을 모릅니다.

1022
00:43:50,020 --> 00:43:51,717
그래서 지난 몇 년간

1023
00:43:51,717 --> 00:43:53,800
제 연구 주제는 이 격차를

1024
00:43:53,800 --> 00:43:55,640
어떻게 메우고, 정말 좋은

1025
00:43:55,640 --> 00:43:58,980
멀티모달 언어 모델을 어떻게 만들며, 그

1026
00:43:58,980 --> 00:44:01,340
이해를 전체 커뮤니티에 어떻게

1027
00:44:01,340 --> 00:44:03,060
전파할지에 집중해왔습니다.

1028
00:44:03,060 --> 00:44:08,860
그래서 지난 6개월에서 1년 동안 저희가

1029
00:44:08,860 --> 00:44:12,980
만든 것이 Molmo라는 모델

1030
00:44:12,980 --> 00:44:14,340
계열입니다.

1031
00:44:14,340 --> 00:44:17,540
위쪽에 Molmo의 성능을 보여드리고 있습니다.

1032
00:44:17,540 --> 00:44:20,180
Molmo가 다른 모든 모델과 차별화되는

1033
00:44:20,180 --> 00:44:23,000
점은 완전히 오픈 소스라는 겁니다,

1034
00:44:23,000 --> 00:44:25,160
즉 가중치가 공개되어 있습니다.

1035
00:44:25,160 --> 00:44:26,560
그래서 모델을 다운로드할 수 있습니다.

1036
00:44:26,560 --> 00:44:29,060
데이터도 공개되어 있어서, 학습 세트와 평가 세트 모두

1037
00:44:29,060 --> 00:44:30,223
다운로드할 수 있습니다.

1038
00:44:30,223 --> 00:44:32,140
코드도 공개되어 있어서, 충분한

1039
00:44:32,140 --> 00:44:34,360
GPU가 있다면 집에서 직접

1040
00:44:34,360 --> 00:44:37,700
Molmo를 학습시킬 수 있고, 평가를 하거나

1041
00:44:37,700 --> 00:44:39,810
새로운 평가를 추가하고, 다양한

1042
00:44:39,810 --> 00:44:42,930
새로운 용도에 맞게 모델을 적응시킬 수 있으며,

1043
00:44:42,930 --> 00:44:46,430
물론 다양한 상황에서 바로 사용할 수도 있습니다.

1044
00:44:46,430 --> 00:44:49,270
물론 학술 벤치마크만으로는 충분하지 않습니다.

1045
00:44:49,270 --> 00:44:51,790
왜냐하면 결국 우리가 관심 있는 것은 사람들이 이 모델을

1046
00:44:51,790 --> 00:44:53,610
실제로 사용할 것인가 하는 점이기 때문입니다.

1047
00:44:53,610 --> 00:44:56,830
사람들이 GPT 대신 이 모델을 사용하고 싶어할까요?

1048
00:44:56,830 --> 00:45:00,050
그래서 이를 제대로 평가하기 위해

1049
00:45:00,050 --> 00:45:02,690
Molmo와 함께 플레이그라운드를

1050
00:45:02,690 --> 00:45:05,030
공개했고, 대규모 사용자 연구를

1051
00:45:05,030 --> 00:45:07,510
진행했습니다. 여기서 우리 모델과

1052
00:45:07,510 --> 00:45:10,990
다른 모든 모델의 출력을 직접 비교했죠.

1053
00:45:10,990 --> 00:45:14,710
우리 모델은 GPT와 동일한 Elo 등급을 받았습니다.

1054
00:45:14,710 --> 00:45:17,910
GPT-40과의 Elo 등급 차이는

1055
00:45:17,910 --> 00:45:19,790
1로 2위를 차지했습니다.

1056
00:45:19,790 --> 00:45:22,670
이 그래프는 회전된 형태로, 몇 가지 예시를

1057
00:45:22,670 --> 00:45:24,390
보여드리기 위해 만든 겁니다.

1058
00:45:24,390 --> 00:45:26,770
참고로 이 평가는 엄청난 규모였습니다.

1059
00:45:26,770 --> 00:45:31,010
약 870명의 사용자에게 모델 출력을 보여주었고,

1060
00:45:31,010 --> 00:45:34,030
약 325,000번의 쌍별 비교를 진행하며,

1061
00:45:34,030 --> 00:45:36,800
어떤 모델 출력을 선호하는지 물었습니다.

1062
00:45:36,800 --> 00:45:39,340
우리 Moima 모델은 다시 말씀드리지만

1063
00:45:39,340 --> 00:45:41,303
2위를 차지했고, GPT와 우리

1064
00:45:41,303 --> 00:45:42,720
모델 사이에서 선호도가

1065
00:45:42,720 --> 00:45:44,960
거의 동전 던지기 수준이었습니다.

1066
00:45:44,960 --> 00:45:49,520
하지만 이미 Gemini 1.5 Pro와 Claude 3.5를 능가했습니다.

1067
00:45:49,520 --> 00:45:52,640
지금 큰 차이점은 저희가 작은 연구실인데도

1068
00:45:52,640 --> 00:45:56,080
구글의 수십억 달러 투자인 Gemini와

1069
00:45:56,080 --> 00:45:58,160
Anthropic의 수십억 달러 투자에

1070
00:45:58,160 --> 00:46:00,280
맞서 이미 GPT와 동등한 성능을

1071
00:46:00,280 --> 00:46:02,080
내고 있다는 점입니다.

1072
00:46:02,080 --> 00:46:05,000
그래서 저희는 이 전체 과정에 매우 흥분했습니다.

1073
00:46:05,000 --> 00:46:08,240
하지만 저희는 그 큰 모델들 바로 다음에 오는 70억

1074
00:46:08,240 --> 00:46:10,140
파라미터 모델도 개발했습니다.

1075
00:46:10,140 --> 00:46:12,680
이 70억 모델이 정말 흥미로운 이유는

1076
00:46:12,680 --> 00:46:15,240
단일 GPU에 올릴 수 있기 때문입니다.

1077
00:46:15,240 --> 00:46:17,360
즉, 이제 단일 GPU에서

1078
00:46:17,360 --> 00:46:19,680
다양한 비전 작업을 수행할 수

1079
00:46:19,680 --> 00:46:23,160
있는 모델을 많은 사람들이 사용하고 다양한 용도로

1080
00:46:23,160 --> 00:46:25,920
미세 조정할 수 있다는 뜻입니다.

1081
00:46:25,920 --> 00:46:28,200
저희는 이 모델을 9월 25일에

1082
00:46:28,200 --> 00:46:30,720
공개했고 커뮤니티가 매우 흥분했습니다.

1083
00:46:30,720 --> 00:46:34,440
이것은 매우 성능 좋은 멀티모달 비전 언어 모델이 처음 공개된

1084
00:46:34,440 --> 00:46:36,610
사례였고, 많은 사람들이 이 모델을

1085
00:46:36,610 --> 00:46:39,530
어떻게 활용할지에 대해 이야기하고 글을 쓰기

1086
00:46:39,530 --> 00:46:40,358
시작했습니다.

1087
00:46:40,358 --> 00:46:42,650
계속해서 반복적으로 나온 활용

1088
00:46:42,650 --> 00:46:46,310
사례 중 하나는 Molmo를 드디어 로보틱스 응용에

1089
00:46:46,310 --> 00:46:48,010
사용하자는 아이디어였습니다.

1090
00:46:48,010 --> 00:46:50,250
오늘은 로보틱스에 대해 이야기하지

1091
00:46:50,250 --> 00:46:53,210
않겠습니다. 다음 수업에서 배우실 거니까요.

1092
00:46:53,210 --> 00:46:55,330
하지만 사람들이 로보틱스에

1093
00:46:55,330 --> 00:46:59,050
대해 흥분했던 몇 가지 예는 보여드리고 싶습니다.

1094
00:46:59,050 --> 00:47:02,790
많은 사람들이, 심지어 NVIDIA 사람들조차도, 사적인

1095
00:47:02,790 --> 00:47:05,512
모델 개발이 아무리 많아도 오픈

1096
00:47:05,512 --> 00:47:06,970
소스를 절대 무시하면 안

1097
00:47:06,970 --> 00:47:09,270
된다고 이야기하기 시작했습니다.

1098
00:47:09,270 --> 00:47:11,750
결국 오픈 소스 커뮤니티가 따라잡게 되어

1099
00:47:11,750 --> 00:47:14,610
있고, 저희도 그 시점에 따라잡고 있었습니다.

1100
00:47:14,610 --> 00:47:17,490
그래서 저희 모델이 공개되자 메타는 빠르게

1101
00:47:17,490 --> 00:47:22,170
대응해 LLaVA 3.2 모델을 공개했고, 많은 사람들이

1102
00:47:22,170 --> 00:47:26,110
Molmo와 메타의 LLaVA 모델을 비교 평가했습니다.

1103
00:47:26,110 --> 00:47:29,050
그리고 저는 Molmo가 LLaVA보다도 우수한 결과를 낸

1104
00:47:29,050 --> 00:47:30,130
점에 매우 기쁩니다.

1105
00:47:30,130 --> 00:47:32,730
그럼 Molmo가 왜 이렇게 잘 작동하는지 보여드리겠습니다.

1106
00:47:32,730 --> 00:47:36,180
이 모델들이 매우 잘 작동하게 만든 비결은 무엇일까요?

1107
00:47:36,180 --> 00:47:38,860
비결은 모델의 의사결정을 픽셀

1108
00:47:38,860 --> 00:47:41,260
자체에 기반을 둔다는 점입니다.

1109
00:47:41,260 --> 00:47:44,260
보통 모델에 '보트가 몇 대 있나요?'

1110
00:47:44,260 --> 00:47:47,000
같은 질문을 하면 숫자를 대답하는데,

1111
00:47:47,000 --> 00:47:48,740
종종 환각을 일으키죠.

1112
00:47:48,740 --> 00:47:50,380
하지만 저희 모델이 특별한

1113
00:47:50,380 --> 00:47:52,700
점은 실제로 세고 있는 모든 대상에 포인터를

1114
00:47:52,700 --> 00:47:53,740
생성한다는 겁니다.

1115
00:47:53,740 --> 00:47:56,180
즉, 모든 보트에 포인터를

1116
00:47:56,180 --> 00:47:58,620
찍고 최종 숫자를 출력합니다.

1117
00:47:58,620 --> 00:48:02,300
그래서 의사결정이 픽셀 자체에 기반을 둡니다.

1118
00:48:02,300 --> 00:48:05,260
이 덕분에 메타의 LLaVA가 약 60억

1119
00:48:05,260 --> 00:48:08,260
개 이미지-텍스트 쌍으로 학습된 반면,

1120
00:48:08,260 --> 00:48:12,380
저희 모델은 단 70만 개 이미지-텍스트 쌍으로 학습할

1121
00:48:12,380 --> 00:48:13,580
수 있었습니다.

1122
00:48:13,580 --> 00:48:17,580
큰 차이는 저희가 70만 개 이미지-텍스트

1123
00:48:17,580 --> 00:48:20,940
쌍을 직접 손수 선별했다는 점이고, 이것이

1124
00:48:20,940 --> 00:48:23,780
저희가 이 회사들이 만든 모델과

1125
00:48:23,780 --> 00:48:26,460
차별화된 가장 큰 이유입니다.

1126
00:48:26,460 --> 00:48:30,700
현재 많은 분들이 인터넷에서 이미지-텍스트 쌍을 다운로드하려고

1127
00:48:30,700 --> 00:48:32,228
시도하고 있습니다.

1128
00:48:32,228 --> 00:48:34,270
이것이 많은 사람들이 비전 언어

1129
00:48:34,270 --> 00:48:36,010
모델을 학습하는 기반이 되어왔죠.

1130
00:48:36,010 --> 00:48:39,070
이미지와 관련된 텍스트가 있는 인터넷 데이터를

1131
00:48:39,070 --> 00:48:40,450
많이 수집하는 겁니다.

1132
00:48:40,450 --> 00:48:43,110
하지만 인터넷 데이터의 문제는 그것이 우연적이라는 점입니다.

1133
00:48:43,110 --> 00:48:45,470
이미지에 종종 연결된 텍스트는

1134
00:48:45,470 --> 00:48:47,670
주관적이거나 업로더가 이미지에 대해

1135
00:48:47,670 --> 00:48:50,310
느낀 감정을 설명하는 경우가 많습니다.

1136
00:48:50,310 --> 00:48:52,550
실제로 이미지 내용 자체를

1137
00:48:52,550 --> 00:48:54,070
설명하는 경우는 드뭅니다.

1138
00:48:54,070 --> 00:48:57,230
반면, 저희 데이터는 이렇게 생겼습니다.

1139
00:48:57,230 --> 00:49:01,110
단일 이미지에 대해 그 이미지의 실제 내용에

1140
00:49:01,110 --> 00:49:03,650
대한 밀도 높은 설명이 있습니다.

1141
00:49:03,650 --> 00:49:05,150
그리고 인터넷에서는 사람들이 절대

1142
00:49:05,150 --> 00:49:06,690
이야기하지 않는 내용들도 포함되어 있습니다.

1143
00:49:06,690 --> 00:49:09,550
시각 세계에 관한 수많은 작업과 지식이 있지만 우리는

1144
00:49:09,550 --> 00:49:11,370
그것에 대해 전혀 이야기하지 않습니다.

1145
00:49:11,370 --> 00:49:12,870
무언가가 다른 것의 왼쪽에 있다고

1146
00:49:12,870 --> 00:49:14,410
절대 말하지 않을 겁니다,

1147
00:49:14,410 --> 00:49:16,930
왜냐하면 우리에게는 그게 자연스럽지 않기 때문입니다.

1148
00:49:16,930 --> 00:49:18,470
무언가가 다른 것의 왼쪽에 있다는

1149
00:49:18,470 --> 00:49:19,730
것은 너무나도 명백합니다.

1150
00:49:19,730 --> 00:49:21,850
왜 그런 정보를 굳이 전달하겠습니까?

1151
00:49:21,850 --> 00:49:23,225
그래서 우리는 사람들에게서 그런

1152
00:49:23,225 --> 00:49:24,950
종류의 정보를 끌어내기 시작했습니다.

1153
00:49:24,950 --> 00:49:27,117
우리는 사람들이 사물의 크기가

1154
00:49:27,117 --> 00:49:29,470
크다거나 직사각형 모양이라는

1155
00:49:29,470 --> 00:49:31,710
식으로 말하게 했습니다.

1156
00:49:31,710 --> 00:49:34,550
우리는 광택이 있다거나 풍부한 재질 같은

1157
00:49:34,550 --> 00:49:36,630
소재, 그리고 이미지 전체에서

1158
00:49:36,630 --> 00:49:40,350
수평면을 가로지르는 위치 같은 것도 이야기했습니다.

1159
00:49:40,350 --> 00:49:42,350
이 모든 정보가 바로 이런 모델들을

1160
00:49:42,350 --> 00:49:44,650
더 성능 좋게 만드는 요소입니다.

1161
00:49:44,650 --> 00:49:46,750
데이터셋에서 또 다른 예를 보여드리겠습니다.

1162
00:49:46,750 --> 00:49:50,110
아주 단순한 휴대폰 화면이나 태블릿 화면 이미지입니다.

1163
00:49:50,110 --> 00:49:53,750
여기에는 인터넷에서는 완전히 빠져 있는 정보가 있습니다,

1164
00:49:53,750 --> 00:49:56,990
사람들이 유용하게 여길 만한 정보들이죠.

1165
00:49:56,990 --> 00:49:58,730
예를 들어 이것은 태블릿 기기라는 것.

1166
00:49:58,730 --> 00:50:02,270
시간은 이렇고, 기기에 남은 배터리 양은

1167
00:50:02,270 --> 00:50:03,000
이렇다는 것.

1168
00:50:03,000 --> 00:50:04,750
이런 정보들이 사람들이 이 모델들을

1169
00:50:04,750 --> 00:50:06,398
더 잘 사용할 수 있게 도와줍니다.

1170
00:50:06,398 --> 00:50:08,190
하지만 이런 정보는 인터넷에서

1171
00:50:08,190 --> 00:50:10,630
절대 이야기하지 않는 종류의 정보입니다.

1172
00:50:10,630 --> 00:50:12,370
그래서 이런 정보를 얻기 위해

1173
00:50:12,370 --> 00:50:14,370
우리는 다양한 질문들을 설계했습니다.

1174
00:50:14,370 --> 00:50:17,030
우리는 인터넷에 부족한 올바른 정보나

1175
00:50:17,030 --> 00:50:19,710
조각이 무엇인지 알아내고, 그것을

1176
00:50:19,710 --> 00:50:22,210
최대한 효과적으로 이끌어내는 방법을

1177
00:50:22,210 --> 00:50:25,710
찾기 위해 2년간 다양한 유도 연구를 진행했습니다.

1178
00:50:25,710 --> 00:50:27,480
매우 중요한 점 중

1179
00:50:27,480 --> 00:50:31,860
하나는 모든 주석자들이 설명을 타이핑하지 않고,

1180
00:50:31,860 --> 00:50:34,720
설명에 대해 말하도록 했다는

1181
00:50:34,720 --> 00:50:39,760
겁니다. 말하기는 자동으로 많은 고정관념을 깨뜨립니다. [Gricean ?]
격률들입니다.

1182
00:50:39,760 --> 00:50:41,808
사람들이 말하도록 함으로써,

1183
00:50:41,808 --> 00:50:43,600
그들이 평소에는 절대 타이핑하지

1184
00:50:43,600 --> 00:50:46,250
않을 것들에 대해 말하게 만들었습니다.

1185
00:50:46,250 --> 00:50:48,500
모델 자체는 LLaVA와 크게 다르지 않았습니다.

1186
00:50:48,500 --> 00:50:51,900
CLIP과 코딩이 들어오는 동일한 구성을 사용했습니다.

1187
00:50:51,900 --> 00:50:54,320
연결자는 단순한 선형 계층이었고,

1188
00:50:54,320 --> 00:50:56,080
그런 다음 대형 언어

1189
00:50:56,080 --> 00:50:59,320
모델이 모든 토큰을 받아서 원하는 어떤

1190
00:50:59,320 --> 00:51:00,500
것도 출력했습니다.

1191
00:51:00,500 --> 00:51:03,660
그래서 모델 자체는 기존 모델과 매우 비슷해 보였습니다.

1192
00:51:03,660 --> 00:51:05,480
가장 큰 차이는 데이터와

1193
00:51:05,480 --> 00:51:08,400
데이터 자체의 품질 및 밀도에 있었습니다.

1194
00:51:08,400 --> 00:51:11,220
그리고 모델이 이미지 자체에 근거하여 의사결정을

1195
00:51:11,220 --> 00:51:12,720
하는 근거 능력

1196
00:51:12,720 --> 00:51:14,920
덕분에, Malmo는 다른 어떤 모델도

1197
00:51:14,920 --> 00:51:16,560
할 수 없는 일을 할

1198
00:51:16,560 --> 00:51:19,080
수 있었습니다; 예를 들어 메뉴를

1199
00:51:19,080 --> 00:51:20,500
가리키는 것 같은 일이죠.

1200
00:51:20,500 --> 00:51:22,960
실제로 메뉴 항목이 어디에 있는지 알려주거나,

1201
00:51:22,960 --> 00:51:26,370
검색 옵션을 설정할 수 있는 위치를 가리키라고

1202
00:51:26,370 --> 00:51:27,970
하면, '여기가 옵션을 설정할

1203
00:51:27,970 --> 00:51:31,490
수 있는 곳입니다'라고 보여줍니다. 또는 중간 크기

1204
00:51:31,490 --> 00:51:33,530
데이터 세트가 어디 있는지 가리키고,

1205
00:51:33,530 --> 00:51:35,670
이동해야 할 옵션을 알려줍니다.

1206
00:51:35,670 --> 00:51:36,170
이미

1207
00:51:36,170 --> 00:51:38,550
카운트를 가리킬 수 있다는 것을

1208
00:51:38,550 --> 00:51:41,870
보여드렸지만, 더 세밀한 작업도 할 수 있습니다. 예를

1209
00:51:41,870 --> 00:51:45,410
들어 이 버스의 노선 번호가 무엇인지 물어볼 수 있죠.

1210
00:51:45,410 --> 00:51:47,550
Molmo는 단순히 답을 주는 것이 아니라,

1211
00:51:47,550 --> 00:51:49,810
이미지 내 어디를 가리키는지 보여줍니다.

1212
00:51:49,810 --> 00:51:51,690
이 경우, 바로 이 영역에

1213
00:51:51,690 --> 00:51:54,170
버스 번호가 포함되어 있고,

1214
00:51:54,170 --> 00:51:56,050
그 번호를 반환합니다.

1215
00:51:56,050 --> 00:51:57,970
왼쪽에 있는 차와 오른쪽에 있는 차가

1216
00:51:57,970 --> 00:52:00,630
각각 몇 대인지 추론해 달라고 요청할 수도 있습니다.

1217
00:52:00,630 --> 00:52:04,070
깊이 이미지나 항공 이미지, 심지어 매우 혼잡한

1218
00:52:04,070 --> 00:52:08,445
장면이나 스포츠 구역에 대해서도 추론하도록 요청할 수 있습니다.

1219
00:52:08,445 --> 00:52:10,070
또한 정말 흥미로운 점은,

1220
00:52:10,070 --> 00:52:11,695
잠시 후에 더 자세히

1221
00:52:11,695 --> 00:52:13,650
이야기하겠지만, 오늘날 멀티모달

1222
00:52:13,650 --> 00:52:16,670
모델 전반에서 계속 등장하는 체이닝 개념입니다.

1223
00:52:16,670 --> 00:52:20,250
Molmo를 다른 모델과 연결하는 아이디어인데, Molmo의

1224
00:52:20,250 --> 00:52:22,930
출력을 SAM 2 같은 다른 모델의 입력으로

1225
00:52:22,930 --> 00:52:25,500
연결할 수 있습니다. 그래서 Molmo에게

1226
00:52:25,500 --> 00:52:28,280
크리켓 배트를 가리키라고 할 수 있습니다.

1227
00:52:28,280 --> 00:52:31,580
그 다음 그 지점을 받아서 세그멘테이션을 수행하는 SAM

1228
00:52:31,580 --> 00:52:33,700
2 같은 모델에 입력하면, 시간에

1229
00:52:33,700 --> 00:52:36,700
따른 크리켓 배트의 세그멘테이션을 할 수 있습니다.

1230
00:52:36,700 --> 00:52:39,620
이렇게 해서 다양한 새로운 응용 프로그램을 가능하게 할 수 있습니다.

1231
00:52:39,620 --> 00:52:43,318
사무실에서 실험해본 예가 있는데, 다음

1232
00:52:43,318 --> 00:52:44,860
강의에서 로보틱스에

1233
00:52:44,860 --> 00:52:48,700
대해 배우실 때 아마도 알게 될 겁니다.

1234
00:52:48,700 --> 00:52:52,160
Molmo에게 물병이 어디 있는지 가리키라고 했습니다.

1235
00:52:52,160 --> 00:52:55,380
그 다음 간단한 모션 플래너를 사용해 로봇을 그 물병

1236
00:52:55,380 --> 00:52:56,700
쪽으로 이동시켰습니다.

1237
00:52:56,700 --> 00:52:59,420
다음으로, 그 물병을 더러운 그릇이 있는

1238
00:52:59,420 --> 00:53:01,140
곳으로 옮기라고 했습니다.

1239
00:53:01,140 --> 00:53:02,460
그릇, 아니 싱크대를

1240
00:53:02,460 --> 00:53:05,460
가리키고 로봇을 그곳으로 이동시켰습니다.

1241
00:53:05,460 --> 00:53:07,300
그리고 싱크대 내의 빈

1242
00:53:07,300 --> 00:53:09,540
공간을 가리켜 그 위치에

1243
00:53:09,540 --> 00:53:11,460
물병을 놓으라고 했습니다.

1244
00:53:11,460 --> 00:53:15,100
이처럼 이제 이 모든 기능을 결합하고

1245
00:53:15,100 --> 00:53:19,460
체이닝해서 로보틱스 응용을 많이 자동화할 수 있습니다.

1246
00:53:19,460 --> 00:53:21,660
제 연구실에서는 지금 이 시각-언어

1247
00:53:21,660 --> 00:53:24,070
모델들을 실제 물리 영역에

1248
00:53:24,070 --> 00:53:26,150
적응시키고 일반화를 가능하게 하는

1249
00:53:26,150 --> 00:53:28,750
데 많은 노력을 기울이고 있습니다.

1250
00:53:28,750 --> 00:53:31,590
질문은, 이미지 해상도를 항상

1251
00:53:31,590 --> 00:53:34,950
고정된 해상도로 바꾸면 이 모델들이

1252
00:53:34,950 --> 00:53:38,830
제대로 가리킬 수 있느냐는 것입니다.

1253
00:53:38,830 --> 00:53:41,030
사실 요즘은 이 모델들을 어떤

1254
00:53:41,030 --> 00:53:43,750
해상도에도 적응시킬 수 있습니다.

1255
00:53:43,750 --> 00:53:47,910
[INAUDIBLE] 같은 메커니즘이

1256
00:53:47,910 --> 00:53:52,350
도입되어 가변 크기 이미지 입력을 허용하고, 그

1257
00:53:52,350 --> 00:53:57,230
새로운 공간에서 가리키도록 적응할 수 있습니다.

1258
00:53:57,230 --> 00:53:59,430
그래서 모델의 위치 임베딩은

1259
00:53:59,430 --> 00:54:03,710
이미지 크기에 따라 기본적으로 달라지고, 모델은

1260
00:54:03,710 --> 00:54:07,230
보통 잘 일반화하는 경향이 있습니다.

1261
00:54:07,230 --> 00:54:10,510
그래서 비전과 멀티모달 모델을 함께 추가하는

1262
00:54:10,510 --> 00:54:12,270
것에 대한 대화였습니다.

1263
00:54:12,270 --> 00:54:14,467
남은 20분 동안은 이 파운데이션

1264
00:54:14,467 --> 00:54:16,550
모델들을 이미지 분류와

1265
00:54:16,550 --> 00:54:19,690
텍스트뿐만 아니라, 관심 있는 어떤 출력 공간에도

1266
00:54:19,690 --> 00:54:22,120
일반화할 수 있도록 하는 것에

1267
00:54:22,120 --> 00:54:23,720
대해 이야기하고 싶습니다.

1268
00:54:23,720 --> 00:54:25,320
이 분야에서 정말 인기를 끌고

1269
00:54:25,320 --> 00:54:29,440
있는 모델 중 하나가 바로 Segment Anything Model입니다.

1270
00:54:29,440 --> 00:54:31,480
Segment Anything

1271
00:54:31,480 --> 00:54:34,320
Model, 줄여서 SAM은

1272
00:54:34,320 --> 00:54:37,000
모든 종류의 분할 작업을 위한

1273
00:54:37,000 --> 00:54:40,360
파운데이션 모델을 구축하려고 시도합니다.

1274
00:54:40,360 --> 00:54:41,920
즉, 누구나

1275
00:54:41,920 --> 00:54:45,720
이미지에서 관심 있는 부분을 가리키면,

1276
00:54:45,720 --> 00:54:48,080
모델이 그 부분에

1277
00:54:48,080 --> 00:54:53,360
대한 마스크를 출력할 수 있게 하려는 겁니다.

1278
00:54:53,360 --> 00:54:56,000
예를 들어, 고정된 카테고리 수를 넘어서

1279
00:54:56,000 --> 00:54:58,560
사용자가 관심 있는 어떤 카테고리에도

1280
00:54:58,560 --> 00:55:00,760
일반화할 수 있는 모델을 원하실 겁니다.

1281
00:55:00,760 --> 00:55:03,080
그리고 이상적으로는 사용자가 관심

1282
00:55:03,080 --> 00:55:07,220
있는 어떤 카테고리든 마스크 형태로 출력되길 원하실 겁니다.

1283
00:55:07,220 --> 00:55:09,080
그래서 두 가지 목표가 있습니다.

1284
00:55:09,080 --> 00:55:11,720
어떤 카테고리든, 아주 많은 카테고리에도

1285
00:55:11,720 --> 00:55:12,700
일반화하는 것과,

1286
00:55:12,700 --> 00:55:15,800
그리고 이상적으로는 사용자가 정말로 관심 있는 것을

1287
00:55:15,800 --> 00:55:18,300
아주 구체적으로 출력할 수 있게 하는 것입니다.

1288
00:55:18,300 --> 00:55:19,510
이 두 가지 모두 도전 과제입니다.

1289
00:55:19,510 --> 00:55:20,770
다양한 카테고리를

1290
00:55:20,770 --> 00:55:22,770
아우르는 방대한 데이터를 어떻게

1291
00:55:22,770 --> 00:55:24,870
수집할지, 그리고 사용자가

1292
00:55:24,870 --> 00:55:27,330
정말로 관심 있는 것을 정확히 찾아내는

1293
00:55:27,330 --> 00:55:31,050
아키텍처를 어떻게 설계할지 모두 어려운 문제입니다.

1294
00:55:31,050 --> 00:55:33,930
이제 두 번째 질문부터 시작해 보겠습니다.

1295
00:55:33,930 --> 00:55:38,910
무언가에 대한 마스크를 원할 때 정말 모호할 수 있습니다.

1296
00:55:38,910 --> 00:55:42,850
예를 들어, 이미지에 고양이 두 마리가 있고 사용자가

1297
00:55:42,850 --> 00:55:44,570
'고양이 분할을 원한다'고

1298
00:55:44,570 --> 00:55:47,450
말할 때, 어느 고양이를 원하는지

1299
00:55:47,450 --> 00:55:50,090
명확하지 않은 상황을 상상해 보세요.

1300
00:55:50,090 --> 00:55:52,750
이상적으로는 마스크를 가리키는 기능이 더 많으면, 사용자가

1301
00:55:52,750 --> 00:55:54,990
관심 있는 고양이를 정확히 가리킬 수 있겠죠.

1302
00:55:54,990 --> 00:55:56,490
그리고 포인트에

1303
00:55:56,490 --> 00:55:59,690
따라 중요한 마스크를 생성할 수 있습니다.

1304
00:55:59,690 --> 00:56:01,755
물론, 지금은 이 마스크들이 아주 좋지는 않습니다.

1305
00:56:01,755 --> 00:56:03,130
이상적으로는,

1306
00:56:03,130 --> 00:56:06,290
이런 마스크들이 매우 높은 품질을

1307
00:56:06,290 --> 00:56:09,170
가져야 하며, 이미지 편집이나

1308
00:56:09,170 --> 00:56:11,770
다양한 후속 응용 프로그램을

1309
00:56:11,770 --> 00:56:13,930
지원할 수 있어야 합니다.

1310
00:56:13,930 --> 00:56:16,650
그래서 사용자가 정확히 무엇에

1311
00:56:16,650 --> 00:56:19,280
관심 있는지 지정할 수 있게

1312
00:56:19,280 --> 00:56:22,700
하려면, 단순히 텍스트로 입력하는 것 이상의

1313
00:56:22,700 --> 00:56:24,620
방법이 필요합니다.

1314
00:56:24,620 --> 00:56:28,538
SAM 아키텍처에는 두 가지, 아니 세 가지 구성 요소가

1315
00:56:28,538 --> 00:56:29,080
있습니다.

1316
00:56:29,080 --> 00:56:31,760
이미지 인코더가 있는데, 이것은 다시 말해

1317
00:56:31,760 --> 00:56:33,780
clip 인코더일 수도 있습니다.

1318
00:56:33,780 --> 00:56:36,720
그리고 프롬프트 인코더가 있는데, 이것은 특별한 역할을 합니다.

1319
00:56:36,720 --> 00:56:40,580
이 프롬프트 인코더는 텍스트, 포인트, 바운딩 박스 등

1320
00:56:40,580 --> 00:56:43,180
사용자가 관심 있는 부분을 지정하는 다양한

1321
00:56:43,180 --> 00:56:45,300
방식을 인코딩하려고 합니다.

1322
00:56:45,300 --> 00:56:47,220
그리고 이 두 가지를 받은 후,

1323
00:56:47,220 --> 00:56:49,460
아주 가벼운 디코더를 거쳐

1324
00:56:49,460 --> 00:56:50,640
마스크를 출력합니다.

1325
00:56:50,640 --> 00:56:53,900
디코더는 이 강의에서 이미 본 세그멘테이션

1326
00:56:53,900 --> 00:56:56,900
디코더와 매우 비슷하게 생겼습니다.

1327
00:56:56,900 --> 00:56:59,540
전체적으로 모델은 이렇게 생겼습니다.

1328
00:56:59,540 --> 00:57:03,580
이미지를 받으면 이미지 인코더를 통해 인코딩합니다.

1329
00:57:03,580 --> 00:57:06,100
그리고 여러 가지 프롬프트가

1330
00:57:06,100 --> 00:57:09,340
이미지 인코딩과 상호작용하며

1331
00:57:09,340 --> 00:57:12,040
디코더를 통과해

1332
00:57:12,040 --> 00:57:13,980
마스크를 출력합니다.

1333
00:57:13,980 --> 00:57:16,790
이것이 전체 아키텍처 설계입니다.

1334
00:57:16,790 --> 00:57:20,330
이제 세그멘테이션에서 큰 문제가 하나 있습니다.

1335
00:57:20,330 --> 00:57:23,150
예를 들어 사용자가 특정 위치를 가리키며, '여기

1336
00:57:23,150 --> 00:57:25,270
위치에 대한 세그멘테이션 마스크를 원한다'고

1337
00:57:25,270 --> 00:57:26,477
말한다고 합시다.

1338
00:57:26,477 --> 00:57:28,310
이제 그 세그멘테이션 마스크의

1339
00:57:28,310 --> 00:57:30,530
문제는, 포인트가 있어도

1340
00:57:30,530 --> 00:57:32,830
여전히 모호하다는 겁니다. 그

1341
00:57:32,830 --> 00:57:36,710
포인트가 가위를 전체를 가리키는 것일 수도 있기 때문입니다.

1342
00:57:36,710 --> 00:57:39,790
그 포인트가 잡을 수 있는 부분만 가리키거나,

1343
00:57:39,790 --> 00:57:42,870
잡을 수 있는 부분 중 하나만 가리킬

1344
00:57:42,870 --> 00:57:43,950
수도 있습니다.

1345
00:57:43,950 --> 00:57:47,550
그래서 이 모호함을 해결하는 것은 정말 어렵습니다.

1346
00:57:47,550 --> 00:57:49,270
그리고 모델이 잘못 선택했다고

1347
00:57:49,270 --> 00:57:50,830
해서 벌점을 주고 싶지 않습니다.

1348
00:57:50,830 --> 00:57:52,590
그래서 SAM 아키텍처는 하나의

1349
00:57:52,590 --> 00:57:55,050
세그멘테이션 마스크를 출력하는 대신,

1350
00:57:55,050 --> 00:57:57,150
서로 다른 세분화 수준에서 세 개의

1351
00:57:57,150 --> 00:57:59,070
세그멘테이션 마스크를 출력합니다.

1352
00:57:59,070 --> 00:58:01,350
그리고 그 중에서 가장 실제 정답에

1353
00:58:01,350 --> 00:58:02,990
가까운 마스크를 선택해

1354
00:58:02,990 --> 00:58:06,110
손실을 계산하고, 다른 마스크들은 벌점으로

1355
00:58:06,110 --> 00:58:07,190
처리하지 않습니다.

1356
00:58:07,190 --> 00:58:09,870
그래서 궁극적으로 이 모델은 다양한 종류의

1357
00:58:09,870 --> 00:58:12,130
마스크를 출력하는 법을 배우고,

1358
00:58:12,130 --> 00:58:14,120
사용자가 자신의 용도에

1359
00:58:14,120 --> 00:58:17,280
가장 적합한 마스크를 선택할 수 있게 되는 거죠.

1360
00:58:17,280 --> 00:58:19,860
이 모든 것을 합치면,

1361
00:58:19,860 --> 00:58:22,860
이제 필요한 것은 데이터뿐입니다.

1362
00:58:22,860 --> 00:58:25,720
다양한 카테고리에 걸쳐 많은 데이터가

1363
00:58:25,720 --> 00:58:28,000
있어야 이 모델이 가능해집니다.

1364
00:58:28,000 --> 00:58:30,880
문제는, 이 모델이 2023년에

1365
00:58:30,880 --> 00:58:34,960
나올 때까지, 그러니까 1년 반에서 2년 전까지는,

1366
00:58:34,960 --> 00:58:38,120
대부분의 세그멘테이션 데이터셋이

1367
00:58:38,120 --> 00:58:39,840
매우 작았다는 겁니다.

1368
00:58:39,840 --> 00:58:43,365
이 논문의 저자들은 기존 세그멘테이션

1369
00:58:43,365 --> 00:58:45,740
데이터셋의 이미지 수를 약

1370
00:58:45,740 --> 00:58:49,000
6배, 세그멘테이션 마스크 수를 약

1371
00:58:49,000 --> 00:58:51,040
400배로 늘렸습니다.

1372
00:58:51,040 --> 00:58:55,080
그래서 이 모델이 최대한 성능을 낼 수 있도록 많은

1373
00:58:55,080 --> 00:58:57,560
마스크를 수집하고 크게 확장한 거죠.

1374
00:58:57,560 --> 00:58:59,800
다시 말하지만, 이 메시지는 Flamingo나

1375
00:58:59,800 --> 00:59:02,280
Molmo에서 했던 메시지와 매우 비슷합니다.

1376
00:59:02,280 --> 00:59:04,460
즉, 정말 정말 좋은 고품질

1377
00:59:04,460 --> 00:59:07,240
데이터가 있어야 이 모델들이 최대한 성능을

1378
00:59:07,240 --> 00:59:08,860
낼 수 있다는 겁니다.

1379
00:59:08,860 --> 00:59:12,050
그리고 많은 비전 작업에서는 인터넷에 데이터가

1380
00:59:12,050 --> 00:59:15,210
전혀 없기 때문에, 직접 나가서 데이터를 찾아

1381
00:59:15,210 --> 00:59:18,330
수집해야 이 모델들이 잘 작동할 수 있습니다.

1382
00:59:18,330 --> 00:59:21,250
그래서 이 데이터를 만들기 위해, 그들은

1383
00:59:21,250 --> 00:59:23,450
루프 내 프로세스를 만들었는데,

1384
00:59:23,450 --> 00:59:27,010
처음에는 어느 정도의 데이터에 주석을 달았습니 다.

1385
00:59:27,010 --> 00:59:28,490
그 주석에서 훈련 데이터 세트를

1386
00:59:28,490 --> 00:59:30,430
만들고, 모델을 훈련시켰으며, 그 모델을

1387
00:59:30,430 --> 00:59:32,730
사용해 더 많은 데이터에 주석을 달았습니다.

1388
00:59:32,730 --> 00:59:34,690
그리고 나서 사용자들을 이용해

1389
00:59:34,690 --> 00:59:37,650
모델이 생성한 세그먼트를 반복적으로 다듬으며

1390
00:59:37,650 --> 00:59:39,450
이 과정을 계속했습니다.

1391
00:59:39,450 --> 00:59:41,730
그래서 그들은 루프 내 인간-모델

1392
00:59:41,730 --> 00:59:45,210
프로세스를 갖고 있었는데, 모델이 세그먼트를 제안하고 인간

1393
00:59:45,210 --> 00:59:47,890
주석자가 그 세그먼트를 수정하는 방식입니다.

1394
00:59:47,890 --> 00:59:50,750
이것이 그들의 데이터 세트에서 나온 예시 이미지입니다.

1395
00:59:50,750 --> 00:59:52,590
카테고리가 꽤 많습니다.

1396
00:59:52,590 --> 00:59:57,130
여기 있는 각 개별 채소마다 고유한 마스크로 주석이 달려 있습니다.

1397
00:59:57,130 --> 00:59:59,570
그래서 수집하는 데 비용이 많이 듭니다.

1398
00:59:59,570 --> 01:00:04,770
그리고 수백만 장의 이미지에 걸쳐 이 작업을 했습니다.

1399
01:00:04,770 --> 01:00:07,850
여기 또 다른 예시가 있는데, 다시 모든 우산 하나하나에

1400
01:00:07,850 --> 01:00:09,290
주석이 달려 있습니다.

1401
01:00:09,290 --> 01:00:11,560
또 다른 예시로는 수중 이미지도 있습니다.

1402
01:00:11,560 --> 01:00:14,080
그리고 물론, 그림도 있습니다.

1403
01:00:14,080 --> 01:00:16,380
그림에도 세그먼테이션이 되어 있습니다.

1404
01:00:16,380 --> 01:00:18,140
이 모든 것이 합쳐져서

1405
01:00:18,140 --> 01:00:20,820
세그먼테이션을 위한 이 기초 모델을

1406
01:00:20,820 --> 01:00:24,028
만드는 데 정말 기반이 되었습니다, 알겠죠?

1407
01:00:24,028 --> 01:00:25,320
이것이 Segment Anything에 관한 내용입니다.

1408
01:00:25,320 --> 01:00:27,153
그리고 오늘 남은

1409
01:00:27,153 --> 01:00:29,380
몇 분 동안은 멀티모달

1410
01:00:29,380 --> 01:00:34,100
언어 모델의 마지막 부분인 체이닝에 정말 집중하고

1411
01:00:34,100 --> 01:00:35,100
싶습니다.

1412
01:00:35,100 --> 01:00:38,880
체이닝의 아이디어는 여러분이 이미 본 적이 있는 것입니다.

1413
01:00:38,880 --> 01:00:41,370
이 강의 내내 힌트를 드렸습니다.

1414
01:00:41,370 --> 01:00:43,620
그리고 아이디어는 서로 다른 모델들을

1415
01:00:43,620 --> 01:00:47,420
결합해서 단일 모델이 혼자서는 할 수 없는 일을 가능하게 하는 것입니다.

1416
01:00:47,420 --> 01:00:50,660
우리 수업에서 할 수 있는 재미있는 작은 연습이 있습니다.

1417
01:00:50,660 --> 01:00:53,740
네 개의 이미지를 드리고 네 개의

1418
01:00:53,740 --> 01:00:55,860
카테고리도 드리겠습니다.

1419
01:00:55,860 --> 01:00:59,020
이 카테고리들은 여러분 중 일부가 한 번도 본 적

1420
01:00:59,020 --> 01:01:00,540
없는 것일 수도 있습니다.

1421
01:01:00,540 --> 01:01:03,460
그리고 CLIP도 본 적 없는 카테고리들이 있습니다.

1422
01:01:03,460 --> 01:01:05,540
그래서 CLIP은 이 카테고리들에 대해

1423
01:01:05,540 --> 01:01:07,123
실패하는데, 어떤 것이 무엇과

1424
01:01:07,123 --> 01:01:09,110
연관되어 있는지 전혀 모르기 때문입니다.

1425
01:01:09,110 --> 01:01:12,190
여기 계신 분 중에 어떤 것이 무엇인지 아시는 분 있나요?

1426
01:01:12,190 --> 01:01:13,930
마림바요, 네, 두 번째 것이 마림바입니다.

1427
01:01:13,930 --> 01:01:14,472
맞습니다.

1428
01:01:14,472 --> 01:01:16,250
조금 쉬운 것도 있죠.

1429
01:01:16,250 --> 01:01:17,750
바이아덕트요.

1430
01:01:17,750 --> 01:01:21,630
네, 많은 분들이 어떤 것인지 아실 겁니다.

1431
01:01:21,630 --> 01:01:25,390
그런데, 개가 어느 것이고 새가 어느 것인지 아시나요?

1432
01:01:25,390 --> 01:01:28,430
이 대신에 이것들을 드리면 어떨까요?

1433
01:01:28,430 --> 01:01:31,322
이제 제가 이 물건들에 대한 설명을

1434
01:01:31,322 --> 01:01:33,030
드리면, 각 물건을

1435
01:01:33,030 --> 01:01:36,276
올바른 카테고리와 쉽게 연결할 수 있습니다.

1436
01:01:36,276 --> 01:01:38,830
이것이 바로 chaining의 기본

1437
01:01:38,830 --> 01:01:41,290
아이디어인데, CLIP이 이 이미지들을 본

1438
01:01:41,290 --> 01:01:44,150
적이 없더라도, 인터넷에서 어느 정도 이

1439
01:01:44,150 --> 01:01:46,030
개념들이 언급되었을 가능성이

1440
01:01:46,030 --> 01:01:49,870
높고, GPT가 이를 설명할 수 있을 가능성이 있다는 겁니다.

1441
01:01:49,870 --> 01:01:52,350
그리고 GPT가 그런 설명을 만들어낼 수

1442
01:01:52,350 --> 01:01:54,750
있다면, 그 설명들이 바로 어떤

1443
01:01:54,750 --> 01:01:58,150
카테고리가 무엇인지 정확히 분류하는 좋은 방법이 됩니다.

1444
01:01:58,150 --> 01:01:59,730
chaining의

1445
01:01:59,730 --> 01:02:01,690
아이디어는 한 모델의 강점을

1446
01:02:01,690 --> 01:02:03,890
다른 모델의 능력과

1447
01:02:03,890 --> 01:02:06,230
결합해서, 이전에는 없던 새로운

1448
01:02:06,230 --> 01:02:07,887
능력을 얻는 것입니다.

1449
01:02:07,887 --> 01:02:09,720
그래서 CLIP에 훈련 데이터가

1450
01:02:09,720 --> 01:02:12,240
전혀 없는 수많은 카테고리를 다룰 수 있습니다.

1451
01:02:12,240 --> 01:02:14,080
하지만 모든 카테고리를 설명할 수

1452
01:02:14,080 --> 01:02:16,500
있다면, CLIP이 많은 설명을 본 경험 덕분에

1453
01:02:16,500 --> 01:02:19,080
이제 모든 카테고리를 아주 잘 분류할 수 있게 됩니다.

1454
01:02:19,080 --> 01:02:21,600
그리고 개별 꽃, 개별 자동차,

1455
01:02:21,600 --> 01:02:24,360
개별 공간, 심지어 다양한 종류의

1456
01:02:24,360 --> 01:02:28,280
반려동물에 대한 분류를 CLIP으로 생성할 수 있습니다.

1457
01:02:28,280 --> 01:02:30,080
그리고 더 세분화되고

1458
01:02:30,080 --> 01:02:32,000
전문화된 카테고리에 관한 여러

1459
01:02:32,000 --> 01:02:35,258
데이터셋에서 이런 향상을 확인할 수 있습니다.

1460
01:02:35,258 --> 01:02:36,800
이것이 가능한 유일한

1461
01:02:36,800 --> 01:02:39,680
이유는 GPT가 그런 것들을 설명할 수

1462
01:02:39,680 --> 01:02:42,600
있는 능력을 어느 정도 습득했기 때문입니다.

1463
01:02:42,600 --> 01:02:45,600
새로운 능력을 일반화할 수 있다는

1464
01:02:45,600 --> 01:02:49,180
이 아이디어는 작년에 매우 인기가 있었고,

1465
01:02:49,180 --> 01:02:52,520
올해도 여전히 인기가 많습니다.

1466
01:02:52,520 --> 01:02:56,280
그리고 어떤 질문에 대해서도 chaining을

1467
01:02:56,280 --> 01:02:58,960
통해 해결할 수 있습니다.

1468
01:02:58,960 --> 01:03:01,640
예를 들어, 제가 '배에 세

1469
01:03:01,640 --> 01:03:04,120
사람이 있나요?'라고 물으면,

1470
01:03:04,120 --> 01:03:07,480
이 질문에 답하기 위해 멀티모달 언어

1471
01:03:07,480 --> 01:03:09,940
모델에 물어볼 수도 있고,

1472
01:03:09,940 --> 01:03:13,360
지난 수십 년간 개발된 수백 개의 전문화된

1473
01:03:13,360 --> 01:03:15,440
비전 모델을 모두 사용할

1474
01:03:15,440 --> 01:03:17,062
수도 있습니다.

1475
01:03:17,062 --> 01:03:19,520
수업에서 배운 객체 탐지 모델들이

1476
01:03:19,520 --> 01:03:20,300
있습니다.

1477
01:03:20,300 --> 01:03:21,800
객체 탐지기를 사용하면 세

1478
01:03:21,800 --> 01:03:24,133
사람 각각에 대한 탐지를 얻을 수 있습니다.

1479
01:03:24,133 --> 01:03:25,800
그럼 탐지가 세 개 있으니, '사람이

1480
01:03:25,800 --> 01:03:27,640
3명입니다'라고 말할 수 있겠죠.

1481
01:03:27,640 --> 01:03:32,800
일반적인 아이디어는 다른 모델들의 출력을 함께 학습시켜서 새로운

1482
01:03:32,800 --> 01:03:35,720
기능을 수행할 수 있다는 겁니다.

1483
01:03:35,720 --> 01:03:37,320
여기 또 다른 예가 있습니다.

1484
01:03:37,320 --> 01:03:39,200
이 두 배에 총 몇

1485
01:03:39,200 --> 01:03:42,518
명이 있는지 물어본다면, 여섯 명인가요?

1486
01:03:42,518 --> 01:03:44,060
그리고 마찬가지로 같은 방법을 사용할 수 있습니다.

1487
01:03:44,060 --> 01:03:47,120
이미지 1에서 객체 탐지를 하고 이미지 2에서

1488
01:03:47,120 --> 01:03:49,000
객체 탐지를 한 다음, 그

1489
01:03:49,000 --> 01:03:52,260
결과들을 모두 더하는 프로그램을 작성할 수 있습니다.

1490
01:03:52,260 --> 01:03:57,933
이것이 지금 우리가 체이닝(chaining)이라고 부르는 기본
아이디어입니다.

1491
01:03:57,933 --> 01:03:59,600
이 아이디어는 작년에

1492
01:03:59,600 --> 01:04:03,370
Best Paper 상을 받은 VisProg라는 논문으로

1493
01:04:03,370 --> 01:04:06,930
대중화되었습니다. 이 VisProg 논문, 즉 비주얼

1494
01:04:06,930 --> 01:04:10,810
프로그래밍 논문에서는 어떤 이미지나 질문이 주어지면

1495
01:04:10,810 --> 01:04:13,090
프로그램을 생성하는 개념이었습니다.

1496
01:04:13,090 --> 01:04:16,530
이미지 1에 대해 답하고, 이미지 2에

1497
01:04:16,530 --> 01:04:19,290
대해 답한 다음, 그 답들을 합쳐

1498
01:04:19,290 --> 01:04:22,890
최종 답을 내는 프로그램을 생성하는 거죠.

1499
01:04:22,890 --> 01:04:25,230
파이썬 함수로 작성하는데,

1500
01:04:25,230 --> 01:04:27,230
그 함수 안에서

1501
01:04:27,230 --> 01:04:30,170
이미 학습된 다른 모델들을

1502
01:04:30,170 --> 01:04:32,290
개별적으로 호출합니다.

1503
01:04:32,290 --> 01:04:35,890
예를 들어, 이 질문을 할 때, 왼쪽과 오른쪽

1504
01:04:35,890 --> 01:04:37,742
이미지에 총 여섯

1505
01:04:37,742 --> 01:04:39,450
명과 두 척의 배가

1506
01:04:39,450 --> 01:04:41,610
있는지 여부를 판단하는 거죠.

1507
01:04:41,610 --> 01:04:44,210
GPT에게 이 질문에 답하는

1508
01:04:44,210 --> 01:04:47,030
프로그램을 실제로 만들어 보라고 할

1509
01:04:47,030 --> 01:04:51,610
수 있고, 그 프로그램의 답을 받아올 수 있습니다.

1510
01:04:51,610 --> 01:04:55,530
또한 GPT에게 다른 함수들을 사용해 생성할 수 있는 프로그램 예시들을
주면서

1511
01:04:55,530 --> 01:04:57,450
문맥 내 예시(in-context

1512
01:04:57,450 --> 01:04:59,670
examples)를 하게 할 수도 있습니다.

1513
01:04:59,670 --> 01:05:02,900
그 결과 GPT가 새로운 질문에도 일반화해서 자신이

1514
01:05:02,900 --> 01:05:05,460
가진 모든 기능을 사용하기 시작하는

1515
01:05:05,460 --> 01:05:06,835
걸 볼 수 있습니다.

1516
01:05:06,835 --> 01:05:08,460
물론, 반드시 해야 할 한 가지는

1517
01:05:08,460 --> 01:05:10,680
함수들 자체를 GPT에게 알려주는 겁니다.

1518
01:05:10,680 --> 01:05:12,260
즉, 다른 모델들로부터

1519
01:05:12,260 --> 01:05:15,280
사용할 수 있는 기능들이 있다고 알려줘야 합니다.

1520
01:05:15,280 --> 01:05:17,600
객체 탐지기를 사용해 위치를 찾을 수 있다는 것도 알려줘야 하죠.

1521
01:05:17,600 --> 01:05:21,380
얼굴 탐지기를 사용해서 얼굴 위치를 찾을 수 있습니다.

1522
01:05:21,380 --> 01:05:24,340
그리고 사람들이 만든 여러 다른 모델들에서

1523
01:05:24,340 --> 01:05:27,760
이런 다양한 기능들을 모두 활용할 수 있습니다.

1524
01:05:27,760 --> 01:05:30,800
그리고 이들을 연결해서 다양한 종류의 작업을 수행할 수 있습니다.

1525
01:05:30,800 --> 01:05:32,800
네, 두 가지 방법이 있습니다.

1526
01:05:32,800 --> 01:05:34,500
하나는 정적인 방법으로,

1527
01:05:34,500 --> 01:05:36,860
가능한 다양한 예시를 많이 주고

1528
01:05:36,860 --> 01:05:38,680
일반화되길 기대하는 방식입니다.

1529
01:05:38,680 --> 01:05:40,840
다른 하나는 이 질문에 대해 어떤

1530
01:05:40,840 --> 01:05:43,860
인컨텍스트 예시들이 가장 좋은지 동적으로

1531
01:05:43,860 --> 01:05:44,900
선택하는 방법입니다.

1532
01:05:44,900 --> 01:05:48,420
그래서 이것을 또 다른 검색 과정으로 보고, 최적의 예시를

1533
01:05:48,420 --> 01:05:50,780
찾아내고, 그 다음 프로그램을

1534
01:05:50,780 --> 01:05:52,540
생성하도록 요청할 수 있습니다.

1535
01:05:52,540 --> 01:05:54,640
이 방법이 훨씬 더 좋은

1536
01:05:54,640 --> 01:05:58,620
성능을 내지만, 좋은 검색 시스템이 있어야 가능합니다.

1537
01:05:58,620 --> 01:06:00,690
네, 많은 계산 자원이 필요합니다.

1538
01:06:00,690 --> 01:06:04,030
GPT를 호출하는 데 필요한 계산이 있고, 이는 API를

1539
01:06:04,030 --> 01:06:05,335
통해 해야 합니다.

1540
01:06:05,335 --> 01:06:07,710
그리고 각각의 개별 모델을

1541
01:06:07,710 --> 01:06:10,790
메모리에 불러와서 순차적으로 실행해야 합니다.

1542
01:06:10,790 --> 01:06:13,630
그래서 실제로 훨씬 더 비용이 많이 들 수 있습니다.

1543
01:06:13,630 --> 01:06:15,770
그래서 사람들이 시도하는 것은 이런 기능들을

1544
01:06:15,770 --> 01:06:17,150
하나의 모델에 증류할

1545
01:06:17,150 --> 01:06:18,670
수 있을지 알아보는 것입니다.

1546
01:06:18,670 --> 01:06:20,430
이것이 2025년

1547
01:06:20,430 --> 01:06:22,550
현재 연구의 큰 부분입니다.

1548
01:06:22,550 --> 01:06:24,570
물론, 사람들이 여전히 이런

1549
01:06:24,570 --> 01:06:27,070
모델들을 효과적으로 연결하는 방법도

1550
01:06:27,070 --> 01:06:28,150
연구하고 있습니다.

1551
01:06:28,150 --> 01:06:29,810
네, 이것을 에이전트라고 생각할 수 있습니다.

1552
01:06:29,810 --> 01:06:30,310
네.

1553
01:06:30,310 --> 01:06:32,570
즉, 에이전트가 이 질문에 대해 어떤

1554
01:06:32,570 --> 01:06:35,030
다른 모델들의 도움이 필요한지 결정하고,

1555
01:06:35,030 --> 01:06:39,282
이들을 어떻게 연결해서 새로운 기능을 만들지 결정하는 겁니다.

1556
01:06:39,282 --> 01:06:40,490
이런 모습입니다.

1557
01:06:40,490 --> 01:06:41,590
네.

1558
01:06:41,590 --> 01:06:44,550
또 다른 예로 이미지 편집을 하고 싶을 때가 있습니다.

1559
01:06:44,550 --> 01:06:47,030
사막의 모래를 푸른 잔디로

1560
01:06:47,030 --> 01:06:48,990
바꾸고 싶을 수도 있죠.

1561
01:06:48,990 --> 01:06:51,923
물론 이미지 편집 모델은 아직 초기 단계입니다.

1562
01:06:51,923 --> 01:06:53,590
그래서 대신

1563
01:06:53,590 --> 01:06:56,850
분할(segmentation) 모델을 호출해서 사막을

1564
01:06:56,850 --> 01:07:01,520
식별하고, 사막 부분의 픽셀만 잔디로 바꾼 다음 합성해서

1565
01:07:01,520 --> 01:07:05,000
새로운 이미지를 만드는 방법을 쓸 수 있습니다.

1566
01:07:05,000 --> 01:07:08,440
이것이 제가 다양한 기능에 대해 이야기하고

1567
01:07:08,440 --> 01:07:10,200
싶었던 전부입니다.

1568
01:07:10,200 --> 01:07:13,520
여기 보시면 파운데이션 모델을 어떻게 생각할지에

1569
01:07:13,520 --> 01:07:16,440
관한 몇 가지 기능들이 있습니다.

1570
01:07:16,440 --> 01:07:18,560
결국 한 가지 작업을

1571
01:07:18,560 --> 01:07:22,120
위해 모델을 훈련시키고, 그

1572
01:07:22,120 --> 01:07:25,640
단일 작업에서 여러 하위 응용

1573
01:07:25,640 --> 01:07:28,560
분야로 일반화하는 능력입니다.

1574
01:07:28,560 --> 01:07:30,520
분류 작업에서 인터넷의 많은

1575
01:07:30,520 --> 01:07:33,360
이미지-텍스트 쌍을 가져와 함께 훈련시켜

1576
01:07:33,360 --> 01:07:36,040
다양한 작업을 수행하는 모델을 만드는

1577
01:07:36,040 --> 01:07:37,920
방법에 대해 이야기했습니다.

1578
01:07:37,920 --> 01:07:39,400
이것은 실제 세계에

1579
01:07:39,400 --> 01:07:41,800
존재하지 않거나 라벨이 없는

1580
01:07:41,800 --> 01:07:44,080
새로운 데이터셋에도 일반화할

1581
01:07:44,080 --> 01:07:45,720
수 있게 해줍니다.

1582
01:07:45,720 --> 01:07:48,040
또한 언어 모델과 결합해 캡션

1583
01:07:48,040 --> 01:07:51,640
생성, 개수 세기, OCR 같은 인컨텍스트 예시 작업을

1584
01:07:51,640 --> 01:07:53,360
훈련시킬 수 있습니다.

1585
01:07:53,360 --> 01:07:55,280
이것들도 여러 응용 분야를

1586
01:07:55,280 --> 01:07:57,010
가능하게 하는 기능들입니다.

1587
01:07:57,010 --> 01:07:58,890
그리고 물론 출력이 항상 언어나

1588
01:07:58,890 --> 01:08:00,510
카테고리일 필요는 없습니다.

1589
01:08:00,510 --> 01:08:02,610
이것들은 세분화 마스크일 수도 있는데,

1590
01:08:02,610 --> 01:08:04,810
사용자의 다양한 입력에 따라 여러

1591
01:08:04,810 --> 01:08:06,850
종류의 마스크를 선택할 수 있습니다.

1592
01:08:06,850 --> 01:08:09,650
그리고 이러한 여러 기초 모델이나 더 작은

1593
01:08:09,650 --> 01:08:11,850
모델들을 프로그램을 통해 결합해서

1594
01:08:11,850 --> 01:08:16,450
훨씬 더 일반화할 수 있고, 새로운 다양한 작업을 할 수 있습니다.

1595
01:08:16,450 --> 01:08:20,490
그래서 환각 현상은 여전히 전반적으로 발생합니다.

1596
01:08:20,490 --> 01:08:22,450
우리가 보여드리는 것은,

1597
01:08:22,450 --> 01:08:25,090
포인팅이 생성물에 대한 근거를 찾아야 하기

1598
01:08:25,090 --> 01:08:27,569
때문에 환각 현상을 꽤 줄여주는 것처럼

1599
01:08:27,569 --> 01:08:29,021
보인다는 점입니다.

1600
01:08:29,021 --> 01:08:30,729
하지만 그렇다고 해서 반드시

1601
01:08:30,729 --> 01:08:33,649
올바른 대상을 가리킨다는 보장은 없습니다.

1602
01:08:33,649 --> 01:08:35,790
그래서 이를 해결하는 방법은 여러 가지가 있습니다.

1603
01:08:35,790 --> 01:08:38,850
첫 번째는 물론, 하고자 하는 추론과 관련된

1604
01:08:38,850 --> 01:08:41,490
데이터를 더 많이 수집하는 것입니다.

1605
01:08:41,490 --> 01:08:45,130
하지만 더 좋은 방법은 포인트를 기반으로 출력물이

1606
01:08:45,130 --> 01:08:48,410
신뢰할 만한지 검증하는 검증 방법을

1607
01:08:48,410 --> 01:08:49,545
갖추는 것입니다.

1608
01:08:49,545 --> 01:08:51,670
그래서 많은 대형

1609
01:08:51,670 --> 01:08:55,660
모델과 대기업들은 보통 모델 하나만으로

1610
01:08:55,660 --> 01:08:58,240
출력을 생성하지 않습니다.

1611
01:08:58,240 --> 01:09:01,540
출력을 받은 후에 사용자에게 전달하기 전에

1612
01:09:01,540 --> 01:09:03,700
다른 검증기를 거치게 합니다.

1613
01:09:03,700 --> 01:09:06,060
이렇게 하면 이런 문제들을 어느 정도 완화할 수 있습니다.

1614
01:09:06,060 --> 01:09:08,680
하지만 이 문제는 여전히 활발히 연구 중인 분야입니다.

1615
01:09:08,680 --> 01:09:11,700
어떻게 하면 환각 현상을 줄이고 모델의 실제

1616
01:09:11,700 --> 01:09:13,420
정확도를 높일 수 있을지 말이죠.

1617
01:09:13,420 --> 01:09:14,380
네.

1618
01:09:14,380 --> 01:09:17,380
질문을 다시 말씀드리면, 모델이 자신에게

1619
01:09:17,380 --> 01:09:21,220
없는 도구가 필요한 기능을 수행할 때 새로운 도구를

1620
01:09:21,220 --> 01:09:22,859
만들 수 있느냐는 거죠?

1621
01:09:22,859 --> 01:09:24,580
네, 가능합니다.

1622
01:09:24,580 --> 01:09:28,300
그 방향으로 몇 가지 예비 실험도 진행

1623
01:09:28,300 --> 01:09:29,840
중인데, 모델에게

1624
01:09:29,840 --> 01:09:31,843
원하는 기능을 알려주면,

1625
01:09:31,843 --> 01:09:33,260
특정 사용 사례에

1626
01:09:33,260 --> 01:09:35,979
맞는 도구를 자동으로 만들기 위해 학습

1627
01:09:35,979 --> 01:09:39,260
데이터를 수집하는 시스템을 구축할 수 있습니다.

1628
01:09:39,260 --> 01:09:42,180
하지만 이 연구 분야는 아직 초기 단계입니다.

1629
01:09:42,180 --> 01:09:44,300
우리가 적극적으로 연구하고 있는 분야이고,

1630
01:09:44,300 --> 01:09:48,829
많은 분들이 그 방향에 대해 기대하고 있습니다.
