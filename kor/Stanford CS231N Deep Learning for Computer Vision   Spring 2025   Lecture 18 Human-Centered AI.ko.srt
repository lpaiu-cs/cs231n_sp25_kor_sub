1
00:00:05,210 --> 00:00:10,410
CS231N 이번 학기 마지막 강의에 오신 것을 환영합니다.

2
00:00:10,410 --> 00:00:14,150
처음과 끝에 여러분을 뵙게 되어

3
00:00:14,150 --> 00:00:15,630
정말 반가웠습니다.

4
00:00:15,630 --> 00:00:18,360
이번 강의는 조금 다른 방향입니다.

5
00:00:18,360 --> 00:00:21,230
알고리즘 측면에서 새로운 내용을

6
00:00:21,230 --> 00:00:23,100
가르치지는 않을 겁니다.

7
00:00:23,100 --> 00:00:27,590
오히려 학생들에게 장기적인

8
00:00:27,590 --> 00:00:33,830
연구 발전과 오늘날 AI에서 중요한

9
00:00:33,830 --> 00:00:39,140
또 다른 차원인 인간 관점에

10
00:00:39,140 --> 00:00:42,140
대해 이야기하고자

11
00:00:42,140 --> 00:00:43,560
합니다.

12
00:00:43,560 --> 00:00:47,400
자료의 완전성을 위해 이 강의에서

13
00:00:47,400 --> 00:00:49,520
다른 부분과 약간

14
00:00:49,520 --> 00:00:53,040
겹치는 내용이 있을 수 있지만,

15
00:00:53,040 --> 00:00:58,980
전체적으로 이해하는 데 도움이 되길 바랍니다.

16
00:00:58,980 --> 00:01:05,200
이 슬라이드와 강의 제목은 '우리가 보는 것과 가치 있는

17
00:01:05,200 --> 00:01:08,960
것, 인간 관점의 AI'입니다.

18
00:01:08,960 --> 00:01:12,040
이미 이 내용을 들어본 분들도 계실 겁니다.

19
00:01:12,040 --> 00:01:15,380
이것은 진화와 기술 측면에서

20
00:01:15,380 --> 00:01:18,430
시각의 시작, 즉 기원에

21
00:01:18,430 --> 00:01:20,270
관한 이야기입니다.

22
00:01:20,270 --> 00:01:23,830
우리는 동물 세계에 시각이 처음

23
00:01:23,830 --> 00:01:30,260
등장한 5억 4천만 년 전의 첫 슬라이드에 대해 이야기했습니다.

24
00:01:30,260 --> 00:01:35,360
그때 동물, 특히 삼엽충이 광감지

25
00:01:35,360 --> 00:01:41,410
세포를 발달시켜 외부 세계를 인지하기

26
00:01:41,410 --> 00:01:43,670
시작했습니다.

27
00:01:43,670 --> 00:01:49,510
동물학자 Andrew Parker에

28
00:01:49,510 --> 00:01:56,960
따르면, 시각의 출현은 진화적 군비 경쟁을

29
00:01:56,960 --> 00:02:02,370
촉발했고, 동물들은 진화하거나

30
00:02:02,370 --> 00:02:04,600
멸종했습니다.

31
00:02:04,600 --> 00:02:09,210
이 군비 경쟁은 동물의 폭발적인 종 분화를

32
00:02:09,210 --> 00:02:12,280
일으켰고, 이를

33
00:02:12,280 --> 00:02:16,470
동물학자들은 캄브리아기 폭발 또는 진화의

34
00:02:16,470 --> 00:02:18,580
빅뱅이라 부릅니다.

35
00:02:18,580 --> 00:02:22,470
물론 시각은 오늘날까지도

36
00:02:22,470 --> 00:02:25,120
많은 동물에서 주요

37
00:02:25,120 --> 00:02:31,420
감각 지능 시스템임을 놀라워하지 않을 겁니다.

38
00:02:31,420 --> 00:02:35,980
모든 동물이 시각을 사용하는 것은 아니지만, 많은 동물이 사용합니다.

39
00:02:35,980 --> 00:02:39,300
인간에게도 시각은 주요 감각 시스템

40
00:02:39,300 --> 00:02:41,170
중 하나입니다.

41
00:02:41,170 --> 00:02:48,060
우리는 생존, 일, 오락, 사회화, 학습,

42
00:02:48,060 --> 00:02:51,180
발달 등 모든 활동에

43
00:02:51,180 --> 00:02:54,730
시각을 활용합니다.

44
00:02:54,730 --> 00:03:01,390
이것이 진화에 대한 요약입니다.

45
00:03:01,390 --> 00:03:07,060
또한 1960년대 컴퓨터 비전이 몇몇 학부생을

46
00:03:07,060 --> 00:03:13,260
동원해 시각 시스템의 중요한 부분을 구성하려는

47
00:03:13,260 --> 00:03:18,240
시도였던 여름 프로젝트였다는 점도 간략히

48
00:03:18,240 --> 00:03:19,570
언급했습니다.

49
00:03:19,570 --> 00:03:23,770
이는 AI 역사와도 일치하는데, 우리는

50
00:03:23,770 --> 00:03:28,660
북극성 같은 명확한 목표는 있었지만,

51
00:03:28,660 --> 00:03:32,230
소요 시간을 과소평가했습니다.

52
00:03:32,230 --> 00:03:34,990
아마 지금도 그런 상황을 겪고

53
00:03:34,990 --> 00:03:37,874
있지만, 많은 발전이 있었습니다.

54
00:03:37,874 --> 00:03:42,270
자율주행차부터 이미지 이해, 생성

55
00:03:42,270 --> 00:03:45,600
AI 혁명까지, 시각이

56
00:03:45,600 --> 00:03:49,110
큰 역할을 하며 많은

57
00:03:49,110 --> 00:03:54,770
부분에서 선도하고 있다는 것은 여러분도 잘

58
00:03:54,770 --> 00:03:56,630
아실 겁니다.

59
00:03:56,630 --> 00:04:02,580
이제 역사적 관점과 미래를 향해 이 문제를 다르게

60
00:04:02,580 --> 00:04:05,240
바라볼 때입니다.

61
00:04:05,240 --> 00:04:09,090
우리는 어디에서 왔고 어디로 가고 있나요?

62
00:04:09,090 --> 00:04:13,650
이 주제는 매우 중요합니다. 과거의

63
00:04:13,650 --> 00:04:16,310
많은 일이 미래를

64
00:04:16,310 --> 00:04:18,950
알려주기 때문입니다.

65
00:04:18,950 --> 00:04:23,390
이 강의는 세 부분으로 구성됩니다.

66
00:04:23,390 --> 00:04:28,980
첫째, 인간이 보는 것을 AI가 보게 만드는 것.

67
00:04:28,980 --> 00:04:30,660
이것이 우리가 출발한 지점입니다.

68
00:04:30,660 --> 00:04:33,980
인간의 능력에 영감을 받아

69
00:04:33,980 --> 00:04:37,560
기계도 똑같이 하길 원했습니다.

70
00:04:37,560 --> 00:04:39,380
다음으로, 인간이 보지 못하는

71
00:04:39,380 --> 00:04:41,640
것을 AI가 보게 만드는 것.

72
00:04:41,640 --> 00:04:44,390
마지막으로, 인간이 보고 싶어 하는

73
00:04:44,390 --> 00:04:46,970
것을 AI가 보게 만드는 것입니다.

74
00:04:46,970 --> 00:04:48,840
첫 번째 주제부터 시작하겠습니다.

75
00:04:48,840 --> 00:04:50,820
인간이 보는 것을 AI가 보게 만드는 것.

76
00:04:50,820 --> 00:04:53,440
다시 한 번, 간단히 복습해 보겠습니다.

77
00:04:53,440 --> 00:04:55,930
인간은 시각에 매우 능숙합니다.

78
00:04:55,930 --> 00:04:57,050
우리는 이것을 알고 있습니다.

79
00:04:57,050 --> 00:05:02,830
이것은 반세기 전 실험인데, 한 번도 본 적 없는

80
00:05:02,830 --> 00:05:07,810
영상을 10헤르츠로 재생하면, 즉 한

81
00:05:07,810 --> 00:05:11,890
프레임이 화면에 100밀리초 정도만

82
00:05:11,890 --> 00:05:14,750
나타나는데도, 인간의 눈은

83
00:05:14,750 --> 00:05:18,220
여전히 그 안의 목표물, 이

84
00:05:18,220 --> 00:05:22,900
경우 사람을 감지하는 데 문제가 없습니다.

85
00:05:22,900 --> 00:05:26,950
이 사람이 누구인지 전혀 사전

86
00:05:26,950 --> 00:05:29,290
지식이 없는 복잡한

87
00:05:29,290 --> 00:05:36,530
장면에서도, 인간의 시각 이해 능력, 특히 객체 중심 이해

88
00:05:36,530 --> 00:05:39,560
능력이 뛰어남을 강조합니다.

89
00:05:39,560 --> 00:05:43,900
또한 세기 전환기 즈음에

90
00:05:43,900 --> 00:05:47,920
신경생리학자들이 EEG

91
00:05:47,920 --> 00:05:53,820
캡으로 측정한 뇌 전기 신호를

92
00:05:53,820 --> 00:06:01,290
통해 인간이 복잡한 객체를 보는 속도를 측정한

93
00:06:01,290 --> 00:06:05,740
것을 잠깐 언급했습니다.

94
00:06:05,740 --> 00:06:10,020
동물과 동물을 구분하거나 분류하는 것은

95
00:06:10,020 --> 00:06:13,480
매우 복잡한 작업이지만,

96
00:06:13,480 --> 00:06:18,930
인간은 자극이 시작된 지 150밀리초 만에 이를

97
00:06:18,930 --> 00:06:21,160
수행할 수 있습니다.

98
00:06:21,160 --> 00:06:25,230
이는 두개골 아래의 생물학적 하드웨어를

99
00:06:25,230 --> 00:06:28,300
고려할 때 놀라운 속도입니다.

100
00:06:28,300 --> 00:06:31,830
또한 신경생리학자들은 객체

101
00:06:31,830 --> 00:06:38,070
인식이 인간 시각 지능에서 매우 중요한

102
00:06:38,070 --> 00:06:41,050
기능임을 알려주었습니다.

103
00:06:41,050 --> 00:06:45,390
너무 중요해서 뇌에는 객체 이해에 전념하는 신경 부위들이 있습니다.

104
00:06:45,390 --> 00:06:49,240
예를 들어 얼굴 영역, 장소

105
00:06:49,240 --> 00:06:54,330
영역, 신체 부위 영역 등이 있습니다.

106
00:06:54,330 --> 00:06:57,680
이는 진화가 객체 인식과 관련된 시각

107
00:06:57,680 --> 00:07:01,250
지능 능력을 갈고닦는 데 많은 시간을

108
00:07:01,250 --> 00:07:03,720
투자했다는 것을 보여줍니다.

109
00:07:03,720 --> 00:07:10,040
이 모든 것이 컴퓨터 비전 분야의 역사를 쌓아왔고,

110
00:07:10,040 --> 00:07:13,850
수십 년 전부터 객체 인식이 시각

111
00:07:13,850 --> 00:07:17,820
지능의 기본 구성 요소가 되었습니다.

112
00:07:17,820 --> 00:07:20,640
우리는 기계에게 그 능력을 부여하고자 합니다.

113
00:07:20,640 --> 00:07:25,280
이를 위해 문제를 정의하는데,

114
00:07:25,280 --> 00:07:29,490
적어도 원래 문제는 이미지가

115
00:07:29,490 --> 00:07:35,870
주어졌을 때, 컴퓨터가 이미지 내 객체가

116
00:07:35,870 --> 00:07:40,370
무엇인지 인식하도록 하는 것입니다.

117
00:07:40,370 --> 00:07:43,410
이것은 인간에게는 매우 수월한 작업입니다.

118
00:07:43,410 --> 00:07:45,230
하지만 생각해보면,

119
00:07:45,230 --> 00:07:48,950
이제 컴퓨터 비전을 충분히 배워서

120
00:07:48,950 --> 00:07:54,200
수학적으로 조명, 질감, 배경, 가림, 시점,

121
00:07:54,200 --> 00:07:59,000
크기 조절 등 다양한 변수 때문에 어떤 객체든

122
00:07:59,000 --> 00:08:04,610
인식할 수 있는 무한한 가능성이 있다는 걸 알게

123
00:08:04,610 --> 00:08:05,460
되었죠.

124
00:08:05,460 --> 00:08:12,500
그래서 이 문제는 근본적으로 어려운 과제입니다.

125
00:08:12,500 --> 00:08:16,920
딥러닝 이전의 역사도 매우 흥미롭습니다.

126
00:08:16,920 --> 00:08:20,120
일반화 가능한 객체 인식

127
00:08:20,120 --> 00:08:25,040
문제를 해결하려는 꽤 용감한 시도들이

128
00:08:25,040 --> 00:08:26,010
있었습니다.

129
00:08:26,010 --> 00:08:30,920
첫 번째 시도는 사실 심리학에서 많은 영감을

130
00:08:30,920 --> 00:08:32,730
받았습니다.

131
00:08:32,730 --> 00:08:36,470
우리는 때때로 지나친 자기성찰까지

132
00:08:36,470 --> 00:08:39,140
하면서 스스로를 돌아봅니다.

133
00:08:39,140 --> 00:08:43,415
사람들이 부품을 조합한다고 생각합니다.

134
00:08:43,415 --> 00:08:47,060
우리는 객체를 볼 때 기하학적 부품을 보고,

135
00:08:47,060 --> 00:08:50,360
그것들을 조합해 다양한 객체를 만듭니다.

136
00:08:50,360 --> 00:08:57,850
미리 정해진 부품이나 형태를 특정 방식으로

137
00:08:57,850 --> 00:09:03,340
조합하는 아이디어가 객체 인식의

138
00:09:03,340 --> 00:09:07,340
첫 번째 물결이었습니다.

139
00:09:07,340 --> 00:09:11,260
이것들은 70년대, 80년대, 심지어

140
00:09:11,260 --> 00:09:15,430
90년대까지 이어진 다양한 연구나 모델로, 부품과

141
00:09:15,430 --> 00:09:18,220
구성을 사용해 객체를 인식하려

142
00:09:18,220 --> 00:09:19,040
했습니다.

143
00:09:19,040 --> 00:09:21,260
물론, 실제로는 잘 작동하지 않았습니다.

144
00:09:21,260 --> 00:09:24,590
수학적으로는 아름답고 단순하지만, 효과는 없었죠.

145
00:09:24,590 --> 00:09:30,070
딥러닝 이전의 두 번째 객체 인식

146
00:09:30,070 --> 00:09:36,970
물결은 AI 분야에서 정말 중요한 시기였습니다.

147
00:09:36,970 --> 00:09:39,380
바로 머신러닝의 시작이었죠.

148
00:09:39,380 --> 00:09:41,330
통계적 머신러닝입니다.

149
00:09:41,330 --> 00:09:44,140
컴퓨터 프로그래밍과 통계

150
00:09:44,140 --> 00:09:46,480
모델링의 결합이었습니다.

151
00:09:46,480 --> 00:09:49,320
그리고 그 결합으로 우리는

152
00:09:49,320 --> 00:09:53,550
세상이 매우 복잡하다는 것을 깨닫기 시작합니다.

153
00:09:53,550 --> 00:09:57,000
이러한 문제들, 즉 시각 지능이든

154
00:09:57,000 --> 00:10:00,150
언어 지능이든 다른 종류의

155
00:10:00,150 --> 00:10:05,710
지능이든, 일반화하기 위해서는 파라미터를 학습해야 합니다.

156
00:10:05,710 --> 00:10:15,190
손으로 조정한 모델을 사용해서 좋은 학습을 얻는 것은 매우 어렵습니다.

157
00:10:15,190 --> 00:10:19,140
우리는 이제 데이터가 필요하다는 것을 압니다. 그 당시에는 얼마나 많은
데이터가

158
00:10:19,140 --> 00:10:20,350
필요한지 몰랐지만요.

159
00:10:20,350 --> 00:10:25,200
하지만 우리는 또한 통계 모델을 설계하거나 아키텍처화해서

160
00:10:25,200 --> 00:10:28,320
다양한 학습 규칙을 통해 학습할 수

161
00:10:28,320 --> 00:10:31,950
있는 능력을 갖추어야 한다는 것도 알고

162
00:10:31,950 --> 00:10:32,710
있습니다.

163
00:10:32,710 --> 00:10:37,650
그래서 그 시기에 랜덤 필드, 베이즈 네트워크,

164
00:10:37,650 --> 00:10:41,250
서포트 벡터 머신 등 다양한

165
00:10:41,250 --> 00:10:45,110
모델들이 꽃피는 것을 보았습니다.

166
00:10:45,110 --> 00:10:50,720
사실 21세기 첫 10년 동안 객체 인식 분야에서

167
00:10:50,720 --> 00:10:55,230
많은 진전이 있었고, 소수의 객체

168
00:10:55,230 --> 00:10:59,390
클래스를 대상으로 국제 벤치마크가

169
00:10:59,390 --> 00:11:04,970
만들어져 모두가 자신의 알고리즘을 비교하도록

170
00:11:04,970 --> 00:11:06,210
장려했습니다.

171
00:11:06,210 --> 00:11:08,160
그래서 우리는 점점 가까워지고 있습니다.

172
00:11:08,160 --> 00:11:12,920
객체 인식의 마지막 열쇠는 다시

173
00:11:12,920 --> 00:11:15,900
인지과학으로 돌아갑니다.

174
00:11:15,900 --> 00:11:19,170
이 심리학자 Irv Biederman은

175
00:11:19,170 --> 00:11:23,600
인간이 엄청난 수의 객체를 인식할 수 있다고

176
00:11:23,600 --> 00:11:26,190
오래전부터 추측했습니다.

177
00:11:26,190 --> 00:11:28,980
이것은 우리의 상식에 부합하는 직관적인 사실입니다.

178
00:11:28,980 --> 00:11:32,000
하지만 그는 실제로 숫자를 제시했습니다.

179
00:11:32,000 --> 00:11:34,620
제가 개인적으로 Biederman

180
00:11:34,620 --> 00:11:40,530
숫자라고 부르는 건데, 그는 6~7세 아이들이 약

181
00:11:40,530 --> 00:11:46,070
3만에서 10만 개의 서로 다른 시각 범주를 인식할 수 있다고

182
00:11:46,070 --> 00:11:47,580
추측했습니다.

183
00:11:47,580 --> 00:11:50,040
그는 이 숫자를 어디서 얻었을까요?

184
00:11:50,040 --> 00:11:54,000
이 숫자는 사전에서 명사의 수를

185
00:11:54,000 --> 00:11:58,610
살펴보고, 아이들이 다양한 객체를 인식하는 시각

186
00:11:58,610 --> 00:12:01,590
연구를 결합한 결과입니다.

187
00:12:01,590 --> 00:12:06,020
하지만 이 숫자는 컴퓨터 비전 분야에 매우

188
00:12:06,020 --> 00:12:10,320
벅차고 냉정한 현실을 보여줍니다.

189
00:12:10,320 --> 00:12:13,440
왜냐하면 21세기 첫 10년

190
00:12:13,440 --> 00:12:18,770
중반까지 우리는 인간이 경험하는 것에 비해 매우 적은

191
00:12:18,770 --> 00:12:23,150
수의 객체 범주와 이미지로 작업했기

192
00:12:23,150 --> 00:12:24,120
때문입니다.

193
00:12:24,120 --> 00:12:29,120
이것이 바로 ImageNet 프로젝트의

194
00:12:29,120 --> 00:12:36,380
동기였습니다. 이 프로젝트는 이 숫자를 진지하게 받아들여 심리학자

195
00:12:36,380 --> 00:12:38,630
Biederman이

196
00:12:38,630 --> 00:12:42,590
추측한 것과 비슷한 수준, 즉 약

197
00:12:42,590 --> 00:12:45,300
22,000개의 객체

198
00:12:45,300 --> 00:12:51,480
클래스와 1,500만 장 이상의 이미지를 구축했습니다.

199
00:12:51,480 --> 00:12:53,720
물론 이것이 여러분이 이

200
00:12:53,720 --> 00:12:57,330
수업에 들어오게 되는 시작점입니다.

201
00:12:57,330 --> 00:13:01,890
ImageNet이 제공한 대규모 데이터 덕분에, 우리는

202
00:13:01,890 --> 00:13:07,250
강력한 알고리즘, 특히 처음에는 convolutional neural

203
00:13:07,250 --> 00:13:09,630
network, 지금은

204
00:13:09,630 --> 00:13:12,500
transformer 등으로 불리는

205
00:13:12,500 --> 00:13:17,750
알고리즘들이 빅데이터를 통해 그 힘을 발휘하는 것을 보기 시작했습니다.

206
00:13:17,750 --> 00:13:23,360
이것은 이 내용을 배우지 않은 분들을 위한 일반적인

207
00:13:23,360 --> 00:13:24,630
슬라이드입니다.

208
00:13:24,630 --> 00:13:26,790
여러분 모두 아시니까 이 부분은 건너뛰겠습니다.

209
00:13:26,790 --> 00:13:31,740
간단한 역사로는, ImageNet이 나오고

210
00:13:31,740 --> 00:13:35,360
convolutional neural

211
00:13:35,360 --> 00:13:38,270
network를 사용한

212
00:13:38,270 --> 00:13:41,460
지 몇 년 만에 객체 인식

213
00:13:41,460 --> 00:13:45,610
문제를 해결하는 문이 활짝 열렸습니다.

214
00:13:45,610 --> 00:13:48,120
이제 우리는 세상의

215
00:13:48,120 --> 00:13:51,420
어떤 사진이든 보고 크고 작고

216
00:13:51,420 --> 00:13:55,440
어떤 방향이든 객체를 인식할 수

217
00:13:55,440 --> 00:13:59,250
있는 알고리즘을 갖게 되었습니다.

218
00:13:59,250 --> 00:14:01,030
100% 완벽한가요?

219
00:14:01,030 --> 00:14:01,530
아니요.

220
00:14:01,530 --> 00:14:04,840
항상 해결해야 할 긴 꼬리 문제들이 존재합니다.

221
00:14:04,840 --> 00:14:07,720
하지만 산업적 응용 측면에서는

222
00:14:07,720 --> 00:14:10,590
이 문제가 크게 진전되어

223
00:14:10,590 --> 00:14:13,630
성숙한 문제로 자리 잡았습니다.

224
00:14:13,630 --> 00:14:16,860
그리고 여러분 모두 아시다시피, 이 모든 것은

225
00:14:16,860 --> 00:14:21,660
2012년이라는 수렴점에서 이루어졌습니다. 그 해 ImageNet

226
00:14:21,660 --> 00:14:24,240
챌린지가 convolutional

227
00:14:24,240 --> 00:14:26,770
neural network를 위한 데이터를

228
00:14:26,770 --> 00:14:30,340
제공했고, 당시 두 개의 GPU를 사용했습니다.

229
00:14:30,340 --> 00:14:34,590
세 가지 요소가 합쳐져 딥러닝의

230
00:14:34,590 --> 00:14:37,580
순간, 즉 딥러닝의 탄생을

231
00:14:37,580 --> 00:14:39,300
가져왔습니다.

232
00:14:39,300 --> 00:14:42,980
이 수업에서는 또한 지난 10년간

233
00:14:42,980 --> 00:14:46,220
ImageNet 챌린지가 낳은 다양한

234
00:14:46,220 --> 00:14:49,880
아키텍처, 예를 들어 convolutional

235
00:14:49,880 --> 00:14:53,510
neural network나

236
00:14:53,510 --> 00:14:57,990
ResNet 등에 대해서도 조금 다뤘습니다.

237
00:14:57,990 --> 00:15:05,610
이것이 바로 딥러닝 혁명의 시작입니다.

238
00:15:05,610 --> 00:15:09,510
그리고 시각 지능을 향한 탐구는 단순히 장면

239
00:15:09,510 --> 00:15:13,220
속 객체에 라벨을 붙이는 데서 멈추지 않을

240
00:15:13,220 --> 00:15:14,070
것입니다.

241
00:15:14,070 --> 00:15:18,420
예를 들어, 이 두 장면에서 단순히 객체만 라벨링한다면,

242
00:15:18,420 --> 00:15:21,150
라마와 사람이라고 생각할 겁니다.

243
00:15:21,150 --> 00:15:23,450
하지만 두 번째 장면인 라마와

244
00:15:23,450 --> 00:15:25,670
사람을 보여드리면 이야기가

245
00:15:25,670 --> 00:15:27,420
완전히 달라집니다.

246
00:15:27,420 --> 00:15:29,730
같은 객체가 있어도

247
00:15:29,730 --> 00:15:31,920
관계는 매우 다릅니다.

248
00:15:31,920 --> 00:15:35,290
그래서 인지과학자들은 다시 한 번

249
00:15:35,290 --> 00:15:37,780
컴퓨터 과학자들의 선두에 서서

250
00:15:37,780 --> 00:15:43,270
단순히 객체를 이름 붙이거나 분류하는 것을 넘어서 시각적

251
00:15:43,270 --> 00:15:47,330
지능에 대해 생각하도록 영감을 주었습니다.

252
00:15:47,330 --> 00:15:49,790
이 논문에서, 꽤 유명한

253
00:15:49,790 --> 00:15:53,530
심리학자인 Jeremy Wolfe가

254
00:15:53,530 --> 00:15:56,830
복잡한 자연 장면을 이해하는 데

255
00:15:56,830 --> 00:15:59,590
있어 객체 간의 관계가

256
00:15:59,590 --> 00:16:02,890
반드시 코드화되어야 한다고 지적하는

257
00:16:02,890 --> 00:16:05,420
훌륭한 논문을 썼습니다.

258
00:16:05,420 --> 00:16:09,160
그 연구에 영감을 받아 컴퓨터 비전 분야는

259
00:16:09,160 --> 00:16:13,220
관계를 어떻게 이해할지 연구하기 시작했습니다.

260
00:16:13,220 --> 00:16:15,140
이것은 초기 연구입니다.

261
00:16:15,140 --> 00:16:20,730
여러분은 지난주에 Ranjay의 강의를 들었죠.

262
00:16:20,730 --> 00:16:21,230
여러분은 지난주에 Ranjay의 강의를 들었죠.

263
00:16:21,230 --> 00:16:21,910
네, 지난주입니다.

264
00:16:21,910 --> 00:16:27,850
이것은 그의 박사 논문으로, 장면 그래프를 표현으로 사용하여

265
00:16:27,850 --> 00:16:31,820
객체 간 관계를 학습하는 연구였습니다.

266
00:16:31,820 --> 00:16:34,390
이 경우 장면 그래프는

267
00:16:34,390 --> 00:16:37,880
객체인 엔티티 노드들로 정의되고,

268
00:16:37,880 --> 00:16:40,510
이들의 관계는 노드 간

269
00:16:40,510 --> 00:16:42,710
연결성으로 정의되며,

270
00:16:42,710 --> 00:16:45,760
때로는 특정 객체를 정의하는

271
00:16:45,760 --> 00:16:48,380
속성 관계도 포함됩니다.

272
00:16:48,380 --> 00:16:51,250
그리고 이렇게 단순하게 두 사람이

273
00:16:51,250 --> 00:16:56,590
한 사람이 다른 사람에게 케이크를 먹이는 장면만 있어도,

274
00:16:56,590 --> 00:17:01,510
시각 장면의 풍부함 때문에 매우 밀집된 장면 그래프를

275
00:17:01,510 --> 00:17:02,900
만들 수 있습니다.

276
00:17:02,900 --> 00:17:10,630
이것은 객체 인식 오류 이후 Ranjay의 논문으로, 우리는

277
00:17:10,630 --> 00:17:16,480
visual genome이라는 데이터셋을 구축했는데,

278
00:17:16,480 --> 00:17:22,810
여기서는 객체 관계와 이야기 설명을 함께 모으려고

279
00:17:22,810 --> 00:17:24,380
했습니다.

280
00:17:24,380 --> 00:17:27,440
Ranjay가 한 작업 중 제가

281
00:17:27,440 --> 00:17:30,820
정말 재미있다고 생각한 것은 특이한

282
00:17:30,820 --> 00:17:33,790
객체 관계의 제로샷 학습이었습니다.

283
00:17:33,790 --> 00:17:38,560
예를 들어, 사람이 말을 타는 것은 흔한 일이죠.

284
00:17:38,560 --> 00:17:42,070
사람이 모자를 쓰는 것도 흔한 일이지만,

285
00:17:42,070 --> 00:17:45,910
말이 모자를 쓰는 것은 일반적으로 드문 일입니다.

286
00:17:45,910 --> 00:17:49,390
빅데이터 학습 시대에도 이런 종류의 데이터를

287
00:17:49,390 --> 00:17:51,540
반복해서 얻기 어렵습니다.

288
00:17:51,540 --> 00:17:54,580
왜냐하면 그런 데이터가 많지 않기 때문입니다.

289
00:17:54,580 --> 00:17:58,750
하지만 이 합성 장면 그래프 표현을 사용하면

290
00:17:58,750 --> 00:18:02,280
더 흔한 관계들을 학습하고, 그

291
00:18:02,280 --> 00:18:06,880
표현에서 드문 관계들도 유도할 수 있습니다.

292
00:18:06,880 --> 00:18:09,630
그리고 다시 말하지만, 이것은

293
00:18:09,630 --> 00:18:13,920
제로샷 학습의 또 다른 예로, 의자에 앉은 사람이나

294
00:18:13,920 --> 00:18:16,080
잔디밭이나 들판에 있는

295
00:18:16,080 --> 00:18:18,840
소화전은 모두 흔한 관계지만,

296
00:18:18,840 --> 00:18:22,880
소화전에 앉은 사람은 데이터가 부족해서 얻기

297
00:18:22,880 --> 00:18:24,370
어려운 관계입니다.

298
00:18:24,370 --> 00:18:27,790
우리는 그것을 가능하게 만들 수 있었습니다.

299
00:18:27,790 --> 00:18:31,190
이것은 논문에서 가져온 그림으로,

300
00:18:31,190 --> 00:18:35,060
당시 Ranjay의 연구가 다른 많은 방법들과

301
00:18:35,060 --> 00:18:39,530
비교해 최첨단 인식률을 달성했음을 보여줍니다.

302
00:18:39,530 --> 00:18:42,530
하지만 관계만으로는 충분하지 않습니다.

303
00:18:42,530 --> 00:18:46,220
더 풍부한 이야기를 실제로

304
00:18:46,220 --> 00:18:50,630
전달하는 능력, 즉 자연어를

305
00:18:50,630 --> 00:18:55,560
사용하는 것이 다음 큰 목표입니다.

306
00:18:55,560 --> 00:19:00,920
그래서 2014년경, 그 문제에 대해 연구를

307
00:19:00,920 --> 00:19:04,710
시작하고 고민하기 시작했습니다.

308
00:19:04,710 --> 00:19:09,360
그것은 Alex의 이미지가 나온 지 불과 2년 후였습니다.

309
00:19:09,360 --> 00:19:12,660
하지만 이 분야는 매우 빠르게 발전하고 있었습니다.

310
00:19:12,660 --> 00:19:16,850
우리는 합성곱 신경망과 LSTM이라는

311
00:19:16,850 --> 00:19:21,950
언어 모델의 조합으로 할 수

312
00:19:21,950 --> 00:19:26,880
있는 일에 매우 영감을 받았습니다.

313
00:19:26,880 --> 00:19:29,960
이것은 Andrej Karpathy의

314
00:19:29,960 --> 00:19:34,480
논문으로, 우리가 이미지 캡셔닝이나 스토리텔링, 그리고

315
00:19:34,480 --> 00:19:38,800
Justin Johnson이 수행한 작업의 일부인 밀집

316
00:19:38,800 --> 00:19:42,340
캡셔닝을 어떻게 하는지 처음으로 보여준 팀

317
00:19:42,340 --> 00:19:43,820
중 하나였습니다.

318
00:19:43,820 --> 00:19:48,380
그리고 그는 이 강좌의 공동 강사 중 한 명임을 알고 있습니다.

319
00:19:48,380 --> 00:19:53,840
그것은 2015년부터 2018년 사이의 시기였습니다.

320
00:19:53,840 --> 00:19:58,130
이 문제를 해결하기 위해 많은 연구가 진행되었습니다.

321
00:19:58,130 --> 00:20:01,940
물론 오늘날에는 멀티모달 LLM을

322
00:20:01,940 --> 00:20:04,420
사용해 이 문제 해결을

323
00:20:04,420 --> 00:20:07,790
한 단계 더 발전시켰습니다.

324
00:20:07,790 --> 00:20:10,940
하지만 이것이 그 연구 분야의 시작입니다.

325
00:20:10,940 --> 00:20:15,730
솔직히 말해서, 저 자신도 세기 초에 이 분야에

326
00:20:15,730 --> 00:20:18,860
들어온 컴퓨터 비전 과학자로서,

327
00:20:18,860 --> 00:20:24,130
데이터와 신경망 알고리즘만 확보되자마자 우리

328
00:20:24,130 --> 00:20:27,130
분야가 이 문제를 얼마나 빠르게

329
00:20:27,130 --> 00:20:29,970
해결했는지 매우 놀랐습니다.

330
00:20:29,970 --> 00:20:34,660
하지만 훨씬 더 어려운 문제는 사실 동적 장면에 있습니다.

331
00:20:34,660 --> 00:20:38,400
동적 장면에서는 훨씬 더 복잡한 관계가

332
00:20:38,400 --> 00:20:40,030
존재합니다.

333
00:20:40,030 --> 00:20:42,760
훨씬 더 복잡한 움직임들이 있죠.

334
00:20:42,760 --> 00:20:50,350
그리고 카메라 움직임이나 장면 내의 객체, 즉 배우들이

335
00:20:50,350 --> 00:20:55,000
다양한 행동을 할 수 있습니다.

336
00:20:55,000 --> 00:20:57,720
그래서 Esan과 우리 연구실

337
00:20:57,720 --> 00:21:01,390
학생들과의 협업으로 진행한 이 연구에서는

338
00:21:01,390 --> 00:21:05,040
이를 다중 객체 다중 배우 활동 이해라고

339
00:21:05,040 --> 00:21:05,970
부릅니다.

340
00:21:05,970 --> 00:21:07,960
이 연구는 훨씬 더 최근의 작업입니다.

341
00:21:07,960 --> 00:21:11,040
우리는 이 연구를 불과 몇 년 전에 발표했습니다.

342
00:21:11,040 --> 00:21:15,570
동적 장면에서 이 배우들과 그들의 활동 간의 관계를

343
00:21:15,570 --> 00:21:19,630
포착하는 문제는 아직도, 제가 말하건대,

344
00:21:19,630 --> 00:21:22,180
해결되지 않은 문제입니다.

345
00:21:22,180 --> 00:21:24,970
이 문제는 매우 깊은 영향을 미칠 것입니다.

346
00:21:24,970 --> 00:21:27,370
여러분은 실리콘밸리에 있으니 아시겠지만,

347
00:21:27,370 --> 00:21:34,510
예를 들어 로봇에 대한 엄청난 기대감을 많이 듣고 계실

348
00:21:34,510 --> 00:21:35,410
겁니다.

349
00:21:35,410 --> 00:21:38,580
만약 우리가 일상에서 함께 일하는 로봇을

350
00:21:38,580 --> 00:21:42,370
꿈꾼다면, 로봇은 이 문제를 해결해야 합니다.

351
00:21:42,370 --> 00:21:46,180
장면이 얼마나 복잡한지, 사람들이 무엇을 하는지, 누가 무엇을 하는지,

352
00:21:46,180 --> 00:21:48,370
다음에 무엇이 일어날지 이해해야 하죠.

353
00:21:48,370 --> 00:21:49,985
이것은 아직 해결되지 않은 문제입니다.

354
00:21:53,730 --> 00:21:57,680
또한 제가 보여드린 것 외에도, 여러분은

355
00:21:57,680 --> 00:22:02,340
이 수업과 관련된 컴퓨터 비전 문제에 대해 조금

356
00:22:02,340 --> 00:22:06,670
배웠지만, 자세히 다룰 시간이 없었습니다.

357
00:22:06,670 --> 00:22:11,010
예를 들어 3D 컴퓨터 비전이나 인간 자세 이해 같은 것들이죠.

358
00:22:11,010 --> 00:22:15,580
그리고 물론 생성 AI와 생성 모델도 있습니다.

359
00:22:15,580 --> 00:22:18,390
이것은 현대 AI가 부활한

360
00:22:18,390 --> 00:22:22,800
이후 컴퓨터 비전 분야가 얼마나 엄청나게

361
00:22:22,800 --> 00:22:27,410
빠르게 발전해 왔는지를 보여주기 위한 것입니다.

362
00:22:27,410 --> 00:22:31,220
하지만 이 섹션에서 제가 전달하고 싶은

363
00:22:31,220 --> 00:22:34,110
핵심 메시지는 두 가지입니다.

364
00:22:34,110 --> 00:22:38,240
첫째는 데이터, 연산 능력, 신경망 알고리즘이 약 10년

365
00:22:38,240 --> 00:22:42,030
전 또는 13년 전에 진정으로 융합되었다는 점입니다.

366
00:22:42,030 --> 00:22:45,770
그때가 현대 AI 또는 딥러닝 혁명이 일어난

367
00:22:45,770 --> 00:22:47,340
순간이었습니다.

368
00:22:47,340 --> 00:22:50,060
하지만 그 역사와 우리가

369
00:22:50,060 --> 00:22:52,320
작업해 온 많은 문제들은

370
00:22:52,320 --> 00:22:57,110
인지과학, 심리학, 신경과학에서 진정한 영감을

371
00:22:57,110 --> 00:22:58,580
받았습니다.

372
00:22:58,580 --> 00:23:05,930
그리고 저에게 그것은 앞으로도 계속될 것입니다.

373
00:23:05,930 --> 00:23:10,640
즉, 우리는 뇌가 할 수 있는 것과 뇌가 일을 처리하는

374
00:23:10,640 --> 00:23:14,300
방식을 계속해서 영감을 받을 것이고, AI를

375
00:23:14,300 --> 00:23:18,270
사용해 뇌 연구를 돕는 일도 계속할 것입니다.

376
00:23:18,270 --> 00:23:22,450
그래서 오늘날 AI와 인지과학, 신경과학,

377
00:23:22,450 --> 00:23:25,510
뇌과학 사이에는 매우 밀접한 관계가

378
00:23:25,510 --> 00:23:26,810
있습니다.

379
00:23:26,810 --> 00:23:29,540
이것이 첫 번째 섹션입니다.

380
00:23:29,540 --> 00:23:33,640
그리고 물론 제가 방금 발표한 내용에는

381
00:23:33,640 --> 00:23:37,840
많은 학생들과 협력자들이 기여했습니다.

382
00:23:37,840 --> 00:23:42,580
이제 인간이 보지 못하는 것을 보기 위해 AI를 단순히 구축하는

383
00:23:42,580 --> 00:23:45,320
것을 넘어서는 이야기를 해보겠습니다.

384
00:23:45,320 --> 00:23:49,600
이것이 바로 AI를 인간 능력 이상으로, 즉 초인적으로

385
00:23:49,600 --> 00:23:51,320
발전시키는 부분입니다.

386
00:23:51,320 --> 00:23:57,380
예를 들어 대부분 사람들은 공룡을 많이 인식하지 못합니다.

387
00:23:57,380 --> 00:23:59,450
아마 몇 가지 이름만 대실 수 있을 겁니다.

388
00:23:59,450 --> 00:24:01,910
어떤 아이들은 정말 많은 이름을 알 수 있습니다.

389
00:24:01,910 --> 00:24:05,920
수천, 수만 종의 새

390
00:24:05,920 --> 00:24:11,440
종류나 수만 개의 자동차 종류는

391
00:24:11,440 --> 00:24:14,480
말할 것도 없고요.

392
00:24:14,480 --> 00:24:19,600
이런 일을 저는 세밀한 객체 분류(fine-grained object
categorization)라고

393
00:24:19,600 --> 00:24:20,870
부릅니다.

394
00:24:20,870 --> 00:24:24,010
인간은 이걸 그렇게 잘하지 못합니다.

395
00:24:24,010 --> 00:24:27,780
솔직히 말해서, 이 문제는 아직 완전히

396
00:24:27,780 --> 00:24:31,170
해결되지 않았다고 생각합니다.

397
00:24:31,170 --> 00:24:34,170
특히 이 생성 AI 시대에, 우리는

398
00:24:34,170 --> 00:24:37,600
멀티모달 LLM에 대해 많이 이야기하고 있죠.

399
00:24:37,600 --> 00:24:42,160
이 문제는 다소 간과되었거나

400
00:24:42,160 --> 00:24:44,950
주류 문제는 아닙니다.

401
00:24:44,950 --> 00:24:46,650
하지만 여전히

402
00:24:46,650 --> 00:24:50,650
중요한 역할을 할 겁니다.

403
00:24:50,650 --> 00:24:53,910
이 초기 세밀한 새 종

404
00:24:53,910 --> 00:24:58,320
인식 연구에서, 우리는 4,000마리

405
00:24:58,320 --> 00:25:02,230
새 데이터셋을 모았습니다.

406
00:25:02,230 --> 00:25:04,770
보시다시피,

407
00:25:04,770 --> 00:25:13,570
종 분류 단계가 올라갈수록, 즉 더 일반적인

408
00:25:13,570 --> 00:25:20,490
이름일수록 오류가 줄어듭니다. 다시

409
00:25:20,490 --> 00:25:24,620
말해, 세밀한 수준에서

410
00:25:24,620 --> 00:25:30,120
여전히 많은 오류가 발생한다는

411
00:25:30,120 --> 00:25:32,490
뜻입니다.

412
00:25:32,490 --> 00:25:39,710
알고리즘이 아직 완전히 준비되지 않았습니다.

413
00:25:39,710 --> 00:25:42,980
제가 흥미롭게 생각하는

414
00:25:42,980 --> 00:25:50,660
또 다른 연구는 몇 년 전 제 연구실 학생들이 만든 세밀한

415
00:25:50,660 --> 00:25:54,110
자동차 분류기입니다.

416
00:25:54,110 --> 00:25:58,680
제조사, 모델, 연도별로 분류했죠.

417
00:25:58,680 --> 00:26:07,010
1970년대 이후로는 제조사, 모델, 연도별로 수천

418
00:26:07,010 --> 00:26:11,460
개의 자동차 모델이 존재합니다.

419
00:26:11,460 --> 00:26:13,410
우리는 미국 전역

420
00:26:13,410 --> 00:26:20,910
100~200개 주요 도시의 Google Street View

421
00:26:20,910 --> 00:26:23,970
이미지를 사용했습니다.

422
00:26:23,970 --> 00:26:28,250
그리고 세밀한 자동차 탐지기를 이용해

423
00:26:28,250 --> 00:26:32,610
이 도시 거리의 자동차를 탐지했습니다.

424
00:26:32,610 --> 00:26:36,930
이걸 사회 패턴을 연구하는 렌즈로 사용했죠.

425
00:26:36,930 --> 00:26:40,620
예를 들어, 제가 보여준

426
00:26:40,620 --> 00:26:43,465
교육 패턴입니다.

427
00:26:43,465 --> 00:26:46,220
자동차 모델과 교육

428
00:26:46,220 --> 00:26:53,850
수준, 소득 패턴이 매우 높은 상관관계를 보였습니다.

429
00:26:53,850 --> 00:26:59,060
그 논문에서는 투표 패턴도 매우 높은 상관관계를 보였고,

430
00:26:59,060 --> 00:27:03,270
환경 패턴도 마찬가지였습니다.

431
00:27:03,270 --> 00:27:07,010
그래서 컴퓨터 비전을 사회를 연구하는 렌즈로

432
00:27:07,010 --> 00:27:09,720
활용하는 매우 흥미로운 방법입니다.

433
00:27:09,720 --> 00:27:12,000
어떤 인간도, 심지어

434
00:27:12,000 --> 00:27:17,380
인간 집단도 이걸 쉽게 할 수 없습니다.

435
00:27:17,380 --> 00:27:24,070
그래서 AI는 인간이 볼 수 있는 한계를 확장하고 있습니다.

436
00:27:24,070 --> 00:27:27,980
이 아이디어를 확실히 하기 위해, 몇 가지 테스트를 해보겠습니다.

437
00:27:27,980 --> 00:27:31,840
인간도 한계가 있습니다.

438
00:27:31,840 --> 00:27:35,120
방금 인간의 시각 능력을 칭찬했지만,

439
00:27:35,120 --> 00:27:36,680
우리는 한계도 가지고 있습니다.

440
00:27:36,680 --> 00:27:39,490
이것은 매우 유명한 시각 착시 테스트인

441
00:27:39,490 --> 00:27:41,240
Stroop 테스트입니다.

442
00:27:41,240 --> 00:27:44,390
여러분은 단어를 읽을 수 있습니다.

443
00:27:44,390 --> 00:27:49,810
하지만 단어의 색깔을 가능한 빨리 왼쪽에서 오른쪽,

444
00:27:49,810 --> 00:27:54,080
위에서 아래로 읽으라고 하면 쉽지

445
00:27:54,080 --> 00:27:56,660
않다는 걸 알게 됩니다.

446
00:27:56,660 --> 00:28:04,910
읽어보세요, 빨강, 노랑, 초록, 보라, 파랑,

447
00:28:04,910 --> 00:28:07,095
검정, 주황.

448
00:28:07,095 --> 00:28:09,680
이게 여러분과 싸우는 거죠.

449
00:28:09,680 --> 00:28:13,520
이것은 시각적 주의력과 그 모든 것 사이의 싸움입니다.

450
00:28:13,520 --> 00:28:15,810
여기 또 다른 예가 있습니다.

451
00:28:15,810 --> 00:28:20,440
그림이 두 개의 교차하는 이미지로 되어 있습니다.

452
00:28:20,440 --> 00:28:22,960
그리고 한 가지 변화가 있습니다.

453
00:28:22,960 --> 00:28:26,250
두 교차하는 그림 사이에 일어나는 꽤

454
00:28:26,250 --> 00:28:27,640
큰 변화입니다.

455
00:28:27,640 --> 00:28:29,950
변화를 발견하셨는지 모르겠습니다.

456
00:28:29,950 --> 00:28:31,616
발견하셨나요?

457
00:28:31,616 --> 00:28:32,490
엔진--

458
00:28:32,490 --> 00:28:34,320
네, 엔진입니다.

459
00:28:34,320 --> 00:28:37,240
그래서 발견하는 데 시간이 좀 걸립니다.

460
00:28:37,240 --> 00:28:41,640
이것은 변화맹(change blindness)이라는 아주 유명한

461
00:28:41,640 --> 00:28:43,150
심리학 실험입니다.

462
00:28:43,150 --> 00:28:44,820
이 모든 것이 재미있습니다.

463
00:28:44,820 --> 00:28:46,870
스트룹 테스트도 재미있고, 이것도 재미있습니다.

464
00:28:46,870 --> 00:28:48,810
하지만 이것은 재미없습니다.

465
00:28:48,810 --> 00:28:51,850
인간의 주의력은 한계가 있습니다.

466
00:28:51,850 --> 00:28:56,260
그리고 어떤 상황에서는 우리의 업무 생활에서

467
00:28:56,260 --> 00:29:00,280
그런 주의력 한계가 치명적일 수 있습니다.

468
00:29:00,280 --> 00:29:05,940
예를 들어, 의료 과실은 미국 의료 시스템에서

469
00:29:05,940 --> 00:29:10,210
세 번째로 큰 사망 원인입니다.

470
00:29:10,210 --> 00:29:13,370
물론, 환자의 몸 안에 가위를

471
00:29:13,370 --> 00:29:16,700
남겨두는 것은 의료 과실의 상징적인

472
00:29:16,700 --> 00:29:18,000
이미지입니다.

473
00:29:18,000 --> 00:29:20,130
하지만 의료 과실은 매우 많습니다.

474
00:29:20,130 --> 00:29:25,350
약물 오류, 절차 오류, 사무 오류,

475
00:29:25,350 --> 00:29:28,590
진단 오류 등이 있습니다.

476
00:29:28,590 --> 00:29:31,260
그래서 매우 조심해야 합니다.

477
00:29:31,260 --> 00:29:35,630
예를 들어, 수술실에서는 솔직히 가위가 몸 안에

478
00:29:35,630 --> 00:29:40,130
남는 경우는 보통 없지만, 훨씬 작은 것들, 예를

479
00:29:40,130 --> 00:29:45,930
들어 봉합 바늘이나 거즈 조각 같은 것들이 남을 수 있습니다.

480
00:29:45,930 --> 00:29:52,615
그래서 오늘날 대부분은 여전히 손으로 추적하고 있습니다.

481
00:29:52,615 --> 00:30:00,180
수술실에서는 이런 것을 추적하기 위한 체크리스트가 있습니다.

482
00:30:00,180 --> 00:30:04,170
무언가 빠지면 수술을 일시 중지해야 합니다.

483
00:30:04,170 --> 00:30:07,920
평균적으로 그 일시 중지는 거의 한 시간에 가깝습니다.

484
00:30:07,920 --> 00:30:11,040
그리고 환자에게 얼마나 위험한지 생각해 보십시오.

485
00:30:11,040 --> 00:30:14,650
그 항목을 찾기 위해 박테리아

486
00:30:14,650 --> 00:30:18,470
노출과 출혈 등이 발생할 수 있습니다.

487
00:30:18,470 --> 00:30:25,810
그래서 AI를 사용해 의사와 외과의를 도와 항목을 추적할 수

488
00:30:25,810 --> 00:30:29,510
있다면 매우 강력할 것입니다.

489
00:30:29,510 --> 00:30:30,860
이것은 단지 데모입니다.

490
00:30:30,860 --> 00:30:33,200
배포 시스템이 아닙니다.

491
00:30:33,200 --> 00:30:35,690
정확도 면에서는 아직 그 단계에 이르지 못했습니다.

492
00:30:35,690 --> 00:30:37,570
하지만 이것은 AI를 사용해

493
00:30:37,570 --> 00:30:42,560
이 경우 거즈 등을 세는 것이 가능하다는 것을 보여주는 데모입니다.

494
00:30:42,560 --> 00:30:49,090
이것은 AI가 인간이 보지 못하는 것을 보도록

495
00:30:49,090 --> 00:30:51,740
하는 예시일 뿐입니다.

496
00:30:51,740 --> 00:30:56,030
여기 정말 재미있는 또 다른 예가 있습니다.

497
00:30:56,030 --> 00:30:57,800
이걸 전에 보여줬는지 잘 모르겠네요.

498
00:30:57,800 --> 00:31:01,060
하지만 이건 제가 가장 좋아하는 시각적 착시 중

499
00:31:01,060 --> 00:31:03,620
하나인데, 제가 답을 바로 알려드리는 겁니다.

500
00:31:03,620 --> 00:31:07,030
체커보드 위쪽에 있는 두 사각형

501
00:31:07,030 --> 00:31:11,170
A와 B를 보면, 같은 그레이스케일이나 밝기를

502
00:31:11,170 --> 00:31:14,838
가졌다는 게 믿기 어려울 정도입니다.

503
00:31:14,838 --> 00:31:16,880
그리고 아래쪽을 보면, 아, 그렇구나 싶죠.

504
00:31:16,880 --> 00:31:18,310
물론 그렇습니다.

505
00:31:18,310 --> 00:31:19,400
그런데 왜 그런 걸까요?

506
00:31:19,400 --> 00:31:24,500
아래 그림을 보고 있어도 위 그림을

507
00:31:24,500 --> 00:31:28,820
보면 착시 현상이 생깁니다.

508
00:31:28,820 --> 00:31:29,680
왜 그럴까요?

509
00:31:29,680 --> 00:31:33,676
진화 과정에서 우리는 사물의 형태,

510
00:31:33,676 --> 00:31:39,800
빛의 방향, 그림자가 생기는 방식 등 일반적인 물리

511
00:31:39,800 --> 00:31:42,430
법칙에 따라 세상을

512
00:31:42,430 --> 00:31:48,380
추론하거나 이해하도록 미리 연결되어 있기 때문입니다.

513
00:31:48,380 --> 00:31:54,460
이것은 우리의 시각 발달에 깊이 자리 잡고 있습니다.

514
00:31:54,460 --> 00:31:59,480
그래서 다른 방식으로 보는 것이 어렵습니다.

515
00:31:59,480 --> 00:32:04,280
제가 말하고자 하는 것은 인간의 시각 시스템에는

516
00:32:04,280 --> 00:32:06,410
편향이 있다는 겁니다.

517
00:32:06,410 --> 00:32:10,360
그 편향은 진화적 구조에서 올 수도

518
00:32:10,360 --> 00:32:14,860
있고, 사회적 경험에서 올 수도 있으며,

519
00:32:14,860 --> 00:32:19,420
우리가 접하는 데이터에서 올 수도 있습니다.

520
00:32:19,420 --> 00:32:22,980
하지만 이러한 편향 중 일부는 해로울 수 있습니다.

521
00:32:22,980 --> 00:32:26,280
편향이 발생하면 특정 집단에게

522
00:32:26,280 --> 00:32:29,350
불공평해질 수 있습니다.

523
00:32:29,350 --> 00:32:30,580
커뮤니티입니다.

524
00:32:30,580 --> 00:32:32,530
그리고 우리는 이것을 인지해야 합니다.

525
00:32:32,530 --> 00:32:35,520
몇 년 전만 해도 얼굴 인식

526
00:32:35,520 --> 00:32:39,540
알고리즘이 좋지 않았고, 특정 피부색이나 심지어

527
00:32:39,540 --> 00:32:43,750
성별을 더 잘 인식하는 경향이 있었습니다.

528
00:32:43,750 --> 00:32:45,400
이것은 결과를 초래합니다.

529
00:32:45,400 --> 00:32:47,230
자율주행차를 생각해 보세요.

530
00:32:47,230 --> 00:32:53,070
그리고 많은 다른 의료 사례들도 생각해 보세요.

531
00:32:53,070 --> 00:32:56,100
그래서 우리는 이에 대해 경계해야 합니다.

532
00:32:56,100 --> 00:33:03,210
저는 AI 편향이 지금 사람들이 안고 있는 문제라고

533
00:33:03,210 --> 00:33:04,460
믿습니다.

534
00:33:04,460 --> 00:33:07,490
몇 년 전만 해도 이 문제는 너무 새로워서

535
00:33:07,490 --> 00:33:10,440
많은 사람들이 관심조차 두지 않았습니다.

536
00:33:10,440 --> 00:33:13,070
하지만 2025년으로 빠르게 넘어가

537
00:33:13,070 --> 00:33:14,880
보면, 이 문제를

538
00:33:14,880 --> 00:33:18,230
완전히 해결했다고는 말할 수 없지만, 개인적으로

539
00:33:18,230 --> 00:33:21,450
학계뿐만 아니라 산업계에서도 이렇게

540
00:33:21,450 --> 00:33:25,730
많은 사람들이 관심을 갖는 모습을 보니 훨씬 기쁩니다.

541
00:33:25,730 --> 00:33:28,860
그리고 또 다른 종류의 '보이지 않는 것'이 있습니다.

542
00:33:28,860 --> 00:33:30,810
이것이 흥미로운 점입니다.

543
00:33:30,810 --> 00:33:34,280
때로는 '보이지 않는 것'이 바로 우리가 원하는

544
00:33:34,280 --> 00:33:38,700
것이기도 합니다. 왜냐하면 프라이버시를 존중하고 싶기 때문입니다.

545
00:33:38,700 --> 00:33:45,350
그렇다면 사람들을 돕는 AI를 어떻게

546
00:33:45,350 --> 00:33:46,680
만들면서도,

547
00:33:46,680 --> 00:33:49,200
사람들이 보지 않길 원하는 것은 보지 않게 할 수 있을까요?

548
00:33:49,200 --> 00:33:50,960
이것은 매우 깊은 문제로,

549
00:33:50,960 --> 00:33:54,180
기술적인 문제이자 인간적인 문제이기도 합니다.

550
00:33:54,180 --> 00:33:57,120
기술적인 관점에서 보면, 머신러닝

551
00:33:57,120 --> 00:34:01,560
프라이버시를 고려하는 방법은 여러 가지가 있습니다.

552
00:34:01,560 --> 00:34:05,990
여기서는 시각적 접근 관점에서 몇 년 전에

553
00:34:05,990 --> 00:34:08,949
우리 연구실에서 환자 방이나

554
00:34:08,949 --> 00:34:14,679
환자 집에 스마트 카메라를 사용해 의사들이 더 잘 볼 수 있도록

555
00:34:14,679 --> 00:34:17,409
도운 논문을 소개합니다.

556
00:34:17,409 --> 00:34:20,920
하지만 그곳에서도 얼굴이나 전신

557
00:34:20,920 --> 00:34:27,050
정보, 심지어 집 자체와 같은 문제들을 인식해야 합니다.

558
00:34:27,050 --> 00:34:30,230
이것은 가능한 해결책들의 목록입니다.

559
00:34:30,230 --> 00:34:34,460
예를 들어, 블러 처리, 마스킹, 차원

560
00:34:34,460 --> 00:34:37,130
축소를 할 수 있고, 서버로

561
00:34:37,130 --> 00:34:41,750
모든 데이터를 보내지 않는 연합 학습(federated

562
00:34:41,750 --> 00:34:44,080
learning)이나

563
00:34:44,080 --> 00:34:49,550
암호화 같은 다른 접근법도 시도할 수 있습니다.

564
00:34:49,550 --> 00:34:51,620
자세히 설명하지는 않겠지만, 보여드리고

565
00:34:51,620 --> 00:34:53,420
싶은 연구가 하나 있습니다.

566
00:34:53,420 --> 00:34:56,330
제 연구는 아니지만, 정말 마음에 드는 연구입니다.

567
00:34:56,330 --> 00:35:02,560
사람들의 영상을 촬영해 행동을

568
00:35:02,560 --> 00:35:06,460
인식하면서도 프라이버시를

569
00:35:06,460 --> 00:35:09,740
존중하는 연구입니다.

570
00:35:09,740 --> 00:35:10,760
어떻게 하냐고요?

571
00:35:10,760 --> 00:35:15,310
예를 들어, 이 경우에는 장면에서

572
00:35:15,310 --> 00:35:20,200
움직이는 아이의 영상을 찍고 싶을 때입니다.

573
00:35:20,200 --> 00:35:22,610
이걸 할 수 있는 방법들이 있습니다.

574
00:35:22,610 --> 00:35:30,615
블러 처리하거나 초점을 흐리게 하거나 이런 방법들을 쓰면, 프라이버시는
보호할

575
00:35:30,615 --> 00:35:31,810
수 있지만,

576
00:35:31,810 --> 00:35:35,020
동시에 이 사람이 무엇을 하는지 알 수

577
00:35:35,020 --> 00:35:37,700
없을 정도로 정보가 손실됩니다.

578
00:35:37,700 --> 00:35:40,030
많은 응용 분야에서 핵심 목표는 바로

579
00:35:40,030 --> 00:35:42,410
이 사람이 무엇을 하는지 아는 것입니다.

580
00:35:42,410 --> 00:35:48,680
그래서 Carl 학생들이 주도한 이 연구에서는

581
00:35:48,680 --> 00:35:52,690
하드웨어와 소프트웨어를

582
00:35:52,690 --> 00:35:58,360
결합한 방식을 사용했습니다. 특별히

583
00:35:58,360 --> 00:36:06,940
시각 데이터를 특정 방식으로 필터링하는 렌즈를 직접 제작했죠.

584
00:36:06,940 --> 00:36:12,100
그래서 위쪽 행을 보면, 이 렌즈가 카메라에

585
00:36:12,100 --> 00:36:14,910
담는 영상은 프라이버시를

586
00:36:14,910 --> 00:36:17,080
상당히 보호합니다.

587
00:36:17,080 --> 00:36:18,960
사람의 얼굴도 보이지

588
00:36:18,960 --> 00:36:21,460
않고, 몸도 보이지 않습니다.

589
00:36:21,460 --> 00:36:24,750
하지만 소프트웨어와 연동되도록

590
00:36:24,750 --> 00:36:28,360
특별히 설계된 렌즈이기

591
00:36:28,360 --> 00:36:32,460
때문에, 위상 정보를 유지하면서

592
00:36:32,460 --> 00:36:35,880
움직임 정보나 인간 활동

593
00:36:35,880 --> 00:36:39,730
정보를 차단할 수 있습니다.

594
00:36:39,730 --> 00:36:42,100
정말 흥미로운 접근법입니다.

595
00:36:42,100 --> 00:36:45,400
하드웨어와 소프트웨어의 하이브리드인 셈이죠.

596
00:36:45,400 --> 00:36:48,570
사람을 보호하기 위해 보고 싶지만,

597
00:36:48,570 --> 00:36:51,790
프라이버시를 존중하기 위해 너무 많이 보고

598
00:36:51,790 --> 00:36:54,180
싶지 않은 중요한 응용 분야를

599
00:36:54,180 --> 00:36:55,570
목표로 합니다.

600
00:36:55,570 --> 00:36:57,910
그래서 저는 그 연구를 정말 좋아합니다.

601
00:36:57,910 --> 00:37:00,847
그 연구의 정신이 정말 마음에 듭니다.

602
00:37:00,847 --> 00:37:02,040
좋습니다.

603
00:37:02,040 --> 00:37:06,110
이번 강의 부분에서는 인간이 보지 못하는

604
00:37:06,110 --> 00:37:10,460
것을 보기 위해 AI를 구축할 때 고려해야 할 여러

605
00:37:10,460 --> 00:37:12,510
가지를 공유했습니다.

606
00:37:12,510 --> 00:37:16,580
때로는 새의 미세한 구분 같은 인간 능력을

607
00:37:16,580 --> 00:37:19,350
넘어서는 AI를 개발하기도 합니다.

608
00:37:19,350 --> 00:37:21,030
그것은 초인적인 능력입니다.

609
00:37:21,030 --> 00:37:23,580
때로는 인간이 잘하지 못하는 점을 알고 있습니다.

610
00:37:23,580 --> 00:37:26,820
우리는 편향이 있거나 주의 집중에 문제가 있죠.

611
00:37:26,820 --> 00:37:29,250
그래서 AI를 사용해 도움을 받고자 합니다.

612
00:37:29,250 --> 00:37:33,140
그리고 때로는 정말 아무도 보지 않길 원하는

613
00:37:33,140 --> 00:37:35,070
상황이 있습니다.

614
00:37:35,070 --> 00:37:37,790
그럴 때 AI를 어떻게 활용해 프라이버시를

615
00:37:37,790 --> 00:37:41,580
침해하지 않으면서 계속 도움을 줄 수 있을까요?

616
00:37:41,580 --> 00:37:45,840
그래서 AI가 매우 흥미롭고 강력한 도구라는 것을 알 수 있습니다.

617
00:37:45,840 --> 00:37:49,890
AI는 우리를 도울 수도 있고, 우리를 증폭시킬 수도 있습니다.

618
00:37:49,890 --> 00:37:54,870
그리고 만약 우리가 편견이나 문제가 있다면, AI도 그것을 증폭시킬 수
있습니다.

619
00:37:54,870 --> 00:37:58,570
그래서 AI를 만들 때는 기술적인

620
00:37:58,570 --> 00:38:02,380
관점뿐만 아니라 인간적인 관점도 매우 중요하며,

621
00:38:02,380 --> 00:38:05,200
예측을 연구하고 AI가

622
00:38:05,200 --> 00:38:09,430
인간에 미치는 영향을 이해하며 인간의 가치를

623
00:38:09,430 --> 00:38:12,980
존중하도록 이끄는 데 전념해야 합니다.

624
00:38:12,980 --> 00:38:17,600
이것이 두 번째 핵심 메시지입니다.

625
00:38:17,600 --> 00:38:21,100
그리고 다시 말해, 여러 협력자들과

626
00:38:21,100 --> 00:38:23,800
학생들이 이 작업에 참여했습니다.

627
00:38:23,800 --> 00:38:24,670
좋습니다.

628
00:38:24,670 --> 00:38:27,400
이제 AI를 만들어서 인간이 보고 싶어 하는 것을

629
00:38:27,400 --> 00:38:29,480
보게 하는 것에 대해 이야기해 보겠습니다.

630
00:38:29,480 --> 00:38:32,660
사실 우리는 보는 것을 넘어서서,

631
00:38:32,660 --> 00:38:35,630
보는 것과 행동하는 것을 연결할 것입니다.

632
00:38:35,630 --> 00:38:42,260
오늘날 AI에 대한 사회적 불안 중

633
00:38:42,260 --> 00:38:46,450
하나는 노동 문제입니다.

634
00:38:46,450 --> 00:38:51,170
많은 헤드라인 뉴스가 노동이 위협받고 있다고 말하죠.

635
00:38:51,170 --> 00:38:54,370
로봇이 일자리를 빼앗는다는 이야기입니다.

636
00:38:54,370 --> 00:38:58,080
하지만 진실은 상황이 복잡하다는 겁니다.

637
00:38:58,080 --> 00:39:01,690
일자리 변화 자체를 부정하는 것은 잘못된 것입니다.

638
00:39:01,690 --> 00:39:05,040
인류 역사에서 모든 기술적 변화는 노동

639
00:39:05,040 --> 00:39:08,700
시장 변화를 일으켰고, 그중 일부는 매우

640
00:39:08,700 --> 00:39:10,030
고통스러웠습니다.

641
00:39:10,030 --> 00:39:15,780
그 중 일부는 심지어 내전과 전쟁으로까지 이어질 수 있습니다.

642
00:39:15,780 --> 00:39:20,290
하지만 또한, 그 변화는 때때로 불가피합니다.

643
00:39:20,290 --> 00:39:24,610
잠깐 딴 얘기를 하자면요.

644
00:39:24,610 --> 00:39:27,690
우리가 듣고 있는 노동 위협 담론의

645
00:39:27,690 --> 00:39:31,680
대부분은 육체 노동을 생각하게 합니다.

646
00:39:31,680 --> 00:39:34,360
하지만 오늘날, 지난

647
00:39:34,360 --> 00:39:39,330
2년간 GenAI의 영향은 특히 소프트웨어

648
00:39:39,330 --> 00:39:42,730
엔지니어링과 사무실 내 분석

649
00:39:42,730 --> 00:39:49,240
업무 같은 화이트칼라 직업에 급격한 영향을 미치고 있습니다.

650
00:39:49,240 --> 00:39:53,020
그래서 분명히 노동 변화가 일어나고 있습니다.

651
00:39:53,020 --> 00:39:55,020
하지만 그 사이에, AI가

652
00:39:55,020 --> 00:39:59,320
도움이 될 수도 있다는 점도 인식해야 합니다.

653
00:39:59,320 --> 00:40:03,360
실제로 우리는 많은 상황에서 근본적으로 인력

654
00:40:03,360 --> 00:40:08,100
부족을 겪고 있습니다, 특히 노인 돌봄과 건강

655
00:40:08,100 --> 00:40:08,920
분야에서요.

656
00:40:08,920 --> 00:40:12,690
우선, 현대 의학이 발전하면서 인간의

657
00:40:12,690 --> 00:40:15,670
기대 수명이 늘어나고 있습니다.

658
00:40:15,670 --> 00:40:21,790
그리고 그것은 필연적으로 사회를 더 오래 사는 방향으로 밀어갑니다.

659
00:40:21,790 --> 00:40:23,230
그것은 좋은 일입니다.

660
00:40:23,230 --> 00:40:28,090
하지만 그 사이에 노동자 부족 문제가 있습니다.

661
00:40:28,090 --> 00:40:29,760
젊은 사람들이 일해야 합니다.

662
00:40:29,760 --> 00:40:33,490
그것이 이 사회를 활기차게 만드는 방법입니다.

663
00:40:33,490 --> 00:40:34,750
경제를 활기차게 만드는 거죠.

664
00:40:34,750 --> 00:40:37,170
그런데 누가 우리 노인들을 돌보고 있습니까?

665
00:40:37,170 --> 00:40:40,230
만성 질환자를 돌보는 사람들은 누구일까요?

666
00:40:40,230 --> 00:40:45,090
미국 병원에서도 특히 간호사를 중심으로

667
00:40:45,090 --> 00:40:49,110
의료 인력이 너무 많이 줄어들어

668
00:40:49,110 --> 00:40:55,080
환자를 도울 손과 귀, 눈이 부족한 상황입니다.

669
00:40:55,080 --> 00:40:59,280
그래서 이 '대체(replace)'라는 단어를 생각하기보다는

670
00:40:59,280 --> 00:41:03,420
AI가 보조(augment)하는 것으로 생각할 수 있습니다.

671
00:41:03,420 --> 00:41:07,140
제 수술실 예시에서 그 모습을 조금 보셨죠.

672
00:41:07,140 --> 00:41:12,560
실제로 건강 분야에는 눈이 부족한

673
00:41:12,560 --> 00:41:15,390
공간이 너무 많습니다.

674
00:41:15,390 --> 00:41:19,410
저는 이를 건강의 어두운 공간(dark spaces)이라고 부릅니다.

675
00:41:19,410 --> 00:41:24,920
수술실에서부터 병실, 제약, 가정 등 다양한 곳이

676
00:41:24,920 --> 00:41:26,250
포함됩니다.

677
00:41:26,250 --> 00:41:29,010
그렇다면 AI가 어떻게 도움을 줄 수 있을까요?

678
00:41:29,010 --> 00:41:31,520
이 부분은 Ihsan이 많은

679
00:41:31,520 --> 00:41:33,750
연구를 이끌고 있습니다.

680
00:41:33,750 --> 00:41:37,070
또한 zing과 함께 우리는

681
00:41:37,070 --> 00:41:40,830
스마트 센서와 머신러닝

682
00:41:40,830 --> 00:41:45,620
알고리즘을 결합해 건강 환경에서 중요한

683
00:41:45,620 --> 00:41:52,780
통찰을 얻고, 환자나 가족, 의사에게 적시에 알림을

684
00:41:52,780 --> 00:41:57,790
주는 'ambient intelligence'

685
00:41:57,790 --> 00:42:01,970
문제를 연구해왔습니다.

686
00:42:01,970 --> 00:42:05,620
더 자세한 내용은 몇 년 전에 발표한

687
00:42:05,620 --> 00:42:07,550
논문에 있습니다.

688
00:42:07,550 --> 00:42:10,340
몇 가지 예를 들어드리겠습니다.

689
00:42:10,340 --> 00:42:13,360
첫 번째 예는 코로나 이전부터

690
00:42:13,360 --> 00:42:16,360
시작된 손 위생 프로젝트입니다.

691
00:42:16,360 --> 00:42:21,730
손 위생은 병원 감염을 낮추는

692
00:42:21,730 --> 00:42:25,030
데 정말 중요합니다.

693
00:42:25,030 --> 00:42:28,810
병원 내 감염은 미국 병원에서

694
00:42:28,810 --> 00:42:32,110
환자 사망의 주요

695
00:42:32,110 --> 00:42:34,490
원인 중 하나입니다.

696
00:42:34,490 --> 00:42:39,520
연간 교통사고 사망자 수보다 세 배나 더 많은

697
00:42:39,520 --> 00:42:43,850
사람을 죽이며, 통제가 매우 어렵습니다.

698
00:42:43,850 --> 00:42:46,510
대부분의 세균은 환자 방에서 환자 방으로

699
00:42:46,510 --> 00:42:47,560
전파됩니다.

700
00:42:47,560 --> 00:42:50,630
그리고 나서 함께 증식하는 거죠.

701
00:42:50,630 --> 00:42:52,690
그럼 우리는 무엇을 해야 할까요?

702
00:42:52,690 --> 00:42:55,390
병원에서는 인간 감사관을 사용하려고 하지만,

703
00:42:55,390 --> 00:42:58,420
방금 말했듯이 간호사도 충분하지 않은데

704
00:42:58,420 --> 00:43:00,610
감사관을 고용하는 것은 더 어렵습니다.

705
00:43:00,610 --> 00:43:04,030
그리고 충분히 많이 고용할 수도 없습니다.

706
00:43:04,030 --> 00:43:05,620
사람은 피로를 느끼기 때문입니다.

707
00:43:05,620 --> 00:43:07,930
우리는 인간의 주의력 문제에 대해 이야기한 겁니다.

708
00:43:07,930 --> 00:43:12,160
그래서 이것은 그다지 효과적인 해결책이 아닙니다.

709
00:43:12,160 --> 00:43:16,150
RFID 같은 기술적 해결책도 있었습니다, 배지를

710
00:43:16,150 --> 00:43:17,170
부착하는 거죠.

711
00:43:17,170 --> 00:43:19,470
배지나 배지를 착용한

712
00:43:19,470 --> 00:43:25,170
사람이 싱크대나 손 위생용 손 소독기 근처에 있으면, 그

713
00:43:25,170 --> 00:43:28,210
사람이, 보통 의사나 간호사가

714
00:43:28,210 --> 00:43:31,600
손을 씻고 있다는 신호를 줍니다.

715
00:43:31,600 --> 00:43:33,340
하지만 그건 매우 비특이적입니다.

716
00:43:33,340 --> 00:43:34,440
보장할 수 없습니다.

717
00:43:34,440 --> 00:43:36,790
병실은 꽤 작고 복도도 좁아서,

718
00:43:36,790 --> 00:43:40,650
무언가 옆에 서 있다고 해서 손을 씻고 있다는

719
00:43:40,650 --> 00:43:42,130
뜻은 아닙니다.

720
00:43:42,130 --> 00:43:45,150
몇 년 전, 우리는 프라이버시를

721
00:43:45,150 --> 00:43:50,150
보호하기 위해 깊이 정보만 수집하는 스마트 센서를

722
00:43:50,150 --> 00:43:54,590
설치하는 프로젝트를 했습니다, 저 파란

723
00:43:54,590 --> 00:43:56,880
화면이나 파란 영상처럼요.

724
00:43:56,880 --> 00:44:01,890
그리고 컴퓨터 비전 알고리즘을 사용해 행동을 분류했습니다.

725
00:44:01,890 --> 00:44:04,770
그 사람이 손을 씻고 있는지 아닌지 말이죠.

726
00:44:04,770 --> 00:44:08,510
결과적으로, 알고리즘 출력과

727
00:44:08,510 --> 00:44:17,240
인간의 결과를 실제 정답과 비교해 보면, 알고리즘이 훨씬 더

728
00:44:17,240 --> 00:44:21,110
우수하고 일관성이 있다는

729
00:44:21,110 --> 00:44:23,780
것을 알 수 있습니다.

730
00:44:23,780 --> 00:44:29,720
거의 AI만큼 좋은 결과를 얻으려면 거의 같은 영상을 네

731
00:44:29,720 --> 00:44:35,520
명의 사람에게 보여줘야 하는데, 이는 현실적으로 불가능합니다.

732
00:44:35,520 --> 00:44:40,050
한 사람만 보면 탐지가 얼마나 드문지 알 수 있고, 이는

733
00:44:40,050 --> 00:44:41,070
좋지 않습니다.

734
00:44:41,070 --> 00:44:43,110
이것이 하나의 응용 사례입니다.

735
00:44:43,110 --> 00:44:48,620
우리가 작업한 또 다른 응용 분야는 ICU입니다.

736
00:44:48,620 --> 00:44:52,110
ICU는 환자들이 생사를 다투는 곳입니다.

737
00:44:52,110 --> 00:44:59,070
ICU는 미국 GDP의 1%가 지출되는 곳이기도 합니다.

738
00:44:59,070 --> 00:45:04,700
그래서 ICU를 최대한 안전하고 효과적으로 만드는 것이

739
00:45:04,700 --> 00:45:06,300
최우선 과제입니다.

740
00:45:06,300 --> 00:45:10,250
ICU의 목표 중 하나는 환자들을

741
00:45:10,250 --> 00:45:14,390
안전하게 ICU에서 퇴원시켜 단계별

742
00:45:14,390 --> 00:45:17,490
병동이나 집으로 보내는 것입니다.

743
00:45:17,490 --> 00:45:23,360
그래서 ICU에서 사람들이 가장 중요하게 배운 것 중 하나는

744
00:45:23,360 --> 00:45:26,900
환자들이 움직일 수 있도록 돕는 것입니다.

745
00:45:26,900 --> 00:45:30,180
우리가 동원(mobilization)이라고 부르는 적절한

746
00:45:30,180 --> 00:45:32,400
움직임은 회복에 실제로 중요합니다.

747
00:45:32,400 --> 00:45:34,770
하지만 이것은 매우 까다로운 상황입니다.

748
00:45:34,770 --> 00:45:36,690
간호사들의 도움이 필요합니다.

749
00:45:36,690 --> 00:45:40,980
의사들은 명령을 내려야 하고, 환자는 적절하게

750
00:45:40,980 --> 00:45:45,390
움직여야 하며, 지정된 시간에 해야 합니다.

751
00:45:45,390 --> 00:45:47,560
그리고 움직임을 평가해야 하는데, 이

752
00:45:47,560 --> 00:45:49,790
모든 것이 쉽지 않습니다, 그렇죠?

753
00:45:49,790 --> 00:45:54,220
그래서 우리는 Stanford와 Utah의 Intermountain

754
00:45:54,220 --> 00:45:57,730
병원과 협력하여 ICU에 스마트 센서를

755
00:45:57,730 --> 00:46:02,710
설치하고 의사들이 환자의 움직임, 특히 침대에서 일어나기, 침대에 눕기,

756
00:46:02,710 --> 00:46:05,925
의자에서 일어나기, 의자에 앉기 등 네

757
00:46:05,925 --> 00:46:08,300
가지 움직임을 모니터링할 수 있도록

758
00:46:08,300 --> 00:46:09,320
도왔습니다.

759
00:46:09,320 --> 00:46:12,350
이것들은 ICU 환자들에게 매우 중요합니다.

760
00:46:12,350 --> 00:46:15,080
우리에게는 당연한 일이지만,

761
00:46:15,080 --> 00:46:18,440
정말로 중요한 문제입니다.

762
00:46:18,440 --> 00:46:23,560
그리고 제가 도울 수 있는 것은 의사들에게 매우

763
00:46:23,560 --> 00:46:28,510
도움이 되는 감지와 예측을 하는 것입니다, 특히

764
00:46:28,510 --> 00:46:30,610
인력 부족 상황에서요.

765
00:46:30,610 --> 00:46:34,250
마지막이지만 결코 덜 중요하지 않은 예는 'aging in
place'입니다.

766
00:46:34,250 --> 00:46:38,510
이것은 여러 가지 이유로 매우 중요합니다.

767
00:46:38,510 --> 00:46:42,220
노인분들은 집에서 독립적이고 건강하게 살고

768
00:46:42,220 --> 00:46:43,600
싶어 하십니다.

769
00:46:43,600 --> 00:46:47,820
COVID 초기에 노인층에서

770
00:46:47,820 --> 00:46:54,250
많은 사망자가 발생했을 때가 기억납니다.

771
00:46:54,250 --> 00:46:59,700
병원이 과부하되고 과도하게 부담을 받는 것과 관련이 많아서,

772
00:46:59,700 --> 00:47:04,500
노인분들을 집에서 안전하고 건강하게 지키는

773
00:47:04,500 --> 00:47:06,730
것이 정말 중요합니다.

774
00:47:06,730 --> 00:47:10,410
스마트 센서를 사용하면 감염을 조기에 감지하는

775
00:47:10,410 --> 00:47:12,900
데 도움이 될 수 있습니다,

776
00:47:12,900 --> 00:47:16,390
특히 열화상 카메라나 이동성 감지, ICU에서

777
00:47:16,390 --> 00:47:19,080
방금 이야기한 것과 비슷하게요,

778
00:47:19,080 --> 00:47:22,050
또는 수면 패턴이나 식습관을

779
00:47:22,050 --> 00:47:23,320
이해하는 데도요.

780
00:47:23,320 --> 00:47:26,370
이 모든 것이 AI와 스마트

781
00:47:26,370 --> 00:47:29,700
센서가 가능한 영역입니다.

782
00:47:29,700 --> 00:47:33,990
그리고 마지막으로, 스마트 센서가 있어도 인력

783
00:47:33,990 --> 00:47:36,700
부족이 계속된다면 어떻게 할까요?

784
00:47:36,700 --> 00:47:40,740
스마트 센서는 정보를 수집하는

785
00:47:40,740 --> 00:47:46,010
시스템이지만, 환자를 돌보거나

786
00:47:46,010 --> 00:47:51,450
물이나 약을 가져다 줄 수는 없습니다.

787
00:47:51,450 --> 00:47:55,730
그래서 마지막 기술 주제는

788
00:47:55,730 --> 00:47:59,540
'embodied AI'인데,

789
00:47:59,540 --> 00:48:03,960
여기서 큰 부분은 로보틱스입니다.

790
00:48:03,960 --> 00:48:07,790
이 부분이 매우 흥미로운데, 인지와

791
00:48:07,790 --> 00:48:11,850
행동 사이의 연결고리를 완성하기 때문입니다.

792
00:48:11,850 --> 00:48:18,350
진화의 캄브리아기 폭발을 생각해 보면, 눈이 생기면서

793
00:48:18,350 --> 00:48:22,680
동물들이 움직이기 시작했습니다.

794
00:48:22,680 --> 00:48:26,420
로보틱스 분야는 보는 것과 행동하는 것을 연결하는

795
00:48:26,420 --> 00:48:29,310
고리를 완성할 수 있는 곳입니다.

796
00:48:29,310 --> 00:48:31,140
하지만 쉽지 않죠?

797
00:48:31,140 --> 00:48:34,730
로봇은 우리가 매우 흥분하는 만큼, 여전히

798
00:48:34,730 --> 00:48:36,360
매우, 매우 느립니다.

799
00:48:36,360 --> 00:48:38,850
로봇은 매우, 매우 서툴러요.

800
00:48:38,850 --> 00:48:45,730
일반화 가능한 상황에 적응하는 것이 매우 어렵습니다.

801
00:48:45,730 --> 00:48:50,170
오늘날 로봇 연구 분야에서 우리는 많은

802
00:48:50,170 --> 00:48:55,840
진전을 이루었고, 스탠포드는 확실히 로봇 학습의

803
00:48:55,840 --> 00:48:58,520
중심지 중 하나입니다.

804
00:48:58,520 --> 00:49:03,860
하지만 여전히 대부분의 연구는 제한된 환경에서

805
00:49:03,860 --> 00:49:07,240
이루어지고 있고, 단기

806
00:49:07,240 --> 00:49:15,070
과제인 집기와 놓기 작업에 국한되어 있으며, 일화적인 설정과 표준

807
00:49:15,070 --> 00:49:18,260
벤치마크의 부족이 있습니다.

808
00:49:18,260 --> 00:49:22,870
그래서 우리 연구실의 몇 가지 연구를 공유하겠습니다.

809
00:49:22,870 --> 00:49:25,540
몇 년 전 한 연구는 로봇을 야생

810
00:49:25,540 --> 00:49:29,120
환경으로 데려가는 방법을 살펴보았습니다.

811
00:49:29,120 --> 00:49:35,270
만약 우리가 미리 작업 세트를 지정해야 한다면, 만족스럽지 않습니다.

812
00:49:35,270 --> 00:49:37,970
반면에 오늘날의 LLM은 완전히

813
00:49:37,970 --> 00:49:39,560
야생 환경에 있습니다.

814
00:49:39,560 --> 00:49:41,090
무엇이든 이야기할 수 있죠.

815
00:49:41,090 --> 00:49:44,320
그래서 제 학생 몇 명이 이 간극을

816
00:49:44,320 --> 00:49:45,950
메우고 싶어 했습니다.

817
00:49:45,950 --> 00:49:50,410
이 아이디어는 로봇에게 열린 지시를 어떻게

818
00:49:50,410 --> 00:49:55,120
주느냐, 즉 닫힌 세계에서 모든 것을

819
00:49:55,120 --> 00:50:02,200
사전 학습하지 않고도 로봇이 어떤 작업을 수행할 수 있게 하는

820
00:50:02,200 --> 00:50:03,680
방법입니다.

821
00:50:03,680 --> 00:50:08,650
예를 들어, 훈련 세트가 '문을 열어라'라고 한다면,

822
00:50:08,650 --> 00:50:11,720
야생에는 그런 문들이 있습니다.

823
00:50:11,720 --> 00:50:17,830
그 문제에서 어떻게 진전을 이룰 수 있을까요?

824
00:50:17,830 --> 00:50:22,760
그래서 목표는 실제 환경에서의 일반화입니다.

825
00:50:22,760 --> 00:50:31,130
여기 결과 또는 전체 알고리즘이 있습니다.

826
00:50:31,130 --> 00:50:34,330
이게 그렇게 오류가 많은지는 모르겠지만, 어쨌든요.

827
00:50:34,330 --> 00:50:38,970
말씀하신 것은 이 로봇 팔에게 꽃을

828
00:50:38,970 --> 00:50:45,240
쓰러뜨리지 않도록 움직임 경로를 계획해서 서랍을 열라고

829
00:50:45,240 --> 00:50:46,990
하는 거죠.

830
00:50:46,990 --> 00:50:51,520
그리고 이 모든 명령은 사전 학습된 것이 아닙니다.

831
00:50:51,520 --> 00:50:57,840
그래서 우리는 최신 LLM과 시각 언어

832
00:50:57,840 --> 00:51:02,320
모델의 발전을 차용합니다.

833
00:51:02,320 --> 00:51:10,020
아이디어는 LLM과 VLM을 사용해 명령 세트를 만드는

834
00:51:10,020 --> 00:51:11,980
것입니다.

835
00:51:11,980 --> 00:51:14,250
그리고 시각 언어 모델을 사용해

836
00:51:14,250 --> 00:51:17,680
환경을 인식하거나 이해하는 데 도움을 받습니다.

837
00:51:17,680 --> 00:51:21,480
그다음에 그걸 모션 플래닝 맵으로 바꿔서

838
00:51:21,480 --> 00:51:24,790
로봇 팔이 실행할 수 있게 합니다.

839
00:51:24,790 --> 00:51:29,290
그리고 저희가 LLM과 VLM을 함께 사용하기

840
00:51:29,290 --> 00:51:36,110
때문에, 로봇을 닫힌 세계에서 훈련하는 문제를 없애고 더 일반화되거나

841
00:51:36,110 --> 00:51:39,890
실제 환경에서도 작동할 수 있게 합니다.

842
00:51:39,890 --> 00:51:44,510
자세한 내용은 '열린 상단 서랍'이라는 명령이

843
00:51:44,510 --> 00:51:45,690
들어오는 겁니다.

844
00:51:45,690 --> 00:51:51,240
LLM이 이걸 문자 그대로 코드로 변환합니다.

845
00:51:51,240 --> 00:51:55,220
그리고 '서랍'이나 '손잡이'

846
00:51:55,220 --> 00:52:02,700
같은 명령 때문에 이 정보를 VLM 모델에 보냅니다.

847
00:52:02,700 --> 00:52:07,260
그 모델이 장면에서 서랍과 손잡이를 감지합니다.

848
00:52:07,260 --> 00:52:13,700
그 결과 정보를 업데이트하고, 모션

849
00:52:13,700 --> 00:52:16,740
맵도 갱신합니다.

850
00:52:16,740 --> 00:52:19,730
이것은 히트맵으로 표현되어 로봇 팔이

851
00:52:19,730 --> 00:52:22,020
어디에 집중해야 하는지, 어디에

852
00:52:22,020 --> 00:52:25,970
집중하지 말아야 하는지를 보여줍니다. 그리고 나서

853
00:52:25,970 --> 00:52:30,030
또 다른 명령을 주는데, 꽃병을 조심하라는 겁니다.

854
00:52:30,030 --> 00:52:34,000
다시 말해, LLM을 통해 같은 과정을

855
00:52:34,000 --> 00:52:39,650
거쳐 코드를 작성하거나 생성하고, VLM 모델에 전달합니다.

856
00:52:39,650 --> 00:52:45,130
VLM 모델이 객체를 감지한 후 모션 플래닝 맵을

857
00:52:45,130 --> 00:52:45,920
업데이트합니다.

858
00:52:45,920 --> 00:52:49,040
이 경우에는 피해야 하므로 긍정이

859
00:52:49,040 --> 00:52:51,050
아니라 부정입니다.

860
00:52:51,050 --> 00:52:53,840
그리고 이전 맵과 결합하면 어디를 피하고

861
00:52:53,840 --> 00:52:57,950
어디로 가야 하는지 알 수 있는 히트맵을 얻습니다.

862
00:52:57,950 --> 00:53:04,140
결국 우리가 하는 것은 모션 플래닝 맵에 대해 이 작업을 수행하는

863
00:53:04,140 --> 00:53:04,640
겁니다.

864
00:53:04,640 --> 00:53:08,230
회전, 그리퍼, 속도에 대해서도

865
00:53:08,230 --> 00:53:11,650
하고, 이것이 결과입니다. 사실, 이걸

866
00:53:11,650 --> 00:53:13,360
보여드리겠습니다.

867
00:53:13,360 --> 00:53:18,560
이것이 로봇의 실제 결과입니다.

868
00:53:18,560 --> 00:53:21,675
그리고 여러 다른 작업에 대해서도 이렇게 합니다, 맞죠?

869
00:53:21,675 --> 00:53:26,120
관절형 객체 조작에도 할 수 있습니다.

870
00:53:26,120 --> 00:53:27,345
여기 다양한

871
00:53:30,670 --> 00:53:33,640
예시들이 있습니다.

872
00:53:33,640 --> 00:53:37,470
냅킨이나 바닥 쓸기 같은 거죠.

873
00:53:37,470 --> 00:53:38,530
이게 뭐죠?

874
00:53:38,530 --> 00:53:44,670
토스트를 꺼내고, 테이블을 세팅하고, 온라인

875
00:53:44,670 --> 00:53:48,190
방해 요소도 처리하는 겁니다.

876
00:53:48,190 --> 00:53:50,710
이게 한 가지 연구입니다.

877
00:53:50,710 --> 00:53:54,810
또 다른 연구를 간단히 보여드리자면,

878
00:53:54,810 --> 00:54:03,420
전반적인 로봇 연구는 아직 좋은 벤치마크가 부족합니다.

879
00:54:03,420 --> 00:54:10,240
실험실에서는 계속 실험하지만, 실제 세계는 훨씬 더

880
00:54:10,240 --> 00:54:14,400
복잡하고 불확실하며, 변동성이 크고,

881
00:54:14,400 --> 00:54:17,140
상호작용적이고 사회적이며,

882
00:54:17,140 --> 00:54:20,400
다중 작업이 많다는

883
00:54:20,400 --> 00:54:23,020
걸 알고 있습니다.

884
00:54:23,020 --> 00:54:26,850
자연어와 컴퓨터 비전 분야가 훈련과

885
00:54:26,850 --> 00:54:33,060
벤치마크를 위한 중요한 대규모 데이터셋 구축으로 많은

886
00:54:33,060 --> 00:54:36,460
혜택을 본 것도 알고 있습니다.

887
00:54:36,460 --> 00:54:40,680
그래서 저희 연구실에서는 생태학적

888
00:54:40,680 --> 00:54:47,250
로봇 학습 환경을 구축하고, 연구자들이 크고 다양한

889
00:54:47,250 --> 00:54:52,590
활동 집합에 대해 벤치마크할 수 있도록

890
00:54:52,590 --> 00:54:57,190
장려하는 프로젝트를 진행하고 있습니다.

891
00:54:57,190 --> 00:55:00,280
이것이 바로 일상 가사

892
00:55:00,280 --> 00:55:03,330
활동을 위한 가상 상호작용

893
00:55:03,330 --> 00:55:06,330
생태 환경에서의 행동

894
00:55:06,330 --> 00:55:08,130
벤치마크입니다.

895
00:55:08,130 --> 00:55:11,160
이 강의가 인간의 가치와 관련이 많기

896
00:55:11,160 --> 00:55:15,240
때문에 질문이 하나 있습니다. 로봇이 어떤 작업을

897
00:55:15,240 --> 00:55:17,830
해야 하는지는 누가 결정하나요?

898
00:55:17,830 --> 00:55:19,830
로봇 연구를 하는 대학원생들은

899
00:55:19,830 --> 00:55:22,600
보통 두 가지 작업만 원합니다.

900
00:55:22,600 --> 00:55:25,440
하나는 세탁, 다른 하나는 식기세척기입니다.

901
00:55:25,440 --> 00:55:31,790
좋습니다만, 대학원 이후에는 로봇에게 어떤 작업을

902
00:55:31,790 --> 00:55:34,200
시켜야 할까요?

903
00:55:34,200 --> 00:55:38,760
그래서 우리가 작업 목록을 만드는 대신, 인간 중심

904
00:55:38,760 --> 00:55:44,390
설문조사를 통해 사람들에게 물어봤습니다—아, 죄송합니다, 로봇에게가

905
00:55:44,390 --> 00:55:46,000
아니라

906
00:55:46,000 --> 00:55:47,660
사람들에게요—[웃음]—로봇이

907
00:55:47,660 --> 00:55:49,920
어떤 도움을 주길 원하는지요?

908
00:55:49,920 --> 00:55:52,100
테스트해 보겠습니다.

909
00:55:52,100 --> 00:55:57,320
로봇이 주방 바닥 청소를 도와주길 원하시나요?

910
00:55:57,320 --> 00:55:58,700
예 아니오로 대답하세요.

911
00:55:58,700 --> 00:56:00,320
좋습니다.

912
00:56:00,320 --> 00:56:03,440
보통 사람들은 예라고 할 겁니다.

913
00:56:03,440 --> 00:56:05,600
눈 치우기?

914
00:56:05,600 --> 00:56:06,440
좋습니다.

915
00:56:06,440 --> 00:56:07,555
세탁물 개기?

916
00:56:07,555 --> 00:56:08,360
네.

917
00:56:08,360 --> 00:56:09,950
좋습니다.

918
00:56:09,950 --> 00:56:11,460
아침 준비 중이신가요?

919
00:56:11,460 --> 00:56:11,960
아니요.

920
00:56:11,960 --> 00:56:12,710
네.

921
00:56:12,710 --> 00:56:16,490
보시다시피, 답변이 섞여 나오고 있죠?

922
00:56:16,490 --> 00:56:18,790
크리스마스 선물 열기는 어떤가요?

923
00:56:18,790 --> 00:56:19,490
아니요.

924
00:56:19,490 --> 00:56:21,150
정확합니다.

925
00:56:21,150 --> 00:56:22,600
사람들은 다릅니다.

926
00:56:22,600 --> 00:56:25,320
사실 로봇이 이걸 꽤 잘할 수

927
00:56:25,320 --> 00:56:28,970
있다고 생각하지만, 우리는 원하지 않습니다.

928
00:56:28,970 --> 00:56:31,400
우리가 요청하는 작업 중 하나는 결혼 반지 사기입니다.

929
00:56:31,400 --> 00:56:33,220
상상할 수 있나요?

930
00:56:33,220 --> 00:56:37,330
그래서 우리가 한 것은, 인간의 선호를

931
00:56:37,330 --> 00:56:39,380
존중하고자 했습니다.

932
00:56:39,380 --> 00:56:42,970
미국과 유럽 노동부의

933
00:56:42,970 --> 00:56:48,410
정부 설문조사를 모아서 수천

934
00:56:48,410 --> 00:56:55,190
개의 일상 활동 작업을 정리했습니다.

935
00:56:55,190 --> 00:56:59,270
그리고 온라인에서 사람들을 찾았습니다.

936
00:56:59,270 --> 00:57:02,030
가능한 한 다양하게 하려고

937
00:57:02,030 --> 00:57:05,690
했지만, 개선할 여지가 있다고 생각합니다.

938
00:57:05,690 --> 00:57:08,450
하지만 1,400명을 찾았습니다.

939
00:57:08,450 --> 00:57:12,700
이 작업들에 답변하고 로봇이 도움을 주길 원하는

940
00:57:12,700 --> 00:57:16,790
작업을 알려주면, 그걸 순위 매겼습니다.

941
00:57:16,790 --> 00:57:20,830
보시다시피 대학원생들처럼, 사람들은 청소,

942
00:57:20,830 --> 00:57:25,610
특히 화장실 청소, 바닥 청소에 로봇이 도움을

943
00:57:25,610 --> 00:57:30,150
주길 원하지만, 스쿼시를 대신해주거나 결혼

944
00:57:30,150 --> 00:57:34,390
반지를 사주거나 아기 시리얼을 섞어주는

945
00:57:34,390 --> 00:57:36,550
것은 원하지 않습니다.

946
00:57:36,550 --> 00:57:38,430
인간에게 감정적이거나

947
00:57:38,430 --> 00:57:42,370
사회적으로 중요한 작업들이 많습니다.

948
00:57:42,370 --> 00:57:51,810
그래서 우리의 목표는 먼저 원칙에 따라 로봇을 훈련시킬 1,000개의 작업을
결정하는

949
00:57:51,810 --> 00:57:57,030
것입니다. 그 작업들은 인간이 도움을 받고

950
00:57:57,030 --> 00:57:59,950
싶어 하는 작업들입니다.

951
00:57:59,950 --> 00:58:05,010
그걸 염두에 두고, 실제로 가상 환경을

952
00:58:05,010 --> 00:58:07,060
만들어야 했습니다.

953
00:58:07,060 --> 00:58:15,900
우리는 레스토랑, 아파트, 식료품점, 사무실 등 50가지

954
00:58:15,900 --> 00:58:19,410
실제 환경에서 3D

955
00:58:19,410 --> 00:58:24,820
장면을 스캔하거나 획득했습니다.

956
00:58:24,820 --> 00:58:29,980
그리고 이 숫자는 업데이트 전이지만, 10,

957
00:58:29,980 --> 00:58:34,000
000개 이상의 3D 객체 자산을

958
00:58:34,000 --> 00:58:37,930
획득했으며, 관절성, 변형

959
00:58:37,930 --> 00:58:41,160
가능성 등 다양한 속성을

960
00:58:41,160 --> 00:58:43,030
포함하고 있습니다.

961
00:58:43,030 --> 00:58:47,620
그리고 시뮬레이션 환경을 구축해야 했습니다.

962
00:58:47,620 --> 00:58:50,350
많은 사람들이 시뮬레이션 환경을 만들었죠.

963
00:58:50,350 --> 00:58:51,730
잠시 건너뛰겠습니다.

964
00:58:51,730 --> 00:58:54,810
하지만 우리 시뮬레이션 환경은 NVIDIA의

965
00:58:54,810 --> 00:58:58,090
Omniverse 그룹과 협력하여 만들었습니다.

966
00:58:58,090 --> 00:59:03,010
우리는 물리적, 지각적, 그리고

967
00:59:03,010 --> 00:59:06,390
상호작용적으로 고품질의 시뮬레이션

968
00:59:06,390 --> 00:59:11,700
환경을 구축하려 했고, 특히 열 투과성,

969
00:59:11,700 --> 00:59:14,340
변형 가능성

970
00:59:14,340 --> 00:59:18,550
같은 물리적 효과를 고려했습니다.

971
00:59:18,550 --> 00:59:22,830
또한 인간 사용자 연구를 통해 지각적

972
00:59:22,830 --> 00:59:27,740
현실성 면에서 우리 환경을 다른 환경과

973
00:59:27,740 --> 00:59:29,730
비교 평가했습니다.

974
00:59:29,730 --> 00:59:32,840
여기 천이나 액체

975
00:59:32,840 --> 00:59:39,270
같은 물리적 상호작용 예시들이 있습니다.

976
00:59:39,270 --> 00:59:43,350
이 작업에는 많은 미묘한 부분들이 포함되어 있습니다.

977
00:59:43,350 --> 00:59:48,530
자, 제가 그냥 빨리 넘길게요. 이것들은 다른 연구들과

978
00:59:48,530 --> 00:59:51,030
비교한 벤치마크 결과입니다.

979
00:59:51,030 --> 00:59:51,530
네.

980
00:59:51,530 --> 00:59:54,410
제가 빨리 넘기겠습니다.

981
00:59:54,410 --> 00:59:58,320
이것은 사실 우리 연구실에서

982
00:59:58,320 --> 01:00:04,790
진행 중인 작업인데, 행동을 이용해서 로봇

983
01:00:04,790 --> 01:00:08,360
학습을 돕고, 더 흥미로운

984
01:00:08,360 --> 01:00:11,960
데이터를 수집하며,

985
01:00:11,960 --> 01:00:17,700
심지어 인지 연구에도 활용하고 있습니다.

986
01:00:17,700 --> 01:00:19,253
제가 빨리 넘기겠습니다.

987
01:00:23,410 --> 01:00:28,150
여러분께 공유하고 싶은 한 가지는,

988
01:00:28,150 --> 01:00:33,670
오늘날의 알고리즘으로는 아직 행동 과제를 수행할

989
01:00:33,670 --> 01:00:35,990
수 없다는 점입니다.

990
01:00:35,990 --> 01:00:41,320
이 모든 역할 중에서 가장 위에 있는 역할이 우리가

991
01:00:41,320 --> 01:00:45,110
로봇이 할 수 있기를 바라는 역할입니다.

992
01:00:45,110 --> 01:00:47,210
특권 정보를 전혀 주지 않고,

993
01:00:47,210 --> 01:00:50,750
로봇을 환경에 바로 투입해서 이 과제들을 수행하게 하는 겁니다.

994
01:00:50,750 --> 01:00:53,710
우리는 오늘날의 로봇 알고리즘으로

995
01:00:53,710 --> 01:00:57,620
세 가지 행동 과제를 벤치마킹했는데,

996
01:00:57,620 --> 01:01:00,260
성능은 거의 0에 가깝습니다.

997
01:01:00,260 --> 01:01:06,280
하지만 더 많은 특권 정보를 주거나, 마법

998
01:01:06,280 --> 01:01:08,530
같은 움직임이나

999
01:01:08,530 --> 01:01:15,290
완벽한 기억 같은 가정을 해서 과제를 단순화하면,

1000
01:01:15,290 --> 01:01:18,130
성능이 점점 좋아집니다.

1001
01:01:18,130 --> 01:01:23,110
위쪽 행만 보시면, 오늘날 로봇들이 얼마나

1002
01:01:23,110 --> 01:01:25,780
실망스러운지 알 수 있습니다.

1003
01:01:25,780 --> 01:01:27,720
하지만 대학원생 여러분께는

1004
01:01:27,720 --> 01:01:33,750
영감을 드리고 싶습니다. 왜냐하면 성장할 여지가 많다는 뜻이니까요.

1005
01:01:33,750 --> 01:01:35,070
네.

1006
01:01:35,070 --> 01:01:37,360
이것들은 우리 연구실의 다른 논문들입니다.

1007
01:01:37,360 --> 01:01:39,780
사실 이 부분은 충분히 이야기한

1008
01:01:39,780 --> 01:01:42,035
것 같아서, 빨리 넘기겠습니다.

1009
01:01:44,910 --> 01:01:50,040
참고로, 우리는 디지털 환경과 실제 환경 모두에서

1010
01:01:50,040 --> 01:01:52,830
행동의 디지털 트윈을 만들고

1011
01:01:52,830 --> 01:01:56,340
있는데, 이것은 실제에서

1012
01:01:56,340 --> 01:02:00,880
시뮬레이션으로의 전이를 테스트하는 좋은 방법입니다.

1013
01:02:00,880 --> 01:02:03,730
다시 말하지만, 이것은 아직 해결되지 않은 문제이고,

1014
01:02:03,730 --> 01:02:07,720
아직 갈 길이 멉니다.

1015
01:02:07,720 --> 01:02:09,930
특히 이 경우에는, 속도를

1016
01:02:09,930 --> 01:02:13,590
높이지 않고도 이 로봇이 얼마나

1017
01:02:13,590 --> 01:02:15,580
느린지 볼 수 있습니다.

1018
01:02:15,580 --> 01:02:17,515
이 로봇은 방을 청소하려고 하고 있습니다.

1019
01:02:21,200 --> 01:02:21,710
네.

1020
01:02:21,710 --> 01:02:23,420
좋습니다.

1021
01:02:23,420 --> 01:02:27,590
이것은 이 로봇이 저지르는 몇

1022
01:02:27,590 --> 01:02:30,890
가지 실수들입니다. 예를

1023
01:02:30,890 --> 01:02:35,120
들어, 병을 집지 못하거나, 아까는

1024
01:02:35,120 --> 01:02:41,720
잘못된 방향으로 가서 잘못된 곳에 물건을

1025
01:02:41,720 --> 01:02:42,690
놓았습니다.

1026
01:02:42,690 --> 01:02:45,420
아직도 실수가 많습니다.

1027
01:02:45,420 --> 01:02:50,150
네, 제가 빨리 넘기겠습니다.

1028
01:02:50,150 --> 01:02:53,330
우리는 이 환경을 시각 장애

1029
01:02:53,330 --> 01:02:56,040
환자 연구에도 활용하고

1030
01:02:56,040 --> 01:02:59,630
있는데, 환자들을 안전한 환경에

1031
01:02:59,630 --> 01:03:03,950
두고 연구할 수 있는 좋은 방법입니다.

1032
01:03:03,950 --> 01:03:07,950
마지막으로 보여드리고 싶은 정말 멋진

1033
01:03:07,950 --> 01:03:12,740
작업이 하나 있는데, 이것이 제가 보여드릴

1034
01:03:12,740 --> 01:03:20,060
마지막 기술 작업입니다. 우리는 심리학자와 의사들과 협력하여

1035
01:03:20,060 --> 01:03:25,100
뇌파를 이용해 로봇을 제어하는 연구를 하고

1036
01:03:25,100 --> 01:03:26,040
있습니다.

1037
01:03:26,040 --> 01:03:28,790
여기 보시는 것은

1038
01:03:28,790 --> 01:03:34,380
대학원생이 EEG 캡을 쓰고,

1039
01:03:34,380 --> 01:03:38,130
생각만으로 로봇 팔이

1040
01:03:38,130 --> 01:03:42,440
일본 요리를 만드는

1041
01:03:42,440 --> 01:03:43,740
시연입니다.

1042
01:03:43,740 --> 01:03:47,250
침습적인 뇌 제어가 전혀 없습니다.

1043
01:03:47,250 --> 01:03:49,530
이것은 전기 신호에서 나온 것입니다.

1044
01:03:49,530 --> 01:03:53,580
우리가 해야 할 일은 이러한 생각들을 사전 학습하는 것입니다.

1045
01:03:53,580 --> 01:03:59,030
예를 들어, 로봇 팔을 들어 올리기, 놓기, 떨어뜨리기

1046
01:03:59,030 --> 01:04:01,680
등으로 사전 학습해야 합니다.

1047
01:04:01,680 --> 01:04:04,640
그리고 그렇게 하면, 이 파동을 기반으로

1048
01:04:04,640 --> 01:04:06,750
한 완전한 식사가 완성됩니다.

1049
01:04:06,750 --> 01:04:08,820
이것은 정말 공상 과학 영화 같은 일입니다.

1050
01:04:08,820 --> 01:04:11,640
그리고 이것은 작년에 일어났습니다.

1051
01:04:11,640 --> 01:04:16,840
그래서 저는 시각, 인지, 로봇 공학을 결합하고 임상

1052
01:04:16,840 --> 01:04:19,630
환경에서 사람들을 돕는 이 모든

1053
01:04:19,630 --> 01:04:23,180
방향에 대해 매우 기대하고 있습니다.

1054
01:04:23,180 --> 01:04:25,450
이것이 바로 중증

1055
01:04:25,450 --> 01:04:29,530
마비 환자들을 돕는 미래입니다.

1056
01:04:29,530 --> 01:04:34,000
네, 그래서 행동 프로젝트는 사람들을 보조하는

1057
01:04:34,000 --> 01:04:37,150
데 정말 초점을 맞추고 있습니다.

1058
01:04:37,150 --> 01:04:41,690
이것은 대규모 다양성 벤치마크입니다.

1059
01:04:41,690 --> 01:04:45,670
그리고 현실적이고 생태학적인 물리학과 인지를

1060
01:04:45,670 --> 01:04:46,880
갖추고 있습니다.

1061
01:04:46,880 --> 01:04:52,510
마지막으로 전하고 싶은 메시지는, 우리는 단순히 AI가 무언가를

1062
01:04:52,510 --> 01:04:57,280
하거나 보게 만드는 것뿐만 아니라, 진정으로 사람들을 돕기

1063
01:04:57,280 --> 01:05:00,400
위해 AI를 만들고 싶다는 것입니다.

1064
01:05:00,400 --> 01:05:05,920
AI가 인간을 대체하는 도구가 아니라 인간을 보조하고

1065
01:05:05,920 --> 01:05:10,050
향상시키는 도구가 되는 것이 매우 중요합니다.
