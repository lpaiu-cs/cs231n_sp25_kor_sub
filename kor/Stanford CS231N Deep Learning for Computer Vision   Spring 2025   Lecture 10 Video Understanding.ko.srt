1
00:00:05,600 --> 00:00:09,040
강의 초반에 몇몇 게스트 강사분들을

2
00:00:09,760 --> 00:00:15,120
초청해서 그분들이 아주 잘 아시는 주제로 단일

3
00:00:15,120 --> 00:00:18,960
강의를 하실 거라고 말씀드렸습니다. 그리고 오늘 그 첫 번째

4
00:00:18,960 --> 00:00:24,880
강의를 하게 되어 매우 기쁩니다. 그럼 소개하겠습니다. Ruohan Gao
박사님입니다.

5
00:00:24,880 --> 00:00:29,440
그분은 메릴랜드 대학교

6
00:00:30,080 --> 00:00:35,280
컴퓨터과학과의 조교수입니다. 그리고 그곳에서 Multisensory
Machine Intelligence Lab을 이끌고 계십니다.
2022년부터 2023년까지

7
00:00:35,280 --> 00:00:44,160
CS231N 강사로 활동하셨습니다. 그 기간 동안 Fei-Fei Li,
Jiajun Wu, Silvio Savarese와

8
00:00:45,280 --> 00:00:49,520
함께 포닥 과정을 마치셨습니다. 그럼 바로 Ruohan 박사님께서

9
00:00:49,520 --> 00:00:53,680
오늘 발표를 진행하겠습니다.

10
00:00:53,680 --> 00:01:00,400
네, 감사합니다. 안녕하세요, 여러분. CS231N 강의에 다시

11
00:01:00,400 --> 00:01:05,840
오게 되어 정말 기쁩니다. 저는 Ruohan이고, [? Zen ?]가 소개한
것처럼요. 보시다시피 저는 멀티모달 분야에 매우 관심이 많습니다. 즉,
비전뿐 아니라 오디오, 촉각 같은 다른 감각 모달리티도 활용해서 우리
인간처럼

12
00:01:05,840 --> 00:01:10,560
다중 감각 세계를 인지하고 이해하며 상호작용하는 방법에 대해 연구합니다.
물론 비전이

13
00:01:10,560 --> 00:01:16,160
가장 중요한

14
00:01:16,160 --> 00:01:21,280
모달리티입니다.

15
00:01:21,280 --> 00:01:25,120
그래서 이 강의가 컴퓨터 비전을 위한 딥러닝 강의인 거죠. 지금까지 여러분은
이미지

16
00:01:25,120 --> 00:01:30,560
분류에 익숙해졌을 겁니다. 이런 2D 이미지를 보고 개인지

17
00:01:30,560 --> 00:01:38,880
고양이인지, 트럭인지 비행기인지 라벨을 붙이는 작업이죠. 이것이 2D 기반
이미지

18
00:01:38,880 --> 00:01:43,520
분류입니다. 지난 강의에서 이미지에 할 수 있는 다른 작업들도 배웠을
겁니다.

19
00:01:43,520 --> 00:01:47,920
단순히 고양이인지 아닌지

20
00:01:47,920 --> 00:01:53,120
한 라벨만 붙이는 게 아니라, 의미론적 분할을 통해 그림을 여러 부분과 구성
요소로 나누고, 잔디가 어디인지, 고양이가 어디인지, 나무가 어디인지 같은
의미를 부여할 수 있습니다. 또 이미지에서 객체를 감지해 개가

21
00:01:53,120 --> 00:01:59,120
어디 있는지, 고양이가 어디 있는지 바운딩

22
00:01:59,120 --> 00:02:02,880
박스를 그릴 수도 있습니다.

23
00:02:02,880 --> 00:02:08,880
그리고 인스턴스 분할도 할 수 있는데,

24
00:02:08,880 --> 00:02:13,040
이는 카테고리뿐 아니라 같은

25
00:02:13,040 --> 00:02:18,720
카테고리 내 두 마리 개 각각에 대해

26
00:02:18,720 --> 00:02:22,560
분할 마스크를 만드는 겁니다. 이것이 인스턴스 분할입니다.

27
00:02:22,560 --> 00:02:28,880
이처럼 2D 이미지를 기반으로 분류, 인식 작업을 많이 할 수 있습니다.

28
00:02:28,880 --> 00:02:34,480
하지만 컴퓨터 비전 시스템이 할 수 있는 것은 이것만이 아닙니다. 그리고
우리 세계는 이렇게

29
00:02:34,480 --> 00:02:41,440
정적인 것만 있는 게 아닙니다. 이 이미지를 보면, 지금까지 배운

30
00:02:41,440 --> 00:02:49,840
도구들로 이 공간이 거실임을 분류하는

31
00:02:49,840 --> 00:02:55,760
모델을 훈련할 수 있을 겁니다. 또한 여러 도구들이 있습니다. 여러분은 이게
개이고, 이게 아기라는 것을 보기

32
00:02:55,760 --> 00:03:00,800
위해 바운딩 박스를 그리는 법을 배웠습니다. 또한, 세그멘테이션 마스크를
사용해서 이미지 내에서

33
00:03:00,800 --> 00:03:05,920
감지한 객체들이 어디에 있는지 확인할 수도 있습니다.

34
00:03:05,920 --> 00:03:10,400
오늘은 비디오 이해에 집중할 것입니다. 좀 더 공식적으로, 비디오란
무엇일까요?

35
00:03:10,400 --> 00:03:16,480
기본적으로 비디오는 2D 이미지에 시간이라는 차원이 더해진 것입니다.
추가적인 시간 차원이 존재합니다. 그래서 이제 우리는 3D 이미지뿐만

36
00:03:16,480 --> 00:03:23,520
아니라 4D 문제도 다루고 있습니다. 여기서 3은 H, W,

37
00:03:25,600 --> 00:03:30,080
그리고 T입니다. T는 시간적 차원입니다. H와 W는 공간적 차원입니다.
이제 우리는 이런 이미지와

38
00:03:30,080 --> 00:03:37,840
비디오를 비디오 프레임들의 볼륨으로 간주합니다.

39
00:03:37,840 --> 00:03:43,120
예를 들어, 비디오 분류 작업이 있습니다. 이미지 분류와 비슷합니다. 이런
비디오가

40
00:03:43,120 --> 00:03:50,320
주어집니다. 어떤 사람이 달리고 있습니다. 이 비디오를 입력으로

41
00:03:50,320 --> 00:03:54,160
받습니다. 그리고 딥러닝 모델 같은 어떤 모델을 학습시키고자 합니다. 우리는
이 사람이 수영을 하는지, 달리기를

42
00:03:54,160 --> 00:03:58,320
하는지, 점프를 하는지, 아니면 어떤 행동을 하는지

43
00:03:58,320 --> 00:04:04,560
단지 이 시간에 따른 비디오 프레임 스트림만으로 분류하고자 합니다.

44
00:04:04,560 --> 00:04:09,440
그리고 이전 강의에서, 여러분은 이미 교차 엔트로피

45
00:04:09,440 --> 00:04:13,920
손실 같은 손실 함수들을 배웠을 겁니다. 그리고 이미지 분류기를 학습할 수
있습니다. 마찬가지로, 비슷한 도구들을

46
00:04:14,720 --> 00:04:18,640
사용할 수 있습니다. 그냥 비디오 분류기를 학습하면 됩니다. 특징을
추출하고,

47
00:04:18,640 --> 00:04:23,120
같은 손실 함수를 사용해서 비디오 분류기를 학습하는 거죠. 그래서 이제
비디오 이해 문제는, 이전

48
00:04:23,120 --> 00:04:28,480
강의에서 배운 손실 함수를 적용할 수 있는 비디오의

49
00:04:28,480 --> 00:04:32,240
특징을 어떻게 얻을 수 있느냐입니다.

50
00:04:34,080 --> 00:04:38,720
또한 이미지 분류와 비디오 분류, 비디오 이해의

51
00:04:38,720 --> 00:04:42,400
또 다른 차이점은, 지금 하고자

52
00:04:42,400 --> 00:04:47,760
하는 작업이 이전 예제와 조금 다를 수 있다는 점입니다. 이미지 분류에서는
보통

53
00:04:47,760 --> 00:04:53,440
장면이나 객체에 더 신경 씁니다. 단순히 분류를 하죠. 객체 카테고리가
무엇인지요.

54
00:04:53,440 --> 00:04:57,120
비디오에서는 보통, 여기서 보여드리는

55
00:04:57,120 --> 00:05:00,000
예처럼, 행동을 분류하고자 합니다. 비디오 속 인물이나 동물이

56
00:05:00,000 --> 00:05:06,160
어떤 활동을 하는지, 행동을 분류하는 경우가 많습니다. 이것이 보통 비디오
이해에서

57
00:05:06,160 --> 00:05:12,400
우리가 관심을 가지는 부분입니다. 그래서 인식해야 할 대상의 본질이 조금
다를 수 있습니다.

58
00:05:12,400 --> 00:05:17,520
비디오 이해에서 조심해야 할 또 다른

59
00:05:17,520 --> 00:05:24,000
문제는 비디오가 보통 매우 크다는 점입니다. 이미지에 대해 이야기할 때는
단순히

60
00:05:24,000 --> 00:05:30,640
3 곱하기 H 곱하기 W 정도입니다. RGB 숫자로 이루어진 단일 행렬이죠.

61
00:05:30,640 --> 00:05:34,880
하지만 비디오를 고려하면, 프레임들의 연속입니다. 초당 30프레임

62
00:05:34,880 --> 00:05:43,280
정도 될 수 있습니다. 영화에서는 때때로 더 높은 해상도와 시간

63
00:05:43,280 --> 00:05:50,240
해상도를 가진 비디오 프레임도 있습니다. 그래서 비디오를 저장할 공간을
고려해야 합니다.

64
00:05:50,240 --> 00:05:56,320
예를 들어, 표준 해상도 비디오를 저장하면 분당

65
00:05:56,320 --> 00:06:01,600
약 1.5기가바이트가 필요할 수 있습니다. 더 높은 해상도, 예를 들어

66
00:06:01,600 --> 00:06:08,960
1920 곱하기 1080을 고려하면, 분당 약 10기가바이트가

67
00:06:08,960 --> 00:06:14,800
필요합니다. 이런 비디오 데이터를 저장하려면 엄청난 공간이 필요합니다.

68
00:06:14,800 --> 00:06:18,560
또한, 이런 데이터를

69
00:06:18,560 --> 00:06:24,240
GPU에 직접 넣을 방법이 없습니다. 입력만 해도 이런 데이터를 저장할

70
00:06:24,240 --> 00:06:28,560
많은 저장 공간이 필요합니다. 그리고 가중치나 컨볼루션

71
00:06:28,560 --> 00:06:36,560
신경망의 활성화 값 같은 다른 것도 저장해야 합니다. 그래서 모델이 매우
커질 수밖에 없습니다.

72
00:06:36,560 --> 00:06:44,960
그렇다면 비디오를 작게 만들어 처리할 수 있게 하는 해결책은 무엇일까요?
간단한 해결책은 비디오를

73
00:06:44,960 --> 00:06:51,760
작게 만드는 것입니다. 고해상도 비디오나 원본 비디오가 길어도,

74
00:06:51,760 --> 00:06:58,720
시간적, 공간적으로 모두 축소할 수 있습니다.

75
00:06:59,920 --> 00:07:05,040
예를 들어, 3.2초짜리 비디오라면, 초당

76
00:07:05,040 --> 00:07:09,600
모든 프레임이 필요하지 않을 수 있습니다.

77
00:07:09,600 --> 00:07:13,360
중복되는 프레임이 많으니 초당 5프레임만 가져가도 됩니다.

78
00:07:13,360 --> 00:07:19,680
초당 프레임 수를 줄이고, 공간 해상도도 112

79
00:07:19,680 --> 00:07:24,000
곱하기 112처럼 작게 하면, 비디오 크기를

80
00:07:24,000 --> 00:07:32,240
조금 줄일 수 있습니다. 예를 들어, 이 간단한 비디오는 5.588KB
정도입니다. 물론, 계산 능력이 있다면 이미지처럼 더 큰

81
00:07:32,240 --> 00:07:36,960
해상도로도 할 수 있습니다.

82
00:07:36,960 --> 00:07:45,120
그리고 긴 비디오에서 모델을 어떻게 학습할까요? 이전 슬라이드에서
3.2초짜리 비디오

83
00:07:45,120 --> 00:07:49,520
분류기를 학습한다고 했지만, 비디오는 매우 길어서 몇 분,

84
00:07:49,520 --> 00:07:54,400
몇 시간일 수도 있습니다. 사람들이 하는 한 가지 방법은 클립 단위로
학습하는

85
00:07:54,400 --> 00:08:00,320
것입니다. 즉, 비디오 프레임 덩어리로 학습하는 거죠.

86
00:08:00,320 --> 00:08:04,960
짧은 클립을 낮은 fps로

87
00:08:06,240 --> 00:08:12,320
분류하도록 모델을 학습합니다. 그리고 슬라이딩 윈도우를 사용해서, 다양한
클립을 샘플링해 학습

88
00:08:12,320 --> 00:08:16,720
데이터로 사용합니다. 그렇게 분류기를 학습하는 겁니다. 그리고 테스트, 즉
추론 시에는

89
00:08:16,720 --> 00:08:22,400
모델을 여러 클립에 대해 실행합니다. 몇 개의 클립을 샘플링합니다. 우리는
10개의 클립을 만들었습니다. 그리고 예측

90
00:08:22,400 --> 00:08:29,520
결과를 평균 냅니다. 그것이 이 긴 비디오에 대한 우리의 예측입니다.

91
00:08:29,520 --> 00:08:37,440
그렇다면 같은 방식으로 사용할 수 있는 비디오 분류 모델은 무엇일까요?
비디오는 기본적으로 이미지 시퀀스, 즉 비디오

92
00:08:37,440 --> 00:08:45,760
이미지 프레임의 연속이라는 점을 말씀드렸습니다. 그래서 가장 간단한 방법은
이들을

93
00:08:45,760 --> 00:08:50,800
이미지로 취급하는 것입니다. 그게 우리가 이미 가진

94
00:08:50,800 --> 00:08:56,640
가장 간단한 도구입니다. 우리는 단일 프레임 컨볼루션 신경망을 실행합니다.
이미 모든 도구가 준비되어 있기 때문입니다.

95
00:08:56,640 --> 00:09:00,720
이미지 분류기를 학습할 수 있다는 것을 배웠습니다. 이미지 분류기를 이
비디오 프레임에

96
00:09:00,720 --> 00:09:05,200
적용해 이미지처럼 처리하면, 실제로

97
00:09:05,200 --> 00:09:09,200
괜찮은 예측을 얻을 수 있습니다. 특히 이런 비디오 같은 경우에는요. 비디오
전반에 큰 변화가 많지

98
00:09:09,200 --> 00:09:13,120
않다는 것을 알 수 있습니다. 사람이 달리고 있죠. 몸의 움직임이 조금

99
00:09:13,120 --> 00:09:17,760
다를 수는 있습니다. 하지만 일반적으로는 꽤 비슷해 보입니다.

100
00:09:17,760 --> 00:09:25,040
아마 이미지 액션 분류기를 실행할 수도 있겠죠. 그리고 모든 프레임에서,
아마 모든 프레임이

101
00:09:25,040 --> 00:09:29,440
달리기라고 말할 겁니다. 각 이미지, 각 비디오 프레임의 예측 결과를 평균

102
00:09:29,440 --> 00:09:34,480
내면, 이 특정 비디오에 대해 달리기라고 예측할 수 있습니다.

103
00:09:34,480 --> 00:09:40,960
사실, 이것은 보통 아주 강력한 기본 기준이

104
00:09:41,920 --> 00:09:45,600
됩니다, 특히 이런 비디오에서는

105
00:09:45,600 --> 00:09:51,200
영상 간 변화가 많지 않기 때문입니다. 그래서 비디오 분류기를 설계하려면
항상 이

106
00:09:51,200 --> 00:09:56,560
방법부터 시도해야 합니다, 간단한 방법이니까요. 그리고 이미 꽤 괜찮은
결과를 얻을 수도 있습니다.

107
00:09:56,560 --> 00:10:02,400
질문은 단일 프레임을 실행할지, 아니면 여러 프레임 묶음을 실행할지입니다.
이 경우, 간단히 단일

108
00:10:02,400 --> 00:10:08,480
프레임을 예로 들어보죠. 기본적으로 30프레임짜리 비디오가 있습니다. 아마
몇 프레임만

109
00:10:08,480 --> 00:10:13,360
샘플링할 수도 있겠죠. 샘플링한 10프레임에 이미지 분류기를

110
00:10:13,360 --> 00:10:18,720
적용하고, 그냥 이미지로 취급하는 겁니다. 그리고 결과를 직접 평균 내는
거죠. 이게 기본적인 프레임별 CNN입니다.

111
00:10:18,720 --> 00:10:22,000
중요한 질문은 프레임을 어떻게 샘플링하느냐입니다.

112
00:10:22,000 --> 00:10:26,480
이게 아주 핵심적인 질문입니다. 왜냐하면 우리가-- 제가 말하는 것은
프레임을 샘플링하고

113
00:10:26,480 --> 00:10:30,000
그 프레임에 CNN을 적용하고 싶다는 겁니다. 그럼 그 프레임들을

114
00:10:30,000 --> 00:10:33,680
어떻게 얻을까요? 사실 이것도 활발한 연구 분야입니다.

115
00:10:33,680 --> 00:10:38,800
간단한 방법 중 하나는 무작위 샘플링입니다. 한 시간짜리 영상이 있다고
합시다. 어디에 흥미로운 부분이나 중요한

116
00:10:38,800 --> 00:10:42,720
부분이 있는지 모릅니다. 우리는 그냥 1분마다 한

117
00:10:42,720 --> 00:10:46,880
프레임씩 샘플링할 수 있습니다. 그리고 이미지 분류기를 실행합니다. 결과를
평균 냅니다. 물론, 이 방법이 어느 정도

118
00:10:46,880 --> 00:10:51,200
좋은 결과를 줄 수도 있습니다. 하지만 아마도 이것이 가장 똑똑한 샘플링
방법은 아닐 겁니다.

119
00:10:51,200 --> 00:10:55,040
그래서 더 똑똑한 샘플링 전략을 제안하는 다른 방법들도 있습니다.

120
00:10:55,040 --> 00:10:59,600
예를 들어 한 프레임을 샘플링하고, 그 결정에 따라 어디를

121
00:10:59,600 --> 00:11:05,600
더 샘플링할지 정할 수 있습니다. 사실 나중 강의 슬라이드에서 몇 가지
예시도 보여드릴 겁니다.

122
00:11:05,600 --> 00:11:10,560
이것이 매우, 매우 간단한 비디오 분류기입니다. 우리가 이미지 분류기, 즉
단일

123
00:11:10,560 --> 00:11:17,200
프레임 CNN을 사용하는 것과 마찬가지입니다. 그리고 비슷하게, 한 단계

124
00:11:17,200 --> 00:11:23,840
더 나아갈 수도 있습니다. 단순히 단일 프레임 CNN을 돌려서 예측 결과를
평균

125
00:11:23,840 --> 00:11:32,400
내는 대신, 단일 프레임 CNN의 특징들 간에 융합을 할 수도 있습니다.

126
00:11:32,400 --> 00:11:39,200
이것을 흔히 late fusion이라고 부릅니다. 기본적으로 아이디어는
여전히 2D CNN을 사용하는 겁니다.

127
00:11:39,200 --> 00:11:48,080
그리고 입력으로는 아마도 키 프레임들이 있을 겁니다. 각 프레임마다 2D
CNN을 사용합니다. 그리고 특징 벡터를

128
00:11:48,080 --> 00:11:56,080
추출합니다. 그럼 D 곱하기 H' 곱하기 W' 크기의 특징 맵을 얻을 수
있습니다.

129
00:11:56,080 --> 00:11:59,360
그리고 T개의 프레임이 있으니까요. 기본적으로

130
00:11:59,360 --> 00:12:06,800
T개의 특징 맵이 있습니다. 간단하게는 모든 특징 맵을 벡터로

131
00:12:06,800 --> 00:12:12,720
평탄화(flatten)한 뒤 이어 붙입니다. 그러면 모든 프레임의 모든
정보, 모든 특징을 담은

132
00:12:12,720 --> 00:12:16,480
거대한 특징 벡터가 생깁니다.

133
00:12:16,480 --> 00:12:19,120
그 다음에 우리가 배운 도구들, 예를

134
00:12:19,120 --> 00:12:25,360
들어 완전 연결 네트워크(fully connected networks)를
사용할 수 있습니다. 이 벡터를 어떤 신경 차원(neural

135
00:12:25,360 --> 00:12:30,400
dimension)으로 매핑하는 MLP를 학습합니다. 그리고 그 위에
분류기를 학습시켜서 클래스 점수 C로 매핑합니다.

136
00:12:30,400 --> 00:12:37,040
이것을 late fusion이라고 합니다. 기본적으로 feature map을
추출하는 것을 볼 수 있습니다. 그리고 그것들을 매우

137
00:12:37,040 --> 00:12:41,600
독립적으로 처리합니다. 그리고 아주 늦은 단계에서 feature
vector들을

138
00:12:41,600 --> 00:12:45,280
연결(concatenate)하고, 완전 연결층을 사용해 분류를 수행합니다.

139
00:12:45,280 --> 00:12:54,800
이 방법은 유용합니다. 하지만 제 설명과 예시에서 이미 알 수 있듯이,

140
00:12:54,800 --> 00:13:00,160
이 완전 연결층은 많은

141
00:13:00,160 --> 00:13:06,240
파라미터를 도입하게 됩니다. 왜냐하면 여러 feature vector들을
시간 축을 따라 평탄화(flatten)해서 연결하기 때문입니다.

142
00:13:06,240 --> 00:13:10,720
그리고 이 feature vector들은 T가 얼마나

143
00:13:10,720 --> 00:13:14,480
크냐에 따라 거대한 벡터가 될 수 있습니다. 그런 다음 이 거대한
feature vector를 사용합니다. 이것을 낮은 차원으로

144
00:13:14,480 --> 00:13:19,760
매핑하고 싶어 하죠. 그러면 매우 큰 완전

145
00:13:19,760 --> 00:13:23,440
연결층이 필요합니다. 이것은 많은 파라미터를 도입하게 됩니다. 그래서
효율적이지 않습니다.

146
00:13:23,440 --> 00:13:28,400
다른 방법은,

147
00:13:29,040 --> 00:13:34,240
연결(concatenate)을 하지 않고, 거대한 feature
vector를 만들지

148
00:13:34,240 --> 00:13:39,920
않고, 완전 연결층으로 점수를 매기지 않는 것입니다. 대신 간단한
풀링(pooling)을

149
00:13:39,920 --> 00:13:45,440
할 수 있습니다. 풀링을 할 때는 피처 벡터의 길이가 늘어나지 않습니다.

150
00:13:45,440 --> 00:13:51,440
기본적으로, 단일 프레임에 대한 피처 차원이 있고 시간에

151
00:13:51,440 --> 00:13:54,160
걸쳐 풀링을 하면,

152
00:13:54,160 --> 00:14:01,360
이 T개의 프레임에 대해 시간적 집계를 하는 풀링을 하는 겁니다. 그리고 이
클립 피처 D를 기반으로, D 곱하기 T 대신에 풀링을 하면 여전히 시간

153
00:14:01,360 --> 00:14:04,480
차원 D의 피처 벡터를 갖게 됩니다.

154
00:14:04,480 --> 00:14:10,640
그다음에 선형 레이어를 사용해 D를 클래스 점수와 맞는 차원 C로
매핑합니다. 그리고 그 위에 크로스

155
00:14:10,640 --> 00:14:16,640
엔트로피 손실을 학습합니다. 이것도 후반 융합(late fusion)입니다.
하지만 이제는 풀링을

156
00:14:16,640 --> 00:14:22,080
사용하고 있습니다. 좋은 점은 이제 매우 큰 완전 연결층을

157
00:14:22,080 --> 00:14:26,320
가질 필요가 없다는 겁니다. 하지만 풀링은 중요한

158
00:14:28,320 --> 00:14:34,400
정보를 잃어버릴 수도 있습니다. 그래서 이것이 이 연산의 단점입니다.

159
00:14:34,400 --> 00:14:41,280
제가 후반 융합이라고 부르는 이유는 중요한 부분이 '후반'이기 때문입니다.
그리고 후반일 때는 이미 어떤 정보가

160
00:14:41,280 --> 00:14:46,560
손실되었을 수도 있습니다. 2D 컨볼루션 네트워크를 사용해

161
00:14:46,560 --> 00:14:52,800
이미지를 처리하는 경우가 있습니다. 예를 들어, 여기 빨간 원으로 표시된
부분처럼요. 이 비디오를 인식하는 데

162
00:14:52,800 --> 00:14:59,440
매우 중요한 것은 이 남자의

163
00:14:59,440 --> 00:15:04,480
발 움직임입니다. 발이 위아래로 움직이고 있죠. 그리고 아마 그가 달리고
있다는 것을 알 수 있습니다.

164
00:15:04,480 --> 00:15:10,240
그래서 만약 우리가 단일 2D CNN을 사용해 이들을

165
00:15:11,040 --> 00:15:16,320
독립적인 2D 이미지로 처리하고 어떤 특징 맵을

166
00:15:18,720 --> 00:15:25,200
추출한다면, 아주 늦은 단계의 특징 맵까지 가도 이 남자의

167
00:15:25,200 --> 00:15:31,040
발 움직임 정보는 더 이상 포함되어 있지 않습니다. 그래서 이 발의 위아래
움직임에 대한 정보는 빨간 원으로

168
00:15:31,040 --> 00:15:35,520
표시된 부분처럼 유용한 단서가 될 수 있습니다. 하지만 지금은 특징 맵에 그
정보가 없습니다.

169
00:15:35,520 --> 00:15:40,880
직관적으로 말하면, 초기 레이어에서 특징을

170
00:15:41,440 --> 00:15:48,400
추출하면 원본 비디오 프레임과 매우 가깝습니다. 그래서 이 저수준 정보, 즉
비디오 프레임에서의

171
00:15:48,400 --> 00:15:54,480
움직임이 포함될 가능성이 더 큽니다. 또한, 시간에 따라 이들을 연결하거나
풀링하면

172
00:15:54,480 --> 00:16:00,880
시간에 따른 움직임을 분석할 수 있습니다.

173
00:16:00,880 --> 00:16:05,840
하지만 많은 합성곱과 풀링을 거쳐 늦은 단계까지 처리하기 때문에,

174
00:16:05,840 --> 00:16:09,440
아주 늦은 단계에서는 이 저수준 움직임 정보 대신

175
00:16:09,440 --> 00:16:13,520
의미론적 정보 같은 고수준 정보가 더 많이 포함됩니다. 그래서 그 정보가

176
00:16:13,520 --> 00:16:19,520
없을 가능성이 가장 큽니다. 이것이 늦은 융합의 단점입니다.

177
00:16:19,520 --> 00:16:24,160
그래서 늦은 융합 대신에 실제로는 초기 융합을 할 수 있습니다. 초기 융합을
하려면, 실제 비디오

178
00:16:24,160 --> 00:16:28,240
프레임에 더 가까운 특징 벡터를 활용하고

179
00:16:28,240 --> 00:16:38,960
싶다면, 입력을 받아서 바로 3T 곱하기 H 곱하기 W 형태로 재구성할 수
있습니다. 처음부터 시간적으로 정보를 직접

180
00:16:38,960 --> 00:16:42,640
집계하는 겁니다.

181
00:16:42,640 --> 00:16:47,760
그리고 첫 번째 2D 합성곱 층을

182
00:16:47,760 --> 00:16:55,200
사용해 채널 차원을 3T에서 D로 바로 매핑합니다. 기본적으로 2D 합성곱을
사용해 첫 번째 층에서 시간 정보를

183
00:16:55,200 --> 00:17:03,200
처리하고, 채널 차원을 3T에서 D로 매핑해 합성곱 신경망의 아주 초기부터

184
00:17:03,200 --> 00:17:08,960
모든 프레임의 비디오 정보를 처리하는 겁니다.

185
00:17:08,960 --> 00:17:11,680
그리고 나머지 네트워크는 표준 2D CNN으로 진행됩니다. 그리고 유일한
차이점은 이제

186
00:17:13,040 --> 00:17:16,320
단일 레이어를 사용해 모든 시간

187
00:17:18,240 --> 00:17:21,920
정보를 파괴하고 축소한다는 점입니다. 그리고 나머지는 이미지 분류와
똑같습니다. 그리고 표준 크로스 엔트로피

188
00:17:21,920 --> 00:17:26,080
손실을 사용해 이 분류를 수행합니다.

189
00:17:26,080 --> 00:17:32,320
각 프레임마다 D와 같은 특징을 얻습니다. 각 단일 프레임은 특징 D를
제공합니다. 그래서 T가

190
00:17:32,320 --> 00:17:36,400
있습니다. 이 특징 벡터는 D입니다. 그래서 풀링은 특징들 위에서
수행합니다. 기본적으로 특징들의 평균을 내는 평균 풀링이나

191
00:17:36,400 --> 00:17:41,280
특징들 중 최대값을 취하는 맥스 풀링을 할 수 있습니다.

192
00:17:41,280 --> 00:17:48,640
그 후에도 여전히 D 크기의 특징을 얻습니다. 즉, 프레임이 아니라 특징들
위에서 풀링하는 겁니다.

193
00:17:49,200 --> 00:17:54,880
이것이 초기 융합(early fusion)입니다. 초기 융합의 단점은 초기
레이어에서 모션을

194
00:17:56,240 --> 00:18:00,800
명시적으로 처리하려 하지만, 너무 욕심을 부린다는

195
00:18:05,200 --> 00:18:08,640
점입니다. 모든 것을 단일 레이어에서 포착하려고 합니다.

196
00:18:08,640 --> 00:18:12,960
모든 프레임을 단순히 연결(concatenate)하고 단일

197
00:18:12,960 --> 00:18:18,000
컨볼루션 네트워크를 사용해 모든 [INAUDIBLE] 정보를 축소하는 거죠.
아마 우리가 원하는 결과를 얻지 못할 수도 있습니다.

198
00:18:18,000 --> 00:18:23,920
그래서 또 다른 해결책은, 늦은 융합이나 이른

199
00:18:23,920 --> 00:18:30,240
융합 대신에 그 중간 어딘가에서 뭔가를 하는 것입니다. 그것이 바로 슬로우
퓨전입니다. 이 3D 컨볼루션 신경망이 바로

200
00:18:30,240 --> 00:18:35,440
그런 역할을 하는 거죠. 직관적으로는 3D 버전의 컨볼루션과 풀링을 사용하고
싶다는 겁니다.

201
00:18:35,440 --> 00:18:40,400
네트워크를 거치면서 정보를 천천히 융합하고 싶습니다. 아주 늦은 단계나 아주
이른 단계에서

202
00:18:40,400 --> 00:18:45,760
하는 대신에, 시간 차원과 공간

203
00:18:45,760 --> 00:18:52,640
차원을 점차 줄여가면서 3D 특징 맵을 얻는 거죠. 이것이 3D 컨볼루션
신경망의 아이디어입니다.

204
00:18:52,640 --> 00:18:57,040
그냥 3D 컨볼루션과 3D 풀링 연산을 사용하는 겁니다. 그럼 3D
컨볼루션과

205
00:18:57,040 --> 00:19:02,400
3D 풀링이란 무엇일까요? 2D 컨볼루션을 배웠죠. 네. 2D 컨볼루션
레이어에 대해요. 기본적으로 이렇게

206
00:19:02,400 --> 00:19:10,960
32x32x3 이미지가 있다고 합시다. 2D 컨볼루션을 사용하면, 각

207
00:19:10,960 --> 00:19:15,520
커널마다 필터가 있고,

208
00:19:16,400 --> 00:19:23,440
5x5x3 크기의 컨볼루션 커널이 공간을 슬라이딩

209
00:19:23,440 --> 00:19:34,240
윈도우 방식으로 쭉 훑으면서 깊이 차원 전체를 커버하는 것을 배웠습니다.

210
00:19:34,240 --> 00:19:42,560
그래서 각 계산마다, 그걸 최종 activation map의 단일 값에
매핑합니다.

211
00:19:42,560 --> 00:19:48,640
그리고 마지막으로, 이 경우 28 곱하기 28 곱하기 1

212
00:19:48,640 --> 00:19:53,520
크기의 activation map을 얻습니다. 모든 공간 위치에 대해
convolution을

213
00:19:53,520 --> 00:20:01,840
수행하고, 채널 차원, 즉 깊이 방향으로 모두 지나가면서, 이 경우 3에서
1로 매핑합니다.

214
00:20:01,840 --> 00:20:06,400
이것이 2D convolution입니다. 차이점은 3D
convolution에서는

215
00:20:06,400 --> 00:20:14,960
이제 한 개의 추가 차원이 있다는 겁니다. 여기서 입력은 C 곱하기 T
곱하기 H

216
00:20:14,960 --> 00:20:21,520
곱하기 W라고 생각할 수 있습니다. 추가된 차원이 바로 T 차원입니다.
이것은 시간적 차원입니다.

217
00:20:21,520 --> 00:20:26,720
하지만 제가 여기서 보여주는 것은 3D까지만 보여줄 수 있기 때문입니다,
4D는 보여줄 수 없죠.

218
00:20:26,720 --> 00:20:30,400
그래서 실제로 여기서 보이지 않는 차원이 하나 있습니다. 그것이 C
차원입니다. 채널 차원이

219
00:20:30,400 --> 00:20:35,200
여기서는 보이지 않습니다. 그래서 이 feature map의 각 그리드

220
00:20:35,200 --> 00:20:42,880
포인트마다 여러 개의 feature가 있다고 생각할 수 있습니다. 그 그리드
포인트에는 C개의 feature가 있습니다.

221
00:20:42,880 --> 00:20:49,200
그리고 이 3D convolution에 대해 말하자면, 기본적으로 6 곱하기
6 곱하기 6

222
00:20:49,200 --> 00:20:52,800
convolution인데, 한 개의 추가 차원이 있기 때문입니다. 이미지의
공간 차원인

223
00:20:52,800 --> 00:20:58,000
H와 W 차원만 슬라이드하는

224
00:20:58,000 --> 00:21:06,720
대신, 이제는 이 큐브 전체를 슬라이드하는 겁니다. 우리는 T 곱하기 H
곱하기 W 차원의 이 큐브 위를 슬라이딩하고 있습니다.

225
00:21:06,720 --> 00:21:13,200
그래서 공간 차원과 시간 차원 둘 다 포함하고 있습니다. 그리고 채널 차원

226
00:21:13,200 --> 00:21:20,000
전체를 따라 이동합니다. 그래서 점차적으로, 2D 미적분처럼 할 수
있습니다. 다른 부분은 2D 컨볼루션과

227
00:21:20,000 --> 00:21:24,480
같습니다. 단지 여기에 추가 차원이 있는 거죠.

228
00:21:24,480 --> 00:21:29,520
그럼 6 곱하기 6 곱하기 6 3D 컨볼루션을 얻게 됩니다. 그리고 아마도
5 곱하기 5 크기의

229
00:21:29,520 --> 00:21:35,120
또 다른 레이어가 있을 겁니다. 마지막으로, 3D convolution
연산을

230
00:21:35,120 --> 00:21:39,520
처리한 후에, feature 벡터를

231
00:21:39,520 --> 00:21:45,920
평탄화하고, 완전 연결층을 사용해 클래스 점수로 매핑합니다. 이것이
기본적으로 3D convolution의 개념입니다.

232
00:21:46,800 --> 00:21:54,320
이해를 돕기 위해, 초기, 후기, 그리고 3D

233
00:21:54,320 --> 00:22:03,680
convolution 신경망을 비교하는 간단한 예제를 살펴보겠습니다. 어떻게
작동하는지 감을 잡으시라고요. 실제로는, 확실히, 훨씬 크고 복잡할

234
00:22:03,680 --> 00:22:07,520
수 있습니다. 하지만 여기서는 특징 맵의 크기와 수용

235
00:22:07,520 --> 00:22:13,040
영역을 살펴보면서 초기 융합과 후기 융합, 그리고 3D 컨볼루션

236
00:22:13,040 --> 00:22:16,320
신경망 간의 차이를 이해할 수

237
00:22:16,320 --> 00:22:20,000
있도록 간단한 예제를 보여드리려는 겁니다.

238
00:22:20,000 --> 00:22:25,920
그래서 후기 융합의 경우, 예를 들어 이 경우에는

239
00:22:25,920 --> 00:22:32,280
원래 입력이 3 곱하기 20이라고 생각할 수 있습니다. 20은 시간적
차원입니다. 그리고 64,

240
00:22:32,280 --> 00:22:40,720
64는 공간적 차원입니다. 그리고 2D convolution을 사용합니다.
왜냐하면 우리는 late fusion을

241
00:22:40,720 --> 00:22:45,280
하고 있기 때문입니다. 처음에는 시간적 차원에 대해 아무 작업도 하지
않습니다. 그냥 20, 시간적

242
00:22:45,280 --> 00:22:49,040
차원을 유지합니다. 공간적으로 receptive field를 확장하는
겁니다.

243
00:22:49,040 --> 00:22:53,520
이제 2D convolution 레이어로 채널 차원을 3에서

244
00:22:53,520 --> 00:22:58,000
12로 매핑하지만, 시간적 차원 20은 그대로 유지합니다. 그리고
점차적으로, 아마도

245
00:22:58,000 --> 00:23:02,640
pooling 레이어를 사용합니다. 그래도 시간적 차원에 대해서는 아무
작업도 하지 않았습니다. 그래서 여전히 20입니다.

246
00:23:02,640 --> 00:23:08,960
하지만 pooling 연산 덕분에 공간적 차원에서 receptive
field를 확장합니다.

247
00:23:08,960 --> 00:23:14,160
그리고 점차적으로, 또 다른 2D 레이어를 사용할 수도 있습니다. 이제
feature map은 24 곱하기

248
00:23:14,160 --> 00:23:19,680
20 곱하기 16 곱하기 16입니다. 그리고 공간적 receptive
field도 점차 확장하지만,

249
00:23:19,680 --> 00:23:23,920
시간적 차원 20은 계속 유지합니다. 그래서 시간적 차원에 대해서는 아무
작업도 하지 않았습니다.

250
00:23:23,920 --> 00:23:28,480
마지막으로, 단일 global average pooling을

251
00:23:28,480 --> 00:23:34,400
사용해서 feature map 20 곱하기 16 곱하기 16을 통합합니다.
그래서 공간 차원에서 두 번 모두 풀링을 합니다. 이제 20 곱하기

252
00:23:34,400 --> 00:23:39,760
16 곱하기 16에서 1 곱하기 1 곱하기 1 피처 포인트를 얻습니다.
기본적으로 마지막 단일 층에서 모든

253
00:23:39,760 --> 00:23:42,640
것을 축소하는 거죠. 그리고 단일 층에서

254
00:23:42,640 --> 00:23:47,760
시간적 수용 영역을 구축합니다. 이것이 바로 늦은 융합(late
fusion)입니다.

255
00:23:47,760 --> 00:23:50,400
그렇다면 초기 융합(early fusion)은 뭐가 다를까요? 이제 공간에서
천천히 쌓거나

256
00:23:50,400 --> 00:23:54,320
시간에서 한 번에 N으로 쌓는 대신,

257
00:23:54,320 --> 00:24:00,960
공간에서 천천히 쌓으면서 시간은 처음부터 한 번에 쌓는 겁니다. 입력은
여전히 3 곱하기 20

258
00:24:00,960 --> 00:24:07,360
곱하기 64 곱하기 64입니다. 하지만 이제 단일 conv 2D 층만
사용합니다.

259
00:24:07,360 --> 00:24:11,520
이제 3 곱하기 20을 채널 차원으로 취급하는 거죠. 모든 것을

260
00:24:11,520 --> 00:24:16,640
매핑합니다. 3 곱하기 30이 있죠. 모두를 채널 차원으로

261
00:24:16,640 --> 00:24:23,200
보고 12로 매핑하는 겁니다. 즉, 단일 2D 컨볼루션 층을 사용해 처음부터
모든 시간

262
00:24:24,160 --> 00:24:26,800
정보를 축소하는 거죠.

263
00:24:26,800 --> 00:24:31,200
그래서 첫 번째 층에서 시간적 수용 영역을 구축합니다. 이제 시간적 수용
영역이

264
00:24:31,200 --> 00:24:36,320
1에서 20으로 확장됩니다. 그리고 공간적 수용

265
00:24:36,320 --> 00:24:42,320
영역은 점차 쌓입니다. 그리고 늦은 융합과 마찬가지로 풀링과 2D 컨볼루션을
사용해 공간 차원을 쌓습니다.

266
00:24:42,320 --> 00:24:46,080
마지막으로 글로벌 평균 풀링을 사용합니다. 글로벌 평균 풀링은 공간

267
00:24:46,080 --> 00:24:53,360
전체에서 평균을 내는 풀링을 하는 겁니다. 그래서 공간에서는 천천히 쌓지만
시간에서는

268
00:24:53,360 --> 00:24:58,000
처음부터 한 번에 쌓는 거죠. 이것이 초기 융합입니다.

269
00:24:58,000 --> 00:25:03,360
그렇다면 3D 컨볼루션 신경망은 무엇일까요? 3D 컨볼루션 층은 기본적으로
공간과

270
00:25:03,360 --> 00:25:08,800
시간 모두에서 천천히 쌓습니다. 그래서 이를 느린 융합(slow
fusion)이라고 부릅니다. 입력은 여전히 3 곱하기 20

271
00:25:08,800 --> 00:25:14,960
곱하기 64 곱하기 64일 수 있습니다. 하지만 이제 3D 컨볼루션을
사용합니다.

272
00:25:14,960 --> 00:25:27,840
첫 번째 층에서는 3에서 12로 매핑하지만, 이 경우 시간 차원도

273
00:25:27,840 --> 00:25:32,160
유지합니다. 그리고 시간적 수용 영역과 공간적

274
00:25:32,160 --> 00:25:36,880
수용 영역을 조금씩 확장합니다. 그리고 풀링 층을 사용합니다. 그리고 풀링
레이어에서는 4

275
00:25:36,880 --> 00:25:41,760
곱하기 4 곱하기 4입니다. 그리고 우리는 이 시간적 특징과

276
00:25:41,760 --> 00:25:46,240
공간적 특징을 조금씩 뽑아냅니다. 그리고 나서 공간적, 시간적 수용 영역을
더 확장합니다.

277
00:25:46,240 --> 00:25:51,600
그리고 또 다른 3D 컨볼루션 레이어가 있어서 공간적,

278
00:25:51,600 --> 00:25:55,920
시간적 수용 영역을 더 확장합니다. 마지막으로, 글로벌 평균 풀링을
사용합니다. 하지만 이제는 4 곱하기 16 곱하기

279
00:25:55,920 --> 00:26:01,520
16 피처 맵 위에서 풀링을 하면서 시간적,

280
00:26:01,520 --> 00:26:06,000
공간적 수용 영역을 더 확장합니다. 그래서 공간과 시간 모두에서 점차 확장해
나가는 겁니다.

281
00:26:06,000 --> 00:26:12,000
이것이 초기 융합, 후기 융합, 그리고 3D 컨볼루션 신경망의 차이입니다.

282
00:26:12,000 --> 00:26:18,880
초기 융합과 3D 컨볼루션 신경망을 보면 알 수 있습니다. 두 경우 모두
시간에 따른

283
00:26:18,880 --> 00:26:24,400
수용 영역을 만듭니다. 그런데 실제 차이가 뭘까요? 좀 더

284
00:26:25,200 --> 00:26:36,160
자세히 살펴보겠습니다. 각 공간 격자점에 대한 특징 벡터로 생각하면,

285
00:26:36,160 --> 00:26:44,560
일반적인 필터, 즉 2D 컨볼루션에서는 이

286
00:26:45,200 --> 00:26:54,400
격자점에 대해 모든 시간 차원을 고려합니다. T는 16입니다.

287
00:26:55,120 --> 00:27:04,000
그래서 공간에서는 국소적이지만 시간에서는 완전히 확장됩니다. 이것이 2D
컨볼루션

288
00:27:04,000 --> 00:27:10,400
신경망의 필터입니다. 그런데 문제는 무엇일까요? 한번 생각해 보세요. 그래서
만약 이 2D 컨볼루션에서 시간 차원을 쭉

289
00:27:10,400 --> 00:27:15,120
따라간다면 어떤 문제가 발생할까요?

290
00:27:15,120 --> 00:27:22,480
그 단점은 시간적 이동 불변성이 없다는 겁니다. 그 이유는 2D 필터가 이제
시간

291
00:27:22,480 --> 00:27:29,280
축을 완전히 확장하기 때문입니다. 그래서 만약 우리가 서로 다른 시간의 색상
전환

292
00:27:29,280 --> 00:27:32,240
같은 전역적인 변화를

293
00:27:33,680 --> 00:27:39,600
학습하고 싶다면, 예를 들어, 비디오니까요. 우리는 시간 정보를 인식하고
싶습니다.

294
00:27:39,600 --> 00:27:43,360
만약 어떤 변화가 시간

295
00:27:43,360 --> 00:27:49,280
단계 4에서 파란색에서 주황색으로 일어나고, 또 같은 변화가 시간

296
00:27:49,280 --> 00:27:55,600
단계 15에서도 일어난다면, 하지만 그것은 파란색에서 주황색으로의 같은
변화입니다.

297
00:27:56,640 --> 00:28:04,080
만약 필터가 시간 축을 완전히 확장한다면, 서로

298
00:28:04,080 --> 00:28:09,840
다른 시간에서 전역적인 변화를 학습하려면

299
00:28:09,840 --> 00:28:15,200
완전히 다른 필터가 필요합니다. 그래서 서로 다른 시간 단계에서 이런

300
00:28:15,200 --> 00:28:21,920
변화를 학습하려면 다른 커널을 배워야 합니다. 즉, 시간적 이동 불변성이
없다는 겁니다.

301
00:28:22,800 --> 00:28:30,080
그렇다면 공간과 시간 어디에서든 이런 파란색에서 주황색으로의 변화를 어떻게
인식할 수 있을까요? 이미지 분류를 할 때 공간적 불변성을

302
00:28:30,080 --> 00:28:35,840
가지려 하는 것과 마찬가지입니다. 우리는 이미지에 고양이가 있다는

303
00:28:35,840 --> 00:28:41,120
것을 인식할 수 있어야 합니다. 고양이가 오른쪽 구석에 있든 왼쪽 구석에
있든,

304
00:28:41,120 --> 00:28:49,520
우리는 다른 공간 위치에 있는 것을 인식할 수 있도록 커널을 공유하고
싶습니다.

305
00:28:49,520 --> 00:28:53,040
여기서는 서로 다른 시간 단계에서 서로

306
00:28:53,040 --> 00:28:59,200
다른 종류의 움직임, 서로 다른 시간 패턴을 학습할 수 있기를 원합니다.
그래서 비슷한 개념입니다. 이것이 바로 3D convolution

307
00:28:59,200 --> 00:29:04,880
neural networks의 장점입니다. 지금은 시간 차원 T에서 완전히
확장되는 대신,

308
00:29:04,880 --> 00:29:10,160
원래 초기 버전에서는 T가 시간 차원

309
00:29:10,160 --> 00:29:14,400
전체에 걸쳐 16까지 확장되었습니다.

310
00:29:14,400 --> 00:29:19,520
하지만 이제 T가 3이라고 가정해봅시다. 그리고 시간 차원에서 슬라이딩할 수
있습니다. 공간 불변성을 필터와 국소 영역을

311
00:29:19,520 --> 00:29:24,000
사용해 배운 것처럼 말이죠. 이제 이 conv 필터는 시간에서 국소

312
00:29:24,000 --> 00:29:29,680
윈도우만 차지하고 시간 차원에서 슬라이딩합니다.

313
00:29:29,680 --> 00:29:36,720
그래서 이점은 이제 시간 이동 불변성을 가질 수 있다는 겁니다. 각 필터가
시간에 따라

314
00:29:36,720 --> 00:29:42,480
슬라이딩하기 때문입니다. 그래서 이 필터를 재사용해 이 차원들에서

315
00:29:42,480 --> 00:29:48,160
다양한 움직임 패턴을 인식할 수 있습니다. 파란색에서 주황색으로의 전환을
이제 시간의 모든 순간에 인식할 수 있습니다.

316
00:29:49,360 --> 00:29:55,600
그리고 이점은 별도의 필터가 필요 없다는 겁니다. 이제 표현 효율성이

317
00:29:55,600 --> 00:30:01,120
더 좋아졌습니다. 더 이상 별도의 필터를 학습할 필요가 없습니다. 이것이
기본적으로 2D conv 초기 융합과 3D

318
00:30:01,120 --> 00:30:08,560
convolution neural network의 주요 차이점입니다.

319
00:30:08,560 --> 00:30:13,920
또한 지난 강의에서 2D convolution

320
00:30:13,920 --> 00:30:20,480
neural networks에서 학습한

321
00:30:20,480 --> 00:30:27,200
것을 시각화할 수 있는 도구의 예도 보셨을 겁니다. 마찬가지로 3D
convolution networks의

322
00:30:27,200 --> 00:30:35,280
필터도 이렇게 비디오 클립으로 시각화할 수 있습니다. 보이시는지 모르겠지만,

323
00:30:35,280 --> 00:30:42,400
3D convolution neural networks에서 학습한

324
00:30:42,400 --> 00:30:51,520
필터들은 공간과 시간 모두에 걸쳐 확장되기 때문에 비디오 클립으로 볼 수
있습니다. 이 필터들 중 일부는 이미지 분류기에서

325
00:30:52,480 --> 00:30:58,560
보던 필터와 비슷해서,

326
00:30:58,560 --> 00:31:03,520
색상 패턴이나 다양한 에지들을 포함하고 있습니다. 하지만 다른

327
00:31:03,520 --> 00:31:09,600
필터들도 있습니다. 한 코너에서 다른 코너로, 혹은 한 에지 패턴에서

328
00:31:09,600 --> 00:31:16,960
다른 에지 패턴으로의 시간적 전환을 학습한 필터도 있습니다. 어떤 필터는
움직임을 학습하지 않고, 어떤

329
00:31:16,960 --> 00:31:23,440
필터는 색상 패턴에만 집중하기도 합니다. 하지만 어떤 필터는 다양한 방향의
움직임을 학습합니다.

330
00:31:23,440 --> 00:31:29,040
이렇게 커널을 시각화해서 해석할 수 있습니다. 기본적으로 두 가지 차이가
있는데, 하나는

331
00:31:30,080 --> 00:31:36,400
이 느린 융합(slow fusion)입니다. 컨볼루션 연산 측면에서 보면,
네, 기본적으로 3D

332
00:31:36,400 --> 00:31:39,520
convolution입니다. 3D convolution이 확실히 그렇습니다.

333
00:31:39,520 --> 00:31:44,960
2D convolution과는 완전히 다릅니다. 컨볼루션에 또 다른 차원이
있습니다. 시간적 차원이

334
00:31:44,960 --> 00:31:47,120
있습니다. 그래서 차이점은

335
00:31:47,120 --> 00:31:52,560
컨볼루션 연산에 이 시간적 차원이 포함된다는 겁니다. 하지만 실제로는 3D
컨볼루션

336
00:31:52,560 --> 00:31:57,280
신경망을 사용합니다. 이것은 공간과 시간에 걸쳐 점차적으로 수용 영역을
확장합니다.

337
00:32:01,040 --> 00:32:08,800
그래서 우리는 3D 컨볼루션 네트워크나 아키텍처라는 도구에 대해
이야기했습니다.

338
00:32:08,800 --> 00:32:13,360
그런데 ImageNet처럼 어떤 데이터를 사용할 수 있을까요? 또는 비디오
분류기를 학습시키기 위해

339
00:32:13,360 --> 00:32:18,960
어떤 비디오 데이터를 사용할 수 있을까요? 사람들이 도전해온 예시 데이터셋
중 하나는

340
00:32:19,600 --> 00:32:27,680
2014년에 소개된 Sports 1 million이라는 데이터셋입니다.

341
00:32:27,680 --> 00:32:33,120
이 데이터셋으로 어떤 작업을 할 수 있는지 볼 수 있습니다. 매우 세분화된
스포츠 카테고리

342
00:32:33,120 --> 00:32:37,120
분류를 할 수 있습니다. 여기서 파란색은

343
00:32:37,120 --> 00:32:46,000
실제 정답을 나타냅니다. 노란색은 상위 다섯 개 예측을 보여줍니다. 초록색은
올바른 예측을

344
00:32:46,000 --> 00:32:49,200
나타냅니다. 빨간색은 틀린 예측을 나타냅니다.

345
00:32:49,200 --> 00:32:52,560
데이터셋의 행동 카테고리가

346
00:32:52,560 --> 00:32:59,600
매우 세분화되어 있음을 알 수 있습니다. 스포츠 종류는 487가지가
있습니다. 마라톤, 울트라마라톤 같은 것도

347
00:32:59,600 --> 00:33:02,960
있을 수 있습니다. 사실, 그 차이를 잘 모르겠지만,

348
00:33:02,960 --> 00:33:09,680
이 데이터셋에서 다양한 스포츠 카테고리를 구분하는 데 매우 중요합니다.

349
00:33:09,680 --> 00:33:14,960
그리고 여기 우리가 이야기한 다양한 분류기들을 Sports

350
00:33:14,960 --> 00:33:20,720
1 million 데이터셋에 학습시켰을 때의 결과가 있습니다. 여기서 아마
가장 놀라운 결과는, 제가 여러분에게

351
00:33:20,720 --> 00:33:27,520
시도해보라고 한 단일 프레임 모델이 비디오 분류 모델을

352
00:33:27,520 --> 00:33:33,440
개발할 때 실제로 매우 좋은 성능을 보인다는 점입니다.

353
00:33:33,440 --> 00:33:38,880
단일 프레임 모델을 이미지 분류기로만 취급해도,

354
00:33:38,880 --> 00:33:45,760
실제로 77.7%의 Top-5 정확도를 보여줍니다. 우리가 이야기한 초기
융합 모델은

355
00:33:45,760 --> 00:33:51,040
약간 더 낮은 성능을 보입니다. 후기 융합 모델은 약간 더 좋은 성능을
보입니다. 그리고 3D 컨볼루션 신경망을 이

356
00:33:51,040 --> 00:33:59,120
데이터셋에 적용하면 2%에서 3% 정도 성능 향상이 있습니다.

357
00:33:59,840 --> 00:34:04,800
여기서 얻을 수 있는 핵심 메시지는,

358
00:34:04,800 --> 00:34:12,800
단일 프레임 모델을 꼭 시도해보라는 것입니다. [INAUDIBLE]가 실제로
꽤 잘 작동합니다. 하지만 제가 여기서 보여준 3D 컨볼루션 신경망은

359
00:34:12,800 --> 00:34:18,720
2014년에 사용된 모델입니다. 지난 10년 동안 많은

360
00:34:18,720 --> 00:34:23,600
발전이 있었습니다. 그래서 제가 뒤쪽 슬라이드에서 이야기할 것처럼 성능

361
00:34:23,600 --> 00:34:26,640
수치도 훨씬 더 좋아지고 있습니다.

362
00:34:26,640 --> 00:34:30,400
학습과 테스트 모두 비디오를 이미지로

363
00:34:30,400 --> 00:34:34,880
취급해서 이미지 분류기를 학습시키는 방식입니다. 이것이 바로 단일 프레임
모델이 하는 일입니다. 기본적으로 질문을 제대로 이해했다면,

364
00:34:34,880 --> 00:34:38,240
이미지 분류기를 사용하는 겁니다. 하지만 비디오의 많은

365
00:34:38,240 --> 00:34:41,520
프레임을 가지고 학습합니다. 각 비디오에서 단일 프레임이 아닙니다.

366
00:34:43,520 --> 00:34:47,600
그리고 이 데이터셋은 매우 큰 데이터셋입니다. 앞서 말했듯이,

367
00:34:47,600 --> 00:34:55,600
비디오는 용량이 매우 큽니다. 사람들이 비디오 데이터셋을 공유할 때,

368
00:34:55,600 --> 00:35:02,640
ImageNet처럼 그냥 공유할 수 없습니다. 사람들이 어떤 데이터베이스에서
다운로드할 수 있는데, 비디오가 정말 크기 때문입니다. 이 데이터셋에는
아마도 100만

369
00:35:02,640 --> 00:35:08,800
개의 비디오가 있을 겁니다. 모두 다운로드해서 공유하는 것은 현실적이지
않습니다.

370
00:35:08,800 --> 00:35:14,160
사실 이 비디오는 원래 공개될 때 YouTube

371
00:35:14,160 --> 00:35:19,280
URL 목록으로 공유되었습니다. 하지만 이 YouTube 비디오 URL에서
예상할 수 있는 한

372
00:35:20,160 --> 00:35:24,880
가지는 사람들이 비디오를 수정하거나 삭제한다는 점입니다. 그래서 원래
목록에는 100만

373
00:35:24,880 --> 00:35:30,560
개의 비디오가 있을 수 있지만, 지금은 아마 절반 정도가 이미

374
00:35:30,560 --> 00:35:36,960
사라졌거나 존재하지 않을 겁니다. 그래서 이 데이터셋은 이런 이유로 매우
안정적이지 않습니다.

375
00:35:37,600 --> 00:35:49,920
앞서 말했듯이, 3D 컨볼루션 신경망은 2014년 5월

376
00:35:49,920 --> 00:35:56,000
이후로 점차 개선되고 있습니다. 이 3D 컨볼루션 네트워크의 초기 인기

377
00:35:56,000 --> 00:36:01,760
모델 중 하나는 [?]라는 모델입니다. C3N ? 네트워크입니다.
기본적으로, 사실 매우 간단합니다. 기본적으로, 2D 이미지 분류에 사용하는

378
00:36:03,440 --> 00:36:08,640
VGG 아키텍처와 매우 유사합니다.

379
00:36:09,200 --> 00:36:19,120
하지만 이제는 이것을 3차원 컨볼루션 신경망으로 변환한 것입니다. 예를
들어, 3D CNN에서는 3x3x3 컨볼루션과

380
00:36:19,120 --> 00:36:25,760
2x2x2 풀링을 사용합니다.

381
00:36:25,760 --> 00:36:31,280
첫 번째 레이어를 제외하면 약간의 변화가 있습니다. 그래서 전체 아키텍처는
VGG

382
00:36:31,280 --> 00:36:36,640
아키텍처와 매우 유사합니다. 단지 여기에 추가적인 차원이 생긴 거죠.

383
00:36:36,640 --> 00:36:43,600
그래서 이것을 3D CNN의 VGG라고 부르는 겁니다. 이 모델은 제가 방금
언급한 Sports

384
00:36:44,160 --> 00:36:50,400
1 million 데이터셋으로

385
00:36:50,960 --> 00:36:58,080
훈련되었고, 2014년에 소개되었습니다. 그 당시 이런 모델을 훈련시키려면
많은 연산 자원이 필요했습니다.

386
00:36:58,080 --> 00:37:07,120
왜냐하면 그때는 많은 사람들이 GPU를 많이 사용할 수 없었기 때문입니다.

387
00:37:07,120 --> 00:37:13,040
그래서 실제로 이 모델은 Facebook에서 훈련되었습니다. 그리고 그들은
이 모델의 사전 훈련된 가중치를 공개했는데,

388
00:37:13,040 --> 00:37:19,920
Sports 1 million에서 3D 모델을 훈련시키고 나서 사전

389
00:37:19,920 --> 00:37:23,760
훈련된 특징 추출기 모델로 공개했습니다. 그래서 많은 사람들이 직접 비디오
모델을 훈련시킬

390
00:37:23,760 --> 00:37:28,400
여력이 없어서 이 모델을 특징 추출기로 사용하기 시작했습니다.

391
00:37:28,400 --> 00:37:31,600
그래서 그들은 그냥 비디오를 가져와서 이

392
00:37:33,600 --> 00:37:37,360
사전 학습된 모델을 사용해 특징을 추출할 수 있습니다. 그래서 3D 모델을
만들고,

393
00:37:37,360 --> 00:37:44,880
아마도 다른 선형 분류기를 훈련시킬 수도 있겠죠. 그래서 사람들이 사용하기
시작했습니다. 그래서 인기가 있었던 겁니다.

394
00:37:44,880 --> 00:37:48,720
그래서 질문은 기본적으로 우리가 이야기하는, [잘 안 들림] 분류에서 몇

395
00:37:48,720 --> 00:37:52,480
개의 프레임을 입력으로 받아 특징을 추출해야 하는가에 관한 것입니다.
기본적으로 우리가 이야기하는

396
00:37:52,480 --> 00:37:56,720
모든 모델에 대해, 우리는 미리 정의된

397
00:37:56,720 --> 00:38:03,520
길이의 클립, 예를 들어 16프레임이나 32프레임을

398
00:38:03,520 --> 00:38:09,280
단일 모델에 입력으로 넣어 훈련한다고 가정합니다. 그리고 클립 단위 예측을
어떻게

399
00:38:09,280 --> 00:38:16,080
집계할지에 관한 다른 기법들도 있습니다. 하지만 지금은 클립 단위 특징
추출만 하고 있습니다.

400
00:38:16,080 --> 00:38:21,360
이 3D CNN의 단점은 계산 비용이 매우 크다는 점입니다. 기본적으로
우리는 그냥

401
00:38:21,360 --> 00:38:27,760
무식하게 2D에서 3D로

402
00:38:27,760 --> 00:38:35,440
VGG 스타일을 직접 만듭니다. AlexNet을 보면 알 수 있습니다.
GFLOP는 기본적으로

403
00:38:35,440 --> 00:38:40,800
기가플롭스라는 뜻입니다. 단일 순전파에 필요한 부동소수점 연산 수를 측정해서

404
00:38:40,800 --> 00:38:45,680
네트워크가 효율적인지 아닌지를 평가하려는 겁니다.

405
00:38:45,680 --> 00:38:54,320
AlexNet은 0.7 GFLOPS를 사용합니다. VGG-16은 약 13.6
GFLOPS를 사용하죠. 하지만 C3D는 2D에서 3D로 매핑하는

406
00:38:54,320 --> 00:39:01,040
작업을 실제로 수행합니다. 그래서 이제 39.5 GFLOPS를 사용합니다.
VGG보다 2.9배나

407
00:39:01,040 --> 00:39:10,080
많아서 효율적이지 않습니다. 이런 네트워크의 단점이 바로 그것입니다.

408
00:39:10,640 --> 00:39:16,640
Sports 1 million 데이터셋에서

409
00:39:17,280 --> 00:39:26,960
성능을 보면, 이 모델은 탑 5 정확도에서 약 4% 향상을 보입니다. 이것은
우리가 할 수 있는 3D 컨볼루션 네트워크의

410
00:39:26,960 --> 00:39:31,520
한 예일 뿐입니다. 하지만 분명히 다른

411
00:39:31,520 --> 00:39:36,240
방법들도 있을 수 있습니다. 2D 이미지 분류에서 할 수 있는 여러 가지
트릭들을 이야기했죠.

412
00:39:36,240 --> 00:39:41,200
ResNet에서 보았던 잔차 연결 같은 것들이요. 분명히 3D에도 잔차
연결이나 2D

413
00:39:41,200 --> 00:39:45,600
컨볼루션에서 이야기한 다른

414
00:39:45,600 --> 00:39:50,320
기법들을 적용해 개선할 수 있습니다. 실제로 다양한 비디오

415
00:39:50,320 --> 00:39:59,280
아키텍처를 개선하려는 많은 연구와 논문들이 있습니다.

416
00:39:59,280 --> 00:40:05,280
하지만 그 외에, 공간과 시간을 별도로

417
00:40:05,280 --> 00:40:10,800
다뤄야 하는지 좀 더 생각해 봅시다. 공간 정보와 시간 정보는 정말

418
00:40:10,800 --> 00:40:14,320
다른 것이니까요. 그래서 아마도 시간적으로

419
00:40:14,320 --> 00:40:19,600
존재하는 것을 명시적으로 모델링해야 할 겁니다. 그것이 바로 모션입니다.

420
00:40:20,240 --> 00:40:26,720
사람은 모션을 처리하는 데 정말 뛰어난 능력을 가지고 있습니다. 그래서 이
간단한 비디오에서

421
00:40:26,720 --> 00:40:30,000
사람이

422
00:40:30,800 --> 00:40:45,840
어떤 행동을 하는지 한번 맞춰보세요. 원하시면 말해도 됩니다. 이게 뭐죠?
앉아 있는 거죠.

423
00:40:45,840 --> 00:40:51,600
네, 이렇게 아주 적은 점들만으로도 이

424
00:40:51,600 --> 00:41:03,280
사람이 어떤 행동을 하는지, 혹은 두 사람이 하는 행동을 잘 인식할 수
있습니다. 지금은 어떤 외형 정보도

425
00:41:03,280 --> 00:41:08,800
없어요. 단지 몇 개의 점들만 있죠. 단지 움직임만 있습니다.

426
00:41:08,800 --> 00:41:11,440
우리는 실제로 이 영상에서

427
00:41:11,440 --> 00:41:17,600
일어나는 어떤 활동에 대해 아주 잘 이해할 수 있습니다. 그래서 외형과
움직임을 처리하는 방식이

428
00:41:17,600 --> 00:41:22,960
매우 다를 수 있다는 겁니다. 아마도 각각을 처리하기 위해 별도의 네트워크가
필요할 수도 있죠. 실제로 이것이 2014년에 소개된

429
00:41:22,960 --> 00:41:28,160
이 연구의 동기 중 하나입니다.

430
00:41:30,240 --> 00:41:37,200
이들은 외형 정보와 움직임 정보를 각각 따로 처리하는

431
00:41:37,200 --> 00:41:43,120
투스트림 네트워크를 제안하려고 했습니다. 기본적으로 움직임을 명시적으로
측정하는 한 가지 방법은

432
00:41:43,120 --> 00:41:47,840
optical flow라는 개념을 사용하는 것입니다. optical
flow의 기본 아이디어는 인접한

433
00:41:48,720 --> 00:41:55,920
프레임에서 픽셀의 움직임, 즉 움직임의 변화를 측정하는 것입니다.

434
00:41:55,920 --> 00:41:58,000
기본적으로 첫 번째 프레임에서

435
00:41:58,000 --> 00:42:02,320
각 픽셀이 두 번째 프레임에서 어떻게 움직일지를 측정하는 거죠. 그래서
프레임 내의 점들에 대한

436
00:42:02,320 --> 00:42:06,960
속도를 계산하고 다음 프레임, 즉

437
00:42:06,960 --> 00:42:12,400
시퀀스에서 점들이 어디에 있을지 추정합니다. 예를 들어, 이 경우에는 T
프레임과 p+1 프레임에 대해 말이죠.

438
00:42:12,400 --> 00:42:20,000
기본적으로, 이 플로우 필드는 두 차원으로 되어 있고,

439
00:42:20,000 --> 00:42:28,400
각 픽셀이 다음 프레임에서 어디로 이동할지를 알려줍니다. 그래서 F(x,
y)는 (dx, dy)와 같습니다. 그리고 I t plus 1 (x plus
dx)는 다음 프레임에서 픽셀

440
00:42:28,400 --> 00:42:35,840
E가 위치하는 곳이고, 현재 프레임의 I t (x, y)와 같습니다.

441
00:42:35,840 --> 00:42:42,080
즉, 픽셀의 움직임을 명확하게 측정하는 방법입니다. 실제로 한 쌍의 프레임이
주어졌을 때

442
00:42:42,080 --> 00:42:46,560
옵틱 플로우를 계산하는

443
00:42:46,560 --> 00:42:53,600
방법과 관련된 연구 논문들이 많이 있습니다. 다양한 가정을 하는 방법들이
있죠. 어떤 연구들은 옵틱 플로우가 밝기가 일정하다고 가정합니다,

444
00:42:53,600 --> 00:42:57,200
물체가 움직여도 밝기가

445
00:42:57,200 --> 00:43:00,800
변하지 않는다고 보는 거죠. 그리고 이런 옵틱 플로우를 계산하는 기법들을
제안하려고 합니다.

446
00:43:00,800 --> 00:43:03,680
하지만 일단 이해하면, 두

447
00:43:03,680 --> 00:43:09,440
인접한 프레임 간의 움직임 정보를 기본적으로 포착합니다. 그리고 이것이
2차원인 이유는 픽셀이 수평과 수직으로

448
00:43:09,440 --> 00:43:14,560
어떻게 움직이는지를 포착하려 하기 때문입니다. 그래서 실제로 이것을 따로

449
00:43:14,560 --> 00:43:19,200
시각화할 수도 있습니다. 수평 움직임, 수평 흐름인

450
00:43:19,200 --> 00:43:23,600
dx를 시각화할 수 있습니다. 그리고 수직 흐름인 dy도 시각화할 수
있습니다. 수평 움직임과 수직 움직임을

451
00:43:23,600 --> 00:43:28,640
포착하는 것을 볼 수 있습니다. 이렇게 저수준의 움직임 신호를 포착하는
거죠.

452
00:43:28,640 --> 00:43:32,720
이런 움직임 신호를 optical flow로

453
00:43:32,720 --> 00:43:38,880
포착할 방법이 생기자, 사람들은 움직임 분류기와 외형 분류기를 학습시키는

454
00:43:38,880 --> 00:43:43,760
two-stream 네트워크를 제안하려 했습니다. 이것은 행동 인식을 위한
유명한 투스트림 네트워크입니다. 기본적으로 단일 프레임 모델이 있어서

455
00:43:43,760 --> 00:43:48,880
어떤 행동인지 나타내는 외형 분류를 합니다.

456
00:43:48,880 --> 00:43:55,920
그리고 별도의 스트림이 있습니다. 그것이 다중 프레임 옵티컬 플로우를
입력으로

457
00:43:55,920 --> 00:44:01,440
받는 시간적 스트림 ConvNet입니다. 인접한 두 프레임마다 옵티컬 플로우
맵을 계산합니다. 또한 수평 운동 옵티컬 플로우와 수직

458
00:44:01,440 --> 00:44:06,720
운동 옵티컬 플로우를 따로 처리합니다. 그리고 그것들을

459
00:44:06,720 --> 00:44:11,440
쌓아서 합칩니다. 그다음 시간적 스트림 컨볼루션 신경망으로 처리합니다.

460
00:44:11,440 --> 00:44:14,800
그리고 예측을 합니다. 모션 스트림과 외형

461
00:44:14,800 --> 00:44:18,640
스트림의 예측 결과를 합쳐

462
00:44:18,640 --> 00:44:22,400
최종 예측을 만듭니다. 이것이 투스트림 네트워크의 아이디어입니다.

463
00:44:22,400 --> 00:44:27,840
그리고 실제로 꽤 잘 작동합니다. UCF-101이라는 또 다른
데이터셋에서요. 이 데이터셋에는 101개의

464
00:44:29,440 --> 00:44:33,840
행동 카테고리가 있습니다. 그래서 한 가지 놀라운 점은 모션만

465
00:44:33,840 --> 00:44:38,800
사용해도 의외로 아주 잘 작동한다는 겁니다.

466
00:44:40,160 --> 00:44:45,280
3D 컨볼루션 네트워크와 공간 정보만 사용하는 경우의 성능을 볼 수
있습니다. 그리고 그건 단지 appearance

467
00:44:45,280 --> 00:44:48,560
스트림입니다. 그리고 temporal만 있는 게 motion 스트림입니다.
motion 스트림이 실제로

468
00:44:48,560 --> 00:44:55,840
spatial만 있는 appearance 스트림보다 훨씬 잘 작동하는 걸 볼
수 있습니다.

469
00:44:55,840 --> 00:45:03,840
제 가설은 과적합이 덜 하다는 겁니다. 왜냐하면 motion의 경우 배경
정보가 많아서

470
00:45:04,800 --> 00:45:09,440
액션 분류에 중요하지 않을

471
00:45:09,440 --> 00:45:13,040
수 있기 때문입니다. 하지만 motion 스트림은

472
00:45:13,040 --> 00:45:17,760
실제로 핵심 정보인 움직임을 포함하고

473
00:45:17,760 --> 00:45:22,240
있어서 과적합이 덜 하다는 겁니다. 실제로 이 데이터셋에서 더 좋은 결과를
얻을 수 있습니다.

474
00:45:22,240 --> 00:45:32,080
지금까지 우리는 비디오의 단기 구조에 대해 이야기했습니다. 그리고 아까도
누군가가 분류를 위해

475
00:45:32,080 --> 00:45:39,280
몇 프레임을 사용해야 하는지

476
00:45:39,280 --> 00:45:42,960
물어봤죠? 분명히 더 먼 시간까지 인식하려면

477
00:45:42,960 --> 00:45:50,080
장기적인 시간 구조를 모델링하는 게 매우 중요합니다.

478
00:45:50,640 --> 00:45:57,200
우리는 이미 알고 있습니다. 저는 이미 시퀀스를 처리할 수 있는 도구를

479
00:45:58,240 --> 00:46:07,280
가지고 있습니다. 순환 신경망을 사용해서 단어 시퀀스를 처리하고

480
00:46:07,280 --> 00:46:13,920
캡션 작업이나 예측 작업을 할 수 있죠. 그래서 비슷한 도구, 즉 순환
신경망을 사용할 수 있습니다.

481
00:46:13,920 --> 00:46:20,320
우리는 단지 컨볼루션 신경망을 가지고 있습니다. 단일 프레임 컨볼루션
신경망으로 2D

482
00:46:20,320 --> 00:46:26,160
특징 벡터를 얻든, 클립에서 3D

483
00:46:26,160 --> 00:46:32,880
컨볼루션 네트워크를 사용하든 상관없습니다. 하지만 훨씬 더 긴 비디오가
있습니다. 우리는 피처 벡터를 얻고, 그 다음에 지금까지 이야기한 RNN이나
LSTM을

484
00:46:34,960 --> 00:46:40,320
사용해서 장기적인 시간 구조를 모델링할 수 있습니다.

485
00:46:40,320 --> 00:46:48,880
우리는 이 순환 신경망을 사용해 지역 피처를 처리하고, 단일 비디오 수준
분류를

486
00:46:48,880 --> 00:46:53,520
하고 싶다면 마지막 타임스텝에서 최종 예측을

487
00:46:53,520 --> 00:46:59,440
합니다. 즉, 비디오 끝에서 다대일 출력하는 거죠. 그리고 우리가 이야기한
것처럼 일대일 매핑도

488
00:47:01,680 --> 00:47:03,760
할 수 있습니다.

489
00:47:04,400 --> 00:47:09,680
그래서 각 프레임마다 예측을 할 수 있습니다. 아마도 각 비디오 프레임마다

490
00:47:09,680 --> 00:47:16,960
예측하고 싶은 것들이 있을 수 있습니다. 그리고 이 출력도 LSTM이나 순환
신경망에서 얻을 수 있습니다.

491
00:47:16,960 --> 00:47:27,280
사실 이런 아이디어는 2011년에 이미 탐구된 바 있습니다. 사실 그건
시대를 훨씬

492
00:47:27,280 --> 00:47:35,280
앞서간 겁니다. AlexNet이 2012년에 소개되었기 때문입니다. 하지만
이 2015년 논문에서 더 대중화되었습니다.

493
00:47:35,280 --> 00:47:41,760
그래서 장기적인 시간 구조를 모델링하기 위해 이런 순환 구조를 훈련하고

494
00:47:41,760 --> 00:47:48,080
싶다면, 보통 이 [잘 안 들림]을 통해서만 역전파할 수 있습니다. 또는
CNN을 융합할

495
00:47:48,080 --> 00:47:51,840
수도 있습니다. 일부 클립이나 이미지

496
00:47:51,840 --> 00:47:57,040
분류에서 미리 학습시킬 수 있습니다. 그렇지 않으면 순환 부분과 이 합성곱

497
00:47:57,040 --> 00:48:02,480
부분이 있는 거대한 네트워크가 됩니다. 이들을 끝까지 모두 함께 훈련하는
것은 매우 어렵습니다. 그래서 3D [잘 안 들림]를 특징 추출기로

498
00:48:02,480 --> 00:48:07,440
사용하고 이 순환 신경망만 훈련할 수 있습니다.

499
00:48:07,440 --> 00:48:14,240
우리는 이미 시간적 구조를 모델링하는 두 가지 접근법을 살펴보았습니다. 이
두 가지 접근법, 즉 convolution neural networks와

500
00:48:14,240 --> 00:48:19,760
recurrent neural network를 결합하면 어떨까요?

501
00:48:19,760 --> 00:48:24,880
두 방법 모두 장점이 있습니다. 그래서 이 비디오 데이터를 처리하기 위해 두
가지를 하나의

502
00:48:25,680 --> 00:48:28,480
아키텍처로 결합할 수 있습니다.

503
00:48:28,480 --> 00:48:32,640
실제로, 우리가 이야기한 다층 recurrent

504
00:48:32,640 --> 00:48:38,000
neural networks에서 영감을 얻을 수 있습니다. 각 타임스텝은
같은 층의 이전 숨겨진 타임스텝과

505
00:48:38,000 --> 00:48:42,000
이전 층의 같은 타임스텝

506
00:48:42,000 --> 00:48:44,880
출력을 받을 수 있습니다. 이것이 다층 RNN의 기본 아이디어입니다.

507
00:48:44,880 --> 00:48:50,640
비슷하게, 비디오에도 똑같이 적용할 수 있습니다. 이제 recurrent
convolution

508
00:48:50,640 --> 00:48:59,360
neural networks를 소개합니다. 매우 비슷합니다. 지금은 각
특징이 3차원 벡터인

509
00:48:59,360 --> 00:49:05,200
특징 그리드를 만듭니다. 두 개의 공간 차원과 하나의 채널 차원입니다.

510
00:49:07,040 --> 00:49:12,560
각 특징 벡터는 사실 4차원 C 곱하기 H 곱하기 W와 같습니다.

511
00:49:12,560 --> 00:49:16,080
각 벡터는 두 입력에 의존합니다. 각 특징 맵은 같은 층의

512
00:49:16,080 --> 00:49:20,480
이전 타임스텝 특징 맵에 의존합니다. 하지만 또한 이전 층의 같은

513
00:49:20,480 --> 00:49:24,560
타임스텝 특징 맵에도 의존합니다.

514
00:49:26,800 --> 00:49:33,680
그래서 2D convolution network에서는 입력 피처에서

515
00:49:33,680 --> 00:49:39,360
출력 피처로 이 피처 맵을 매핑하는 방식을 기록합니다. 하지만 여기
recurrent convolution

516
00:49:39,360 --> 00:49:48,240
network에서는 이 두 개의 3D 텐서를 입력으로 사용할 수 있습니다.
하나는 같은 레이어의 이전 시간 단계에서 온 것이고, 다른 하나는 같은 시간
단계의

517
00:49:48,240 --> 00:49:50,960
이전 레이어에서 온 것입니다.

518
00:49:50,960 --> 00:50:00,800
RNN을 기억해보면, 숨겨진 레이어 피처

519
00:50:00,800 --> 00:50:06,880
맵 ht-1이 있습니다. 현재 시간 단계의 입력을 받아서,

520
00:50:06,880 --> 00:50:13,920
파라미터 W를 가진 어떤 함수와 함께 처리합니다. 그리고 새로운 상태 피처
벡터 ht를 계산하죠. 이것이 RNN의 핵심입니다.

521
00:50:13,920 --> 00:50:19,040
이제, 대신에 RNN의 벡터 형태를 바꿉니다. RNN에서의 모든 행렬 곱셈을

522
00:50:19,040 --> 00:50:25,440
2D convolution으로 대체합니다. 이렇게 해서 recurrent
convolution neural

523
00:50:25,440 --> 00:50:28,800
networks를 얻습니다. 피처 맵이 있고,

524
00:50:28,800 --> 00:50:32,880
행렬 곱셈 대신 2D convolution을 수행합니다. 또 다른

525
00:50:32,880 --> 00:50:37,600
피처 맵을 얻죠. 그리고 이전 레이어의 같은 시간

526
00:50:37,600 --> 00:50:43,440
단계 피처도 마찬가지로 처리합니다. 그 다음 2D convolution
결과를 더하고, 또

527
00:50:43,440 --> 00:50:50,720
다른 tanh 레이어를 거쳐서 현재 숨겨진 레이어의 피처 맵을 얻습니다.

528
00:50:50,720 --> 00:50:54,640
이것이 바로 recurrent convolution neural
network의 기본 아이디어입니다. 기본적으로 convolution

529
00:50:54,640 --> 00:50:57,520
연산과

530
00:50:57,520 --> 00:51:02,320
recurrent 연산을 결합하는 겁니다. 그리고 실제로 GRU나 LSTM
같은 다양한 RNN 변형에도

531
00:51:02,320 --> 00:51:08,080
이 방식을 적용할 수 있습니다. 아마 이전 수업에서 배우셨을 겁니다.

532
00:51:08,080 --> 00:51:14,160
이제 두 가지 장점을 성공적으로 결합할 수 있습니다. recurrent
convolution neural

533
00:51:14,160 --> 00:51:22,240
network 안에 공간적, 시간적 융합이 모두 존재하는 거죠. 하지만 이
모델은 많이 사용되지 않았는데, 그 이유는 RNN의

534
00:51:22,240 --> 00:51:29,680
큰 단점 때문입니다. 아마 이미 배우셨겠지만, RNN은

535
00:51:29,680 --> 00:51:35,440
비시퀀스 데이터를 처리할 때 매우 느립니다. 그리고 비디오는 보통 매우

536
00:51:35,440 --> 00:51:42,720
길고, 병렬 처리가 필요합니다. 하지만 RNN은 병렬화가 매우 어렵습니다.

537
00:51:42,720 --> 00:51:48,560
하지만 이전 강의에서 배운 또 다른

538
00:51:48,560 --> 00:51:56,400
중요한 모델이 있습니다. 우리가 할 수 있는 방법이죠.
self-attention 같은 연산을 사용해서 비디오를 처리할 수
있습니다. self-attention에서는 queries,

539
00:51:56,400 --> 00:52:03,920
keys, values가 있습니다. self-attention 레이어를
독립적인 연산으로

540
00:52:03,920 --> 00:52:08,320
이미지 처리에 사용할 수 있죠. 여기서는 비디오에도 적용할 수 있습니다.

541
00:52:08,320 --> 00:52:15,360
self-attention의 큰 장점 중 하나는 높은 병렬화가 가능하다는
점입니다. 모든 입력에 대한 정렬과 어텐션 점수 계산이

542
00:52:15,360 --> 00:52:21,840
완전히 병렬로 수행될 수 있습니다. 그래서 실제로 사람들은 비디오에도

543
00:52:21,840 --> 00:52:29,040
self-attention을 사용하려고 시도하고 있습니다. 그들은
self-attention을 3D에 직접 적용합니다.

544
00:52:29,040 --> 00:52:31,440
아마도 여러분은 3D convolution neural network를
가지고 있을 겁니다. C 곱하기 T 곱하기 H

545
00:52:31,440 --> 00:52:36,720
곱하기 W 같은 feature map을 얻을 수 있습니다. 그리고 비슷하게,
query feature

546
00:52:36,720 --> 00:52:41,760
map도 얻을 수 있습니다. 1 곱하기 1 곱하기 1 3D
convolution을 사용해서 채널 차원을 바꿔서 query

547
00:52:41,760 --> 00:52:46,560
feature map인 C prime 곱하기 T 곱하기 H 곱하기 W로
매핑할 수 있습니다.

548
00:52:46,560 --> 00:52:50,960
키에 대해서도 마찬가지로 이 feature map을 얻고, 값에 대해서도 이
feature map을 얻습니다. 그리고 나서 attention

549
00:52:50,960 --> 00:52:56,160
weight를 구하고 싶습니다. 기본적으로, query에서 이

550
00:52:56,160 --> 00:53:07,280
feature map을 transpose하는 작업을 합니다. 그리고 벡터화된
곱셈을 통해 각 query와 key feature 쌍에 대한

551
00:53:07,280 --> 00:53:13,360
attention score를 얻습니다. 그다음 이 attention
map을 얻고, 이를 사용해서 값을 조건화합니다.

552
00:53:13,360 --> 00:53:21,200
그리고 또 다른 값, 즉 feature map을 얻을 수 있습니다. 그리고
그것들을 매핑할 수 있습니다. 다시 1 곱하기 1 곱하기 1
convolution을 사용해서

553
00:53:21,200 --> 00:53:25,520
원래 차원 C로 매핑해서 원래

554
00:53:25,520 --> 00:53:31,280
feature 입력과 연결할 수 있게 합니다. 이것이 residual
connection입니다. 전체적으로 보면, self-attention
연산과

555
00:53:31,280 --> 00:53:36,160
매우 비슷하다는 것을 알 수 있습니다.

556
00:53:36,160 --> 00:53:41,280
하지만 이제는 3D로 확장한 겁니다. 이것은 독립적인 하나의 블록입니다.

557
00:53:41,280 --> 00:53:46,320
스스로 독립적으로 작동할 수 있습니다. 이 논문에서는 이것을 nonlocal

558
00:53:46,320 --> 00:53:51,120
neural network라고 부릅니다. 이 블록을 소개하고 nonlocal
block이라고 부릅니다. 비디오 처리를 위한 빌딩 블록으로 사용할 수

559
00:53:51,680 --> 00:53:55,680
있고, 비디오 이해에 활용할 수 있습니다.

560
00:53:55,680 --> 00:53:59,840
예를 들어, 기존 3D convolution

561
00:53:59,840 --> 00:54:06,560
neural network 아키텍처에 nonlocal block을 추가할 수
있습니다. 3D CNN 블록에 nonlocal block을 추가하고, 또
다른

562
00:54:06,560 --> 00:54:12,480
3D CNN 블록에도 nonlocal block을 추가합니다. 각
nonlocal block은 공간과 시간 모두를

563
00:54:12,480 --> 00:54:18,640
융합하는 데 매우 강력하며, 최종적으로 분류에 활용됩니다.

564
00:54:18,640 --> 00:54:23,040
아직 이야기하지 않은 한 가지는, 3D convolution neural
network가 무엇인가 하는 점입니다.

565
00:54:23,040 --> 00:54:30,640
여기서 무엇을 사용해야 할까요? 과거에 사람들이 탐구한 매우 흥미로운
아이디어는, 우리가 알고 있는 성공적인

566
00:54:30,640 --> 00:54:35,520
2D convolution neural network

567
00:54:35,520 --> 00:54:41,280
아키텍처를 직접 3D에 재사용할 수 있느냐는 것입니다.

568
00:54:41,840 --> 00:54:48,560
2D 네트워크를 확장(inflation)하는 방식입니다. 그렇게 하면 3D
convolution neural network를 얻을 수 있습니다.

569
00:54:48,560 --> 00:54:55,120
이 작업은 I3D 아키텍처라고 불립니다. 아이디어는 2D

570
00:54:55,120 --> 00:55:05,760
CNN 아키텍처를 가져와서, 각 2D conv pool 레이어, 즉 Kh
곱하기 Kw 차원을 갖는 레이어를,

571
00:55:05,760 --> 00:55:15,520
3D 버전인 Kt 곱하기 Kh 곱하기 Kw로 교체하는 것입니다. 기본적으로
확장한 거죠. 그리고 inception

572
00:55:15,520 --> 00:55:28,320
블록 위에 그것을 사용합니다. 그 다음에 이 inflation을 한 후에,
비디오를 처리하는 아키텍처가 생기고,

573
00:55:28,320 --> 00:55:35,200
기존 아키텍처를 직접 재사용할 수 있습니다.

574
00:55:35,200 --> 00:55:42,240
이제 2D에서 꽤 잘 작동하는 아키텍처를 3D에서도

575
00:55:42,240 --> 00:55:47,040
작동하도록 전이할 수 있습니다. 하지만 한 걸음 더 나아가서,

576
00:55:49,600 --> 00:55:54,320
사람들은 아키텍처뿐만 아니라 실제로 가중치도 전이할

577
00:55:54,320 --> 00:56:00,800
수 있다는 시도를 해왔습니다. 이미 이미지 데이터셋으로 많은 아키텍처 모델을
사전 학습했기 때문입니다.

578
00:56:00,800 --> 00:56:03,840
어쩌면 거기서 학습한 가중치를 재사용할 수 있을지도 모릅니다. 좋은 사전
정보가

579
00:56:03,840 --> 00:56:08,240
있을 수 있거든요. 그래서 할 수 있는 한 가지 방법은, 이미지로 학습된
가중치로

580
00:56:09,360 --> 00:56:12,240
inflated CNN을 초기화하는 것입니다.

581
00:56:12,240 --> 00:56:22,000
예를 들어, 원래 2D conv 커널이 하나 있다고 합시다. 그 커널을
Kt만큼

582
00:56:22,000 --> 00:56:29,680
복사합니다. 그리고 Kt로 나눕니다. 원래는 단일 이미지를 입력으로 받았죠.

583
00:56:29,680 --> 00:56:34,880
이제는 3 x Kt x H x W 크기의

584
00:56:34,880 --> 00:56:42,912
비디오를 입력으로 받습니다. 왜냐하면 Kt로 나누었기 때문입니다. 이
inflated 버전을 사용해서 가중치를 Kt만큼 복사합니다. 그러면 단일
프레임을 입력하든, 일정한 프레임

585
00:56:42,912 --> 00:56:51,440
수의 비디오를 입력하든 같은 출력을 얻을 수 있습니다.

586
00:56:51,440 --> 00:56:59,520
이제 2D 이미지 기반 아키텍처와 2D 이미지 이해에서

587
00:56:59,520 --> 00:57:05,760
얻은 가중치를 재활용하는 방법이 생겼습니다. 그리고 실제로 꽤 잘
작동합니다. 성능을 보면,

588
00:57:05,760 --> 00:57:10,400
inflation을 했을 때 이 two-stream convolutional
network와

589
00:57:10,400 --> 00:57:14,080
비교해서 실제로 더 좋은 성능을 냅니다. 그리고 appearance
프레임뿐만 아니라

590
00:57:14,080 --> 00:57:20,880
motion stream도 inflation할 수 있습니다. 그래서 추가적인
성능 향상을 얻습니다.

591
00:57:20,880 --> 00:57:25,840
기본적으로, 이것은 3D convolutional

592
00:57:25,840 --> 00:57:33,600
network와 독립적으로 재사용할 수 있는 기술입니다. non-local
블록, 이 부분도 만들 수 있습니다. 제가 말하고 싶은 것은, 2D
convolution

593
00:57:33,600 --> 00:57:40,800
neural network와 그 가중치가 많다는 겁니다.

594
00:57:40,800 --> 00:57:45,840
성공한 사람들이 그것들이 매우 성공적임을 보여주었고, 우리가 그것들을
재사용하고 싶다면,

595
00:57:45,840 --> 00:57:52,080
가중치를 복사해서 비디오에 직접 사용할

596
00:57:53,120 --> 00:57:58,480
수 있다는 것을 보여주었습니다. 이것이 기본적인 큰 아이디어입니다.

597
00:57:58,480 --> 00:58:05,040
이 초기화를 한 후에도 비디오 데이터에 대해 미세 조정할 수 있습니다.
하지만 이미지에서 사전 학습된 가중치를

598
00:58:05,040 --> 00:58:08,400
가지고 있기 때문에, 비디오 모델 학습에 좋은

599
00:58:08,400 --> 00:58:12,160
초기값을 제공할 수 있습니다. 이 I3D 네트워크 아이디어는 기본적으로

600
00:58:13,120 --> 00:58:18,960
가중치를 복사하고 인플레이션을 하는 것을 시도하는 겁니다.

601
00:58:20,560 --> 00:58:26,400
이것도 비디오 이해 네트워크 모델의 한 예일 뿐입니다. 그리고 비디오 이해를
위해 제안된

602
00:58:26,400 --> 00:58:33,440
다른 많은 비디오 전이 모델들도 있습니다. 예를 들어, 이 연구는 시공간
주의를 더 분해된

603
00:58:34,560 --> 00:58:37,840
형태로 하여 공간과

604
00:58:37,840 --> 00:58:42,640
시간을 모두 주목하려고 합니다.

605
00:58:42,640 --> 00:58:46,720
또한, 이 트랜스포머 아키텍처

606
00:58:46,720 --> 00:58:54,000
측면에서 더 효율적이려고 하는 다른 방법들도 있습니다. 또는 여러분이 들어본
마스크 오토인코더는 더 효율적이고 확장 가능한 비디오

607
00:58:54,000 --> 00:58:59,200
수준 사전학습을 통해 비디오 이해를 하려고 합니다. 그래서 여기 수업에서는
자세히

608
00:58:59,200 --> 00:59:02,720
다루지 않겠습니다. 하지만 관심이 있다면 논문들을 확인해 보실 수 있습니다.
더 나은 미디어 이해 모델을

609
00:59:02,720 --> 00:59:07,760
위한 많은 진전이 이루어지고 있기 때문입니다.

610
00:59:07,760 --> 00:59:10,800
그리고 성능 진전을 보면, 우리는 단일 프레임

611
00:59:10,800 --> 00:59:16,720
모델 62.2에서 시작했는데—이것은 또 다른 데이터셋인
Kinetics-400입니다.

612
00:59:16,720 --> 00:59:22,240
이것은 큰 비디오 데이터셋입니다. 그리고 이제 이 비디오 모델 인코더는

613
00:59:22,240 --> 00:59:30,880
이미 90% 정확도에 도달한 것을 볼 수 있습니다. 또 다른 새로운
트랜스포머 모델도 제안되었습니다.

614
00:59:30,880 --> 00:59:39,440
그래서 비디오 분류에서 매우 좋은 성과를 내고 있습니다. 지난 수업의 이미지
분류와 마찬가지로, 비디오 모델 시각화에도

615
00:59:39,440 --> 00:59:45,200
비슷한 기법을 사용할 수 있습니다. 예를 들어, 이 투스트림

616
00:59:45,200 --> 00:59:51,440
네트워크를 예로 들 수 있습니다. 우리는 appearance 이미지와
flow 이미지를 무작위로

617
00:59:51,440 --> 00:59:57,680
초기화하고, 순전파를 수행한 후 점수를 계산할 수 있습니다.

618
00:59:57,680 --> 01:00:01,840
그 다음 특정 클래스의 점수에 대해 역전파를

619
01:00:01,840 --> 01:00:08,880
수행하고, 분류 점수를 최대화하기 위해 그래디언트 상승법을 사용할 수
있습니다. 이미지 기반 모델의

620
01:00:08,880 --> 01:00:15,760
시각화를 할 때와 똑같이요. 이렇게 해서 학습된 내용을 시각화하고

621
01:00:15,760 --> 01:00:20,240
해석할 수 있습니다.

622
01:00:20,240 --> 01:00:26,320
왼쪽은 appearance 스트림에 대해 최적화된 이미지입니다. 시각
스트림에서 무슨 일이 일어나고

623
01:00:26,320 --> 01:00:34,240
있는지 추측하기 어려울 수도 있습니다. 오른쪽은 flow 스트림에 대해
최적화된 이미지입니다. 하나는 temporal 스트림이 너무 빠르게

624
01:00:34,240 --> 01:00:41,280
변하지 않도록 시간적 제약을 둡니다. 그래서 느린 움직임을

625
01:00:41,280 --> 01:00:46,800
포착할 수 있습니다. 다른 하나는 빠른 움직임을 포착합니다. 그래서 어떤
행동인지 추측할 수 있습니다.

626
01:00:46,800 --> 01:00:55,040
이 경우에는 꽤 명확합니다. 이 행동이 무엇인지요. 이것은
웨이트리프팅입니다. 가운데 이미지는 바를 흔드는

627
01:00:55,040 --> 01:01:01,920
동작을 하고 있습니다. 오른쪽 이미지는 머리 위로 밀어

628
01:01:02,560 --> 01:01:08,640
올리는 동작을 하고 있습니다. 그래서 실제로 이 비디오 모델이 움직임에 대해

629
01:01:08,640 --> 01:01:13,120
뭔가를 학습하고 있다는 것을 볼 수

630
01:01:16,480 --> 01:01:23,360
있습니다. 지금까지는 이 짧은 클립들, 예를 들어 수영이나

631
01:01:23,360 --> 01:01:30,560
달리기를 어떻게 분류할 수 있는지에 대해 이야기했습니다. 하지만 또 다른
매우 중요한 것은, temporal action

632
01:01:31,600 --> 01:01:37,200
localization이라고 하는 다른 작업입니다. 이는 단순히

633
01:01:37,200 --> 01:01:42,240
클립 단위 분류만 하는 것이 아니라는 겁니다. 때로는 객체 탐지처럼 위치를
정확히 찾아내고 싶을 때가 있습니다.

634
01:01:42,240 --> 01:01:46,800
이제는 비디오에서 언제 행동이 일어나는지를 위치를 찾아내고 싶습니다. 어떤
때는 사람이

635
01:01:46,800 --> 01:01:51,440
달리고 있을 수도 있고, 어떤 때는 점프하고 있을 수도 있습니다. 그래서 또
다른 작업이 있습니다. 이것이 temporal action

636
01:01:51,440 --> 01:01:57,840
localization이라는 또 다른 분류입니다. faster R-CNN에서
사용한 비슷한

637
01:01:57,840 --> 01:02:03,600
아이디어를 사용할 수 있습니다. 일정한 시간 구간을
제안(proposal)으로 생성한 뒤 분류를 수행할 수 있습니다.

638
01:02:03,600 --> 01:02:08,400
또한 두 가지를 모두 할 수도 있습니다. 이것이 바로 spatial
temporal detection입니다. 기본적으로 공간뿐만 아니라
시간에서도

639
01:02:09,680 --> 01:02:14,000
위치를 찾아내고 싶다는 뜻입니다. 공간상에서 행동이 일어나는 위치,

640
01:02:14,000 --> 01:02:20,640
시간상에서 행동이 일어나는 위치를 찾는 겁니다. 이것이 spatial
temporal detection이라는 또 다른 작업입니다.

641
01:02:21,920 --> 01:02:27,520
지금까지는 temporal stream과 3D CNN,

642
01:02:27,520 --> 01:02:33,440
two-stream neural networks,
spatial-temporal

643
01:02:33,440 --> 01:02:39,520
self-attention 같은 아키텍처에 대해 이야기했습니다. 그리고
그것을 구현하는 몇 가지 도구에 대해서도 이미 이야기했죠.

644
01:02:39,520 --> 01:02:46,080
마지막 10분 정도 남았으니 다시 한번 정리해 보겠습니다. 시간 내에 마치길

645
01:02:46,080 --> 01:02:50,160
바랍니다. 오늘 시작할 때 보여드린

646
01:02:50,160 --> 01:02:56,698
영상 예제로 다시 돌아가 보겠습니다. 하지만 그게 아직 전체 그림은
아닙니다.

647
01:02:56,698 --> 01:02:56,714
[영상

648
01:02:56,714 --> 01:03:10,533
재생] [아기 웃음소리] [개 짖는 소리] [재생 종료] 그래서 우리는
영상을

649
01:03:10,533 --> 01:03:10,546
보

650
01:03:10,546 --> 01:03:10,560
고

651
01:03:10,560 --> 01:03:13,120
있습니다. 비디오 이해에는 지금까지

652
01:03:13,120 --> 01:03:18,240
다루지 않았던 아주 중요한 또 다른 차원이 있습니다. 바로

653
01:03:18,240 --> 01:03:22,000
소리입니다. 오디오가 있죠. 영상에는 다른 모달리티들도 존재합니다. 그
요소를 놓치면

654
01:03:22,000 --> 01:03:26,400
많은 재미를 잃게 됩니다. 감정을 느낄 수 있거든요.

655
01:03:26,400 --> 01:03:31,360
시각과 움직임을 결합하면 또 다른 상호작용도 가능합니다.

656
01:03:31,360 --> 01:03:34,800
그래서 오디오와 비전을 함께 고려하면,

657
01:03:34,800 --> 01:03:38,400
사람들은 많은 흥미로운 과제들을 제안했습니다. 또한 비디오 이해를 위한

658
01:03:38,400 --> 01:03:43,360
다른 과제들도 탐구해 왔습니다. 여기 또 다른 예가 있습니다. 비디오에서
여러 개의

659
01:03:44,800 --> 01:03:48,800
객체나 여러 명의 화자가 있을 수 있죠.

660
01:03:48,800 --> 01:03:53,680
그리고 실제로 제가 과거에 개인적으로 탐구했던 작업 중 하나가

661
01:03:53,680 --> 01:03:57,280
시각적으로 안내된 오디오 소스 분리입니다. 시각적이고 음향적인 정보를

662
01:03:57,280 --> 01:04:01,600
함께 처리하려고 시도하는 거죠. 시각 정보를 사용해서 소스

663
01:04:01,600 --> 01:04:06,080
분리를 안내할 수 있습니다. 소리 구성 요소를 분리하고 싶습니다. 원래는
혼합된 상태일

664
01:04:06,080 --> 01:04:08,560
수 있죠. 시각 정보를 사용해서

665
01:04:08,560 --> 01:04:12,480
소리 구성 요소로 분리하려는 겁니다. 이것을 시각적으로 안내된 오디오 소스
분리라고 합니다.

666
01:04:12,480 --> 01:04:17,440
이 작업에 대한 예를 하나 들어보겠습니다. 예를 들어, 여기 음성 혼합물이
있습니다. 때때로 각 사람의 소리를 개별적으로

667
01:04:17,440 --> 01:04:21,600
듣고 싶을 때가 있죠. 그럴 때 우리는 시각 정보와

668
01:04:21,600 --> 01:04:25,200
음성 정보를 함께 사용해서

669
01:04:25,200 --> 01:04:29,280
소리를 분리할 수 있습니다. 그래서 여기서 할 수 있는 일이 있습니다. 왼쪽
스피커의 목소리를 분리할 수 있습니다.

670
01:04:29,280 --> 01:04:33,920
이렇게 할 수 있는 것은 사람의 음성에 대해서만 가능합니다. 오디오, 음성,
그리고 시각 스트림을

671
01:04:33,920 --> 01:04:38,000
함께 처리해야 할 때 말입니다. 하지만 우리는 음악 악기 같은 다른 종류의

672
01:04:38,000 --> 01:04:41,680
소리에도 이 방법을 적용할 수 있습니다. 여기 또 다른 예가 있습니다. 음악
악기 분리도 모션과

673
01:04:41,680 --> 01:04:46,160
객체 중심 정보, 오디오

674
01:04:46,160 --> 01:04:51,920
스트림을 분석해서 분리할 수 있습니다. 이것이 이 작업에 대한 또 다른
예입니다.

675
01:04:51,920 --> 01:04:56,880
그리고 오디오라는 새로운 모달리티를 도입하면 단순히

676
01:04:56,880 --> 01:05:01,280
비디오 이해 분류만 하는 것이 아닙니다. 오디오도 유용한 단서가 될 수
있습니다. 실제로 이미지나 비디오를 패치로 매핑하는 것뿐만

677
01:05:01,280 --> 01:05:04,720
아니라, 트랜스포머 어텐션 기반

678
01:05:04,720 --> 01:05:10,480
모델에서 제안된 오디오-비주얼 비디오 이해 작업도 있습니다.

679
01:05:10,480 --> 01:05:15,040
오디오 스펙트럼을 패치로 매핑하고

680
01:05:15,040 --> 01:05:21,040
트랜스포머 아키텍처를 사용해 분류를 수행하는 거죠. 또는 마스크 오토인코더
스타일도 할 수 있습니다. 이미지와 스펙트로그램의 패치를 예측하면서

681
01:05:21,040 --> 01:05:26,240
비디오 이해를 하는 겁니다.

682
01:05:28,000 --> 01:05:33,440
또한 사람들이 탐구하는 또 다른

683
01:05:33,440 --> 01:05:39,920
측면은 효율적인 비디오 이해 방법입니다. 몇 가지 예를 빠르게
보여드리겠습니다. 이 수업 내내 저는 주로 클립

684
01:05:39,920 --> 01:05:45,360
단위 분류에 집중하고 있습니다. 클립 하나를 주고 어떻게

685
01:05:45,360 --> 01:05:50,400
분류하는지 다루는 거죠. 그리고 많은 클립을 분류한 후 정보를 모아

686
01:05:50,400 --> 01:05:54,560
비디오 단위 예측을 하고 싶을 때, 그것이 긴 비디오에서의 행동 인식입니다.

687
01:05:54,560 --> 01:05:58,480
효율적인 비디오 이해를 위해 왜

688
01:05:58,480 --> 01:06:02,400
효율적인 비디오 이해를 해야 할까요? 비디오가 매우 길기 때문입니다. 모든
클립을 하나씩

689
01:06:02,400 --> 01:06:09,360
처리할 여유가 없습니다. 그래서 단일 클립의 효율성을 높이려고 하는데, 예를
들어 X3D는

690
01:06:09,360 --> 01:06:14,640
더 나은 3D 컨볼루션 신경망을 만들려고 합니다.

691
01:06:14,640 --> 01:06:22,960
또한 SD sampler처럼 어떤 클립이

692
01:06:23,600 --> 01:06:29,360
가장 유용한지 예측하려고 합니다. 그런 다음 예측 결과만 결합할 수
있습니다. 중요한 클립에 대해서만 클립

693
01:06:29,360 --> 01:06:35,600
분류기를 실행할 수 있습니다. 또한 정책 학습을 통해 어떤 모달리티를
사용해야 할지 예측하려고

694
01:06:35,600 --> 01:06:39,440
합니다. 예를 들어 비디오를 몇 개 사용할지,

695
01:06:39,440 --> 01:06:45,520
오디오나 다른 센서 데이터를 사용할지 선택할 수 있습니다.

696
01:06:46,160 --> 01:06:51,360
여기 오디오를 미리보기 메커니즘으로

697
01:06:52,880 --> 01:07:00,480
사용해 중요한 순간이 어디인지 예측하는 예가 있습니다. 그런 다음 이를
가이드 단서로 사용해 클립을

698
01:07:00,480 --> 01:07:05,840
처리하고 결과를 평균화합니다. 이것이 효율적인 비디오

699
01:07:05,840 --> 01:07:10,400
이해에 관한 내용입니다. 이것도 연구 분야 중 하나입니다.

700
01:07:10,400 --> 01:07:16,640
그리고 요즘은 VR, AR, 스마트 글래스로 이동하는 사람들이 있습니다. 또
앞으로는 1인칭 시점의 비디오 스트림이

701
01:07:16,640 --> 01:07:20,560
많아질 것으로 예상합니다. 그것이 비디오 이해의

702
01:07:20,560 --> 01:07:24,640
또 다른 측면입니다. 그래서 여러분은 이 자기 중심 비디오뿐만 아니라

703
01:07:24,640 --> 01:07:29,680
다중 마이크, 마이크 배열, 다중 채널 오디오도 가지고 있습니다.

704
01:07:30,880 --> 01:07:33,120
이 자기 중심 멀티모달 비디오

705
01:07:33,120 --> 01:07:38,000
스트림에서 비디오 이해를 더 잘하는 방법도 뜨거운 주제입니다. 우리는 비디오
스트림, 오디오, 다중 채널

706
01:07:39,360 --> 01:07:45,360
오디오, 시각 정보를 사용해 누가 누구에게 말하고 누가

707
01:07:45,360 --> 01:07:50,720
누구를 듣는지 예측할 수 있다는 것을 탐구했습니다. 미래에는 여러분이 이런
스마트 안경을

708
01:07:50,720 --> 01:07:55,360
착용한다고 상상해 보세요. 이것을 사용해 다양한 사회적 상호작용을

709
01:07:55,360 --> 01:08:00,640
이해하는 데 도움을 받고 싶을 겁니다. 이것이 바로 자기 중심 비디오
이해입니다.

710
01:08:00,640 --> 01:08:04,880
마지막 슬라이드로, 현재 LLM들도

711
01:08:06,240 --> 01:08:09,520
비디오 수준의 기초 모델을

712
01:08:09,520 --> 01:08:14,320
구축하려는 많은 연구가 진행 중입니다. 비디오 이해를 LLM에 어떻게 통합할
것인가 하는 문제입니다. 실제로 비디오를 토크나이즈하고 LLM 임베딩

713
01:08:14,320 --> 01:08:23,280
공간에 매핑한 뒤, 비디오 기초 모델에 '사람이 어디에 있고, 무엇을

714
01:08:23,280 --> 01:08:27,600
하는지'를 프롬프트하는 연구들이 있습니다.

715
01:08:27,600 --> 01:08:35,680
그리고 비디오를 설명하는 텍스트를 출력하죠. 그래서 비디오 이해를 LLM에
통합하려는

716
01:08:35,680 --> 01:08:41,520
연구가 많이 진행 중입니다. 이것도 현재 매우 뜨거운 주제입니다.
