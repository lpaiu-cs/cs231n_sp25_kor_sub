1
00:00:05,600 --> 00:00:09,040
강의 시작 부분에서, 우리는 몇 명의 게스트 강사를

2
00:00:09,760 --> 00:00:15,120
초청할 것이라고 발표했습니다. 이들은 이전에 이 과목을 가르쳤던 분들로,

3
00:00:15,120 --> 00:00:18,960
그들이 잘 아는 주제에 대해 단독 강의를 하게 됩니다. 오늘 그 첫 번째

4
00:00:18,960 --> 00:00:24,880
강의를 하게 되어 매우 기쁩니다. 그럼 Dr.를 소개하겠습니다. 루한
가오입니다.

5
00:00:24,880 --> 00:00:29,440
그는 메릴랜드 대학교

6
00:00:30,080 --> 00:00:35,280
컴퓨터 과학과의 조교수입니다. 그곳에서 다감각 기계 지능 연구소를 이끌고
있습니다. 그는 2022년부터

7
00:00:35,280 --> 00:00:44,160
2023년까지 CS231N의 강사였습니다. 그리고 이는 그가 페이페이 리,
자이준 우, 실비오 사바레세와 함께

8
00:00:45,280 --> 00:00:49,520
포스트닥을 진행하는 동안의 일입니다. 그럼 더 이상 지체하지 않고,

9
00:00:49,520 --> 00:00:53,680
오늘 발표를 루한에게 맡기겠습니다.

10
00:00:53,680 --> 00:01:00,400
네, 감사합니다. 안녕하세요, 여러분. CS231N 수업에 다시 돌아오게

11
00:01:00,400 --> 00:01:05,840
되어 정말 기쁩니다. 저는 루한입니다, [? 젠 ?] 소개된 대로.
보시다시피, 저는 다중 모달

12
00:01:05,840 --> 00:01:10,560
관련에 매우 관심이 많습니다. 그래서 시각뿐만 아니라, 우리가 인간처럼
오디오, 촉각

13
00:01:10,560 --> 00:01:16,160
또는 다른 감각 모달리티를 어떻게 활용할 수 있는지, 이 다감각 세계를

14
00:01:16,160 --> 00:01:21,280
인식하고 이해하며 상호작용할 수 있는 방법에 대해 이야기합니다.

15
00:01:21,280 --> 00:01:25,120
하지만 물론, 시각이 가장 중요한 모달리티입니다. 그래서 우리는 이 과정을

16
00:01:25,120 --> 00:01:30,560
마련했습니다, 컴퓨터 비전을 위한 딥러닝. 지금까지 여러분이 이미지

17
00:01:30,560 --> 00:01:38,880
분류에 매우 익숙하다는 것을 확신합니다. 이런 2D 이미지를 주어졌을 때,
그것이 개인지, 고양이인지, 트럭인지,

18
00:01:38,880 --> 00:01:43,520
비행기인지 레이블을 어떻게 붙일 수 있는지. 이것이 2D 기반 이미지
분류입니다.

19
00:01:43,520 --> 00:01:47,920
지난 강의에서 여러분은 이미지에서 수행할

20
00:01:47,920 --> 00:01:53,120
수 있는 다른 작업들도 배웠을 것입니다. 단순히 고양이라고 말하기 위해 단일
레이블을 할당할 수 있는 것뿐만 아니라. 또한, 의미론적 분할을 통해 그림을
서로 다른 부분,

21
00:01:53,120 --> 00:01:59,120
구성 요소로 나누고, 풀, 고양이, 나무가 어디에 있는지와

22
00:01:59,120 --> 00:02:02,880
같은 의미론적 의미를 가질 수 있습니다.

23
00:02:02,880 --> 00:02:08,880
그리고 이미지에서 감지한 객체 위에 바운딩 박스를 놓아 개가

24
00:02:08,880 --> 00:02:13,040
어디에 있는지, 고양이가 어디에 있는지 볼

25
00:02:13,040 --> 00:02:18,720
수 있으며, 각 카테고리에 대해 두 마리의 개가 있다면 각

26
00:02:18,720 --> 00:02:22,560
카테고리에 대한 분할 마스크를 원합니다. 이것이 인스턴스 분할입니다.

27
00:02:22,560 --> 00:02:28,880
그래서 2D 이미지를 기반으로 할 수 있는 많은 작업, 분류 및 인식 작업이
있습니다.

28
00:02:28,880 --> 00:02:34,480
하지만 그것이 우리가 컴퓨터 비전 시스템을 위해 할 수 있는 유일한 것은
아닙니다. 그리고 우리의 세계는

29
00:02:34,480 --> 00:02:41,440
이렇게 정적이지 않습니다. 이 이미지를 보면, 지금까지 여러분이 이

30
00:02:41,440 --> 00:02:49,840
모델을 훈련시켜 이것이 거실이라고 분류할 수

31
00:02:49,840 --> 00:02:55,760
있는 많은 도구를 배웠기를 바랍니다. 여러분은 또한 도구를 가지고 있습니다.
당신은 이것이 개이고 이것이 아기임을 보기 위해

32
00:02:55,760 --> 00:03:00,800
경계 상자를 설정하는 방법을 배웠습니다. 또한, 감지한 객체가 이미지에서
어디에 있는지 보기

33
00:03:00,800 --> 00:03:05,920
위해 세그멘테이션 마스크를 사용할 수도 있습니다.

34
00:03:05,920 --> 00:03:10,400
그래서 오늘은 비디오 이해에 집중할 것입니다. 좀 더 공식적으로, 비디오란
무엇인가요?

35
00:03:10,400 --> 00:03:16,480
기본적으로 비디오는 이 2D 이미지에 시간 차원이 추가된 것입니다. 추가적인
시간 차원이 있습니다. 이제 우리는 3D 이미지뿐만 아니라

36
00:03:16,480 --> 00:03:23,520
4D에서도 문제를 다루고 있습니다. 우리는 이것을 3배의 T로

37
00:03:25,600 --> 00:03:30,080
가지고 있습니다. T는 시간 차원입니다. H와 W는 공간 차원입니다. 이제
우리는 이러한 종류의 이미지와

38
00:03:30,080 --> 00:03:37,840
비디오를 비디오 프레임의 이미지 볼륨으로 고려하고 있습니다.

39
00:03:37,840 --> 00:03:43,120
예시 작업은 이미지 분류와 같은 비디오 분류입니다. 우리는 이렇게

40
00:03:43,120 --> 00:03:50,320
비디오를 받습니다. 어떤 사람이 달리고 있습니다. 우리는 이 비디오를
입력으로

41
00:03:50,320 --> 00:03:54,160
사용하고 싶습니다. 또한 우리는 딥러닝 모델을 훈련시키고 싶습니다. 우리는
이 사람이 수영, 달리기,

42
00:03:54,160 --> 00:03:58,320
점프 또는 그가 하고 있는 행동을 이

43
00:03:58,320 --> 00:04:04,560
시간적 비디오 프레임 스트림만으로 분류하고자 합니다.

44
00:04:04,560 --> 00:04:09,440
이전 강의에서 이미 교차 엔트로피 손실과 같은

45
00:04:09,440 --> 00:04:13,920
손실 함수에 대해 배웠을 것이라고 확신합니다. 그리고 이미지 분류기를 훈련할
수 있습니다. 유사한 도구를 사용할

46
00:04:14,720 --> 00:04:18,640
수 있습니다. 비디오 분류기를 훈련하면 됩니다. 특징을 추출하면 됩니다.

47
00:04:18,640 --> 00:04:23,120
같은 손실 함수를 사용하여 비디오 분류기를 훈련합니다. 현재 비디오 이해의
문제는 이전 강의에서

48
00:04:23,120 --> 00:04:28,480
배운 손실 함수를 적용할 수 있는 비디오의

49
00:04:28,480 --> 00:04:32,240
특징을 어떻게 얻을 수 있는가입니다.

50
00:04:34,080 --> 00:04:38,720
또한 이미지 분류와 비디오 분류, 비디오 이해의

51
00:04:38,720 --> 00:04:42,400
또 다른 차이점은 지금 하고자 하는

52
00:04:42,400 --> 00:04:47,760
작업이 이전 예제와는 조금 다를 수 있다는 것입니다. 이미지 분류의 경우,
일반적으로

53
00:04:47,760 --> 00:04:53,440
장면과 객체에 더 관심을 가집니다. 단순히 분류를 하고자 합니다. 객체
카테고리는 무엇인가요?

54
00:04:53,440 --> 00:04:57,120
비디오의 경우, 제가 여기 보여주는 이

55
00:04:57,120 --> 00:05:00,000
예제처럼, 행동을 분류하고자 합니다. 주로 비디오에서 사람이나

56
00:05:00,000 --> 00:05:06,160
동물이 어떤 활동을 하고 있는지를 나타내는 행동입니다. 이것이 우리가 비디오
이해에서

57
00:05:06,160 --> 00:05:12,400
일반적으로 관심을 가지는 것입니다. 따라서 인식해야 할 사물의 본질은 조금
다를 수 있습니다.

58
00:05:12,400 --> 00:05:17,520
비디오 이해를 위해 주의해야 할 또

59
00:05:17,520 --> 00:05:24,000
다른 문제는 비디오가 보통 매우 크다는 것입니다. 이미지에 대해 이야기할
때,

60
00:05:24,000 --> 00:05:30,640
그것은 단지 3배의 H와 W입니다. RGB 숫자의 단일 행렬입니다.

61
00:05:30,640 --> 00:05:34,880
하지만 이제 비디오를 고려하면, 그것은 프레임의 연속입니다. 초당

62
00:05:34,880 --> 00:05:43,280
30프레임일 수 있습니다. 영화에서는 때때로 더 높은 해상도와 시간 해상도의

63
00:05:43,280 --> 00:05:50,240
비디오 프레임을 가질 수 있습니다. 그래서 비디오를 저장할 공간을
고려합니다.

64
00:05:50,240 --> 00:05:56,320
예를 들어, 표준 해상도 비디오를 고려하면, 이 비디오를

65
00:05:56,320 --> 00:06:01,600
저장할 경우 분당 약 1.5기가바이트가 필요합니다. 더 높은 해상도, 예를

66
00:06:01,600 --> 00:06:08,960
들어 1920x1080을 고려하면. 이제 분당 약 10기가바이트가

67
00:06:08,960 --> 00:06:14,800
필요합니다. 이런 종류의 비디오 데이터를 저장하려면 거대한 공간이
필요합니다.

68
00:06:14,800 --> 00:06:18,560
또한, 이러한 데이터를

69
00:06:18,560 --> 00:06:24,240
GPU에 직접 맞출 방법이 없습니다. 입력만 있으면, 이를 저장하기 위해

70
00:06:24,240 --> 00:06:28,560
많은 저장 공간이 필요합니다. 또한, 합성곱 신경망의

71
00:06:28,560 --> 00:06:36,560
가중치와 활성화와 같은 다른 것들도 저장해야 합니다. 그래서 모델이 매우
커질 것입니다.

72
00:06:36,560 --> 00:06:44,960
해결책은 비디오를 작게 만들어 처리 가능하게 하는 것입니다. 하나의 간단한
해결책은 비디오를

73
00:06:44,960 --> 00:06:51,760
작게 만드는 것입니다. 고화질 비디오와 원본 비디오가 길더라도, 우리는

74
00:06:51,760 --> 00:06:58,720
시간적으로나 공간적으로 모두 축소할 수 있습니다.

75
00:06:59,920 --> 00:07:05,040
예를 들어, 3.2초 비디오의 경우, 각 초마다

76
00:07:05,040 --> 00:07:09,600
모든 프레임이 필요하지 않을 수 있습니다.

77
00:07:09,600 --> 00:07:13,360
비디오 프레임에 많은 중복이 있으므로 다섯 개의 프레임만 가져옵시다.

78
00:07:13,360 --> 00:07:19,680
초당 프레임을 가져오고, 112x112와 같은

79
00:07:19,680 --> 00:07:24,000
더 작은 공간 해상도를 가집니다. 이제 비디오를

80
00:07:24,000 --> 00:07:32,240
약간 작게 만들 수 있습니다. 예를 들어, 이 간단한 비디오의 경우
588KB입니다. 하지만 확실히, 컴퓨팅이 가능하다면 더

81
00:07:32,240 --> 00:07:36,960
큰 해상도도 가능합니다.

82
00:07:36,960 --> 00:07:45,120
그리고 이러한 긴 비디오에서 모델을 어떻게 훈련할까요? 이전 슬라이드에서
3.2초 비디오 분류기를 훈련하고

83
00:07:45,120 --> 00:07:49,520
있다고 보여주었습니다. 하지만 비디오는 매우 길 수

84
00:07:49,520 --> 00:07:54,400
있으며, 분이나 시간일 수 있습니다. 사람들이 하는 한 가지 방법은 클립을
훈련하는 것입니다.

85
00:07:54,400 --> 00:08:00,320
비디오 프레임의 조각을 사용하여 비디오 분류기로 훈련합니다.

86
00:08:00,320 --> 00:08:04,960
우리가 하는 것은 짧은 클립을 저

87
00:08:06,240 --> 00:08:12,320
fps로 분류하는 모델을 훈련하는 것입니다. 그리고 우리는 슬라이딩 윈도우를
사용합니다. 많은 다양한 클립을 샘플링하고 이를

88
00:08:12,320 --> 00:08:16,720
훈련 데이터로 사용합니다. 그리고 우리는 분류기를 훈련합니다. 그리고 테스트
중, 추론 시간 동안,

89
00:08:16,720 --> 00:08:22,400
우리는 다른 클립에서 모델을 실행합니다. 우리는 몇 개의 클립을
샘플링합니다. 우리는 10개의 클립을 만들었습니다. 그리고 예측

90
00:08:22,400 --> 00:08:29,520
결과를 평균냅니다. 그것이 이 긴 비디오에 대한 우리의 예측입니다.

91
00:08:29,520 --> 00:08:37,440
그리고 우리가 사용할 수 있는 동일한 포스트, 즉 비디오 분류 모델은
무엇인가요? 그래서 기본적으로 비디오는 이미지의 시퀀스,

92
00:08:37,440 --> 00:08:45,760
즉 비디오 이미지 프레임의 시퀀스와 같습니다. 그래서 우리는 그것들을
이미지로 취급하는

93
00:08:45,760 --> 00:08:50,800
간단한 방법이 있습니다. 그것이 우리가 이미 가지고

94
00:08:50,800 --> 00:08:56,640
있는 가장 간단한 도구입니다. 우리는 이미 모든 도구가 있기 때문에 단일
프레임 합성곱 신경망을 실행합니다.

95
00:08:56,640 --> 00:09:00,720
우리는 이미지 분류기를 훈련할 수 있다는 것을 배웠습니다. 이미지 분류기를
가져와서 이러한 비디오 프레임

96
00:09:00,720 --> 00:09:05,200
위에서 실행하고 그것들을 이미지로 취급하면,

97
00:09:05,200 --> 00:09:09,200
실제로 괜찮은 예측을 얻을 수 있습니다. 특히 이런 비디오의 경우. 비디오
간에 큰 변화가 없다는

98
00:09:09,200 --> 00:09:13,120
것을 볼 수 있습니다. 사람이 달리고 있습니다. 몸에서 약간의 다른

99
00:09:13,120 --> 00:09:17,760
움직임이 있을 수 있습니다. 하지만 일반적으로 꽤 비슷해 보입니다.

100
00:09:17,760 --> 00:09:25,040
어쩌면 이미지 액션 분류기를 실행할 수 있습니다. 모든 프레임에서 아마도
모든 프레임이 달리고

101
00:09:25,040 --> 00:09:29,440
있다고 말할 것입니다. 각 이미지, 각 비디오 프레임에서 예측 결과를
평균내면,

102
00:09:29,440 --> 00:09:34,480
이 특정 비디오에 대해 달리고 있다고 예측할 것입니다.

103
00:09:34,480 --> 00:09:40,960
그래서 실제로, 이것은 이 간단한 이미지 분류기에 대해 매우

104
00:09:41,920 --> 00:09:45,600
강력한 기준선입니다, 특히 이런

105
00:09:45,600 --> 00:09:51,200
비디오의 경우 비디오 간에 큰 변화가 없기 때문입니다. 그래서 비디오
분류기를 설계하려고 한다면, 항상 이것을 먼저

106
00:09:51,200 --> 00:09:56,560
실행해야 합니다. 왜냐하면 시도하기에 간단한 것이기 때문입니다. 그리고
아마도 이미 꽤 괜찮은 결과를 얻을 수 있습니다.

107
00:09:56,560 --> 00:10:02,400
그래서 질문은, 단일 프레임을 실행할지 아니면 프레임 덩어리를
실행할지입니다. 그래서 이것에 대해, 간단한

108
00:10:02,400 --> 00:10:08,480
단일 프레임을 말하겠습니다. 기본적으로 우리는 30프레임의 비디오를 가지고
있습니다. 아마도 몇 개의 프레임을

109
00:10:08,480 --> 00:10:13,360
샘플링할 수 있습니다. 그리고 샘플링한 10프레임에서 이미지

110
00:10:13,360 --> 00:10:18,720
분류기를 실행하고 그것들을 이미지로 취급합니다. 그리고 결과를 직접
평균냅니다. 그것이 기본적으로 프레임별 CNN입니다.

111
00:10:18,720 --> 00:10:22,000
그래서 당신이 매우 중요한 질문이 있다고 생각합니다. 프레임을 어떻게
샘플링할까요?

112
00:10:22,000 --> 00:10:26,480
그것이 매우 중요한 질문입니다. 우리는 몇 개의 프레임을 샘플링하고 그
프레임에서

113
00:10:26,480 --> 00:10:30,000
CNN을 실행하고 싶다고 말씀드리고 있습니다. 그럼 그 프레임을

114
00:10:30,000 --> 00:10:33,680
어떻게 얻을까요? 사실 이것은 현재도 활발한 연구 분야입니다.

115
00:10:33,680 --> 00:10:38,800
한 가지 간단한 방법은 무작위 샘플링을 하는 것입니다. 당신은 한 시간
분량의 비디오를 가지고 있습니다. 흥미로운 부분이나 중요한 부분이

116
00:10:38,800 --> 00:10:42,720
어디인지 모르겠습니다. 우리는 아마 매 1분마다 하나의

117
00:10:42,720 --> 00:10:46,880
프레임을 샘플링할 것입니다. 그리고 나서 이미지 분류기를 실행합니다. 결과를
평균냅니다. 하지만 분명히, 이것이

118
00:10:46,880 --> 00:10:51,200
좋은 결과를 줄 수 있습니다. 하지만 아마도 이것이 샘플링을 하는 가장
스마트한 방법은 아닐 것입니다.

119
00:10:51,200 --> 00:10:55,040
그래서 더 스마트한 샘플링 전략을 제안하려는 다른 방법들이 있습니다.

120
00:10:55,040 --> 00:10:59,600
아마도 하나의 프레임을 샘플링할 수 있습니다. 그 결정을 사용하여 다른

121
00:10:59,600 --> 00:11:05,600
곳을 샘플링할지를 결정할 수 있습니다. 사실 나중 강의 슬라이드에서 몇 가지
예시도 있습니다.

122
00:11:05,600 --> 00:11:10,560
이것은 매우 간단한 비디오 분류기입니다. 우리가 이미지 분류기를 채택하듯이,

123
00:11:10,560 --> 00:11:17,200
단일 프레임 CNN을 사용합니다. 그리고 비슷하게, 아마도

124
00:11:17,200 --> 00:11:23,840
한 걸음 더 나아갑니다. 단일 프레임 CNN을 직접 실행하고 예측 결과를
평균내는

125
00:11:23,840 --> 00:11:32,400
대신, 단일 프레임 CNN을 사용하여 특징을 융합할 수 있습니다.

126
00:11:32,400 --> 00:11:39,200
이것은 종종 늦은 융합이라고 불립니다. 기본적으로, 우리는 여전히 일부 2D
CNN을 사용합니다.

127
00:11:39,200 --> 00:11:48,080
그리고 입력이 있을 수 있습니다, 아마도 키 프레임입니다. 각 프레임에 대해
2D CNN을 사용합니다. 그리고 나서 일부 특징

128
00:11:48,080 --> 00:11:56,080
벡터를 추출합니다. 그런 다음 D x H' x W'의 특징 맵을 얻습니다.

129
00:11:56,080 --> 00:11:59,360
그리고 T 프레임이 있기 때문에. 기본적으로

130
00:11:59,360 --> 00:12:06,800
T개의 특징 맵이 있습니다. 그런 다음 우리는 모든 특징 맵을

131
00:12:06,800 --> 00:12:12,720
벡터로 평탄화하고 연결합니다. 그럼 우리는 모든 프레임의 모든 정보와 특징을
포함하는

132
00:12:12,720 --> 00:12:16,480
거대한 특징 벡터를 가지게 됩니다.

133
00:12:16,480 --> 00:12:19,120
그리고 우리가 할 수 있는

134
00:12:19,120 --> 00:12:25,360
것은, 우리가 배운 도구인 완전 연결 네트워크를 사용하는 것입니다. 우리는
이 벡터를 일부 신경 차원으로

135
00:12:25,360 --> 00:12:30,400
매핑하는 MLP를 훈련합니다. 그리고 그 위에 분류기를 훈련하여 클래스 점수
C로 매핑합니다.

136
00:12:30,400 --> 00:12:37,040
이것을 늦은 융합이라고 합니다. 기본적으로 우리는 특징 맵을 추출한다는 것을
알 수 있습니다. 그리고 우리는 그것들을 매우

137
00:12:37,040 --> 00:12:41,600
독립적으로 처리합니다. 그리고 아주 늦은 단계에서, 우리는 특징 벡터를
연결하고 분류를

138
00:12:41,600 --> 00:12:45,280
수행하기 위해 몇 개의 완전 연결 레이어를 실행합니다.

139
00:12:45,280 --> 00:12:54,800
그래서 이것은 유용합니다. 하지만 이 예시에서 설명한 바와 같이, 이

140
00:12:54,800 --> 00:13:00,160
완전 연결 레이어는 많은

141
00:13:00,160 --> 00:13:06,240
매개변수를 도입할 것입니다. 왜냐하면 우리가 많은 것을 연결하면-- 시간을
따라 평탄화하기 때문입니다.

142
00:13:06,240 --> 00:13:10,720
그리고 이 특징 벡터는 T가 얼마나 긴지에 따라

143
00:13:10,720 --> 00:13:14,480
거대한 특징 벡터를 가질 수 있습니다. 그리고 이 거대한 특징 벡터를
사용합니다. 이것을 더 낮은

144
00:13:14,480 --> 00:13:19,760
차원으로 매핑하고 싶습니다. 그리고 매우 큰 완전 연결

145
00:13:19,760 --> 00:13:23,440
레이어가 있습니다. 그것은 많은 매개변수를 도입할 것입니다. 그래서 매우
효율적이지 않습니다.

146
00:13:23,440 --> 00:13:28,400
이것을 수행하는 또 다른 방법은 연결하는

147
00:13:29,040 --> 00:13:34,240
대신, 우리는 연결을 하지 않습니다. 우리는 단순히 거대한 특징 벡터를
사용하지 않고,

148
00:13:34,240 --> 00:13:39,920
점수를 매핑하기 위해 완전 연결 레이어를 두지 않습니다. 실제로 간단한
풀링을

149
00:13:39,920 --> 00:13:45,440
수행할 수 있습니다. 풀링을 수행하면 특징 벡터의 길이를 증가시키지
않습니다.

150
00:13:45,440 --> 00:13:51,440
기본적으로, 단일 프레임에 대한 특징 차원이 있고,

151
00:13:51,440 --> 00:13:54,160
시간을 따라 풀링하면,

152
00:13:54,160 --> 00:14:01,360
이 T 프레임에 대해 시간 집계를 수행하는 것입니다. 그리고 이 클립 특징
D를 기반으로, 풀링을 수행하면 여전히 시간

153
00:14:01,360 --> 00:14:04,480
D의 특징 벡터를 가집니다.

154
00:14:04,480 --> 00:14:10,640
그리고 D를 클래스 점수와 일치하는 차원 C로 매핑하는 선형 레이어가
있습니다. 그리고 그 위에 교차

155
00:14:10,640 --> 00:14:16,640
엔트로피 손실을 훈련합니다. 그리고 이것도 늦은 융합입니다. 하지만 이제
우리는 풀링을

156
00:14:16,640 --> 00:14:22,080
사용하고 있습니다. 여기서 좋은 점은 이제 매우 큰 완전 연결

157
00:14:22,080 --> 00:14:26,320
레이어가 필요하지 않다는 것입니다. 하지만 풀링은 중요할 수

158
00:14:28,320 --> 00:14:34,400
있는 정보를 제거할 수도 있습니다. 그래서 이것이 이 작업의 단점입니다.

159
00:14:34,400 --> 00:14:41,280
내가 늦은 융합이라고 부르는 이유는-- 중요한 부분은 늦다는 것입니다. 늦을
때, 이미 잃어버린 정보가

160
00:14:41,280 --> 00:14:46,560
있을 수 있습니다. 그래서 우리는 이 2D 컨볼루션

161
00:14:46,560 --> 00:14:52,800
네트워크를 사용하여 이미지를 처리하고 있습니다. 예를 들어, 여기 빨간
원으로 표시된 것처럼. 이 비디오에서 매우 중요한

162
00:14:52,800 --> 00:14:59,440
것은 이 남자의 발의 움직임을

163
00:14:59,440 --> 00:15:04,480
인식하는 것입니다. 위아래로 움직이고 있습니다. 그가 달리고 있다는 것을 알
수 있을 것입니다.

164
00:15:04,480 --> 00:15:10,240
따라서 단일 2D CNN을 사용하여 이를 독립적으로

165
00:15:11,040 --> 00:15:16,320
2D 이미지로 처리하고 일부 특징 맵을

166
00:15:18,720 --> 00:15:25,200
추출하면, 아주 늦은 단계의 특징 맵에서는 이 남자의

167
00:15:25,200 --> 00:15:31,040
발 움직임 정보가 더 이상 포함되지 않습니다. 발의 위아래 움직임의 일부
정보가 유용한 단서가

168
00:15:31,040 --> 00:15:35,520
되어야 하는 빨간 원으로 나타납니다. 하지만 이제 그것은 특징 맵에
없습니다.

169
00:15:35,520 --> 00:15:40,880
직접적인 직관은 초기 레이어에서 특징을

170
00:15:41,440 --> 00:15:48,400
추출하면 원본 비디오 프레임과 매우 가깝다는 것입니다. 따라서 이 저수준
정보, 즉 비디오 프레임의

171
00:15:48,400 --> 00:15:54,480
움직임이 포함될 가능성이 더 큽니다. 또한, 시간에 따라 이들을 연결하거나
풀링하면

172
00:15:54,480 --> 00:16:00,880
시간에 따른 움직임을 분석할 수 있습니다.

173
00:16:00,880 --> 00:16:05,840
하지만 많은 합성곱 풀링을 처리하기 때문에, 아주 늦은

174
00:16:05,840 --> 00:16:09,440
단계에서도 더 높은 수준의 정보, 즉 저수준의

175
00:16:09,440 --> 00:16:13,520
움직임 정보가 아닌 의미론적 정보가 포함됩니다. 그래서 가장

176
00:16:13,520 --> 00:16:19,520
가능성이 낮은 이유입니다. 이것이 늦은 융합의 단점입니다.

177
00:16:19,520 --> 00:16:24,160
늦은 융합 대신, 우리는 사실 초기 융합을 할 수 있습니다. 초기 융합을
하려면, 실제 비디오

178
00:16:24,160 --> 00:16:28,240
프레임에 더 가까운 특징 벡터를

179
00:16:28,240 --> 00:16:38,960
활용하고 싶다면, 이 입력을 가져와서 3T x H x W로 직접 재구성할 수
있습니다. 우리는 처음부터 정보를 시간적으로 직접

180
00:16:38,960 --> 00:16:42,640
집계합니다.

181
00:16:42,640 --> 00:16:47,760
그리고 나서 첫 번째 2D 합성곱

182
00:16:47,760 --> 00:16:55,200
레이어를 사용하여 채널 차원 3T에서 D로 직접 매핑합니다. 기본적으로,
우리는 첫 번째 레이어에서 이 시간 정보를 처리하기 위해

183
00:16:55,200 --> 00:17:03,200
2D 합성곱을 사용하여 채널 차원을 3T에서 D로 매핑하여 합성곱 신경망의

184
00:17:03,200 --> 00:17:08,960
초기 프레임에서 모든 정보의 비디오 프레임을 처리합니다.

185
00:17:08,960 --> 00:17:11,680
그리고 나머지 네트워크는 표준 2D CNN입니다. 그리고 유일한 차이점은
이제

186
00:17:13,040 --> 00:17:16,320
단일 레이어를 사용하여 모든 시간

187
00:17:18,240 --> 00:17:21,920
정보를 파괴하고 축소한다는 것입니다. 그리고 나머지는 이미지 분류와
같습니다. 그리고 표준 교차 엔트로피 손실을

188
00:17:21,920 --> 00:17:26,080
사용하여 이 분류를 수행합니다.

189
00:17:26,080 --> 00:17:32,320
각 프레임에 대해 D와 같은 특징을 얻습니다. 각 단일 프레임은 당신에게
특징 D를 제공합니다. 그래서 T가

190
00:17:32,320 --> 00:17:36,400
있습니다. 이 특징 벡터 D입니다. 그래서 풀링을 위해 우리는 특징을
풀링하고 있습니다. 기본적으로, 우리는 특징을 평균화하기 위해 평균 풀링을

191
00:17:36,400 --> 00:17:41,280
하거나 특징에서 최대값을 취하는 최대 풀링을 할 수 있습니다.

192
00:17:41,280 --> 00:17:48,640
그 후에도 여전히 D인 특징을 얻습니다. 그래서 프레임이 아니라 특징에 대한
풀링입니다.

193
00:17:49,200 --> 00:17:54,880
그래서 이것이 초기 융합입니다. 그러나 초기 융합의 단점은 우리가 초기
레이어에서 움직임을

194
00:17:56,240 --> 00:18:00,800
처리하려고 명시적으로 시도하고 있다는 것입니다. 하지만 우리는 너무 야심차게

195
00:18:05,200 --> 00:18:08,640
접근하고 있습니다. 우리는 모든 것을 단일 레이어에서 포착하려고 합니다.

196
00:18:08,640 --> 00:18:12,960
우리는 모든 프레임을 연결한 다음 단일 합성곱 네트워크를

197
00:18:12,960 --> 00:18:18,000
사용하여 모든 [INAUDIBLE] 정보를 축소합니다. 아마 우리가 원하는
것을 달성하지 못할 것입니다.

198
00:18:18,000 --> 00:18:23,920
그래서 또 다른 해결책은 늦은 융합이나 이른

199
00:18:23,920 --> 00:18:30,240
융합 대신에 중간 정도의 방법을 사용하는 것입니다. 그것이 느린 융합입니다.
이 3D 컨볼루션 신경망이 바로

200
00:18:30,240 --> 00:18:35,440
그런 방식으로 작동합니다. 직관적으로 우리는 이 3D 버전의 컨볼루션과
풀링을 사용하고 싶습니다.

201
00:18:35,440 --> 00:18:40,400
우리는 네트워크의 과정에 걸쳐 정보를 천천히 융합하고 싶습니다. 매우 늦은
단계나 매우 이른 단계에서

202
00:18:40,400 --> 00:18:45,760
하는 대신, 우리는 시간 차원과 공간 차원을

203
00:18:45,760 --> 00:18:52,640
점진적으로 축소하여 이 3D 특징 맵을 얻습니다. 그래서 이것이 3D
컨볼루션 신경망의 아이디어입니다.

204
00:18:52,640 --> 00:18:57,040
우리는 3D 컨볼루션과 3D 풀링 작업을 사용합니다. 그렇다면 3D
컨볼루션과

205
00:18:57,040 --> 00:19:02,400
3D 풀링이란 무엇인가요? 여러분은 2D 컨볼루션을 배웠습니다. 맞습니다.
2D 컨볼루션 레이어에 대해. 기본적으로, 이렇게 32x32x3

206
00:19:02,400 --> 00:19:10,960
이미지와 같은 이미지를 가져옵니다. 그리고 2D 컨볼루션을 사용하면,
기본적으로 각 커널에 대해 이

207
00:19:10,960 --> 00:19:15,520
필터가 있다는 것을 배웠습니다.

208
00:19:16,400 --> 00:19:23,440
슬라이딩 윈도우 접근 방식처럼 작동하는 5x5x3

209
00:19:23,440 --> 00:19:34,240
컨볼루션 커널이 있을 수 있으며, 공간을 가로질러 슬라이드하고 깊이 차원까지
진행합니다.

210
00:19:34,240 --> 00:19:42,560
그래서 각 계산에 대해 최종 활성화 맵의 단일 값에 매핑합니다.

211
00:19:42,560 --> 00:19:48,640
그리고 마지막으로, 이 경우 28x28x1의

212
00:19:48,640 --> 00:19:53,520
활성화 맵을 얻습니다. 모든 공간 위치에서 컨볼루션을 수행하고 이

213
00:19:53,520 --> 00:20:01,840
채널 차원, 깊이를 매핑하여 채널 차원에서 3에서 1로 매핑합니다.

214
00:20:01,840 --> 00:20:06,400
그래서 이것이 2D 컨볼루션입니다. 차이점은 3D 컨볼루션의

215
00:20:06,400 --> 00:20:14,960
경우 이제 하나의 추가 차원이 있다는 것입니다. 여기서 입력은 C x T x
H x

216
00:20:14,960 --> 00:20:21,520
W로 생각할 수 있습니다. 추가된 것은 이 T 차원입니다. 그것은 시간
차원입니다.

217
00:20:21,520 --> 00:20:26,720
하지만 제가 여기서 보여주는 것은 3D로만 보여줄 수 있기 때문에 4D로는
보여줄 수 없습니다.

218
00:20:26,720 --> 00:20:30,400
그래서 실제로 여기서 보여지지 않는 하나의 차원이 있습니다. 그것은 C
차원입니다. 채널 차원이 여기서

219
00:20:30,400 --> 00:20:35,200
보여지지 않습니다. 그래서 이 특징 맵의 각 그리드

220
00:20:35,200 --> 00:20:42,880
포인트에 대해 많은 특징이 있다고 생각할 수 있습니다. 그 그리드 포인트에는
C개의 특징이 있습니다.

221
00:20:42,880 --> 00:20:49,200
그리고 이 3D 컨볼루션에 대해 이야기하면, 6x6x6 컨볼루션에 대해
이야기하는

222
00:20:49,200 --> 00:20:52,800
것입니다. 추가 차원이 있기 때문입니다. 이제 이미지를 H와 W

223
00:20:52,800 --> 00:20:58,000
차원에서만 공간 차원을 슬라이드하는

224
00:20:58,000 --> 00:21:06,720
대신, 우리는 이 큐브를 슬라이드하고 있습니다. 우리는 T x H x W
차원의 이 큐브를 슬라이딩하고 있습니다.

225
00:21:06,720 --> 00:21:13,200
그래서 공간 차원과 시간 차원을 모두 포함합니다. 그리고 채널

226
00:21:13,200 --> 00:21:20,000
차원에沿해서도 진행됩니다. 그래서 점차적으로, 2D 미적분처럼 할 수
있습니다. 다른 부분은 2D 컨볼루션과

227
00:21:20,000 --> 00:21:24,480
같습니다. 단지 우리는 이 추가 차원이 있습니다.

228
00:21:24,480 --> 00:21:29,520
그럼 6 x 6 x 6 3D 컨볼루션을 얻습니다. 그리고 아마 5 x 5의

229
00:21:29,520 --> 00:21:35,120
또 다른 레이어가 있습니다. 마지막으로, 이러한 3D 컨볼루션 작업을 처리한

230
00:21:35,120 --> 00:21:39,520
후, 피처 벡터를 평탄화하고,

231
00:21:39,520 --> 00:21:45,920
완전 연결 레이어를 사용하여 클래스 점수에 매핑합니다. 그래서 이것이
기본적으로 3D 컨볼루션의 아이디어입니다.

232
00:21:46,800 --> 00:21:54,320
그래서 아마도 몇 가지 장난감 예제를 통해 더 잘

233
00:21:54,320 --> 00:22:03,680
이해하고, 초기, 후기 및 3D 컨볼루션 신경망을 비교해 보겠습니다. 작동
방식에 대한 감을 주기 위해서입니다. 실제로는 확실히 더 크고 복잡할

234
00:22:03,680 --> 00:22:07,520
수 있습니다. 하지만 여기서는 피처 맵의 크기와

235
00:22:07,520 --> 00:22:13,040
수용 필드를 통해 초기 융합과 후기 융합, 3D 컨볼루션

236
00:22:13,040 --> 00:22:16,320
신경망의 차이를 이해하기 위한

237
00:22:16,320 --> 00:22:20,000
장난감 예제를 보여주려고 합니다.

238
00:22:20,000 --> 00:22:25,920
후기 융합의 경우, 예를 들어, 이 경우 원래

239
00:22:25,920 --> 00:22:32,280
입력이 3 x 20이라고 생각할 수 있습니다. 20은 시간 차원입니다.
그리고 64,

240
00:22:32,280 --> 00:22:40,720
64는 공간 차원입니다. 그리고 2D 컨볼루션을 사용합니다. 왜냐하면 우리는
늦은 융합을 하고

241
00:22:40,720 --> 00:22:45,280
있기 때문입니다. 처음에는 시간 차원에 대해 아무것도 하지 않습니다. 우리는
단지 20, 시간

242
00:22:45,280 --> 00:22:49,040
차원을 유지합니다. 우리는 공간적으로 수용 필드를 구축합니다.

243
00:22:49,040 --> 00:22:53,520
이제 우리는 채널 차원을 3에서 12로 매핑하는 2D

244
00:22:53,520 --> 00:22:58,000
컨볼루션 레이어가 있지만, 시간 차원 20은 그대로 유지합니다. 그리고 나서
점차적으로, 아마도 몇

245
00:22:58,000 --> 00:23:02,640
개의 풀링 레이어를 사용할 것입니다. 여전히 시간 차원에 대해 아무것도 하지
않았습니다. 그래서 여전히 20입니다.

246
00:23:02,640 --> 00:23:08,960
하지만 풀링 작업 때문에 우리는 공간 차원에서 수용 필드를 구축합니다.

247
00:23:08,960 --> 00:23:14,160
그리고 나서 점차적으로, 아마도 또 다른 2D 레이어를 사용할 것입니다.
현재 특징 맵은 24 x

248
00:23:14,160 --> 00:23:19,680
20 x 16 x 16입니다. 우리는 또한 공간 수용 필드를 점차적으로
증가시키지만,

249
00:23:19,680 --> 00:23:23,920
시간 차원 20은 여전히 유지됩니다. 그래서 우리는 시간 차원에 대해
아무것도 하지 않았습니다.

250
00:23:23,920 --> 00:23:28,480
마지막으로, 단일 글로벌 평균 풀링을

251
00:23:28,480 --> 00:23:34,400
사용하여 특징 맵을 20 x 16 x 16으로 가져옵니다. 그래서 우리는
공간 차원에서 시간 모두를 끌어옵니다. 그리고 이제 우리는 20 곱하기

252
00:23:34,400 --> 00:23:39,760
16 곱하기 16을 얻습니다. 우리는 1 곱하기 1 곱하기 1의 특징점을
얻습니다. 기본적으로 우리는 최종 단일 레이어에서

253
00:23:39,760 --> 00:23:42,640
모든 것을 축소합니다. 그리고 우리는 단일

254
00:23:42,640 --> 00:23:47,760
레이어에서 시간 수용 필드를 구축합니다. 그래서 이것이 늦은 융합입니다.

255
00:23:47,760 --> 00:23:50,400
그렇다면 조기 융합의 차이는 무엇인가요? 이제 N에서 공간을 천천히

256
00:23:50,400 --> 00:23:54,320
구축하는 대신, 우리는 처음부터

257
00:23:54,320 --> 00:24:00,960
공간을 천천히 구축하고 시간에서 모두 한 번에 구축합니다. 입력은 여전히 3
곱하기 20

258
00:24:00,960 --> 00:24:07,360
곱하기 64 곱하기 64입니다. 하지만 이제 우리는 단일 2D 컨볼루션
레이어만 사용하고 있습니다.

259
00:24:07,360 --> 00:24:11,520
이제 우리는 3 곱하기 20을 단일 채널 차원으로 취급합니다. 우리는 모든

260
00:24:11,520 --> 00:24:16,640
것을 매핑합니다. 3 곱하기 30이 있습니다. 모두를 채널 차원으로

261
00:24:16,640 --> 00:24:23,200
취급하고, 12로 매핑합니다. 기본적으로 우리는 모든 시간 정보를 처음부터
축소하기 위해 단일 컨볼루션 레이어,

262
00:24:24,160 --> 00:24:26,800
2D 컨볼루션 레이어를 사용합니다.

263
00:24:26,800 --> 00:24:31,200
그래서 우리는 첫 번째 레이어에서 시간 수용 필드를 구축합니다. 이제 시간
수용 필드는

264
00:24:31,200 --> 00:24:36,320
1에서 20으로 변합니다. 그리고 공간 수용 필드는

265
00:24:36,320 --> 00:24:42,320
점차적으로 구축됩니다. 그리고 우리는 늦은 융합처럼 공간 차원을 구축하기
위해 풀링과 2D 컨볼루션을 사용합니다.

266
00:24:42,320 --> 00:24:46,080
마지막으로, 우리는 글로벌 평균 풀링을 사용합니다. 이제 글로벌 평균 풀링은

267
00:24:46,080 --> 00:24:53,360
공간을 가로질러 평균을 내고 풀링을 수행하는 것뿐입니다. 그래서 우리는
공간에서 천천히 구축하지만,

268
00:24:53,360 --> 00:24:58,000
처음부터 모두 한 번에 구축합니다. 그래서 이것이 조기 융합입니다.

269
00:24:58,000 --> 00:25:03,360
그렇다면 3D 컨볼루션 신경망은 무엇인가요? 3D 컨볼루션 레이어의 경우,
기본적으로

270
00:25:03,360 --> 00:25:08,800
우리는 공간과 시간 모두에서 천천히 구축합니다. 그래서 우리는 이것을 느린
융합이라고 부릅니다. 입력은 여전히 같은 3 곱하기 20

271
00:25:08,800 --> 00:25:14,960
곱하기 64 곱하기 64일 수 있습니다. 하지만 이제 우리는 3D 컨볼루션을
사용하고 있습니다.

272
00:25:14,960 --> 00:25:27,840
첫 번째 레이어에서는 3에서 12로 매핑합니다. 하지만 이 경우 시간 차원도

273
00:25:27,840 --> 00:25:32,160
유지합니다. 그리고 우리는 약간의 시간 수용

274
00:25:32,160 --> 00:25:36,880
필드와 공간 수용 필드를 구축합니다. 그리고 우리는 풀링 레이어를
사용합니다. 그리고 풀링 레이어를 위해 4

275
00:25:36,880 --> 00:25:41,760
곱하기 4 곱하기 4를 합니다. 그리고 우리는 이 시간적 특징과

276
00:25:41,760 --> 00:25:46,240
공간적 특징을 조금 뽑아냅니다. 그리고 우리는 공간적 및 시간적 수용 필드를
더욱 구축합니다.

277
00:25:46,240 --> 00:25:51,600
그리고 또 다른 3D 컨볼루션 레이어가 있으며, 공간적

278
00:25:51,600 --> 00:25:55,920
및 시간적 수용 필드를 더욱 구축합니다. 마지막으로, 우리는 글로벌 평균
풀링을 사용합니다. 하지만 이제 우리는 이 4 곱하기

279
00:25:55,920 --> 00:26:01,520
16 곱하기 16 특징 맵을 풀링하고, 시간적 및

280
00:26:01,520 --> 00:26:06,000
공간적 수용 필드를 더욱 증가시킵니다. 그래서 우리는 공간과 시간 모두에서
점진적으로 구축하고 있습니다.

281
00:26:06,000 --> 00:26:12,000
그래서 초기 융합, 후기 융합, 그리고 3D 컨볼루션 신경망의 차이입니다.

282
00:26:12,000 --> 00:26:18,880
그래서 초기 융합과 3D 컨볼루션 신경망을 보면 알 수 있습니다. 두 가지
모두 시간에 따라

283
00:26:18,880 --> 00:26:24,400
수용 필드를 구축합니다. 하지만 실제 차이는 무엇일까요? 좀 더

284
00:26:25,200 --> 00:26:36,160
자세히 살펴봅시다. 각 공간 그리드 포인트에 대한 특징 벡터로 생각해보면,

285
00:26:36,160 --> 00:26:44,560
공통 필터가 2D 컨볼루션일 경우, 이

286
00:26:45,200 --> 00:26:54,400
그리드 포인트는 모든 시간 차원을 고려합니다. T는 16과 같습니다.

287
00:26:55,120 --> 00:27:04,000
그래서 공간에서는 국소적이지만 시간에서는 완전히 확장됩니다. 이것이 2D
컨볼루션

288
00:27:04,000 --> 00:27:10,400
신경망의 필터입니다. 하지만 문제는 무엇일까요? 생각해보세요. 그래서 우리가
이 2D 컨볼루션에서 시간 차원을 통해

289
00:27:10,400 --> 00:27:15,120
직접 진행한다면 어떤 문제가 발생할까요?

290
00:27:15,120 --> 00:27:22,480
그 단점은 시간 이동 불변성이 없다는 것입니다. 그것은 2D 필터가 이제
시간에서

291
00:27:22,480 --> 00:27:29,280
완전히 확장되기 때문입니다. 그래서 만약 우리가 다른 시간의 색상에서 글로벌

292
00:27:29,280 --> 00:27:32,240
전환을 배우고

293
00:27:33,680 --> 00:27:39,600
싶다면, 예를 들어, 비디오이기 때문에. 우리는 시간 정보를 인식하고
싶습니다.

294
00:27:39,600 --> 00:27:43,360
만약 다른 시간 단계에서 파란색에서 주황색으로의

295
00:27:43,360 --> 00:27:49,280
변화가 있다면, 아마도 시간 단계 4에서 어떤 변화가 발생하고 있습니다.
시간 단계 15에서도 동일한

296
00:27:49,280 --> 00:27:55,600
변화가 발생하고 있습니다. 하지만 파란색에서 주황색으로의 동일한 변화입니다.

297
00:27:56,640 --> 00:28:04,080
시간을 통해 완전히 진행하면 필터가 시간에서 완전히 확장되므로,

298
00:28:04,080 --> 00:28:09,840
다른 시간에서 글로벌 전환을 배우고 싶다면, 이를

299
00:28:09,840 --> 00:28:15,200
배우기 위해 전체 별도의 필터가 필요합니다. 그래서 우리는 다른 시간
단계에서 이러한

300
00:28:15,200 --> 00:28:21,920
다양한 전환을 배우기 위해 이 커널을 배워야 합니다. 그래서 시간 이동
불변성이 없습니다.

301
00:28:22,800 --> 00:28:30,080
그렇다면 공간과 시간 어디에서나 이러한 파란색에서 주황색으로의 전환을 어떻게
인식할까요? 이미지 분류를 할 때처럼, 우리는

302
00:28:30,080 --> 00:28:35,840
공간 불변성을 원합니다. 우리는 이미지가 고양이를 포함하고

303
00:28:35,840 --> 00:28:41,120
있음을 인식할 수 있기를 원합니다. 고양이가 오른쪽 모서리든 왼쪽 모서리든
관계없이,

304
00:28:41,120 --> 00:28:49,520
우리는 다른 공간 위치에서 사물을 인식할 수 있도록 커널을 공유하고자
합니다.

305
00:28:49,520 --> 00:28:53,040
여기서 우리는 서로 다른 시간 단계에서

306
00:28:53,040 --> 00:28:59,200
서로 다른 유형의 움직임과 시간 패턴을 학습할 수 있기를 원합니다. 그래서
비슷한 아이디어입니다. 그래서 그게 바로 3D 합성곱

307
00:28:59,200 --> 00:29:04,880
신경망의 장점입니다. 현재 이 T 차원에서 완전히 시간적으로 확장하는

308
00:29:04,880 --> 00:29:10,160
대신, 원래 이 초기 버전 T는 시간

309
00:29:10,160 --> 00:29:14,400
차원 T에서 16까지 확장됩니다.

310
00:29:14,400 --> 00:29:19,520
하지만 이제 T가 3이라면. 그리고 우리는 시간 차원을 따라 슬라이드할 수
있습니다. 필터와 지역을 사용하여 공간

311
00:29:19,520 --> 00:29:24,000
불변성을 학습한 것처럼. 이제 이 합성곱 필터는 시간에서 국소

312
00:29:24,000 --> 00:29:29,680
창만을 차지하고 시간 차원에서 슬라이드합니다.

313
00:29:29,680 --> 00:29:36,720
그래서 그 장점은 이제 우리는 시간 이동 불변성을 가질 수 있다는 것입니다.
각 필터가 시간을 따라

314
00:29:36,720 --> 00:29:42,480
슬라이드하기 때문입니다. 그래서 우리는 이 필터를 재사용하여 이러한 차원에서

315
00:29:42,480 --> 00:29:48,160
서로 다른 움직임 패턴을 인식할 수 있습니다. 그래서 파란색에서 주황색으로의
전환은 이제 매 순간 인식될 수 있습니다.

316
00:29:49,360 --> 00:29:55,600
그리고 이점은 별도의 필터가 필요하지 않다는 것입니다. 이제 우리는 더
효율적인

317
00:29:55,600 --> 00:30:01,120
표현을 하고 있습니다. 더 이상 별도의 필터를 학습할 필요가 없습니다.
그래서 이것이 2D 합성곱 초기 융합과

318
00:30:01,120 --> 00:30:08,560
3D 합성곱 신경망 간의 주요 차이점입니다.

319
00:30:08,560 --> 00:30:13,920
그리고 지난 강의에서, 여러분은 2D 합성곱

320
00:30:13,920 --> 00:30:20,480
신경망에서 우리가 학습한 내용을 시각화하는 데 사용할

321
00:30:20,480 --> 00:30:27,200
수 있는 몇 가지 도구의 예를 이미 보았다고 생각합니다. 유사하게, 우리는
이 3D 합성곱 네트워크의

322
00:30:27,200 --> 00:30:35,280
필터도 이러한 비디오 클립으로 시각화할 수 있습니다. 여러분은 그것을 볼 수
있습니다-- 제가 여러분이 볼 수 있을지 확신할 수는 없습니다.

323
00:30:35,280 --> 00:30:42,400
3D 합성곱 신경망에서 학습된 필터는 이제 필터가

324
00:30:42,400 --> 00:30:51,520
공간과 시간 모두에서 확장되기 때문에 비디오 클립으로 볼 수 있습니다.
그리고 이 필터들 중 일부는 이미지 분류기에서

325
00:30:52,480 --> 00:30:58,560
본 필터와 유사합니다.

326
00:30:58,560 --> 00:31:03,520
여러분은 이러한 색상 패턴과 다양한 엣지를 가질 수 있습니다. 하지만 다른
필터들도

327
00:31:03,520 --> 00:31:09,600
있다는 것을 볼 수 있습니다. 한 모서리에서 다른 모서리로의 시간적 전환이

328
00:31:09,600 --> 00:31:16,960
있거나 한 엣지 패턴에서 다른 패턴으로의 전환이 있습니다. 그리고 어떤 것은
움직임을 학습하지 않고, 어떤

329
00:31:16,960 --> 00:31:23,440
것은 색상 패턴에만 집중할 수 있습니다. 하지만 어떤 것은 서로 다른
방향으로의 움직임을 학습합니다.

330
00:31:23,440 --> 00:31:29,040
그래서 우리는 이러한 커널을 이렇게 시각화하여 해석할 수 있습니다.
기본적으로 두 가지 차이점이

331
00:31:30,080 --> 00:31:36,400
있습니다-- 하나는 느린 융합입니다. 합성곱 연산 측면에서, 네, 실제로
기본적으로

332
00:31:36,400 --> 00:31:39,520
3D 합성곱입니다. 3D 합성곱, 확실히.

333
00:31:39,520 --> 00:31:44,960
그리고 2D 합성곱은 완전히 다릅니다. 당신은 또 다른 차원의 합성이
있습니다. 당신은 이 시간적 차원을 가지고

334
00:31:44,960 --> 00:31:47,120
있습니다. 그래서 차이점은

335
00:31:47,120 --> 00:31:52,560
합성 작업에 이 시간적 차원이 있다는 것입니다. 하지만 실제로는 3D 합성

336
00:31:52,560 --> 00:31:57,280
신경망을 사용합니다. 이것은 공간과 시간에 걸쳐 점진적으로 수용 필드를
구축합니다.

337
00:32:01,040 --> 00:32:08,800
그래서 우리는 이 도구들, 3D 합성 네트워크나 아키텍처에 대해
이야기했습니다.

338
00:32:08,800 --> 00:32:13,360
하지만 ImageNet처럼 어떤 데이터를 사용할 수 있을까요? 비디오
분류기를 훈련시키기 위해

339
00:32:13,360 --> 00:32:18,960
어떤 데이터를 사용할 수 있을까요? 사람들이 도전해온 예시 데이터셋 중
하나는

340
00:32:19,600 --> 00:32:27,680
2014년에 소개된 Sports 1 million이라는 데이터셋입니다.

341
00:32:27,680 --> 00:32:33,120
이 데이터셋에 대해 우리가 할 수 있는 작업을 볼 수 있습니다. 우리는 매우
세밀한 스포츠 카테고리

342
00:32:33,120 --> 00:32:37,120
분류를 할 수 있습니다. 여기에서

343
00:32:37,120 --> 00:32:46,000
파란색은 바닥 신발을 나타냅니다. 노란색은 상위 다섯 개 예측을 나타냅니다.
녹색은 올바른 예측을

344
00:32:46,000 --> 00:32:49,200
나타냅니다. 빨간색은 잘못된 예측을 나타냅니다.

345
00:32:49,200 --> 00:32:52,560
데이터셋의 행동

346
00:32:52,560 --> 00:32:59,600
카테고리는 매우 세밀합니다. 487종의 다양한 스포츠가 있습니다. 마라톤과
울트라 마라톤이 있을

347
00:32:59,600 --> 00:33:02,960
수 있습니다. 사실, 그들 간의 차이는 모르지만, 이

348
00:33:02,960 --> 00:33:09,680
데이터셋의 다양한 스포츠 카테고리에 대해 매우 [INAUDIBLE]합니다.

349
00:33:09,680 --> 00:33:14,960
그리고 여기 우리가 이야기한 다양한 분류기를 이 Sports

350
00:33:14,960 --> 00:33:20,720
1 million 데이터셋에서 훈련했을 때의 결과가 있습니다. 여기서 아마도
가장 충격적인 결과는 제가 시도해

351
00:33:20,720 --> 00:33:27,520
보라고 요청한 단일 프레임 모델이 비디오 분류 모델을 개발하고자

352
00:33:27,520 --> 00:33:33,440
할 때 실제로 매우 좋은 성능을 보인다는 것입니다.

353
00:33:33,440 --> 00:33:38,880
단일 프레임 모델을 이미지 분류기로 간주하면

354
00:33:38,880 --> 00:33:45,760
77.7%의 분류 정확도와 상위 5개 정확도를 제공합니다. 우리가 이야기한
초기 융합은 약간

355
00:33:45,760 --> 00:33:51,040
더 나쁜 성능을 보입니다. 후기 융합은 약간 더 나은 성능을 보입니다. 이
데이터셋에서 3D 합성곱

356
00:33:51,040 --> 00:33:59,120
신경망을 사용하면 2%에서 3%의 향상을 얻습니다.

357
00:33:59,840 --> 00:34:04,800
여기서의 핵심 메시지는 단일

358
00:34:04,800 --> 00:34:12,800
프레임 모델을 반드시 시도해야 한다는 것입니다. [INAUDIBLE] 실제로
잘 작동합니다. 제가 여기서 보여준 3D 합성곱 신경망은

359
00:34:12,800 --> 00:34:18,720
2014년에 사용된 것입니다. 하지만 지난 10년 동안

360
00:34:18,720 --> 00:34:23,600
많은 발전이 있었습니다. 그래서 숫자도 훨씬 더 좋아지고 있으며, 이후

361
00:34:23,600 --> 00:34:26,640
슬라이드에서 이야기할 것입니다.

362
00:34:26,640 --> 00:34:30,400
훈련과 테스트 모두 비디오를 이미지로

363
00:34:30,400 --> 00:34:34,880
간주하고 이미지 분류기를 훈련하는 것입니다. 그것이 바로 단일 프레임이 하는
일입니다. 기본적으로, 제가 질문을 제대로 이해했다면,

364
00:34:34,880 --> 00:34:38,240
이미지 분류기를 사용하고 있습니다. 하지만 많은 비디오 프레임에서

365
00:34:38,240 --> 00:34:41,520
학습하고 있습니다. 각 비디오에서 단일 프레임이 아닙니다.

366
00:34:43,520 --> 00:34:47,600
그리고 이 데이터셋은 방대한 데이터셋이기 때문입니다. 제가 언급했듯이,

367
00:34:47,600 --> 00:34:55,600
비디오는 매우 큽니다. 사람들이 비디오 데이터셋을 공유할 때, 우리는

368
00:34:55,600 --> 00:35:02,640
ImageNet처럼 단순히 공유할 수 없습니다. 사람들은 비디오가 정말 크기
때문에 어떤 데이터베이스에서 다운로드할 수 있습니다. 이 데이터셋은 아마도
100만 개의

369
00:35:02,640 --> 00:35:08,800
비디오를 포함하고 있습니다. 모두 다운로드하여 공유하는 것은 매우
어렵습니다.

370
00:35:08,800 --> 00:35:14,160
사실, 이 비디오는 원래 출시될 때 URL 목록으로

371
00:35:14,160 --> 00:35:19,280
공유되었습니다, YouTube URL입니다. 하지만 이 YouTube 비디오
URL에서 기대할 수 있는 한

372
00:35:20,160 --> 00:35:24,880
가지는 사람들이 비디오를 수정하고 삭제한다는 것입니다. 그래서 그 원래
목록은 아마도 100만

373
00:35:24,880 --> 00:35:30,560
개의 비디오를 포함하고 있었을 것입니다. 하지만 지금은 아마도 절반의

374
00:35:30,560 --> 00:35:36,960
비디오가 이미 사라졌거나 없을 것입니다. 그래서 이 데이터셋은 이러한 이유로
그리 안정적이지 않습니다.

375
00:35:37,600 --> 00:35:49,920
그래서 제가 언급했듯이, 3D 컨볼루션 신경망은 2014년 5월

376
00:35:49,920 --> 00:35:56,000
이후로 점진적으로 개선되고 있습니다. 그래서 이 3D 컨볼루션 네트워크의
초기

377
00:35:56,000 --> 00:36:01,760
인기 버전 중 하나는 [?라는 모델입니다. C3N ?] 네트워크.
기본적으로, 사실 매우 간단합니다. 기본적으로, 2D 이미지 분류에 사용하는

378
00:36:03,440 --> 00:36:08,640
VGG 아키텍처와 매우 유사합니다.

379
00:36:09,200 --> 00:36:19,120
하지만 이제 우리는 3차원 합성곱 신경망으로 변환합니다. 예를 들어, 3D
CNN의 경우 3x3x3 합성곱과

380
00:36:19,120 --> 00:36:25,760
2x2x2 풀링을 사용합니다.

381
00:36:25,760 --> 00:36:31,280
첫 번째 레이어를 제외하고는 약간의 변화가 있습니다. 그래서 전체 아키텍처는
VGG

382
00:36:31,280 --> 00:36:36,640
아키텍처와 매우 유사합니다. 이제 우리는 이 추가 차원이 있습니다.

383
00:36:36,640 --> 00:36:43,600
그래서 이것이 3D CNN의 VGG라고 불리는 이유입니다. 모델은 제가 방금
언급한 Sports

384
00:36:44,160 --> 00:36:50,400
1 million 데이터셋에서

385
00:36:50,960 --> 00:36:58,080
훈련되었으며, 2014년에 소개되었습니다. 그 당시, 이런 모델을 훈련시키고
싶다고 상상해 보세요.

386
00:36:58,080 --> 00:37:07,120
많은 컴퓨팅 자원이 필요합니다. 그 당시에는 많은 사람들이 GPU에 접근할
수 없었습니다.

387
00:37:07,120 --> 00:37:13,040
사실 이 모델은 페이스북에서 훈련되었습니다. 그들은 이 모델과 사전 훈련된
가중치를 공개했으며,

388
00:37:13,040 --> 00:37:19,920
Sports 1 million에서 3D 모델을 훈련시킨 후 기능 추출기로서

389
00:37:19,920 --> 00:37:23,760
사전 훈련된 모델을 공개했습니다. 그래서 실제로 비디오 모델을 직접 훈련할
여유가 없는

390
00:37:23,760 --> 00:37:28,400
많은 사람들이 이 모델을 기능 추출기로 사용하기 시작했습니다.

391
00:37:28,400 --> 00:37:31,600
그래서 그들은 비디오를 가져와서 이 사전

392
00:37:33,600 --> 00:37:37,360
훈련된 모델을 사용하여 특징을 추출할 수 있습니다. 그래서 3D 모델을
만들고,

393
00:37:37,360 --> 00:37:44,880
아마 다른 선형 분류기를 훈련시킬 수도 있습니다. 그래서 사람들이 사용하기
시작합니다. 그게 인기를 끌게 된 이유입니다.

394
00:37:44,880 --> 00:37:48,720
그래서 질문은 기본적으로 우리가 이야기하고 있는 것, [INAUDIBLE]
분류에

395
00:37:48,720 --> 00:37:52,480
관한 것으로, 특징을 추출하기 위해 몇 개의 프레임을 입력으로 받아야
하는가입니다. 기본적으로 우리가 이야기하는

396
00:37:52,480 --> 00:37:56,720
모든 모델에 대해, 우리는 단일 모델을

397
00:37:56,720 --> 00:38:03,520
훈련시키기 위해 16프레임 또는 32프레임과 같은

398
00:38:03,520 --> 00:38:09,280
미리 정의된 길이의 클립을 전달한다고 가정합니다. 그리고 이 클립 수준
예측을 집계하는 방법에

399
00:38:09,280 --> 00:38:16,080
대해 이야기할 수 있는 다른 기술들이 있습니다. 하지만 지금은 클립 수준
특징 추출만 하고 있습니다.

400
00:38:16,080 --> 00:38:21,360
이 3D CNN의 단점은 매우 계산 비용이 많이 든다는 것입니다. 기본적으로
우리는 그냥

401
00:38:21,360 --> 00:38:27,760
직접적으로, 무차별적으로, 2D에서

402
00:38:27,760 --> 00:38:35,440
3D로 이 VGG 스타일을 만듭니다. 그리고 AlexNet에 대해 볼 수
있습니다. 이 GFLOP은 기본적으로

403
00:38:35,440 --> 00:38:40,800
기가플롭을 의미합니다. 단일 전방 패스를 위해 필요한 부동 소수점 연산의
수를 측정하려고 하며,

404
00:38:40,800 --> 00:38:45,680
기본적으로 네트워크가 효율적인지 아닌지를 측정하려고 합니다.

405
00:38:45,680 --> 00:38:54,320
그래서 AlexNet은 0.7 GFLOPS가 필요합니다. VGG-16은 약
13.6 GFLOPS가 필요합니다. 하지만 C3D의 경우, 실제로 2D에서

406
00:38:54,320 --> 00:39:01,040
3D로 매핑을 하고 있습니다. 그리고 이제 39.5 GFLOPS가
필요합니다. 그래서 VGG의 2.9배로,

407
00:39:01,040 --> 00:39:10,080
그렇게 효율적이지 않습니다. 그래서 이것이 이러한 종류의 네트워크의
단점입니다.

408
00:39:10,640 --> 00:39:16,640
그리고 Sports 1 million에서 성능을

409
00:39:17,280 --> 00:39:26,960
살펴보면, 지금 360에서 상위 5개 정확도 측면에서 약 4% 향상을
얻습니다. 그래서 이것은 우리가 할 수 있는 3D 컨볼루션

410
00:39:26,960 --> 00:39:31,520
네트워크의 한 예일 뿐입니다. 하지만 분명히 다른

411
00:39:31,520 --> 00:39:36,240
것들도 있을 수 있습니다. 우리는 2D 이미지 분류를 위해 할 수 있는 많은
트릭에 대해 이야기하고 있습니다.

412
00:39:36,240 --> 00:39:41,200
ResNet에서 본 것처럼 잔여 연결을 가질 수 있습니다. 하지만 분명히
3D를 개선하기 위해 잔여

413
00:39:41,200 --> 00:39:45,600
연결이나 2D 컨볼루션에서 이야기한

414
00:39:45,600 --> 00:39:50,320
다른 기술을 추가할 수도 있습니다. 실제로 이러한 다양한 유형의 비디오

415
00:39:50,320 --> 00:39:59,280
아키텍처를 개선하려는 많은 작업과 관련된 논문도 있습니다.

416
00:39:59,280 --> 00:40:05,280
하지만 그 외에도, 공간과 시간을 별도로

417
00:40:05,280 --> 00:40:10,800
다뤄야 할지에 대해 조금 더 생각해 봅시다. 왜냐하면 그것은 실제로 매우
다른 것들입니다--

418
00:40:10,800 --> 00:40:14,320
공간 정보와 시간 정보. 그래서 아마도 우리는 실제로 그곳에

419
00:40:14,320 --> 00:40:19,600
존재하는 것을 시간적으로 모델링하려고 시도해야 할 것입니다. 그것은
움직임입니다.

420
00:40:20,240 --> 00:40:26,720
그래서 인간은 실제로 움직임을 처리하는 놀라운 작업을 수행할 수 있습니다.
그래서 이 간단한 비디오에서

421
00:40:26,720 --> 00:40:30,000
인간이

422
00:40:30,800 --> 00:40:45,840
하고 있는 행동을 추측해 보세요. 원하시면 말해도 됩니다. 이게 뭐죠? 앉아
있습니다.

423
00:40:45,840 --> 00:40:51,600
네, 이 몇 가지 포인트만으로도 이

424
00:40:51,600 --> 00:41:03,280
사람이 하고 있는 행동이나 두 사람의 행동을 잘 인식할 수 있습니다. 현재는
어떤 외관 정보도

425
00:41:03,280 --> 00:41:08,800
없습니다. 단지 몇 가지 포인트입니다. 단지 움직임입니다.

426
00:41:08,800 --> 00:41:11,440
우리는 이 비디오에서

427
00:41:11,440 --> 00:41:17,600
일어나는 몇 가지 활동에 대해 매우 잘 이해할 수 있습니다. 그래서 외관과
움직임을 처리하는

428
00:41:17,600 --> 00:41:22,960
방식이 매우 다를 수 있습니다. 아마도 우리는 이를 처리하기 위해 별도의
네트워크를 가져야 할 것입니다. 실제로, 이것이 2014년에

429
00:41:22,960 --> 00:41:28,160
소개된 이 작업의 동기입니다.

430
00:41:30,240 --> 00:41:37,200
그들은 외관 정보와 움직임 정보를 별도로 처리하기

431
00:41:37,200 --> 00:41:43,120
위해 이중 스트림 네트워크를 제안하려고 합니다. 기본적으로 움직임을
명시적으로 측정하는 한 가지 방법은

432
00:41:43,120 --> 00:41:47,840
광학 흐름이라는 개념을 사용하는 것입니다. 광학 흐름의 경우, 기본
아이디어는 인접한

433
00:41:48,720 --> 00:41:55,920
프레임에서 픽셀의 움직임, 즉 움직임의 변화를 측정하고자 하는 것입니다.

434
00:41:55,920 --> 00:41:58,000
기본적으로 첫 번째 프레임에서

435
00:41:58,000 --> 00:42:02,320
모든 픽셀이 두 번째 프레임에서 어떻게 움직일지를 측정합니다. 그래서 프레임
내의 포인트에 대한

436
00:42:02,320 --> 00:42:06,960
속도를 계산하고 다음 프레임에서

437
00:42:06,960 --> 00:42:12,400
포인트가 있을 수 있는 위치를 추정합니다. 예를 들어, 이 경우 T 프레임과
p+1에 대해.

438
00:42:12,400 --> 00:42:20,000
기본적으로 이 흐름 필드는 2차원이며 각 픽셀이

439
00:42:20,000 --> 00:42:28,400
다음 프레임에서 어디로 이동할지를 알려줍니다. 그래서 F(x, y)는
(dx, dy)와 같습니다. 그리고 I t+1(x+dx)는 다음 프레임의
픽셀 E의

440
00:42:28,400 --> 00:42:35,840
위치이며 현재 프레임의 I t(x, y)와 같습니다.

441
00:42:35,840 --> 00:42:42,080
그래서 픽셀의 움직임을 명시적으로 측정하는 방법입니다. 실제로 두 프레임
쌍을 주어 광학

442
00:42:42,080 --> 00:42:46,560
흐름을 계산하는 방법에

443
00:42:46,560 --> 00:42:53,600
대한 연구 논문이 많이 있습니다. 다양한 유형의 가정을 할 수 있는 방법이
있습니다. 일부 연구는 광학 흐름이 물체가 움직일

444
00:42:53,600 --> 00:42:57,200
때 일정하게

445
00:42:57,200 --> 00:43:00,800
유지된다고 가정합니다. 그리고 이 광학 흐름을 계산하기 위한 몇 가지 기술을
제안합니다.

446
00:43:00,800 --> 00:43:03,680
하지만 일단 얻으면, 기본적으로

447
00:43:03,680 --> 00:43:09,440
두 개의 인접한 프레임에 대한 움직임 정보를 캡처합니다. 또한, 이는 픽셀이
수평 및 수직으로 어떻게

448
00:43:09,440 --> 00:43:14,560
움직이는지를 캡처하려고 하기 때문에 2차원입니다. 그래서 이를 별도로

449
00:43:14,560 --> 00:43:19,200
시각화할 수도 있습니다. 수평 움직임, 수평 흐름

450
00:43:19,200 --> 00:43:23,600
dx를 시각화할 수 있습니다. 그리고 수직 흐름 dy도 시각화할 수
있습니다. 수평 움직임과 수직 움직임을

451
00:43:23,600 --> 00:43:28,640
캡처하는 것을 볼 수 있습니다. 그래서 우리는 이러한 저수준의 움직임 단서를
캡처합니다.

452
00:43:28,640 --> 00:43:32,720
이러한 움직임 단서를 광학 흐름으로 캡처하는

453
00:43:32,720 --> 00:43:38,880
방법이 있으면, 사람들은 움직임 분류기와 외관 분류기를 훈련하기

454
00:43:38,880 --> 00:43:43,760
위해 이중 스트림 네트워크를 제안하려고 합니다. 그래서 이것은 행동 인식을
위한 유명한 두 개의 스트림 네트워크입니다. 기본적으로, 외관 분류를
수행하여 어떤

455
00:43:43,760 --> 00:43:48,880
행동인지 알려주는 단일 프레임 모델이 있습니다.

456
00:43:48,880 --> 00:43:55,920
그리고 별도의 스트림이 있습니다. 그것은 이 다중 프레임 광학 흐름을
사용하는

457
00:43:55,920 --> 00:44:01,440
시간적 스트림 ConvNet입니다. 인접한 두 프레임마다 광학 흐름 맵을
계산합니다. 또한 수평 운동 광학 흐름과

458
00:44:01,440 --> 00:44:06,720
수직 흐름을 별도로 처리합니다. 그리고 그것들을

459
00:44:06,720 --> 00:44:11,440
함께 쌓습니다. 그런 다음 시간적 스트림 합성곱 신경망을 사용하여
처리합니다.

460
00:44:11,440 --> 00:44:14,800
그리고 예측을 합니다. 그런 다음 운동 스트림과 외관

461
00:44:14,800 --> 00:44:18,640
스트림 모두에 대한 예측 결과를

462
00:44:18,640 --> 00:44:22,400
집계하여 최종 예측을 얻습니다. 그래서 이것이 이 두 개의 스트림 네트워크의
아이디어입니다.

463
00:44:22,400 --> 00:44:27,840
실제로 꽤 잘 작동합니다. UCF-101이라는 또 다른 데이터셋에서
사용됩니다. 이 데이터셋에는 101개의

464
00:44:29,440 --> 00:44:33,840
행동 카테고리가 있습니다. 그래서 당신은 볼 수 있습니다 - 놀라운 점은

465
00:44:33,840 --> 00:44:38,800
오직 운동만 사용해도 실제로 매우 잘 작동한다는 것입니다.

466
00:44:40,160 --> 00:44:45,280
3D 합성곱 네트워크와 공간 전용의 성능을 볼 수 있습니다. 그리고 그것은
오직 외관

467
00:44:45,280 --> 00:44:48,560
스트링입니다. 그리고 시간 전용, 그것이 운동 스트림입니다. 운동 스트림이
공간 전용 스트림,

468
00:44:48,560 --> 00:44:55,840
외관 스트림에 비해 실제로 훨씬 더 잘 작동한다는 것을 볼 수 있습니다.

469
00:44:55,840 --> 00:45:03,840
그래서 제 가설은 과적합이 덜 쉽다는 것입니다. 왜냐하면 운동의 경우, 배경
정보가 많아서

470
00:45:04,800 --> 00:45:09,440
행동 분류에 중요하지 않을

471
00:45:09,440 --> 00:45:13,040
수 있기 때문입니다. 하지만 운동 스트림은 실제로

472
00:45:13,040 --> 00:45:17,760
움직임이라는 핵심 정보를

473
00:45:17,760 --> 00:45:22,240
포함하고 있어 과적합이 덜 쉽습니다. 실제로 이 데이터셋에서 더 나은 결과를
얻을 수 있습니다.

474
00:45:22,240 --> 00:45:32,080
지금까지 우리는 비디오의 단기 구조에 대해 이야기하고 있었습니다. 그리고
아까, 사람들이 분류를 위해 실제로 몇

475
00:45:32,080 --> 00:45:39,280
개의 프레임을 사용해야 하는지

476
00:45:39,280 --> 00:45:42,960
질문한 것 같습니다. 그래서 확실히, 더 먼 시간에서

477
00:45:42,960 --> 00:45:50,080
인식하기 위해 장기 시간 구조를 모델링하는 것이 매우 중요합니다.

478
00:45:50,640 --> 00:45:57,200
그래서 우리는 실제로 이미 알고 있습니다. 저는 순환 네트워크를 사용하여
단어

479
00:45:58,240 --> 00:46:07,280
시퀀스를 처리하고 캡션 작업 및 예측 작업을

480
00:46:07,280 --> 00:46:13,920
수행할 도구를 이미 가지고 있습니다. 그래서 우리는 유사한 도구, 즉 순환
신경망을 사용할 수 있습니다.

481
00:46:13,920 --> 00:46:20,320
우리는 합성곱 신경망을 가지고 있습니다. 단일 프레임 합성곱 신경망을
사용하여 2D 특징

482
00:46:20,320 --> 00:46:26,160
벡터를 얻든, 클립에서 특징 벡터를 얻기

483
00:46:26,160 --> 00:46:32,880
위해 3D 합성곱 네트워크를 사용하든 상관없이. 하지만 당신은 훨씬 더 긴
비디오를 가지고 있습니다. 우리는 특징 벡터를 얻을 수 있고, 그 다음에
우리가 이야기한 RN이나

484
00:46:34,960 --> 00:46:40,320
LSTM을 사용하여 장기적인 시간 구조를 모델링합니다.

485
00:46:40,320 --> 00:46:48,880
우리는 이 순환 신경망을 사용하여 지역 특징을 처리하고, 비디오의

486
00:46:48,880 --> 00:46:53,520
마지막 시간 단계에서 단일 비디오

487
00:46:53,520 --> 00:46:59,440
수준 분류를 원할 경우 최종 예측을 합니다. 우리가 이야기한 것처럼 일대일
매핑도

488
00:47:01,680 --> 00:47:03,760
가능합니다.

489
00:47:04,400 --> 00:47:09,680
각 프레임에 대해 예측을 할 수 있습니다. 각 비디오 프레임에 대해

490
00:47:09,680 --> 00:47:16,960
예측하고 싶은 것이 있을 수 있습니다. 우리는 LSTM이나 순환 신경망에서
이 출력을 얻을 수도 있습니다.

491
00:47:16,960 --> 00:47:27,280
사실, 이러한 아이디어는 2011년에 이미 탐구되었습니다. 실제로, 이는
시대를

492
00:47:27,280 --> 00:47:35,280
앞서간 것입니다. 알렉스넷은 2012년에 소개되었습니다. 하지만 2015년
논문에 의해 더 대중화되었습니다.

493
00:47:35,280 --> 00:47:41,760
장기적인 시간 구조를 모델링하기 위해 이러한 순환 아키텍처를 훈련하고

494
00:47:41,760 --> 00:47:48,080
싶다면, 종종 이 [INAUDIBLE]를 통해서만 역전파할 수 있습니다.
또는 CNN을 융합할

495
00:47:48,080 --> 00:47:51,840
수 있습니다. 이미지 분류에서 일부 클립에

496
00:47:51,840 --> 00:47:57,040
대해 사전 훈련할 수 있습니다. 그렇지 않으면, 순환 부분과 이 합성곱

497
00:47:57,040 --> 00:48:02,480
부분이 있는 거대한 네트워크가 있습니다. 끝에서 끝으로 훈련하기가 매우
어렵습니다. 그래서 [INAUDIBLE] 3D를 특징 추출기로

498
00:48:02,480 --> 00:48:07,440
사용하고 이 순환 신경망을 훈련할 수 있습니다.

499
00:48:07,440 --> 00:48:14,240
우리는 이미 시간 구조를 모델링하기 위한 두 가지 접근 방식을 보았습니다.
이 두 가지 접근 방식을 결합할 수 있을까요?

500
00:48:14,240 --> 00:48:19,760
이 합성곱 신경망과 이 순환 신경망.

501
00:48:19,760 --> 00:48:24,880
두 가지 모두 장점이 있습니다. 그래서 비디오 데이터를 처리하기 위해 단일
아키텍처로

502
00:48:25,680 --> 00:48:28,480
결합할 수 있습니다.

503
00:48:28,480 --> 00:48:32,640
실제로, 우리가 이야기한 다층

504
00:48:32,640 --> 00:48:38,000
순환 신경망에서 영감을 받을 수 있습니다. 각 시간 단계는 동일한 층의 이전
숨겨진 시간 단계와

505
00:48:38,000 --> 00:48:42,000
이전 층의 동일한 시간 단계에서

506
00:48:42,000 --> 00:48:44,880
출력을 가져올 수 있습니다. 이것이 기본적으로 다층 RNN의 아이디어입니다.

507
00:48:44,880 --> 00:48:50,640
하지만 비디오에 대해서도 비슷하게 할 수 있습니다. 이제 우리는 순환

508
00:48:50,640 --> 00:48:59,360
합성곱 신경망을 소개합니다. 매우 유사합니다. 이제 우리는 각 특징이 3차원
벡터인

509
00:48:59,360 --> 00:49:05,200
특징 그리드를 구축합니다. 두 개의 공간 차원과 하나의 채널 차원입니다.

510
00:49:07,040 --> 00:49:12,560
각 특징 벡터는 사실상 네 번째 차원 C 곱하기 H 곱하기 W입니다.

511
00:49:12,560 --> 00:49:16,080
각 벡터는 두 개의 입력에 따라 다릅니다. 각 특징 맵은 동일한 층의 이전

512
00:49:16,080 --> 00:49:20,480
시간 단계에서의 특징 맵에 따라 다릅니다. 그러나 동일한 시간 단계에서 이전

513
00:49:20,480 --> 00:49:24,560
층의 특징 맵에도 의존합니다.

514
00:49:26,800 --> 00:49:33,680
그래서 우리는 2D 컨볼루션 네트워크에서 입력 피처를

515
00:49:33,680 --> 00:49:39,360
출력 피처로 매핑하는 이 피처 맵을 기록합니다. 하지만 여기서 이 순환
컨볼루션 네트워크에서는

516
00:49:39,360 --> 00:49:48,240
이 두 개의 3D 텐서를 입력으로 사용할 수 있습니다. 하나는 같은 레이어와
이전 타임스텝에서, 다른 하나는 같은 타임스텝의 이전

517
00:49:48,240 --> 00:49:50,960
레이어에서 가져옵니다.

518
00:49:50,960 --> 00:50:00,800
순환 네트워크를 기억해보면, ht-1이라는 숨겨진

519
00:50:00,800 --> 00:50:06,880
레이어 피처 맵이 있습니다. 현재 타임스텝의 입력을 받아서,

520
00:50:06,880 --> 00:50:13,920
어떤 파라미터 W를 가진 함수가 있습니다. 그리고 새로운 상태 피처 벡터
ht를 처리합니다. 이것이 RNN의 기본 키입니다.

521
00:50:13,920 --> 00:50:19,040
그래서 이제 대신, 우리는 RNN의 이 벡터 형태를 변경합니다. 우리는 순환
신경망의 모든 행렬

522
00:50:19,040 --> 00:50:25,440
곱셈을 2D 컨볼루션으로 대체합니다. 이제 우리는 이 순환 컨볼루션

523
00:50:25,440 --> 00:50:28,800
신경망을 얻습니다. 그래서 우리는 피처 맵을 가지고 있습니다.

524
00:50:28,800 --> 00:50:32,880
행렬 곱셈 대신 2D 컨볼루션을 수행합니다. 그리고 또 다른

525
00:50:32,880 --> 00:50:37,600
피처 맵을 얻습니다. 이전 레이어의 같은 타임스텝의

526
00:50:37,600 --> 00:50:43,440
피처에 대해서도 동일하게 수행합니다. 그리고 이 2D 컨볼루션을 수행한 후,
이들을 더하고 또

527
00:50:43,440 --> 00:50:50,720
다른 tanh 레이어를 사용하여 현재 숨겨진 레이어의 피처 맵을 얻습니다.

528
00:50:50,720 --> 00:50:54,640
그래서 이것이 기본적으로 순환 컨볼루션 신경망의 아이디어입니다. 기본적으로
우리는 컨볼루션

529
00:50:54,640 --> 00:50:57,520
연산과 순환

530
00:50:57,520 --> 00:51:02,320
연산을 결합할 수 있습니다. 그리고 우리는 GRU와 LSTM과 같은 모든
종류의 순환 신경망 변형에도

531
00:51:02,320 --> 00:51:08,080
실제로 이것을 수행할 수 있습니다. 아마도 이전 수업에서 이미 배웠을
것입니다.

532
00:51:08,080 --> 00:51:14,160
그래서 이제 우리는 두 가지의 이점을 성공적으로 결합할 수 있습니다. 우리는
이 순환 컨볼루션 신경망

533
00:51:14,160 --> 00:51:22,240
안에 공간적 및 시간적 융합을 가지고 있습니다. 하지만 이 모델은 많이
사용되지 않았습니다. 왜냐하면 순환 네트워크의

534
00:51:22,240 --> 00:51:29,680
큰 단점이 하나 있기 때문입니다. 아마도 여러분은 이미 배웠을

535
00:51:29,680 --> 00:51:35,440
것입니다. RNN은 비순차적 처리에 매우 느립니다. 그리고 비디오는 보통
매우

536
00:51:35,440 --> 00:51:42,720
길고, 병렬로 처리해야 합니다. 하지만 RNN은 병렬화하기가 매우
어렵습니다.

537
00:51:42,720 --> 00:51:48,560
하지만 우리가 이전 강의에서 배운 또

538
00:51:48,560 --> 00:51:56,400
다른 중요한 모델이 있습니다. 우리가 할 수 있는 것은. 우리는 비디오를
처리하기 위해 자기 주의와 같은 연산을 사용할 수 있습니다. 자기 주의에서는
쿼리,

539
00:51:56,400 --> 00:52:03,920
키, 값이 있습니다. 그리고 이미지를 처리하기 위해 자기 주의 레이어를
독립적인

540
00:52:03,920 --> 00:52:08,320
연산으로 사용할 수 있습니다. 여기서 우리는 비디오에도 이것을 수행할 수
있습니다.

541
00:52:08,320 --> 00:52:15,360
자기 주의의 매우 큰 장점 중 하나는 높은 병렬화 가능성입니다. 모든 정렬
및 이 주의 점수는 모든 입력에

542
00:52:15,360 --> 00:52:21,840
대해 완전히 병렬로 수행될 수 있습니다. 실제로 사람들은 비디오에서도 자기

543
00:52:21,840 --> 00:52:29,040
주의를 사용하려고 하고 있습니다. 그래서 그들은 자기 주의를 3D로 직접
적용합니다.

544
00:52:29,040 --> 00:52:31,440
아마도 3D 합성곱 신경망이 있을 것입니다. C 곱하기 T 곱하기

545
00:52:31,440 --> 00:52:36,720
H 곱하기 W와 같은 특징 맵을 얻습니다. 그리고 비슷하게, 쿼리 특징

546
00:52:36,720 --> 00:52:41,760
맵을 얻을 수 있습니다. 채널 차원을 변경하기 위해 1 곱하기 1 곱하기 1
3D 합성곱을 사용하여 쿼리

547
00:52:41,760 --> 00:52:46,560
특징 맵인 C 프라임 곱하기 T 곱하기 H 곱하기 W로 매핑할 수 있습니다.

548
00:52:46,560 --> 00:52:50,960
키에 대해서도 마찬가지로, 값에 대한 이 특징 맵을 얻습니다. 그리고 나서
주의

549
00:52:50,960 --> 00:52:56,160
가중치를 얻고자 합니다. 기본적으로 쿼리의 이

550
00:52:56,160 --> 00:53:07,280
특징 맵을 전치(transpose)하는 것입니다. 벡터화된 곱셈을 통해 각
쿼리와 키 특징 쌍에

551
00:53:07,280 --> 00:53:13,360
대한 주의 점수를 얻습니다. 그리고 나서 이 주의 맵을 얻고, 이를 사용하여
값을 조건화할 수 있습니다.

552
00:53:13,360 --> 00:53:21,200
그리고 또 다른 값, 즉 특징 맵을 얻을 수 있습니다. 그리고 그것들을
매핑할 수 있습니다. 다시 원래의 특징 입력과 연결할 수 있도록 같은 차원

553
00:53:21,200 --> 00:53:25,520
C로 매핑하기 위해 또 다른 1

554
00:53:25,520 --> 00:53:31,280
곱하기 1 곱하기 1 합성곱을 사용합니다. 그래서 이것이 잔차 연결입니다.
결국, 자가 주의 작업과 매우

555
00:53:31,280 --> 00:53:36,160
유사하다는 것을 알 수 있습니다.

556
00:53:36,160 --> 00:53:41,280
하지만 이제 우리는 3D로 이동합니다. 그리고 이것은 매우 독립적인 하나의
블록입니다.

557
00:53:41,280 --> 00:53:46,320
스스로 독립적으로 존재할 수 있습니다. 그래서 이 논문에서 보듯이, 비국소
신경망(nonlocal

558
00:53:46,320 --> 00:53:51,120
neural network)이라고 불립니다. 블록을 도입하고 이를 비국소
블록(nonlocal block)이라고 부릅니다. 비디오 처리를 위한 빌딩

559
00:53:51,680 --> 00:53:55,680
블록으로 사용할 수 있습니다.

560
00:53:55,680 --> 00:53:59,840
예를 들어, 기존의 3D 합성곱

561
00:53:59,840 --> 00:54:06,560
신경망 아키텍처에 이 비국소 블록을 추가할 수 있습니다. 비국소 블록이 있는
3D CNN을 갖고, 또 다른 비국소

562
00:54:06,560 --> 00:54:12,480
블록이 있는 3D CNN 블록을 추가합니다. 각 비국소 블록은 공간과 시간
모두에서 융합하는

563
00:54:12,480 --> 00:54:18,640
데 매우 강력하며, 결국 이 분류로 이어집니다.

564
00:54:18,640 --> 00:54:23,040
우리가 아직 이야기하지 않은 한 가지는 이 3D 합성곱 신경망이
무엇인지입니다.

565
00:54:23,040 --> 00:54:30,640
그래서 여기서 무엇을 사용해야 할까요? 과거에 사람들이 탐구한 매우 흥미로운
아이디어는, 우리가 이야기한

566
00:54:30,640 --> 00:54:35,520
많은 성공적인 아키텍처인 2D 합성곱 신경망을

567
00:54:35,520 --> 00:54:41,280
3D로 직접 재사용할 수 있을까 하는 것입니다.

568
00:54:41,840 --> 00:54:48,560
우리는 이 2D 네트워크를 약간 팽창(inflate)합니다. 그래서 우리는
3D 합성곱 신경망을 얻을 수 있습니다.

569
00:54:48,560 --> 00:54:55,120
이 작업은 I3D 아키텍처라고 불립니다. 아이디어는 그들이 2D

570
00:54:55,120 --> 00:55:05,760
CNN 아키텍처를 가져온 것입니다. 그들은 차원이 Kh 곱하기 Kw인 각
2D 합성곱 풀링 레이어를 대체합니다.

571
00:55:05,760 --> 00:55:15,520
하지만 이제 우리는 Kt 곱하기 Kh 곱하기 Kw인 3D 버전으로
대체합니다. 기본적으로 팽창된 것입니다. 그들은 이를 인셉션

572
00:55:15,520 --> 00:55:28,320
블록 위에서 사용합니다. 그리고 나서 이 팽창을 한 후, 기존 아키텍처를
재사용하여 비디오를

573
00:55:28,320 --> 00:55:35,200
처리하는 아키텍처를 갖게 됩니다.

574
00:55:35,200 --> 00:55:42,240
이제 2D에서 잘 작동하는 아키텍처를 3D에서도

575
00:55:42,240 --> 00:55:47,040
작동하도록 전이할 수 있습니다. 하지만 한 걸음 더 나아가, 사람들은

576
00:55:49,600 --> 00:55:54,320
아키텍처를 전이할 수 있을 뿐만 아니라. 실제로 가중치도

577
00:55:54,320 --> 00:56:00,800
전이할 수 있습니다. 이미 이미지 데이터셋에서 많은 아키텍처 모델을 사전
훈련했기 때문입니다.

578
00:56:00,800 --> 00:56:03,840
아마도 우리는 그곳에서 배운 가중치를 재사용할 수 있을 것입니다. 좋은 사전
정보가

579
00:56:03,840 --> 00:56:08,240
있을 수 있습니다. 할 수 있는 한 가지는 이미지를 기반으로 훈련된 가중치로

580
00:56:09,360 --> 00:56:12,240
팽창된 CNN을 초기화하는 것입니다.

581
00:56:12,240 --> 00:56:22,000
예를 들어, 원래 2D 컨볼루션 커널이 있을 수 있습니다. 그 커널을 Kt
배

582
00:56:22,000 --> 00:56:29,680
만큼 복사합니다. 그리고 Kt로 나눕니다. 원래는 단일 이미지를 입력으로
받습니다.

583
00:56:29,680 --> 00:56:34,880
이제 Kt로 나눈

584
00:56:34,880 --> 00:56:42,912
비디오를 입력으로 받습니다. 이 팽창된 버전을 사용하고 Kt 배 만큼
가중치를 복사합니다. 단일 프레임이나 일정한 프레임의

585
00:56:42,912 --> 00:56:51,440
비디오를 입력하면 동일한 출력을 얻습니다.

586
00:56:51,440 --> 00:56:59,520
이제 우리는 2D 이미지 이해에서 이 아키텍처와 가중치를 기반으로 기존

587
00:56:59,520 --> 00:57:05,760
2D 이미지를 재활용할 수 있는 방법을 갖게 되었습니다. 실제로 잘
작동합니다. 성능을 보면, 그들을

588
00:57:05,760 --> 00:57:10,400
팽창시킵니다. 이 이중 스트림 컨볼루션 네트워크와 비교했을 때,

589
00:57:10,400 --> 00:57:14,080
실제로 더 나은 성능을 보입니다. 실제로 외관 프레임뿐만 아니라.

590
00:57:14,080 --> 00:57:20,880
모션 스트림도 팽창할 수 있습니다. 그래서 실제로 추가적인 개선을
가져옵니다.

591
00:57:20,880 --> 00:57:25,840
기본적으로, 이것은 3D 컨볼루션

592
00:57:25,840 --> 00:57:33,600
네트워크와 독립적으로 재사용할 수 있는 기술입니다. 이 비국소 블록을 구축할
수 있습니다. 내가 말하고자 하는 것은 우리가 많은 2D 컨볼루션

593
00:57:33,600 --> 00:57:40,800
신경망과 그 가중치를 가지고 있다는 것입니다.

594
00:57:40,800 --> 00:57:45,840
성공적인 사람들은 그들이 매우 성공적이라는 것을 보여주었습니다. 우리가
그것들을 재사용하고 싶다면,

595
00:57:45,840 --> 00:57:52,080
사람들은 실제로 가중치를 복사하고

596
00:57:53,120 --> 00:57:58,480
재사용할 수 있다고 보여주었습니다. 기본적으로, 그것이 고수준의
아이디어입니다.

597
00:57:58,480 --> 00:58:05,040
이 초기화를 한 후에도 비디오 데이터에서 미세 조정할 수 있습니다. 하지만
이미지에서 사전 훈련된

598
00:58:05,040 --> 00:58:08,400
가중치가 있습니다. 그래서 우리는 비디오 모델 훈련을

599
00:58:08,400 --> 00:58:12,160
위한 좋은 초기화를 제공할 수 있습니다. 이 I3D 네트워크의 아이디어는

600
00:58:13,120 --> 00:58:18,960
기본적으로 가중치를 복사하고 팽창을 수행하는 것입니다.

601
00:58:20,560 --> 00:58:26,400
이것은 비디오 이해 네트워크 모델의 한 예일 뿐입니다. 비디오 이해를 위한
다른 많은

602
00:58:26,400 --> 00:58:33,440
비디오 전이 모델도 제안되었습니다. 예를 들어, 이 작업에서는 시공간
주의(attention)가 공간과

603
00:58:34,560 --> 00:58:37,840
시간 모두에 주의를 기울이기

604
00:58:37,840 --> 00:58:42,640
위해 더 분리된 주의를 시도하고 있습니다.

605
00:58:42,640 --> 00:58:46,720
또한, 이 변환기 아키텍처

606
00:58:46,720 --> 00:58:54,000
측면에서 더 효율적이기 위한 다른 방법들도 있습니다. 또는 이 마스크
오토인코더는 비디오 이해를 위한 더 효율적이고 확장 가능한 비디오

607
00:58:54,000 --> 00:58:59,200
레벨 사전 학습을 수행하는 것으로 들어보셨을 것입니다. 그래서 저는 이
수업에서 그것들에 대해

608
00:58:59,200 --> 00:59:02,720
이야기하지 않을 것입니다. 하지만 관심이 있으시면 그들의 논문을 확인할 수
있습니다. 더 나은 미디어 이해 모델을

609
00:59:02,720 --> 00:59:07,760
갖기 위해 많은 진전이 이루어졌기 때문입니다.

610
00:59:07,760 --> 00:59:10,800
그러한 방식으로 진행 상황의 성능을 살펴보면, 우리는

611
00:59:10,800 --> 00:59:16,720
단일 프레임 모델에서 62.2로 시작했다고 생각합니다. 이것은 또 다른
데이터셋인 Kinetics-400입니다.

612
00:59:16,720 --> 00:59:22,240
이는 대규모 비디오 데이터셋입니다. 그리고 이제 이 비디오 모델

613
00:59:22,240 --> 00:59:30,880
인코더는 이미 90% 정확도에 도달했습니다. 그래서 새로운 변환기 모델이
제안되었습니다.

614
00:59:30,880 --> 00:59:39,440
우리는 비디오 분류에서 매우 잘하고 있습니다. 지난 수업의 이미지 분류와
유사하게, 비디오 모델을 시각화하기

615
00:59:39,440 --> 00:59:45,200
위해 유사한 기법을 사용할 수 있습니다. 이 두 스트림 네트워크를

616
00:59:45,200 --> 00:59:51,440
예로 들 수 있습니다. 우리는 외관 이미지와 흐름 이미지를 무작위로
초기화하고,

617
00:59:51,440 --> 00:59:57,680
순방향 패스를 수행한 다음 점수를 계산합니다.

618
00:59:57,680 --> 01:00:01,840
그런 다음 특정 클래스의 점수에 대해

619
01:00:01,840 --> 01:00:08,880
역전파하고 기울기 상승을 사용하여 분류 점수를 극대화할 수 있습니다. 이미지
기반 모델의 시각화를

620
01:00:08,880 --> 01:00:15,760
위해 우리가 하는 것과 같습니다. 그렇게 해서 우리가 배운 것에 대한
시각화와

621
01:00:15,760 --> 01:00:20,240
해석을 수행할 수 있습니다.

622
01:00:20,240 --> 01:00:26,320
왼쪽은 외관 스트링에 대한 최적화된 이미지입니다. 비주얼 스트림에서 무슨
일이

623
01:00:26,320 --> 01:00:34,240
일어나고 있는지 추측하기 어려울 수 있습니다. 오른쪽은 흐름 스트림에 대한
최적화된 이미지입니다. 하나는 시간 스트림이 너무 빠르게 변하지

624
01:00:34,240 --> 01:00:41,280
않도록 하는 시간 제약이 있습니다. 그래서 느린 움직임을

625
01:00:41,280 --> 01:00:46,800
포착할 수 있습니다. 다른 하나는 빠른 움직임을 포착합니다. 그래서 행동이
무엇인지 추측할 수 있습니다.

626
01:00:46,800 --> 01:00:55,040
그리고 아마도 이 경우에는 꽤 명확합니다. 이것은 어떤 행동인가요. 이것은
역도입니다. 중간 이미지는 바를 흔드는

627
01:00:55,040 --> 01:01:01,920
동작을 하고 있습니다. 오른쪽 이미지는 머리 위로

628
01:01:02,560 --> 01:01:08,640
밀어내는 동작을 하고 있습니다. 그래서 실제로 이 비디오 모델이 실제로 어떤
모델이

629
01:01:08,640 --> 01:01:13,120
이 움직임에 대해 학습하고 있는지를

630
01:01:16,480 --> 01:01:23,360
볼 수 있습니다. 지금까지 저는 수영과 달리기와 같은

631
01:01:23,360 --> 01:01:30,560
짧은 클립을 어떻게 분류할 수 있는지에 대해 이야기했습니다. 하지만 또 다른
매우 중요한 것은 우리가 할 수 있는 다른 작업으로, 이것은

632
01:01:31,600 --> 01:01:37,200
시간적 행동 위치 지정이라고 불리며, 단순히 클립 수준이나

633
01:01:37,200 --> 01:01:42,240
분류를 하는 것만으로는 충분하지 않다는 것입니다. 때때로 우리는 단순히 물체
탐지를 하고 싶습니다.

634
01:01:42,240 --> 01:01:46,800
이제 우리는 비디오에서 행동이 발생하는 위치를 로컬라이즈하고자 합니다.
어쩌면 때때로 그

635
01:01:46,800 --> 01:01:51,440
사람이 달리고 있습니다. 때때로, 점프하고 있습니다. 그래서 또 다른 작업이
있습니다. 이것은 시간적 행동 위치

636
01:01:51,440 --> 01:01:57,840
지정이라는 또 다른 클래스입니다. 더 빠른 R-CNN의 유사한

637
01:01:57,840 --> 01:02:03,600
아이디어를 사용할 수 있습니다. 일부 시간적 제안을 생성한 다음 분류를
수행할 수 있습니다.

638
01:02:03,600 --> 01:02:08,400
또한 두 가지 모두를 수행할 수 있습니다. 이것은 공간-시간 탐지입니다.
기본적으로 공간뿐만 아니라 시간에서도

639
01:02:09,680 --> 01:02:14,000
로컬라이즈하고자 합니다. 행동이 공간에서 발생하는 위치.

640
01:02:14,000 --> 01:02:20,640
행동이 시간적으로 발생하는 위치. 그래서 이것은 공간-시간 탐지라는 또 다른
작업입니다.

641
01:02:21,920 --> 01:02:27,520
지금까지 저는 시간적 스트림과 3D CNN, 두

642
01:02:27,520 --> 01:02:33,440
개의 스트림 신경망, 공간-시간 자기 주의 메커니즘을

643
01:02:33,440 --> 01:02:39,520
사용할 수 있는 아키텍처에 대해 이야기했습니다. 그리고 우리는 이를 수행하기
위한 몇 가지 도구에 대해 이미 이야기했습니다.

644
01:02:39,520 --> 01:02:46,080
하지만 네, 마지막 10분 동안 다시 살펴보겠습니다. 제 시간 안에 마치기를

645
01:02:46,080 --> 01:02:50,160
바랍니다. 오늘 시작한 예제를 다시

646
01:02:50,160 --> 01:02:56,698
살펴보겠습니다. 제가 비디오를 보여드렸던 곳입니다. 하지만 여전히 전체
그림은 아닐 수 있습니다.

647
01:02:56,698 --> 01:02:56,714
[비디오

648
01:02:56,714 --> 01:03:10,533
재생] [아기 웃음] [개 짖는 소리] [재생 종료] 우리는 비디오를

649
01:03:10,533 --> 01:03:10,546
보

650
01:03:10,546 --> 01:03:10,560
고

651
01:03:10,560 --> 01:03:13,120
있습니다. 비디오 이해에는 지금까지 다루지

652
01:03:13,120 --> 01:03:18,240
않은 또 다른 매우 중요한 차원이 있다고 생각합니다. 그것은 바로 소리가

653
01:03:18,240 --> 01:03:22,000
있다는 것입니다. 오디오가 있습니다. 비디오에는 다른 양식이 있습니다. 그
요소를 놓치면

654
01:03:22,000 --> 01:03:26,400
많은 재미를 잃게 됩니다. 감정을 인식할 수 있습니다.

655
01:03:26,400 --> 01:03:31,360
이 시각적 요소와 움직임을 결합하면 다른 상호작용을 할 수 있습니다.

656
01:03:31,360 --> 01:03:34,800
따라서 이 오디오를 염두에 두고 이 비전 스트림이 있다면,

657
01:03:34,800 --> 01:03:38,400
사람들은 또한 많은 다른 흥미로운 작업을 제안했습니다. 또한 우리는 비디오
이해를

658
01:03:38,400 --> 01:03:43,360
위한 다른 작업을 탐색했습니다. 여기 여러 객체와 여러 화자가 있는

659
01:03:44,800 --> 01:03:48,800
비디오의 또 다른 예가 있습니다.

660
01:03:48,800 --> 01:03:53,680
그리고 실제로 제가 과거에 시각적으로 유도된 오디오 소스 분리에

661
01:03:53,680 --> 01:03:57,280
대해 탐구한 작업, 예시 작업이 있습니다. 그리고 실제로 시각적이고
청각적으로

662
01:03:57,280 --> 01:04:01,600
사물을 처리하려고 이해할 수 있습니다. 시각 정보를 사용하여 소스

663
01:04:01,600 --> 01:04:06,080
분리를 안내할 수 있습니다. 소리 구성 요소를 분리하고 싶습니다. 원래는
혼합이 있을 수

664
01:04:06,080 --> 01:04:08,560
있습니다. 시각 정보를 사용하여 일부

665
01:04:08,560 --> 01:04:12,480
소리 구성 요소로 분리하고 싶습니다. 이것을 시각적으로 유도된 오디오 소스
분리라고 합니다.

666
01:04:12,480 --> 01:04:17,440
이 작업에 대한 예를 드리겠습니다. 예를 들어, 여기 음성 혼합이 있습니다.
때때로 각 사람의 소리를 개별적으로

667
01:04:17,440 --> 01:04:21,600
듣고 싶을 수 있습니다. 그리고 우리는 그들의 시각 정보와

668
01:04:21,600 --> 01:04:25,200
오디오 정보를 함께 처리하여

669
01:04:25,200 --> 01:04:29,280
소리를 분리할 수 있습니다. 여기서 우리가 할 수 있는 일이 있습니다. 왼쪽
화자의 목소리를 분리할 수 있습니다.

670
01:04:29,280 --> 01:04:33,920
그래서 우리는 사람과 화음에 대해서만 이것을 할 수 있습니다. 오디오와
화음, 시각

671
01:04:33,920 --> 01:04:38,000
스트림을 처리해야 할 때. 하지만 우리는 음악 악기와 같은 다른 유형의

672
01:04:38,000 --> 01:04:41,680
소리에 대해서도 이것을 할 수 있습니다. 여기 또 다른 예가 있습니다.
우리는 모션과 객체 중심

673
01:04:41,680 --> 01:04:46,160
정보를 오디오 스트림과 분석하여

674
01:04:46,160 --> 01:04:51,920
음악 악기 분리를 할 수 있습니다. 그래서 이것은 이 작업에 대한 또 다른
예입니다.

675
01:04:51,920 --> 01:04:56,880
그리고 또한, 이 새로운 오디오 모달리티를

676
01:04:56,880 --> 01:05:01,280
도입하면 비디오 이해 분류를 하고 싶습니다. 오디오는 유용한 단서가 될 수
있습니다. 실제로, 변환기 주의 기반 모델에서 제안된

677
01:05:01,280 --> 01:05:04,720
오디오-비주얼 비디오 이해 작업이 있습니다.

678
01:05:04,720 --> 01:05:10,480
우리는 이미지와 비디오를 패치에 매핑하는 것뿐만 아니라.

679
01:05:10,480 --> 01:05:15,040
오디오 스펙트럼을 패치에 매핑하고 일부

680
01:05:15,040 --> 01:05:21,040
변환기 아키텍처를 사용하여 분류를 수행할 수 있습니다. 또는 마스크
오토인코더 스타일을 사용할 수도 있습니다. 우리는 이미지와 스펙트로그램의
패치를 예측하고

681
01:05:21,040 --> 01:05:26,240
비디오 이해를 수행하고자 합니다.

682
01:05:28,000 --> 01:05:33,440
또한 사람들이 탐구하고 있는 또 다른

683
01:05:33,440 --> 01:05:39,920
측면은 효율적인 비디오 이해를 수행하는 방법입니다. 그래서 저는 몇 가지
예를 빠르게 제시하겠습니다. 이 수업 전반에 걸쳐 저는 주로 클립 수준

684
01:05:39,920 --> 01:05:45,360
분류에 집중하고 있다고 생각합니다. 클립을 주고 이 분류를

685
01:05:45,360 --> 01:05:50,400
수행하는 방법을 제시합니다. 많은 클립을 분류한 후 정보를 집계하여

686
01:05:50,400 --> 01:05:54,560
비디오 수준 예측을 얻고자 합니다. 그것이 긴 비디오에서의 행동 인식입니다.

687
01:05:54,560 --> 01:05:58,480
효율적인 비디오 이해를 위해, 왜

688
01:05:58,480 --> 01:06:02,400
효율적인 비디오 이해를 하고 싶을까요? 비디오는 매우 깁니다. 모든 클립을
하나씩

689
01:06:02,400 --> 01:06:09,360
처리할 여유가 없습니다. 그래서 우리는 단일 클립의 효율성을 높이려고 하고
있으며, X3D와 같은 것을

690
01:06:09,360 --> 01:06:14,640
구축하여 더 나은 3D 컨볼루션 신경망을 만들고 있습니다.

691
01:06:14,640 --> 01:06:22,960
또한, SD 샘플러와 같은 것을 사용하여 어떤 클립이

692
01:06:23,600 --> 01:06:29,360
가장 유용한지 예측하려고 하고 있습니다. 그런 다음 예측을 결합할 수
있습니다. 중요한 클립에서만 클립

693
01:06:29,360 --> 01:06:35,600
분류기를 실행하세요. 또한, 정책 학습을 통해 이 행동 분류를 수행하기 위해
어떤 모달리티를 사용해야 하는지

694
01:06:35,600 --> 01:06:39,440
예측하려고 합니다. 비디오를 사용할지, 몇 개의 비디오

695
01:06:39,440 --> 01:06:45,520
클립을 사용할지, 오디오나 다른 감각 데이터를 사용할지 여부를 선택할 수
있습니다.

696
01:06:46,160 --> 01:06:51,360
여기 오디오를 사용하여 중요한 순간을

697
01:06:52,880 --> 01:07:00,480
예측하는 미리보기 메커니즘의 한 예가 있습니다. 그런 다음 이를 가이드
단서로 사용하여 클립을

698
01:07:00,480 --> 01:07:05,840
처리하고 결과를 평균화합니다. 그래서 이것이 효율적인 비디오

699
01:07:05,840 --> 01:07:10,400
이해에 관한 것입니다. 그래서 이것도 연구의 한 분야입니다.

700
01:07:10,400 --> 01:07:16,640
또한 요즘에는 VR과 AR, 스마트 안경으로 이동하는 사람들이 있습니다.
그리고 이제 미래에는 많은 자아 중심 비디오

701
01:07:16,640 --> 01:07:20,560
스트림이 있을 것이라고 추측합니다. 이것은 비디오 이해의

702
01:07:20,560 --> 01:07:24,640
또 다른 측면입니다. 자아 중심 비디오뿐만 아니라 다중

703
01:07:24,640 --> 01:07:29,680
마이크로폰, 마이크 배열, 다채널 오디오도 있습니다.

704
01:07:30,880 --> 01:07:33,120
자아 중심 다중 모달 자아 중심

705
01:07:33,120 --> 01:07:38,000
비디오 스트림에서 비디오 이해를 더 잘 수행하는 방법도 뜨거운 주제입니다.
아마도 우리는 비디오 스트림, 오디오, 다채널 오디오

706
01:07:39,360 --> 01:07:45,360
및 시각 정보를 사용하여 누가 누구에게 말하고 있고 누가 누구를

707
01:07:45,360 --> 01:07:50,720
듣고 있는지를 예측할 수 있다는 것을 탐구했을 것입니다. 미래에 당신이
이러한 스마트

708
01:07:50,720 --> 01:07:55,360
안경을 쓴다고 상상해 보세요. 이 다양한 유형의 사회적 상호작용을

709
01:07:55,360 --> 01:08:00,640
이해하는 데 도움이 되기를 원합니다. 그래서 이것이 자아 중심 비디오
이해입니다.

710
01:08:00,640 --> 01:08:04,880
마지막 슬라이드입니다. 현재 LLM에 대한

711
01:08:06,240 --> 01:08:09,520
많은 진행 중인 작업이 있으며,

712
01:08:09,520 --> 01:08:14,320
비디오 수준의 기초 모델을 구축하려고 하고 있습니다. 비디오 이해를 LLM에
어떻게 수집할까요? 실제로 비디오를 토큰화하고 LLM 임베딩

713
01:08:14,320 --> 01:08:23,280
공간에 매핑하려고 하는 작업이 있으며, 비디오에서 사람이 어디에 있는지,
무엇을

714
01:08:23,280 --> 01:08:27,600
하고 있는지를 프롬프트할 수 있습니다.

715
01:08:27,600 --> 01:08:35,680
그런 다음 비디오를 설명하는 텍스트를 출력합니다. 비디오 이해와 LLM을
수집하려는

716
01:08:35,680 --> 01:08:41,520
많은 작업이 있습니다. 그래서 이것도 현재 뜨거운 주제입니다.
