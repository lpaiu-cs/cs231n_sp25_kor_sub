1
00:00:05,400 --> 00:00:09,260
CS231N 강의 13에 다시 오신 것을 환영합니다.

2
00:00:09,260 --> 00:00:11,480
오늘은 생성 모델에 대해 이야기할 것입니다.

3
00:00:11,480 --> 00:00:14,132
지난 시간에는 자가 지도 학습에 대해

4
00:00:14,132 --> 00:00:15,840
이야기했는데, 이는 감독

5
00:00:15,840 --> 00:00:18,360
없이, 레이블 없이 데이터에서 직접

6
00:00:18,360 --> 00:00:22,120
구조를 배우고자 하는 매우 흥미로운 패러다임입니다.

7
00:00:22,120 --> 00:00:25,277
우리가 지난 시간에 여러 예를 통해 이야기한 자가 지도

8
00:00:25,277 --> 00:00:27,360
학습의 전형적인 형식은 레이블이 없는

9
00:00:27,360 --> 00:00:29,540
대규모 데이터 세트를 갖는 것입니다.

10
00:00:29,540 --> 00:00:30,582
이상적으로는 단순히 이미지입니다.

11
00:00:30,582 --> 00:00:31,290
정말 훌륭합니다.

12
00:00:31,290 --> 00:00:32,479
많은 이미지를 얻을 수 있습니다.

13
00:00:32,479 --> 00:00:34,729
이 이미지를 통해 특징 표현을

14
00:00:34,729 --> 00:00:37,300
추출할 인코더를 통과시키고, 그

15
00:00:37,300 --> 00:00:40,080
특징 표현에서 무언가를 예측할 디코더를

16
00:00:40,080 --> 00:00:41,642
통과시킬 것입니다.

17
00:00:41,642 --> 00:00:43,600
자가 지도 학습의 전체

18
00:00:43,600 --> 00:00:45,800
요점은 인간 주석이나 레이블

19
00:00:45,800 --> 00:00:47,360
없이 이 전체 시스템을

20
00:00:47,360 --> 00:00:51,200
훈련할 수 있는 사전 작업을 생각해내는 것입니다.

21
00:00:51,200 --> 00:00:52,960
그래서 우리는 자가 지도

22
00:00:52,960 --> 00:00:55,160
학습 목표를 공식화하기 위해

23
00:00:55,160 --> 00:00:59,560
사용할 수 있는 다양한 작업의 회전과 같은 것들에 대해

24
00:00:59,560 --> 00:01:00,560
이야기했습니다.

25
00:01:00,560 --> 00:01:03,440
그리고 일반적으로, 이는 두 단계 절차로,

26
00:01:03,440 --> 00:01:05,200
먼저 자가 지도 작업에서

27
00:01:05,200 --> 00:01:07,320
찾을 수 있는 모든 데이터에 대해

28
00:01:07,320 --> 00:01:10,340
자가 지도 인코더-디코더를 학습하게 됩니다.

29
00:01:10,340 --> 00:01:11,715
그 후에는 디코더를

30
00:01:11,715 --> 00:01:15,160
버리고, 새로운, 아마도 작은 완전 연결 네트워크를

31
00:01:15,160 --> 00:01:17,400
삽입하여 이 네트워크를 끝에서

32
00:01:17,400 --> 00:01:18,860
끝으로 훈련하거나, 아마도

33
00:01:18,860 --> 00:01:21,520
작은 레이블 작업에서 완전 연결

34
00:01:21,520 --> 00:01:23,120
네트워크만 학습하게 됩니다.

35
00:01:23,120 --> 00:01:25,920
여기서의 아이디어는 자가 지도 학습을 통해 이

36
00:01:25,920 --> 00:01:28,940
사전 작업을 통해, 고품질 인간 레이블에 접근할

37
00:01:28,940 --> 00:01:31,200
수 없는 수백만, 수억, 수십억

38
00:01:31,200 --> 00:01:33,980
개의 샘플에 대해 훈련할 수 있다는 것입니다.

39
00:01:33,980 --> 00:01:35,940
자가 지도 학습 과정에서 이미지를

40
00:01:35,940 --> 00:01:38,315
포함한 데이터의 일반 구조에 대해

41
00:01:38,315 --> 00:01:39,600
무언가를 배우게 됩니다.

42
00:01:39,600 --> 00:01:42,400
그런 다음 이 지식을 소량의 인간 레이블이 있는

43
00:01:42,400 --> 00:01:44,310
하위 작업으로 전이할 수 있습니다.

44
00:01:44,310 --> 00:01:46,560
따라서 자가 지도 학습에서 우리가

45
00:01:46,560 --> 00:01:48,893
작업하고자 하는 전형적인 설정은

46
00:01:48,893 --> 00:01:51,762
인터넷에서 얻은 10억 개의 레이블 없는

47
00:01:51,762 --> 00:01:53,720
이미지를 훈련하고, 그런 다음

48
00:01:53,720 --> 00:01:56,440
우리가 정말로 중요하게 생각하는 특정 작업에

49
00:01:56,440 --> 00:01:59,860
대해 수십, 수백, 수천 개의 예제를 레이블링할

50
00:01:59,860 --> 00:02:02,040
의향이 있는 작업으로 그 특징을

51
00:02:02,040 --> 00:02:03,440
전이하는 것입니다.

52
00:02:03,440 --> 00:02:05,960
하지만 우리는 이 작업들이 자가

53
00:02:05,960 --> 00:02:08,280
지도 사전 작업을 통해 배운 일반

54
00:02:08,280 --> 00:02:10,853
지식에 의해 개선되기를 원합니다.

55
00:02:10,853 --> 00:02:13,520
지난 시간에는 회전, 재배치, 재구성

56
00:02:13,520 --> 00:02:17,080
등 다양한 종류의 사전 작업에 대해 이야기했습니다.

57
00:02:17,080 --> 00:02:18,640
이 모든 것은 기본적으로

58
00:02:18,640 --> 00:02:21,860
입력 픽셀에 대해 기하학적 섭동,

59
00:02:21,860 --> 00:02:23,995
기하학적 방해를 가하고,

60
00:02:23,995 --> 00:02:26,120
모델에게 그 섭동에서 회복하도록

61
00:02:26,120 --> 00:02:27,950
요청하는 것입니다.

62
00:02:27,950 --> 00:02:30,200
회전의 경우, 이미지를 회전시키고 모델에게

63
00:02:30,200 --> 00:02:32,720
얼마나 회전했는지 예측하도록 요청할 수 있습니다.

64
00:02:32,720 --> 00:02:35,525
재배치 또는 퍼즐을 푸는 경우, 이미지를

65
00:02:35,525 --> 00:02:37,400
패치로 잘라서 모델에게 원래

66
00:02:37,400 --> 00:02:38,960
이미지에서 그 패치의

67
00:02:38,960 --> 00:02:40,880
상대적 배열이 무엇인지 예측하도록

68
00:02:40,880 --> 00:02:42,343
요청할 것입니다.

69
00:02:42,343 --> 00:02:43,760
또는 재구성의

70
00:02:43,760 --> 00:02:45,677
경우, 입력 이미지의 일부를

71
00:02:45,677 --> 00:02:48,080
삭제하고 모델에게 이를

72
00:02:48,080 --> 00:02:50,360
채우도록 요청할 수 있습니다.

73
00:02:50,360 --> 00:02:52,000
이들은 꽤 성공적입니다.

74
00:02:52,000 --> 00:02:54,680
지난 시간에는 매우 성공적인 자가 지도

75
00:02:54,680 --> 00:02:57,180
학습의 다른 형식인 대조 학습에

76
00:02:57,180 --> 00:02:58,800
대해서도 이야기했습니다.

77
00:02:58,800 --> 00:03:01,312
여기서 여러분이 몇 가지 후속 방법을

78
00:03:01,312 --> 00:03:03,020
다루기 위해 시간이 부족했다고

79
00:03:03,020 --> 00:03:04,687
들었으므로, 오늘

80
00:03:04,687 --> 00:03:07,960
강의의 시작 부분에서 이를 빠르게 다루고 싶었습니다.

81
00:03:07,960 --> 00:03:10,760
대조 학습의 아이디어는 유사한

82
00:03:10,760 --> 00:03:12,937
쌍과 비유사한 쌍을 얻고,

83
00:03:12,937 --> 00:03:14,520
유사한 쌍을 함께

84
00:03:14,520 --> 00:03:16,600
모으고 비유사한 쌍을

85
00:03:16,600 --> 00:03:18,300
멀리하는 것입니다.

86
00:03:18,300 --> 00:03:19,920
자가 지도 학습의 맥락에서

87
00:03:19,920 --> 00:03:22,200
이를 수행하는 일반적인 방법은

88
00:03:22,200 --> 00:03:24,380
입력 이미지를 시작하는 것입니다.

89
00:03:24,380 --> 00:03:27,020
다시 말해, 이들은 레이블이 없는 이미지입니다.

90
00:03:27,020 --> 00:03:28,420
이들에 대한 레이블이 없습니다.

91
00:03:28,420 --> 00:03:29,920
이제 각 입력 이미지에

92
00:03:29,920 --> 00:03:32,580
대해 두 개의 무작위 변환을 적용할 것입니다.

93
00:03:32,580 --> 00:03:34,720
고양이의 경우, 고양이 얼굴 주위에서

94
00:03:34,720 --> 00:03:36,800
하나의 크롭을 하고, 고양이 뒷면

95
00:03:36,800 --> 00:03:38,440
주위에서 또 다른 크롭을 했습니다.

96
00:03:38,440 --> 00:03:41,520
원숭이의 경우, 원숭이 얼굴 주위에서 하나를

97
00:03:41,520 --> 00:03:44,600
하고, 흑백으로 바꾸는 등의 작업을 했습니다.

98
00:03:44,600 --> 00:03:46,700
기본적으로 각 입력

99
00:03:46,700 --> 00:03:49,860
이미지에 대해 두 개 또는 그 이상의 무작위

100
00:03:49,860 --> 00:03:53,480
변형을 적용할 것입니다. 두 개는

101
00:03:53,480 --> 00:03:55,360
최소한의 하위 집합입니다.

102
00:03:55,360 --> 00:03:58,230
이제 이러한 무작위로 변형된 입력 데이터 버전을

103
00:03:58,230 --> 00:04:02,110
기능 추출기에 공급할 것입니다. 기능 추출기는 VIT일

104
00:04:02,110 --> 00:04:05,070
수도 있고 CNN일 수도 있으며, 이미지를

105
00:04:05,070 --> 00:04:08,230
입력받아 특징 표현을 출력할 수 있는 신경망입니다.

106
00:04:08,230 --> 00:04:11,190
그런 다음 이 대조 개념을 적용하고자 합니다.

107
00:04:11,190 --> 00:04:14,270
고양이에서 나온 두 개의 증강에 대해,

108
00:04:14,270 --> 00:04:17,149
이 두 개의 특징 벡터가 같기를

109
00:04:17,149 --> 00:04:19,329
원하므로 녹색으로 표시합니다.

110
00:04:19,329 --> 00:04:22,630
기본적으로 n 제곱 유사성 행렬을

111
00:04:22,630 --> 00:04:23,230
계산하는데,

112
00:04:23,230 --> 00:04:28,910
만약-- 글쎄요, 2n(2n) 제곱이므로 4n 제곱입니다.

113
00:04:28,910 --> 00:04:31,710
n개의 이미지가 있고 각 이미지에

114
00:04:31,710 --> 00:04:38,150
두 개의 변형을 적용하므로, 변형된 증강 샘플에 대해 거대한 2n

115
00:04:38,150 --> 00:04:40,590
x 2n 행렬이 생성됩니다.

116
00:04:40,590 --> 00:04:44,930
이제 기본적으로 원본 이미지에서 나온 두 개의

117
00:04:44,930 --> 00:04:47,870
증강을 함께 모으고자 합니다.

118
00:04:47,870 --> 00:04:51,310
서로 다른 원본 이미지에서 나온 모든 증강

119
00:04:51,310 --> 00:04:53,130
쌍은 서로 멀어지도록

120
00:04:53,130 --> 00:04:54,430
하고자 합니다.

121
00:04:54,430 --> 00:04:57,990
따라서 이러한 모든 것을 기능 추출기를 통해

122
00:04:57,990 --> 00:05:00,470
실행하고, 이 거대한 4n 제곱 행렬을

123
00:05:00,470 --> 00:05:03,725
계산하여 특징 벡터 간의 스칼라 유사성을

124
00:05:03,725 --> 00:05:06,350
구한 다음, 유사한 것들은 함께

125
00:05:06,350 --> 00:05:08,910
모으고, 달라야 할 것들은 멀어지게

126
00:05:08,910 --> 00:05:12,150
하는 것이 대조 학습의 기본 아이디어입니다.

127
00:05:12,150 --> 00:05:15,190
몇 년 전 이 모든 것을 통합한 논문 중 하나는

128
00:05:15,190 --> 00:05:17,410
SimCLR이라는 제목으로,

129
00:05:17,410 --> 00:05:19,030
이미지의 자기 지도 표현

130
00:05:19,030 --> 00:05:20,590
학습에 매우 성공적으로

131
00:05:20,590 --> 00:05:22,430
적용되었습니다. 이 논문이

132
00:05:22,430 --> 00:05:24,430
지난번에 다룬 것이라고 생각합니다.

133
00:05:24,430 --> 00:05:26,590
하지만 SimCLR 설정의 한

134
00:05:26,590 --> 00:05:28,390
가지 문제는 좋은 수렴을

135
00:05:28,390 --> 00:05:33,327
얻기 위해 상당히 큰 배치 크기가 필요하다는 것입니다. 네트워크에겐 너무

136
00:05:33,327 --> 00:05:34,910
쉬운 문제이기 때문입니다.

137
00:05:34,910 --> 00:05:36,327
샘플이 많지 않으면 비슷해

138
00:05:36,327 --> 00:05:38,070
보이는 두 개의 고양이 이미지를

139
00:05:38,070 --> 00:05:39,330
선택하는 것이 너무 쉽습니다.

140
00:05:39,330 --> 00:05:41,030
네트워크에 충분한 학습 신호를

141
00:05:41,030 --> 00:05:42,610
주기 위해 문제를 충분히 어렵게

142
00:05:42,610 --> 00:05:44,110
만들기 위해서는 이 모델이

143
00:05:44,110 --> 00:05:47,052
좋은 특징으로 수렴하도록 상당히 큰 배치 크기가 필요합니다.

144
00:05:47,052 --> 00:05:48,510
그런 다음 그렇게 하면 몇

145
00:05:48,510 --> 00:05:50,750
강의 전에 이야기한 대규모 분산 훈련에 대한

146
00:05:50,750 --> 00:05:53,047
모든 아이디어를 다시 열어야 하며, 이는

147
00:05:53,047 --> 00:05:54,130
완전히 실행 가능합니다.

148
00:05:54,130 --> 00:05:55,130
완전히 작동합니다.

149
00:05:55,130 --> 00:05:56,630
하지만 그 없이 해결할

150
00:05:56,630 --> 00:05:59,550
수 있는 방법이 있는지 물어볼 수 있습니다.

151
00:05:59,550 --> 00:06:01,310
그것은 제가 너무 자세히 설명하고 싶지

152
00:06:01,310 --> 00:06:03,198
않은 몇 가지 접근 방식으로 이어집니다.

153
00:06:03,198 --> 00:06:04,990
사실 저는 이들을 자세히 설명하고 어떻게

154
00:06:04,990 --> 00:06:06,550
작동하는지 말씀드리고 싶지 않습니다.

155
00:06:06,550 --> 00:06:08,550
저는 단지 이들의 존재를 인식하고

156
00:06:08,550 --> 00:06:10,270
그들이 달성하려는 목표의 일반적인

157
00:06:10,270 --> 00:06:11,870
느낌을 전달하고 싶습니다.

158
00:06:11,870 --> 00:06:14,190
따라서 이 MoCo 또는 모멘텀 대비 접근

159
00:06:14,190 --> 00:06:15,970
방식의 자기 지도 학습에서 설정은

160
00:06:15,970 --> 00:06:18,650
우리가 방금 SimCLR에서 본 것과 매우 유사합니다.

161
00:06:18,650 --> 00:06:19,530
데이터를 가져옵니다.

162
00:06:19,530 --> 00:06:20,970
증강된 쌍을 얻습니다.

163
00:06:20,970 --> 00:06:22,595
그들을 특징 인코더를 통해 실행합니다.

164
00:06:22,595 --> 00:06:24,220
유사한 것들은 함께 모으고,

165
00:06:24,220 --> 00:06:26,670
비유사한 것들은 멀리 밀어내고 싶습니다.

166
00:06:26,670 --> 00:06:28,630
하지만 다른 점은 매 반복마다

167
00:06:28,630 --> 00:06:31,190
거대한 배치 크기를 유지할 필요가

168
00:06:31,190 --> 00:06:32,650
없도록 하려는 것입니다.

169
00:06:32,650 --> 00:06:38,070
그래서 그렇게 하기 위해, 그들은 이전 훈련

170
00:06:38,070 --> 00:06:40,950
반복에서 샘플 q를 유지합니다.

171
00:06:40,950 --> 00:06:42,690
그리고 매 훈련 반복마다,

172
00:06:42,690 --> 00:06:46,050
제 x 쿼리는 현재의 새로운 데이터 배치입니다.

173
00:06:46,050 --> 00:06:49,810
그리고 저는 이전 훈련 반복에서 본 이전 데이터

174
00:06:49,810 --> 00:06:51,230
배치인 q,

175
00:06:51,230 --> 00:06:54,390
x0, x1, x2 키를 가지고 있습니다.

176
00:06:54,390 --> 00:06:56,045
현재 데이터 배치는 항상

177
00:06:56,045 --> 00:06:57,670
하던 대로 인코더

178
00:06:57,670 --> 00:07:01,790
네트워크를 통해 실행하고, SimCLR과 동일한 방식으로

179
00:07:01,790 --> 00:07:03,750
대조 손실을 계산할 것입니다.

180
00:07:03,750 --> 00:07:07,690
그리고 이 더 큰 q, 이전 배치의 역사들은 다른

181
00:07:07,690 --> 00:07:09,190
것을 통해 실행할 것이며,

182
00:07:09,190 --> 00:07:11,815
모멘텀 인코더를 통해 여전히

183
00:07:11,815 --> 00:07:13,190
특징 표현을 얻고

184
00:07:13,190 --> 00:07:17,710
SimCLR 방식으로 계산한 것과 동일한 유사성을 계산할

185
00:07:17,710 --> 00:07:18,310
것입니다.

186
00:07:18,310 --> 00:07:19,852
하지만 문제는 모멘텀 인코더로

187
00:07:19,852 --> 00:07:21,790
역전파하고 싶지 않다는 것입니다. 왜냐하면

188
00:07:21,790 --> 00:07:23,247
데이터가 너무 많기 때문입니다.

189
00:07:23,247 --> 00:07:24,330
배치가 너무 큽니다.

190
00:07:24,330 --> 00:07:26,470
우리는 그것을 GPU 메모리에 맞출 여유가 없습니다.

191
00:07:26,470 --> 00:07:30,090
그래서 우리는 그 부분을 통해 역전파를 하지 않기를 원합니다.

192
00:07:30,090 --> 00:07:33,830
즉, 우리는 이 모멘텀 인코더, 두 번째 인코더를

193
00:07:33,830 --> 00:07:36,590
경량 하강법으로 업데이트할 수 없습니다.

194
00:07:36,590 --> 00:07:38,610
대신 우리는 이상한 일을 할 것입니다.

195
00:07:38,610 --> 00:07:40,790
우리가 할 것은 이 모멘텀 인코더가 자체

196
00:07:40,790 --> 00:07:42,050
가중치를 갖도록 하는 것입니다.

197
00:07:42,050 --> 00:07:44,332
우리는 그것을 경량 하강법으로 배우지 않을 것입니다.

198
00:07:44,332 --> 00:07:46,790
대신 우리가 할 것은 모멘텀 인코더가 일반

199
00:07:46,790 --> 00:07:49,070
인코더의 가중치에 대한 지수 이동 평균이

200
00:07:49,070 --> 00:07:50,335
되도록 하는 것입니다.

201
00:07:50,335 --> 00:07:51,710
그래서 일반 인코더는 경량

202
00:07:51,710 --> 00:07:52,960
하강법으로 배울 것입니다.

203
00:07:52,960 --> 00:07:53,990
모든 것이 정상입니다.

204
00:07:53,990 --> 00:07:55,370
우리는 순전파 또는 역전파를 할 것입니다.

205
00:07:55,370 --> 00:07:56,130
우리는 그래디언트를 얻을 것입니다.

206
00:07:56,130 --> 00:07:57,505
우리는 일반 인코더에 대해

207
00:07:57,505 --> 00:08:00,670
그래디언트 업데이트 단계를 만들 것입니다. 그것이 일반적인 것입니다.

208
00:08:00,670 --> 00:08:06,130
하지만 그 후에, 모멘텀 인코더는 현재 모멘텀 인코더

209
00:08:06,130 --> 00:08:08,470
가중치를 0.99로

210
00:08:08,470 --> 00:08:11,110
감소시키고 인코더 가중치의 1%를

211
00:08:11,110 --> 00:08:13,100
추가할 것입니다.

212
00:08:13,100 --> 00:08:15,350
그래서 모멘텀 인코더는 인코더

213
00:08:15,350 --> 00:08:18,110
가중치의 지연된, 후행하는 지수 이동 평균을

214
00:08:18,110 --> 00:08:20,830
가지는 다른 업데이트 규칙을 갖습니다.

215
00:08:20,830 --> 00:08:23,470
그리고 이것이 정확히 왜 의미가 있는지에 대한 좋은

216
00:08:23,470 --> 00:08:24,680
직관이나 설명이 없습니다.

217
00:08:24,680 --> 00:08:26,430
하지만 이것이 효과가 있다는 강력한

218
00:08:26,430 --> 00:08:27,510
경험적 증거가 있습니다.

219
00:08:27,510 --> 00:08:30,410
그래서 이것이 현재 상황입니다.

220
00:08:30,410 --> 00:08:32,750
하지만 이것은 좋습니다. 왜냐하면 이제

221
00:08:32,750 --> 00:08:35,070
매 반복마다 이 거대한 부정 샘플

222
00:08:35,070 --> 00:08:37,270
배치를 가질 필요 없이 이러한 자기

223
00:08:37,270 --> 00:08:39,517
감독 표현을 배울 수 있기 때문입니다.

224
00:08:39,517 --> 00:08:40,809
그리고 이것은 꽤 성공적이었습니다.

225
00:08:40,809 --> 00:08:42,392
이 방향으로 나아간 후속

226
00:08:42,392 --> 00:08:43,830
논문들이 많이 있었습니다.

227
00:08:43,830 --> 00:08:46,310
당신이 알아야 할 또 다른 것은 DINO라는 것입니다.

228
00:08:46,310 --> 00:08:47,930
다시 말해, 아이디어는 매우 유사합니다.

229
00:08:47,930 --> 00:08:50,330
이것은 경량 하강법으로 학습된 이중

230
00:08:50,330 --> 00:08:52,470
일반 인코더와 모멘텀 인코더를

231
00:08:52,470 --> 00:08:54,110
사용하는 유사한 모멘텀

232
00:08:54,110 --> 00:08:55,665
인코더를 사용합니다.

233
00:08:55,665 --> 00:08:57,290
하지만 손실은 약간 다릅니다.

234
00:08:57,290 --> 00:09:00,990
소프트맥스를 사용하는 대신, 그들은 KL 발산 손실을 사용합니다.

235
00:09:00,990 --> 00:09:02,710
내가 이 것을 언급하는

236
00:09:02,710 --> 00:09:05,850
이유는 DINO V2의 존재를 알아야 하기 때문입니다.

237
00:09:05,850 --> 00:09:08,330
우리가 그것이 정확히 무엇을

238
00:09:08,330 --> 00:09:12,790
하는지에 대해 이야기하지 않더라도, DINO V2는 요즘 실제로 많이

239
00:09:12,790 --> 00:09:14,990
사용되는 자기 감독 기능을 위한

240
00:09:14,990 --> 00:09:16,590
매우 강력한 모델입니다.

241
00:09:16,590 --> 00:09:19,070
그래서 그들이 기본적으로 한 것은 DINO V1의 이

242
00:09:19,070 --> 00:09:21,590
레시피를 가져온 것입니다. 이는 MoCo와 유사하고

243
00:09:21,590 --> 00:09:23,730
SimCLR의 많은 아이디어도 포함되어 있지만,

244
00:09:23,730 --> 00:09:26,550
그들의 접근 방식에는 많은 독특한 세부 사항도 있습니다.

245
00:09:26,550 --> 00:09:28,682
하지만 DINO V2의 큰 차이점은 훈련

246
00:09:28,682 --> 00:09:30,890
데이터를 상당히 많이 확장했다는 것입니다.

247
00:09:30,890 --> 00:09:33,112
그래서 이전의 많은 자기 감독 접근 방식은

248
00:09:33,112 --> 00:09:35,070
100만 개의 이미지로 구성된

249
00:09:35,070 --> 00:09:37,510
ImageNet 데이터셋에서 훈련되었고, DINO V2는

250
00:09:37,510 --> 00:09:39,150
약 1억 4200만 개의 이미지로

251
00:09:39,150 --> 00:09:42,090
구성된 훨씬 더 큰 훈련 세트로 이 접근 방식을 성공적으로

252
00:09:42,090 --> 00:09:43,490
확장할 수 있었습니다.

253
00:09:43,490 --> 00:09:46,210
딥러닝에서는 더 큰 네트워크, 더 많은 데이터, 더

254
00:09:46,210 --> 00:09:49,000
많은 GPU, 더 많은 플롭을 좋아합니다. 우리는

255
00:09:49,000 --> 00:09:50,460
그런 모든 것을 좋아합니다.

256
00:09:50,460 --> 00:09:52,500
그리고 DINO V2는 이 훨씬 더 큰

257
00:09:52,500 --> 00:09:54,860
데이터셋에 성공적으로 확장된 자기 감독 학습을 위한

258
00:09:54,860 --> 00:09:56,900
레시피를 찾을 수 있었고, 매우 강력한

259
00:09:56,900 --> 00:09:58,200
자기 감독 기능을 제공합니다.

260
00:09:58,200 --> 00:10:00,780
그리고 이것은 오늘날 실제로 기능을

261
00:10:00,780 --> 00:10:03,580
선택하고 이를 미세 조정하거나 감독하여

262
00:10:03,580 --> 00:10:07,150
자신의 다운스트림 작업에 사용할 때 많이 사용됩니다.

263
00:10:07,150 --> 00:10:08,400
다시 말하지만, 저는 여러분이 이

264
00:10:08,400 --> 00:10:10,400
작업의 모든 세부 사항을 이해하기를 기대하지 않습니다.

265
00:10:10,400 --> 00:10:11,140
여러분이 알기를 기대하지 않습니다.

266
00:10:11,140 --> 00:10:13,180
하지만 여러분이 나중에 자신의

267
00:10:13,180 --> 00:10:15,820
프로젝트에 사용할 수 있도록

268
00:10:15,820 --> 00:10:18,520
존재한다는 것을 아는 것이 중요합니다.

269
00:10:18,520 --> 00:10:20,020
그래서 자가 지도 학습에 대해

270
00:10:20,020 --> 00:10:21,700
제가 말하고 싶었던 것은 이 정도입니다.

271
00:10:21,700 --> 00:10:23,700
오늘 강의의 핵심 내용으로 넘어가기

272
00:10:23,700 --> 00:10:25,590
전에 이에 대한 질문이 있나요?

273
00:10:32,920 --> 00:10:33,420
좋아요.

274
00:10:33,420 --> 00:10:34,780
아닌 것 같군요.

275
00:10:34,780 --> 00:10:38,500
그래서 오늘의 주요 주제는 생성 모델입니다.

276
00:10:38,500 --> 00:10:39,580
정말 멋집니다.

277
00:10:39,580 --> 00:10:41,780
이것은 10년 전에는 전혀

278
00:10:41,780 --> 00:10:45,320
작동하지 않다가 최근 몇 년 동안 정말 잘 작동하게

279
00:10:45,320 --> 00:10:47,580
된 딥러닝의 한 분야입니다.

280
00:10:47,580 --> 00:10:50,520
이로 인해 언어 모델과 같은 것들이 등장했습니다.

281
00:10:50,520 --> 00:10:52,460
이들은 생성 모델로 볼 수

282
00:10:52,460 --> 00:10:55,340
있으며, 모든 이미지 생성 모델, 비디오

283
00:10:55,340 --> 00:10:56,960
생성 모델도 포함됩니다.

284
00:10:56,960 --> 00:10:59,900
이들은 제가 대학원에 있을 때 전혀

285
00:10:59,900 --> 00:11:01,220
작동하지 않았습니다.

286
00:11:01,220 --> 00:11:02,860
이 샘플들을 보고 들여다보면,

287
00:11:02,860 --> 00:11:04,600
해상도가 낮고 완전히 흐릿한

288
00:11:04,600 --> 00:11:06,040
쓰레기처럼 보였습니다.

289
00:11:06,040 --> 00:11:08,545
하지만 어 somehow 그들 속에서 어떤 가능성을 볼 수 있었습니다.

290
00:11:08,545 --> 00:11:10,420
사람들이 계속해서 그 흐릿한 쓰레기를

291
00:11:10,420 --> 00:11:11,740
뚫고 나아가고 지난 10년

292
00:11:11,740 --> 00:11:13,440
동안 이를 확장해 온 것이

293
00:11:13,440 --> 00:11:15,680
기쁩니다. 이제 많은 기술들이 정말로 작동하고,

294
00:11:15,680 --> 00:11:17,180
이는 매우 흥미롭습니다.

295
00:11:17,180 --> 00:11:19,780
이것은 우리가 이 수업을 처음 가르쳤을

296
00:11:19,780 --> 00:11:22,100
때 전혀 작동하지 않았던 딥러닝의

297
00:11:22,100 --> 00:11:25,980
한 분야이며, 이제 작동한다는 것이 정말 멋집니다.

298
00:11:25,980 --> 00:11:28,260
하지만 그렇다고 해도 생성 모델링에 대한 많은

299
00:11:28,260 --> 00:11:30,820
기본 아이디어는 여전히 동일하게 유지되고 있습니다.

300
00:11:30,820 --> 00:11:34,100
데이터를 생각하는 방법, 이를 모델링하기

301
00:11:34,100 --> 00:11:36,700
위한 접근 방식에 대한 아이디어는

302
00:11:36,700 --> 00:11:41,340
지난 10년 동안 그렇게 많이 변하지 않았습니다.

303
00:11:41,340 --> 00:11:44,460
변화한 것은 더 많은 계산, 더 안정적인

304
00:11:44,460 --> 00:11:47,340
훈련 레시피, 더 큰 데이터셋, 분산

305
00:11:47,340 --> 00:11:48,380
훈련입니다.

306
00:11:48,380 --> 00:11:51,000
이 모든 것을 더 유용한 작업으로 확장할 수 있는

307
00:11:51,000 --> 00:11:52,792
능력이 지난 10년 동안의 발전을

308
00:11:52,792 --> 00:11:53,980
이끌었다고 생각합니다.

309
00:11:53,980 --> 00:11:55,980
알고리즘의 몇 가지 조정이 있었고,

310
00:11:55,980 --> 00:12:00,540
특히 다음 강의에서 확산 모델에 대해 이야기할 때 이를 볼 것입니다.

311
00:12:00,540 --> 00:12:02,880
하지만 먼저 생성 모델링에 대해

312
00:12:02,880 --> 00:12:05,380
이야기하기 전에, 감독 학습과 비감독

313
00:12:05,380 --> 00:12:08,060
학습에 대해 잠시 되짚어보고 싶습니다.

314
00:12:08,060 --> 00:12:10,060
딥러닝에서 접근하려고 하는 몇

315
00:12:10,060 --> 00:12:12,100
가지 다른 작업이 있으며, 때때로

316
00:12:12,100 --> 00:12:14,180
몇 가지 다른 직교 축을

317
00:12:14,180 --> 00:12:15,587
따라 나눌 수 있습니다.

318
00:12:15,587 --> 00:12:17,420
그래서 우리는 용어와 명칭을 명확히

319
00:12:17,420 --> 00:12:20,540
하기 위해 그것들에 대해 조금 이야기하고 싶었습니다.

320
00:12:20,540 --> 00:12:22,340
감독 학습은 지난 강의를

321
00:12:22,340 --> 00:12:25,900
제외하고 이번 학기 동안 우리가 주로 해온 것입니다.

322
00:12:25,900 --> 00:12:29,317
감독 학습에서는 x와 y의 쌍으로 이루어진 데이터셋이 있습니다.

323
00:12:29,317 --> 00:12:30,900
목표는 입력 데이터 x에서

324
00:12:30,900 --> 00:12:34,620
목표 또는 레이블 y로 매핑하는 함수를 배우는 것입니다.

325
00:12:34,620 --> 00:12:38,180
지금까지 이 접근 방식의 많은 예를 보았습니다.

326
00:12:38,180 --> 00:12:40,380
예를 들어 이미지 분류에서, 우리의 입력

327
00:12:40,380 --> 00:12:41,240
x는 이미지입니다.

328
00:12:41,240 --> 00:12:45,360
출력 y는 레이블 또는 이미지 캡션이 될 것입니다.

329
00:12:45,360 --> 00:12:47,160
입력 x는 이미지가 될 것입니다.

330
00:12:47,160 --> 00:12:49,700
출력 y는 우리가 그 이미지에서 보는 것을 설명하는

331
00:12:49,700 --> 00:12:50,940
텍스트의 일부가 될 것입니다.

332
00:12:50,940 --> 00:12:51,780
객체 탐지.

333
00:12:51,780 --> 00:12:52,840
입력은 이미지입니다.

334
00:12:52,840 --> 00:12:54,740
출력은 이미지에 나타나는 객체를

335
00:12:54,740 --> 00:12:57,260
설명하는 상자와 범주 레이블의 집합입니다.

336
00:12:57,260 --> 00:12:58,160
또는 분할.

337
00:12:58,160 --> 00:13:02,550
입력 이미지의 모든 픽셀에 레이블을 할당할 수도 있습니다.

338
00:13:02,550 --> 00:13:04,300
이들은 감독 학습 문제입니다.

339
00:13:04,300 --> 00:13:06,298
해결하려는 작업, 예측하고자

340
00:13:06,298 --> 00:13:08,340
하는 것은 데이터 세트에

341
00:13:08,340 --> 00:13:09,440
정확히 있습니다.

342
00:13:09,440 --> 00:13:11,920
어떤 의미에서, 당신이 해야 할 일은 훈련

343
00:13:11,920 --> 00:13:13,563
데이터 세트에서 x에서

344
00:13:13,563 --> 00:13:14,980
y로의 매핑을 모방하는

345
00:13:14,980 --> 00:13:17,860
함수를 배우고, 그 매핑을 훈련 데이터 세트를

346
00:13:17,860 --> 00:13:20,180
넘어 새로운 샘플에 일반화하는 것입니다.

347
00:13:20,180 --> 00:13:22,540
이제 비지도 학습은 조금 더

348
00:13:22,540 --> 00:13:26,620
수상하고 신비롭고 설명하기 어려운 것입니다.

349
00:13:26,620 --> 00:13:28,600
비지도 학습, 또는 때때로 자기

350
00:13:28,600 --> 00:13:30,400
감독 학습의 아이디어는

351
00:13:30,400 --> 00:13:32,680
레이블이 없고 데이터만 있다는 것입니다.

352
00:13:32,680 --> 00:13:34,340
샘플 x만 있습니다.

353
00:13:34,340 --> 00:13:35,620
이미지만 있습니다.

354
00:13:35,620 --> 00:13:38,620
그 데이터에서 어떤 구조를 배우고 싶습니다.

355
00:13:38,620 --> 00:13:41,200
특정한 목표 작업이 없습니다.

356
00:13:41,200 --> 00:13:43,480
그저 모든 데이터에서 좋은 표현과

357
00:13:43,480 --> 00:13:45,820
좋은 구조를 발견하려고 합니다.

358
00:13:45,820 --> 00:13:46,500
왜냐하면?

359
00:13:46,500 --> 00:13:49,020
우리가 자기 감독 학습에서 자주 이야기한 것처럼,

360
00:13:49,020 --> 00:13:52,340
나중에 다운스트림 작업에 적용할 수 있도록 하기 위해서입니다.

361
00:13:52,340 --> 00:13:55,380
하지만 비지도 학습의 작업

362
00:13:55,380 --> 00:13:58,380
자체는 종종 다소 불특정합니다.

363
00:13:58,380 --> 00:14:00,400
이의 몇 가지 예는 K-평균

364
00:14:00,400 --> 00:14:02,820
군집화로, 데이터에서 클러스터를

365
00:14:02,820 --> 00:14:06,060
식별하려고 할 수 있습니다. 이는

366
00:14:06,060 --> 00:14:10,300
레이블이 없더라도 원시 픽셀에서 검토할 수 있는 구조입니다.

367
00:14:10,300 --> 00:14:12,318
또는 차원 축소, PCA로,

368
00:14:12,318 --> 00:14:14,860
데이터의 구조를 설명하는 낮은 차원의

369
00:14:14,860 --> 00:14:17,020
부분 공간이나 낮은 차원의 다양체를

370
00:14:17,020 --> 00:14:18,340
발견하려고 합니다.

371
00:14:18,340 --> 00:14:19,380
그리고 다시, 이는 우리가

372
00:14:19,380 --> 00:14:21,080
데이터 자체에서 발견하려고 하는 것입니다.

373
00:14:21,080 --> 00:14:24,100
이것이 무엇이어야 하는지에 대한 주석이 없습니다.

374
00:14:24,100 --> 00:14:25,017
또는 밀도 추정.

375
00:14:25,017 --> 00:14:27,183
아마도 우리는 데이터에 확률 분포를 맞추려고

376
00:14:27,183 --> 00:14:27,880
할 것입니다.

377
00:14:27,880 --> 00:14:30,060
우리가 보고 있는 데이터 샘플을 생성한

378
00:14:30,060 --> 00:14:32,360
확률 함수가 무엇인지 이해하려고 합니다.

379
00:14:32,360 --> 00:14:34,027
그리고 다시, 우리는 이를 위한 명시적인

380
00:14:34,027 --> 00:14:35,960
레이블이나 명시적인 훈련 세트가 없습니다.

381
00:14:35,960 --> 00:14:37,740
그래서 이는 우리가 훈련 과정을

382
00:14:37,740 --> 00:14:40,820
통해 발견하려고 하는 숨겨진 또는 잠재적인 구조입니다.

383
00:14:40,820 --> 00:14:44,140
이 비지도 이분법은 항상 염두에 두어야

384
00:14:44,140 --> 00:14:45,340
할 것입니다.

385
00:14:45,340 --> 00:14:47,560
확률적이지 않거나 반드시 생성적이지

386
00:14:47,560 --> 00:14:50,080
않은 비지도 학습을 할 수 있습니다.

387
00:14:50,080 --> 00:14:52,480
군집화와 PCA와 같은 것은

388
00:14:52,480 --> 00:14:54,582
종종 확률적 해석을 가집니다.

389
00:14:54,582 --> 00:14:56,540
하지만 이들은 반드시 생성적이거나

390
00:14:56,540 --> 00:14:58,940
확률적 해석을 가지지 않거나 그렇게

391
00:14:58,940 --> 00:15:01,900
생각할 필요가 없는 비지도 학습의 예입니다.

392
00:15:01,900 --> 00:15:04,900
그래서 저는 비지도 이분법을 방법이나

393
00:15:04,900 --> 00:15:10,220
시스템이 위치할 수 있는 하나의 스펙트럼으로 생각하는 것을 좋아합니다.

394
00:15:10,220 --> 00:15:13,780
시스템이나 작업을 분류할 수 있는 별도의 스펙트럼은

395
00:15:13,780 --> 00:15:16,260
생성 모델과 판별 모델의 차이입니다.

396
00:15:16,260 --> 00:15:17,975
이들은 본질적으로 확률적입니다.

397
00:15:17,975 --> 00:15:20,100
생성 모델이나 판별 모델에 대해

398
00:15:20,100 --> 00:15:23,140
이야기할 때, 우리는 항상 데이터를 통해

399
00:15:23,140 --> 00:15:25,820
발견하거나 배우려는 확률적 구조를 상상합니다.

400
00:15:25,820 --> 00:15:27,300
차이는 우리가

401
00:15:27,300 --> 00:15:28,780
모델링하려는 변수 간의

402
00:15:28,780 --> 00:15:31,300
확률적 관계가 무엇인지입니다.

403
00:15:31,300 --> 00:15:32,780
판별 모델에서는

404
00:15:32,780 --> 00:15:35,460
일반적으로 y와 x가 있습니다.

405
00:15:35,460 --> 00:15:37,860
보통 x는 크고 고차원적인 것으로,

406
00:15:37,860 --> 00:15:40,280
우리 경우에는 이미지입니다.

407
00:15:40,280 --> 00:15:42,410
y는 어떤 레이블이나

408
00:15:42,410 --> 00:15:44,410
설명, 보조 정보입니다.

409
00:15:44,410 --> 00:15:47,110
예를 들어, 텍스트, 캡션,

410
00:15:47,110 --> 00:15:50,050
카테고리 레이블과 같은 것입니다.

411
00:15:50,050 --> 00:15:52,352
판별 모델에 대해 이야기할 때,

412
00:15:52,352 --> 00:15:53,810
우리는 x를 주어진

413
00:15:53,810 --> 00:15:56,290
y의 확률 분포를 배우려고 합니다.

414
00:15:56,290 --> 00:15:57,970
즉, 입력 이미지

415
00:15:57,970 --> 00:16:02,570
x에 조건화된 레이블에 대한 분포를 배우려고 합니다.

416
00:16:02,570 --> 00:16:07,070
확률적으로 무슨 일이 일어나고 있는지 진정으로 이해하려면, 확률 분포의

417
00:16:07,070 --> 00:16:09,690
매우 중요한 특징 하나를 기억해야 합니다.

418
00:16:09,690 --> 00:16:12,280
그것은 정규화되어 있다는 것입니다.

419
00:16:12,280 --> 00:16:14,530
확률 분포 또는 더 일반적으로

420
00:16:14,530 --> 00:16:16,890
x의 밀도 함수 p에

421
00:16:16,890 --> 00:16:22,250
대해 이야기할 때, p는 기본적으로 모든 가능한 입력 x에 비제로

422
00:16:22,250 --> 00:16:27,010
숫자를 할당하는 함수이며, 매우 중요한 정규화 제약

423
00:16:27,010 --> 00:16:29,570
조건이 있습니다. 즉, 모든

424
00:16:29,570 --> 00:16:33,590
가능한 x의 전체 공간을 적분하면 1이 됩니다.

425
00:16:33,590 --> 00:16:35,130
이 정규화 제약 조건은

426
00:16:35,130 --> 00:16:38,210
어떤 면에서 확률 모델의 힘을 발생시킵니다.

427
00:16:38,210 --> 00:16:41,050
왜냐하면 정규화 제약 조건은 모든 x가

428
00:16:41,050 --> 00:16:44,870
확률 질량을 놓고 경쟁해야 함을 의미하기 때문입니다.

429
00:16:44,870 --> 00:16:49,110
확률 질량의 고정된 단위 양이 있으며, 확률 분포나 밀도

430
00:16:49,110 --> 00:16:51,890
함수를 선택하는 것은 기본적으로 그

431
00:16:51,890 --> 00:16:53,610
고정된 양의 확률 질량을

432
00:16:53,610 --> 00:16:55,370
할당하고 존재할 수

433
00:16:55,370 --> 00:16:57,810
있는 모든 x의 가능한 값에

434
00:16:57,810 --> 00:16:59,090
분산시키는 것입니다.

435
00:16:59,090 --> 00:17:01,035
모든 x는 경쟁 중입니다. 왜냐하면

436
00:17:01,035 --> 00:17:02,410
주어질 수 있는 질량의 고정된

437
00:17:02,410 --> 00:17:03,990
단위 양이 있기 때문입니다.

438
00:17:03,990 --> 00:17:06,329
따라서 하나의 x의 확률을

439
00:17:06,329 --> 00:17:09,050
높이려면, 반드시 다른 x의 확률이나

440
00:17:09,050 --> 00:17:11,050
밀도가 낮아져야 합니다.

441
00:17:11,050 --> 00:17:14,089
따라서 이러한 다양한 확률 모델의

442
00:17:14,089 --> 00:17:16,353
공식화에서 기본적으로 변화하는

443
00:17:16,353 --> 00:17:17,770
것은 확률 질량을

444
00:17:17,770 --> 00:17:19,755
놓고 경쟁하는 변수들입니다.

445
00:17:19,755 --> 00:17:22,130
즉, 우리가 페이지에 쓰는 기호가

446
00:17:22,130 --> 00:17:26,849
매우 유사하게 보이더라도, 확률 질량을 놓고 경쟁하는 것들

447
00:17:26,849 --> 00:17:28,927
간의 다양한 경쟁은 모델이

448
00:17:28,927 --> 00:17:31,010
배우거나 발견하려는 매우

449
00:17:31,010 --> 00:17:32,890
다른 구조를 유도합니다.

450
00:17:32,890 --> 00:17:34,670
판별 모델의 경우, 우리는

451
00:17:34,670 --> 00:17:36,170
x에 조건화된 y의

452
00:17:36,170 --> 00:17:40,050
확률 모델을 배우고 있습니다. 즉, 모든 x에 대해 모델이

453
00:17:40,050 --> 00:17:42,690
가능한 모든 레이블에 대한 확률 분포를

454
00:17:42,690 --> 00:17:44,210
예측하고 있습니다.

455
00:17:44,210 --> 00:17:47,370
따라서 레이블이 고유하고 범주형인 경우, 예를

456
00:17:47,370 --> 00:17:51,060
들어 고양이 또는 개와 같은 경우, 확률은 0에서 1

457
00:17:51,060 --> 00:17:55,210
사이의 고정된 양이 있으며, 고양이와 개는 1이 되어야 합니다.

458
00:17:55,210 --> 00:17:57,810
모든 입력 x에 대해 레이블에

459
00:17:57,810 --> 00:18:00,508
대한 별도의 확률 분포가 있습니다.

460
00:18:00,508 --> 00:18:02,050
중요하게도, 여기서 이미지

461
00:18:02,050 --> 00:18:05,510
간의 확률 질량에 대한 경쟁이 없다는 점에 유의하세요.

462
00:18:05,510 --> 00:18:07,970
각 이미지는 레이블 공간에 대한 자신의

463
00:18:07,970 --> 00:18:10,450
분포를 유도하므로, 서로 다른 이미지

464
00:18:10,450 --> 00:18:13,170
간에 질량을 놓고 경쟁하지 않습니다.

465
00:18:13,170 --> 00:18:14,970
경쟁하는 것은 각 이미지에

466
00:18:14,970 --> 00:18:16,943
대한 서로 다른 레이블뿐입니다.

467
00:18:16,943 --> 00:18:18,610
이것은 판별 모델링을

468
00:18:18,610 --> 00:18:20,850
생각할 때 매우 중요합니다.

469
00:18:20,850 --> 00:18:22,730
또한 판별 모델링의

470
00:18:22,730 --> 00:18:25,810
흥미로운 다른 측면은 비합리적인

471
00:18:25,810 --> 00:18:29,090
입력을 거부할 방법이 없다는 것입니다.

472
00:18:29,090 --> 00:18:31,310
예를 들어, 고양이와 개라는

473
00:18:31,310 --> 00:18:33,970
레이블 공간을 고정한 후, 원숭이나

474
00:18:33,970 --> 00:18:36,530
추상 미술과 같은 고양이나 개가 아닌

475
00:18:36,530 --> 00:18:39,590
것을 입력하면 시스템은 유연성이 없습니다.

476
00:18:39,590 --> 00:18:41,790
비합리적이라고 말할 자유가 없습니다.

477
00:18:41,790 --> 00:18:43,650
시작할 때 할당한 고정된

478
00:18:43,650 --> 00:18:46,490
어휘에 대한 분포를 출력해야 합니다.

479
00:18:46,490 --> 00:18:48,828
이것은 단점으로 볼 수 있지만,

480
00:18:48,828 --> 00:18:50,370
다양한 데이터를 확률적으로

481
00:18:50,370 --> 00:18:52,177
모델링할 때 내부에서

482
00:18:52,177 --> 00:18:54,010
정확히 무슨 일이 일어나고

483
00:18:54,010 --> 00:18:56,103
있는지 이해하는 것이 중요합니다.

484
00:18:56,103 --> 00:18:58,270
이제 생성 모델은 매우 다릅니다.

485
00:18:58,270 --> 00:19:00,395
생성 모델에서는 x의 분포

486
00:19:00,395 --> 00:19:02,210
p를 배우고 있습니다.

487
00:19:02,210 --> 00:19:05,770
우리는 모든 가능한 이미지 x에 대한 분포를 배우고자 합니다.

488
00:19:05,770 --> 00:19:07,590
그리고 이제 이것은 매우 흥미롭습니다.

489
00:19:07,590 --> 00:19:09,850
이는 우주에서 존재할 수 있는 모든

490
00:19:09,850 --> 00:19:11,650
가능한 이미지가 확률 질량을

491
00:19:11,650 --> 00:19:14,610
놓고 서로 경쟁하고 있다는 것을 의미합니다.

492
00:19:14,610 --> 00:19:18,010
그리고 이는 이제 정말 어려운 질문입니다. 겉보기에는

493
00:19:18,010 --> 00:19:21,835
간단해 보이지만, 이는 세계에 대한 매우 깊고

494
00:19:21,835 --> 00:19:23,210
철학적인 문제에 직면하게

495
00:19:23,210 --> 00:19:25,870
만듭니다. 왜냐하면 이제 모든 이미지가

496
00:19:25,870 --> 00:19:29,070
확률 질량을 놓고 경쟁하고 있기 때문입니다.

497
00:19:29,070 --> 00:19:31,410
이를 모델링하기 위해서는 세 발

498
00:19:31,410 --> 00:19:34,730
달린 개의 이미지가 세 팔 달린 원숭이의

499
00:19:34,730 --> 00:19:36,850
이미지와 어떻게 확률 질량을

500
00:19:36,850 --> 00:19:38,530
가져야 하는지와 같은

501
00:19:38,530 --> 00:19:40,187
질문에 답해야 합니다.

502
00:19:40,187 --> 00:19:42,770
아마도 세 발 달린 개는 다리를 잃은 개가 있을 수

503
00:19:42,770 --> 00:19:45,187
있기 때문에 더 많은 확률 질량을 가져야 할 것입니다.

504
00:19:45,187 --> 00:19:47,270
하지만 세 팔 달린 원숭이는 어떻게 얻을 수 있을까요?

505
00:19:47,270 --> 00:19:49,210
모르겠습니다. 그것은 훨씬 더 드문 것처럼

506
00:19:49,210 --> 00:19:51,910
보입니다. 아니면 공상 과학 이미지를 모델링하지 않는 한 말이죠.

507
00:19:51,910 --> 00:19:54,890
모든 가능한 이미지가 확률 질량을 놓고 경쟁하는

508
00:19:54,890 --> 00:19:56,690
이 영역에 들어가면, 이제

509
00:19:56,690 --> 00:19:59,330
모델은 데이터에서 존재할 수 있는 구조에

510
00:19:59,330 --> 00:20:01,010
대해 매우 신중하게 생각해야

511
00:20:01,010 --> 00:20:04,290
하며, 문제를 해결하는 것이 훨씬 더 어려워집니다.

512
00:20:04,290 --> 00:20:06,330
여기서 또 흥미로운 점은 이제 생성 모델을

513
00:20:06,330 --> 00:20:07,990
사용하면 모델이 기본적으로 '아니요,

514
00:20:07,990 --> 00:20:10,770
이것은 합리적인 이미지가 아닙니다.'라고 말할 수 있는

515
00:20:10,770 --> 00:20:12,310
능력을 갖추게 되었다는 것입니다.

516
00:20:12,310 --> 00:20:13,710
이것은 합리적인 입력이 아닙니다.

517
00:20:13,710 --> 00:20:15,127
모델이 그렇게 할 수

518
00:20:15,127 --> 00:20:19,090
있는 방법은 얻은 이미지에 낮거나 심지어 0의 확률 질량을

519
00:20:19,090 --> 00:20:20,310
할당하는 것입니다.

520
00:20:20,310 --> 00:20:21,910
그래서 아마도 우리의 생성

521
00:20:21,910 --> 00:20:25,015
모델은 동물원 동물의 생성 모델만 원할 것입니다.

522
00:20:25,015 --> 00:20:27,390
동물원 동물의 생성 모델을 원한다면,

523
00:20:27,390 --> 00:20:30,150
추상 예술의 이미지를 입력하면 0의

524
00:20:30,150 --> 00:20:31,910
확률 질량을 가져야 합니다.

525
00:20:31,910 --> 00:20:34,410
그래서 이제 우리는 이 유형의 이미지가 우리가

526
00:20:34,410 --> 00:20:36,770
관심 있는 범위에 포함되지 않는다고 거부하거나

527
00:20:36,770 --> 00:20:39,130
말할 수 있는 메커니즘을 갖게 되었습니다.

528
00:20:39,130 --> 00:20:42,233
그리고 이제 조건부 생성 모델은 더욱 흥미롭습니다.

529
00:20:42,233 --> 00:20:44,650
여기서 우리는 어떤 레이블 신호

530
00:20:44,650 --> 00:20:49,450
y에 조건화된 이미지 x에 대한 조건부 분포를 배우고 있습니다.

531
00:20:49,450 --> 00:20:52,810
이는 이제 모든 가능한 레이블에 대해 모든 가능한

532
00:20:52,810 --> 00:20:56,570
이미지 간의 경쟁을 유도하고 있다는 것을 의미합니다.

533
00:20:56,570 --> 00:20:59,010
이 경우 y가 고양이와 개의 범주형

534
00:20:59,010 --> 00:21:03,690
레이블이라고 가정하면, 이제 각 가능한 범주형 레이블인 고양이와 개에 대해

535
00:21:03,690 --> 00:21:06,330
모델이 모든 가능한 이미지 간의 경쟁을

536
00:21:06,330 --> 00:21:07,970
별도로 유도하고 있습니다.

537
00:21:07,970 --> 00:21:10,370
그래서 이제 상위 분포에서, 아마도 이것은

538
00:21:10,370 --> 00:21:12,890
고양이 레이블에 조건화된 모든 이미지의 확률일

539
00:21:12,890 --> 00:21:14,970
것이므로, 고양이 이미지의 확률은

540
00:21:14,970 --> 00:21:15,750
높아야 합니다.

541
00:21:15,750 --> 00:21:17,610
아마도 원숭이와 개의 이미지는 적어도

542
00:21:17,610 --> 00:21:19,570
여전히 포유류이기 때문에 다소

543
00:21:19,570 --> 00:21:20,310
높아야 합니다.

544
00:21:20,310 --> 00:21:22,850
하지만 추상 예술은 매우 낮아야 하며, 아마도 0일 것입니다.

545
00:21:22,850 --> 00:21:25,170
그리고 개 레이블에 조건화된 이미지

546
00:21:25,170 --> 00:21:27,352
간의 다른 분포가 있습니다.

547
00:21:27,352 --> 00:21:28,810
그리고 만약 당신의

548
00:21:28,810 --> 00:21:30,643
조건 신호 y가 단일

549
00:21:30,643 --> 00:21:33,285
범주형 레이블보다 훨씬 더 풍부한

550
00:21:33,285 --> 00:21:36,080
것이라면, 그 조건 신호 y는 텍스트

551
00:21:36,080 --> 00:21:37,572
설명일 수 있습니다.

552
00:21:37,572 --> 00:21:39,780
전체 단락의 서면 텍스트일 수 있습니다.

553
00:21:39,780 --> 00:21:42,260
또는 다른 이미지와 텍스트 조각일 수 있습니다.

554
00:21:42,260 --> 00:21:44,920
그래서 이제 이러한 매우 풍부한 출력 공간

555
00:21:44,920 --> 00:21:48,652
x를 모델링하고, 매우 풍부한 입력 공간 y에 조건화하면,

556
00:21:48,652 --> 00:21:50,360
이제 모델에게 관련된

557
00:21:50,360 --> 00:21:53,000
객체에 대한 매우 깊은 추론이 필요한

558
00:21:53,000 --> 00:21:55,120
매우 복잡하고 다소 정의되지 않은

559
00:21:55,120 --> 00:21:57,478
문제를 해결하도록 요청하는 것입니다.

560
00:21:57,478 --> 00:21:59,520
그래서 저는 생성 모델링이 매우

561
00:21:59,520 --> 00:22:03,860
흥미로운 주제라고 생각합니다. 왜냐하면 그것은 간단해 보이기 때문입니다.

562
00:22:03,860 --> 00:22:05,540
우리가 한 일은 x와 y를 뒤집는 것뿐입니다.

563
00:22:05,540 --> 00:22:06,583
얼마나 어려울 수 있을까요?

564
00:22:06,583 --> 00:22:08,000
하지만 갑자기 우리는 시각

565
00:22:08,000 --> 00:22:09,375
세계에서 무슨 일이 일어나고

566
00:22:09,375 --> 00:22:11,560
있는지에 대해 정말로 깊이 생각해야 했습니다.

567
00:22:11,560 --> 00:22:13,760
또 흥미로운 점은 우리가 판별

568
00:22:13,760 --> 00:22:16,560
생성 모델과 조건부 생성 모델을 세

569
00:22:16,560 --> 00:22:19,680
가지 별개의 범주로 작성했다는 것입니다.

570
00:22:19,680 --> 00:22:21,793
하지만 사실 이들은 모두 관련이 있습니다.

571
00:22:21,793 --> 00:22:23,460
그리고 이들은 베이즈

572
00:22:23,460 --> 00:22:28,880
정리를 통해 연결되어 있으며, 이는 확률에서 가장 놀라운 관계 중
하나입니다.

573
00:22:28,880 --> 00:22:33,720
특히, 만약 우리가 판별 모델 p(y|x)와 무조건 생성

574
00:22:33,720 --> 00:22:36,360
모델 p(x), 그리고 레이블

575
00:22:36,360 --> 00:22:38,920
y에 대한 사전 분포를 가지고

576
00:22:38,920 --> 00:22:41,440
있다면, 이를 조합하여

577
00:22:41,440 --> 00:22:44,240
조건부 생성 모델 p(y|x)를

578
00:22:44,240 --> 00:22:45,920
구축할 수 있습니다.

579
00:22:45,920 --> 00:22:48,160
또한 일반적으로 베이즈 정리를 어떤

580
00:22:48,160 --> 00:22:50,240
방식으로든 재배열할 수 있어, 이 중

581
00:22:50,240 --> 00:22:54,780
두 개를 가지고 있다면 항상 세 번째를 얻을 수 있습니다. 이는 꽤
멋집니다.

582
00:22:54,780 --> 00:22:57,813
이론적으로, 원칙적으로는 다른 두 구성 요소로부터

583
00:22:57,813 --> 00:22:59,480
조건부 생성 모델을

584
00:22:59,480 --> 00:23:00,780
구축할 수 있습니다.

585
00:23:00,780 --> 00:23:03,620
하지만 실제로는 이렇게 하지는 않습니다.

586
00:23:03,620 --> 00:23:06,040
보통은 조건부 생성 모델을

587
00:23:06,040 --> 00:23:08,557
독립적으로 처음부터 학습합니다.

588
00:23:08,557 --> 00:23:10,640
하지만 우리가 확산에 대해 이야기할 때,

589
00:23:10,640 --> 00:23:13,000
어떤 이유로 인해 조건부 및 무조건

590
00:23:13,000 --> 00:23:14,985
모델을 함께 학습하는 경우도 있습니다.

591
00:23:14,985 --> 00:23:16,360
하지만 이러한

592
00:23:16,360 --> 00:23:17,902
다양한 확률 모델의

593
00:23:17,902 --> 00:23:20,962
깊은 관계를 기억하는 것이 좋습니다.

594
00:23:20,962 --> 00:23:22,420
그렇다면, 이러한 다양한

595
00:23:22,420 --> 00:23:24,212
확률 모델로 무엇을 할 수 있을지

596
00:23:24,212 --> 00:23:25,360
궁금할 수 있습니다.

597
00:23:25,360 --> 00:23:26,920
판별 모델의 경우, 이 모델은 많은

598
00:23:26,920 --> 00:23:28,503
창의성을 요구하지 않아야 합니다.

599
00:23:28,503 --> 00:23:31,453
이번 분기 동안 이러한 예를 많이 보았습니다.

600
00:23:31,453 --> 00:23:33,620
판별 모델을 훈련한 후에는 데이터에

601
00:23:33,620 --> 00:23:34,940
레이블을 할당할 수 있습니다.

602
00:23:34,940 --> 00:23:36,500
특징 학습도 할 수 있습니다.

603
00:23:36,500 --> 00:23:39,260
예를 들어, ImageNet에서 감독

604
00:23:39,260 --> 00:23:41,000
학습을 하는 경우, 이미지의

605
00:23:41,000 --> 00:23:43,700
범주형 레이블을 예측하려고 하는

606
00:23:43,700 --> 00:23:46,280
과정에서 이러한 모델은 중간에 유용한

607
00:23:46,280 --> 00:23:47,822
특징 표현을 학습하는

608
00:23:47,822 --> 00:23:48,860
경향이 있습니다.

609
00:23:48,860 --> 00:23:49,598
따라서 판별

610
00:23:49,598 --> 00:23:51,640
모델은 관심 있는 y를 직접

611
00:23:51,640 --> 00:23:53,580
예측하거나, 이러한 y를

612
00:23:53,580 --> 00:23:55,760
예측하는 과정에서 유도된 특징 표현을

613
00:23:55,760 --> 00:23:57,677
학습하는 데 사용됩니다.

614
00:24:00,380 --> 00:24:02,280
무조건 생성 모델은 일반적으로

615
00:24:02,280 --> 00:24:04,980
실제로는 쓸모가 없다고 생각합니다.

616
00:24:04,980 --> 00:24:08,360
하지만 이 모델은 아마도 이상치를 감지할 수 있게 해줍니다.

617
00:24:08,360 --> 00:24:11,400
이미지를 보고, 그것들이 정말로 낮은 확률 질량을 가지고

618
00:24:11,400 --> 00:24:12,500
있는지 확인합니다.

619
00:24:12,500 --> 00:24:15,260
그것들이 비합리적인 이미지인지 확인합니다.

620
00:24:15,260 --> 00:24:18,020
데이터나 레이블 없이 특징 학습에 사용할

621
00:24:18,020 --> 00:24:19,160
수 있습니다.

622
00:24:19,160 --> 00:24:20,880
따라서 무조건 분포

623
00:24:20,880 --> 00:24:23,180
p(x)를 맞추는 과정에서 모델이

624
00:24:23,180 --> 00:24:25,960
유용한 특징 표현을 학습하기를 바랍니다.

625
00:24:25,960 --> 00:24:27,840
하지만 일반적으로 이러한

626
00:24:27,840 --> 00:24:29,680
모델은 자기 감독 학습에 대해

627
00:24:29,680 --> 00:24:30,960
그다지 성공적이지

628
00:24:30,960 --> 00:24:32,240
않았으며, 이전

629
00:24:32,240 --> 00:24:34,280
강의에서 이야기한 대조 방법이

630
00:24:34,280 --> 00:24:36,280
실제로는 무조건 밀도 추정보다

631
00:24:36,280 --> 00:24:39,560
자기 감독 학습에 훨씬 더 성공적이었습니다.

632
00:24:39,560 --> 00:24:42,320
원칙적으로는 이 무조건 생성 모델을 사용하여

633
00:24:42,320 --> 00:24:44,800
샘플링하고 새로운 샘플 x를 생성할 수 있습니다.

634
00:24:44,800 --> 00:24:46,960
하지만 이는 샘플링되는 것에 대한 제어가

635
00:24:46,960 --> 00:24:49,960
없기 때문에 실제로는 쓸모가 없다고 생각합니다.

636
00:24:49,960 --> 00:24:52,300
이미지의 무조건 생성 모델이 있다면, 이를

637
00:24:52,300 --> 00:24:54,258
샘플링하여 새로운 이미지를 얻을 수 있지만,

638
00:24:54,258 --> 00:24:57,040
그 이미지에 무엇이 들어 있는지에 대한 제어는 없습니다.

639
00:24:57,040 --> 00:25:00,280
따라서 이러한 모델을 구축하는 방법에 대해 수학적으로

640
00:25:00,280 --> 00:25:02,160
생각하는 것은 흥미롭지만,

641
00:25:02,160 --> 00:25:05,200
실용적인 의미는 크지 않다고 생각합니다.

642
00:25:05,200 --> 00:25:08,160
조건부 생성 모델은 가장 유용하고 흥미로운

643
00:25:08,160 --> 00:25:10,098
모델이라고 생각합니다.

644
00:25:10,098 --> 00:25:12,640
이 모델들은 실제로 훈련되고 사용되는

645
00:25:12,640 --> 00:25:14,840
생성 모델 중 가장 많습니다.

646
00:25:14,840 --> 00:25:17,120
따라서 원칙적으로는 이상치를 거부하면서 레이블을

647
00:25:17,120 --> 00:25:18,660
할당하는 데 사용할 수 있습니다.

648
00:25:18,660 --> 00:25:21,420
데이터 x가 있다면, 가능한

649
00:25:21,420 --> 00:25:25,280
모든 y에 대해 p(x|y)를 살펴보고,

650
00:25:25,280 --> 00:25:28,200
그 값이 너무 낮으면 거부할

651
00:25:28,200 --> 00:25:29,500
수 있습니다.

652
00:25:29,500 --> 00:25:32,080
따라서 원칙적으로는 조건부 생성 모델을

653
00:25:32,080 --> 00:25:35,000
사용하여 분류를 수행하면서 이상치를

654
00:25:35,000 --> 00:25:36,380
거부할 수 있습니다.

655
00:25:36,380 --> 00:25:37,880
실제로는 그렇게 많이

656
00:25:37,880 --> 00:25:39,640
사용되지 않는다고 생각합니다.

657
00:25:39,640 --> 00:25:42,520
조건부 생성 모델에 정말 유용하고 실제로 항상

658
00:25:42,520 --> 00:25:44,540
사용되는 것은 레이블에서 새로운

659
00:25:44,540 --> 00:25:46,800
데이터를 생성하기 위한 샘플링입니다.

660
00:25:46,800 --> 00:25:48,360
여기서 실제로 생성되는

661
00:25:48,360 --> 00:25:50,200
것을 제어할 수 있습니다.

662
00:25:50,200 --> 00:25:52,600
예를 들어, y가 텍스트 조각이라면,

663
00:25:52,600 --> 00:25:54,240
"나는 달에서 핫도그 맛의

664
00:25:54,240 --> 00:25:58,838
티셔츠를 입은 고양이를 보고 싶다"라고 쓸 수 있고, 그러면 당신이

665
00:25:58,838 --> 00:26:00,880
좋아하는 이미지 생성 모델이 그

666
00:26:00,880 --> 00:26:03,360
레이블 y에 조건화된 새로운 이미지 x를

667
00:26:03,360 --> 00:26:04,513
생성할 것입니다.

668
00:26:04,513 --> 00:26:07,180
그래서 여기서 모든 재미와

669
00:26:07,180 --> 00:26:09,440
마법이 있다고 생각합니다.

670
00:26:09,440 --> 00:26:11,700
문헌에서는 생성 모델이라는 용어를

671
00:26:11,700 --> 00:26:13,920
볼 때 무조건적 및 조건부 생성

672
00:26:13,920 --> 00:26:16,960
모델링을 혼합하는 경우가 있어 다소 혼란스러울

673
00:26:16,960 --> 00:26:17,898
수 있습니다.

674
00:26:17,898 --> 00:26:19,440
읽는 많은 논문에서는

675
00:26:19,440 --> 00:26:21,718
수학을 더 깔끔하게 보이게

676
00:26:21,718 --> 00:26:23,760
하기 위해 조건 신호 y를

677
00:26:23,760 --> 00:26:25,360
생략하기도 합니다.

678
00:26:25,360 --> 00:26:27,200
하지만 무조건적 생성 모델링은 그리

679
00:26:27,200 --> 00:26:28,437
유용하지 않다고 생각합니다.

680
00:26:28,437 --> 00:26:30,520
대부분의 경우, 실제로 하고 싶은

681
00:26:30,520 --> 00:26:33,200
것은 거의 항상 조건부 생성 모델링입니다.

682
00:26:33,200 --> 00:26:37,140
그래서 논문을 읽거나, 방정식을 보거나, 사람들이 이야기할 때,

683
00:26:37,140 --> 00:26:39,880
그들이 생성 모델링에 대해 이야기할 때,

684
00:26:39,880 --> 00:26:42,400
그들이 더 훈련에 신경 쓰는 것은 조건부

685
00:26:42,400 --> 00:26:44,200
생성 모델일 가능성이

686
00:26:44,200 --> 00:26:46,720
높습니다. 수학이나 방정식, 표기법이 이를

687
00:26:46,720 --> 00:26:47,920
반영하지 않더라도요.

688
00:26:47,920 --> 00:26:50,880
무조건적 생성 모델의

689
00:26:50,880 --> 00:26:53,920
입력과 출력은 무엇인가요?

690
00:26:53,920 --> 00:26:55,680
그에 대해 정말로 말씀드리지 않았습니다.

691
00:26:55,680 --> 00:26:58,800
그것을 매개변수화하는 방법은 실제로

692
00:26:58,800 --> 00:27:00,060
많이 달라집니다.

693
00:27:00,060 --> 00:27:01,280
이 모든 것에 대한

694
00:27:01,280 --> 00:27:02,420
다양한 공식이 있습니다.

695
00:27:02,420 --> 00:27:04,360
네트워크의 입력과 출력이 정확히

696
00:27:04,360 --> 00:27:06,610
무엇인지도 공식에 따라 많이

697
00:27:06,610 --> 00:27:07,520
달라질 것입니다.

698
00:27:07,520 --> 00:27:09,480
몇 슬라이드 후에 그것에 대한 전체 분류법에 대해

699
00:27:09,480 --> 00:27:10,272
이야기할 것입니다.

700
00:27:12,786 --> 00:27:15,160
그렇다면 왜 생성 모델인가요?

701
00:27:15,160 --> 00:27:17,400
생성 모델을 구축하려는 주된

702
00:27:17,400 --> 00:27:19,840
이유는 모델링하려는 작업에

703
00:27:19,840 --> 00:27:21,440
모호성이 있을 때입니다.

704
00:27:21,440 --> 00:27:25,400
y가 주어졌을 때 x의 확률 모델 p의 아름다움은 그것이

705
00:27:25,400 --> 00:27:27,150
확률적이라는 것입니다.

706
00:27:27,150 --> 00:27:30,630
입력 레이블 y에 조건화된 가능한 출력 x의 전체

707
00:27:30,630 --> 00:27:32,150
공간이 있을 수 있습니다.

708
00:27:32,150 --> 00:27:34,612
때로는 결정론적 매핑만

709
00:27:34,612 --> 00:27:36,570
있을 수 있습니다.

710
00:27:36,570 --> 00:27:37,790
나는 이미지를 보고, 그 이미지에

711
00:27:37,790 --> 00:27:39,090
고양이가 몇 마리 있는지 묻고 싶습니다.

712
00:27:39,090 --> 00:27:40,090
고양이는 세 마리입니다.

713
00:27:40,090 --> 00:27:41,270
정확한 답은 하나입니다.

714
00:27:41,270 --> 00:27:43,130
하지만 많은 경우, 더 미묘합니다.

715
00:27:43,130 --> 00:27:46,615
핫도그 모자를 쓴 개의 사진을 요청하면, 그 쿼리에

716
00:27:46,615 --> 00:27:47,990
따라 존재할 수 있는

717
00:27:47,990 --> 00:27:49,870
다양한 이미지가 많습니다.

718
00:27:49,870 --> 00:27:51,362
출력에 불확실성이 있습니다.

719
00:27:51,362 --> 00:27:53,070
그리고 그것이 바로 생성 모델이 모델링하려고

720
00:27:53,070 --> 00:27:53,903
하는 것입니다.

721
00:27:53,903 --> 00:27:56,153
그들은 입력 신호에 조건화된 출력의 전체

722
00:27:56,153 --> 00:27:57,110
분포를 모델링합니다.

723
00:27:57,110 --> 00:27:59,910
입력에 조건화된 모델이 생성해야 하는

724
00:27:59,910 --> 00:28:01,870
출력에 모호성이 있을

725
00:28:01,870 --> 00:28:03,328
때마다, 생성 모델로

726
00:28:03,328 --> 00:28:04,910
전환하고 싶습니다.

727
00:28:04,910 --> 00:28:05,690
이것이 바로--

728
00:28:05,690 --> 00:28:08,230
그리고 이것은-- 지난 몇 년 동안

729
00:28:08,230 --> 00:28:10,870
많이 사용된 몇 가지 예를 볼 것입니다.

730
00:28:10,870 --> 00:28:12,590
하나의 예는 언어 모델링입니다.

731
00:28:12,590 --> 00:28:14,498
누군가 방금 ChatGPT에 대해 물었습니다.

732
00:28:14,498 --> 00:28:16,790
언어 모델링에서는 입력 텍스트

733
00:28:16,790 --> 00:28:20,750
y로부터 출력 텍스트 x를 예측하려고 합니다.

734
00:28:20,750 --> 00:28:23,870
죄송하지만, 이 예제에서 x와

735
00:28:23,870 --> 00:28:26,950
y가 어색하게 뒤바뀌었습니다.

736
00:28:26,950 --> 00:28:29,230
여기 ChatGPT의 예가 있습니다.

737
00:28:29,230 --> 00:28:30,830
입력은 생성 모델에 대한 짧은

738
00:28:30,830 --> 00:28:32,370
운율 시를 써 달라는 것입니다.

739
00:28:32,370 --> 00:28:34,067
와우, 실제로 작동하네요.

740
00:28:34,067 --> 00:28:34,650
이건 미쳤어요.

741
00:28:34,650 --> 00:28:37,248
이 수업을 처음 가르칠 때는 전혀 작동하지 않았습니다.

742
00:28:37,248 --> 00:28:38,290
읽지 않겠습니다.

743
00:28:38,290 --> 00:28:39,415
그건 창피할 것입니다.

744
00:28:39,415 --> 00:28:41,097
당신이 직접 읽어보세요.

745
00:28:41,097 --> 00:28:42,930
이것은 조건부 생성 모델입니다.

746
00:28:42,930 --> 00:28:45,472
생성 모델에 대한 다양한 운율

747
00:28:45,472 --> 00:28:48,550
시가 있을 수 있으며, 그 중 하나를

748
00:28:48,550 --> 00:28:49,842
선택해야 했습니다.

749
00:28:49,842 --> 00:28:51,342
생성 모델의 아름다움은

750
00:28:51,342 --> 00:28:53,750
원칙적으로 그 입력에 조건화된 가능한

751
00:28:53,750 --> 00:28:57,070
출력에 대한 전체 분포를 모델링한다는 것입니다.

752
00:28:57,070 --> 00:28:58,455
또는 텍스트에서 이미지로.

753
00:28:58,455 --> 00:28:59,830
화이트보드 앞에서 생성

754
00:28:59,830 --> 00:29:02,590
모델을 가르치는 사람의 이미지를 만들어 주세요.

755
00:29:02,590 --> 00:29:04,770
당신은 당신의 눈을 통해 하나의 예를 보고 있습니다.

756
00:29:04,770 --> 00:29:06,590
ChatGPT는 다른 예를 제공했습니다.

757
00:29:06,590 --> 00:29:08,830
이 입력 텍스트와 일치할 수 있는

758
00:29:08,830 --> 00:29:11,255
가능한 이미지의 전체 공간이 있습니다.

759
00:29:11,255 --> 00:29:12,630
생성 모델은 그 전체

760
00:29:12,630 --> 00:29:14,797
공간을 모델링하고 원하는 것에

761
00:29:14,797 --> 00:29:17,070
따라 샘플링할 수 있게 해줍니다.

762
00:29:17,070 --> 00:29:18,590
또는 이미지에서 비디오로.

763
00:29:18,590 --> 00:29:21,110
이미지를 입력하면 다음에 무슨 일이 일어날까요?

764
00:29:21,110 --> 00:29:24,170
이건 제가 종이 상자 위에 AirPods를 들고 있는 모습입니다.

765
00:29:24,170 --> 00:29:25,330
아마도 제가 그것을 떨어뜨릴 것입니다.

766
00:29:25,330 --> 00:29:26,730
아마도 제가 손을 움직일 것입니다.

767
00:29:26,730 --> 00:29:29,310
아마도 제가 손을 움직이면 AirPods가 다른

768
00:29:29,310 --> 00:29:31,228
AirPods로 변형될 것입니다.

769
00:29:31,228 --> 00:29:32,770
일어날 수 있는 모든 일이 있습니다.

770
00:29:32,770 --> 00:29:34,010
원칙적으로 생성 모델은 이러한

771
00:29:34,010 --> 00:29:36,260
가능한 미래를 모델링하고 샘플링할 수 있게 해줍니다.

772
00:29:39,910 --> 00:29:42,392
그래서 이것이 우리가 생성 모델링에 관심을 가져야 하는 이유입니다.

773
00:29:42,392 --> 00:29:44,350
출력에 모호성이 있을

774
00:29:44,350 --> 00:29:48,430
때마다 생성 모델을 사용하여 해결하려고 해야 합니다.

775
00:29:48,430 --> 00:29:50,770
누군가 입력과 출력이 무엇인지 물었습니다.

776
00:29:50,770 --> 00:29:52,790
이것은 거대한 분야입니다.

777
00:29:52,790 --> 00:29:55,270
그리고 이는 확률 분포를 모델링하는

778
00:29:55,270 --> 00:29:57,630
다양한 방법에 대해 생각해야 하기

779
00:29:57,630 --> 00:29:59,590
때문에 놀랍게도 꽤 수학적인

780
00:29:59,590 --> 00:30:01,130
딥러닝 분야입니다.

781
00:30:01,130 --> 00:30:02,630
우리는 올바른 일이 일어나게

782
00:30:02,630 --> 00:30:05,010
하는 손실 함수를 어떻게 작성할 수 있을까요?

783
00:30:05,010 --> 00:30:06,590
그래서 논문을 읽을 때

784
00:30:06,590 --> 00:30:08,410
수학과 방정식이 많을 수

785
00:30:08,410 --> 00:30:09,830
있는 한 분야입니다.

786
00:30:09,830 --> 00:30:12,670
실제로 무슨 일이 일어나고 있는지 이해하기 위해 그 방정식을

787
00:30:12,670 --> 00:30:14,730
꽤 신중하게 생각해야 할 수도 있습니다.

788
00:30:14,730 --> 00:30:17,470
그래서 이 분야는 더 많은 수학과 방정식을 포함하는

789
00:30:17,470 --> 00:30:19,910
경향이 있으며, 저는 그게 재미있고 흥미롭다고

790
00:30:19,910 --> 00:30:20,788
생각합니다.

791
00:30:20,788 --> 00:30:22,830
사람들이 만드는 다양한 생성

792
00:30:22,830 --> 00:30:24,750
모델의 전체 분류가 있습니다.

793
00:30:24,750 --> 00:30:26,510
한편으로는 우리가 명시적

794
00:30:26,510 --> 00:30:30,110
밀도 방법이라고 부르는 가족 나무의 한 부분을 상상할

795
00:30:30,110 --> 00:30:30,930
수 있습니다.

796
00:30:30,930 --> 00:30:33,163
이것들은 모델이 실제로 p(x) 또는

797
00:30:33,163 --> 00:30:34,830
y가 주어졌을 때 p(x)를

798
00:30:34,830 --> 00:30:36,750
모델링하려고 하는 것입니다.

799
00:30:36,750 --> 00:30:38,590
이 명시적 밀도 방법을 사용하면 실제로

800
00:30:38,590 --> 00:30:40,910
p(x) 값을 계산할 수 있으며, 어떤

801
00:30:40,910 --> 00:30:43,350
샘플 x에 대해서도 그 값을 얻을 수 있습니다.

802
00:30:43,350 --> 00:30:46,565
그리고 반대편은 암시적 밀도 방법입니다.

803
00:30:46,565 --> 00:30:48,190
이것들은 모델에서

804
00:30:48,190 --> 00:30:50,470
그 확률 질량 값을 실제로

805
00:30:50,470 --> 00:30:53,290
얻을 수는 없지만, 그 확률

806
00:30:53,290 --> 00:30:56,650
분포에서 샘플링할 수 있는 것들입니다.

807
00:30:56,650 --> 00:31:00,150
여기서의 차이점은 암시적 모델에서는 밀도 함수의

808
00:31:00,150 --> 00:31:03,418
값을 실제로 접근할 수 없지만, 기본 밀도

809
00:31:03,418 --> 00:31:05,710
함수에서 샘플링할 수 있다는

810
00:31:05,710 --> 00:31:06,290
것입니다.

811
00:31:06,290 --> 00:31:07,790
모델은 값을 얻을 수 없더라도

812
00:31:07,790 --> 00:31:10,590
밀도를 모델링하는 것을 암시적으로 학습했습니다.

813
00:31:10,590 --> 00:31:12,550
명시적 밀도 측면에서는

814
00:31:12,550 --> 00:31:15,870
많은 경우에 명시적 밀도 값을 얻을 수

815
00:31:15,870 --> 00:31:19,057
있지만, 샘플링은 때때로 이러한 명시적

816
00:31:19,057 --> 00:31:21,390
밀도 방법으로 더 복잡해지는

817
00:31:21,390 --> 00:31:23,090
경향이 있습니다.

818
00:31:23,090 --> 00:31:25,270
항상 그런 것은 아니지만, 때때로 그렇습니다.

819
00:31:25,270 --> 00:31:27,670
암시적 모델로 전환할 수 있는 이유는

820
00:31:27,670 --> 00:31:29,830
많은 경우에 어떤 입력의 밀도 값이

821
00:31:29,830 --> 00:31:32,510
정확히 무엇인지 아는 것에 대해 신경 쓰지

822
00:31:32,510 --> 00:31:33,950
않을 수 있기 때문입니다.

823
00:31:33,950 --> 00:31:35,990
어쩌면 당신이 신경 쓰는 것은

824
00:31:35,990 --> 00:31:39,967
샘플을 생성하고 좋은 다양성의 샘플을 생성하는 것뿐일 수 있습니다.

825
00:31:39,967 --> 00:31:42,050
그래서 당신이 정말로 신경 쓰는

826
00:31:42,050 --> 00:31:43,467
것이 샘플링이라면,

827
00:31:43,467 --> 00:31:45,230
아마도 어떤 입력에 대한

828
00:31:45,230 --> 00:31:48,270
밀도의 값을 명시적으로 볼 필요는 없을 것입니다.

829
00:31:48,270 --> 00:31:50,270
그리고 여기서부터 상황이 무너지고

830
00:31:50,270 --> 00:31:52,110
연쇄적으로 더 프랙탈처럼 됩니다.

831
00:31:52,110 --> 00:31:54,395
명시적 밀도 방법 내부에는 실제로

832
00:31:54,395 --> 00:31:55,770
모델링되는 p(x)를

833
00:31:55,770 --> 00:31:58,630
계산할 수 있는 것들이 있습니다.

834
00:31:58,630 --> 00:32:01,710
자기 회귀 모델은 그 예 중 하나입니다.

835
00:32:01,710 --> 00:32:04,590
또 다른 버전의 명시적 밀도 방법은

836
00:32:04,590 --> 00:32:07,087
밀도 값을 얻을 수 있지만, 실제

837
00:32:07,087 --> 00:32:08,170
값은 아닙니다.

838
00:32:08,170 --> 00:32:11,710
그것은 데이터의 진정한 밀도에 대한 근사치입니다.

839
00:32:11,710 --> 00:32:13,870
변분 오토인코더는 우리가

840
00:32:13,870 --> 00:32:16,630
볼 명시적이지만 근사적인 생성

841
00:32:16,630 --> 00:32:18,190
방법의 한 예입니다.

842
00:32:18,190 --> 00:32:20,830
이제 가족 나무의 다른 가지에서는 암시적 밀도에

843
00:32:20,830 --> 00:32:23,338
대한 직접적인 방법을 생각할 수 있습니다.

844
00:32:23,338 --> 00:32:25,630
이것들은 모델링되는 기본 분포에서

845
00:32:25,630 --> 00:32:30,110
샘플을 그리기 위해 단일 네트워크 평가가 필요할 수 있는

846
00:32:30,110 --> 00:32:31,090
것들입니다.

847
00:32:31,090 --> 00:32:32,590
생성적 적대 신경망은 이

848
00:32:32,590 --> 00:32:35,173
가족 나무의 이 부분에 있는 생성 모델의

849
00:32:35,173 --> 00:32:35,870
예입니다.

850
00:32:35,870 --> 00:32:36,910
그리고 다른 부분은--

851
00:32:36,910 --> 00:32:39,705
좋은 이름이 있는지 모르겠지만, 저는 그것을 간접적이라고 부릅니다.

852
00:32:39,705 --> 00:32:41,330
하지만 이것은 제가 어제 만든

853
00:32:41,330 --> 00:32:43,390
이름이므로, 더 나은 용어가 있다면

854
00:32:43,390 --> 00:32:44,830
자유롭게 수정해 주세요.

855
00:32:44,830 --> 00:32:46,950
하지만 이 간접적인 것들은

856
00:32:46,950 --> 00:32:50,910
모델링되는 기본 밀도 p(x)에서 샘플링할 수

857
00:32:50,910 --> 00:32:52,750
있지만, 어떤 반복

858
00:32:52,750 --> 00:32:54,330
절차가 필요합니다.

859
00:32:54,330 --> 00:32:57,310
입력하고 샘플을 직접 얻을 수 있는 피드 포워드

860
00:32:57,310 --> 00:32:58,310
함수는 없습니다.

861
00:32:58,310 --> 00:32:59,893
모델링되는 기본 밀도에서

862
00:32:59,893 --> 00:33:01,590
샘플을 그리기 위해 실행해야

863
00:33:01,590 --> 00:33:03,652
하는 반복적인 방법이 있습니다.

864
00:33:03,652 --> 00:33:05,110
확산 모델은 우리가

865
00:33:05,110 --> 00:33:06,680
다음 시간에 볼 예입니다.

866
00:33:06,680 --> 00:33:08,430
몇 슬라이드 전에 사람들이 표기법에 대해

867
00:33:08,430 --> 00:33:10,207
부주의하고 y를 생략한다고 말했으며, 이

868
00:33:10,207 --> 00:33:12,290
슬라이드에서 의도적으로 그렇게 했습니다. 그래서

869
00:33:12,290 --> 00:33:13,730
누군가 그 질문을 하게 될 것이고,

870
00:33:13,730 --> 00:33:15,750
여러분은 항상 그 사실에 주의를 기울일 것입니다.

871
00:33:15,750 --> 00:33:16,677
그래서 맞습니다.

872
00:33:16,677 --> 00:33:18,510
이 슬라이드와 이번 강의의 나머지

873
00:33:18,510 --> 00:33:21,040
슬라이드에서 p of x를 쓸 때마다, 저도

874
00:33:21,040 --> 00:33:22,960
게으르게 y를 생략했지만, 여러분은

875
00:33:22,960 --> 00:33:25,740
이번 강의에서 보는 모든 p of x에 대해

876
00:33:25,740 --> 00:33:28,460
y에 조건을 추가해야 한다고 항상 상상해야 합니다.

877
00:33:28,460 --> 00:33:30,300
그래서 질문해 주셔서 감사합니다.

878
00:33:30,300 --> 00:33:34,180
질문은 그 간접 반복 절차를 블랙 박스로

879
00:33:34,180 --> 00:33:36,700
취급하고 이를 직접 샘플링 방법으로

880
00:33:36,700 --> 00:33:39,580
취급할 수 있냐는 것이었습니다.

881
00:33:39,580 --> 00:33:40,800
원칙적으로는 그렇습니다.

882
00:33:40,800 --> 00:33:43,660
하지만 실제로는 그렇지 않습니다. 왜냐하면

883
00:33:43,660 --> 00:33:47,600
샘플이 정확히 그 방법에 따라 근사적으로 끝나기 때문입니다.

884
00:33:47,600 --> 00:33:49,713
하지만 확산 모델을 사용하면

885
00:33:49,713 --> 00:33:51,380
진정한 샘플을 얻기 위해

886
00:33:51,380 --> 00:33:54,020
무한한 단계를 거쳐야 하므로, 대신 유한한

887
00:33:54,020 --> 00:33:55,520
단계로 근사합니다.

888
00:33:55,520 --> 00:33:57,360
다른 방법에도 해당됩니다.

889
00:33:57,360 --> 00:33:59,940
현재 확산 모델이 가장 일반적입니다.

890
00:33:59,940 --> 00:34:03,340
하지만 과거 몇 년 동안의 마르코프 체인 방법이나 MCMC

891
00:34:03,340 --> 00:34:05,220
방법도 반복 절차가 있는 이 속성을

892
00:34:05,220 --> 00:34:06,520
가졌을 수 있습니다.

893
00:34:06,520 --> 00:34:08,699
그러나 모델링되는 분포에서 정확한 샘플을

894
00:34:08,699 --> 00:34:10,540
얻으려면 수렴하기 위해 무한한

895
00:34:10,540 --> 00:34:11,580
단계가 필요합니다.

896
00:34:11,580 --> 00:34:16,820
그래서 우리는 항상 유한한 단계를 거쳐 근사합니다.

897
00:34:16,820 --> 00:34:20,380
이 분류법이 매우 대칭적이어서

898
00:34:20,380 --> 00:34:22,159
자랑스럽습니다.

899
00:34:22,159 --> 00:34:25,360
네 개의 잎이 있습니다.

900
00:34:25,360 --> 00:34:26,397
두 개의 가지가 있습니다.

901
00:34:26,397 --> 00:34:28,980
오늘은 나무의 절반을 다루고 다음 시간에 나무의 절반을

902
00:34:28,980 --> 00:34:29,860
다룰 것입니다.

903
00:34:29,860 --> 00:34:32,277
그래서 저는 이것이 꽤 멋진 분해라고 생각했습니다.

904
00:34:32,277 --> 00:34:33,860
질문은 근사 밀도와

905
00:34:33,860 --> 00:34:36,739
암묵적인 p of x에서 직접 샘플링하는

906
00:34:36,739 --> 00:34:38,659
것의 차이가 무엇인가입니다.

907
00:34:38,659 --> 00:34:42,040
차이는 간접적이지만 암묵적인 방법에서는 어디에서도 밀도

908
00:34:42,040 --> 00:34:44,102
값을 찾을 수 없다는 것입니다.

909
00:34:44,102 --> 00:34:46,060
하나도 계산할 수 없지만, 여전히 어떤

910
00:34:46,060 --> 00:34:47,900
방식으로 반복적으로 샘플링할 수 있습니다.

911
00:34:47,900 --> 00:34:49,677
근사 밀도 방법을 사용하면 값을

912
00:34:49,677 --> 00:34:52,260
얻을 수 있습니다. 실제로는 진정한 p of

913
00:34:52,260 --> 00:34:54,552
x에 대한 근사치나 경계가 될 밀도

914
00:34:54,552 --> 00:34:55,760
값을 얻을 수 있습니다.

915
00:34:59,020 --> 00:35:01,583
그래서 우리가 실제로 조금

916
00:35:01,583 --> 00:35:03,500
더 구체적으로 이야기할

917
00:35:03,500 --> 00:35:07,632
첫 번째 생성 모델은 자기 회귀 모델입니다.

918
00:35:07,632 --> 00:35:09,340
자기 회귀 모델에 대해--

919
00:35:09,340 --> 00:35:10,923
우리는 실제로 약간의

920
00:35:10,923 --> 00:35:13,203
우회로를 통해 모든 생성 모델링의

921
00:35:13,203 --> 00:35:14,620
일반적인 아이디어인 최대

922
00:35:14,620 --> 00:35:17,060
우도 추정에 대해 이야기할 것입니다.

923
00:35:17,060 --> 00:35:18,598
최대 우도 추정은 유한한

924
00:35:18,598 --> 00:35:20,140
샘플 집합을 주어진

925
00:35:20,140 --> 00:35:22,620
확률 모델을 맞추기 위해 사용할 수 있는

926
00:35:22,620 --> 00:35:24,153
꽤 일반적인 절차입니다.

927
00:35:24,153 --> 00:35:25,820
아이디어는 밀도를 위한

928
00:35:25,820 --> 00:35:28,060
명시적 함수를 작성하는 것입니다.

929
00:35:28,060 --> 00:35:32,060
일부 방법은 밀도를 명시적으로 모델링할

930
00:35:32,060 --> 00:35:32,880
것입니다.

931
00:35:32,880 --> 00:35:34,680
그럼 신경망으로 해봅시다.

932
00:35:34,680 --> 00:35:36,138
데이터 x를

933
00:35:36,138 --> 00:35:39,780
입력하고 신경망의 가중치 W를 입력하여

934
00:35:39,780 --> 00:35:41,235
밀도를

935
00:35:41,235 --> 00:35:42,860
알려주는 숫자를

936
00:35:42,860 --> 00:35:45,540
출력하는 신경망을 작성합시다.

937
00:35:45,540 --> 00:35:47,820
그런 다음, 샘플 x1, x2,

938
00:35:47,820 --> 00:35:50,460
xN의 데이터 세트를 주어진 모델을

939
00:35:50,460 --> 00:35:53,360
이 목표 함수에 따라 훈련할 것입니다.

940
00:35:53,360 --> 00:35:58,338
우리는 데이터 세트를 가장 가능성이 높은 가중치를 찾고자 합니다.

941
00:35:58,338 --> 00:36:00,880
우리가 원하는 것은 [?] 수락하시겠습니까? 가중치를 변화시키면 네트워크가

942
00:36:00,880 --> 00:36:02,660
모델링하는 밀도가

943
00:36:02,660 --> 00:36:04,000
변화하기 때문입니다.

944
00:36:04,000 --> 00:36:07,580
그래서 우리는 네트워크가 데이터의 가능성을 극대화하는

945
00:36:07,580 --> 00:36:09,420
밀도를 선택하기를 원합니다.

946
00:36:09,420 --> 00:36:12,140
우리가 가능성(likelihood)이라고 말한 점에 주목하세요.

947
00:36:12,140 --> 00:36:15,820
확률(probability)이라는 단어 대신에요. 이는 깊은 철학적 논의로
빠질 수 있습니다.

948
00:36:15,820 --> 00:36:17,420
차이는 우리가 변화시키는 것입니다.

949
00:36:17,420 --> 00:36:18,878
확률에 대해 생각하면

950
00:36:18,878 --> 00:36:21,020
밀도가 고정되어 있고 x를

951
00:36:21,020 --> 00:36:23,160
이동시키며 고정된 분포에서 x의

952
00:36:23,160 --> 00:36:25,760
확률이 어떻게 변화하는지를 상상합니다.

953
00:36:25,760 --> 00:36:27,860
반면 가능성에 대해

954
00:36:27,860 --> 00:36:29,940
이야기할 때는 샘플 x를

955
00:36:29,940 --> 00:36:34,004
고정하고 분포 자체를 변화시키며, 다양한 분포를

956
00:36:34,004 --> 00:36:37,100
변화시킬 때 그 샘플의 확률 밀도가

957
00:36:37,100 --> 00:36:39,512
어떻게 변화하는지를 말합니다.

958
00:36:39,512 --> 00:36:41,220
따라서 이 방정식에서 무엇이 고정되고

959
00:36:41,220 --> 00:36:43,660
무엇이 변화하는지를 매우 신중하게 생각해야 합니다.

960
00:36:43,660 --> 00:36:46,460
최대 가능성 추정(maximum likelihood

961
00:36:46,460 --> 00:36:49,340
estimation) 과정에서 우리가

962
00:36:49,340 --> 00:36:52,580
하는 것은 신경망이 모델링하는 분포를 변화시켜서 훈련

963
00:36:52,580 --> 00:36:54,380
세트에서 그 분포로부터 고정된

964
00:36:54,380 --> 00:36:57,280
샘플 집합의 확률을 극대화하려고 하는 것입니다.

965
00:36:57,280 --> 00:36:59,380
그리고 이 모든 것 뒤에 말하지

966
00:36:59,380 --> 00:37:02,660
않은 것은 우리가 우주가 우리가 보고 있는 데이터를

967
00:37:02,660 --> 00:37:05,900
생성하는 데 사용한 어떤 기본적인 진짜 확률 분포 p

968
00:37:05,900 --> 00:37:08,260
data가 있다고 가정한다는 것입니다.

969
00:37:08,260 --> 00:37:10,340
어떤 의미에서 우리는 항상 그

970
00:37:10,340 --> 00:37:13,500
진짜 기본 미지의 분포 p data를 모델링하려고

971
00:37:13,500 --> 00:37:14,360
합니다.

972
00:37:14,360 --> 00:37:16,820
우리는 우주가 우리에게

973
00:37:16,820 --> 00:37:19,340
준 p data의 샘플을

974
00:37:19,340 --> 00:37:22,940
가지고 있기 때문에 p data에

975
00:37:22,940 --> 00:37:24,880
접근할 수 없습니다.

976
00:37:24,880 --> 00:37:27,340
우리가 학습 절차를 통해 하려는 것은 그

977
00:37:27,340 --> 00:37:29,515
미지의 분포에서 유한한 수의 샘플을

978
00:37:29,515 --> 00:37:31,900
주어진 상태에서 그 미지의 분포 p

979
00:37:31,900 --> 00:37:33,740
data를 밝혀내는 것입니다.

980
00:37:33,740 --> 00:37:35,900
이 문제를 해결하는 한 가지

981
00:37:35,900 --> 00:37:38,340
절차는, 내가 본 데이터를

982
00:37:38,340 --> 00:37:41,180
실제로 가장 가능성이 높은 분포를

983
00:37:41,180 --> 00:37:45,645
선택하자는 것입니다. 이것이 최대 가능성 목표 함수입니다.

984
00:37:45,645 --> 00:37:47,020
그리고 여기서 우리가 사용하는

985
00:37:47,020 --> 00:37:50,140
일반적인 트릭은 데이터가 IID, 독립적이고 동일하게 분포되어

986
00:37:50,140 --> 00:37:51,440
있다고 가정하는 것입니다.

987
00:37:51,440 --> 00:37:53,460
그래서 우리는 각 x가 그 진짜 p

988
00:37:53,460 --> 00:37:55,417
data 분포에서 추출되었다고 가정합니다.

989
00:37:55,417 --> 00:37:57,500
이제 우리가 본 모든 데이터의 결합 분포를

990
00:37:57,500 --> 00:37:58,800
극대화하고자 합니다.

991
00:37:58,800 --> 00:38:01,300
하지만 독립적이기 때문에 각

992
00:38:01,300 --> 00:38:05,220
독립 샘플의 독립 가능성으로 분해할 수 있습니다.

993
00:38:05,220 --> 00:38:07,880
그리고 우리가 항상 사용하는 일반적인 트릭은 로그 트릭입니다.

994
00:38:07,880 --> 00:38:10,040
로그는 단조 함수입니다.

995
00:38:10,040 --> 00:38:12,700
무언가를 극대화하면, 그것의 로그를 극대화하는

996
00:38:12,700 --> 00:38:15,300
것과 동등합니다. 왜냐하면 로그는 단조

997
00:38:15,300 --> 00:38:16,642
함수이기 때문입니다.

998
00:38:16,642 --> 00:38:18,100
로그는 또한 합과 곱을

999
00:38:18,100 --> 00:38:20,240
바꿔주기 때문에 매우 편리합니다.

1000
00:38:20,240 --> 00:38:21,100
그래서

1001
00:38:21,100 --> 00:38:25,695
데이터의 가능성을 극대화하는 대신 데이터의 로그 가능성을

1002
00:38:25,695 --> 00:38:28,320
극대화할 것이며, 이는 가능성을

1003
00:38:28,320 --> 00:38:30,362
극대화하는 것과 동일합니다.

1004
00:38:30,362 --> 00:38:33,880
로그를 적용하면 그 곱이 합으로 분리되고,

1005
00:38:33,880 --> 00:38:36,340
합은 처리하기가 더 쉽습니다.

1006
00:38:36,340 --> 00:38:38,660
이제 우리의 신경망을 삽입합니다. 왜냐하면

1007
00:38:38,660 --> 00:38:40,660
우리의 신경망이 이제 밀도를 직접 출력할

1008
00:38:40,660 --> 00:38:41,712
수 있기 때문입니다.

1009
00:38:41,712 --> 00:38:43,420
이것은 신경망을 훈련시키는 데

1010
00:38:43,420 --> 00:38:46,148
사용할 수 있는 직접적인 목표 함수를 제공합니다.

1011
00:38:46,148 --> 00:38:47,940
이것은 이 생성 모델링 문제를 해결하기

1012
00:38:47,940 --> 00:38:50,023
위해 신경망을 훈련시키는 데 사용할 수

1013
00:38:50,023 --> 00:38:52,147
있는 매우 구체적인 손실 함수를 제공합니다.

1014
00:38:52,147 --> 00:38:53,980
하지만 실제로 진행하기

1015
00:38:53,980 --> 00:38:56,900
위해서는 조금 더 구조가 필요합니다.

1016
00:38:56,900 --> 00:38:59,140
최대 가능성 추정의 이 아이디어는 매우

1017
00:38:59,140 --> 00:38:59,960
일반적입니다.

1018
00:38:59,960 --> 00:39:02,135
데이터에 대해 특별히 가정하지 않습니다.

1019
00:39:02,135 --> 00:39:04,260
데이터의 구조에 대해서도 특별히 가정하지 않습니다.

1020
00:39:04,260 --> 00:39:06,677
일반적으로 우리는 이 문제를 해결하기 위해

1021
00:39:06,677 --> 00:39:08,500
조금 더 구조를 추가해야 합니다.

1022
00:39:08,500 --> 00:39:11,060
오토회귀 모델은 기본적으로 데이터를

1023
00:39:11,060 --> 00:39:14,500
x로 보고 각 데이터 샘플 x를 일부 하위 부분

1024
00:39:14,500 --> 00:39:19,140
x1, x2, xT의 시퀀스로 나눌 수 있는 어떤 표준적인

1025
00:39:19,140 --> 00:39:20,820
방법이 있다고 가정합니다.

1026
00:39:20,820 --> 00:39:22,600
여기서 인덱스에 주의해야 합니다.

1027
00:39:22,600 --> 00:39:24,080
여기서 하위 부분이라고 말했습니다.

1028
00:39:24,080 --> 00:39:26,380
이들은 단일 샘플의 하위 부분입니다.

1029
00:39:26,380 --> 00:39:27,680
그래서 저는 아래 첨자를 사용합니다.

1030
00:39:27,680 --> 00:39:29,620
이전 슬라이드에서는 서로 다른 샘플

1031
00:39:29,620 --> 00:39:32,980
x1에서 xN까지를 나타내기 위해 위 첨자를 사용했습니다.

1032
00:39:32,980 --> 00:39:34,580
그러니 그 점에 주의하세요.

1033
00:39:34,580 --> 00:39:37,340
이 슬라이드의 위 첨자는 서로 다른 샘플 x입니다.

1034
00:39:37,340 --> 00:39:39,220
이 슬라이드의 아래 첨자는 동일한 샘플의

1035
00:39:39,220 --> 00:39:40,467
서로 다른 부분을 의미합니다.

1036
00:39:40,467 --> 00:39:42,300
그래서 우리는 데이터 샘플 x를

1037
00:39:42,300 --> 00:39:46,500
일부 하위 부분의 시퀀스로 나누는 어떤 표준적인 방법이 있다고 가정합니다.

1038
00:39:46,500 --> 00:39:48,820
이제 우리는 확률의 연쇄 법칙을 적용할 수 있습니다.

1039
00:39:48,820 --> 00:39:52,220
x의 확률은 x1에서 xT까지의

1040
00:39:52,220 --> 00:39:55,580
모든 하위 부분의 결합 확률입니다.

1041
00:39:55,580 --> 00:39:57,640
그리고 어떤 확률 분포가

1042
00:39:57,640 --> 00:40:00,180
주어지면, 항상 이 연쇄

1043
00:40:00,180 --> 00:40:03,640
법칙으로 나눌 수 있습니다. 모든 변수의 결합

1044
00:40:03,640 --> 00:40:06,220
분포는 첫 번째 변수의 확률과

1045
00:40:06,220 --> 00:40:08,480
첫 번째 변수에 조건부인

1046
00:40:08,480 --> 00:40:09,970
두 번째 변수의

1047
00:40:09,970 --> 00:40:12,230
확률, 첫 번째와 두 번째

1048
00:40:12,230 --> 00:40:13,730
변수에 조건부인 세

1049
00:40:13,730 --> 00:40:16,910
번째 변수의 확률 등으로 표현됩니다.

1050
00:40:16,910 --> 00:40:19,070
이것이 확률의 연쇄 법칙입니다.

1051
00:40:19,070 --> 00:40:20,310
이것은 어떤 가정도 필요하지 않습니다.

1052
00:40:20,310 --> 00:40:22,570
이는 모든 랜덤 변수의

1053
00:40:22,570 --> 00:40:24,970
결합 분포에 항상 적용됩니다.

1054
00:40:24,970 --> 00:40:28,510
그리고 이것이 우리의 목적 함수가 됩니다.

1055
00:40:28,510 --> 00:40:31,170
그럼 기본적으로 이전 시퀀스 부분을

1056
00:40:31,170 --> 00:40:35,228
입력으로 받아 다음 시퀀스 부분에 대한 확률

1057
00:40:35,228 --> 00:40:36,770
분포를 제공하려고 하는

1058
00:40:36,770 --> 00:40:39,290
신경망을 훈련할 수 있습니다.

1059
00:40:39,290 --> 00:40:40,610
익숙하게 들리나요?

1060
00:40:40,610 --> 00:40:43,170
이전에 했던 것처럼 들리나요?

1061
00:40:43,170 --> 00:40:44,310
RNN, 맞습니다.

1062
00:40:44,310 --> 00:40:46,330
그래서 RNN이 하는 일이 바로 그것입니다.

1063
00:40:46,330 --> 00:40:48,290
RNN은 숨겨진 상태를 시간에

1064
00:40:48,290 --> 00:40:51,930
따라 전달함으로써 매우 자연스러운 구조를 가지고 있으며,

1065
00:40:51,930 --> 00:40:54,770
숨겨진 상태는 항상 현재 지점까지의

1066
00:40:54,770 --> 00:40:56,250
시퀀스의 시작에 의존합니다.

1067
00:40:56,250 --> 00:40:57,930
그래서 RNN을 오토회귀

1068
00:40:57,930 --> 00:41:01,010
모델링에 사용하는 매우 자연스러운 방법이 있습니다.

1069
00:41:01,010 --> 00:41:02,930
당신은 기본적으로 시퀀스를 요약하는

1070
00:41:02,930 --> 00:41:05,510
숨겨진 상태의 시퀀스를 가지고 있습니다.

1071
00:41:05,510 --> 00:41:07,010
그리고 각 숨겨진

1072
00:41:07,010 --> 00:41:08,810
상태에서, 시퀀스의 모든 이전

1073
00:41:08,810 --> 00:41:11,850
부분에 조건부로 다음 시퀀스 조각의 확률을

1074
00:41:11,850 --> 00:41:13,850
예측합니다. 이것이 우리가

1075
00:41:13,850 --> 00:41:16,810
몇 강의 전에 본 RNN 언어 모델입니다.

1076
00:41:16,810 --> 00:41:19,370
이와 같은 다른 것을 본 적이 있나요?

1077
00:41:19,370 --> 00:41:20,930
네, 트랜스포머입니다.

1078
00:41:20,930 --> 00:41:23,690
특히 마스크 트랜스포머입니다.

1079
00:41:23,690 --> 00:41:25,850
트랜스포머 강의에서,

1080
00:41:25,850 --> 00:41:29,050
트랜스포머는 주의 행렬을 올바른 방식으로 마스킹하여

1081
00:41:29,050 --> 00:41:30,830
각 출력이 시퀀스의

1082
00:41:30,830 --> 00:41:32,810
접두사에만 의존하도록

1083
00:41:32,810 --> 00:41:35,230
하는 구조를 가질 수 있습니다.

1084
00:41:35,230 --> 00:41:38,590
그래서 우리는 오토회귀 모델링을 위해 트랜스포머를 사용할 수도 있습니다.

1085
00:41:38,590 --> 00:41:40,850
그리고 이들은 매우 일반적으로 사용됩니다.

1086
00:41:40,850 --> 00:41:44,250
그래서 이것은-- 하지만 자기 회귀 모델링의 문제는

1087
00:41:44,250 --> 00:41:46,750
데이터를 시퀀스로 나누어야 한다는 것입니다.

1088
00:41:46,750 --> 00:41:50,490
텍스트 데이터는 본래 1D 시퀀스이기 때문에

1089
00:41:50,490 --> 00:41:52,850
이것은 매우 자연스럽습니다.

1090
00:41:52,850 --> 00:41:54,870
그리고 이는 이산적인 것들의 1D

1091
00:41:54,870 --> 00:41:57,370
시퀀스이기 때문에, 이산적인 것들의 확률을

1092
00:41:57,370 --> 00:41:58,530
모델링하기가 매우 쉽습니다.

1093
00:41:58,530 --> 00:42:00,210
우리는 좋아하는 교차 엔트로피

1094
00:42:00,210 --> 00:42:02,630
소프트맥스 손실로 한 학기 내내 그렇게 해왔습니다.

1095
00:42:02,630 --> 00:42:04,730
교차 엔트로피 소프트맥스

1096
00:42:04,730 --> 00:42:09,410
손실은 항상 고정된 이산 범주 수에 대한 분포입니다.

1097
00:42:09,410 --> 00:42:11,568
네트워크는 각 범주에 대한 점수를 예측합니다.

1098
00:42:11,568 --> 00:42:14,110
소프트맥스로 정규화하고, 교차 엔트로피 손실로 학습합니다.

1099
00:42:14,110 --> 00:42:15,410
우리는 그것을 하는 방법을 알고 있습니다.

1100
00:42:15,410 --> 00:42:17,650
그래서 이러한 것들이 언어 모델에

1101
00:42:17,650 --> 00:42:21,410
매우 자연스럽게 맞는 이유는 언어가 이미 이산적이기 때문입니다.

1102
00:42:21,410 --> 00:42:23,335
언어는 이미 1D 시퀀스입니다.

1103
00:42:23,335 --> 00:42:25,210
그래서 어떻게-- 토크나이저가 있는지

1104
00:42:25,210 --> 00:42:26,870
약간의 모호함이 있습니다.

1105
00:42:26,870 --> 00:42:28,290
우리는 그것에 대해 깊이 들어가지 않을 것입니다.

1106
00:42:28,290 --> 00:42:30,870
하지만 이러한 것들은 언어가 이미 1D이고

1107
00:42:30,870 --> 00:42:34,170
이산적이기 때문에 언어 문제에 매우 잘 맞습니다.

1108
00:42:34,170 --> 00:42:38,210
이미지는 더 까다롭습니다. 왜냐하면 이미지는 본래 1D가 아니기 때문입니다.

1109
00:42:38,210 --> 00:42:40,470
이미지는 또한 본래 이산적이지 않습니다.

1110
00:42:40,470 --> 00:42:43,690
우리는 종종 이미지를 연속적이고 실수 값인 것으로 생각합니다.

1111
00:42:43,690 --> 00:42:46,850
그래서 이들은 이미지에 자연스럽게 잘

1112
00:42:46,850 --> 00:42:48,370
맞지 않습니다.

1113
00:42:48,370 --> 00:42:51,550
하지만 망치가 있다면, 못을 쳐야 합니다.

1114
00:42:51,550 --> 00:42:53,970
그래서 사람들은 확실히 몇 년

1115
00:42:53,970 --> 00:42:58,210
전에는 이미지에 자기 회귀 모델을 단순하게 적용했습니다.

1116
00:42:58,210 --> 00:43:02,810
자기 회귀 모델로 이미지를 모델링하기 위해 할 수

1117
00:43:02,810 --> 00:43:06,410
있는 한 가지는 이미지를 픽셀 시퀀스로

1118
00:43:06,410 --> 00:43:08,370
취급하는 것입니다.

1119
00:43:08,370 --> 00:43:11,330
특히 각 픽셀은 실제로 세 개의 숫자입니다.

1120
00:43:11,330 --> 00:43:16,710
대부분의 디스플레이와 이미지 표현에서 이러한 숫자는 실제로

1121
00:43:16,710 --> 00:43:18,330
이산적입니다.

1122
00:43:18,330 --> 00:43:20,190
대부분의 JPEG 또는 PNG--

1123
00:43:20,190 --> 00:43:23,330
우리가 이미지를 저장하는 데 사용하는 파일 형식은 일반적으로

1124
00:43:23,330 --> 00:43:24,470
채널당 8비트입니다.

1125
00:43:24,470 --> 00:43:27,530
따라서 각 픽셀이 가질 수 있는 값의 수는 실제로

1126
00:43:27,530 --> 00:43:29,030
고정되어 있습니다.

1127
00:43:29,030 --> 00:43:32,390
픽셀은 단지 세 개의 단일 바이트 값입니다.

1128
00:43:32,390 --> 00:43:35,650
단일 바이트는 0에서 255까지의 정수와 같습니다.

1129
00:43:35,650 --> 00:43:38,010
따라서 픽셀은 단지 세 개의 정수입니다.

1130
00:43:38,010 --> 00:43:40,490
각 정수는 0에서 255까지일 수 있습니다.

1131
00:43:40,490 --> 00:43:43,370
우리가 할 수 있는 것은 이미지를

1132
00:43:43,370 --> 00:43:46,610
가져와서 긴 시퀀스로 레스터화하여 시퀀스의

1133
00:43:46,610 --> 00:43:49,010
각 요소가 이미지의 서브픽셀

1134
00:43:49,010 --> 00:43:51,970
값 중 하나가 되도록 하는 것입니다.

1135
00:43:51,970 --> 00:43:56,350
이제 우리는 이미지를 1차원 시퀀스로 변환했으며, 그

1136
00:43:56,350 --> 00:43:58,750
시퀀스의 각 항목은 이산 값입니다.

1137
00:43:58,750 --> 00:44:00,792
따라서 언어 모델에 대해 했던 것과 정확히

1138
00:44:00,792 --> 00:44:02,907
같은 방식으로 그 시퀀스에 자기 회귀 모델링을

1139
00:44:02,907 --> 00:44:03,990
직접 적용할 수 있습니다.

1140
00:44:03,990 --> 00:44:07,210
저는 RNN 또는 변환기를 사용하고 있습니다.

1141
00:44:07,210 --> 00:44:09,955
이 접근 방식에서 문제를 발견할 수 있는 사람이 있나요?

1142
00:44:09,955 --> 00:44:10,830
[? 너무 ?] [? 많습니다. ?]

1143
00:44:10,830 --> 00:44:11,590
매우 비쌉니다.

1144
00:44:11,590 --> 00:44:13,010
매우, 매우 비쌉니다.

1145
00:44:13,010 --> 00:44:15,970
모델링하고 싶은 합리적인 이미지는 아마도

1146
00:44:15,970 --> 00:44:18,850
1024 x 1024일 것입니다.

1147
00:44:18,850 --> 00:44:20,870
그렇게 높은 해상도는 아니지만,

1148
00:44:20,870 --> 00:44:22,510
꽤 좋은 해상도입니다.

1149
00:44:22,510 --> 00:44:25,350
하지만 1024 x 1024 이미지를 가지면,

1150
00:44:25,350 --> 00:44:28,050
이는 300만 픽셀의 시퀀스가 될 것입니다.

1151
00:44:28,050 --> 00:44:32,190
사람들은 실제로 요즘 수백만 개의 시퀀스를 모델링할 수 있습니다.

1152
00:44:32,190 --> 00:44:33,690
하지만 매우, 매우 비쌉니다.

1153
00:44:33,690 --> 00:44:35,977
더 효율적인 방법이 있어야 합니다.

1154
00:44:35,977 --> 00:44:37,810
몇 년 전 몇몇 논문에서

1155
00:44:37,810 --> 00:44:40,185
사람들이 이러한 자기 회귀 모델을 이미지의

1156
00:44:40,185 --> 00:44:42,210
픽셀에 직접 적용했지만,

1157
00:44:42,210 --> 00:44:44,490
고해상도로 확장하기가 매우 어려워서

1158
00:44:44,490 --> 00:44:46,890
그다지 성공적이지 않았던 것 같습니다.

1159
00:44:46,890 --> 00:44:49,450
다음 강의에서 좀 더 이야기할 스포일러

1160
00:44:49,450 --> 00:44:51,570
알림은, 실제로 지난 몇 년

1161
00:44:51,570 --> 00:44:53,810
동안 다시 부활했다는 것입니다.

1162
00:44:53,810 --> 00:44:56,050
하지만 요령은 시퀀스의 개별 픽셀을

1163
00:44:56,050 --> 00:44:58,590
개별 픽셀 값으로 모델링하는 것이 아니라,

1164
00:44:58,590 --> 00:45:01,770
대신 이미지를 일차원 토큰의 시퀀스로 분해하기 위해

1165
00:45:01,770 --> 00:45:04,610
다른 프로세스나 절차 또는 모델, 아마도

1166
00:45:04,610 --> 00:45:06,130
신경망을 사용하는 것입니다.

1167
00:45:06,130 --> 00:45:08,770
그래서 다음 강의에서 좀 더 이야기할 내용입니다.

1168
00:45:08,770 --> 00:45:10,312
하지만 이것은 적어도 자기 회귀 모델이

1169
00:45:10,312 --> 00:45:11,915
무엇인지에 대한 감각을 제공합니다.

1170
00:45:11,915 --> 00:45:13,790
그들의 확률론적 공식화는 무엇인가요.

1171
00:45:13,790 --> 00:45:15,207
언어에 어떻게 적용하나요.

1172
00:45:15,207 --> 00:45:16,770
이미지에 어떻게 적용하나요.

1173
00:45:16,770 --> 00:45:18,890
그럼 자기 회귀 모델에서

1174
00:45:18,890 --> 00:45:21,770
다음으로 변분 오토인코더로 넘어갑니다.

1175
00:45:21,770 --> 00:45:24,400
변분 오토인코더는 꽤 재미있습니다.

1176
00:45:28,570 --> 00:45:32,130
우리가 이야기한 이러한 자기 회귀 모델에서는 최대

1177
00:45:32,130 --> 00:45:33,750
우도를 하려고 합니다.

1178
00:45:33,750 --> 00:45:35,750
데이터를 시퀀스의 부분으로 나누었습니다.

1179
00:45:35,750 --> 00:45:38,280
데이터의 우도를 최대화하려고 합니다.

1180
00:45:38,280 --> 00:45:40,530
변분 오토인코더는 조금 다른

1181
00:45:40,530 --> 00:45:41,890
일을 할 것입니다.

1182
00:45:41,890 --> 00:45:44,470
대신, 여전히 명시적인 방법이 될 것입니다.

1183
00:45:44,470 --> 00:45:47,450
여전히 우리가 계산할 수 있는 밀도가 있을 것이지만,

1184
00:45:47,450 --> 00:45:49,510
그것은 다루기 어려울 것입니다.

1185
00:45:49,510 --> 00:45:51,290
우리는 그것을 근사할 수 있을 것입니다.

1186
00:45:51,290 --> 00:45:52,510
왜 그렇게 할 것인가요?

1187
00:45:52,510 --> 00:45:55,888
우리는 밀도를 정확하게 계산하는 완벽한 방법을 가지고 있었습니다.

1188
00:45:55,888 --> 00:45:57,930
그것을 위해 포기할 것은 우리가

1189
00:45:57,930 --> 00:45:59,110
무언가를 얻는 것입니다.

1190
00:45:59,110 --> 00:46:01,610
우리는 데이터에 대해 합리적인 잠재 벡터를 계산할 수

1191
00:46:01,610 --> 00:46:02,980
있는 능력을 얻게 될 것입니다.

1192
00:46:02,980 --> 00:46:05,040
우리는 학습 과정에서 자연스럽게

1193
00:46:05,040 --> 00:46:08,240
나타나는 데이터를 나타내는 벡터를 가질 것입니다.

1194
00:46:08,240 --> 00:46:10,740
그리고 그 벡터들은 그 자체로 유용할 것입니다.

1195
00:46:10,740 --> 00:46:13,000
그 잠재 벡터에 접근할 수 있는

1196
00:46:13,000 --> 00:46:14,920
능력은 우리가 정확한 밀도를

1197
00:46:14,920 --> 00:46:17,268
계산하는 것을 포기할 만큼 충분히

1198
00:46:17,268 --> 00:46:19,560
유용할 것이며, 대신 실제 밀도의

1199
00:46:19,560 --> 00:46:22,480
하한인 이러한 근사 밀도로 정착할 것입니다.

1200
00:46:22,480 --> 00:46:24,400
오토회귀 모델에서 시퀀스를

1201
00:46:24,400 --> 00:46:26,360
나누는 동기는 문제를

1202
00:46:26,360 --> 00:46:27,660
분해하기 때문입니다.

1203
00:46:27,660 --> 00:46:30,400
각 부분을 모델링하기가 더 쉬워집니다.

1204
00:46:30,400 --> 00:46:33,520
언어 모델링을 하고 있다고 상상해 보세요. 그리고

1205
00:46:33,520 --> 00:46:35,600
V개의 단어로 구성된 어휘가 있습니다.

1206
00:46:35,600 --> 00:46:38,920
두 단어의 확률을 공동으로 모델링하고

1207
00:46:38,920 --> 00:46:39,580
싶습니다.

1208
00:46:39,580 --> 00:46:41,900
가능한 두 단어 시퀀스는 몇 개입니까?

1209
00:46:41,900 --> 00:46:42,978
V 제곱입니다.

1210
00:46:42,978 --> 00:46:45,020
가능한 세 단어 시퀀스는 몇 개입니까?

1211
00:46:45,020 --> 00:46:46,240
V 세제곱입니다.

1212
00:46:46,240 --> 00:46:48,080
일반적으로 T개의 단어 시퀀스와

1213
00:46:48,080 --> 00:46:50,620
어휘 V가 있을 때, 몇 개가 있습니까?

1214
00:46:50,620 --> 00:46:53,562
V의 T 제곱입니다. 그래서 그건 나쁩니다.

1215
00:46:53,562 --> 00:46:54,520
지수적으로 증가합니다.

1216
00:46:54,520 --> 00:46:56,812
T개의 항목 시퀀스의 결합 분포를

1217
00:46:56,812 --> 00:46:59,520
직접 모델링하려면, 모델링해야 할

1218
00:46:59,520 --> 00:47:02,360
이산 확률 분포의 항목 수가 시퀀스

1219
00:47:02,360 --> 00:47:04,360
길이에 따라 지수적으로

1220
00:47:04,360 --> 00:47:05,440
증가할 것입니다.

1221
00:47:05,440 --> 00:47:07,815
그리고 긴 시퀀스로 가고 싶다면 그것은 빠르게

1222
00:47:07,815 --> 00:47:09,342
완전히 다루기 어려워질 것입니다.

1223
00:47:09,342 --> 00:47:10,800
그래서 우리가 그것을 나누는 이유는

1224
00:47:10,800 --> 00:47:12,900
한 번에 모두 모델링할 필요가 없기 때문입니다.

1225
00:47:12,900 --> 00:47:15,400
이런 식으로 분해하고 이전 부분에 조건을

1226
00:47:15,400 --> 00:47:17,000
두고 한 부분만 예측합니다.

1227
00:47:17,000 --> 00:47:17,900
좋은 질문입니다.

1228
00:47:17,900 --> 00:47:19,740
그것을 완화하기 위해 로그 트릭을 적용할 수 있을까요?

1229
00:47:19,740 --> 00:47:20,460
네, 정확히 그렇습니다.

1230
00:47:20,460 --> 00:47:23,640
실제로는 이러한 확률 밀도 값을 모델링하는 것을 결코

1231
00:47:23,640 --> 00:47:24,828
보지 못할 것입니다.

1232
00:47:24,828 --> 00:47:27,120
거의 항상 로그 확률로 작업할

1233
00:47:27,120 --> 00:47:27,920
것입니다.

1234
00:47:27,920 --> 00:47:31,020
그래서 모델은 로그 확률을 출력할 것입니다.

1235
00:47:31,020 --> 00:47:33,100
당신은 로그 공간에서 손실을 계산할 것입니다.

1236
00:47:33,100 --> 00:47:34,600
수치적 안정성을 위해, 실제로는

1237
00:47:34,600 --> 00:47:37,138
거의 모든 것을 로그 공간에서 계산할 것입니다.

1238
00:47:37,138 --> 00:47:38,680
그래서 x의 p는

1239
00:47:38,680 --> 00:47:41,560
변환기의 맨 위에서 모든 이전 토큰에 조건을

1240
00:47:41,560 --> 00:47:43,600
두고 다음 토큰에 대한 확률

1241
00:47:43,600 --> 00:47:46,178
분포를 출력하기 때문에 생성됩니다.

1242
00:47:46,178 --> 00:47:48,220
그리고 그것은 시퀀스의 모든 지점에 대해 그렇게 합니다.

1243
00:47:48,220 --> 00:47:50,720
따라서 실제로 이 정확한 확률 밀도

1244
00:47:50,720 --> 00:47:53,360
값을 시퀀스의 모든 점에서 값을 곱하여

1245
00:47:53,360 --> 00:47:54,740
복구할 수 있습니다.

1246
00:47:54,740 --> 00:47:56,160
입력 시퀀스가 있다면,

1247
00:47:56,160 --> 00:47:58,160
이를 변환기에 통과시키면

1248
00:47:58,160 --> 00:48:01,400
변환기는 시퀀스의 모든 지점에서 이전 부분에

1249
00:48:01,400 --> 00:48:03,600
조건화된 모든 토큰에 대한

1250
00:48:03,600 --> 00:48:05,480
분포를 예측할 것입니다.

1251
00:48:05,480 --> 00:48:07,780
그리고 실제 다음 토큰이 무엇이었는지, 다음

1252
00:48:07,780 --> 00:48:09,780
토큰의 예측 확률이 무엇이었는지를 계산한

1253
00:48:09,780 --> 00:48:12,387
다음, 이를 전체 시퀀스에 걸쳐 모두 곱할 수 있습니다.

1254
00:48:12,387 --> 00:48:14,720
이것이 우리가 이러한 자기 회귀 모델 중 하나에서 정확한

1255
00:48:14,720 --> 00:48:16,303
밀도 값을 복구할 수 있는 방법입니다.

1256
00:48:16,303 --> 00:48:17,803
그리고 이것은 RNN이나 변환기

1257
00:48:17,803 --> 00:48:18,980
모두에 적용될 수 있습니다.

1258
00:48:22,560 --> 00:48:24,040
좋은 질문입니다.

1259
00:48:24,040 --> 00:48:27,778
그럼 변분 오토인코더에서는 상황이 복잡해집니다.

1260
00:48:27,778 --> 00:48:29,320
우리는 실제로 V를 생략하고 몇

1261
00:48:29,320 --> 00:48:31,612
슬라이드 동안 오토인코더에 대해 이야기할 것입니다.

1262
00:48:31,612 --> 00:48:34,280
왜냐하면 이 과정에서 아직 그렇게 하지 않았기 때문입니다.

1263
00:48:34,280 --> 00:48:36,162
변분 오토인코더에서는

1264
00:48:36,162 --> 00:48:38,120
기본적으로 레이블 없이 입력

1265
00:48:38,120 --> 00:48:43,080
x에서 특징 z를 추출하는 비지도 학습 방법이 될 것입니다.

1266
00:48:43,080 --> 00:48:47,320
그리고 이것은 우리가 방금 이야기한 자기 지도

1267
00:48:47,320 --> 00:48:49,120
학습의 맥락에 있습니다.

1268
00:48:49,120 --> 00:48:51,120
우리의 개념은 특징이 데이터에 대한

1269
00:48:51,120 --> 00:48:53,580
유용한 정보를 추출해야 한다는 것입니다.

1270
00:48:53,580 --> 00:48:55,680
아마도 그것들은 이미지에서 객체의

1271
00:48:55,680 --> 00:48:59,160
정체성이 무엇인지, 얼마나 많은지 암묵적으로 인코딩할

1272
00:48:59,160 --> 00:48:59,660
것입니다.

1273
00:48:59,660 --> 00:49:00,827
그들의 색상은 무엇인가요?

1274
00:49:00,827 --> 00:49:03,480
우리는 이 특징 벡터 z가 입력 x에

1275
00:49:03,480 --> 00:49:05,958
대한 유용한 정보를 포함하기를 원합니다.

1276
00:49:05,958 --> 00:49:08,000
그리고 이 인코더 자체는 어떤 아키텍처의

1277
00:49:08,000 --> 00:49:08,833
신경망일 수 있습니다.

1278
00:49:08,833 --> 00:49:11,920
MLP, 변환기, CNN 등 원하는 대로 가능합니다.

1279
00:49:11,920 --> 00:49:13,440
하지만 입력은 데이터 x입니다.

1280
00:49:13,440 --> 00:49:16,240
그리고 나서 어떤 벡터 z를 출력할 것입니다.

1281
00:49:16,240 --> 00:49:18,732
그런 다음 질문은 레이블 없이 이것을 어떻게 할 것인가입니다.

1282
00:49:18,732 --> 00:49:20,440
우리는 이전 강의에서 이와 관련된

1283
00:49:20,440 --> 00:49:21,720
많은 예를 보았습니다.

1284
00:49:21,720 --> 00:49:23,320
하지만 아주 간단한 예가 있는데,

1285
00:49:23,320 --> 00:49:25,360
입력을 재구성하려고 시도하는 것입니다.

1286
00:49:25,360 --> 00:49:28,080
그래서 이제 z를 입력하고 다시 x를

1287
00:49:28,080 --> 00:49:30,920
출력할 두 번째 모델 부분인 디코더를

1288
00:49:30,920 --> 00:49:32,320
가질 것입니다.

1289
00:49:32,320 --> 00:49:34,960
그리고 우리는-- 아, x를 생략했습니다.

1290
00:49:34,960 --> 00:49:36,425
우리는 이 모델의

1291
00:49:36,425 --> 00:49:37,800
출력이 실제로 입력과

1292
00:49:37,800 --> 00:49:40,220
일치하도록 훈련할 것입니다.

1293
00:49:40,220 --> 00:49:42,220
어떤 면에서는 가장 어리석은 손실 함수입니다.

1294
00:49:42,220 --> 00:49:44,790
우리는 단순히 모델을 정체성 함수를 모방하도록 훈련하고 있습니다.

1295
00:49:44,790 --> 00:49:45,540
왜 그렇게 하죠?

1296
00:49:45,540 --> 00:49:46,860
우리는 이미 정체성 함수를 알고 있습니다.

1297
00:49:46,860 --> 00:49:48,160
왜 우리는 이미 알고 있는

1298
00:49:48,160 --> 00:49:49,840
정체성 함수를 배우기 위해 많은 연산을

1299
00:49:49,840 --> 00:49:52,240
하고 큰 데이터 세트에서 신경망을 훈련하고 있나요?

1300
00:49:52,240 --> 00:49:55,000
그것은 우리가 어떤 방식으로든 병목 현상을 만들 것이기 때문입니다.

1301
00:49:55,000 --> 00:49:57,020
예를 들어, 이 모델이 무한한 용량을

1302
00:49:57,020 --> 00:49:59,440
가지고 있다면, z 벡터가 매우 넓고 학습에

1303
00:49:59,440 --> 00:50:02,080
제약이 없다면, 신경망이 이 문제를 완벽하게 해결할

1304
00:50:02,080 --> 00:50:03,520
것이라고 기대할 것입니다.

1305
00:50:03,520 --> 00:50:06,240
하지만 우리는 그렇게 하고 싶지 않습니다. 왜냐하면 우리는 이 목표를 배우는
것에

1306
00:50:06,240 --> 00:50:07,717
대해 명시적으로 신경 쓰지 않기 때문입니다.

1307
00:50:07,717 --> 00:50:09,300
우리는 이미 항등 함수를 알고 있습니다.

1308
00:50:09,300 --> 00:50:11,633
이를 계산하기 위해 비싼 신경망이 필요하지 않습니다.

1309
00:50:11,633 --> 00:50:13,520
우리가 하고 싶은 것은 네트워크가 어떤

1310
00:50:13,520 --> 00:50:16,265
제약 조건 하에 항등 함수를 배우도록 강요하는 것입니다.

1311
00:50:16,265 --> 00:50:17,640
전통적인 오토인코더에서

1312
00:50:17,640 --> 00:50:20,680
자주 사용하는 제약 조건은 그 표현 z를

1313
00:50:20,680 --> 00:50:22,120
병목시키는 것입니다.

1314
00:50:22,120 --> 00:50:25,120
특히, 이는 중간의 벡터 z가 입력

1315
00:50:25,120 --> 00:50:28,120
x보다 훨씬 작아야 함을 의미합니다.

1316
00:50:28,120 --> 00:50:31,520
따라서 입력 x는 1024x1024의 고해상도 이미지일

1317
00:50:31,520 --> 00:50:34,040
수 있으며, 이는 300만 개의 부동

1318
00:50:34,040 --> 00:50:35,380
소수점으로 구성됩니다.

1319
00:50:35,380 --> 00:50:39,240
하지만 z는 128 차원의 잠재 코드일 수 있습니다.

1320
00:50:39,240 --> 00:50:41,920
그래서 모델은 이제 출력, 즉

1321
00:50:41,920 --> 00:50:43,940
데이터 x를 재구성하되

1322
00:50:43,940 --> 00:50:46,040
중간의 병목 표현을 통해

1323
00:50:46,040 --> 00:50:49,278
압축해야 하는 문제를 해결해야 합니다.

1324
00:50:49,278 --> 00:50:51,320
우리는 이것이 모델이

1325
00:50:51,320 --> 00:50:54,960
데이터에 대한 비트리비얼 구조를

1326
00:50:54,960 --> 00:50:57,000
배우도록 강요할

1327
00:50:57,000 --> 00:50:59,025
것이라고 희망합니다.

1328
00:50:59,025 --> 00:51:00,400
그리고 이렇게 한 후에는

1329
00:51:00,400 --> 00:51:02,400
일반적인 자기 감독 학습 기법을

1330
00:51:02,400 --> 00:51:04,660
적용할 수 있습니다. 디코더를 버리고 이

1331
00:51:04,660 --> 00:51:07,280
Z를 사용하여 하위 작업을 위한 감독 모델을

1332
00:51:07,280 --> 00:51:08,540
초기화할 수 있습니다.

1333
00:51:08,540 --> 00:51:11,160
우리가 방금 본 자기 감독

1334
00:51:11,160 --> 00:51:13,040
이야기와 같은 이야기입니다.

1335
00:51:13,040 --> 00:51:15,320
하지만 만약 우리가 실제로 데이터를

1336
00:51:15,320 --> 00:51:17,265
생성하고 싶다면 어떻게 될까요?

1337
00:51:17,265 --> 00:51:18,640
그렇다면 우리가 정말로

1338
00:51:18,640 --> 00:51:21,220
하고 싶은 것은 자기 감독 이야기의 반대입니다.

1339
00:51:21,220 --> 00:51:23,860
우리가 정말로 하고 싶은 것은 인코더를

1340
00:51:23,860 --> 00:51:27,800
버리고 대신 모델이 데이터를 표현하도록 학습한 z와 일치하는

1341
00:51:27,800 --> 00:51:30,560
z를 샘플링할 수 있는 방법을 찾는 것입니다.

1342
00:51:30,560 --> 00:51:33,840
그리고 만약 우리가 어떤 방식으로든 데이터 분포와

1343
00:51:33,840 --> 00:51:36,300
일치하는 z를 샘플링하는 절차가 있다면,

1344
00:51:36,300 --> 00:51:40,060
우리는 z를 샘플링하고 학습한 디코더를 통해 전달하여 새로운

1345
00:51:40,060 --> 00:51:41,560
샘플을 생성할 수 있습니다.

1346
00:51:41,560 --> 00:51:43,640
이제 이것은 암묵적인 방법입니다.

1347
00:51:43,640 --> 00:51:46,080
어디에도 밀도가 떠다니지 않는다고

1348
00:51:46,080 --> 00:51:46,820
말씀하셨습니다.

1349
00:51:46,820 --> 00:51:48,440
하지만 만약 우리가 이렇게 할

1350
00:51:48,440 --> 00:51:51,240
수 있는 방법이 있다면, 이는 밀도를 명시적으로

1351
00:51:51,240 --> 00:51:55,070
모델링하지 않고도 모델에서 샘플을 추출하는 방법이 될 것입니다.

1352
00:51:55,070 --> 00:51:58,150
하지만 문제는 우리가 이미 여기서 문제를

1353
00:51:58,150 --> 00:52:01,377
미룬 것입니다. 이미지를 생성하고 싶다면 x를

1354
00:52:01,377 --> 00:52:03,210
생성하고 싶다고 말했습니다.

1355
00:52:03,210 --> 00:52:04,710
우리는 x의 데이터셋을 가지고 있습니다.

1356
00:52:04,710 --> 00:52:05,895
우리는 어떻게 해야 할까요?

1357
00:52:05,895 --> 00:52:07,270
우리는 이 오토인코더를 훈련시켜서

1358
00:52:07,270 --> 00:52:08,523
해결하겠다고 말했습니다.

1359
00:52:08,523 --> 00:52:10,190
이제 우리는 z의 데이터셋을 가지고 있으며,

1360
00:52:10,190 --> 00:52:11,530
z 공간에서 샘플링해야 합니다.

1361
00:52:11,530 --> 00:52:12,730
그것은 더 쉽지 않습니다.

1362
00:52:12,730 --> 00:52:14,870
그래서 우리는 막혔습니다.

1363
00:52:14,870 --> 00:52:18,510
변분 오토인코더의 아이디어는 z에 어떤

1364
00:52:18,510 --> 00:52:22,110
구조를 강요할 수 있다면 어떨까요?

1365
00:52:22,110 --> 00:52:24,630
전통적인 오토인코더 구조를 가지고 있다면,

1366
00:52:24,630 --> 00:52:27,270
모델이 z에 대해 알려진 구조를 강요하도록

1367
00:52:27,270 --> 00:52:27,970
하지 않습니다.

1368
00:52:27,970 --> 00:52:30,590
단지 잠재 표현을 주어진 데이터로 재구성하도록

1369
00:52:30,590 --> 00:52:32,050
요청하는 것입니다.

1370
00:52:32,050 --> 00:52:33,750
하지만 z가 가우시안 분포 또는

1371
00:52:33,750 --> 00:52:36,490
다른 알려진 분포에서 나오도록 강요할 수 있는

1372
00:52:36,490 --> 00:52:38,050
메커니즘이 있다면 어떻게 될까요?

1373
00:52:38,050 --> 00:52:41,190
그렇다면 추론 시점에 그 알려진 분포에서 샘플을

1374
00:52:41,190 --> 00:52:42,293
그릴 수 있습니다.

1375
00:52:42,293 --> 00:52:44,710
이 모델이 훈련된 후, 그 알려진

1376
00:52:44,710 --> 00:52:47,890
분포에서 샘플을 뽑고 디코더를 통해

1377
00:52:47,890 --> 00:52:51,070
전달하면 이제 샘플을 얻을 수 있습니다.

1378
00:52:51,070 --> 00:52:53,870
따라서 이러한 오토인코더가 확률적이도록

1379
00:52:53,870 --> 00:52:57,170
강요하고 잠재 공간에 확률적 구조를 부여하는

1380
00:52:57,170 --> 00:53:00,510
것이 변분 오토인코더가 하려는 것입니다.

1381
00:53:00,510 --> 00:53:01,790
왜 변분인가요?

1382
00:53:01,790 --> 00:53:02,990
긴 이야기입니다.

1383
00:53:02,990 --> 00:53:06,470
문헌에서 그 용어에 대한 오랜 역사가

1384
00:53:06,470 --> 00:53:08,070
있다고 합니다.

1385
00:53:08,070 --> 00:53:09,790
기본적으로 변분 오토인코더는

1386
00:53:09,790 --> 00:53:12,950
전통적인 오토인코더에 대한 확률적 변형입니다.

1387
00:53:12,950 --> 00:53:16,530
따라서 원시 데이터에서 잠재 특징 z를 학습하게 됩니다.

1388
00:53:16,530 --> 00:53:19,310
그런 다음 모델이 훈련된 후

1389
00:53:19,310 --> 00:53:22,110
추론 시점에서 샘플링할 수 있도록

1390
00:53:22,110 --> 00:53:24,350
학습된 잠재 공간 z에

1391
00:53:24,350 --> 00:53:26,710
구조를 부여할 수 있습니다.

1392
00:53:26,710 --> 00:53:30,610
좀 더 구체적으로, 훈련 데이터 xi를 가정하겠습니다.

1393
00:53:30,610 --> 00:53:33,110
여기서 위 첨자 i는 서로

1394
00:53:33,110 --> 00:53:35,510
다른 독립 샘플 x를 의미합니다.

1395
00:53:35,510 --> 00:53:39,110
각 xi가 어떤 기본 잠재 벡터 z에서

1396
00:53:39,110 --> 00:53:42,510
생성되었다고 가정하며, 모든 xi와

1397
00:53:42,510 --> 00:53:45,670
관련된 어떤 zi가 표면

1398
00:53:45,670 --> 00:53:47,550
아래에 숨어 있습니다.

1399
00:53:47,550 --> 00:53:50,370
데이터를 생성하는 우주의 과정에서,

1400
00:53:50,370 --> 00:53:55,212
먼저 z를 생성한 다음 zi에서 xi를 생성했습니다.

1401
00:53:55,212 --> 00:53:56,670
우주가 우리가 본

1402
00:53:56,670 --> 00:53:59,670
이미지를 생성하기 위해 알아야 할 모든 것은 그

1403
00:53:59,670 --> 00:54:02,110
잠재 벡터 z에 포함되어 있었습니다.

1404
00:54:02,110 --> 00:54:04,167
하지만 우리는 그 잠재 벡터 z를 볼 수 없습니다.

1405
00:54:04,167 --> 00:54:05,250
우리는 그것들을 관찰할 수 없습니다.

1406
00:54:05,250 --> 00:54:06,810
우리는 그것들의 데이터 세트를 가지고 있지 않습니다.

1407
00:54:06,810 --> 00:54:10,590
직관적으로 x는 이미지이고, z는 그 이미지에 대해 알아야

1408
00:54:10,590 --> 00:54:12,910
할 모든 것을 알려주는 잠재 특징

1409
00:54:12,910 --> 00:54:14,910
표현입니다. 하지만 그 잠재

1410
00:54:14,910 --> 00:54:16,670
벡터를 관찰할 수는 없습니다.

1411
00:54:16,670 --> 00:54:19,480
훈련 후에는 샘플을 생성할 수 있습니다. 아,

1412
00:54:19,480 --> 00:54:21,230
그리고 다른 제약 조건은

1413
00:54:21,230 --> 00:54:24,152
z가 알려진 분포에서 나오도록 강제할 것입니다.

1414
00:54:24,152 --> 00:54:25,610
따라서 모델이 훈련된 후에는 우리가

1415
00:54:25,610 --> 00:54:27,318
방금 말한 대로 정확히 할 수 있습니다.

1416
00:54:27,318 --> 00:54:29,210
그 알려진 분포에서 z를

1417
00:54:29,210 --> 00:54:32,630
뽑고, 디코더를 통과시키면 샘플을 얻을 수 있습니다.

1418
00:54:32,630 --> 00:54:34,910
그리고 우리는 일반적으로 간단한 사전 분포를 가정합니다.

1419
00:54:34,910 --> 00:54:37,430
거의 항상 단위 가우시안

1420
00:54:37,430 --> 00:54:39,590
분포가 가장 일반적입니다.

1421
00:54:39,590 --> 00:54:41,590
그렇다면 우리는 어떻게 이걸 훈련할 수 있을까요?

1422
00:54:41,590 --> 00:54:43,510
이건 불가능한 문제처럼 느껴집니다.

1423
00:54:43,510 --> 00:54:45,430
우리는 이 z를 얻고, 각

1424
00:54:45,430 --> 00:54:49,250
x에 대해 z를 찾는 네트워크를 훈련하고 싶습니다.

1425
00:54:49,250 --> 00:54:50,493
우리는 z를 관찰할 수 없습니다.

1426
00:54:50,493 --> 00:54:51,410
이건 불가능해 보입니다.

1427
00:54:51,410 --> 00:54:52,510
우리는 무엇을 할까요?

1428
00:54:52,510 --> 00:54:54,910
우리는 최대 우도 방법으로 돌아갈 것입니다.

1429
00:54:54,910 --> 00:54:58,750
만약 우리가 x와 z의 데이터 세트를 가지고 있다면, 최대

1430
00:54:58,750 --> 00:55:02,030
우도를 사용하여 같은 로그 트릭을 직접 사용할 수 있습니다.

1431
00:55:02,030 --> 00:55:02,670
로그

1432
00:55:02,670 --> 00:55:03,970
확률을 극대화합니다.

1433
00:55:03,970 --> 00:55:06,875
우리는 이전에 본 것과 정확히 같은 것을 사용할 수 있습니다.

1434
00:55:06,875 --> 00:55:08,750
그리고 z에 조건부로 x의 조건부

1435
00:55:08,750 --> 00:55:10,750
생성 모델 p를 훈련할 수 있습니다.

1436
00:55:10,750 --> 00:55:11,690
하지만 우리는 z를 모릅니다.

1437
00:55:11,690 --> 00:55:13,232
잠시 z를 안다고 가정해 봅시다.

1438
00:55:15,470 --> 00:55:18,630
z를 모르기 때문에, 우리는 주변화하려고 할 수 있습니다.

1439
00:55:18,630 --> 00:55:21,190
x의 p는 z와 x의 결합 분포가 존재해야

1440
00:55:21,190 --> 00:55:24,670
한다는 것을 알고 있습니다. 비록 우리가 그것을

1441
00:55:24,670 --> 00:55:26,110
관찰할 수는 없지만요.

1442
00:55:26,110 --> 00:55:28,110
원칙적으로 z를 적분하여 그것을

1443
00:55:28,110 --> 00:55:31,150
주변화하여 x의 p를 얻을 수 있습니다.

1444
00:55:31,150 --> 00:55:34,350
어쩌면 z를 주변화하고 여전히 최대 우도를

1445
00:55:34,350 --> 00:55:36,750
수행할 수 있는 x와 z의

1446
00:55:36,750 --> 00:55:39,650
결합 분포가 있다고 가정할 수 있습니다.

1447
00:55:39,650 --> 00:55:40,770
이것이 어떻게 작동하는지 봅시다.

1448
00:55:40,770 --> 00:55:41,977
여기서 이

1449
00:55:41,977 --> 00:55:44,310
항은 z가 주어진 x의

1450
00:55:44,310 --> 00:55:48,190
p를 x와 z의 결합 확률로 나누기

1451
00:55:48,190 --> 00:55:51,630
위해 체인 룰을 사용했습니다.

1452
00:55:51,630 --> 00:55:54,230
z가 주어진 x의 p는 괜찮습니다.

1453
00:55:54,230 --> 00:55:56,110
왼쪽의 디코더로 이를 계산할 수

1454
00:55:56,110 --> 00:55:58,277
있습니다. 이는 우리가 훈련하기를 희망하는

1455
00:55:58,277 --> 00:55:59,030
신경망입니다.

1456
00:55:59,030 --> 00:56:00,015
z의 p 항도 괜찮습니다.

1457
00:56:00,015 --> 00:56:01,390
우리는 이것이 단위 가우시안

1458
00:56:01,390 --> 00:56:03,432
또는 우리가 계산하거나 추론할 수 있는

1459
00:56:03,432 --> 00:56:05,430
다른 간단한 분포라고 가정할 것입니다.

1460
00:56:05,430 --> 00:56:06,910
하지만 이 적분은 우리를 힘들게 합니다.

1461
00:56:06,910 --> 00:56:08,910
일반적으로 신경망의 입력

1462
00:56:08,910 --> 00:56:12,470
전체 공간에 대해 적분할 수 있는 방법이 없습니다.

1463
00:56:12,470 --> 00:56:14,088
z가 주어진 x의 p는 신경망에

1464
00:56:14,088 --> 00:56:15,630
의해 모델링된 매우 복잡한

1465
00:56:15,630 --> 00:56:16,890
함수가 될 것입니다.

1466
00:56:16,890 --> 00:56:19,015
우리가 이를 해석적으로 또는 정확하게

1467
00:56:19,015 --> 00:56:20,877
적분할 수 있는 방법은 없습니다.

1468
00:56:20,877 --> 00:56:23,210
여기서 개별 부분에 대해 신경망을 훈련할 수 있습니다.

1469
00:56:23,210 --> 00:56:24,748
이 확률 모델링을 할

1470
00:56:24,748 --> 00:56:26,790
때의 기본 개념은 우리가 몇

1471
00:56:26,790 --> 00:56:29,292
가지 확률적 항을 적어내려는 것입니다.

1472
00:56:29,292 --> 00:56:31,750
희망적으로 그 중 일부는 우리가 해석적으로 적어내고

1473
00:56:31,750 --> 00:56:33,958
추론할 수 있는 간단한 분포가 될 것입니다.

1474
00:56:33,958 --> 00:56:36,083
일부는 학습된 신경망 구성 요소가 될

1475
00:56:36,083 --> 00:56:36,730
것입니다.

1476
00:56:36,730 --> 00:56:38,710
우리는 z가 주어진 x의 확률이

1477
00:56:38,710 --> 00:56:40,630
최대 우도를 통해 배울

1478
00:56:40,630 --> 00:56:44,110
수 있는 신경망이 될 것이라고 가정하고 있습니다.

1479
00:56:44,110 --> 00:56:47,550
하지만 우리는 최대 우도를 통해 그 신경망을 배우기 위해 어떤

1480
00:56:47,550 --> 00:56:50,467
목표를 사용할 수 있을지 적어내기 시작하고 있습니다.

1481
00:56:50,467 --> 00:56:52,550
여기서는 z에 대해 적분할 방법이 없기

1482
00:56:52,550 --> 00:56:53,870
때문에 운이 좋지 않습니다.

1483
00:56:53,870 --> 00:56:55,670
유한 샘플링을 통해

1484
00:56:55,670 --> 00:56:58,683
그 적분을 근사하려고 시도할 수

1485
00:56:58,683 --> 00:57:00,350
있지만, 일반적으로

1486
00:57:00,350 --> 00:57:04,230
z는 매우 고차원 공간이기 때문에 훈련의 내부

1487
00:57:04,230 --> 00:57:07,070
루프에서 근사 수치 적분을

1488
00:57:07,070 --> 00:57:09,910
수행하는 것은 좋은 생각이 아닙니다.

1489
00:57:09,910 --> 00:57:11,773
그래서 다른 것을 시도할 수 있습니다.

1490
00:57:11,773 --> 00:57:13,190
베이즈 규칙, 이것은 우리가 확률에서

1491
00:57:13,190 --> 00:57:14,490
항상 하는 또 다른 것입니다.

1492
00:57:14,490 --> 00:57:15,830
그래서 베이즈 규칙을 시도해 봅시다.

1493
00:57:15,830 --> 00:57:18,110
베이즈 규칙이 있다면 p의 x를 적어내기

1494
00:57:18,110 --> 00:57:20,890
위해 사용할 수 있는 또 다른 공식이 있습니다.

1495
00:57:20,890 --> 00:57:24,550
그래서 화면의 이 방정식에서 베이즈 규칙을 사용하여

1496
00:57:24,550 --> 00:57:26,430
p의 x를 적어낼 수 있습니다.

1497
00:57:26,430 --> 00:57:28,710
이 항들로 무엇을 할 수 있는지 봅시다.

1498
00:57:28,710 --> 00:57:31,070
z가 주어진 x의 p는 다시

1499
00:57:31,070 --> 00:57:32,870
디코더로 계산할 수 있습니다.

1500
00:57:32,870 --> 00:57:35,930
z의 p도 괜찮습니다. 우리는 이것이 가우시안이라고 가정하므로 이를

1501
00:57:35,930 --> 00:57:37,510
통해 무언가를 계산할 수 있습니다.

1502
00:57:37,510 --> 00:57:38,593
여기에는 적분이 없습니다.

1503
00:57:38,593 --> 00:57:39,170
좋습니다.

1504
00:57:39,170 --> 00:57:40,870
우리는 좋은 상태입니다.

1505
00:57:40,870 --> 00:57:43,950
하지만 이제 우리는 운이 다한 것 같습니다. x에 주어진 z의 p입니다.

1506
00:57:43,950 --> 00:57:47,070
x에 주어진 z의 후행 확률을 계산할 좋은

1507
00:57:47,070 --> 00:57:48,243
방법이 없습니다.

1508
00:57:48,243 --> 00:57:49,660
이 항을 계산하기

1509
00:57:49,660 --> 00:57:52,000
위해서는 어떤 종류의 적분이 필요합니다.

1510
00:57:52,000 --> 00:57:52,880
우리는 그것을 계산할 수 없습니다.

1511
00:57:52,880 --> 00:57:55,380
우리는 무엇을 할까요?

1512
00:57:55,380 --> 00:57:56,980
다른 신경망을 사용합시다.

1513
00:57:56,980 --> 00:57:58,660
변분 오토인코더 트릭은

1514
00:57:58,660 --> 00:58:01,340
아래의 확률적 항을 사용하고, 우리가

1515
00:58:01,340 --> 00:58:04,180
계산할 수 없는 베이즈 규칙을 사용하여,

1516
00:58:04,180 --> 00:58:05,900
다른 신경망을 넣어 그것을

1517
00:58:05,900 --> 00:58:07,460
계산해 보자는 것입니다.

1518
00:58:07,460 --> 00:58:09,335
그래서 우리는 다른 가중치

1519
00:58:09,335 --> 00:58:12,300
phi를 가진 또 다른 신경망 q를 가질

1520
00:58:12,300 --> 00:58:15,420
것이며, 이는 x에 주어진 z의 다른

1521
00:58:15,420 --> 00:58:17,860
조건부 확률 분포를 학습할 것입니다.

1522
00:58:17,860 --> 00:58:20,220
전체적인 목표는 이 다른 신경망이 첫

1523
00:58:20,220 --> 00:58:22,940
번째 신경망의 z에 주어진 x의 진짜

1524
00:58:22,940 --> 00:58:24,900
p를 근사하도록 하는 것입니다.

1525
00:58:24,900 --> 00:58:27,400
일반적으로 이것을 강제할 수는

1526
00:58:27,400 --> 00:58:31,740
없지만, 신경망을 넣고 우리가 할 수 있는 것을 봅시다.

1527
00:58:31,740 --> 00:58:34,540
그래서 우리가 계산할 수 없는

1528
00:58:34,540 --> 00:58:36,248
아래의 항을 근사하는

1529
00:58:36,248 --> 00:58:38,620
다른 신경망이 있다면,

1530
00:58:38,620 --> 00:58:41,220
우리는 가능성을 계산하고 최대

1531
00:58:41,220 --> 00:58:43,820
가능성을 수행할 수 있습니다.

1532
00:58:43,820 --> 00:58:45,420
그래서 이것이 변분 오토인코더를 훈련할

1533
00:58:45,420 --> 00:58:46,517
때 우리가 하는 일입니다.

1534
00:58:46,517 --> 00:58:48,100
우리는 기본적으로 두 개의 서로 다른

1535
00:58:48,100 --> 00:58:49,520
신경망을 공동으로 학습할 것입니다.

1536
00:58:49,520 --> 00:58:52,220
하나는 디코더로, 잠재 코드 z를

1537
00:58:52,220 --> 00:58:55,340
입력받아 데이터 x에 대한 분포를 출력합니다.

1538
00:58:55,340 --> 00:58:57,460
다른 하나는 인코더로, 데이터

1539
00:58:57,460 --> 00:59:00,620
x를 입력받아 잠재 코드 z에 대한 분포를

1540
00:59:00,620 --> 00:59:01,755
출력합니다.

1541
00:59:01,755 --> 00:59:04,380
각각은 독립적인 가중치로

1542
00:59:04,380 --> 00:59:07,365
별도로 훈련된 별도의 신경망입니다.

1543
00:59:07,365 --> 00:59:08,740
여러분이 가질 수 있는

1544
00:59:08,740 --> 00:59:11,420
질문은, 신경망에서 어떻게 확률 분포를 출력할

1545
00:59:11,420 --> 00:59:12,960
수 있느냐는 것입니다.

1546
00:59:12,960 --> 00:59:15,900
그것은 혼란스럽고 어렵고 불분명해 보입니다.

1547
00:59:15,900 --> 00:59:18,540
여기서의 요령은 모든 것을 정규 분포로

1548
00:59:18,540 --> 00:59:20,070
강제하는 것입니다.

1549
00:59:20,070 --> 00:59:21,820
신경망이 정규 분포의

1550
00:59:21,820 --> 00:59:24,358
매개변수를 출력하도록 할 것입니다.

1551
00:59:24,358 --> 00:59:25,900
일반적으로 디코더

1552
00:59:25,900 --> 00:59:28,100
네트워크의 경우,

1553
00:59:28,100 --> 00:59:30,180
디코더의 출력 분포가

1554
00:59:30,180 --> 00:59:34,020
대각선 가우시안이라고 가정하며, 대각선의

1555
00:59:34,020 --> 00:59:37,020
항목은 신경망의 픽셀입니다.

1556
00:59:37,020 --> 00:59:38,980
모델은 그 대각선 가우시안 분포의

1557
00:59:38,980 --> 00:59:40,762
평균을 출력할 것입니다.

1558
00:59:40,762 --> 00:59:42,220
일반적으로 디코더의

1559
00:59:42,220 --> 00:59:45,260
경우 고정된 분산 또는 표준 편차 sigma

1560
00:59:45,260 --> 00:59:46,620
제곱을 가정합니다.

1561
00:59:46,620 --> 00:59:49,520
인코더 네트워크도 같은 아이디어입니다.

1562
00:59:49,520 --> 00:59:51,723
모델은 데이터 샘플 x를 입력받습니다.

1563
00:59:51,723 --> 00:59:54,140
그런 다음 x에 주어진 z의

1564
00:59:54,140 --> 00:59:59,340
분포 q를 모델링하는 가우시안 분포의 매개변수를 출력합니다.

1565
00:59:59,340 --> 01:00:01,740
이 경우 인코더 네트워크는

1566
01:00:01,740 --> 01:00:04,700
가우시안 분포의 평균을 나타내는 하나의 벡터와

1567
01:00:04,700 --> 01:00:06,420
그 가우시안 분포의

1568
01:00:06,420 --> 01:00:09,580
공분산 대각선을 나타내는 또 다른 벡터를

1569
01:00:09,580 --> 01:00:10,480
출력합니다.

1570
01:00:10,480 --> 01:00:11,980
여기서 대각선 구조를

1571
01:00:11,980 --> 01:00:15,020
가정하는 것이 매우 중요합니다. 그렇지

1572
01:00:15,020 --> 01:00:19,060
않으면 전체 공분산 행렬에서 h 제곱 항목을 모델링해야

1573
01:00:19,060 --> 01:00:19,820
합니다.

1574
01:00:19,820 --> 01:00:24,180
여기서 h x w 픽셀의 이미지를 상상해 보세요. 이는

1575
01:00:24,180 --> 01:00:26,820
대각선 행렬의 항목이 전체-- 원칙적으로

1576
01:00:26,820 --> 01:00:29,400
이미지를 구성하는 모든 픽셀

1577
01:00:29,400 --> 01:00:31,460
쌍 간의 전체 공분산을 모델링할

1578
01:00:31,460 --> 01:00:33,295
수 있지만, 이는 h

1579
01:00:33,295 --> 01:00:35,420
제곱 w 제곱 항목이 필요하며

1580
01:00:35,420 --> 01:00:36,700
너무 큽니다.

1581
01:00:36,700 --> 01:00:39,380
그래서 대신 서로 다른 값들 간의

1582
01:00:39,380 --> 01:00:41,460
상관 구조를 무시하겠습니다.

1583
01:00:41,460 --> 01:00:44,060
이제 대각선 공분산은 데이터

1584
01:00:44,060 --> 01:00:46,700
자체와 같은 크기의 벡터가 됩니다.

1585
01:00:46,700 --> 01:00:48,460
즉, x에 주어진 z의 mu입니다.

1586
01:00:48,460 --> 01:00:50,620
이 z의 시그마는 x가 주어졌을

1587
01:00:50,620 --> 01:00:52,980
때 z와 동일한 형태의 벡터입니다.

1588
01:00:52,980 --> 01:00:56,900
따라서 우리는 기본적으로 신경망이 동일한 형태의 두 벡터를

1589
01:00:56,900 --> 01:00:58,500
출력하고, 이를 이

1590
01:00:58,500 --> 01:01:01,300
가우시안 분포의 매개변수로 취급합니다.

1591
01:01:01,300 --> 01:01:03,340
그래서 이렇게 신경망에서 분포를

1592
01:01:03,340 --> 01:01:04,860
출력할 수 있습니다.

1593
01:01:04,860 --> 01:01:08,460
고정된 표준 편차로 이 작업에 대해 최대

1594
01:01:08,460 --> 01:01:10,620
우도를 수행하면 실제로

1595
01:01:10,620 --> 01:01:14,043
L2와 동등해지며, 이는 좋은 트릭입니다.

1596
01:01:14,043 --> 01:01:15,460
그렇게 하려는 이유는

1597
01:01:15,460 --> 01:01:19,260
대각선을 모델링하려고 하면, 원칙적으로

1598
01:01:19,260 --> 01:01:21,780
디코더에서 동일한 것을 모델링하고

1599
01:01:21,780 --> 01:01:25,060
각 픽셀의 분산을 따로 모델링할 수 있기

1600
01:01:25,060 --> 01:01:25,960
때문입니다.

1601
01:01:25,960 --> 01:01:28,622
하지만 이는 쓸모가 없을 것입니다.

1602
01:01:28,622 --> 01:01:30,580
픽셀 간의 공분산 구조를 모델링하지

1603
01:01:30,580 --> 01:01:32,163
않는다면, 각

1604
01:01:32,163 --> 01:01:35,500
픽셀이 조금씩 변할 수 있다고 말하는 것이며, 각

1605
01:01:35,500 --> 01:01:37,820
픽셀이 허용되는 변동량은 픽셀에

1606
01:01:37,820 --> 01:01:38,753
따라 다릅니다.

1607
01:01:38,753 --> 01:01:40,420
그런 다음 그 분포에서

1608
01:01:40,420 --> 01:01:43,500
샘플링하는 것은 기본적으로 평균을 고정하고

1609
01:01:43,500 --> 01:01:46,420
각 픽셀의 분산에 따라 조정된 독립적인

1610
01:01:46,420 --> 01:01:48,360
노이즈를 추가하는 것과 같으며,

1611
01:01:48,360 --> 01:01:50,540
이는 합리적인 일이 아닙니다.

1612
01:01:50,540 --> 01:01:55,300
따라서 일반적으로 디코더에서는 조금 속이는 것입니다. 확률 분포를

1613
01:01:55,300 --> 01:01:58,040
출력하는 것처럼 보이지만, 실제로는 그

1614
01:01:58,040 --> 01:01:59,220
분포에서 샘플링하지

1615
01:01:59,220 --> 01:02:00,178
않을 것입니다.

1616
01:02:00,178 --> 01:02:02,380
대신 우리는 항상 평균을 출력할 것입니다.

1617
01:02:02,380 --> 01:02:03,760
이해가 되나요?

1618
01:02:03,760 --> 01:02:04,260
네.

1619
01:02:04,260 --> 01:02:06,960
그리고 이 내용을 적어보면, 그

1620
01:02:06,960 --> 01:02:09,020
상수 시그마 제곱은 앞쪽에서

1621
01:02:09,020 --> 01:02:10,420
상수로 나옵니다.

1622
01:02:10,420 --> 01:02:14,380
실제로 고정된 대각선 분산을 가진 가우시안 분포의

1623
01:02:14,380 --> 01:02:18,807
로그 우도를 최대화하는 것은 평균과 x 간의 L2

1624
01:02:18,807 --> 01:02:21,140
거리를 최소화하는 것과

1625
01:02:21,140 --> 01:02:23,163
동등합니다. 이는 좋습니다.

1626
01:02:23,163 --> 01:02:24,080
네, 좋은 질문입니다.

1627
01:02:24,080 --> 01:02:27,700
픽셀 이동에 대한 이상한 불변성 또는

1628
01:02:27,700 --> 01:02:29,818
비불변성 구조가 있나요?

1629
01:02:29,818 --> 01:02:31,860
이는 신경망을 구축하기 위해 선택한

1630
01:02:31,860 --> 01:02:34,180
아키텍처의 속성에 더 가깝습니다.

1631
01:02:34,180 --> 01:02:37,452
이러한 예측을 하는 네트워크 아키텍처를 구축할 수

1632
01:02:37,452 --> 01:02:38,160
있습니다.

1633
01:02:38,160 --> 01:02:41,540
아키텍처에 불변성 또는 동등성 속성을

1634
01:02:41,540 --> 01:02:43,118
구축할 수 있습니다.

1635
01:02:43,118 --> 01:02:45,660
하지만 일반적으로 이는 손실 수준에서

1636
01:02:45,660 --> 01:02:47,320
고려되지 않는 것이 맞습니다.

1637
01:02:50,180 --> 01:02:51,800
이제 우리는 이 아이디어를 갖게 되었습니다.

1638
01:02:51,800 --> 01:02:53,683
인코더와 디코더가 있습니다.

1639
01:02:53,683 --> 01:02:55,100
하나는 x를 입력하고

1640
01:02:55,100 --> 01:02:59,160
z에 대한 분포를 출력하며, 다른 하나는 x에 대한 분포를 입력합니다.

1641
01:02:59,160 --> 01:03:00,660
우리의 훈련 목표는 무엇인가요?

1642
01:03:00,660 --> 01:03:03,380
여기서 우리는 수학을 할 슬라이드가 하나 있습니다.

1643
01:03:03,380 --> 01:03:04,820
하지만 볼 것입니다.

1644
01:03:04,820 --> 01:03:06,020
여기서 우리는

1645
01:03:06,020 --> 01:03:08,435
기본적으로 최대 우도를 수행하려고 합니다.

1646
01:03:08,435 --> 01:03:10,060
이는 일반적으로

1647
01:03:10,060 --> 01:03:11,580
생성 모델링의 많은

1648
01:03:11,580 --> 01:03:14,620
목표 뒤에 있는 안내 원칙입니다.

1649
01:03:14,620 --> 01:03:17,185
따라서 우리는 log p of x를 최대화하려고 합니다.

1650
01:03:17,185 --> 01:03:19,060
그런 다음 베이즈 규칙을 사용하여

1651
01:03:19,060 --> 01:03:21,980
이를 베이즈 규칙 표현의 log p로 쓸 수 있습니다.

1652
01:03:21,980 --> 01:03:24,460
이는 정확한 동등성입니다.

1653
01:03:24,460 --> 01:03:26,400
이제 우리는 어리석은 일을 할 것입니다.

1654
01:03:26,400 --> 01:03:28,500
우리는 이것의 위와 아래에 q of z

1655
01:03:28,500 --> 01:03:30,015
given x를 곱할 것입니다.

1656
01:03:30,015 --> 01:03:32,140
기억하세요, 우리는 x가 주어졌을 때

1657
01:03:32,140 --> 01:03:35,660
z의 다른 분포 q를 모델링하는 또 다른 신경망 q를 갑자기

1658
01:03:35,660 --> 01:03:36,667
도입했습니다.

1659
01:03:36,667 --> 01:03:38,500
이제 우리는 이 베이즈

1660
01:03:38,500 --> 01:03:42,410
규칙 표현의 위와 아래에 밀도 항을 곱할 것입니다.

1661
01:03:42,410 --> 01:03:44,450
이제 로그 수학을 수행할 것입니다.

1662
01:03:44,450 --> 01:03:49,650
그리고 만약 당신이 어떤 예지력을 가지고 있다면, 특정한 순서로 이 항들을

1663
01:03:49,650 --> 01:03:52,078
재배열하기로 결정할 것입니다.

1664
01:03:52,078 --> 01:03:54,370
그래서 제가 색상을 코드화했으니 나중에 어떤 항이

1665
01:03:54,370 --> 01:03:55,810
어디로 갔는지 추적할 수 있습니다.

1666
01:03:55,810 --> 01:03:58,690
하지만 우리는 로그를 수행하고 이것을

1667
01:03:58,690 --> 01:04:01,370
세 개의 개별 항으로 나눕니다.

1668
01:04:01,370 --> 01:04:04,690
이제 당신은 또 다른 마법 같은 관찰을 해야

1669
01:04:04,690 --> 01:04:09,050
합니다. 즉, 이 p(x)는 실제로 z에 의존하지 않습니다.

1670
01:04:09,050 --> 01:04:11,490
지금까지 이 세 개의 항의 순서는

1671
01:04:11,490 --> 01:04:12,890
모두 정확히 동등합니다.

1672
01:04:12,890 --> 01:04:14,510
이들은 모두 정확한 동등식입니다.

1673
01:04:14,510 --> 01:04:16,565
그래서 이 표현에 z가 있지만, 실제로는

1674
01:04:16,565 --> 01:04:18,690
z에 의존하지 않습니다. 왜냐하면 모든

1675
01:04:18,690 --> 01:04:20,170
z가 서로 상쇄되기 때문입니다.

1676
01:04:20,170 --> 01:04:22,310
z에 의존하지 않는 것이 있다면,

1677
01:04:22,310 --> 01:04:25,850
항상 그 것에 대해 z에 대한 기대값을 감쌀

1678
01:04:25,850 --> 01:04:26,770
수 있습니다.

1679
01:04:26,770 --> 01:04:29,250
그래서 이 경우, 우리는 이것이 우리의 p(x)라는 것을 알고 있습니다.

1680
01:04:29,250 --> 01:04:33,090
우리는 항상 p(x)의 원하는 분포에

1681
01:04:33,090 --> 01:04:37,410
따라 샘플링된 z의 기대값을 감쌀 수 있습니다.

1682
01:04:37,410 --> 01:04:39,870
그리고 그 내부의 것은 z에

1683
01:04:39,870 --> 01:04:43,010
의존하지 않기 때문에, 우리가 이 기대값을 취할

1684
01:04:43,010 --> 01:04:46,582
수 있는 어떤 분포에 대해서도 항상 참입니다.

1685
01:04:46,582 --> 01:04:50,393
그래서 기대값이 선형적이기 때문에, 우리는 이 세

1686
01:04:50,393 --> 01:04:52,810
개의 항 각각에 기대값을 적용할

1687
01:04:52,810 --> 01:04:53,850
수 있습니다.

1688
01:04:53,850 --> 01:04:57,090
이제 우리는 각각 매우 신비롭게 보이는 이 세

1689
01:04:57,090 --> 01:04:58,970
개의 항을 가지고 있습니다.

1690
01:04:58,970 --> 01:05:02,343
하지만 확률에 대한 많은 직관이 있다면, 이전

1691
01:05:02,343 --> 01:05:04,010
통계나 확률 과정에서 본

1692
01:05:04,010 --> 01:05:06,610
모든 공식을 암기했을 것이고, 아마도

1693
01:05:06,610 --> 01:05:10,170
이 중 일부를 인식하는 법을 배울 수 있을 것입니다.

1694
01:05:10,170 --> 01:05:13,490
그래서 첫 번째 항은 이전과 같이 그대로 내려갈 것입니다.

1695
01:05:13,490 --> 01:05:18,270
그리고 두 번째 두 항은 실제로 KL 항입니다.

1696
01:05:18,270 --> 01:05:20,370
KL 발산은 확률 분포

1697
01:05:20,370 --> 01:05:23,350
간의 비유사성을 측정하는 척도입니다.

1698
01:05:23,350 --> 01:05:25,850
그리고 우연히도 이 후자의 두 항에 대한

1699
01:05:25,850 --> 01:05:27,730
정확한 정의를 가지고 있습니다.

1700
01:05:27,730 --> 01:05:30,970
그래서 우리는 이것을 첫 번째 항으로 정확히 다시 쓸 수 있습니다. 이것은

1701
01:05:30,970 --> 01:05:33,610
기대값 blah, blah, blah, 우리는 그것에 대해

1702
01:05:33,610 --> 01:05:36,810
이야기할 것입니다. 그리고 그 다음에 이 두 개의 KL 항을 더합니다.

1703
01:05:36,810 --> 01:05:40,770
이 두 KL 항은 기본적으로 우리가 이 슬라이드에서

1704
01:05:40,770 --> 01:05:42,730
떠돌고 있는 서로 다른

1705
01:05:42,730 --> 01:05:44,970
확률 분포 간의 불일치 또는

1706
01:05:44,970 --> 01:05:47,330
비유사성을 측정하고 있습니다.

1707
01:05:47,330 --> 01:05:49,610
이제 이 모든 것이 미친 것처럼 보입니다.

1708
01:05:49,610 --> 01:05:51,750
하지만 이 각 항을 주의 깊게

1709
01:05:51,750 --> 01:05:55,730
살펴보면, 실제로 이 세 개의 항 각각에 대한 해석 가능한

1710
01:05:55,730 --> 01:05:57,730
의미를 회복할 수 있습니다.

1711
01:05:57,730 --> 01:06:00,290
첫 번째 항은 실제로 데이터 재구성 항입니다.

1712
01:06:00,290 --> 01:06:02,090
이것이 무엇을 말하는지 살펴보면,

1713
01:06:02,090 --> 01:06:04,110
우리는 z를 샘플링할 것이라는 것입니다.

1714
01:06:04,110 --> 01:06:08,970
z를 샘플링하는 방법은 x가 주어졌을 때 q(z)로,

1715
01:06:08,970 --> 01:06:11,230
이것이 우리의 인코더입니다.

1716
01:06:11,230 --> 01:06:13,750
그래서 우리는 x를 가져와 인코더에 전달할 것입니다.

1717
01:06:13,750 --> 01:06:17,267
인코더는 x가 주어졌을 때 z의 분포 q를 예측할 것입니다.

1718
01:06:17,267 --> 01:06:18,850
그런 다음 그 예측된 분포에서

1719
01:06:18,850 --> 01:06:20,250
z를 샘플링할 것입니다.

1720
01:06:20,250 --> 01:06:23,130
그런 다음 우리는 모든 z에 대해 기대값을 취하고

1721
01:06:23,130 --> 01:06:26,170
z가 주어졌을 때 x의 로그 확률을 극대화할 것입니다.

1722
01:06:26,170 --> 01:06:28,450
그래서 이것은 기본적으로 데이터 재구성 항입니다.

1723
01:06:28,450 --> 01:06:31,550
이는 우리가 x, 데이터 포인트 x를 가져와

1724
01:06:31,550 --> 01:06:33,890
인코더를 통해 z에 대한 분포를

1725
01:06:33,890 --> 01:06:38,170
얻고, 그 예측된 z 분포의 샘플을 디코더에 전달하면

1726
01:06:38,170 --> 01:06:40,550
x를 복원할 것이라는 의미입니다.

1727
01:06:40,550 --> 01:06:43,490
그래서 이것은 데이터 재구성 항입니다.

1728
01:06:43,490 --> 01:06:45,270
가운데 항은 사전 항입니다.

1729
01:06:45,270 --> 01:06:46,850
이는 x가 주어진 z에

1730
01:06:46,850 --> 01:06:49,930
대한 q와 z에 대한 p 사이의 KL 발산을 측정하고자

1731
01:06:49,930 --> 01:06:51,450
한다는 의미입니다.

1732
01:06:51,450 --> 01:06:54,810
x가 주어진 z에 대한 q, 즉 인코더는

1733
01:06:54,810 --> 01:06:58,410
데이터 x를 입력받아 잠재 공간 z에

1734
01:06:58,410 --> 01:07:00,410
대한 분포를 출력합니다.

1735
01:07:00,410 --> 01:07:02,690
그래서 이것은 인코더의 잠재 공간에

1736
01:07:02,690 --> 01:07:04,030
대한 예측된 분포입니다.

1737
01:07:04,030 --> 01:07:06,338
그리고 이 다른 항 p of z는 사전입니다.

1738
01:07:06,338 --> 01:07:08,630
이것은 우리가 잠재 공간에 대해 가정한 사전으로,

1739
01:07:08,630 --> 01:07:10,030
일반적으로 대각선 가우시안입니다.

1740
01:07:10,030 --> 01:07:11,790
그래서 이 항은 기본적으로

1741
01:07:11,790 --> 01:07:14,130
모델이 x를 주어 z의 분포를

1742
01:07:14,130 --> 01:07:17,070
별도로 예측하고 있으며, 우리는 그 예측된

1743
01:07:17,070 --> 01:07:18,850
분포가 우리가 이전에

1744
01:07:18,850 --> 01:07:20,810
설정한 간단한 가우시안

1745
01:07:20,810 --> 01:07:23,830
사전과 일치하기를 원한다는 것을 의미합니다.

1746
01:07:23,830 --> 01:07:25,290
그래서 이것은 우리 모델이

1747
01:07:25,290 --> 01:07:27,650
학습한 잠재 공간이 사전과 얼마나

1748
01:07:27,650 --> 01:07:29,290
일치하는지를 측정하고 있습니다.

1749
01:07:29,290 --> 01:07:31,770
그리고 이 세 번째 항이 문제를 일으킵니다.

1750
01:07:31,770 --> 01:07:34,330
이 세 번째 항은 x가 주어진 z에

1751
01:07:34,330 --> 01:07:38,050
대한 q로, 이는 인코더에 입력 데이터 x가 주어졌을

1752
01:07:38,050 --> 01:07:41,970
때 z에 대한 예측된 분포이며, 그것이 x가 주어진

1753
01:07:41,970 --> 01:07:44,330
p of z와 얼마나 일치합니까?

1754
01:07:44,330 --> 01:07:47,530
그래서 이것은 디코더가 모델링하는 분포를

1755
01:07:47,530 --> 01:07:49,210
뒤집은 것입니다.

1756
01:07:49,210 --> 01:07:50,990
그리고 우리는 운이 없습니다.

1757
01:07:50,990 --> 01:07:53,490
우리는 이 항을 계산할 수 없습니다. 왜냐하면 처음에

1758
01:07:53,490 --> 01:07:56,270
문제를 일으킨 것이 p of z given x였기 때문입니다.

1759
01:07:56,270 --> 01:08:00,330
우리가 q를 도입한 이유는 p of z given

1760
01:08:00,330 --> 01:08:03,530
x를 계산할 수 없었기 때문입니다.

1761
01:08:03,530 --> 01:08:06,250
그럼 이제 우리는 무엇을 해야 할까요?

1762
01:08:06,250 --> 01:08:07,610
우리는 그것을 버릴 것입니다.

1763
01:08:07,610 --> 01:08:10,090
왜냐하면 KL 발산은 항상 0보다 크거나

1764
01:08:10,090 --> 01:08:12,770
같다는 것을 알고 있기 때문에, 이 마지막 항은

1765
01:08:12,770 --> 01:08:15,167
두 분포의 KL 발산이기 때문에, 비록

1766
01:08:15,167 --> 01:08:17,750
우리가 그 분포를 계산할 수는 없지만, 일반적으로

1767
01:08:17,750 --> 01:08:19,450
그것이 0보다 커야 한다는

1768
01:08:19,450 --> 01:08:22,410
것은 잘 알려진 KL 발산의 속성이기 때문입니다.

1769
01:08:22,410 --> 01:08:25,850
그래서 우리는 그것을 버리고 진짜 확률에 대한 하한을

1770
01:08:25,850 --> 01:08:27,290
얻을 수 있습니다.

1771
01:08:27,290 --> 01:08:29,189
그래서 마지막 항을 버리면,

1772
01:08:29,189 --> 01:08:32,370
우리는 log p of x가 그 두 항, 즉

1773
01:08:32,370 --> 01:08:35,370
재구성 항과 사전 항보다 크거나 같다는 것을

1774
01:08:35,370 --> 01:08:36,175
알게 됩니다.

1775
01:08:36,175 --> 01:08:38,050
그래서 이것이 우리가 변분 오토인코더를

1776
01:08:38,050 --> 01:08:40,090
훈련하는 데 사용할 손실이 될 것입니다.

1777
01:08:40,090 --> 01:08:43,130
그리고 이 아이디어는 이것이 진짜 로그 가능도에 대한 근사라는

1778
01:08:43,130 --> 01:08:43,830
것입니다.

1779
01:08:43,830 --> 01:08:46,029
이것은 로그 가능도의 하한입니다.

1780
01:08:46,029 --> 01:08:48,010
그래서 하한을 최대화하면, 희망적으로

1781
01:08:48,010 --> 01:08:50,649
그것이 진짜 로그 가능도를 최대화할 것입니다

1782
01:08:50,649 --> 01:08:53,090
비록 우리가 정확하게 하지는 않더라도.

1783
01:08:53,090 --> 01:08:55,870
그래서 이것이 변분 오토인코더의 훈련 목표입니다.

1784
01:08:55,870 --> 01:08:58,710
그래서 이것이 요약입니다.

1785
01:08:58,710 --> 01:09:02,130
당신은 인코더 q와 디코더 p를 공동으로

1786
01:09:02,130 --> 01:09:05,930
훈련하여 진짜 데이터 로그 가능도에 대한 변분 하한을

1787
01:09:05,930 --> 01:09:07,513
최대화할 것입니다.

1788
01:09:07,513 --> 01:09:09,930
그리고 이것은 때때로 증거 하한 또는 ELBo라고도

1789
01:09:09,930 --> 01:09:10,793
불립니다.

1790
01:09:10,793 --> 01:09:11,710
그래서 이것은 단지 ELBo입니다.

1791
01:09:11,710 --> 01:09:13,529
우리는 ELBo를 최대화할 것입니다.

1792
01:09:13,529 --> 01:09:14,950
그리고 이 특정 용어가 있습니다.

1793
01:09:14,950 --> 01:09:17,529
우리는 이 인코더 네트워크와 디코더 네트워크를 가지고 있습니다.

1794
01:09:17,529 --> 01:09:18,649
그게 우리가 하는 일입니다.

1795
01:09:18,649 --> 01:09:21,410
그래서 훈련 절차가 어떻게 진행되는지

1796
01:09:21,410 --> 01:09:24,609
더 명확히 살펴보면, 우리는 이

1797
01:09:24,609 --> 01:09:26,930
신경망 인코더가 x를 입력받아

1798
01:09:26,930 --> 01:09:29,850
z에 대한 분포를 출력하게 하고,

1799
01:09:29,850 --> 01:09:32,130
그 다음에 예측된 분포에

1800
01:09:32,130 --> 01:09:34,520
KL 항을 적용할 것입니다.

1801
01:09:34,520 --> 01:09:35,972
특히, 이것이 예측된

1802
01:09:35,972 --> 01:09:37,680
분포가 단위 가우시안이

1803
01:09:37,680 --> 01:09:39,310
되도록 강제할 것이기

1804
01:09:39,310 --> 01:09:41,560
때문에, 기본적으로 예측된

1805
01:09:41,560 --> 01:09:47,000
평균이 0이 되고 예측된 시그마가 모두 1이 되도록 유도할 것입니다.

1806
01:09:47,000 --> 01:09:50,620
그런 다음 인코더에서 예측된 분포를 얻으면,

1807
01:09:50,620 --> 01:09:53,040
이 소위 재매개변수화 트릭을

1808
01:09:53,040 --> 01:09:55,040
사용하여 그 예측된

1809
01:09:55,040 --> 01:09:57,500
분포에서 샘플링을 할 것입니다.

1810
01:09:57,500 --> 01:10:00,400
그래서 우리는 예측된 분포에서 샘플 z를 추출합니다.

1811
01:10:00,400 --> 01:10:04,840
이 샘플 z를 얻으면, 디코더를 통해 실행하여

1812
01:10:04,840 --> 01:10:08,680
디코더가 예측한 정규 분포를 얻고,

1813
01:10:08,680 --> 01:10:12,520
그 다음 디코더의 출력에 손실의 재구성

1814
01:10:12,520 --> 01:10:14,120
항을 적용합니다.

1815
01:10:14,120 --> 01:10:18,300
이것이 수학적으로 복잡해 보이지만, 실제로는 이

1816
01:10:18,300 --> 01:10:21,000
모델에 대한 훈련 목표가 그렇게

1817
01:10:21,000 --> 01:10:22,833
미친 것은 아닙니다.

1818
01:10:22,833 --> 01:10:25,000
그리고 이 변분 오토인코더는 두

1819
01:10:25,000 --> 01:10:26,417
손실이 매우 흥미로운

1820
01:10:26,417 --> 01:10:29,732
방식으로 서로 싸우기 때문에 매우 흥미롭다고 생각합니다.

1821
01:10:29,732 --> 01:10:31,440
왜냐하면 우리는 기본적으로

1822
01:10:31,440 --> 01:10:34,000
모델이 이 잠재 공간 z를 통해 병목 현상을

1823
01:10:34,000 --> 01:10:36,480
겪도록 강제하고, 이 두 항이 잠재

1824
01:10:36,480 --> 01:10:39,720
공간에서 서로 다른 것을 원하기 때문입니다. 재구성 손실은

1825
01:10:39,720 --> 01:10:41,840
시그마가 0이 되기를 원하고,

1826
01:10:41,840 --> 01:10:46,000
mu x는 각 데이터 x에 대해 다르고 독특한 벡터가 되기를 원합니다.

1827
01:10:46,000 --> 01:10:47,680
그렇다면 우리는 재구성

1828
01:10:47,680 --> 01:10:50,460
목표를 완벽하게 만족시킬 수 있습니다.

1829
01:10:50,460 --> 01:10:54,480
모든 데이터 포인트에 대해 별도의 고유 벡터를 가질 수 있습니다.

1830
01:10:54,480 --> 01:10:56,350
그리고 그 안에는 확률이 없을 것입니다.

1831
01:10:56,350 --> 01:10:58,100
우리는 모든 것을 완벽하게 재구성할 수 있습니다.

1832
01:10:58,100 --> 01:11:00,420
그래서 그것이 재구성 손실이 원하는 것입니다.

1833
01:11:00,420 --> 01:11:02,560
하지만 사전 손실은 시그마가 모두 1이

1834
01:11:02,560 --> 01:11:05,820
되기를 원합니다. 왜냐하면 그것이 단위 가우시안이 되기를 원하고,

1835
01:11:05,820 --> 01:11:07,880
모든 mu가 0이 되기를 원하기 때문입니다.

1836
01:11:07,880 --> 01:11:10,340
이는 두 손실이 원하는 것과 매우 다릅니다.

1837
01:11:10,340 --> 01:11:12,200
그래서 VAE를 훈련하는

1838
01:11:12,200 --> 01:11:14,760
과정에서, 이 두 손실이 서로 싸우도록 하여

1839
01:11:14,760 --> 01:11:18,120
데이터를 잘 재구성하는 것과 잠재 공간이 사전과

1840
01:11:18,120 --> 01:11:20,560
가깝도록 강제하는 것 사이의 균형을

1841
01:11:20,560 --> 01:11:22,092
찾도록 요청하고 있습니다.

1842
01:11:22,092 --> 01:11:23,800
그리고 훈련이 완료되면,

1843
01:11:23,800 --> 01:11:26,440
사전에서 z를 샘플링하고 디코더를 통과시켜

1844
01:11:26,440 --> 01:11:27,840
샘플을 얻을 수 있습니다.

1845
01:11:27,840 --> 01:11:30,520
또한 좋은 점은 잠재 공간이 대각선

1846
01:11:30,520 --> 01:11:32,560
가우시안이었기 때문에,

1847
01:11:32,560 --> 01:11:37,960
잠재 공간 z의 서로 다른 항목 간에 통계적 독립성의 개념이

1848
01:11:37,960 --> 01:11:39,300
있다는 것입니다.

1849
01:11:39,300 --> 01:11:41,040
그래서 그들을 별도로 변화시킬 수 있습니다.

1850
01:11:41,040 --> 01:11:42,920
그리고 아마도 그 개별 차원은

1851
01:11:42,920 --> 01:11:46,280
종종 데이터에 대해 유용하거나 해석 가능하거나 직교적인

1852
01:11:46,280 --> 01:11:47,360
무언가를 인코딩합니다.

1853
01:11:47,360 --> 01:11:49,120
그래서 이 경우, 우리는 VAE를 사용하여

1854
01:11:49,120 --> 01:11:50,860
손으로 쓴 숫자 데이터 세트에서 훈련했습니다.

1855
01:11:50,860 --> 01:11:54,240
그리고 잠재 공간의 두 차원을 변화시키면, 숫자가

1856
01:11:54,240 --> 01:11:58,940
한 범주에서 다른 범주로 부드럽게 변형되는 것을 볼 수 있습니다.

1857
01:11:58,940 --> 01:12:01,760
그리고 이것은 VAE의 꽤 일반적인 특성입니다.

1858
01:12:01,760 --> 01:12:03,600
그래서 오늘은 기본적으로 여기까지입니다.

1859
01:12:03,600 --> 01:12:06,280
우리가 이야기한 내용을 요약하자면, 우리는 감독 학습과 비감독

1860
01:12:06,280 --> 01:12:07,700
학습에 대해 이야기했습니다.

1861
01:12:07,700 --> 01:12:09,600
우리는 이 세 가지 다른 생성 모델의

1862
01:12:09,600 --> 01:12:10,880
종류에 대해 이야기했습니다.

1863
01:12:10,880 --> 01:12:13,400
그리고 우리는 이 생성 모델의 가족 나무의 한

1864
01:12:13,400 --> 01:12:15,000
가지 가지에 대해 이야기했습니다.

1865
01:12:15,000 --> 01:12:18,623
그래서 다음 시간에는 생성 모델의 가족 나무의

1866
01:12:18,623 --> 01:12:20,040
다른 절반에

1867
01:12:20,040 --> 01:12:22,120
대해, 특히 생성적 적대

1868
01:12:22,120 --> 01:12:25,590
신경망과 확산 모델에 대해 이야기할 것입니다.
