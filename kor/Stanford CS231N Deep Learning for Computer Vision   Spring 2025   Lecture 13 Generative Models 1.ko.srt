1
00:00:05,400 --> 00:00:09,260
CS231N 강의 13에 다시 오신 것을 환영합니다.

2
00:00:09,260 --> 00:00:11,480
오늘은 생성 모델에 대해 이야기하겠습니다.

3
00:00:11,480 --> 00:00:14,132
지난 시간에는 자기지도 학습에 대해

4
00:00:14,132 --> 00:00:15,840
이야기했는데, 이는

5
00:00:15,840 --> 00:00:18,360
감독 없이, 라벨 없이 데이터에서

6
00:00:18,360 --> 00:00:22,120
직접 구조를 학습하려는 매우 흥미로운 패러다임입니다.

7
00:00:22,120 --> 00:00:25,277
우리가 지난 시간에 여러 예제로 다룬 자기지도

8
00:00:25,277 --> 00:00:27,360
학습의 전형적인 공식은 라벨 없는

9
00:00:27,360 --> 00:00:29,540
큰 데이터셋을 가진다는 것입니다.

10
00:00:29,540 --> 00:00:30,582
이상적으로는 그냥 이미지들입니다.

11
00:00:30,582 --> 00:00:31,290
이게 아주 좋죠.

12
00:00:31,290 --> 00:00:32,479
많은 이미지를 얻을 수 있습니다.

13
00:00:32,479 --> 00:00:34,729
이 이미지들을 인코더에 넣어

14
00:00:34,729 --> 00:00:37,300
이미지에서 특징 표현을 추출하고, 그 다음

15
00:00:37,300 --> 00:00:40,080
디코더를 통해 그 특징 표현으로부터

16
00:00:40,080 --> 00:00:41,642
무언가를 예측하게 됩니다.

17
00:00:41,642 --> 00:00:43,600
자기지도 학습의 핵심은 인간의

18
00:00:43,600 --> 00:00:45,800
주석이나 라벨 없이 이 전체

19
00:00:45,800 --> 00:00:47,360
시스템을 학습할 수 있는

20
00:00:47,360 --> 00:00:51,200
사전 과제(pretext task)를 고안하는 것입니다.

21
00:00:51,200 --> 00:00:52,960
그래서 우리는 지난

22
00:00:52,960 --> 00:00:55,160
시간에 회전 같은 여러 사전

23
00:00:55,160 --> 00:00:59,560
과제들을 자기지도 학습 목표로 활용할 수 있다고

24
00:00:59,560 --> 00:01:00,560
이야기했습니다.

25
00:01:00,560 --> 00:01:03,440
보통 이 과정은 두 단계로

26
00:01:03,440 --> 00:01:05,200
진행되는데, 먼저 모든

27
00:01:05,200 --> 00:01:07,320
데이터를 이용해

28
00:01:07,320 --> 00:01:10,340
자기지도 인코더-디코더를 학습합니다.

29
00:01:10,340 --> 00:01:11,715
그 다음 디코더를

30
00:01:11,715 --> 00:01:15,160
버리고, 새롭고 아마도 작은 완전 연결

31
00:01:15,160 --> 00:01:17,400
네트워크를 넣어, 끝까지

32
00:01:17,400 --> 00:01:18,860
학습하거나 작은

33
00:01:18,860 --> 00:01:21,520
라벨된 과제에서 완전 연결

34
00:01:21,520 --> 00:01:23,120
네트워크만 학습합니다.

35
00:01:23,120 --> 00:01:25,920
여기서 아이디어는 자기지도 학습의

36
00:01:25,920 --> 00:01:28,940
사전 과제를 통해 수백만, 수억, 수십억

37
00:01:28,940 --> 00:01:31,200
개의 고품질 인간 라벨이 없는

38
00:01:31,200 --> 00:01:33,980
데이터를 학습할 수 있다는 겁니다.

39
00:01:33,980 --> 00:01:35,940
이 과정에서 이미지나 데이터의

40
00:01:35,940 --> 00:01:38,315
일반적인 구조에 대해 뭔가를

41
00:01:38,315 --> 00:01:39,600
학습하게 됩니다.

42
00:01:39,600 --> 00:01:42,400
그리고 그 지식을 소량의 인간 라벨이 있는 다운스트림

43
00:01:42,400 --> 00:01:44,310
과제로 전이할 수 있습니다.

44
00:01:44,310 --> 00:01:46,560
그래서 자기지도

45
00:01:46,560 --> 00:01:48,893
학습에서 우리가

46
00:01:48,893 --> 00:01:51,762
목표로 하는

47
00:01:51,762 --> 00:01:53,720
전형적인 설정은

48
00:01:53,720 --> 00:01:56,440
인터넷에서 얻은

49
00:01:56,440 --> 00:01:59,860
10억 개의 라벨

50
00:01:59,860 --> 00:02:02,040
없는 이미지로

51
00:02:02,040 --> 00:02:03,440
학습하고,

52
00:02:03,440 --> 00:02:05,960
그 다음 우리가 관심 있는 특정 과제에

53
00:02:05,960 --> 00:02:08,280
대해 수십, 수백, 수천 개의 예제를

54
00:02:08,280 --> 00:02:10,853
라벨링해서 그 과제에 전이하는 것입니다.

55
00:02:10,853 --> 00:02:13,520
이때 이 과제들은 자기지도 사전 과제를

56
00:02:13,520 --> 00:02:17,080
통해 학습한 일반적인 지식으로 성능이 향상되길 바랍니다.

57
00:02:17,080 --> 00:02:18,640
지난 시간에

58
00:02:18,640 --> 00:02:21,860
회전, 재배열, 재구성

59
00:02:21,860 --> 00:02:23,995
같은 여러 사전

60
00:02:23,995 --> 00:02:26,120
과제에 대해

61
00:02:26,120 --> 00:02:27,950
이야기했습니다.

62
00:02:27,950 --> 00:02:30,200
이들은 모두 입력 픽셀에 기하학적 변형을 가하고,

63
00:02:30,200 --> 00:02:32,720
모델이 그 변형을 복구하도록 요구하는 방식입니다.

64
00:02:32,720 --> 00:02:35,525
예를 들어 회전의 경우

65
00:02:35,525 --> 00:02:37,400
이미지를 회전시키고

66
00:02:37,400 --> 00:02:38,960
모델이 얼마나

67
00:02:38,960 --> 00:02:40,880
회전했는지

68
00:02:40,880 --> 00:02:42,343
예측하게 합니다.

69
00:02:42,343 --> 00:02:43,760
재배열이나 퍼즐

70
00:02:43,760 --> 00:02:45,677
맞추기에서는 이미지를 여러 조각으로

71
00:02:45,677 --> 00:02:48,080
나누고, 모델이 원래 이미지에서

72
00:02:48,080 --> 00:02:50,360
조각들의 상대적 배치를 예측하게 합니다.

73
00:02:50,360 --> 00:02:52,000
재구성에서는 입력 이미지의 일부를 삭제하고, 모델이 이를 채우도록 하는
인페인팅 또는 재구성 과제입니다.

74
00:02:52,000 --> 00:02:54,680
이 방법들은

75
00:02:54,680 --> 00:02:57,180
꽤

76
00:02:57,180 --> 00:02:58,800
성공적입니다.

77
00:02:58,800 --> 00:03:01,312
또한 지난 시간에 매우 성공적인 자기지도

78
00:03:01,312 --> 00:03:03,020
학습의 다른 공식인

79
00:03:03,020 --> 00:03:04,687
대조 학습(contrastive

80
00:03:04,687 --> 00:03:07,960
learning)에 대해서도 이야기했습니다.

81
00:03:07,960 --> 00:03:10,760
시간이 부족해 몇 가지 후속

82
00:03:10,760 --> 00:03:12,937
방법을 다루지 못했다고

83
00:03:12,937 --> 00:03:14,520
들었는데, 오늘

84
00:03:14,520 --> 00:03:16,600
강의 초반에 간단히

85
00:03:16,600 --> 00:03:18,300
다루고자 합니다.

86
00:03:18,300 --> 00:03:19,920
대조 학습의 아이디어는 유사한

87
00:03:19,920 --> 00:03:22,200
쌍과 비유사한 쌍을 얻어, 유사한 쌍은 가까이,

88
00:03:22,200 --> 00:03:24,380
비유사한 쌍은 멀리 하도록 학습하는 것입니다.

89
00:03:24,380 --> 00:03:27,020
자기지도 학습 맥락에서 보통 이렇게 합니다: 입력 이미지로 시작합니다.

90
00:03:27,020 --> 00:03:28,420
이 이미지들은 라벨이 없습니다.

91
00:03:28,420 --> 00:03:29,920
각 입력 이미지에

92
00:03:29,920 --> 00:03:32,580
두 가지 무작위 변환을 적용합니다.

93
00:03:32,580 --> 00:03:34,720
예를 들어 고양이의 경우 고양이 얼굴

94
00:03:34,720 --> 00:03:36,800
주변을 한 번 크롭하고, 고양이

95
00:03:36,800 --> 00:03:38,440
뒷부분을 또 한 번 크롭했습니다.

96
00:03:38,440 --> 00:03:41,520
원숭이의 경우 원숭이 얼굴 주변을 한 번 크롭하고,

97
00:03:41,520 --> 00:03:44,600
흑백으로 변환하는 등 여러 변환을 적용했습니다.

98
00:03:44,600 --> 00:03:46,700
기본적으로 입력 이미지

99
00:03:46,700 --> 00:03:49,860
각각에 대해 두 개 또는 그 이상, 하지만

100
00:03:49,860 --> 00:03:53,480
두 개가 적당한 최소 집합인 두 개의

101
00:03:53,480 --> 00:03:55,360
무작위 변형을 적용합니다.

102
00:03:55,360 --> 00:03:58,230
그런 다음 이렇게 무작위로 변형된

103
00:03:58,230 --> 00:04:02,110
모든 입력 데이터를 VIT, CNN, 또는

104
00:04:02,110 --> 00:04:05,070
이미지를 입력받아 특징 표현을 출력할

105
00:04:05,070 --> 00:04:08,230
수 있는 어떤 신경망에 넣습니다.

106
00:04:08,230 --> 00:04:11,190
그다음에는 contrastive 개념을 적용하고자 합니다.

107
00:04:11,190 --> 00:04:14,270
고양이에서 나온 두 가지 증강 각각에

108
00:04:14,270 --> 00:04:17,149
대해 두 특징 벡터가 같아야 하므로

109
00:04:17,149 --> 00:04:19,329
초록색으로 표시합니다.

110
00:04:19,329 --> 00:04:22,630
기본적으로 큰 n 제곱 유사도

111
00:04:22,630 --> 00:04:23,230
행렬을

112
00:04:23,230 --> 00:04:28,910
계산하는데, 음... 2n 제곱, 즉 4n 제곱이 됩니다.

113
00:04:28,910 --> 00:04:31,710
n개의 이미지에 각각 두 개의

114
00:04:31,710 --> 00:04:38,150
변형을 적용하면, 이렇게 변형된 모든 증강 샘플에 대해 2n 곱하기 2n

115
00:04:38,150 --> 00:04:40,590
크기의 거대한 행렬이 생깁니다.

116
00:04:40,590 --> 00:04:44,930
이제 기본적으로 원본 이미지에서 나온 두 증강을

117
00:04:44,930 --> 00:04:47,870
서로 가깝게 당기고자 합니다.

118
00:04:47,870 --> 00:04:51,310
그리고 서로 다른 원본 이미지에서 나온

119
00:04:51,310 --> 00:04:53,130
증강 쌍은 서로 멀리

120
00:04:53,130 --> 00:04:54,430
밀어내고자 합니다.

121
00:04:54,430 --> 00:04:57,990
즉, 모든 샘플을 특징 추출기에 통과시키고, 이

122
00:04:57,990 --> 00:05:00,470
특징 벡터들 사이의 스칼라 유사도를

123
00:05:00,470 --> 00:05:03,725
담은 4n 제곱 크기의 행렬을 계산한 뒤,

124
00:05:03,725 --> 00:05:06,350
비슷한 것들은 당기고, 달라야 할

125
00:05:06,350 --> 00:05:08,910
것들은 밀어내는 것이 contrastive

126
00:05:08,910 --> 00:05:12,150
learning의 기본 아이디어입니다.

127
00:05:12,150 --> 00:05:15,190
몇 년 전 이 모든 것을 잘 정리한 논문이

128
00:05:15,190 --> 00:05:17,410
SimCLR인데, 이미지에서

129
00:05:17,410 --> 00:05:19,030
자기지도 표현 학습에 매우

130
00:05:19,030 --> 00:05:20,590
성공적으로 적용한 연구입니다.

131
00:05:20,590 --> 00:05:22,430
아마 지난번에 여러분이

132
00:05:22,430 --> 00:05:24,430
살펴본 논문이기도 합니다.

133
00:05:24,430 --> 00:05:26,590
하지만 SimCLR의 문제점

134
00:05:26,590 --> 00:05:28,390
중 하나는 좋은 수렴을

135
00:05:28,390 --> 00:05:33,327
위해 꽤 큰 배치 크기가 필요하다는 점입니다. 네트워크에게 너무 쉬운

136
00:05:33,327 --> 00:05:34,910
문제이기 때문입니다.

137
00:05:34,910 --> 00:05:36,327
샘플이 많지 않으면 비슷해

138
00:05:36,327 --> 00:05:38,070
보이는 두 고양이 이미지를

139
00:05:38,070 --> 00:05:39,330
구분하는 게 너무 쉽습니다.

140
00:05:39,330 --> 00:05:41,030
그래서 네트워크가 충분한 학습

141
00:05:41,030 --> 00:05:42,610
신호를 받도록 문제를 어렵게

142
00:05:42,610 --> 00:05:44,110
만들려면 꽤 큰 배치 크기가

143
00:05:44,110 --> 00:05:47,052
필요해서 모델이 좋은 특징으로 수렴할 수 있습니다.

144
00:05:47,052 --> 00:05:48,510
그렇게 하면 몇 강 전

145
00:05:48,510 --> 00:05:50,750
이야기했던 대규모 분산 학습 아이디어를

146
00:05:50,750 --> 00:05:53,047
다시 적용해야 하는데, 이는

147
00:05:53,047 --> 00:05:54,130
충분히 가능합니다.

148
00:05:54,130 --> 00:05:55,130
완전히 잘 작동합니다.

149
00:05:55,130 --> 00:05:56,630
하지만 여러분은 이렇게 하지

150
00:05:56,630 --> 00:05:59,550
않고도 해결할 방법이 있을지 궁금할 수 있습니다.

151
00:05:59,550 --> 00:06:01,310
그리고 이것이 제가 너무 자세히 다루고

152
00:06:01,310 --> 00:06:03,198
싶지 않은 몇 가지 접근법으로 이어집니다.

153
00:06:03,198 --> 00:06:04,990
사실 저는 이것들을 하나하나 설명하며 어떻게

154
00:06:04,990 --> 00:06:06,550
작동하는지 정확히 알려드리고 싶지 않습니다.

155
00:06:06,550 --> 00:06:08,550
그저 이 방법들이 존재한다는 것을

156
00:06:08,550 --> 00:06:10,270
알려드리고, 그들이 달성하려는 일반적인

157
00:06:10,270 --> 00:06:11,870
방향성을 전달하고 싶습니다.

158
00:06:11,870 --> 00:06:14,190
그래서 MoCo, 즉 momentum

159
00:06:14,190 --> 00:06:15,970
contrast라는 자기지도 학습 방법에서는

160
00:06:15,970 --> 00:06:18,650
설정이 방금 본 SimCLR과 매우 비슷합니다.

161
00:06:18,650 --> 00:06:19,530
데이터를 가져오고,

162
00:06:19,530 --> 00:06:20,970
증강된 쌍을 만듭니다.

163
00:06:20,970 --> 00:06:22,595
그것들을 feature encoder에 통과시키고,

164
00:06:22,595 --> 00:06:24,220
비슷한 것들은 가까이 모으고,

165
00:06:24,220 --> 00:06:26,670
비슷하지 않은 것들은 멀리 떨어뜨리려고 합니다.

166
00:06:26,670 --> 00:06:28,630
하지만 다른 점은 매 반복마다

167
00:06:28,630 --> 00:06:31,190
엄청나게 큰 배치 크기를 사용할

168
00:06:31,190 --> 00:06:32,650
필요가 없다는 겁니다.

169
00:06:32,650 --> 00:06:38,070
그래서 그들은 이전 학습 반복에서

170
00:06:38,070 --> 00:06:40,950
샘플 q를 유지합니다.

171
00:06:40,950 --> 00:06:42,690
그리고 매 학습 반복마다,

172
00:06:42,690 --> 00:06:46,050
제 x 쿼리는 현재 새로운 배치 데이터입니다.

173
00:06:46,050 --> 00:06:49,810
그리고 이 q, x0, x1, x2 키는

174
00:06:49,810 --> 00:06:51,230
이전 학습

175
00:06:51,230 --> 00:06:54,390
반복에서 본 이전 배치 데이터입니다.

176
00:06:54,390 --> 00:06:56,045
현재 배치 데이터는

177
00:06:56,045 --> 00:06:57,670
항상 하던 대로 인코더

178
00:06:57,670 --> 00:07:01,790
네트워크에 통과시켜서 SimCLR과 같은 방식으로

179
00:07:01,790 --> 00:07:03,750
대조 손실을 계산합니다.

180
00:07:03,750 --> 00:07:07,690
그리고 이 더 큰 q, 이전 배치 기록들은

181
00:07:07,690 --> 00:07:09,190
모멘텀

182
00:07:09,190 --> 00:07:11,815
인코더를 통해 처리해서 특징

183
00:07:11,815 --> 00:07:13,190
표현을 얻고

184
00:07:13,190 --> 00:07:17,710
SimCLR에서 했던 것과 같은 유사도를 계산합니다.

185
00:07:17,710 --> 00:07:18,310
SimCLR에서 했던 것과 같은 유사도를 계산합니다.

186
00:07:18,310 --> 00:07:19,852
문제는 모멘텀 인코더에

187
00:07:19,852 --> 00:07:21,790
역전파를 하지 않는다는 점입니다. 데이터가

188
00:07:21,790 --> 00:07:23,247
너무 많기 때문입니다.

189
00:07:23,247 --> 00:07:24,330
배치 크기가 너무 크기 때문입니다.

190
00:07:24,330 --> 00:07:26,470
GPU 메모리에 그걸 다 올릴 수가 없습니다.

191
00:07:26,470 --> 00:07:30,090
그래서 그 부분에 대해 역전파를 하지 않으려고 하는 거죠.

192
00:07:30,090 --> 00:07:33,830
즉, 두 번째 모멘텀 인코더는 경사

193
00:07:33,830 --> 00:07:36,590
하강법으로 업데이트할 수 없습니다.

194
00:07:36,590 --> 00:07:38,610
대신 좀 특이한 방법을 쓸 겁니다.

195
00:07:38,610 --> 00:07:40,790
모멘텀 인코더가 자체 가중치 집합을

196
00:07:40,790 --> 00:07:42,050
갖도록 할 겁니다.

197
00:07:42,050 --> 00:07:44,332
그 가중치는 경사 하강법으로 학습하지 않습니다.

198
00:07:44,332 --> 00:07:46,790
대신 모멘텀 인코더는 일반 인코더

199
00:07:46,790 --> 00:07:49,070
가중치의 지수 이동 평균이

200
00:07:49,070 --> 00:07:50,335
되도록 할 겁니다.

201
00:07:50,335 --> 00:07:51,710
일반 인코더는 경사

202
00:07:51,710 --> 00:07:52,960
하강법으로 학습합니다.

203
00:07:52,960 --> 00:07:53,990
모든 게 정상적입니다.

204
00:07:53,990 --> 00:07:55,370
순전파나 역전파를 할 겁니다.

205
00:07:55,370 --> 00:07:56,130
그래디언트를 얻고,

206
00:07:56,130 --> 00:07:57,505
일반 인코더에 대해

207
00:07:57,505 --> 00:08:00,670
그래디언트 업데이트를 수행합니다. 이게 일반적인 방식입니다.

208
00:08:00,670 --> 00:08:06,130
그런 다음 모멘텀 인코더 가중치는 현재 가중치에

209
00:08:06,130 --> 00:08:08,470
0.99를 곱해

210
00:08:08,470 --> 00:08:11,110
감쇠시키고, 인코더 가중치의

211
00:08:11,110 --> 00:08:13,100
1%를 더합니다.

212
00:08:13,100 --> 00:08:15,350
그래서 모멘텀 인코더는 인코더

213
00:08:15,350 --> 00:08:18,110
가중치의 지연된 지수 이동 평균이라는

214
00:08:18,110 --> 00:08:20,830
업데이트 규칙을 갖게 됩니다.

215
00:08:20,830 --> 00:08:23,470
왜 이게 정확히 타당한지에 대한 직관이나

216
00:08:23,470 --> 00:08:24,680
설명은 딱히 없지만,

217
00:08:24,680 --> 00:08:26,430
이 방법이 효과가 있다는 강력한

218
00:08:26,430 --> 00:08:27,510
실험적 증거가 있습니다.

219
00:08:27,510 --> 00:08:30,410
이게 현재 상황입니다.

220
00:08:30,410 --> 00:08:32,750
이 방법이 좋은 점은 매 반복마다

221
00:08:32,750 --> 00:08:35,070
엄청나게 큰 부정 샘플 배치를

222
00:08:35,070 --> 00:08:37,270
갖지 않고도 자기 지도

223
00:08:37,270 --> 00:08:39,517
표현을 학습할 수 있다는 겁니다.

224
00:08:39,517 --> 00:08:40,809
이 방법은 꽤 성공적이었고,

225
00:08:40,809 --> 00:08:42,392
이 방향을 발전시킨 후속

226
00:08:42,392 --> 00:08:43,830
논문들이 많았습니다.

227
00:08:43,830 --> 00:08:46,310
알아두셔야 할 또 다른 방법은 DINO입니다.

228
00:08:46,310 --> 00:08:47,930
기본 아이디어는 매우 비슷합니다.

229
00:08:47,930 --> 00:08:50,330
MoCo처럼 경사 하강법으로

230
00:08:50,330 --> 00:08:52,470
학습하는 일반 인코더와 모멘텀

231
00:08:52,470 --> 00:08:54,110
인코더를 사용하는 듀얼

232
00:08:54,110 --> 00:08:55,665
인코더 방식을 씁니다.

233
00:08:55,665 --> 00:08:57,290
하지만 손실 함수는 약간 다릅니다.

234
00:08:57,290 --> 00:09:00,990
소프트맥스 대신 KL 발산 손실을 사용합니다.

235
00:09:00,990 --> 00:09:02,710
이걸 언급하는 이유는

236
00:09:02,710 --> 00:09:05,850
DINO V2가 무엇을 하는지 정확히

237
00:09:05,850 --> 00:09:08,330
다루지 않더라도 존재를 알아두셔야

238
00:09:08,330 --> 00:09:12,790
하기 때문입니다. DINO V2는 요즘 실무에서

239
00:09:12,790 --> 00:09:14,990
많이 쓰이는 강력한 자기

240
00:09:14,990 --> 00:09:16,590
지도 학습 모델입니다.

241
00:09:16,590 --> 00:09:19,070
기본적으로 DINO V1의 방식을

242
00:09:19,070 --> 00:09:21,590
가져왔는데, DINO V1은 MoCo와

243
00:09:21,590 --> 00:09:23,730
비슷하고 SimCLR의 아이디어도

244
00:09:23,730 --> 00:09:26,550
많이 차용했지만 독특한 세부사항도 많았습니다.

245
00:09:26,550 --> 00:09:28,682
DINO V2의 큰 차이점은 학습

246
00:09:28,682 --> 00:09:30,890
데이터를 크게 확장했다는 점입니다.

247
00:09:30,890 --> 00:09:33,112
이전 자기 지도 학습 방법들은 주로

248
00:09:33,112 --> 00:09:35,070
100만 장 이미지인

249
00:09:35,070 --> 00:09:37,510
ImageNet 데이터셋으로 학습했는데,

250
00:09:37,510 --> 00:09:39,150
DINO V2는 약 1억 4천

251
00:09:39,150 --> 00:09:42,090
2백만 장 이미지라는 훨씬 큰 데이터셋으로

252
00:09:42,090 --> 00:09:43,490
성공적으로 확장했습니다.

253
00:09:43,490 --> 00:09:46,210
딥러닝에서는 더 큰 네트워크, 더 많은

254
00:09:46,210 --> 00:09:49,000
데이터, 더 많은 GPU, 더 많은

255
00:09:49,000 --> 00:09:50,460
연산량을 선호합니다.

256
00:09:50,460 --> 00:09:52,500
DINO V2는 이런 큰 데이터셋에

257
00:09:52,500 --> 00:09:54,860
맞춰 자기 지도 학습을 확장하는 레시피를

258
00:09:54,860 --> 00:09:56,900
찾아냈고, 매우 강력한 자기

259
00:09:56,900 --> 00:09:58,200
지도 특징을 제공합니다.

260
00:09:58,200 --> 00:10:00,780
그래서 오늘날 실무에서 특징을

261
00:10:00,780 --> 00:10:03,580
뽑고 이를 미세 조정하거나

262
00:10:03,580 --> 00:10:07,150
감독 학습에 활용할 때 많이 쓰입니다.

263
00:10:07,150 --> 00:10:08,400
다시 말하지만, 이게 어떻게

264
00:10:08,400 --> 00:10:10,400
작동하는지 모든 세부사항을 설명할 필요는 없습니다.

265
00:10:10,400 --> 00:10:11,140
여러분이 다 알기를 기대하지도 않습니다.

266
00:10:11,140 --> 00:10:13,180
하지만 미래에 여러분이 직접

267
00:10:13,180 --> 00:10:15,820
프로젝트에 활용하고 싶을 때를 대비해 이런

268
00:10:15,820 --> 00:10:18,520
기술이 존재한다는 것만은 알아두셨으면 합니다.

269
00:10:18,520 --> 00:10:20,020
그래서 자기지도학습에 대해서는

270
00:10:20,020 --> 00:10:21,700
이 정도만 말씀드리겠습니다.

271
00:10:21,700 --> 00:10:23,700
오늘 강의의 핵심 내용으로

272
00:10:23,700 --> 00:10:25,590
넘어가기 전에 질문 있으신가요?

273
00:10:32,920 --> 00:10:33,420
네.

274
00:10:33,420 --> 00:10:34,780
없으신 것 같네요.

275
00:10:34,780 --> 00:10:38,500
오늘의 주요 주제는 생성 모델입니다.

276
00:10:38,500 --> 00:10:39,580
정말 멋진 주제입니다.

277
00:10:39,580 --> 00:10:41,780
이 분야는 10년 전만 해도

278
00:10:41,780 --> 00:10:45,320
전혀 작동하지 않았지만, 최근 몇 년 사이에 정말 잘

279
00:10:45,320 --> 00:10:47,580
작동하게 된 딥러닝 영역입니다.

280
00:10:47,580 --> 00:10:50,520
이 덕분에 언어 모델 같은 것들이 등장했습니다.

281
00:10:50,520 --> 00:10:52,460
이들은 생성 모델로 볼 수

282
00:10:52,460 --> 00:10:55,340
있는데, 이미지 생성 모델이나 비디오 생성

283
00:10:55,340 --> 00:10:56,960
모델도 마찬가지입니다.

284
00:10:56,960 --> 00:10:59,900
제가 대학원 다닐 때만 해도 이 모델들은 전혀

285
00:10:59,900 --> 00:11:01,220
작동하지 않았습니다.

286
00:11:01,220 --> 00:11:02,860
샘플을 보면 해상도

287
00:11:02,860 --> 00:11:04,600
낮고 완전히 흐릿한

288
00:11:04,600 --> 00:11:06,040
쓰레기처럼 보였죠.

289
00:11:06,040 --> 00:11:08,545
하지만 어딘가 가능성을 엿볼 수 있었습니다.

290
00:11:08,545 --> 00:11:10,420
사람들이 그 흐릿한 결과물에 굴하지

291
00:11:10,420 --> 00:11:11,740
않고 계속 연구를 이어가

292
00:11:11,740 --> 00:11:13,440
지난 10년간 규모를 키워온

293
00:11:13,440 --> 00:11:15,680
덕분에 이제는 많은 기술들이 정말 잘 작동합니다.

294
00:11:15,680 --> 00:11:17,180
정말 흥미로운 일이죠.

295
00:11:17,180 --> 00:11:19,780
이 분야는 딥러닝의 한 영역인데, 이 수업을

296
00:11:19,780 --> 00:11:22,100
처음 가르칠 때는 전혀 작동하지

297
00:11:22,100 --> 00:11:25,980
않았습니다. 그런데 지금은 잘 작동한다는 점이 정말 멋집니다.

298
00:11:25,980 --> 00:11:28,260
하지만 그렇다고 해도, 생성 모델링에 관한

299
00:11:28,260 --> 00:11:30,820
많은 기본 아이디어들은 사실 그대로 유지되어 왔습니다.

300
00:11:30,820 --> 00:11:34,100
데이터를 어떻게 생각하고, 모델링하는

301
00:11:34,100 --> 00:11:36,700
접근법에 관한 아이디어들, 많은

302
00:11:36,700 --> 00:11:41,340
수학적 기본 원리들은 지난 10년간 크게 변하지 않았습니다.

303
00:11:41,340 --> 00:11:44,460
변한 것은 더 많은 계산 능력, 더 안정적인

304
00:11:44,460 --> 00:11:47,340
학습 방법, 더 큰 데이터셋,

305
00:11:47,340 --> 00:11:48,380
분산 학습입니다.

306
00:11:48,380 --> 00:11:51,000
그리고 이 모든 것을 더 유용한 작업으로 확장할 수

307
00:11:51,000 --> 00:11:52,792
있는 능력이 지난 10년간의 발전을

308
00:11:52,792 --> 00:11:53,980
이끌었다고 생각합니다.

309
00:11:53,980 --> 00:11:55,980
알고리즘적인 약간의 조정도 있었는데,

310
00:11:55,980 --> 00:12:00,540
특히 다음 강의에서 diffusion 모델에 대해 이야기할 때 볼 수
있습니다.

311
00:12:00,540 --> 00:12:02,880
하지만 먼저, 생성 모델링에 대해

312
00:12:02,880 --> 00:12:05,380
이야기하기 전에, 감독 학습과 비감독 학습에

313
00:12:05,380 --> 00:12:08,060
대해 조금 뒤로 물러나서 이야기하고 싶습니다.

314
00:12:08,060 --> 00:12:10,060
딥러닝에서 접근하려는 몇 가지

315
00:12:10,060 --> 00:12:12,100
다른 작업들이 있는데, 이들은 때때로

316
00:12:12,100 --> 00:12:14,180
몇 가지 서로 직교하는 축을

317
00:12:14,180 --> 00:12:15,587
따라 나눌 수 있습니다.

318
00:12:15,587 --> 00:12:17,420
그래서 용어와 명칭을 명확히

319
00:12:17,420 --> 00:12:20,540
하기 위해 그 부분에 대해 조금 이야기하고자 합니다.

320
00:12:20,540 --> 00:12:22,340
감독 학습은 지난 강의를

321
00:12:22,340 --> 00:12:25,900
제외하고 이번 학기 내내 주로 해온 학습 방법입니다.

322
00:12:25,900 --> 00:12:29,317
감독 학습에서는 x와 y 쌍으로 이루어진 데이터셋이 있습니다.

323
00:12:29,317 --> 00:12:30,900
목표는 입력 데이터 x에서

324
00:12:30,900 --> 00:12:34,620
타깃 또는 레이블 y로 매핑하는 함수를 배우는 것입니다.

325
00:12:34,620 --> 00:12:38,180
지금까지 이 접근법의 많은 예를 보았습니다.

326
00:12:38,180 --> 00:12:40,380
예를 들어 이미지 분류에서는 입력 x가

327
00:12:40,380 --> 00:12:41,240
이미지입니다.

328
00:12:41,240 --> 00:12:45,360
출력 y는 레이블이 되거나 이미지 캡셔닝의 경우입니다.

329
00:12:45,360 --> 00:12:47,160
입력 x는 이미지가 됩니다.

330
00:12:47,160 --> 00:12:49,700
출력 y는 그 이미지에서 우리가 보는 것을 설명하는

331
00:12:49,700 --> 00:12:50,940
텍스트 조각이 될 것입니다.

332
00:12:50,940 --> 00:12:51,780
객체 탐지입니다.

333
00:12:51,780 --> 00:12:52,840
입력은 이미지입니다.

334
00:12:52,840 --> 00:12:54,740
출력은 이미지에 나타나는 객체를

335
00:12:54,740 --> 00:12:57,260
설명하는 박스와 카테고리 레이블 세트입니다.

336
00:12:57,260 --> 00:12:58,160
또는 분할입니다.

337
00:12:58,160 --> 00:13:02,550
입력 이미지의 모든 픽셀에 레이블을 할당할 수도 있습니다.

338
00:13:02,550 --> 00:13:04,300
이것들은 감독 학습 문제입니다.

339
00:13:04,300 --> 00:13:06,298
왜냐하면 예측하려는 작업, 즉

340
00:13:06,298 --> 00:13:08,340
예측하고자 하는 것이 데이터 세트에 정확히

341
00:13:08,340 --> 00:13:09,440
있기 때문입니다.

342
00:13:09,440 --> 00:13:11,920
어떤 의미에서는, 훈련 데이터

343
00:13:11,920 --> 00:13:13,563
세트에서 x에서 y로의

344
00:13:13,563 --> 00:13:14,980
매핑을 모방하는 함수를

345
00:13:14,980 --> 00:13:17,860
학습하고, 그 매핑을 새로운 샘플에도

346
00:13:17,860 --> 00:13:20,180
일반화하는 것만 하면 됩니다.

347
00:13:20,180 --> 00:13:22,540
반면, 비지도 학습은 좀 더

348
00:13:22,540 --> 00:13:26,620
복잡하고 신비하며 설명하기 어려운 부분입니다.

349
00:13:26,620 --> 00:13:28,600
하지만 비지도 학습 또는 때때로

350
00:13:28,600 --> 00:13:30,400
자기지도 학습의 아이디어는

351
00:13:30,400 --> 00:13:32,680
레이블이 없고 데이터만 있다는 것입니다.

352
00:13:32,680 --> 00:13:34,340
샘플 x만 있습니다.

353
00:13:34,340 --> 00:13:35,620
이미지만 있습니다.

354
00:13:35,620 --> 00:13:38,620
그리고 그 데이터에서 어떤 구조를 학습하려는 겁니다.

355
00:13:38,620 --> 00:13:41,200
특정한 작업을 반드시 목표로 하는 것은 아닙니다.

356
00:13:41,200 --> 00:13:43,480
그저 그 모든 데이터에서 좋은 표현과

357
00:13:43,480 --> 00:13:45,820
좋은 구조를 발견하려는 것입니다.

358
00:13:45,820 --> 00:13:46,500
왜냐하면?

359
00:13:46,500 --> 00:13:49,020
자기지도 학습에서 자주 이야기했듯이,

360
00:13:49,020 --> 00:13:52,340
나중에 다운스트림 작업에 적용할 수 있기 때문입니다.

361
00:13:52,340 --> 00:13:55,380
하지만 비지도 학습 자체의 작업은

362
00:13:55,380 --> 00:13:58,380
종종 명확하게 정의되어 있지 않습니다.

363
00:13:58,380 --> 00:14:00,400
예를 들어 K-평균

364
00:14:00,400 --> 00:14:02,820
클러스터링은 데이터에서 클러스터를

365
00:14:02,820 --> 00:14:06,060
식별하려는 시도입니다. 이는 레이블이

366
00:14:06,060 --> 00:14:10,300
없더라도 원시 픽셀에서 구조를 살펴볼 수 있는 것입니다.

367
00:14:10,300 --> 00:14:12,318
또는 차원 축소, PCA는

368
00:14:12,318 --> 00:14:14,860
데이터 구조를 설명하는 낮은 차원

369
00:14:14,860 --> 00:14:17,020
부분 공간이나 매니폴드를

370
00:14:17,020 --> 00:14:18,340
발견하려는 것입니다.

371
00:14:18,340 --> 00:14:19,380
그리고 이것도 데이터

372
00:14:19,380 --> 00:14:21,080
자체에서 발견하려는 것입니다.

373
00:14:21,080 --> 00:14:24,100
이것이 무엇이어야 하는지에 대한 주석은 없습니다.

374
00:14:24,100 --> 00:14:25,017
또는 밀도 추정입니다.

375
00:14:25,017 --> 00:14:27,183
데이터에 확률 분포를 맞추려는 시도일

376
00:14:27,183 --> 00:14:27,880
수 있습니다.

377
00:14:27,880 --> 00:14:30,060
우리가 보는 데이터 샘플을 생성한

378
00:14:30,060 --> 00:14:32,360
확률 함수가 무엇인지 이해하려는 것입니다.

379
00:14:32,360 --> 00:14:34,027
그리고 이것에 대한 명시적

380
00:14:34,027 --> 00:14:35,960
레이블이나 훈련 세트는 없습니다.

381
00:14:35,960 --> 00:14:37,740
그래서 이것은 훈련 과정을

382
00:14:37,740 --> 00:14:40,820
통해 발견하려는 숨겨진 또는 잠재 구조입니다.

383
00:14:40,820 --> 00:14:44,140
이 비지도 학습의 이분법은 항상 염두에 두어야

384
00:14:44,140 --> 00:14:45,340
할 것입니다.

385
00:14:45,340 --> 00:14:47,560
비지도 학습은 반드시

386
00:14:47,560 --> 00:14:50,080
확률적이거나 생성적일 필요는 없습니다.

387
00:14:50,080 --> 00:14:52,480
클러스터링이나 PCA 같은 것들이

388
00:14:52,480 --> 00:14:54,582
종종 확률적 해석을 가지지만,

389
00:14:54,582 --> 00:14:56,540
이들은 반드시

390
00:14:56,540 --> 00:14:58,940
생성적이거나 확률적 해석을

391
00:14:58,940 --> 00:15:01,900
가져야 하는 것은 아닙니다.

392
00:15:01,900 --> 00:15:04,900
그래서 저는 비지도 학습의 이분법을 방법이나

393
00:15:04,900 --> 00:15:10,220
시스템이 놓일 수 있는 하나의 스펙트럼으로 생각하는 것을 좋아합니다.

394
00:15:10,220 --> 00:15:13,780
우리가 시스템이나 작업을 분류할 수 있는 또 다른

395
00:15:13,780 --> 00:15:16,260
스펙트럼은 생성 모델과 판별 모델입니다.

396
00:15:16,260 --> 00:15:17,975
이 모델들은 본질적으로 확률적입니다.

397
00:15:17,975 --> 00:15:20,100
생성 모델이나 판별 모델에 대해 이야기할

398
00:15:20,100 --> 00:15:23,140
때 우리는 항상 데이터 내에 어떤 확률적 구조가 있다고

399
00:15:23,140 --> 00:15:25,820
상상하며, 그것을 발견하거나 학습하려고 합니다.

400
00:15:25,820 --> 00:15:27,300
차이는 우리가

401
00:15:27,300 --> 00:15:28,780
모델링하려는 변수들 간의

402
00:15:28,780 --> 00:15:31,300
확률적 관계가 무엇인가에 있습니다.

403
00:15:31,300 --> 00:15:32,780
판별 모델에서는

404
00:15:32,780 --> 00:15:35,460
보통 y와 x가 있습니다.

405
00:15:35,460 --> 00:15:37,860
일반적으로 x는 크고 고차원이며,

406
00:15:37,860 --> 00:15:40,280
우리 경우에는 보통 이미지입니다.

407
00:15:40,280 --> 00:15:42,410
그리고 y는 어떤 레이블이나

408
00:15:42,410 --> 00:15:44,410
설명, 보조 정보입니다.

409
00:15:44,410 --> 00:15:47,110
예를 들어 텍스트, 캡션,

410
00:15:47,110 --> 00:15:50,050
카테고리 레이블 같은 것이죠.

411
00:15:50,050 --> 00:15:52,352
판별 모델에서는 x가

412
00:15:52,352 --> 00:15:53,810
주어졌을 때 y의 확률

413
00:15:53,810 --> 00:15:56,290
분포를 학습하려고 합니다.

414
00:15:56,290 --> 00:15:57,970
즉, 입력 이미지

415
00:15:57,970 --> 00:16:02,570
x에 조건부인 레이블 분포를 학습하는 겁니다.

416
00:16:02,570 --> 00:16:07,070
확률적으로 무슨 일이 일어나는지 제대로 이해하려면 확률 분포의 아주 중요한

417
00:16:07,070 --> 00:16:09,690
특징 하나를 기억해야 합니다. 그것은 바로

418
00:16:09,690 --> 00:16:12,280
확률 분포가 정규화되어 있다는 점입니다.

419
00:16:12,280 --> 00:16:14,530
확률 분포나 더 일반적으로

420
00:16:14,530 --> 00:16:16,890
밀도 함수 p(x)에 대해

421
00:16:16,890 --> 00:16:22,250
이야기할 때, p(x)는 가능한 모든 입력 x에 대해 0이 아닌

422
00:16:22,250 --> 00:16:27,010
값을 할당하는 함수이며, 매우 중요한 정규화 제약 조건이

423
00:16:27,010 --> 00:16:29,570
있습니다. 즉, 가능한 모든

424
00:16:29,570 --> 00:16:33,590
x 공간에 대해 적분하면 1이 된다는 겁니다.

425
00:16:33,590 --> 00:16:35,130
이 정규화 제약 조건이

426
00:16:35,130 --> 00:16:38,210
확률 모델의 힘을 어느 정도 만들어냅니다.

427
00:16:38,210 --> 00:16:41,050
왜냐하면 정규화 제약은 모든 x들이

428
00:16:41,050 --> 00:16:44,870
확률 질량을 놓고 경쟁해야 한다는 의미이기 때문입니다.

429
00:16:44,870 --> 00:16:49,110
확률 질량의 총량은 고정되어 있고, 확률 분포나

430
00:16:49,110 --> 00:16:51,890
밀도 함수를 선택하는 것은

431
00:16:51,890 --> 00:16:53,610
그 고정된 확률

432
00:16:53,610 --> 00:16:55,370
질량을 가능한 모든

433
00:16:55,370 --> 00:16:57,810
x 값에 나누어 퍼뜨리는

434
00:16:57,810 --> 00:16:59,090
것과 같습니다.

435
00:16:59,090 --> 00:17:01,035
모든 x는 경쟁 관계에 있습니다.

436
00:17:01,035 --> 00:17:02,410
왜냐하면 확률 질량의 총량이

437
00:17:02,410 --> 00:17:03,990
한정되어 있기 때문입니다.

438
00:17:03,990 --> 00:17:06,329
따라서 어떤 x의 확률을 높이고

439
00:17:06,329 --> 00:17:09,050
싶으면, 다른 x들의 확률이나 밀도는

440
00:17:09,050 --> 00:17:11,050
반드시 낮아져야 합니다.

441
00:17:11,050 --> 00:17:14,089
그래서 확률 모델의 이러한 다양한 공식화에서

442
00:17:14,089 --> 00:17:16,353
기본적으로 바뀌는 것은 확률

443
00:17:16,353 --> 00:17:17,770
질량을 차지하려고

444
00:17:17,770 --> 00:17:19,755
경쟁하는 변수들이 무엇인가입니다.

445
00:17:19,755 --> 00:17:22,130
즉, 우리가 페이지에 쓰는

446
00:17:22,130 --> 00:17:26,849
기호들이 매우 비슷해 보여도, 확률 질량을 차지하려고 경쟁하는

447
00:17:26,849 --> 00:17:28,927
대상이 다르면 모델이

448
00:17:28,927 --> 00:17:31,010
학습하거나 발견하려는 구조가

449
00:17:31,010 --> 00:17:32,890
매우 다르게 나타납니다.

450
00:17:32,890 --> 00:17:34,670
판별 모델의 경우에는 x에

451
00:17:34,670 --> 00:17:36,170
조건부인 y의 확률

452
00:17:36,170 --> 00:17:40,050
모델을 학습하는 것인데, 이는 모든 x에 대해 모델이

453
00:17:40,050 --> 00:17:42,690
가능한 모든 레이블에 대한 확률 분포를

454
00:17:42,690 --> 00:17:44,210
예측한다는 뜻입니다.

455
00:17:44,210 --> 00:17:47,370
레이블이 고양이나 개처럼 이산적이고

456
00:17:47,370 --> 00:17:51,060
범주형이라면, 확률은 0에서 1 사이의 고정된 총량을

457
00:17:51,060 --> 00:17:55,210
가지며, 고양이와 개의 확률 합은 1이 되어야 합니다.

458
00:17:55,210 --> 00:17:57,810
그리고 각 입력 x마다 레이블에

459
00:17:57,810 --> 00:18:00,508
대한 별도의 확률 분포가 존재합니다.

460
00:18:00,508 --> 00:18:02,050
중요한 점은 여기서

461
00:18:02,050 --> 00:18:05,510
이미지들 간에 확률 질량을 놓고 경쟁이 없다는 것입니다.

462
00:18:05,510 --> 00:18:07,970
각 이미지가 레이블 공간에 대해 자체적인

463
00:18:07,970 --> 00:18:10,450
분포를 유도하기 때문에, 서로

464
00:18:10,450 --> 00:18:13,170
다른 이미지들 간에는 질량 경쟁이 없습니다.

465
00:18:13,170 --> 00:18:14,970
질량을 놓고 경쟁하는 것은 각

466
00:18:14,970 --> 00:18:16,943
이미지에 대해 서로 다른 레이블들뿐입니다.

467
00:18:16,943 --> 00:18:18,610
이것은 판별 모델링을

468
00:18:18,610 --> 00:18:20,850
생각할 때 매우 중요합니다.

469
00:18:20,850 --> 00:18:22,730
그리고 판별 모델링의 또

470
00:18:22,730 --> 00:18:25,810
다른 흥미로운 점은, 비합리적인 입력을

471
00:18:25,810 --> 00:18:29,090
거부할 실제 방법이 없다는 것입니다.

472
00:18:29,090 --> 00:18:31,310
예를 들어, 고양이와 개라는 레이블

473
00:18:31,310 --> 00:18:33,970
공간을 고정했을 때, 원숭이나 추상

474
00:18:33,970 --> 00:18:36,530
미술 작품 같은 고양이나 개가 아닌

475
00:18:36,530 --> 00:18:39,590
것을 입력하면 시스템은 유연성이 없습니다.

476
00:18:39,590 --> 00:18:41,790
비합리적이라고 말할 자유가 없다는 겁니다.

477
00:18:41,790 --> 00:18:43,650
처음에 할당한 고정된 어휘에

478
00:18:43,650 --> 00:18:46,490
대한 분포를 출력할 수밖에 없습니다.

479
00:18:46,490 --> 00:18:48,828
이것은 단점으로 볼 수도 있지만,

480
00:18:48,828 --> 00:18:50,370
확률적으로 다른

481
00:18:50,370 --> 00:18:52,177
데이터를 모델링할 때 내부에서

482
00:18:52,177 --> 00:18:54,010
정확히 무슨 일이 일어나는지

483
00:18:54,010 --> 00:18:56,103
이해하는 것이 중요합니다.

484
00:18:56,103 --> 00:18:58,270
반면, 생성 모델은 매우 다릅니다.

485
00:18:58,270 --> 00:19:00,395
생성 모델에서는 x의 분포

486
00:19:00,395 --> 00:19:02,210
p를 학습하는 것입니다.

487
00:19:02,210 --> 00:19:05,770
우리는 가능한 모든 이미지 x에 대한 분포를 학습하고자 합니다.

488
00:19:05,770 --> 00:19:07,590
이제 이 점이 매우 흥미롭습니다.

489
00:19:07,590 --> 00:19:09,850
이는 우주에 존재할 수 있는 모든

490
00:19:09,850 --> 00:19:11,650
가능한 이미지들이 이제 확률

491
00:19:11,650 --> 00:19:14,610
질량을 놓고 서로 경쟁하고 있다는 뜻입니다.

492
00:19:14,610 --> 00:19:18,010
이제 이 문제는 매우 어려운 질문이 되었는데,

493
00:19:18,010 --> 00:19:21,835
겉보기에는 간단해 보여도 세계에 관한 매우 깊고

494
00:19:21,835 --> 00:19:23,210
철학적인 문제에 직면해야

495
00:19:23,210 --> 00:19:25,870
합니다. 왜냐하면 모든 이미지가

496
00:19:25,870 --> 00:19:29,070
확률 질량을 놓고 경쟁하고 있기 때문입니다.

497
00:19:29,070 --> 00:19:31,410
이를 모델링하기 위해서는 세

498
00:19:31,410 --> 00:19:34,730
다리 개 이미지가 세 팔 원숭이 이미지에

499
00:19:34,730 --> 00:19:36,850
비해 어떻게 확률 질량을

500
00:19:36,850 --> 00:19:38,530
가져야 하는지 같은

501
00:19:38,530 --> 00:19:40,187
질문에 답해야 합니다.

502
00:19:40,187 --> 00:19:42,770
아마도 세 다리 개가 더 많은 확률 질량을 가져야 할 것입니다.

503
00:19:42,770 --> 00:19:45,187
왜냐하면 개가 다리를 잃는 경우가 있을 수 있으니까요.

504
00:19:45,187 --> 00:19:47,270
하지만 세 팔 원숭이는 어떻게 생길까요?

505
00:19:47,270 --> 00:19:49,210
글쎄요, 아마도 공상과학 이미지 같은

506
00:19:49,210 --> 00:19:51,910
걸 모델링하지 않는 이상 훨씬 더 희귀할 것 같습니다.

507
00:19:51,910 --> 00:19:54,890
모든 가능한 이미지가 확률 질량을 놓고

508
00:19:54,890 --> 00:19:56,690
경쟁하는 이 상황에 들어서면,

509
00:19:56,690 --> 00:19:59,330
모델은 데이터 내 존재할 수 있는

510
00:19:59,330 --> 00:20:01,010
구조를 매우 신중하게

511
00:20:01,010 --> 00:20:04,290
고려해야 하며, 문제 해결이 훨씬 더 어려워집니다.

512
00:20:04,290 --> 00:20:06,330
또 다른 흥미로운 점은, 생성 모델은

513
00:20:06,330 --> 00:20:07,990
이제 기본적으로 '이건

514
00:20:07,990 --> 00:20:10,770
합리적인 이미지가 아니다'라고 말할 수 있는 능력을

515
00:20:10,770 --> 00:20:12,310
갖게 되었다는 겁니다.

516
00:20:12,310 --> 00:20:13,710
즉, 합리적인 입력이 아닙니다.

517
00:20:13,710 --> 00:20:15,127
그 방법은 어떤

518
00:20:15,127 --> 00:20:19,090
이미지에 대해 낮거나 심지어 0에 가까운 확률 질량을

519
00:20:19,090 --> 00:20:20,310
할당하는 것입니다.

520
00:20:20,310 --> 00:20:21,910
예를 들어, 우리의 생성

521
00:20:21,910 --> 00:20:25,015
모델이 동물원 동물만 생성하는 모델이라면,

522
00:20:25,015 --> 00:20:27,390
추상 미술 이미지가

523
00:20:27,390 --> 00:20:30,150
들어왔을 때는 확률 질량이

524
00:20:30,150 --> 00:20:31,910
0이어야 합니다.

525
00:20:31,910 --> 00:20:34,410
이제 우리는 이 이미지 유형이 우리가 관심

526
00:20:34,410 --> 00:20:36,770
있는 범위에 속하지 않는다고 거부할

527
00:20:36,770 --> 00:20:39,130
수 있는 메커니즘을 갖게 된 것입니다.

528
00:20:39,130 --> 00:20:42,233
그리고 조건부 생성 모델은 훨씬 더 흥미롭습니다.

529
00:20:42,233 --> 00:20:44,650
여기서는 어떤 레이블 신호

530
00:20:44,650 --> 00:20:49,450
y에 조건부로 이미지 x에 대한 조건부 분포를 학습합니다.

531
00:20:49,450 --> 00:20:52,810
이제 가능한 모든 레이블마다 가능한

532
00:20:52,810 --> 00:20:56,570
모든 이미지 간의 경쟁이 유도됩니다.

533
00:20:56,570 --> 00:20:59,010
예를 들어 y가 고양이와 개라는

534
00:20:59,010 --> 00:21:03,690
범주형 레이블이라면, 각 범주형 레이블마다 모델은

535
00:21:03,690 --> 00:21:06,330
가능한 모든 이미지 간의 경쟁을

536
00:21:06,330 --> 00:21:07,970
별도로 유도합니다.

537
00:21:07,970 --> 00:21:10,370
위 분포는 고양이 레이블에 조건부인

538
00:21:10,370 --> 00:21:12,890
모든 이미지의 확률일 수 있습니다.

539
00:21:12,890 --> 00:21:14,970
따라서 고양이 이미지는 확률이

540
00:21:14,970 --> 00:21:15,750
높아야 합니다.

541
00:21:15,750 --> 00:21:17,610
원숭이와 개 이미지는 적어도

542
00:21:17,610 --> 00:21:19,570
포유류이므로 다소 높을 수

543
00:21:19,570 --> 00:21:20,310
있습니다.

544
00:21:20,310 --> 00:21:22,850
하지만 추상 미술 이미지는 매우 낮거나 0일 수 있습니다.

545
00:21:22,850 --> 00:21:25,170
그리고 개 레이블에 조건부인 이미지들

546
00:21:25,170 --> 00:21:27,352
사이에는 다른 분포가 형성됩니다.

547
00:21:27,352 --> 00:21:28,810
더 흥미로운 점은,

548
00:21:28,810 --> 00:21:30,643
조건부 신호 y가 단일 범주형

549
00:21:30,643 --> 00:21:33,285
레이블보다 훨씬 풍부한 경우를

550
00:21:33,285 --> 00:21:36,080
상상해보는 것입니다. 예를 들어 y가 텍스트

551
00:21:36,080 --> 00:21:37,572
설명일 수도 있고,

552
00:21:37,572 --> 00:21:39,780
한 문단 분량의 글일 수도 있으며,

553
00:21:39,780 --> 00:21:42,260
또는 다른 이미지와 텍스트 조합일 수도 있습니다.

554
00:21:42,260 --> 00:21:44,920
이제 매우 풍부한 입력 공간 y에

555
00:21:44,920 --> 00:21:48,652
조건부인 매우 풍부한 출력 공간 x를 모델링한다는

556
00:21:48,652 --> 00:21:50,360
것은, 모델이 관련 객체에

557
00:21:50,360 --> 00:21:53,000
대해 매우 깊은 추론을 요구하는

558
00:21:53,000 --> 00:21:55,120
매우 복잡하고 정의하기 어려운

559
00:21:55,120 --> 00:21:57,478
문제를 해결해야 한다는 뜻입니다.

560
00:21:57,478 --> 00:21:59,520
그래서 저는 생성 모델링이 매우

561
00:21:59,520 --> 00:22:03,860
흥미로운 주제라고 생각합니다. 왜냐하면 겉보기에는 간단해 보이지만,

562
00:22:03,860 --> 00:22:05,540
x와 y를 바꿨을 뿐인데,

563
00:22:05,540 --> 00:22:06,583
얼마나 어려울 수 있겠냐고 생각했지만,

564
00:22:06,583 --> 00:22:08,000
갑자기 시각 세계에서

565
00:22:08,000 --> 00:22:09,375
무슨 일이 벌어지고 있는지

566
00:22:09,375 --> 00:22:11,560
매우 깊게 생각해야 했기 때문입니다.

567
00:22:11,560 --> 00:22:13,760
또한 우리는 판별 모델,

568
00:22:13,760 --> 00:22:16,560
생성 모델, 조건부 생성 모델을 세

569
00:22:16,560 --> 00:22:19,680
가지 별개의 범주로 나누어 설명했습니다.

570
00:22:19,680 --> 00:22:21,793
하지만 사실 이들은 모두 관련되어 있습니다.

571
00:22:21,793 --> 00:22:23,460
그리고 이들은 확률에서

572
00:22:23,460 --> 00:22:28,880
가장 놀라운 관계 중 하나인 베이즈 규칙을 통해 연결되어 있습니다.

573
00:22:28,880 --> 00:22:33,720
특히, 만약 우리가 판별 모델 p(y|x)와 무조건

574
00:22:33,720 --> 00:22:36,360
생성 모델 p(x), 그리고 레이블

575
00:22:36,360 --> 00:22:38,920
y에 대한 사전 분포를

576
00:22:38,920 --> 00:22:41,440
알고 있다면, 이들을 조합해

577
00:22:41,440 --> 00:22:44,240
조건부 생성 모델 p(y|x)를

578
00:22:44,240 --> 00:22:45,920
만들 수 있습니다.

579
00:22:45,920 --> 00:22:48,160
일반적으로 베이즈 규칙은 언제나 어떤

580
00:22:48,160 --> 00:22:50,240
식으로든 재배열할 수 있어서, 이 세

581
00:22:50,240 --> 00:22:54,780
가지 중 두 가지를 알면 항상 세 번째를 구할 수 있다는 점이 아주
멋집니다.

582
00:22:54,780 --> 00:22:57,813
이론적으로는 원칙적으로 다른 두 구성 요소로부터

583
00:22:57,813 --> 00:22:59,480
조건부 생성 모델을

584
00:22:59,480 --> 00:23:00,780
만들 수 있습니다.

585
00:23:00,780 --> 00:23:03,620
하지만 실제로는 이렇게 하지는 않습니다.

586
00:23:03,620 --> 00:23:06,040
보통 조건부 생성 모델은 처음부터

587
00:23:06,040 --> 00:23:08,557
따로 학습하는 경향이 있습니다.

588
00:23:08,557 --> 00:23:10,640
다만, diffusion에서 이야기하겠지만,

589
00:23:10,640 --> 00:23:13,000
어떤 이유로 조건부와 무조건 모델을

590
00:23:13,000 --> 00:23:14,985
함께 학습하는 경우도 있습니다.

591
00:23:14,985 --> 00:23:16,360
하지만 이렇게 서로 다른

592
00:23:16,360 --> 00:23:17,902
확률 모델 유형 간에 매우

593
00:23:17,902 --> 00:23:20,962
깊은 관계가 있다는 점을 기억해 두는 것이 좋습니다.

594
00:23:20,962 --> 00:23:22,420
그렇다면 여러분은 아마, 이

595
00:23:22,420 --> 00:23:24,212
다양한 확률 모델 유형으로 무엇을 할 수

596
00:23:24,212 --> 00:23:25,360
있을지 궁금할 겁니다.

597
00:23:25,360 --> 00:23:26,920
판별 모델의 경우, 많은

598
00:23:26,920 --> 00:23:28,503
창의력이 필요하지 않을 것입니다.

599
00:23:28,503 --> 00:23:31,453
이번 학기 동안 많은 예제를 봤으니까요.

600
00:23:31,453 --> 00:23:33,620
판별 모델은 학습 후에 데이터를 레이블에

601
00:23:33,620 --> 00:23:34,940
할당할 수 있습니다.

602
00:23:34,940 --> 00:23:36,500
또한 특징 학습도 할 수 있습니다.

603
00:23:36,500 --> 00:23:39,260
예를 들어, ImageNet에서 감독

604
00:23:39,260 --> 00:23:41,000
학습을 할 때,

605
00:23:41,000 --> 00:23:43,700
이미지의 범주형 레이블을 예측하는

606
00:23:43,700 --> 00:23:46,280
과정에서 중간에 유용한 특징 표현을

607
00:23:46,280 --> 00:23:47,822
학습하는 경향이 있음을

608
00:23:47,822 --> 00:23:48,860
보았습니다.

609
00:23:48,860 --> 00:23:49,598
즉, 판별

610
00:23:49,598 --> 00:23:51,640
모델은 관심 있는 y를 직접

611
00:23:51,640 --> 00:23:53,580
예측하거나, y를 예측하는

612
00:23:53,580 --> 00:23:55,760
과정에서 유도된 특징 표현을

613
00:23:55,760 --> 00:23:57,677
학습하는 데 주로 사용됩니다.

614
00:24:00,380 --> 00:24:02,280
이러한 무조건적 생성 모델들은

615
00:24:02,280 --> 00:24:04,980
사실 일반적으로 쓸모없다고 생각합니다.

616
00:24:04,980 --> 00:24:08,360
하지만 이 모델들이 할 수 있는 것은 아마 이상치를 감지하는 것입니다.

617
00:24:08,360 --> 00:24:11,400
이미지를 보고 정말로 확률 질량이 낮은지

618
00:24:11,400 --> 00:24:12,500
판단하는 거죠.

619
00:24:12,500 --> 00:24:15,260
비합리적인 이미지인지 말입니다.

620
00:24:15,260 --> 00:24:18,020
라벨 없이, 데이터 없이 특징 학습에 사용할

621
00:24:18,020 --> 00:24:19,160
수 있습니다.

622
00:24:19,160 --> 00:24:20,880
그래서 x의 무조건적 분포

623
00:24:20,880 --> 00:24:23,180
p를 맞추는 과정에서 모델이 유용한

624
00:24:23,180 --> 00:24:25,960
특징 표현을 배울 수 있기를 기대하는 겁니다.

625
00:24:25,960 --> 00:24:27,840
하지만 일반적으로 이런

626
00:24:27,840 --> 00:24:29,680
모델들은 자기 지도 학습에 크게

627
00:24:29,680 --> 00:24:30,960
성공하지 못했고,

628
00:24:30,960 --> 00:24:32,240
이전 강의에서

629
00:24:32,240 --> 00:24:34,280
이야기한 대조 학습 방법들이 실제로

630
00:24:34,280 --> 00:24:36,280
자기 지도 학습에서 훨씬

631
00:24:36,280 --> 00:24:39,560
더 성공적이었습니다, 무조건적 밀도 추정보다요.

632
00:24:39,560 --> 00:24:42,320
원칙적으로는 이 무조건적 생성 모델을 사용해 샘플을

633
00:24:42,320 --> 00:24:44,800
생성하고 새로운 샘플 x를 만들 수도 있습니다.

634
00:24:44,800 --> 00:24:46,960
하지만 이건 사실 쓸모없다고 생각하는데,

635
00:24:46,960 --> 00:24:49,960
왜냐하면 샘플링되는 것을 제어할 수 없기 때문입니다.

636
00:24:49,960 --> 00:24:52,300
이미지의 무조건적 생성 모델이 있다면, 그 모델에서

637
00:24:52,300 --> 00:24:54,258
샘플링해 새로운 이미지를 얻을 수 있지만,

638
00:24:54,258 --> 00:24:57,040
그 이미지 안에 무엇이 들어있는지는 제어할 수 없습니다.

639
00:24:57,040 --> 00:25:00,280
그래서 이런 모델을 만드는 방법을 수학적으로

640
00:25:00,280 --> 00:25:02,160
생각하는 것은 흥미롭지만,

641
00:25:02,160 --> 00:25:05,200
실용적인 의미는 크지 않다고 봅니다.

642
00:25:05,200 --> 00:25:08,160
그리고 조건부 생성 모델이 가장 유용하고

643
00:25:08,160 --> 00:25:10,098
가장 흥미롭다고 생각합니다.

644
00:25:10,098 --> 00:25:12,640
이 모델들이 실제로 가장 많이

645
00:25:12,640 --> 00:25:14,840
훈련되고 사용되는 생성 모델입니다.

646
00:25:14,840 --> 00:25:17,120
원칙적으로는 이 모델들을 사용해 라벨을 할당하면서

647
00:25:17,120 --> 00:25:18,660
이상치를 거부할 수 있습니다.

648
00:25:18,660 --> 00:25:21,420
데이터 x가 있다면, 가능한

649
00:25:21,420 --> 00:25:25,280
모든 y에 대해 p(x|y)를 보고, 그

650
00:25:25,280 --> 00:25:28,200
중 너무 낮으면 거부할 수

651
00:25:28,200 --> 00:25:29,500
있다는 겁니다.

652
00:25:29,500 --> 00:25:32,080
그래서 원칙적으로 조건부 생성 모델을

653
00:25:32,080 --> 00:25:35,000
사용해 분류를 하면서 이상치 거부 기능도

654
00:25:35,000 --> 00:25:36,380
유지할 수 있습니다.

655
00:25:36,380 --> 00:25:37,880
사실 그게 실무에서 그렇게

656
00:25:37,880 --> 00:25:39,640
많이 쓰인다고 생각하지는 않습니다.

657
00:25:39,640 --> 00:25:42,520
조건부 생성 모델에서 정말 유용하고 실제로 항상

658
00:25:42,520 --> 00:25:44,540
어디서나 쓰이는 것은, 라벨로부터

659
00:25:44,540 --> 00:25:46,800
새로운 데이터를 샘플링해서 생성하는

660
00:25:46,800 --> 00:25:48,360
것이고, 이때 생성되는

661
00:25:48,360 --> 00:25:50,200
것을 직접 제어할 수 있습니다.

662
00:25:50,200 --> 00:25:52,600
예를 들어 y가 텍스트라면,

663
00:25:52,600 --> 00:25:54,240
'달에서 핫도그 맛 티셔츠를

664
00:25:54,240 --> 00:25:58,838
입은 고양이를 보고 싶다'라고 적으면, 여러분이 좋아하는

665
00:25:58,838 --> 00:26:00,880
이미지 생성 모델이 그 라벨

666
00:26:00,880 --> 00:26:03,360
y에 조건화된 새로운 이미지 x를

667
00:26:03,360 --> 00:26:04,513
생성해 줍니다.

668
00:26:04,513 --> 00:26:07,180
그래서 이 부분이 모든 흥미진진한 부분, 모든

669
00:26:07,180 --> 00:26:09,440
마법과 재미가 있는 곳이라고 생각합니다.

670
00:26:09,440 --> 00:26:11,700
다만 문헌에서는 다소 혼란스럽게도,

671
00:26:11,700 --> 00:26:13,920
generative model이라는 용어를 보면

672
00:26:13,920 --> 00:26:16,960
무조건적 생성 모델과 조건부 생성 모델을 섞어서 쓰는

673
00:26:16,960 --> 00:26:17,898
경우가 많습니다.

674
00:26:17,898 --> 00:26:19,440
읽는 논문들 중에는

675
00:26:19,440 --> 00:26:21,718
수식을 더 깔끔하게 보이게 하려고

676
00:26:21,718 --> 00:26:23,760
조건 신호 y를 아예

677
00:26:23,760 --> 00:26:25,360
빼버리는 경우도 있습니다.

678
00:26:25,360 --> 00:26:27,200
하지만 저는 무조건적 생성 모델이 그렇게

679
00:26:27,200 --> 00:26:28,437
유용하다고 생각하지 않습니다.

680
00:26:28,437 --> 00:26:30,520
대부분의 경우 실제로 하고 싶은

681
00:26:30,520 --> 00:26:33,200
것은 거의 항상 조건부 생성 모델입니다.

682
00:26:33,200 --> 00:26:37,140
그래서 논문을 읽거나 수식을 보거나 사람들이 생성 모델에

683
00:26:37,140 --> 00:26:39,880
대해 이야기할 때, 수식이나 표기법이

684
00:26:39,880 --> 00:26:42,400
반영하지 않더라도 그들이 진짜 중요하게

685
00:26:42,400 --> 00:26:44,200
생각하고 훈련하려는 것은

686
00:26:44,200 --> 00:26:46,720
조건부 생성 모델일 가능성이 높다는

687
00:26:46,720 --> 00:26:47,920
점을 알아두세요.

688
00:26:47,920 --> 00:26:50,880
그럼 무조건적 생성 모델의

689
00:26:50,880 --> 00:26:53,920
입력과 출력은 무엇일까요?

690
00:26:53,920 --> 00:26:55,680
사실 그 부분은 아직 말씀드리지 않았습니다.

691
00:26:55,680 --> 00:26:58,800
그리고 제가 살짝 속였는데, 그걸 어떻게 파라미터화하느냐에

692
00:26:58,800 --> 00:27:00,060
따라 많이 달라집니다.

693
00:27:00,060 --> 00:27:01,280
이 모든 것에 대해 다양한

694
00:27:01,280 --> 00:27:02,420
공식화가 많습니다.

695
00:27:02,420 --> 00:27:04,360
네트워크의 입력과 출력이 정확히

696
00:27:04,360 --> 00:27:06,610
무엇이 될지는 공식화에 따라 크게

697
00:27:06,610 --> 00:27:07,520
달라질 겁니다.

698
00:27:07,520 --> 00:27:09,480
몇 장 뒤에서 그런 다양한 분류 체계에 대해

699
00:27:09,480 --> 00:27:10,272
이야기할 예정입니다.

700
00:27:12,786 --> 00:27:15,160
그럼 왜 생성 모델일까요?

701
00:27:15,160 --> 00:27:17,400
생성 모델을 만들고자 하는

702
00:27:17,400 --> 00:27:19,840
주된 이유는 모델링하려는 작업에

703
00:27:19,840 --> 00:27:21,440
모호성이 있을 때입니다.

704
00:27:21,440 --> 00:27:25,400
y가 주어졌을 때 x의 확률 모델 p의 아름다움은 바로

705
00:27:25,400 --> 00:27:27,150
확률적이라는 점입니다.

706
00:27:27,150 --> 00:27:30,630
입력 레이블 y에 조건부인 가능한 출력 x의 전체

707
00:27:30,630 --> 00:27:32,150
공간이 있을 수 있습니다.

708
00:27:32,150 --> 00:27:34,612
때로는 결정론적 매핑만

709
00:27:34,612 --> 00:27:36,570
있는 경우도 있습니다.

710
00:27:36,570 --> 00:27:37,790
이미지를 보고 그 이미지에 고양이가

711
00:27:37,790 --> 00:27:39,090
몇 마리 있는지 묻고 싶을 때,

712
00:27:39,090 --> 00:27:40,090
고양이가 세 마리뿐입니다.

713
00:27:40,090 --> 00:27:41,270
답은 하나뿐입니다.

714
00:27:41,270 --> 00:27:43,130
하지만 많은 경우에는 더 미묘합니다.

715
00:27:43,130 --> 00:27:46,615
만약 핫도그 모자를 쓴 개 사진을 요청하면, 그 쿼리에

716
00:27:46,615 --> 00:27:47,990
따라 존재할 수

717
00:27:47,990 --> 00:27:49,870
있는 다양한 이미지가 많습니다.

718
00:27:49,870 --> 00:27:51,362
출력에 불확실성이 존재합니다.

719
00:27:51,362 --> 00:27:53,070
바로 이것이 생성 모델이 모델링하려는

720
00:27:53,070 --> 00:27:53,903
대상입니다.

721
00:27:53,903 --> 00:27:56,153
입력 신호에 조건부인 출력의 전체 분포를

722
00:27:56,153 --> 00:27:57,110
모델링합니다.

723
00:27:57,110 --> 00:27:59,910
입력에 조건부인 출력에

724
00:27:59,910 --> 00:28:01,870
모호성이 있을 때마다

725
00:28:01,870 --> 00:28:03,328
생성 모델을

726
00:28:03,328 --> 00:28:04,910
사용해야 합니다.

727
00:28:04,910 --> 00:28:05,690
이것이

728
00:28:05,690 --> 00:28:08,230
바로 이유이며, 최근 몇 년간 많이

729
00:28:08,230 --> 00:28:10,870
사용된 몇 가지 예를 보겠습니다.

730
00:28:10,870 --> 00:28:12,590
한 가지 예는 언어 모델링입니다.

731
00:28:12,590 --> 00:28:14,498
조금 전에 누군가 ChatGPT에 대해 물어봤습니다.

732
00:28:14,498 --> 00:28:16,790
그래서 언어 모델링에서는 보통

733
00:28:16,790 --> 00:28:20,750
입력 텍스트 y로부터 출력 텍스트 x를 예측하려고 합니다.

734
00:28:20,750 --> 00:28:23,870
죄송합니다, 이 예제에서는 x와

735
00:28:23,870 --> 00:28:26,950
y가 좀 어색하게 뒤바뀌었네요.

736
00:28:26,950 --> 00:28:29,230
여기 ChatGPT의 예제가 있습니다.

737
00:28:29,230 --> 00:28:30,830
입력은 'generative models에

738
00:28:30,830 --> 00:28:32,370
관한 짧은 운문 시를 써줘'입니다.

739
00:28:32,370 --> 00:28:34,067
와, 실제로 작동하네요.

740
00:28:34,067 --> 00:28:34,650
정말 놀랍습니다.

741
00:28:34,650 --> 00:28:37,248
이 수업을 처음 가르칠 때는 전혀 작동하지 않았습니다.

742
00:28:37,248 --> 00:28:38,290
제가 읽지는 않겠습니다.

743
00:28:38,290 --> 00:28:39,415
그럼 좀 민망할 테니까요.

744
00:28:39,415 --> 00:28:41,097
직접 읽어보실 수 있습니다.

745
00:28:41,097 --> 00:28:42,930
이제, 이것은 조건부 생성 모델입니다.

746
00:28:42,930 --> 00:28:45,472
generative models에 관한

747
00:28:45,472 --> 00:28:48,550
다양한 운문 시가 있을 수 있는데, 그중

748
00:28:48,550 --> 00:28:49,842
하나를 선택한 거죠.

749
00:28:49,842 --> 00:28:51,342
생성 모델의 아름다움은

750
00:28:51,342 --> 00:28:53,750
원칙적으로 입력에 조건부로

751
00:28:53,750 --> 00:28:57,070
가능한 출력 전체 분포를 모델링한다는 점입니다.

752
00:28:57,070 --> 00:28:58,455
또는 텍스트에서 이미지로.

753
00:28:58,455 --> 00:28:59,830
'generative models에 관한

754
00:28:59,830 --> 00:29:02,590
수업을 하는 사람이 화이트보드 앞에 있는 모습을 보여주는 이미지를
만들어줘'라고 하면,

755
00:29:02,590 --> 00:29:04,770
여러분은 눈으로 한 예시를 보고 있는 거고,

756
00:29:04,770 --> 00:29:06,590
ChatGPT는 다른 예시를 보여줬습니다.

757
00:29:06,590 --> 00:29:08,830
입력 텍스트에 맞는 가능한 이미지

758
00:29:08,830 --> 00:29:11,255
공간이 완전히 다를 수 있죠.

759
00:29:11,255 --> 00:29:12,630
생성 모델은 그 전체

760
00:29:12,630 --> 00:29:14,797
공간을 모델링하고, 원하는 대로 그

761
00:29:14,797 --> 00:29:17,070
공간에서 샘플링할 수 있게 해줍니다.

762
00:29:17,070 --> 00:29:18,590
또는 이미지에서 비디오로.

763
00:29:18,590 --> 00:29:21,110
이미지를 입력하면 다음에 무슨 일이 일어날까요?

764
00:29:21,110 --> 00:29:24,170
이건 제가 에어팟을 종이상자 위에 들고 있는 장면입니다.

765
00:29:24,170 --> 00:29:25,330
아마 떨어뜨릴 수도 있고,

766
00:29:25,330 --> 00:29:26,730
손을 움직일 수도 있고,

767
00:29:26,730 --> 00:29:29,310
손을 움직이면서 에어팟이 다른

768
00:29:29,310 --> 00:29:31,228
에어팟으로 변할 수도 있죠.

769
00:29:31,228 --> 00:29:32,770
일어날 수 있는 모든 일이 있습니다.

770
00:29:32,770 --> 00:29:34,010
생성 모델은 원칙적으로 이런

771
00:29:34,010 --> 00:29:36,260
가능한 미래들을 모델링하고 샘플링할 수 있게 해줍니다.

772
00:29:39,910 --> 00:29:42,392
그래서 우리가 생성 모델링에 관심을 가지는 이유입니다.

773
00:29:42,392 --> 00:29:44,350
출력에 애매함이 있을 때,

774
00:29:44,350 --> 00:29:48,430
생성 모델을 사용해 해결하려고 하는 겁니다.

775
00:29:48,430 --> 00:29:50,770
누군가 입력과 출력이 무엇인지 물었는데,

776
00:29:50,770 --> 00:29:52,790
이 분야가 엄청나게 넓다는 사실이 밝혀졌습니다.

777
00:29:52,790 --> 00:29:55,270
그리고 놀랍게도 이 분야는 확률 분포를

778
00:29:55,270 --> 00:29:57,630
모델링하는 다양한 방법을

779
00:29:57,630 --> 00:29:59,590
생각해야 해서 꽤 수학적인 딥러닝

780
00:29:59,590 --> 00:30:01,130
분야 중 하나입니다.

781
00:30:01,130 --> 00:30:02,630
우리가 원하는 결과를 내는

782
00:30:02,630 --> 00:30:05,010
손실 함수를 어떻게 작성할 수 있을까요?

783
00:30:05,010 --> 00:30:06,590
그래서 논문을 읽을 때

784
00:30:06,590 --> 00:30:08,410
수학이나 방정식이 많을 수 있는

785
00:30:08,410 --> 00:30:09,830
분야 중 하나입니다.

786
00:30:09,830 --> 00:30:12,670
그리고 실제로 무슨 일이 일어나는지 이해하려면 그 방정식을

787
00:30:12,670 --> 00:30:14,730
꽤 신중하게 생각해야 할 수도 있습니다.

788
00:30:14,730 --> 00:30:17,470
그래서 이 분야는 수학과 방정식이 더 많은

789
00:30:17,470 --> 00:30:19,910
경향이 있는데, 저는 이게 재미있고 흥미롭다고

790
00:30:19,910 --> 00:30:20,788
생각합니다.

791
00:30:20,788 --> 00:30:22,830
사람들이 만드는 다양한 종류의 생성 모델에

792
00:30:22,830 --> 00:30:24,750
대한 전체 분류 체계가 있습니다.

793
00:30:24,750 --> 00:30:26,510
한편으로는, 가족 계보의 한

794
00:30:26,510 --> 00:30:30,110
부분 또는 우리가 명시적 밀도 방법이라고 부르는 것을 상상할

795
00:30:30,110 --> 00:30:30,930
수 있습니다.

796
00:30:30,930 --> 00:30:33,163
이것들은 모델이 실제로 p(x)

797
00:30:33,163 --> 00:30:34,830
또는 p(x|y)를

798
00:30:34,830 --> 00:30:36,750
모델링하려고 하는 경우입니다.

799
00:30:36,750 --> 00:30:38,590
이 명시적 밀도 방법에서는

800
00:30:38,590 --> 00:30:40,910
어떤 샘플 x에 대해서도 p(x)

801
00:30:40,910 --> 00:30:43,350
값을 실제로 계산할 수 있습니다.

802
00:30:43,350 --> 00:30:46,565
반면에 암묵적 밀도 방법이 있습니다.

803
00:30:46,565 --> 00:30:48,190
이것들은 모델에서 실제로

804
00:30:48,190 --> 00:30:50,470
확률 질량 값, 즉 p(x)

805
00:30:50,470 --> 00:30:53,290
밀도 값을 얻을 수는 없지만, 그

806
00:30:53,290 --> 00:30:56,650
확률 분포에서 샘플링은 할 수 있는 경우입니다.

807
00:30:56,650 --> 00:31:00,150
여기서 차이점은 암묵적 모델에서는 밀도 함수

808
00:31:00,150 --> 00:31:03,418
값을 직접 접근할 수 없지만, 어떻게든 그 밀도

809
00:31:03,418 --> 00:31:05,710
함수에서 샘플링할 수 있다는

810
00:31:05,710 --> 00:31:06,290
겁니다.

811
00:31:06,290 --> 00:31:07,790
즉, 모델이 값을 직접 얻을

812
00:31:07,790 --> 00:31:10,590
수 없더라도 밀도를 암묵적으로 학습했다는 뜻입니다.

813
00:31:10,590 --> 00:31:12,550
명시적 밀도 쪽은

814
00:31:12,550 --> 00:31:15,870
거의 반대인데, 많은 경우에 명시적

815
00:31:15,870 --> 00:31:19,057
밀도 값을 얻을 수 있지만,

816
00:31:19,057 --> 00:31:21,390
샘플링은 때때로 더 복잡할

817
00:31:21,390 --> 00:31:23,090
수 있습니다.

818
00:31:23,090 --> 00:31:25,270
항상 그런 것은 아니지만, 때때로 그렇습니다.

819
00:31:25,270 --> 00:31:27,670
암묵적 모델을 선택하는 이유는,

820
00:31:27,670 --> 00:31:29,830
많은 경우에 어떤 입력에 대한 정확한

821
00:31:29,830 --> 00:31:32,510
밀도 값을 아는 것이 중요하지 않을

822
00:31:32,510 --> 00:31:33,950
수 있기 때문입니다.

823
00:31:33,950 --> 00:31:35,990
아마도 중요한 것은 샘플을

824
00:31:35,990 --> 00:31:39,967
생성하고 다양한 샘플을 잘 생성하는 것일 뿐일 수 있습니다.

825
00:31:39,967 --> 00:31:42,050
그래서 만약 여러분이 정말로 관심

826
00:31:42,050 --> 00:31:43,467
있는 것이 샘플링이라면,

827
00:31:43,467 --> 00:31:45,230
어떤 입력에 대해서도 밀도 값을

828
00:31:45,230 --> 00:31:48,270
명시적으로 볼 수 있을 필요는 없을 수도 있습니다.

829
00:31:48,270 --> 00:31:50,270
그리고 나서 여기서부터는 시스템이 무너지면서

830
00:31:50,270 --> 00:31:52,110
연쇄적으로 더 프랙탈 같은 형태가 됩니다.

831
00:31:52,110 --> 00:31:54,395
명시적 밀도 방법 안에는 실제로

832
00:31:54,395 --> 00:31:55,770
모델링되는 x의 진짜

833
00:31:55,770 --> 00:31:58,630
p 값을 계산할 수 있는 방법들이 있습니다.

834
00:31:58,630 --> 00:32:01,710
그리고 autoregressive 모델이 그 한 예입니다.

835
00:32:01,710 --> 00:32:04,590
또 다른 명시적 밀도 방법의 버전은 밀도 값을

836
00:32:04,590 --> 00:32:07,087
얻을 수는 있지만, 그 값이 진짜 값은

837
00:32:07,087 --> 00:32:08,170
아닌 경우입니다.

838
00:32:08,170 --> 00:32:11,710
그것은 데이터의 진짜 밀도에 대한 어떤 근사값입니다.

839
00:32:11,710 --> 00:32:13,870
그리고 variational

840
00:32:13,870 --> 00:32:16,630
autoencoder는 명시적이지만 근사적인 생성 방법의 한

841
00:32:16,630 --> 00:32:18,190
예로 우리가 볼 것입니다.

842
00:32:18,190 --> 00:32:20,830
이제, 계보의 다른 가지에서는 암묵적 밀도에

843
00:32:20,830 --> 00:32:23,338
대한 직접적인 방법을 생각할 수 있습니다.

844
00:32:23,338 --> 00:32:25,630
이것들은 아마도 모델링되는 기본

845
00:32:25,630 --> 00:32:30,110
분포에서 샘플을 그리기 위해 단일 네트워크 평가만 필요로 하는

846
00:32:30,110 --> 00:32:31,090
방법들입니다.

847
00:32:31,090 --> 00:32:32,590
그리고 generative

848
00:32:32,590 --> 00:32:35,173
adversarial network는 이 계보의 이 부분에 속하는 생성

849
00:32:35,173 --> 00:32:35,870
모델의 예입니다.

850
00:32:35,870 --> 00:32:36,910
그리고 다른 부분은—좋은

851
00:32:36,910 --> 00:32:39,705
이름이 있는지 모르겠는데, 저는 간접적(indirect)이라고 불렀습니다.

852
00:32:39,705 --> 00:32:41,330
하지만 이 이름은 어제 제가

853
00:32:41,330 --> 00:32:43,390
만든 것이니, 더 좋은 용어가 있다면

854
00:32:43,390 --> 00:32:44,830
자유롭게 지적해 주세요.

855
00:32:44,830 --> 00:32:46,950
이 간접적인 방법들은

856
00:32:46,950 --> 00:32:50,910
모델링되는 기본 밀도 p(x)에서 샘플링할 수는

857
00:32:50,910 --> 00:32:52,750
있지만, 반복적인 절차가

858
00:32:52,750 --> 00:32:54,330
필요한 경우입니다.

859
00:32:54,330 --> 00:32:57,310
입력을 넣으면 바로 샘플을 얻는 피드포워드

860
00:32:57,310 --> 00:32:58,310
함수는 없습니다.

861
00:32:58,310 --> 00:32:59,893
모델링되는 기본 밀도에서

862
00:32:59,893 --> 00:33:01,590
샘플을 그리기 위해서는 어떤

863
00:33:01,590 --> 00:33:03,652
반복적인 방법을 실행해야 합니다.

864
00:33:03,652 --> 00:33:05,110
그리고 diffusion 모델이

865
00:33:05,110 --> 00:33:06,680
다음 시간에 볼 이 방법의 한 예입니다.

866
00:33:06,680 --> 00:33:08,430
몇 슬라이드 전에 사람들이 표기법에 대충대충

867
00:33:08,430 --> 00:33:10,207
하는 경향이 있어서 y를 생략한다고 말씀드렸고,

868
00:33:10,207 --> 00:33:12,290
이 슬라이드에서는 일부러 그렇게 했습니다. 그래서

869
00:33:12,290 --> 00:33:13,730
누군가가 그 질문을 하도록 유도했고,

870
00:33:13,730 --> 00:33:15,750
여러분도 항상 그 점을 주의 깊게 보셔야 합니다.

871
00:33:15,750 --> 00:33:16,677
네, 정확히 그렇습니다.

872
00:33:16,677 --> 00:33:18,510
이 슬라이드뿐만 아니라 이

873
00:33:18,510 --> 00:33:21,040
강의의 모든 슬라이드에서 p(x)를 쓸

874
00:33:21,040 --> 00:33:22,960
때마다 저도 귀찮아서 y를

875
00:33:22,960 --> 00:33:25,740
생략했지만, 여러분은 항상 이 p(x)들이

876
00:33:25,740 --> 00:33:28,460
y에 조건부라는 것을 상상하셔야 합니다.

877
00:33:28,460 --> 00:33:30,300
질문해 주셔서 감사합니다.

878
00:33:30,300 --> 00:33:34,180
질문은, 그 간접적인 반복 절차를 블랙박스처럼

879
00:33:34,180 --> 00:33:36,700
취급해서 직접 샘플링 방법으로

880
00:33:36,700 --> 00:33:39,580
사용할 수 있느냐는 것이었죠?

881
00:33:39,580 --> 00:33:40,800
원칙적으로는 그렇습니다.

882
00:33:40,800 --> 00:33:43,660
하지만 실제로는 그렇지 않습니다. 왜냐하면

883
00:33:43,660 --> 00:33:47,600
샘플이 사용하는 정확한 방법에 따라 근사치가 되기 때문입니다.

884
00:33:47,600 --> 00:33:49,713
특히 diffusion 모델에서는

885
00:33:49,713 --> 00:33:51,380
진짜 샘플을 얻으려면

886
00:33:51,380 --> 00:33:54,020
무한한 단계가 필요해서, 대신 유한한

887
00:33:54,020 --> 00:33:55,520
단계로 근사하는 겁니다.

888
00:33:55,520 --> 00:33:57,360
다른 방법들도 마찬가지입니다.

889
00:33:57,360 --> 00:33:59,940
오늘날에는 diffusion 모델이 가장 흔합니다.

890
00:33:59,940 --> 00:34:03,340
하지만 과거에는 마르코프 체인 방법이나 MCMC

891
00:34:03,340 --> 00:34:05,220
방법도 반복 절차가 있는 이런

892
00:34:05,220 --> 00:34:06,520
특성을 가졌습니다.

893
00:34:06,520 --> 00:34:08,699
모델링된 분포에서 정확한 샘플을

894
00:34:08,699 --> 00:34:10,540
뽑으려면 무한한 단계가

895
00:34:10,540 --> 00:34:11,580
필요합니다.

896
00:34:11,580 --> 00:34:16,820
그래서 우리는 항상 유한한 단계로 근사하는 거죠.

897
00:34:16,820 --> 00:34:20,380
이 분류 체계가 매우 대칭적이라서 꽤

898
00:34:20,380 --> 00:34:22,159
자랑스러웠습니다.

899
00:34:22,159 --> 00:34:25,360
네 개의 잎이 있고,

900
00:34:25,360 --> 00:34:26,397
두 개의 가지가 있습니다.

901
00:34:26,397 --> 00:34:28,980
오늘은 트리의 절반을 다루고 다음 시간에 나머지 절반을

902
00:34:28,980 --> 00:34:29,860
다룰 것입니다.

903
00:34:29,860 --> 00:34:32,277
그래서 저는 그게 꽤 괜찮은 분류라고 생각했습니다.

904
00:34:32,277 --> 00:34:33,860
질문은 근사 밀도와

905
00:34:33,860 --> 00:34:36,739
암묵적인 p(x)에서 직접 샘플링하는 것의

906
00:34:36,739 --> 00:34:38,659
차이가 무엇인가 하는 겁니다.

907
00:34:38,659 --> 00:34:42,040
차이점은 간접적이지만 암묵적인 방법에서는 밀도 값을

908
00:34:42,040 --> 00:34:44,102
전혀 찾을 수 없다는 것입니다.

909
00:34:44,102 --> 00:34:46,060
전혀 계산할 수 없지만, 어떤 식으로든

910
00:34:46,060 --> 00:34:47,900
반복적으로 샘플링할 수는 있습니다.

911
00:34:47,900 --> 00:34:49,677
근사 밀도 방법에서는 실제로

912
00:34:49,677 --> 00:34:52,260
밀도 값을 얻을 수 있는데, 이는

913
00:34:52,260 --> 00:34:54,552
근사치이거나 진짜 p(x)에

914
00:34:54,552 --> 00:34:55,760
대한 경계가 됩니다.

915
00:34:59,020 --> 00:35:01,583
그렇다면 우리가 조금 더

916
00:35:01,583 --> 00:35:03,500
구체적으로 이야기할

917
00:35:03,500 --> 00:35:07,632
첫 번째 생성 모델은 자기회귀 모델입니다.

918
00:35:07,632 --> 00:35:09,340
자기회귀 모델에 대해

919
00:35:09,340 --> 00:35:10,923
이야기하기 전에, 생성

920
00:35:10,923 --> 00:35:13,203
모델링 전반에 걸친 아주 일반적인

921
00:35:13,203 --> 00:35:14,620
개념인 최대 우도

922
00:35:14,620 --> 00:35:17,060
추정에 대해 잠시 다룰 것입니다.

923
00:35:17,060 --> 00:35:18,598
최대 우도 추정은 유한한

924
00:35:18,598 --> 00:35:20,140
샘플 집합이 주어졌을 때

925
00:35:20,140 --> 00:35:22,620
확률 모델을 적합시키는 데 사용할 수 있는

926
00:35:22,620 --> 00:35:24,153
꽤 일반적인 절차입니다.

927
00:35:24,153 --> 00:35:25,820
아이디어는 밀도에 대한

928
00:35:25,820 --> 00:35:28,060
명시적인 함수를 작성하는 것입니다.

929
00:35:28,060 --> 00:35:32,060
어떤 방법들은 밀도를 명시적으로 모델링한다고

930
00:35:32,060 --> 00:35:32,880
했죠.

931
00:35:32,880 --> 00:35:34,680
그럼 신경망으로 해봅시다.

932
00:35:34,680 --> 00:35:36,138
데이터 x와

933
00:35:36,138 --> 00:35:39,780
신경망의 가중치 W를 입력으로 받아 밀도가

934
00:35:39,780 --> 00:35:41,235
무엇인지

935
00:35:41,235 --> 00:35:42,860
알려주는 숫자를

936
00:35:42,860 --> 00:35:45,540
출력하는 신경망을 작성합시다.

937
00:35:45,540 --> 00:35:47,820
그다음, 샘플 데이터 집합 x1,

938
00:35:47,820 --> 00:35:50,460
x2, ..., xN이 주어졌을 때

939
00:35:50,460 --> 00:35:53,360
이 목적 함수로 모델을 학습시킬 것입니다.

940
00:35:53,360 --> 00:35:58,338
우리는 데이터 집합이 가장 가능성 높게 되도록 하는 가중치를 찾고자 합니다.

941
00:35:58,338 --> 00:36:00,880
우리가 원하는 것은 [? 수락하시겠습니까?] 가중치를 바꾸면 네트워크가

942
00:36:00,880 --> 00:36:02,660
모델링하는 밀도도 바뀌기

943
00:36:02,660 --> 00:36:04,000
때문입니다.

944
00:36:04,000 --> 00:36:07,580
그래서 네트워크가 데이터의 우도(likelihood)를 최대화하는

945
00:36:07,580 --> 00:36:09,420
밀도를 선택하도록 하는 겁니다.

946
00:36:09,420 --> 00:36:12,140
우리가 확률(probability) 대신 우도(likelihood)라고

947
00:36:12,140 --> 00:36:15,820
한 점에 주목하세요, 이건 깊은 철학적 문제로 빠질 수 있습니다.

948
00:36:15,820 --> 00:36:17,420
차이는 우리가 무엇을 바꾸느냐에 있습니다.

949
00:36:17,420 --> 00:36:18,878
확률을 생각하면

950
00:36:18,878 --> 00:36:21,020
밀도는 고정되어 있고 x를

951
00:36:21,020 --> 00:36:23,160
움직이며 고정된 분포에서 x의

952
00:36:23,160 --> 00:36:25,760
확률이 어떻게 되는지 보는 겁니다.

953
00:36:25,760 --> 00:36:27,860
우도를 말할 때는

954
00:36:27,860 --> 00:36:29,940
보통 샘플 x를

955
00:36:29,940 --> 00:36:34,004
고정하고 분포 자체를 바꾸면서 그

956
00:36:34,004 --> 00:36:37,100
샘플들의 확률 밀도가 어떻게

957
00:36:37,100 --> 00:36:39,512
변하는지 보는 거죠.

958
00:36:39,512 --> 00:36:41,220
그래서 이 식들에서 무엇이 고정되고

959
00:36:41,220 --> 00:36:43,660
무엇이 변하는지 아주 신중히 생각해야 합니다.

960
00:36:43,660 --> 00:36:46,460
최대 우도 추정 과정에서는

961
00:36:46,460 --> 00:36:49,340
신경망이 모델링하는 분포를

962
00:36:49,340 --> 00:36:52,580
바꿔서 훈련 세트에 있는 고정된

963
00:36:52,580 --> 00:36:54,380
샘플 집합의

964
00:36:54,380 --> 00:36:57,280
확률을 최대화하려는 겁니다.

965
00:36:57,280 --> 00:36:59,380
그리고 이 모든 것 뒤에

966
00:36:59,380 --> 00:37:02,660
숨겨진 말은, 우리가 데이터 생성을 위해 우주가

967
00:37:02,660 --> 00:37:05,900
사용한 어떤 진짜 확률 분포 p data가

968
00:37:05,900 --> 00:37:08,260
있다고 가정한다는 겁니다.

969
00:37:08,260 --> 00:37:10,340
어떤 면에서 우리는 항상 그

970
00:37:10,340 --> 00:37:13,500
진짜 알려지지 않은 분포 p data를 모델링하려고

971
00:37:13,500 --> 00:37:14,360
하는 거죠.

972
00:37:14,360 --> 00:37:16,820
p data는 우리가 우주가 어떻게

973
00:37:16,820 --> 00:37:19,340
작동하는지 전지전능하게 알지

974
00:37:19,340 --> 00:37:22,940
못하기 때문에 절대 접근할 수 없지만, 대신 우주가

975
00:37:22,940 --> 00:37:24,880
준 샘플들을 받습니다.

976
00:37:24,880 --> 00:37:27,340
학습 절차를 통해 우리는 그 알려지지

977
00:37:27,340 --> 00:37:29,515
않은 분포 p data에서 나온

978
00:37:29,515 --> 00:37:31,900
유한한 샘플들을 바탕으로 그 분포를

979
00:37:31,900 --> 00:37:33,740
밝혀내려고 하는 겁니다.

980
00:37:33,740 --> 00:37:35,900
그래서 한 가지 방법은,

981
00:37:35,900 --> 00:37:38,340
내가 본 데이터를 가장

982
00:37:38,340 --> 00:37:41,180
가능성 있게 만드는 분포를

983
00:37:41,180 --> 00:37:45,645
선택하는 것, 즉 최대 우도 목표 함수입니다.

984
00:37:45,645 --> 00:37:47,020
그리고 여기서 일반적인

985
00:37:47,020 --> 00:37:50,140
트릭은 데이터가 IID, 독립적이고 동일한 분포에서

986
00:37:50,140 --> 00:37:51,440
나왔다고 가정하는 겁니다.

987
00:37:51,440 --> 00:37:53,460
각 x가 진짜 p data

988
00:37:53,460 --> 00:37:55,417
분포에서 뽑혔다고 가정하는 거죠.

989
00:37:55,417 --> 00:37:57,500
이제 우리가 본 모든 데이터의 결합 분포를

990
00:37:57,500 --> 00:37:58,800
최대화하려고 합니다.

991
00:37:58,800 --> 00:38:01,300
하지만 독립적이기 때문에 각

992
00:38:01,300 --> 00:38:05,220
독립 샘플의 우도로 분해할 수 있습니다.

993
00:38:05,220 --> 00:38:07,880
그리고 우리가 항상 사용하는 일반적인 트릭은 로그 함수입니다.

994
00:38:07,880 --> 00:38:10,040
로그는 단조 함수이기 때문입니다.

995
00:38:10,040 --> 00:38:12,700
무언가를 최대화하는 것은 그 무언가의 로그를

996
00:38:12,700 --> 00:38:15,300
최대화하는 것과 같습니다, 로그가

997
00:38:15,300 --> 00:38:16,642
단조 함수이기 때문이죠.

998
00:38:16,642 --> 00:38:18,100
로그는 또한 곱과 합을

999
00:38:18,100 --> 00:38:20,240
바꾸기 때문에 매우 편리합니다.

1000
00:38:20,240 --> 00:38:21,100
그래서

1001
00:38:21,100 --> 00:38:25,695
데이터의 우도를 최대화하는 대신 로그 우도를 최대화하는

1002
00:38:25,695 --> 00:38:28,320
것이 일반적이며, 이는 우도를

1003
00:38:28,320 --> 00:38:30,362
최대화하는 것과 같습니다.

1004
00:38:30,362 --> 00:38:33,880
로그를 적용하면 곱이 합으로 나뉘고,

1005
00:38:33,880 --> 00:38:36,340
합이 다루기 더 쉽습니다.

1006
00:38:36,340 --> 00:38:38,660
이제 신경망을 넣으면 신경망이

1007
00:38:38,660 --> 00:38:40,660
밀도를 직접 출력할

1008
00:38:40,660 --> 00:38:41,712
수도 있습니다.

1009
00:38:41,712 --> 00:38:43,420
이것이 신경망을 훈련시키는 데

1010
00:38:43,420 --> 00:38:46,148
사용할 수 있는 직접적인 목표 함수가 됩니다.

1011
00:38:46,148 --> 00:38:47,940
이것은 생성 모델링 문제를 해결하기

1012
00:38:47,940 --> 00:38:50,023
위해 신경망을 훈련시키는 데 쓸 수

1013
00:38:50,023 --> 00:38:52,147
있는 구체적인 손실 함수를 제공합니다.

1014
00:38:52,147 --> 00:38:53,980
하지만 실제로 진전을

1015
00:38:53,980 --> 00:38:56,900
이루려면 좀 더 구조가 필요합니다.

1016
00:38:56,900 --> 00:38:59,140
최대 우도 추정이라는 아이디어는 매우

1017
00:38:59,140 --> 00:38:59,960
일반적입니다.

1018
00:38:59,960 --> 00:39:02,135
데이터에 대해 특별한 가정을 하지 않습니다.

1019
00:39:02,135 --> 00:39:04,260
데이터에 어떤 구조가 있다고 가정하지도 않습니다.

1020
00:39:04,260 --> 00:39:06,677
일반적으로 진전을 위해서는 여기에 좀

1021
00:39:06,677 --> 00:39:08,500
더 구조를 부여해야 합니다.

1022
00:39:08,500 --> 00:39:11,060
그래서 autoregressive 모델은

1023
00:39:11,060 --> 00:39:14,500
기본적으로 데이터 x를 어떤 표준적인 방식으로 각

1024
00:39:14,500 --> 00:39:19,140
데이터 샘플 x를 일련의 하위 부분들 x1, x2, ..., xT로

1025
00:39:19,140 --> 00:39:20,820
나눌 수 있다고 가정합니다.

1026
00:39:20,820 --> 00:39:22,600
여기서 인덱스에 주의해야 합니다.

1027
00:39:22,600 --> 00:39:24,080
여기서는 하위 부분(subparts)이라고 했습니다.

1028
00:39:24,080 --> 00:39:26,380
이것들은 단일 샘플의 하위 부분들입니다.

1029
00:39:26,380 --> 00:39:27,680
그래서 아래 첨자(subscript)를 사용했습니다.

1030
00:39:27,680 --> 00:39:29,620
이전 슬라이드에서는 위 첨자(superscript)를

1031
00:39:29,620 --> 00:39:32,980
사용해서 서로 다른 샘플 x1부터 xN까지를 나타냈습니다.

1032
00:39:32,980 --> 00:39:34,580
그 점을 조심하세요.

1033
00:39:34,580 --> 00:39:37,340
이 슬라이드에서 위 첨자는 서로 다른 샘플 x를 의미합니다.

1034
00:39:37,340 --> 00:39:39,220
아래 첨자는 같은 샘플의 서로

1035
00:39:39,220 --> 00:39:40,467
다른 부분을 의미합니다.

1036
00:39:40,467 --> 00:39:42,300
그래서 데이터 샘플 x를 어떤

1037
00:39:42,300 --> 00:39:46,500
표준적인 방식으로 일련의 하위 부분들로 나눌 수 있다고 가정합니다.

1038
00:39:46,500 --> 00:39:48,820
이제 확률의 연쇄 법칙(chain rule)을 적용할 수 있습니다.

1039
00:39:48,820 --> 00:39:52,220
즉, x의 확률은 모든 하위 부분

1040
00:39:52,220 --> 00:39:55,580
x1부터 xT까지의 결합 확률입니다.

1041
00:39:55,580 --> 00:39:57,640
그리고 어떤 확률 분포든

1042
00:39:57,640 --> 00:40:00,180
항상 이 연쇄 법칙으로 쪼갤 수

1043
00:40:00,180 --> 00:40:03,640
있습니다. 모든 변수들의 결합 분포는 첫

1044
00:40:03,640 --> 00:40:06,220
번째 변수의 확률 곱하기 첫 번째

1045
00:40:06,220 --> 00:40:08,480
변수에 조건부인 두 번째

1046
00:40:08,480 --> 00:40:09,970
변수의 확률, 곱하기

1047
00:40:09,970 --> 00:40:12,230
첫 번째와 두 번째 변수에

1048
00:40:12,230 --> 00:40:13,730
조건부인 세 번째

1049
00:40:13,730 --> 00:40:16,910
변수의 확률, 이런 식으로 계속됩니다.

1050
00:40:16,910 --> 00:40:19,070
이것이 확률의 연쇄 법칙입니다.

1051
00:40:19,070 --> 00:40:20,310
이것은 어떤 가정도 필요하지 않습니다.

1052
00:40:20,310 --> 00:40:22,570
임의의 확률 변수들의 결합

1053
00:40:22,570 --> 00:40:24,970
분포에 항상 성립합니다.

1054
00:40:24,970 --> 00:40:28,510
그리고 이것이 우리의 목적 함수가 됩니다.

1055
00:40:28,510 --> 00:40:31,170
그럼 기본적으로 신경망을 훈련시켜서

1056
00:40:31,170 --> 00:40:35,228
시퀀스의 이전 부분을 입력으로 받고 다음 부분에

1057
00:40:35,228 --> 00:40:36,770
대한 확률 분포를

1058
00:40:36,770 --> 00:40:39,290
출력하도록 할 수 있습니다.

1059
00:40:39,290 --> 00:40:40,610
익숙하게 들리시나요?

1060
00:40:40,610 --> 00:40:43,170
전에 해본 것과 비슷하지 않나요?

1061
00:40:43,170 --> 00:40:44,310
RNN이죠, 맞습니다.

1062
00:40:44,310 --> 00:40:46,330
그래서 바로 이것이 RNN이 하는 일입니다.

1063
00:40:46,330 --> 00:40:48,290
RNN은 시간에 따라 은닉 상태(hidden

1064
00:40:48,290 --> 00:40:51,930
state)를 전달하면서 은닉 상태가 시퀀스의 시작부터

1065
00:40:51,930 --> 00:40:54,770
현재 시점까지의 정보를 항상 포함하는 아주 자연스러운

1066
00:40:54,770 --> 00:40:56,250
구조를 가지고 있습니다.

1067
00:40:56,250 --> 00:40:57,930
그래서 RNN을 autoregressive

1068
00:40:57,930 --> 00:41:01,010
모델링에 사용하는 아주 자연스러운 방법이 있습니다.

1069
00:41:01,010 --> 00:41:02,930
시퀀스를 요약하는

1070
00:41:02,930 --> 00:41:05,510
은닉 상태들의 시퀀스가 있고,

1071
00:41:05,510 --> 00:41:07,010
각 은닉 상태에서

1072
00:41:07,010 --> 00:41:08,810
이전 시퀀스 전체에

1073
00:41:08,810 --> 00:41:11,850
조건부인 다음 시퀀스 조각의 확률을 예측합니다.

1074
00:41:11,850 --> 00:41:13,850
이것이 바로 몇

1075
00:41:13,850 --> 00:41:16,810
강의 전에 본 RNN 언어 모델입니다.

1076
00:41:16,810 --> 00:41:19,370
이걸 할 수 있는 다른 게 있었나요?

1077
00:41:19,370 --> 00:41:20,930
네, transformers가 있습니다.

1078
00:41:20,930 --> 00:41:23,690
특히 masked transformers가 그렇습니다.

1079
00:41:23,690 --> 00:41:25,850
transformers 강의에서,

1080
00:41:25,850 --> 00:41:29,050
transformers도 주의(attention) 행렬을

1081
00:41:29,050 --> 00:41:30,830
적절히 마스킹해서 각 출력이

1082
00:41:30,830 --> 00:41:32,810
시퀀스의 접두사(prefix)에만

1083
00:41:32,810 --> 00:41:35,230
의존하도록 할 수 있다고 했습니다.

1084
00:41:35,230 --> 00:41:38,590
그래서 transformers도 autoregressive 모델링에 사용할
수 있습니다.

1085
00:41:38,590 --> 00:41:40,850
그리고 실제로 매우 흔히 사용됩니다.

1086
00:41:40,850 --> 00:41:44,250
하지만 autoregressive 모델링의 문제는

1087
00:41:44,250 --> 00:41:46,750
데이터를 시퀀스로 나눠야 한다는 점입니다.

1088
00:41:46,750 --> 00:41:50,490
텍스트 데이터는 본질적으로 1차원 시퀀스이기

1089
00:41:50,490 --> 00:41:52,850
때문에 매우 자연스럽습니다.

1090
00:41:52,850 --> 00:41:54,870
게다가 이 시퀀스는 이산적인

1091
00:41:54,870 --> 00:41:57,370
요소들의 1차원 시퀀스라 확률 모델링이

1092
00:41:57,370 --> 00:41:58,530
매우 쉽습니다.

1093
00:41:58,530 --> 00:42:00,210
우리는 이번 학기 내내 cross-entropy

1094
00:42:00,210 --> 00:42:02,630
softmax loss로 그렇게 해왔습니다.

1095
00:42:02,630 --> 00:42:04,730
cross entropy

1096
00:42:04,730 --> 00:42:09,410
softmax loss는 항상 고정된 이산 카테고리 분포에 대해 작동합니다.

1097
00:42:09,410 --> 00:42:11,568
네트워크는 각 카테고리에 대한 점수를 예측합니다.

1098
00:42:11,568 --> 00:42:14,110
softmax로 정규화하고 cross-entropy loss로 학습합니다.

1099
00:42:14,110 --> 00:42:15,410
우리는 이 방법을 잘 알고 있습니다.

1100
00:42:15,410 --> 00:42:17,650
그래서 이런 모델들이 언어 모델에

1101
00:42:17,650 --> 00:42:21,410
매우 자연스럽게 맞는 이유는 언어가 이미 이산적이고,

1102
00:42:21,410 --> 00:42:23,335
언어가 이미 1차원 시퀀스이기 때문입니다.

1103
00:42:23,335 --> 00:42:25,210
토크나이저가 있어서 약간

1104
00:42:25,210 --> 00:42:26,870
애매한 부분이 있긴 하지만,

1105
00:42:26,870 --> 00:42:28,290
그 부분은 다루지 않겠습니다.

1106
00:42:28,290 --> 00:42:30,870
하지만 언어가 이미 1차원이고 이산적이기

1107
00:42:30,870 --> 00:42:34,170
때문에 이런 모델들이 언어 문제에 매우 적합합니다.

1108
00:42:34,170 --> 00:42:38,210
이미지는 더 까다로운데, 이미지는 본질적으로 1차원이 아닙니다.

1109
00:42:38,210 --> 00:42:40,470
이미지는 또한 본질적으로 이산적이지 않습니다.

1110
00:42:40,470 --> 00:42:43,690
우리는 이미지를 연속적이고 실수 값으로 생각하는 경우가 많습니다.

1111
00:42:43,690 --> 00:42:46,850
그래서 이런 모델들이 이미지에 자연스럽게

1112
00:42:46,850 --> 00:42:48,370
맞지는 않습니다.

1113
00:42:48,370 --> 00:42:51,550
하지만 망치가 있으면 못을 칠 수밖에 없죠.

1114
00:42:51,550 --> 00:42:53,970
그래서 몇 년 전에는 사람들이 적어도

1115
00:42:53,970 --> 00:42:58,210
순진한 방식으로 autoregressive 모델을 이미지에 적용했습니다.

1116
00:42:58,210 --> 00:43:02,810
이미지를 autoregressive 모델로 다루는

1117
00:43:02,810 --> 00:43:06,410
한 가지 방법은 이미지를 픽셀 시퀀스로

1118
00:43:06,410 --> 00:43:08,370
취급하는 것입니다.

1119
00:43:08,370 --> 00:43:11,330
특히 각 픽셀은 실제로 세 개의 숫자로 이루어져 있습니다.

1120
00:43:11,330 --> 00:43:16,710
대부분의 디스플레이와 이미지 표현에서 그 숫자들은

1121
00:43:16,710 --> 00:43:18,330
이산적입니다.

1122
00:43:18,330 --> 00:43:20,190
대부분 JPEG나 PNG

1123
00:43:20,190 --> 00:43:23,330
같은 이미지 파일 포맷은 보통 채널당

1124
00:43:23,330 --> 00:43:24,470
8비트입니다.

1125
00:43:24,470 --> 00:43:27,530
그래서 각 픽셀이 가질 수 있는 값의 수가

1126
00:43:27,530 --> 00:43:29,030
고정되어 있습니다.

1127
00:43:29,030 --> 00:43:32,390
픽셀은 세 개의 단일 바이트 값일 뿐입니다.

1128
00:43:32,390 --> 00:43:35,650
단일 바이트는 0부터 255까지의 정수입니다.

1129
00:43:35,650 --> 00:43:38,010
그래서 픽셀은 세 개의 정수로 표현됩니다.

1130
00:43:38,010 --> 00:43:40,490
각 정수는 0에서 255 사이입니다.

1131
00:43:40,490 --> 00:43:43,370
그래서 이미지를 긴 시퀀스로

1132
00:43:43,370 --> 00:43:46,610
래스터화해서, 시퀀스의 각 요소가

1133
00:43:46,610 --> 00:43:49,010
이미지의 서브픽셀 값 중

1134
00:43:49,010 --> 00:43:51,970
하나가 되도록 할 수 있습니다.

1135
00:43:51,970 --> 00:43:56,350
이제 이미지를 1차원 시퀀스로 바꿨고, 시퀀스의

1136
00:43:56,350 --> 00:43:58,750
각 항목은 이산 값입니다.

1137
00:43:58,750 --> 00:44:00,792
그래서 언어 모델에 하던 것과 똑같이 그

1138
00:44:00,792 --> 00:44:02,907
시퀀스에 autoregressive 모델링을

1139
00:44:02,907 --> 00:44:03,990
직접 적용할 수 있습니다.

1140
00:44:03,990 --> 00:44:07,210
RNN이나 transformer를 사용하는 거죠.

1141
00:44:07,210 --> 00:44:09,955
이 접근법에서 문제점을 발견할 수 있는 분 계신가요?

1142
00:44:09,955 --> 00:44:10,830
[? 너무 ? [? 많습니다. ?]

1143
00:44:10,830 --> 00:44:11,590
매우 비용이 많이 듭니다.

1144
00:44:11,590 --> 00:44:13,010
아주, 아주 비용이 많이 듭니다.

1145
00:44:13,010 --> 00:44:15,970
그래서 모델링하고 싶은 합리적인 이미지는

1146
00:44:15,970 --> 00:44:18,850
아마 1024x1024일 겁니다.

1147
00:44:18,850 --> 00:44:20,870
사실 그렇게 높은 해상도도 아니지만,

1148
00:44:20,870 --> 00:44:22,510
꽤 좋은 해상도입니다.

1149
00:44:22,510 --> 00:44:25,350
하지만 1024x1024 이미지라면

1150
00:44:25,350 --> 00:44:28,050
300만 개의 픽셀 시퀀스가 됩니다.

1151
00:44:28,050 --> 00:44:32,190
요즘 사람들은 실제로 수백만 개 시퀀스도 모델링할 수 있습니다.

1152
00:44:32,190 --> 00:44:33,690
하지만 매우, 매우 비용이 많이 듭니다.

1153
00:44:33,690 --> 00:44:35,977
더 효율적인 방법이 있어야 합니다.

1154
00:44:35,977 --> 00:44:37,810
몇 년 전 몇몇 논문에서는

1155
00:44:37,810 --> 00:44:40,185
이런 autoregressive 모델을

1156
00:44:40,185 --> 00:44:42,210
이미지 픽셀에 직접 적용했지만,

1157
00:44:42,210 --> 00:44:44,490
고해상도로 확장하기 어려워서

1158
00:44:44,490 --> 00:44:46,890
크게 성공하지 못했다고 생각합니다.

1159
00:44:46,890 --> 00:44:49,450
다음 강의에서 조금 더 이야기할

1160
00:44:49,450 --> 00:44:51,570
스포일러인데, 최근 몇 년 사이에 이

1161
00:44:51,570 --> 00:44:53,810
방법이 다시 주목받고 있습니다.

1162
00:44:53,810 --> 00:44:56,050
하지만 핵심은 시퀀스의 개별

1163
00:44:56,050 --> 00:44:58,590
픽셀 값을 모델링하는 게 아니라,

1164
00:44:58,590 --> 00:45:01,770
다른 절차나 모델, 예를 들어 신경망을

1165
00:45:01,770 --> 00:45:04,610
사용해 이미지를 1차원 토큰 시퀀스로

1166
00:45:04,610 --> 00:45:06,130
분해하는 겁니다.

1167
00:45:06,130 --> 00:45:08,770
이 부분은 다음 강의에서 좀 더 다룰 예정입니다.

1168
00:45:08,770 --> 00:45:10,312
하지만 적어도 autoregressive

1169
00:45:10,312 --> 00:45:11,915
모델이 무엇인지 감을 잡을 수 있습니다.

1170
00:45:11,915 --> 00:45:13,790
이들의 확률적 수식은 무엇인지.

1171
00:45:13,790 --> 00:45:15,207
언어에 어떻게 적용하는지.

1172
00:45:15,207 --> 00:45:16,770
이미지에 어떻게 적용하는지.

1173
00:45:16,770 --> 00:45:18,890
그다음 autoregressive

1174
00:45:18,890 --> 00:45:21,770
모델에서 변분 오토인코더로 넘어갑니다.

1175
00:45:21,770 --> 00:45:24,400
변분 오토인코더는 꽤 재미있습니다.

1176
00:45:28,570 --> 00:45:32,130
우리가 이야기한 autoregressive 모델에서는

1177
00:45:32,130 --> 00:45:33,750
최대 우도법을 시도했습니다.

1178
00:45:33,750 --> 00:45:35,750
데이터를 시퀀스의 여러 부분으로 나누고,

1179
00:45:35,750 --> 00:45:38,280
데이터의 우도를 최대화하려고 했죠.

1180
00:45:38,280 --> 00:45:40,530
변분 오토인코더는 조금 다른

1181
00:45:40,530 --> 00:45:41,890
방식을 사용합니다.

1182
00:45:41,890 --> 00:45:44,470
여전히 명시적인 방법이고,

1183
00:45:44,470 --> 00:45:47,450
계산할 수 있는 어떤

1184
00:45:47,450 --> 00:45:49,510
밀도를 가지지만,

1185
00:45:49,510 --> 00:45:51,290
비현실적(intractable)이라 근사해야 합니다.

1186
00:45:51,290 --> 00:45:52,510
왜 그렇게 할까요?

1187
00:45:52,510 --> 00:45:55,888
우리는 밀도를 정확하게 계산하는 완벽한 방법을 가지고 있었습니다.

1188
00:45:55,888 --> 00:45:57,930
그리고 그 대가로 포기할 것은 있지만,

1189
00:45:57,930 --> 00:45:59,110
대신 얻는 것도 있습니다.

1190
00:45:59,110 --> 00:46:01,610
우리는 데이터에 대해 합리적인 잠재 벡터를 계산할

1191
00:46:01,610 --> 00:46:02,980
수 있는 능력을 얻게 됩니다.

1192
00:46:02,980 --> 00:46:05,040
학습 과정에서 자연스럽게

1193
00:46:05,040 --> 00:46:08,240
튀어나오는 데이터 표현 벡터를 가지게 되는 거죠.

1194
00:46:08,240 --> 00:46:10,740
그리고 그 벡터들은 그 자체로도 유용할 것입니다.

1195
00:46:10,740 --> 00:46:13,000
잠재 벡터에 접근할 수 있는

1196
00:46:13,000 --> 00:46:14,920
능력은 우리가 정확한

1197
00:46:14,920 --> 00:46:17,268
밀도 계산을 포기하고, 대신

1198
00:46:17,268 --> 00:46:19,560
실제 밀도의 하한인 근사

1199
00:46:19,560 --> 00:46:22,480
밀도에 만족할 만큼 충분히 유용합니다.

1200
00:46:22,480 --> 00:46:24,400
오토리그레시브 모델에서 시퀀스를

1201
00:46:24,400 --> 00:46:26,360
분해하는 동기는 문제를

1202
00:46:26,360 --> 00:46:27,660
인수분해하기 때문입니다.

1203
00:46:27,660 --> 00:46:30,400
각 부분을 모델링하기 쉽게 만들어 줍니다.

1204
00:46:30,400 --> 00:46:33,520
예를 들어 언어 모델링을 한다고 상상해 보세요,

1205
00:46:33,520 --> 00:46:35,600
그리고 어휘 수가 V개입니다.

1206
00:46:35,600 --> 00:46:38,920
두 단어의 결합 확률을 모델링하고

1207
00:46:38,920 --> 00:46:39,580
싶습니다.

1208
00:46:39,580 --> 00:46:41,900
두 단어 시퀀스의 가능한 경우의 수는 몇 개일까요?

1209
00:46:41,900 --> 00:46:42,978
V의 제곱입니다.

1210
00:46:42,978 --> 00:46:45,020
세 단어 시퀀스의 가능한 경우의 수는 몇 개일까요?

1211
00:46:45,020 --> 00:46:46,240
V의 세제곱입니다.

1212
00:46:46,240 --> 00:46:48,080
일반적으로, 길이가 T인 단어 시퀀스의

1213
00:46:48,080 --> 00:46:50,620
가능한 경우의 수는 어휘 수 V에 대해 몇 개일까요?

1214
00:46:50,620 --> 00:46:53,562
V의 T제곱입니다. 그건 나쁜 거죠.

1215
00:46:53,562 --> 00:46:54,520
지수적으로 증가합니다.

1216
00:46:54,520 --> 00:46:56,812
만약 T개의 항목으로 이루어진 시퀀스의

1217
00:46:56,812 --> 00:46:59,520
결합 분포를 직접 모델링하고 싶다면,

1218
00:46:59,520 --> 00:47:02,360
모델링해야 하는 이산 확률 분포의 항목 수가

1219
00:47:02,360 --> 00:47:04,360
시퀀스 길이에 따라 지수적으로

1220
00:47:04,360 --> 00:47:05,440
증가할 겁니다.

1221
00:47:05,440 --> 00:47:07,815
그리고 긴 시퀀스로 갈수록 그건 금방

1222
00:47:07,815 --> 00:47:09,342
완전히 다루기 어려워집니다.

1223
00:47:09,342 --> 00:47:10,800
그래서 그걸 나누는 이유는 한꺼번에

1224
00:47:10,800 --> 00:47:12,900
모두 모델링하지 않아도 되게 하기 위해서입니다.

1225
00:47:12,900 --> 00:47:15,400
이렇게 인수분해해서 이전 부분들에 조건부로

1226
00:47:15,400 --> 00:47:17,000
한 부분만 예측하는 거죠.

1227
00:47:17,000 --> 00:47:17,900
좋은 질문입니다.

1228
00:47:17,900 --> 00:47:19,740
그걸 완화하기 위해 로그 기법을 적용할 수 있을까요?

1229
00:47:19,740 --> 00:47:20,460
네, 정확히 그렇습니다.

1230
00:47:20,460 --> 00:47:23,640
실제로는 이런 확률 밀도 값을 직접 모델링하는 걸 거의

1231
00:47:23,640 --> 00:47:24,828
보지 못할 겁니다.

1232
00:47:24,828 --> 00:47:27,120
거의 항상 로그 확률로 작업하게

1233
00:47:27,120 --> 00:47:27,920
됩니다.

1234
00:47:27,920 --> 00:47:31,020
그래서 모델은 로그 확률을 출력할 겁니다.

1235
00:47:31,020 --> 00:47:33,100
손실도 로그 공간에서 계산할 겁니다.

1236
00:47:33,100 --> 00:47:34,600
수치적 안정성을 위해 실제로는

1237
00:47:34,600 --> 00:47:37,138
거의 모든 계산을 로그 공간에서 하게 됩니다.

1238
00:47:37,138 --> 00:47:38,680
그래서 p(x)는

1239
00:47:38,680 --> 00:47:41,560
트랜스포머 상단에서 이전 모든 토큰에 조건부로

1240
00:47:41,560 --> 00:47:43,600
다음 토큰에 대한 확률 분포를

1241
00:47:43,600 --> 00:47:46,178
출력하기 때문에 생성되는 겁니다.

1242
00:47:46,178 --> 00:47:48,220
그리고 시퀀스의 모든 지점에 대해 그렇게 합니다.

1243
00:47:48,220 --> 00:47:50,720
그래서 시퀀스 내 모든 점들의 값을

1244
00:47:50,720 --> 00:47:53,360
곱하면 이 정확한 확률 밀도 값을

1245
00:47:53,360 --> 00:47:54,740
복원할 수 있습니다.

1246
00:47:54,740 --> 00:47:56,160
입력 시퀀스가 있으면,

1247
00:47:56,160 --> 00:47:58,160
이를 transformer에

1248
00:47:58,160 --> 00:48:01,400
통과시키고, transformer는 시퀀스의 각

1249
00:48:01,400 --> 00:48:03,600
지점에서 이전 부분을 조건으로 모든

1250
00:48:03,600 --> 00:48:05,480
토큰에 대한 분포를 예측합니다.

1251
00:48:05,480 --> 00:48:07,780
그리고 실제 다음 토큰이 무엇이었는지, 다음

1252
00:48:07,780 --> 00:48:09,780
토큰의 예측 확률이 얼마였는지를 계산한 후,

1253
00:48:09,780 --> 00:48:12,387
시퀀스 전체에 걸쳐 이 값들을 곱할 수 있습니다.

1254
00:48:12,387 --> 00:48:14,720
이렇게 해서 autoregressive 모델 중 하나에서

1255
00:48:14,720 --> 00:48:16,303
정확한 밀도 값을 복원할 수 있는 거죠.

1256
00:48:16,303 --> 00:48:17,803
그리고 이 방법은 RNN이나

1257
00:48:17,803 --> 00:48:18,980
transformer 모두에 적용됩니다.

1258
00:48:22,560 --> 00:48:24,040
좋은 질문입니다.

1259
00:48:24,040 --> 00:48:27,778
그런데 variational autoencoder에서는 상황이 복잡해집니다.

1260
00:48:27,778 --> 00:48:29,320
그래서 V를 빼고

1261
00:48:29,320 --> 00:48:31,612
autoencoder에 대해서 몇 슬라이드만 이야기할

1262
00:48:31,612 --> 00:48:34,280
건데, 이 과정에서 아직 다루지 않았기 때문입니다.

1263
00:48:34,280 --> 00:48:36,162
variational

1264
00:48:36,162 --> 00:48:38,120
autoencoder는 기본적으로

1265
00:48:38,120 --> 00:48:43,080
레이블 없이 입력 x에서 특징 z를 추출하는 비지도 학습 방법입니다.

1266
00:48:43,080 --> 00:48:47,320
이것은 우리가 방금 이야기한 self-supervised

1267
00:48:47,320 --> 00:48:49,120
learning의 한 종류입니다.

1268
00:48:49,120 --> 00:48:51,120
특징은 데이터에 관한 유용한

1269
00:48:51,120 --> 00:48:53,580
정보를 추출해야 한다는 개념입니다.

1270
00:48:53,580 --> 00:48:55,680
어쩌면 이미지 내 객체의 정체가

1271
00:48:55,680 --> 00:48:59,160
무엇인지, 몇 개가 있는지 암묵적으로 인코딩할 수도

1272
00:48:59,160 --> 00:48:59,660
있겠죠?

1273
00:48:59,660 --> 00:49:00,827
그들의 색깔은 무엇인지도요?

1274
00:49:00,827 --> 00:49:03,480
우리는 이 특징 벡터 z가 입력 x에

1275
00:49:03,480 --> 00:49:05,958
관한 유용한 정보를 담기를 원합니다.

1276
00:49:05,958 --> 00:49:08,000
이 인코더 자체는 어떤 아키텍처의 신경망도

1277
00:49:08,000 --> 00:49:08,833
될 수 있습니다.

1278
00:49:08,833 --> 00:49:11,920
MLP, transformer, CNN, 원하는 어떤 것이든 가능합니다.

1279
00:49:11,920 --> 00:49:13,440
하지만 입력은 데이터 x입니다.

1280
00:49:13,440 --> 00:49:16,240
그리고 나서 어떤 벡터 z를 출력할 겁니다.

1281
00:49:16,240 --> 00:49:18,732
그런데 문제는, 레이블 없이 이걸 어떻게 하느냐는 거죠.

1282
00:49:18,732 --> 00:49:20,440
사실 이전 강의에서 이와 관련된

1283
00:49:20,440 --> 00:49:21,720
예시를 많이 봤습니다.

1284
00:49:21,720 --> 00:49:23,320
그런데 아주 간단한 방법이 하나 있는데,

1285
00:49:23,320 --> 00:49:25,360
바로 입력을 재구성하려고 시도하는 겁니다.

1286
00:49:25,360 --> 00:49:28,080
그래서 이제 모델의 두 번째 부분인 디코더가

1287
00:49:28,080 --> 00:49:30,920
생기는데, 이 디코더는 z를 입력받아 다시

1288
00:49:30,920 --> 00:49:32,320
x를 출력할 겁니다.

1289
00:49:32,320 --> 00:49:34,960
그리고 우리는—아, x를 떨어뜨렸네요.

1290
00:49:34,960 --> 00:49:36,425
이걸 훈련시켜서 모델의

1291
00:49:36,425 --> 00:49:37,800
출력이 실제로

1292
00:49:37,800 --> 00:49:40,220
입력과 일치하도록 만들 겁니다.

1293
00:49:40,220 --> 00:49:42,220
어떤 면에서는 가장 단순한 손실 함수입니다.

1294
00:49:42,220 --> 00:49:44,790
우리는 모델이 항등 함수(identity function)를 흉내 내도록
훈련시키는 거죠.

1295
00:49:44,790 --> 00:49:45,540
왜 이렇게 하냐고요?

1296
00:49:45,540 --> 00:49:46,860
우리는 이미 항등 함수를 알고 있습니다.

1297
00:49:46,860 --> 00:49:48,160
왜 이미 알고 있는 항등

1298
00:49:48,160 --> 00:49:49,840
함수를 배우기 위해서 많은 연산과

1299
00:49:49,840 --> 00:49:52,240
큰 데이터셋으로 신경망을 훈련시키는 걸까요?

1300
00:49:52,240 --> 00:49:55,000
그 이유는 이걸 어떤 식으로든 병목(bottleneck)을 만들 거기
때문입니다.

1301
00:49:55,000 --> 00:49:57,020
만약 이 모델이 무한한 용량을 가졌다면,

1302
00:49:57,020 --> 00:49:59,440
예를 들어 z 벡터가 아주 넓거나 학습에

1303
00:49:59,440 --> 00:50:02,080
제약이 없다면, 신경망이 이 문제를 완벽히

1304
00:50:02,080 --> 00:50:03,520
해결할 거라고 예상합니다.

1305
00:50:03,520 --> 00:50:06,240
하지만 우리는 그렇게 하고 싶지 않습니다, 왜냐하면 명시적으로 이 목적

1306
00:50:06,240 --> 00:50:07,717
함수를 배우는 것에 관심이 없기 때문입니다.

1307
00:50:07,717 --> 00:50:09,300
우리는 이미 항등 함수(identity function)를 알고 있습니다.

1308
00:50:09,300 --> 00:50:11,633
이것을 계산하기 위해 비싼 신경망이 필요하지 않습니다.

1309
00:50:11,633 --> 00:50:13,520
우리가 하고 싶은 것은 네트워크가 어떤

1310
00:50:13,520 --> 00:50:16,265
제약 조건 하에서 항등 함수를 학습하도록 강제하는 것입니다.

1311
00:50:16,265 --> 00:50:17,640
전통적인 오토인코더에서

1312
00:50:17,640 --> 00:50:20,680
자주 사용하는 제약 조건은 표현 z를 병목(bottleneck)으로

1313
00:50:20,680 --> 00:50:22,120
만드는 것입니다.

1314
00:50:22,120 --> 00:50:25,120
특히, 중간에 있는 벡터 z가

1315
00:50:25,120 --> 00:50:28,120
입력 x보다 훨씬 작다는 의미입니다.

1316
00:50:28,120 --> 00:50:31,520
입력 x는 고해상도 이미지일 수 있는데, 예를 들어 1024x1024

1317
00:50:31,520 --> 00:50:34,040
크기의 이미지로, 300만 개의 실수로

1318
00:50:34,040 --> 00:50:35,380
구성되어 있다고 했죠.

1319
00:50:35,380 --> 00:50:39,240
하지만 z는 128차원의 잠재 코드(latent code)일 수 있습니다.

1320
00:50:39,240 --> 00:50:41,920
그래서 모델은 출력, 즉 데이터를

1321
00:50:41,920 --> 00:50:43,940
재구성하는 문제를

1322
00:50:43,940 --> 00:50:46,040
해결해야 하는데, 중간의 병목

1323
00:50:46,040 --> 00:50:49,278
표현을 통해 데이터를 압축해야 합니다.

1324
00:50:49,278 --> 00:50:51,320
우리는 이것이 모델이

1325
00:50:51,320 --> 00:50:54,960
네트워크 중간의 이 표현을 통해 데이터를 비자명한

1326
00:50:54,960 --> 00:50:57,000
구조로 학습하도록

1327
00:50:57,000 --> 00:50:59,025
강제할 것이라고 기대합니다.

1328
00:50:59,025 --> 00:51:00,400
그리고 나서 이 작업을 한 후에는,

1329
00:51:00,400 --> 00:51:02,400
디코더를 버리고 이 z를 사용해 다운스트림

1330
00:51:02,400 --> 00:51:04,660
작업을 위한 지도 학습 모델을 초기화하는 일반적인

1331
00:51:04,660 --> 00:51:07,280
자기지도 학습(self-supervised learning)

1332
00:51:07,280 --> 00:51:08,540
기법을 적용할 수 있습니다.

1333
00:51:08,540 --> 00:51:11,160
방금 본 자기지도 학습 이야기와

1334
00:51:11,160 --> 00:51:13,040
같은 방식입니다.

1335
00:51:13,040 --> 00:51:15,320
그런데 만약 우리가 이걸 데이터 생성에

1336
00:51:15,320 --> 00:51:17,265
실제로 사용하고 싶다면 어떻게 할까요?

1337
00:51:17,265 --> 00:51:18,640
그렇다면 우리가 정말로

1338
00:51:18,640 --> 00:51:21,220
원하는 것은 자기지도 학습 이야기의 반대입니다.

1339
00:51:21,220 --> 00:51:23,860
즉, 인코더를 버리고 대신

1340
00:51:23,860 --> 00:51:27,800
모델이 데이터를 표현하는 z와 일치하는 z를

1341
00:51:27,800 --> 00:51:30,560
샘플링할 수 있어야 합니다.

1342
00:51:30,560 --> 00:51:33,840
만약 데이터 분포와 일치하는 z를 샘플링하는

1343
00:51:33,840 --> 00:51:36,300
절차가 있다면, z를 샘플링해서

1344
00:51:36,300 --> 00:51:40,060
학습된 디코더에 통과시키면 새로운 샘플을

1345
00:51:40,060 --> 00:51:41,560
생성할 수 있습니다.

1346
00:51:41,560 --> 00:51:43,640
이제 이것은 암묵적 방법(implicit method)입니다.

1347
00:51:43,640 --> 00:51:46,080
밀도가 아무 데도 떠다니지 않는다고

1348
00:51:46,080 --> 00:51:46,820
말씀하셨죠.

1349
00:51:46,820 --> 00:51:48,440
하지만 만약 우리가 이렇게

1350
00:51:48,440 --> 00:51:51,240
할 수 있는 방법이 있다면, 밀도를 명시적으로

1351
00:51:51,240 --> 00:51:55,070
모델링하지 않고도 모델에서 샘플을 뽑는 방법이 될 수 있습니다.

1352
00:51:55,070 --> 00:51:58,150
하지만 문제는 우리가 이미 문제를 조금

1353
00:51:58,150 --> 00:52:01,377
미뤄둔 셈이라는 겁니다. 이미지를 생성하려면

1354
00:52:01,377 --> 00:52:03,210
x를 생성해야 하니까요.

1355
00:52:03,210 --> 00:52:04,710
우리는 x의 데이터셋을 가지고 있습니다.

1356
00:52:04,710 --> 00:52:05,895
어떻게 해야 할까요?

1357
00:52:05,895 --> 00:52:07,270
우리는 이 문제를 오토인코더를

1358
00:52:07,270 --> 00:52:08,523
훈련시켜 해결하겠다고 했습니다.

1359
00:52:08,523 --> 00:52:10,190
이제 z의 데이터셋이 생겼고, z

1360
00:52:10,190 --> 00:52:11,530
공간에서 샘플링을 해야 합니다.

1361
00:52:11,530 --> 00:52:12,730
그게 더 쉬운 것도 아니죠.

1362
00:52:12,730 --> 00:52:14,870
그래서 우리는 막혔습니다.

1363
00:52:14,870 --> 00:52:18,510
변분 오토인코더의 아이디어는, 만약 z에 어떤

1364
00:52:18,510 --> 00:52:22,110
구조를 강제할 수 있다면 어떨까 하는 겁니다.

1365
00:52:22,110 --> 00:52:24,630
기존 오토인코더 구조는 z에

1366
00:52:24,630 --> 00:52:27,270
어떤 알려진 구조를 강제하지

1367
00:52:27,270 --> 00:52:27,970
않습니다.

1368
00:52:27,970 --> 00:52:30,590
단지 잠재 표현을 주고 데이터를

1369
00:52:30,590 --> 00:52:32,050
재구성하라고 요구할 뿐이죠.

1370
00:52:32,050 --> 00:52:33,750
하지만 만약 z가 가우시안

1371
00:52:33,750 --> 00:52:36,490
분포나 다른 알려진 분포에서 나오도록 강제하는

1372
00:52:36,490 --> 00:52:38,050
메커니즘이 있다면 어떨까요?

1373
00:52:38,050 --> 00:52:41,190
그렇다면 추론 시에 그냥 그 알려진 분포에서 샘플을

1374
00:52:41,190 --> 00:52:42,293
뽑으면 됩니다.

1375
00:52:42,293 --> 00:52:44,710
이 모델이 훈련된 후에, 그

1376
00:52:44,710 --> 00:52:47,890
알려진 분포에서 샘플을 뽑아 디코더에

1377
00:52:47,890 --> 00:52:51,070
통과시키면, 이제 샘플을 얻을 수 있습니다.

1378
00:52:51,070 --> 00:52:53,870
그래서 오토인코더를 확률적으로 만들고

1379
00:52:53,870 --> 00:52:57,170
잠재 공간에 확률적 구조를 강제하는 것이

1380
00:52:57,170 --> 00:53:00,510
바로 변분 오토인코더가 하려는 일입니다.

1381
00:53:00,510 --> 00:53:01,790
왜 변분인가요?

1382
00:53:01,790 --> 00:53:02,990
이야기가 길어요.

1383
00:53:02,990 --> 00:53:06,470
문헌에서 그 용어에 긴 역사가

1384
00:53:06,470 --> 00:53:08,070
있다고 합니다.

1385
00:53:08,070 --> 00:53:09,790
기본적으로 변분 오토인코더는

1386
00:53:09,790 --> 00:53:12,950
전통적인 오토인코더에 확률적 관점을 더한 것입니다.

1387
00:53:12,950 --> 00:53:16,530
그래서 원시 데이터에서 잠재 특징 z를 학습할 겁니다.

1388
00:53:16,530 --> 00:53:19,310
그리고 학습된 잠재 공간 z에 구조를

1389
00:53:19,310 --> 00:53:22,110
강제해서, 모델이 훈련된 후 추론 시점에

1390
00:53:22,110 --> 00:53:24,350
그 공간에서 샘플링하여

1391
00:53:24,350 --> 00:53:26,710
새로운 샘플을 생성할 수 있습니다.

1392
00:53:26,710 --> 00:53:30,610
좀 더 구체적으로, 훈련 데이터 xi를 가정할 텐데,

1393
00:53:30,610 --> 00:53:33,110
여기서 위 첨자 i는 서로

1394
00:53:33,110 --> 00:53:35,510
독립적인 x 샘플들을 의미합니다.

1395
00:53:35,510 --> 00:53:39,110
각 xi는 어떤 잠재 벡터 z에서

1396
00:53:39,110 --> 00:53:42,510
생성되었다고 가정합니다. 즉, 모든

1397
00:53:42,510 --> 00:53:45,670
xi에 대응하는 잠재 벡터 zi가

1398
00:53:45,670 --> 00:53:47,550
존재한다는 겁니다.

1399
00:53:47,550 --> 00:53:50,370
데이터를 생성하는 우주의 절차는 먼저

1400
00:53:50,370 --> 00:53:55,212
z를 생성하고, 그 다음에 zi로부터 xi를 생성하는 방식입니다.

1401
00:53:55,212 --> 00:53:56,670
우리가 본 이미지를

1402
00:53:56,670 --> 00:53:59,670
생성하는 데 필요한 모든 정보는 그

1403
00:53:59,670 --> 00:54:02,110
잠재 벡터 z에 담겨 있습니다.

1404
00:54:02,110 --> 00:54:04,167
하지만 우리는 그 잠재 벡터 z를 볼 수 없습니다.

1405
00:54:04,167 --> 00:54:05,250
절대 관찰할 수 없죠.

1406
00:54:05,250 --> 00:54:06,810
그런 데이터 세트도 없습니다.

1407
00:54:06,810 --> 00:54:10,590
직관적으로 x는 이미지이고, z는 그 이미지에 대해 알아야

1408
00:54:10,590 --> 00:54:12,910
할 모든 정보를 담은 잠재 특징

1409
00:54:12,910 --> 00:54:14,910
표현이지만, 우리는 그 잠재 벡터를

1410
00:54:14,910 --> 00:54:16,670
절대 관찰할 수 없습니다.

1411
00:54:16,670 --> 00:54:19,480
그리고 훈련 후에는 샘플을 생성할 수 있는데,

1412
00:54:19,480 --> 00:54:21,230
또 다른 제약은 그 z들이

1413
00:54:21,230 --> 00:54:24,152
알려진 분포에서 나오도록 강제한다는 점입니다.

1414
00:54:24,152 --> 00:54:25,610
그래서 모델이 훈련된 후에는 우리가

1415
00:54:25,610 --> 00:54:27,318
방금 말한 것을 정확히 할 수 있습니다.

1416
00:54:27,318 --> 00:54:29,210
알려진 분포에서 z를 뽑아서

1417
00:54:29,210 --> 00:54:32,630
디코더에 통과시키면, 샘플을 얻을 수 있습니다.

1418
00:54:32,630 --> 00:54:34,910
그리고 보통은 간단한 prior를 가정합니다.

1419
00:54:34,910 --> 00:54:37,430
거의 항상 단위 가우시안

1420
00:54:37,430 --> 00:54:39,590
분포가 가장 흔합니다.

1421
00:54:39,590 --> 00:54:41,590
그럼 이걸 어떻게 훈련할 수 있을까요?

1422
00:54:41,590 --> 00:54:43,510
이건 불가능한 문제처럼 느껴집니다.

1423
00:54:43,510 --> 00:54:45,430
기본적으로 이 네트워크를

1424
00:54:45,430 --> 00:54:49,250
훈련시켜서 모든 x에 대해 z를 찾으려고 하는 거죠.

1425
00:54:49,250 --> 00:54:50,493
우리는 z를 관찰할 수 없습니다.

1426
00:54:50,493 --> 00:54:51,410
이건 불가능해 보입니다.

1427
00:54:51,410 --> 00:54:52,510
어떻게 할까요?

1428
00:54:52,510 --> 00:54:54,910
최대 우도법으로 돌아가겠습니다.

1429
00:54:54,910 --> 00:54:58,750
만약 x와 z의 데이터셋이 있다면, 최대 우도법을

1430
00:54:58,750 --> 00:55:02,030
사용해서 로그 확률을 최대화하는 같은 로그

1431
00:55:02,030 --> 00:55:02,670
트릭을 직접

1432
00:55:02,670 --> 00:55:03,970
쓸 수 있습니다.

1433
00:55:03,970 --> 00:55:06,875
이전에 봤던 것과 똑같은 방법을 쓸 수 있죠.

1434
00:55:06,875 --> 00:55:08,750
그리고 z에 조건부인 x의

1435
00:55:08,750 --> 00:55:10,750
생성 모델 p를 훈련시킵니다.

1436
00:55:10,750 --> 00:55:11,690
하지만 우리는 z를 모릅니다.

1437
00:55:11,690 --> 00:55:13,232
하지만 잠시 z를 안다고 가정해 봅시다.

1438
00:55:15,470 --> 00:55:18,630
z를 모르기 때문에 주변화(marginalize)를 시도할 수 있습니다.

1439
00:55:18,630 --> 00:55:21,190
우리는 x의 p가 존재한다고 알고 있습니다--

1440
00:55:21,190 --> 00:55:24,670
관찰할 수는 없지만 x와 z의 어떤 결합 분포가

1441
00:55:24,670 --> 00:55:26,110
반드시 존재해야 합니다.

1442
00:55:26,110 --> 00:55:28,110
그리고 원칙적으로는 z를 적분하여

1443
00:55:28,110 --> 00:55:31,150
주변화함으로써 x의 p를 얻을 수 있습니다.

1444
00:55:31,150 --> 00:55:34,350
그리고 아마도 x와 z의 결합 분포가 있다고

1445
00:55:34,350 --> 00:55:36,750
가정하고, z를 어떻게든 주변화해서

1446
00:55:36,750 --> 00:55:39,650
최대 우도 추정을 할 수 있을 겁니다.

1447
00:55:39,650 --> 00:55:40,770
이게 어떻게 작동하는지 봅시다.

1448
00:55:40,770 --> 00:55:41,977
그래서 이

1449
00:55:41,977 --> 00:55:44,310
항에서-- 그리고 여기서는

1450
00:55:44,310 --> 00:55:48,190
체인 룰을 사용해 x와 z의 결합 확률 p를

1451
00:55:48,190 --> 00:55:51,630
p(x|z)와 p(z)로 나눴습니다.

1452
00:55:51,630 --> 00:55:54,230
이 p(x|z)는 괜찮습니다.

1453
00:55:54,230 --> 00:55:56,110
왼쪽에 있는 디코더로 계산할 수

1454
00:55:56,110 --> 00:55:58,277
있는데, 이 디코더는 우리가 훈련시키려는

1455
00:55:58,277 --> 00:55:59,030
신경망입니다.

1456
00:55:59,030 --> 00:56:00,015
이 p(z) 항도 괜찮습니다.

1457
00:56:00,015 --> 00:56:01,390
우리는 그것이 단위 가우시안이거나

1458
00:56:01,390 --> 00:56:03,432
우리가 계산하거나 추론할 수 있는

1459
00:56:03,432 --> 00:56:05,430
다른 간단한 분포라고 가정할 것입니다.

1460
00:56:05,430 --> 00:56:06,910
하지만 이 적분이 문제를 일으킵니다.

1461
00:56:06,910 --> 00:56:08,910
일반적으로 신경망 입력 전체

1462
00:56:08,910 --> 00:56:12,470
공간에 대해 적분할 현실적인 방법이 없습니다.

1463
00:56:12,470 --> 00:56:14,088
이 p(x|z)는 신경망으로

1464
00:56:14,088 --> 00:56:15,630
모델링된 매우 복잡한

1465
00:56:15,630 --> 00:56:16,890
함수일 것입니다.

1466
00:56:16,890 --> 00:56:19,015
이것을 해석적으로나 정확하게

1467
00:56:19,015 --> 00:56:20,877
적분할 방법은 전혀 없습니다.

1468
00:56:20,877 --> 00:56:23,210
여기서 개별 부분에 대해 신경망을 훈련할 수 있습니다.

1469
00:56:23,210 --> 00:56:24,748
그래서 확률 모델링을

1470
00:56:24,748 --> 00:56:26,790
할 때 기본 개념은

1471
00:56:26,790 --> 00:56:29,292
확률적 항들을 적어내는 것입니다.

1472
00:56:29,292 --> 00:56:31,750
운이 좋으면 그중 일부는 해석적으로 적어내고

1473
00:56:31,750 --> 00:56:33,958
추론할 수 있는 간단한 분포일 것입니다.

1474
00:56:33,958 --> 00:56:36,083
일부는 학습 가능한 신경망 구성 요소일

1475
00:56:36,083 --> 00:56:36,730
것입니다.

1476
00:56:36,730 --> 00:56:38,710
그래서 우리는 z가 주어졌을 때

1477
00:56:38,710 --> 00:56:40,630
x의 확률이 원칙적으로 최대

1478
00:56:40,630 --> 00:56:44,110
우도법으로 학습할 수 있는 신경망일 것이라고 가정합니다.

1479
00:56:44,110 --> 00:56:47,550
하지만 최대 우도법으로 그 신경망을 학습하기 위해 어떤

1480
00:56:47,550 --> 00:56:50,467
목적 함수를 쓸 수 있을지 적기 시작하는 겁니다.

1481
00:56:50,467 --> 00:56:52,550
여기서는 운이 없는데, z에 대해 적분할

1482
00:56:52,550 --> 00:56:53,870
방법이 없기 때문입니다.

1483
00:56:53,870 --> 00:56:55,670
유한한 샘플링으로 그 적분을

1484
00:56:55,670 --> 00:56:58,683
근사하려고 시도할 수는 있지만,

1485
00:56:58,683 --> 00:57:00,350
일반적으로는 잘 작동하지

1486
00:57:00,350 --> 00:57:04,230
않을 겁니다. 왜냐하면 z가 매우 고차원 공간이고, 훈련의

1487
00:57:04,230 --> 00:57:07,070
내부 루프에서 근사 수치 적분을

1488
00:57:07,070 --> 00:57:09,910
하는 것은 좋은 생각이 아니기 때문입니다.

1489
00:57:09,910 --> 00:57:11,773
그래서 다른 방법을 시도해 볼 수 있습니다.

1490
00:57:11,773 --> 00:57:13,190
베이즈 규칙, 확률에서 항상

1491
00:57:13,190 --> 00:57:14,490
하는 또 다른 방법이죠.

1492
00:57:14,490 --> 00:57:15,830
그럼 베이즈 규칙을 써 봅시다.

1493
00:57:15,830 --> 00:57:18,110
베이즈 규칙이 있으면 p(x)를

1494
00:57:18,110 --> 00:57:20,890
표현할 수 있는 또 다른 식이 생깁니다.

1495
00:57:20,890 --> 00:57:24,550
그래서 p(x)는 화면에 있는 이 식에서 베이즈

1496
00:57:24,550 --> 00:57:26,430
규칙으로 쓸 수 있습니다.

1497
00:57:26,430 --> 00:57:28,710
이 항들을 가지고 무엇을 할 수 있는지 봅시다.

1498
00:57:28,710 --> 00:57:31,070
먼저 p(x|z)는 다시 디코더로

1499
00:57:31,070 --> 00:57:32,870
계산할 수 있습니다.

1500
00:57:32,870 --> 00:57:35,930
p(z)는, 이건 가우시안이라고 가정하니까

1501
00:57:35,930 --> 00:57:37,510
계산할 수 있습니다.

1502
00:57:37,510 --> 00:57:38,593
여기에는 적분이 없습니다.

1503
00:57:38,593 --> 00:57:39,170
좋습니다.

1504
00:57:39,170 --> 00:57:40,870
그래서 상황이 좋습니다.

1505
00:57:40,870 --> 00:57:43,950
하지만 이제 우리는 이 p of z given x 항을 계산할 방법이
없습니다.

1506
00:57:43,950 --> 00:57:47,070
이 posterior of z given x는 우리가 계산할

1507
00:57:47,070 --> 00:57:48,243
좋은 방법이 없습니다.

1508
00:57:48,243 --> 00:57:49,660
이 항을 계산하려면

1509
00:57:49,660 --> 00:57:52,000
어떤 적분도 필요할 텐데, 운이 없네요.

1510
00:57:52,000 --> 00:57:52,880
우리는 그것을 계산할 수 없습니다.

1511
00:57:52,880 --> 00:57:55,380
어떻게 할까요?

1512
00:57:55,380 --> 00:57:56,980
다른 신경망을 사용해 봅시다.

1513
00:57:56,980 --> 00:57:58,660
그래서 variational

1514
00:57:58,660 --> 00:58:01,340
autoencoder 트릭은 여기 아래에

1515
00:58:01,340 --> 00:58:04,180
있는 확률항, 즉 우리가 계산할 수 없는 베이즈

1516
00:58:04,180 --> 00:58:05,900
규칙을 대신할 다른 신경망을

1517
00:58:05,900 --> 00:58:07,460
넣어서 계산하려는 겁니다.

1518
00:58:07,460 --> 00:58:09,335
그래서 우리는 다른 가중치

1519
00:58:09,335 --> 00:58:12,300
phi를 가진 또 다른 신경망 q를

1520
00:58:12,300 --> 00:58:15,420
만들어서 z given x의 조건부

1521
00:58:15,420 --> 00:58:17,860
확률 분포를 학습하게 할 겁니다.

1522
00:58:17,860 --> 00:58:20,220
그리고 전체 목표는 이 다른 신경망이

1523
00:58:20,220 --> 00:58:22,940
첫 번째 신경망의 진짜 p of x given

1524
00:58:22,940 --> 00:58:24,900
z를 근사하도록 하는 겁니다.

1525
00:58:24,900 --> 00:58:27,400
일반적으로 이걸 강제할 수는

1526
00:58:27,400 --> 00:58:31,740
없지만, 신경망을 넣고 어떻게 되는지 봅시다.

1527
00:58:31,740 --> 00:58:34,540
만약 우리가 계산할 수 없는 아래 항을

1528
00:58:34,540 --> 00:58:36,248
근사하는 다른 신경망을

1529
00:58:36,248 --> 00:58:38,620
갖게 된다면, 우리는

1530
00:58:38,620 --> 00:58:41,220
우도(likelihood)를 계산하고 최대

1531
00:58:41,220 --> 00:58:43,820
우도 추정을 할 수 있을 겁니다.

1532
00:58:43,820 --> 00:58:45,420
이것이 variational autoencoder를

1533
00:58:45,420 --> 00:58:46,517
훈련할 때 하는 일입니다.

1534
00:58:46,517 --> 00:58:48,100
기본적으로 두 개의 다른

1535
00:58:48,100 --> 00:58:49,520
신경망을 함께 학습하는 거죠.

1536
00:58:49,520 --> 00:58:52,220
하나는 잠재 코드 z를 입력받아 데이터

1537
00:58:52,220 --> 00:58:55,340
x에 대한 분포를 출력하는 디코더입니다.

1538
00:58:55,340 --> 00:58:57,460
다른 하나는 데이터 x를

1539
00:58:57,460 --> 00:59:00,620
입력받아 잠재 코드 z에 대한 분포를 출력하는

1540
00:59:00,620 --> 00:59:01,755
인코더입니다.

1541
00:59:01,755 --> 00:59:04,380
각각은 독립적인 가중치를 가진

1542
00:59:04,380 --> 00:59:07,365
별도의 신경망으로 따로 훈련됩니다.

1543
00:59:07,365 --> 00:59:08,740
여기서 질문이 있을

1544
00:59:08,740 --> 00:59:11,420
수 있습니다. 신경망에서 어떻게 확률

1545
00:59:11,420 --> 00:59:12,960
분포를 출력할 수 있나요?

1546
00:59:12,960 --> 00:59:15,900
그게 혼란스럽고 어렵고 명확하지 않아 보이죠.

1547
00:59:15,900 --> 00:59:18,540
여기서 트릭은 모든 출력을 정규분포로

1548
00:59:18,540 --> 00:59:20,070
강제하는 것입니다.

1549
00:59:20,070 --> 00:59:21,820
그리고 신경망이 정규분포의

1550
00:59:21,820 --> 00:59:24,358
파라미터를 출력하게 하는 거죠.

1551
00:59:24,358 --> 00:59:25,900
보통 디코더 네트워크는

1552
00:59:25,900 --> 00:59:28,100
출력 분포가 대각선

1553
00:59:28,100 --> 00:59:30,180
가우시안(diagonal

1554
00:59:30,180 --> 00:59:34,020
Gaussian)이라고 가정합니다. 여기서

1555
00:59:34,020 --> 00:59:37,020
대각선의 항목들은 신경망의 픽셀들입니다.

1556
00:59:37,020 --> 00:59:38,980
모델은 그 대각선 가우시안

1557
00:59:38,980 --> 00:59:40,762
분포의 평균을 출력합니다.

1558
00:59:40,762 --> 00:59:42,220
보통 디코더는 고정된

1559
00:59:42,220 --> 00:59:45,260
분산 또는 표준편차 sigma

1560
00:59:45,260 --> 00:59:46,620
제곱을 가정합니다.

1561
00:59:46,620 --> 00:59:49,520
인코더 네트워크도 같은 아이디어입니다.

1562
00:59:49,520 --> 00:59:51,723
모델은 데이터 샘플 x를 입력받고,

1563
00:59:51,723 --> 00:59:54,140
그 다음 z given x의

1564
00:59:54,140 --> 00:59:59,340
분포 q를 모델링하는 가우시안 분포의 파라미터를 출력합니다.

1565
00:59:59,340 --> 01:00:01,740
즉, 인코더 네트워크는

1566
01:00:01,740 --> 01:00:04,700
가우시안 분포의 평균 벡터

1567
01:00:04,700 --> 01:00:06,420
하나와 공분산

1568
01:00:06,420 --> 01:00:09,580
행렬의 대각선 벡터 하나를

1569
01:00:09,580 --> 01:00:10,480
출력합니다.

1570
01:00:10,480 --> 01:00:11,980
여기서 대각선 구조를

1571
01:00:11,980 --> 01:00:15,020
가정하는 것이 매우 중요한데, 그렇지 않으면

1572
01:00:15,020 --> 01:00:19,060
전체 공분산 행렬의 h 제곱 개 항목을 모델링해야 하기

1573
01:00:19,060 --> 01:00:19,820
때문입니다.

1574
01:00:19,820 --> 01:00:24,180
예를 들어 이미지가 h 곱 w 픽셀이라면, 대각 행렬의

1575
01:00:24,180 --> 01:00:26,820
항목들은 전체 픽셀에 해당합니다.

1576
01:00:26,820 --> 01:00:29,400
원칙적으로는 이미지 내 모든 픽셀 쌍 간의

1577
01:00:29,400 --> 01:00:31,460
전체 공분산을 모델링할 수

1578
01:00:31,460 --> 01:00:33,295
있지만, 그러면 h 제곱

1579
01:00:33,295 --> 01:00:35,420
곱 w 제곱 개 항목이 필요해서

1580
01:00:35,420 --> 01:00:36,700
너무 커집니다.

1581
01:00:36,700 --> 01:00:39,380
그래서 우리는 서로 다른 값들 간의

1582
01:00:39,380 --> 01:00:41,460
상관관계 구조는 무시합니다.

1583
01:00:41,460 --> 01:00:44,060
이제 대각 공분산은 데이터 자체와

1584
01:00:44,060 --> 01:00:46,700
같은 크기의 벡터가 됩니다.

1585
01:00:46,700 --> 01:00:48,460
즉, 이것이 바로 mu of z given x입니다.

1586
01:00:48,460 --> 01:00:50,620
그리고 x가 주어졌을 때 z의

1587
01:00:50,620 --> 01:00:52,980
시그마도 z와 같은 형태의 벡터입니다.

1588
01:00:52,980 --> 01:00:56,900
그래서 기본적으로 신경망이 같은 형태의 두 벡터를

1589
01:00:56,900 --> 01:00:58,500
출력하고, 이를 이 가우시안

1590
01:00:58,500 --> 01:01:01,300
분포의 파라미터로 취급하는 겁니다.

1591
01:01:01,300 --> 01:01:03,340
이렇게 해서 신경망에서 분포를

1592
01:01:03,340 --> 01:01:04,860
출력할 수 있습니다.

1593
01:01:04,860 --> 01:01:08,460
만약 고정된 표준편차로 최대 우도 추정을

1594
01:01:08,460 --> 01:01:10,620
하면, 실제로 L2와

1595
01:01:10,620 --> 01:01:14,043
동등해지는데, 이게 좋은 트릭입니다.

1596
01:01:14,043 --> 01:01:15,460
이걸 하는 이유는

1597
01:01:15,460 --> 01:01:19,260
대각선 공분산을 모델링하려고 하면, 원칙적으로

1598
01:01:19,260 --> 01:01:21,780
디코더에서 같은 걸 모델링하고

1599
01:01:21,780 --> 01:01:25,060
각 픽셀마다 별도의 분산을 모델링할 수

1600
01:01:25,060 --> 01:01:25,960
있겠지만,

1601
01:01:25,960 --> 01:01:28,622
그건 쓸모가 없는데, 픽셀 간 공분산

1602
01:01:28,622 --> 01:01:30,580
구조를 전혀 모델링하지

1603
01:01:30,580 --> 01:01:32,163
않는다면, 각

1604
01:01:32,163 --> 01:01:35,500
픽셀이 조금씩 변할 수 있고, 그 변동량이

1605
01:01:35,500 --> 01:01:37,820
픽셀마다 다르다는 의미가 되기

1606
01:01:37,820 --> 01:01:38,753
때문입니다.

1607
01:01:38,753 --> 01:01:40,420
그 분포에서 샘플링하는

1608
01:01:40,420 --> 01:01:43,500
건 평균을 고정하고 픽셀별

1609
01:01:43,500 --> 01:01:46,420
독립 노이즈를 픽셀별 분산에 맞게

1610
01:01:46,420 --> 01:01:48,360
더하는 것과 같아서,

1611
01:01:48,360 --> 01:01:50,540
합리적인 방법이 아닙니다.

1612
01:01:50,540 --> 01:01:55,300
그래서 일반적으로 디코더에서는 약간 속이는데, 확률 분포를

1613
01:01:55,300 --> 01:01:58,040
출력하는 척하지만 실제로는 그

1614
01:01:58,040 --> 01:01:59,220
분포에서 샘플링하지

1615
01:01:59,220 --> 01:02:00,178
않습니다.

1616
01:02:00,178 --> 01:02:02,380
대신 항상 평균을 출력하는 거죠.

1617
01:02:02,380 --> 01:02:03,760
이해가 되시나요?

1618
01:02:03,760 --> 01:02:04,260
네.

1619
01:02:04,260 --> 01:02:06,960
그리고 이걸 수식으로 쓰면, 고정된

1620
01:02:06,960 --> 01:02:09,020
시그마 제곱은 앞에

1621
01:02:09,020 --> 01:02:10,420
상수로 빠져나옵니다.

1622
01:02:10,420 --> 01:02:14,380
실제로 대각선에 고정된 분산을 가진 가우시안

1623
01:02:14,380 --> 01:02:18,807
분포의 로그 우도를 최대화하는 것은 평균과 x 사이의

1624
01:02:18,807 --> 01:02:21,140
L2 거리를 최소화하는

1625
01:02:21,140 --> 01:02:23,163
것과 같아서 좋습니다.

1626
01:02:23,163 --> 01:02:24,080
네, 좋은 질문입니다.

1627
01:02:24,080 --> 01:02:27,700
픽셀 이동에 대해 이상한 불변성이나

1628
01:02:27,700 --> 01:02:29,818
비불변성 구조가 있나요?

1629
01:02:29,818 --> 01:02:31,860
그건 신경망을 설계할 때 선택하는

1630
01:02:31,860 --> 01:02:34,180
아키텍처의 특성에 더 가깝습니다.

1631
01:02:34,180 --> 01:02:37,452
그래서 이런 것들을 예측하는 네트워크 아키텍처에 그런 특성을 내장하려고
시도할

1632
01:02:37,452 --> 01:02:38,160
수 있습니다.

1633
01:02:38,160 --> 01:02:41,540
아키텍처에 불변성이나 등변성 같은 속성을

1634
01:02:41,540 --> 01:02:43,118
넣으려고 할 수 있죠.

1635
01:02:43,118 --> 01:02:45,660
하지만 네, 일반적으로 여기 손실 함수 수준에서는 그런

1636
01:02:45,660 --> 01:02:47,320
점이 반영되지 않는 게 맞습니다.

1637
01:02:50,180 --> 01:02:51,800
자, 이제 이런 아이디어가 생겼습니다.

1638
01:02:51,800 --> 01:02:53,683
인코더와 디코더가 있죠.

1639
01:02:53,683 --> 01:02:55,100
하나는 x를 입력받아

1640
01:02:55,100 --> 01:02:59,160
z에 대한 분포를 출력하고, 다른 하나는 x에 대한 분포를 입력받습니다.

1641
01:02:59,160 --> 01:03:00,660
우리의 학습 목표는 무엇일까요?

1642
01:03:00,660 --> 01:03:03,380
여기서 수학을 좀 할 한 슬라이드가 있습니다.

1643
01:03:03,380 --> 01:03:04,820
하지만 금방 끝납니다.

1644
01:03:04,820 --> 01:03:06,020
여기서 기본 아이디어는

1645
01:03:06,020 --> 01:03:08,435
최대 우도법을 하려는 겁니다.

1646
01:03:08,435 --> 01:03:10,060
이것이 보통 우리가 원하는

1647
01:03:10,060 --> 01:03:11,580
단일 목표이고, 생성

1648
01:03:11,580 --> 01:03:14,620
모델링에서 많은 목적 함수의 기본 원칙입니다.

1649
01:03:14,620 --> 01:03:17,185
그래서 우리는 log p(x)를 최대화하려고 합니다.

1650
01:03:17,185 --> 01:03:19,060
그리고 베이즈 정리를 사용해서

1651
01:03:19,060 --> 01:03:21,980
이걸 베이즈 정리 표현의 로그로 쓸 수 있습니다.

1652
01:03:21,980 --> 01:03:24,460
이것은 정확한 동등성입니다.

1653
01:03:24,460 --> 01:03:26,400
이제 좀 이상한 짓을 할 겁니다.

1654
01:03:26,400 --> 01:03:28,500
이 식의 분자와 분모에

1655
01:03:28,500 --> 01:03:30,015
q(z|x)를 곱할 겁니다.

1656
01:03:30,015 --> 01:03:32,140
기억하세요, 우리는 방금 x가 주어졌을

1657
01:03:32,140 --> 01:03:35,660
때 z의 다른 분포 q를 모델링하는 또 다른 신경망 q를

1658
01:03:35,660 --> 01:03:36,667
갑자기 도입했습니다.

1659
01:03:36,667 --> 01:03:38,500
이제 이 베이즈 법칙

1660
01:03:38,500 --> 01:03:42,410
식의 분자와 분모에 그 밀도 항을 곱할 겁니다.

1661
01:03:42,410 --> 01:03:44,450
이제 로그 수학을 좀 해보겠습니다.

1662
01:03:44,450 --> 01:03:49,650
미리 생각해보면, 어떤 이유에서인지 이 항들을 특정한 순서로

1663
01:03:49,650 --> 01:03:52,078
재배열하기로 결정할 겁니다.

1664
01:03:52,078 --> 01:03:54,370
그리고 나중에 각 항이 어디로 갔는지 추적할 수

1665
01:03:54,370 --> 01:03:55,810
있도록 색깔로 구분해 두었습니다.

1666
01:03:55,810 --> 01:03:58,690
로그를 취하고 이걸 세 개의

1667
01:03:58,690 --> 01:04:01,370
별도 항으로 나눕니다.

1668
01:04:01,370 --> 01:04:04,690
이제 또 하나의 중요한 관찰을 해야 하는데,

1669
01:04:04,690 --> 01:04:09,050
바로 이 p(x)는 사실 z에 의존하지 않는다는 점입니다.

1670
01:04:09,050 --> 01:04:11,490
지금까지 이 세 항의 순서는 모두

1671
01:04:11,490 --> 01:04:12,890
정확히 동등합니다.

1672
01:04:12,890 --> 01:04:14,510
이것들은 모두 정확한 등식입니다.

1673
01:04:14,510 --> 01:04:16,565
그래서 이 식에 z가 있긴 하지만

1674
01:04:16,565 --> 01:04:18,690
실제로는 z에 의존하지 않습니다. 모든 z가

1675
01:04:18,690 --> 01:04:20,170
서로 상쇄되기 때문입니다.

1676
01:04:20,170 --> 01:04:22,310
z에 의존하지 않는 어떤 것이

1677
01:04:22,310 --> 01:04:25,850
있다면, 그 위에 z에 대한 기댓값을 항상 씌울

1678
01:04:25,850 --> 01:04:26,770
수 있습니다.

1679
01:04:26,770 --> 01:04:29,250
이 경우, 우리는 이것이 p(x)임을 알고 있습니다.

1680
01:04:29,250 --> 01:04:33,090
우리는 p(x)의 기댓값을 어떤 분포에서

1681
01:04:33,090 --> 01:04:37,410
샘플링한 z에 대해 자유롭게 씌울 수 있습니다.

1682
01:04:37,410 --> 01:04:39,870
그리고 내부 항이 z에

1683
01:04:39,870 --> 01:04:43,010
의존하지 않기 때문에, 어떤 분포를 선택해

1684
01:04:43,010 --> 01:04:46,582
기댓값을 취하더라도 항상 성립합니다.

1685
01:04:46,582 --> 01:04:50,393
기댓값은 선형 연산이므로, 이 기댓값을

1686
01:04:50,393 --> 01:04:52,810
위의 세 항 각각에 적용할 수

1687
01:04:52,810 --> 01:04:53,850
있습니다.

1688
01:04:53,850 --> 01:04:57,090
이제 각각 매우 신비롭게 보이는 세

1689
01:04:57,090 --> 01:04:58,970
개의 항이 생겼습니다.

1690
01:04:58,970 --> 01:05:02,343
하지만 확률에 대한 직관이 많고 이전에

1691
01:05:02,343 --> 01:05:04,010
통계나 확률 수업에서

1692
01:05:04,010 --> 01:05:06,610
봤던 모든 공식을 암기했다면,

1693
01:05:06,610 --> 01:05:10,170
이 중 일부를 인식할 수 있을지도 모릅니다.

1694
01:05:10,170 --> 01:05:13,490
그래서 첫 번째 식은 이전과 똑같이 내려가겠습니다.

1695
01:05:13,490 --> 01:05:18,270
그리고 두 번째와 세 번째 식은 사실 KL 항입니다.

1696
01:05:18,270 --> 01:05:20,370
KL 발산은 확률 분포

1697
01:05:20,370 --> 01:05:23,350
간의 차이를 측정하는 지표입니다.

1698
01:05:23,350 --> 01:05:25,850
그리고 우연히도 이 두 후자의 항에

1699
01:05:25,850 --> 01:05:27,730
정확히 이 정의가 적용됩니다.

1700
01:05:27,730 --> 01:05:30,970
그래서 이 식을 첫 번째 항, 즉 기대값 blah,

1701
01:05:30,970 --> 01:05:33,610
blah, blah(나중에 설명하겠습니다)와

1702
01:05:33,610 --> 01:05:36,810
이 두 개의 KL 항으로 정확히 다시 쓸 수 있습니다.

1703
01:05:36,810 --> 01:05:40,770
이 두 KL 항은 기본적으로 이 슬라이드에

1704
01:05:40,770 --> 01:05:42,730
떠다니는 서로 다른

1705
01:05:42,730 --> 01:05:44,970
확률 분포 간의 불일치나

1706
01:05:44,970 --> 01:05:47,330
차이를 측정하는 겁니다.

1707
01:05:47,330 --> 01:05:49,610
지금 보면 모두 복잡해 보입니다.

1708
01:05:49,610 --> 01:05:51,750
하지만 각 항을 자세히

1709
01:05:51,750 --> 01:05:55,730
보면 이 세 항 각각에 대해 해석 가능한 의미를

1710
01:05:55,730 --> 01:05:57,730
찾을 수 있습니다.

1711
01:05:57,730 --> 01:06:00,290
첫 번째 항은 사실 데이터 재구성 항입니다.

1712
01:06:00,290 --> 01:06:02,090
이게 무슨 뜻인지 살펴보면,

1713
01:06:02,090 --> 01:06:04,110
z를 샘플링한다는 뜻입니다.

1714
01:06:04,110 --> 01:06:08,970
그리고 z를 샘플링하는 방법은 q(z|x), 즉

1715
01:06:08,970 --> 01:06:11,230
인코더를 통해서입니다.

1716
01:06:11,230 --> 01:06:13,750
즉, x를 인코더에 넣습니다.

1717
01:06:13,750 --> 01:06:17,267
인코더는 q(z|x)라는 분포를 예측합니다.

1718
01:06:17,267 --> 01:06:18,850
그 예측된 분포에서

1719
01:06:18,850 --> 01:06:20,250
z를 샘플링합니다.

1720
01:06:20,250 --> 01:06:23,130
그 다음 모든 z에 대해 기대값을 취하고

1721
01:06:23,130 --> 01:06:26,170
x가 주어진 z의 로그 확률을 최대화하는 겁니다.

1722
01:06:26,170 --> 01:06:28,450
이것은 기본적으로 데이터 재구성 항입니다.

1723
01:06:28,450 --> 01:06:31,550
즉, 데이터 포인트 x를 인코더에 넣어

1724
01:06:31,550 --> 01:06:33,890
z에 대한 분포를 얻고, 그

1725
01:06:33,890 --> 01:06:38,170
예측된 z 분포에서 샘플을 뽑아 디코더에 넣으면 x를

1726
01:06:38,170 --> 01:06:40,550
복원할 수 있다는 뜻입니다.

1727
01:06:40,550 --> 01:06:43,490
그래서 이것은 데이터 재구성 항입니다.

1728
01:06:43,490 --> 01:06:45,270
가운데 항은 사전 분포(prior) 항입니다.

1729
01:06:45,270 --> 01:06:46,850
이것은 q(z|x)와

1730
01:06:46,850 --> 01:06:49,930
p(z) 사이의 KL 발산을

1731
01:06:49,930 --> 01:06:51,450
측정한다는 의미입니다.

1732
01:06:51,450 --> 01:06:54,810
기억하세요, q(z|x)는 인코더로,

1733
01:06:54,810 --> 01:06:58,410
입력 데이터 x를 받아 잠재 공간 z에

1734
01:06:58,410 --> 01:07:00,410
대한 분포를 출력합니다.

1735
01:07:00,410 --> 01:07:02,690
즉, 인코더가 예측한 잠재 공간에

1736
01:07:02,690 --> 01:07:04,030
대한 분포입니다.

1737
01:07:04,030 --> 01:07:06,338
그리고 다른 항인 p(z)는 사전 분포입니다.

1738
01:07:06,338 --> 01:07:08,630
보통 대각 가우시안으로 가정한 잠재

1739
01:07:08,630 --> 01:07:10,030
공간의 사전 분포입니다.

1740
01:07:10,030 --> 01:07:11,790
이 항은 모델이

1741
01:07:11,790 --> 01:07:14,130
x에 대해 z 분포를 예측하는데,

1742
01:07:14,130 --> 01:07:17,070
그 예측 분포가 우리가

1743
01:07:17,070 --> 01:07:18,850
미리 설정한 단순한

1744
01:07:18,850 --> 01:07:20,810
가우시안 사전 분포와

1745
01:07:20,810 --> 01:07:23,830
일치하기를 원한다는 뜻입니다.

1746
01:07:23,830 --> 01:07:25,290
즉, 모델이 학습한 잠재

1747
01:07:25,290 --> 01:07:27,650
공간 분포가 사전 분포와 얼마나

1748
01:07:27,650 --> 01:07:29,290
일치하는지를 측정하는 겁니다.

1749
01:07:29,290 --> 01:07:31,770
그리고 세 번째 항이 문제를 일으킵니다.

1750
01:07:31,770 --> 01:07:34,330
세 번째 항은 q(z|x),

1751
01:07:34,330 --> 01:07:38,050
즉 인코더가 입력 데이터 x에 대해 예측한

1752
01:07:38,050 --> 01:07:41,970
z 분포이고, 이것이 p(z|x)와 얼마나

1753
01:07:41,970 --> 01:07:44,330
일치하는지를 나타냅니다.

1754
01:07:44,330 --> 01:07:47,530
이것은 디코더가 모델링하는 분포를

1755
01:07:47,530 --> 01:07:49,210
뒤집은 형태입니다.

1756
01:07:49,210 --> 01:07:50,990
이 항은 계산할 수 없습니다.

1757
01:07:50,990 --> 01:07:53,490
왜냐하면 처음에 문제를 일으킨

1758
01:07:53,490 --> 01:07:56,270
것이 바로 p(z|x)이기 때문입니다.

1759
01:07:56,270 --> 01:08:00,330
q를 도입한 이유는 x가 주어졌을 때 z의

1760
01:08:00,330 --> 01:08:03,530
p를 계산할 수 없었기 때문입니다.

1761
01:08:03,530 --> 01:08:06,250
그럼 이제 무엇을 해야 할까요?

1762
01:08:06,250 --> 01:08:07,610
그것을 버릴 겁니다.

1763
01:08:07,610 --> 01:08:10,090
KL 발산은 항상 0 이상이라는

1764
01:08:10,090 --> 01:08:12,770
것을 알고 있기 때문에, 마지막 항은 두

1765
01:08:12,770 --> 01:08:15,167
분포의 KL 발산입니다. 비록 그

1766
01:08:15,167 --> 01:08:17,750
분포들을 직접 계산할 수 없지만,

1767
01:08:17,750 --> 01:08:19,450
KL 발산의 잘 알려진

1768
01:08:19,450 --> 01:08:22,410
성질로 인해 0 이상임을 알 수 있습니다.

1769
01:08:22,410 --> 01:08:25,850
그래서 그 항을 버리고 진짜 확률에 대한 하한을

1770
01:08:25,850 --> 01:08:27,290
얻을 수 있습니다.

1771
01:08:27,290 --> 01:08:29,189
마지막 항을 버리면,

1772
01:08:29,189 --> 01:08:32,370
log p(x)가 재구성 항과 사전 항 두

1773
01:08:32,370 --> 01:08:35,370
개의 항보다 크거나 같다는 것을 알 수

1774
01:08:35,370 --> 01:08:36,175
있습니다.

1775
01:08:36,175 --> 01:08:38,050
이것이 우리가 변분 오토인코더를

1776
01:08:38,050 --> 01:08:40,090
훈련할 때 사용할 손실 함수입니다.

1777
01:08:40,090 --> 01:08:43,130
이 아이디어는 이것이 진짜 로그 가능도의 근사치라는

1778
01:08:43,130 --> 01:08:43,830
겁니다.

1779
01:08:43,830 --> 01:08:46,029
즉, 로그 가능도의 하한입니다.

1780
01:08:46,029 --> 01:08:48,010
그래서 하한을 최대화하면,

1781
01:08:48,010 --> 01:08:50,649
비록 정확히 하지는 못해도 진짜 로그

1782
01:08:50,649 --> 01:08:53,090
가능도도 최대화할 수 있기를 기대합니다.

1783
01:08:53,090 --> 01:08:55,870
이것이 변분 오토인코더의 훈련 목표입니다.

1784
01:08:55,870 --> 01:08:58,710
요약하자면,

1785
01:08:58,710 --> 01:09:02,130
인코더 q와 디코더 p를 함께 훈련해서

1786
01:09:02,130 --> 01:09:05,930
진짜 데이터 로그 가능도에 대한 변분 하한을

1787
01:09:05,930 --> 01:09:07,513
최대화하는 겁니다.

1788
01:09:07,513 --> 01:09:09,930
이것은 증거 하한(evidence lower bound) 또는

1789
01:09:09,930 --> 01:09:10,793
ELBo라고도 불립니다.

1790
01:09:10,793 --> 01:09:11,710
즉, ELBo를 최대화하는 것입니다.

1791
01:09:11,710 --> 01:09:13,529
우리는 ELBo를 최대화할 겁니다.

1792
01:09:13,529 --> 01:09:14,950
그리고 이것은 특정한 항을 가지고 있습니다.

1793
01:09:14,950 --> 01:09:17,529
우리는 인코더 네트워크와 디코더 네트워크를 가지고 있습니다.

1794
01:09:17,529 --> 01:09:18,649
그것이 우리가 하는 일입니다.

1795
01:09:18,649 --> 01:09:21,410
그럼 훈련 절차가 좀 더 명확하게

1796
01:09:21,410 --> 01:09:24,609
어떻게 진행되는지 살펴보면, 이

1797
01:09:24,609 --> 01:09:26,930
신경망 인코더가 입력 x를

1798
01:09:26,930 --> 01:09:29,850
받아서 z에 대한 분포를 출력합니다.

1799
01:09:29,850 --> 01:09:32,130
그리고 나서 예측된

1800
01:09:32,130 --> 01:09:34,520
분포에 KL 항을 적용합니다.

1801
01:09:34,520 --> 01:09:35,972
특히 이 항은

1802
01:09:35,972 --> 01:09:37,680
예측된 분포가 단위

1803
01:09:37,680 --> 01:09:39,310
가우시안이 되도록

1804
01:09:39,310 --> 01:09:41,560
강제합니다. 즉, 예측된 평균이

1805
01:09:41,560 --> 01:09:47,000
0이 되고 예측된 시그마가 모두 1이 되도록 유도하는 겁니다.

1806
01:09:47,000 --> 01:09:50,620
그다음 인코더에서 나온 예측 분포로부터, 이른바

1807
01:09:50,620 --> 01:09:53,040
재매개변수화 트릭을 사용해서

1808
01:09:53,040 --> 01:09:55,040
샘플링을 합니다. 이

1809
01:09:55,040 --> 01:09:57,500
트릭 덕분에 역전파가 가능합니다.

1810
01:09:57,500 --> 01:10:00,400
그래서 예측 분포에서 샘플 z를 뽑습니다.

1811
01:10:00,400 --> 01:10:04,840
이 샘플 z를 얻으면 디코더에 통과시켜서

1812
01:10:04,840 --> 01:10:08,680
디코더가 예측하는 정규 분포를

1813
01:10:08,680 --> 01:10:12,520
얻고, 그 출력에 재구성 손실

1814
01:10:12,520 --> 01:10:14,120
항을 적용합니다.

1815
01:10:14,120 --> 01:10:18,300
비록 수학적으로 복잡해 보이는 슬라이드였지만,

1816
01:10:18,300 --> 01:10:21,000
실제로는 그렇게 복잡하지 않은

1817
01:10:21,000 --> 01:10:22,833
훈련 목표로 이어집니다.

1818
01:10:22,833 --> 01:10:25,000
그리고 이 변분 오토인코더는

1819
01:10:25,000 --> 01:10:26,417
매우 흥미로운데, 두

1820
01:10:26,417 --> 01:10:29,732
손실 항이 서로 흥미롭게 경쟁하기 때문입니다.

1821
01:10:29,732 --> 01:10:31,440
왜냐하면 모델이 잠재

1822
01:10:31,440 --> 01:10:34,000
공간 z를 통해 병목을 만들도록

1823
01:10:34,000 --> 01:10:36,480
강제하는데, 이 두 항은 잠재 공간에

1824
01:10:36,480 --> 01:10:39,720
대해 서로 다른 요구를 하기 때문입니다. 재구성

1825
01:10:39,720 --> 01:10:41,840
손실은 시그마가 0이 되고,

1826
01:10:41,840 --> 01:10:46,000
각 데이터 x마다 고유한 벡터인 mu x를 원합니다.

1827
01:10:46,000 --> 01:10:47,680
그렇게 되면 재구성 목표를

1828
01:10:47,680 --> 01:10:50,460
완벽히 만족시킬 수 있기 때문입니다.

1829
01:10:50,460 --> 01:10:54,480
각 데이터 포인트마다 별도의 고유 벡터를 가지게 됩니다.

1830
01:10:54,480 --> 01:10:56,350
그러면 확률이 존재하지 않게 됩니다.

1831
01:10:56,350 --> 01:10:58,100
모든 것을 완벽히 재구성할 수 있습니다.

1832
01:10:58,100 --> 01:11:00,420
이것이 재구성 손실이 원하는 바입니다.

1833
01:11:00,420 --> 01:11:02,560
하지만 사전 손실은 시그마가 모두

1834
01:11:02,560 --> 01:11:05,820
1이 되길 원합니다. 단위 가우시안이 되길 원하고, 모든

1835
01:11:05,820 --> 01:11:07,880
mu가 0이 되길 원하는데,

1836
01:11:07,880 --> 01:11:10,340
이는 두 손실이 원하는 것과 매우 다릅니다.

1837
01:11:10,340 --> 01:11:12,200
그래서 VAE를 훈련하는

1838
01:11:12,200 --> 01:11:14,760
과정에서 이 두 손실이 서로

1839
01:11:14,760 --> 01:11:18,120
경쟁하면서 데이터 재구성과 잠재 공간을

1840
01:11:18,120 --> 01:11:20,560
사전에 가깝게 만드는 균형점을

1841
01:11:20,560 --> 01:11:22,092
찾으려 하는 겁니다.

1842
01:11:22,092 --> 01:11:23,800
훈련이 끝나면 사전 분포에서

1843
01:11:23,800 --> 01:11:26,440
z를 샘플링해서 디코더에 통과시키면 샘플을

1844
01:11:26,440 --> 01:11:27,840
얻을 수 있습니다.

1845
01:11:27,840 --> 01:11:30,520
또한 잠재 공간이 대각

1846
01:11:30,520 --> 01:11:32,560
가우시안이기 때문에,

1847
01:11:32,560 --> 01:11:37,960
잠재 공간 z의 각 항목 간에 통계적 독립성 개념이

1848
01:11:37,960 --> 01:11:39,300
있습니다.

1849
01:11:39,300 --> 01:11:41,040
그래서 각각을 따로 조절할 수 있습니다.

1850
01:11:41,040 --> 01:11:42,920
그리고 그 개별 차원들은 종종

1851
01:11:42,920 --> 01:11:46,280
데이터에 대해 유용하거나 해석 가능하거나 직교하는 정보를

1852
01:11:46,280 --> 01:11:47,360
담고 있습니다.

1853
01:11:47,360 --> 01:11:49,120
이 경우, VAE를 손글씨

1854
01:11:49,120 --> 01:11:50,860
숫자 데이터셋에 훈련시켰습니다.

1855
01:11:50,860 --> 01:11:54,240
잠재 공간의 두 차원을 변화시키면 숫자가 한

1856
01:11:54,240 --> 01:11:58,940
범주에서 다른 범주로 부드럽게 변하는 것을 볼 수 있습니다.

1857
01:11:58,940 --> 01:12:01,760
이것은 VAE의 꽤 일반적인 특성입니다.

1858
01:12:01,760 --> 01:12:03,600
오늘은 여기까지입니다.

1859
01:12:03,600 --> 01:12:06,280
오늘 이야기한 내용을 요약하면, 감독 학습과 비감독

1860
01:12:06,280 --> 01:12:07,700
학습에 대해 이야기했고,

1861
01:12:07,700 --> 01:12:09,600
세 가지 다른 종류의 생성 모델에

1862
01:12:09,600 --> 01:12:10,880
대해 이야기했습니다.

1863
01:12:10,880 --> 01:12:13,400
그리고 생성 모델 계통도 중 한 갈래에

1864
01:12:13,400 --> 01:12:15,000
대해 이야기했습니다.

1865
01:12:15,000 --> 01:12:18,623
다음 시간에는 생성 모델 계통도의 다른

1866
01:12:18,623 --> 01:12:20,040
절반, 특히

1867
01:12:20,040 --> 01:12:22,120
생성적 적대 신경망과

1868
01:12:22,120 --> 01:12:25,590
확산 모델에 대해 이야기할 예정입니다.
