1
00:00:05,480 --> 00:00:09,200
오늘 강의 주제는 정규화와 최적화에 관한 내용입니다.

2
00:00:09,200 --> 00:00:12,680
이 두 개념은 딥러닝과 머신러닝

3
00:00:12,680 --> 00:00:15,160
전반에서 매우 중요하지만, 특히 컴퓨터

4
00:00:15,160 --> 00:00:17,520
비전 분야에서 중요합니다.

5
00:00:17,520 --> 00:00:20,040
우리는 지난주 내용을 간단히

6
00:00:20,040 --> 00:00:24,400
복습하고, 그때 다뤘던 주제들을 다시 논의할 것입니다.

7
00:00:24,400 --> 00:00:27,360
우리는 컴퓨터 비전의 핵심 과제인

8
00:00:27,360 --> 00:00:30,960
이미지 분류에 대해 집중적으로 살펴보았습니다.

9
00:00:30,960 --> 00:00:34,560
이 과제는 입력으로 주어진 이미지를

10
00:00:34,560 --> 00:00:40,660
레이블 집합 내의 하나의 레이블로 매핑하는 것입니다.

11
00:00:40,660 --> 00:00:44,400
여기에는 다섯 가지 레이블이 있습니다: 고양이, 개, 새, 사슴, 그리고

12
00:00:44,400 --> 00:00:45,060
트럭입니다.

13
00:00:45,060 --> 00:00:47,440
목표는 입력 이미지에 올바른 레이블을

14
00:00:47,440 --> 00:00:48,720
할당하는 것입니다.

15
00:00:48,720 --> 00:00:50,760
이미지를 입력으로 받아

16
00:00:50,760 --> 00:00:55,360
특정 레이블을 출력하는 모델이나 함수를 만드는 것이죠.

17
00:00:55,360 --> 00:00:58,360
또한 분류의 여러 도전 과제에

18
00:00:58,360 --> 00:01:00,050
대해서도 이야기했습니다.

19
00:01:00,050 --> 00:01:04,670
가장 큰 도전 과제 중 하나는 왼쪽 위에

20
00:01:04,670 --> 00:01:07,730
나와 있는데, 인간이 이미지에서

21
00:01:07,730 --> 00:01:10,370
인지하는 의미(예:

22
00:01:10,370 --> 00:01:12,850
고양이)와 컴퓨터가

23
00:01:12,850 --> 00:01:16,610
표현하는 방식(픽셀 값의 격자, 다차원

24
00:01:16,610 --> 00:01:19,370
배열 또는 텐서, 각

25
00:01:19,370 --> 00:01:24,313
픽셀의 이산 값) 사이의 의미적 간극입니다.

26
00:01:24,313 --> 00:01:26,730
이것은 우리가 이미지를 보고 고양이라고

27
00:01:26,730 --> 00:01:28,950
판단하는 방식과 매우 다릅니다.

28
00:01:28,950 --> 00:01:33,690
복잡한 수치 표현을 인간이 이해할 수 있는 형태로

29
00:01:33,690 --> 00:01:35,570
매핑하는 것이 핵심

30
00:01:35,570 --> 00:01:36,870
도전 과제입니다.

31
00:01:36,870 --> 00:01:39,730
또한 이미지 자체와 관련된 도전 과제도

32
00:01:39,730 --> 00:01:40,430
있습니다.

33
00:01:40,430 --> 00:01:44,670
예를 들어, 장면의 조명 상태를 보면, 조명이

34
00:01:44,670 --> 00:01:46,850
어디에 있느냐에 따라

35
00:01:46,850 --> 00:01:49,090
픽셀 강도가 달라집니다.

36
00:01:49,090 --> 00:01:52,410
또한 객체의 일부가 그림자에 가려져서

37
00:01:52,410 --> 00:01:55,130
보기 어려울 수도 있습니다.

38
00:01:55,130 --> 00:01:56,615
고양이는 본질적으로 변형이

39
00:01:56,615 --> 00:01:58,990
가능한 동물이기 때문에, 움직이고 구부러지며

40
00:01:58,990 --> 00:02:00,730
다양한 형태를 가질 수 있어

41
00:02:00,730 --> 00:02:02,490
항상 같은 모양을 갖지 않습니다.

42
00:02:02,490 --> 00:02:04,157
이것은 객체를 탐지하는

43
00:02:04,157 --> 00:02:06,510
알고리즘을 설계할 때 어려움을 줍니다.

44
00:02:06,510 --> 00:02:08,530
가림 현상(occlusion)도 도전 과제입니다.

45
00:02:08,530 --> 00:02:11,510
예를 들어, 고양이가 쿠션 밑에 숨었을 때,

46
00:02:11,510 --> 00:02:13,150
인간은 꼬리가 보이기

47
00:02:13,150 --> 00:02:16,143
때문에 고양이라는 것을 명확히 알 수

48
00:02:16,143 --> 00:02:16,810
있습니다.

49
00:02:16,810 --> 00:02:18,810
고양이의 행동 방식으로도

50
00:02:18,810 --> 00:02:20,990
고양이라는 것을 추론할 수 있죠.

51
00:02:20,990 --> 00:02:23,270
배경 잡음(background clutter)도

52
00:02:23,270 --> 00:02:25,570
있어서 객체가 배경과 섞여 보일 수 있습니다.

53
00:02:25,570 --> 00:02:27,950
이 부분도 고려해야 합니다.

54
00:02:27,950 --> 00:02:31,830
마지막으로, 같은 카테고리 내에서도 객체들이 매우 다르게 보일 수 있는

55
00:02:31,830 --> 00:02:33,630
클래스 내 변이(intraclass

56
00:02:33,630 --> 00:02:35,690
variation) 문제가 있습니다.

57
00:02:35,690 --> 00:02:38,690
하지만 우리는 이들을 같은 카테고리로 묶어야 합니다.

58
00:02:38,690 --> 00:02:41,490
이처럼 인식 문제에는 많은

59
00:02:41,490 --> 00:02:45,030
도전 과제가 있으며, 단순히 if-else

60
00:02:45,030 --> 00:02:48,070
규칙이나 간단한 논리만으로

61
00:02:48,070 --> 00:02:50,590
해결할 수 없습니다.

62
00:02:50,590 --> 00:02:52,330
논리가 통하지

63
00:02:52,330 --> 00:02:54,270
않는다면, 어떻게

64
00:02:54,270 --> 00:02:56,770
분류기를 만들 수 있을까요?

65
00:02:56,770 --> 00:02:59,620
여기서 데이터 기반 접근법을 이야기했습니다.

66
00:02:59,620 --> 00:03:02,480
가장 단순한 머신러닝 모델인 k-최근접 이웃(k-nearest

67
00:03:02,480 --> 00:03:05,560
neighbors) 모델에 대해 설명했습니다.

68
00:03:05,560 --> 00:03:10,280
이 모델은 새로운 데이터 포인트가 들어왔을 때,

69
00:03:10,280 --> 00:03:13,360
훈련 세트 내에서 가장 가까운

70
00:03:13,360 --> 00:03:18,320
데이터 포인트들을 찾아 그들의 레이블을 참조하는

71
00:03:18,320 --> 00:03:19,240
방식입니다.

72
00:03:19,240 --> 00:03:23,840
1-최근접 이웃의 경우 가장 가까운 데이터 포인트

73
00:03:23,840 --> 00:03:26,960
하나를 찾아 그 레이블을 할당합니다.

74
00:03:26,960 --> 00:03:29,840
또한 여러 개의 최근접 이웃을 보고

75
00:03:29,840 --> 00:03:33,440
그 중 가장 많은 레이블을 할당하는 방법도

76
00:03:33,440 --> 00:03:34,057
있습니다.

77
00:03:34,057 --> 00:03:36,140
이 두 가지 접근법에 대해 이야기했습니다.

78
00:03:36,140 --> 00:03:39,200
데이터 세트를 단순히 훈련과 테스트로

79
00:03:39,200 --> 00:03:41,260
나누는 대신, 훈련,

80
00:03:41,260 --> 00:03:44,120
검증, 테스트로 나누어 검증

81
00:03:44,120 --> 00:03:46,560
세트를 통해 하이퍼파라미터를

82
00:03:46,560 --> 00:03:50,120
선택하는 것이 이상적이라고 했습니다.

83
00:03:50,120 --> 00:03:52,920
k-최근접 이웃의 주요 하이퍼파라미터는 k

84
00:03:52,920 --> 00:03:56,107
값이며, 예를 들어 1이나 5가 될 수 있습니다.

85
00:03:56,107 --> 00:03:57,690
여기서 보여드린

86
00:03:57,690 --> 00:04:00,850
예는, 이 검증 세트에서의 정확도를 다양한

87
00:04:00,850 --> 00:04:03,910
k 값에 대해 플로팅한 것입니다.

88
00:04:03,910 --> 00:04:07,640
그리고 가장 높은 정확도를 가진 값을 선택하시면 됩니다.

89
00:04:07,640 --> 00:04:09,390
이렇게 검증 세트를 사용하고, 이

90
00:04:09,390 --> 00:04:11,010
테스트 세트는 완전히 새로운

91
00:04:11,010 --> 00:04:13,230
데이터에 대해 모델이 어떻게 작동하는지

92
00:04:13,230 --> 00:04:14,350
평가하는 데 남겨둡니다.

93
00:04:14,350 --> 00:04:16,450
이것이 테스트 세트의 목적입니다.

94
00:04:16,450 --> 00:04:18,450
이것들은 모두 복습입니다.

95
00:04:18,450 --> 00:04:20,829
거리 측정법에 대해 약간 혼란이 있었죠.

96
00:04:20,829 --> 00:04:24,330
Ed에 좀 더 자세히 설명한 글을 올렸습니다.

97
00:04:24,330 --> 00:04:27,410
우리는 두 가지 다른 거리 측정법에 대해 이야기했습니다.

98
00:04:27,410 --> 00:04:30,190
머신러닝에서 가장 흔히 쓰이는 두

99
00:04:30,190 --> 00:04:34,530
가지는 Manhattan 거리, 즉 L1 거리와 L2

100
00:04:34,530 --> 00:04:36,770
거리, 즉 유클리드 거리입니다.

101
00:04:36,770 --> 00:04:40,610
L2 거리는 직선 거리로, 우리가

102
00:04:40,610 --> 00:04:42,430
일상에서 거리라고

103
00:04:42,430 --> 00:04:44,490
생각하는 바로 그

104
00:04:44,490 --> 00:04:46,790
기하학적 거리입니다.

105
00:04:46,790 --> 00:04:49,863
Manhattan 거리는 이 그림에서처럼 좌우와

106
00:04:49,863 --> 00:04:52,390
상하로만 이동할 수 있고 대각선으로는 이동할

107
00:04:52,390 --> 00:04:53,830
수 없는 개념입니다.

108
00:04:53,830 --> 00:04:57,260
한 가지 예를 빠르게 보겠습니다.

109
00:04:57,260 --> 00:04:59,060
이 선 위의 모든 점이

110
00:04:59,060 --> 00:05:01,180
원점에서 같은 거리에 있는 이유는

111
00:05:01,180 --> 00:05:03,960
대각선으로 이동할 수 없기 때문에, 여기서는

112
00:05:03,960 --> 00:05:07,920
위로 0.5, 오른쪽으로 0.5만 이동해야 해서 총 거리가

113
00:05:07,920 --> 00:05:10,460
1이 되는 겁니다. 반면에 여기서는

114
00:05:10,460 --> 00:05:13,140
직선으로 이동하지만 역시 거리가 1입니다.

115
00:05:13,140 --> 00:05:14,480
같은 거리라는 뜻입니다.

116
00:05:14,480 --> 00:05:18,020
반면 L2 거리에서는 원점에서 같은 거리에

117
00:05:18,020 --> 00:05:20,420
있는 모든 점이 원을 이루는데,

118
00:05:20,420 --> 00:05:23,660
이는 직선으로 바로 갈 수 있기 때문입니다.

119
00:05:23,660 --> 00:05:26,700
이것이 간단한 설명입니다.

120
00:05:26,700 --> 00:05:30,060
지난 시간에 집중했던 마지막 주제는

121
00:05:30,060 --> 00:05:32,980
선형 분류기라는 개념이었습니다.

122
00:05:32,980 --> 00:05:36,940
기본적인 설정에서 우리가 한 것은,

123
00:05:36,940 --> 00:05:41,660
예를 들어 너비 32, 높이 32인 이미지가

124
00:05:41,660 --> 00:05:44,700
있고, 이미지의 각 공간 위치마다

125
00:05:44,700 --> 00:05:47,700
빨강, 초록, 파랑 강도를

126
00:05:47,700 --> 00:05:52,500
나타내는 세 가지 픽셀 값이 있다는 겁니다.

127
00:05:52,500 --> 00:05:55,900
그리고 이 이미지의 숫자 배열을

128
00:05:55,900 --> 00:05:59,440
3,072개의 숫자로

129
00:05:59,440 --> 00:06:01,400
평탄화(flatten)합니다.

130
00:06:01,400 --> 00:06:06,640
그 다음에 이 벡터를 가중치 행렬 W와 곱합니다.

131
00:06:06,640 --> 00:06:11,440
기본 아이디어는, 가중치 행렬 W가

132
00:06:11,440 --> 00:06:18,300
높이 10, 너비 3,072일 때, 이 행렬의 각 행을

133
00:06:18,300 --> 00:06:22,960
입력 샘플 x와 곱한다는 것입니다.

134
00:06:22,960 --> 00:06:26,680
그러면 10개의 클래스 점수가 나오게 됩니다.

135
00:06:26,680 --> 00:06:29,040
종종 각 클래스마다 하나씩

136
00:06:29,040 --> 00:06:32,620
1개의 바이어스 항을 더하기도 합니다.

137
00:06:32,620 --> 00:06:35,680
이 바이어스 항은 크기 10인 벡터가 됩니다.

138
00:06:35,680 --> 00:06:37,960
그리고 우리는 이러한 선형 모델을 바라보거나

139
00:06:37,960 --> 00:06:41,008
생각할 수 있는 세 가지 다른 방법에 대해서도 이야기했습니다.

140
00:06:41,008 --> 00:06:43,300
하나는 대수학적 관점인데,

141
00:06:43,300 --> 00:06:47,382
여기서 각 행이 독립적으로 클래스를 나타내는

142
00:06:47,382 --> 00:06:48,340
방식입니다.

143
00:06:48,340 --> 00:06:53,410
그리고 입력 벡터 x에 곱하면 점수를 얻고, 편향을

144
00:06:53,410 --> 00:06:56,210
더해 최종 점수를 구합니다.

145
00:06:56,210 --> 00:06:58,410
각 행을 독립적으로 처리하는 의미입니다.

146
00:06:58,410 --> 00:07:04,930
또한 여기 학습된 클래스 가중치를 템플릿으로 볼 수도

147
00:07:04,930 --> 00:07:08,850
있는데, 벡터를 원래 이미지 형태로

148
00:07:08,850 --> 00:07:13,090
다시 펼치면 강도를 시각화해서 각

149
00:07:13,090 --> 00:07:18,130
클래스별 템플릿이 무엇인지 이해할 수

150
00:07:18,130 --> 00:07:21,970
있습니다. 이것이 바로 이 시각화가

151
00:07:21,970 --> 00:07:24,010
나타내는 바입니다.

152
00:07:24,010 --> 00:07:25,570
마지막으로

153
00:07:25,570 --> 00:07:29,130
생각할 수 있는 방법은 기하학적

154
00:07:29,130 --> 00:07:32,930
관점으로, 가중치 행렬의 각

155
00:07:32,930 --> 00:07:38,570
행이 입력 공간에서 이 선들로 표현됩니다.

156
00:07:38,570 --> 00:07:40,690
특히 이 선은

157
00:07:40,690 --> 00:07:45,070
방정식을 0으로 설정한 결정 경계입니다.

158
00:07:45,070 --> 00:07:48,485
그래서 이 선 위쪽에서는 양수 점수를,

159
00:07:48,485 --> 00:07:50,610
아래쪽에서는 음수 점수를

160
00:07:50,610 --> 00:07:54,820
해당 클래스에 대해 가질 수 있는 지점이 형성됩니다.

161
00:07:54,820 --> 00:07:57,380
이것들이 선형 모델을 보는

162
00:07:57,380 --> 00:07:59,240
다양한 관점들입니다.

163
00:07:59,240 --> 00:08:00,780
모두 같은 일을 하고 있습니다.

164
00:08:00,780 --> 00:08:03,620
기하학적 관점의 좋은 점은 데이터를

165
00:08:03,620 --> 00:08:06,180
시각화했을 때, 예를 들어 파란색과

166
00:08:06,180 --> 00:08:09,360
빨간색을 분류하려고 할 때, 데이터를

167
00:08:09,360 --> 00:08:12,300
완벽하게 분리하는 선을 그을 수 없다는 것을

168
00:08:12,300 --> 00:08:14,520
쉽게 알 수 있다는 겁니다.

169
00:08:14,520 --> 00:08:17,500
그래서 선형 모델이 할 수 있는

170
00:08:17,500 --> 00:08:21,340
것에 대한 직관을 얻기에 좋은 방법입니다.

171
00:08:21,340 --> 00:08:22,180
좋습니다.

172
00:08:22,180 --> 00:08:24,500
지난 시간에 논의한 내용을

173
00:08:24,500 --> 00:08:27,220
고수준에서 다시 정리한 것입니다.

174
00:08:27,220 --> 00:08:30,020
이제 이번 강의의 새로운 내용에

175
00:08:30,020 --> 00:08:34,043
대해 좀 더 자세히 설명할 텐데, 지난 시간이나 이번

176
00:08:34,043 --> 00:08:35,460
강의 초반에 논의한

177
00:08:35,460 --> 00:08:37,820
내용에 대해 질문이 있으면

178
00:08:37,820 --> 00:08:41,039
잠시 멈추고 자유롭게 물어보셔도 됩니다.

179
00:08:41,039 --> 00:08:41,539
네.

180
00:08:41,539 --> 00:08:45,720
온라인에 계신 분들의 질문인데, 이 시각적 관점이 k-최근접

181
00:08:45,720 --> 00:08:49,720
이웃(k-nearest neighbors)과 같은 건가요?

182
00:08:49,720 --> 00:08:52,070
그리고 비교하는 이웃 중

183
00:08:52,070 --> 00:08:53,590
하나일 수도 있나요?

184
00:08:53,590 --> 00:08:55,350
수학적으로 동일한가요?

185
00:08:55,350 --> 00:08:58,790
아니요, 같지 않습니다. 이 템플릿들은 이

186
00:08:58,790 --> 00:09:02,910
선에서 형성되기 때문에 특정 데이터 포인트 하나가

187
00:09:02,910 --> 00:09:03,750
아닙니다.

188
00:09:03,750 --> 00:09:09,070
하지만 이 템플릿들은 계산할 수 있고, 이 그림에서

189
00:09:09,070 --> 00:09:11,990
보시면 클래스 방향을

190
00:09:11,990 --> 00:09:15,330
가리키는 선이 있는데, 이 점을

191
00:09:15,330 --> 00:09:18,390
대표한다고 볼 수 있습니다.

192
00:09:18,390 --> 00:09:22,950
네, 질문은 이 3,072라는 숫자를 어떻게 얻었느냐는 거죠?

193
00:09:22,950 --> 00:09:27,830
여기서 아이디어는 이미지 높이가 32픽셀, 너비가 32픽셀이고,

194
00:09:27,830 --> 00:09:31,510
이미지의 각 위치가 빨강, 초록, 파랑 픽셀

195
00:09:31,510 --> 00:09:34,310
강도 세 값으로 표현되므로, 전체

196
00:09:34,310 --> 00:09:38,950
이미지를 표현하기 위해 32 곱하기 32 곱하기 3의 총

197
00:09:38,950 --> 00:09:40,450
값이 나온다는 겁니다.

198
00:09:40,450 --> 00:09:44,190
그래서 이 3,072라는 숫자가 나오는 것입니다.

199
00:09:44,190 --> 00:09:47,790
여기 아주 구체적인 선형

200
00:09:47,790 --> 00:09:50,730
모델의 예가 있습니다.

201
00:09:50,730 --> 00:09:56,270
입력 x에 가중치 행렬 W를 곱하면, 서로 다른

202
00:09:56,270 --> 00:09:59,530
클래스에 대한 점수가 나옵니다.

203
00:09:59,530 --> 00:10:01,530
고양이 클래스 점수가 낮은데, 자동차

204
00:10:01,530 --> 00:10:04,130
점수가 더 높아서 잘못된 걸 알 수 있습니다.

205
00:10:04,130 --> 00:10:07,570
우리는 정답 클래스가 가장 높은 점수를 받길 원합니다.

206
00:10:07,570 --> 00:10:09,508
두 번째 예에서는 제대로

207
00:10:09,508 --> 00:10:11,550
맞춰서 꽤 잘하고 있죠.

208
00:10:11,550 --> 00:10:14,370
하지만 개구리 예에서는 완전히 틀렸는데,

209
00:10:14,370 --> 00:10:16,770
세 점수 중 가장 낮습니다.

210
00:10:16,770 --> 00:10:20,210
직관적으로 이 점수들이 좋지 않다는 걸 알 수 있습니다.

211
00:10:20,210 --> 00:10:23,670
그런데 이 직관을 수학적으로 어떻게 공식화할 수 있을까요?

212
00:10:23,670 --> 00:10:26,690
그리고 주어진 모델이 얼마나 좋은지 어떻게 판단할까요?

213
00:10:26,690 --> 00:10:30,090
이것이 바로 손실 함수(loss function)의 개념인데,

214
00:10:30,090 --> 00:10:34,490
손실 함수는 분류기가 얼마나 좋은지, 더 정확히는 얼마나 나쁜지를
알려줍니다.

215
00:10:34,490 --> 00:10:39,530
예를 들어, i라는 인덱스로 표시된 데이터 세트가 있다고 할

216
00:10:39,530 --> 00:10:43,490
때, Xi는 각 훈련 예제이고, yi는 각

217
00:10:43,490 --> 00:10:45,930
훈련 레이블입니다. 우리는 전체

218
00:10:45,930 --> 00:10:49,360
데이터 세트에 대해 손실을 계산할 수

219
00:10:49,360 --> 00:10:53,900
있는데, 각 훈련 예제에 대해 이 손실을 모델에 통과시켜

220
00:10:53,900 --> 00:10:57,820
계산합니다. 여기서 모델은 f(XiW)입니다.

221
00:10:57,820 --> 00:10:59,080
그럼 레이블을 얻습니다.

222
00:10:59,080 --> 00:11:02,820
그리고 나서 실제 정답 레이블 yi와 비교해서 손실을 계산합니다.

223
00:11:02,820 --> 00:11:05,060
그리고 전체 데이터 세트에 대해 평균을 냅니다.

224
00:11:05,060 --> 00:11:07,280
이렇게 해서 손실을 계산하는 겁니다.

225
00:11:07,280 --> 00:11:10,700
지난 강의에서 다뤘던 Softmax 손실

226
00:11:10,700 --> 00:11:13,500
또는 교차 엔트로피 손실은 분류에

227
00:11:13,500 --> 00:11:16,300
가장 흔히 사용되는 손실 함수입니다.

228
00:11:16,300 --> 00:11:18,800
그래서 여기서는 그 내용을 다시 자세히 다루지 않겠습니다.

229
00:11:18,800 --> 00:11:22,880
기본적으로, 정답 클래스의 확률을 낮게

230
00:11:22,880 --> 00:11:26,360
예측하면 손실이 매우 큽니다.

231
00:11:26,360 --> 00:11:29,460
정답 클래스를 매우 높은 확률로

232
00:11:29,460 --> 00:11:32,300
예측하면 손실이 매우 작습니다.

233
00:11:32,300 --> 00:11:37,620
제가 방금 설명한 내용은 모두 데이터 손실(data loss)이라고 부르는

234
00:11:37,620 --> 00:11:39,940
것 안에 포함되어 있습니다.

235
00:11:39,940 --> 00:11:43,300
이 손실은 모델의 예측이 훈련 데이터와

236
00:11:43,300 --> 00:11:45,880
얼마나 잘 맞는지를 알려줍니다.

237
00:11:45,880 --> 00:11:47,790
당연히 이 값은 매우 낮아야 합니다.

238
00:11:47,790 --> 00:11:49,582
낮다는 것은 모델이 훈련 데이터를

239
00:11:49,582 --> 00:11:51,390
잘 맞추고 있다는 의미입니다.

240
00:11:51,390 --> 00:11:54,770
하지만 오늘 다룰 두 번째 요소가 있는데, 바로 손실

241
00:11:54,770 --> 00:11:58,670
함수의 정규화 항(regularization term)입니다.

242
00:11:58,670 --> 00:12:02,110
이 항은 모델이 훈련 데이터에 너무

243
00:12:02,110 --> 00:12:05,852
과적합하는 것을 막기 위한 것입니다.

244
00:12:05,852 --> 00:12:07,810
그래서 실제로는 훈련 데이터에서 성능이 좀

245
00:12:07,810 --> 00:12:10,470
떨어지지만, 새로운 테스트 데이터나 보지 않은 데이터에서 더

246
00:12:10,470 --> 00:12:11,810
잘하도록 하는 것이 목표입니다.

247
00:12:11,810 --> 00:12:14,410
훈련 데이터에서는 성능이 떨어지지만 테스트 세트에서는 더 좋아지는 거죠.

248
00:12:14,410 --> 00:12:16,403
이것이 정규화의 목적입니다.

249
00:12:16,403 --> 00:12:18,070
다음 슬라이드에서 정규화에

250
00:12:18,070 --> 00:12:20,850
대한 직관적인 이해를 많이 다룰 겁니다.

251
00:12:20,850 --> 00:12:23,730
하지만 큰 그림에서 보면, 훈련 데이터에서는 성능이 떨어지더라도

252
00:12:23,730 --> 00:12:26,850
테스트 데이터나 보지 않은 데이터에서 더 잘하는 것이 정규화의 목표입니다.

253
00:12:26,850 --> 00:12:29,270
바로 이것이 정규화의 핵심입니다.

254
00:12:29,270 --> 00:12:31,110
네, 그래서 우리는 i번째 훈련

255
00:12:31,110 --> 00:12:33,870
샘플 각각에 대해 손실을 계산하고 있습니다.

256
00:12:33,870 --> 00:12:37,630
네, 그래서 i번째 예제의 손실은 Xi와 yi를 사용합니다.

257
00:12:40,183 --> 00:12:40,850
이해가 되시나요?

258
00:12:40,850 --> 00:12:42,490
즉, 여기서 i가 없어도 됩니다.

259
00:12:42,490 --> 00:12:46,910
하지만 이것은 단지 i번째 손실을 말하는 겁니다 [잘 안 들림].

260
00:12:46,910 --> 00:12:47,890
네.

261
00:12:47,890 --> 00:12:50,382
네, 보통 각 i마다 다른 손실을 가지지는 않습니다,

262
00:12:50,382 --> 00:12:51,590
그게 질문하신 거라면요.

263
00:12:51,590 --> 00:12:52,770
네, 맞습니다.

264
00:12:52,770 --> 00:12:55,090
그래서 그냥-- 우리는 Li를 i번째

265
00:12:55,090 --> 00:12:57,703
훈련 예제의 손실로 설명했으니 여기서 그렇게

266
00:12:57,703 --> 00:12:58,870
사용하고 있습니다.

267
00:12:58,870 --> 00:13:02,370
하지만 네, i일 수도 있습니다.

268
00:13:02,370 --> 00:13:05,090
정규화에 대해 이야기할 때,

269
00:13:05,090 --> 00:13:07,070
보통 이런 직관을 갖습니다,

270
00:13:07,070 --> 00:13:09,470
이건 단순한 예제입니다.

271
00:13:09,470 --> 00:13:13,330
아이디어는 입력이 x이고 출력이 y인 함수에 이

272
00:13:13,330 --> 00:13:15,810
점들을 맞추고 싶다는 겁니다.

273
00:13:15,810 --> 00:13:20,930
그리고 두 가지 다른 모델, f1과 f2가 있다고 합시다.

274
00:13:20,930 --> 00:13:23,330
그리고 어느 쪽이 더 나은지 결정하려고 합니다.

275
00:13:23,330 --> 00:13:26,230
f1은 모든 데이터 포인트를 통과하므로

276
00:13:26,230 --> 00:13:29,650
훈련 또는 데이터 손실이 매우 낮을 겁니다,

277
00:13:29,650 --> 00:13:33,530
거의 완벽하게 맞추니까요, 반면 f2는 모든 점을

278
00:13:33,530 --> 00:13:35,370
완벽하게 맞추지는 않습니다.

279
00:13:35,370 --> 00:13:38,690
하지만 직관적으로는, 아마도 f2가 우리가 전에

280
00:13:38,690 --> 00:13:42,330
본 적 없는 새로운 데이터에서 테스트할 때 더 나은

281
00:13:42,330 --> 00:13:43,650
모델일 것 같습니다.

282
00:13:43,650 --> 00:13:47,260
그래서 정규화는 데이터를 너무 과하게 맞추지 말라는

283
00:13:47,260 --> 00:13:49,680
이 직관을 담고 있고, 데이터에

284
00:13:49,680 --> 00:13:51,900
덜 맞지만 더 단순하거나

285
00:13:51,900 --> 00:13:55,380
다른 특성으로 인해 더 나은 선택이 될 수 있는

286
00:13:55,380 --> 00:13:59,060
모델이 실제로 더 나을 수 있다는 걸 의미합니다.

287
00:13:59,060 --> 00:14:02,220
그래서 만약 우리가 묻는다면, 같은 분포 내의 새로운

288
00:14:02,220 --> 00:14:04,840
데이터에 대해 이 모델들이 어떻게 작동할까요?

289
00:14:04,840 --> 00:14:07,680
f2가 모델링을 훨씬 더 잘하는 것을 알 수 있습니다.

290
00:14:07,680 --> 00:14:09,810
여기서는 보지 못한 데이터에 대해 더 잘 작동하고 있죠.

291
00:14:12,420 --> 00:14:14,800
이전 예제에서 잘 보여준 직관이

292
00:14:14,800 --> 00:14:17,300
하나 더 있는데, 우리는 더 단순한

293
00:14:17,300 --> 00:14:19,780
모델을 선호하는 경향이 있습니다. 이는

294
00:14:19,780 --> 00:14:23,642
Occam's razor라는 철학적, 과학적 발견의

295
00:14:23,642 --> 00:14:25,100
개념으로, 여러 경쟁

296
00:14:25,100 --> 00:14:27,552
가설이 있을 때 가장 단순한 가설부터

297
00:14:27,552 --> 00:14:29,260
선택하고, 그것이 확실히

298
00:14:29,260 --> 00:14:31,240
틀렸다고 알게 되면 더

299
00:14:31,240 --> 00:14:33,440
복잡한 가설을 시도해보는 것입니다.

300
00:14:33,440 --> 00:14:34,982
이것이 정규화가

301
00:14:34,982 --> 00:14:40,400
왜 유용할 수 있는지에 대한 직관이기도 합니다.

302
00:14:40,400 --> 00:14:42,260
그리고 아직 언급하지 않은 이

303
00:14:42,260 --> 00:14:45,190
방정식의 마지막 부분은 바로 이 lambda 파라미터입니다.

304
00:14:45,190 --> 00:14:47,790
이것은 정규화 강도, 즉 또 다른

305
00:14:47,790 --> 00:14:49,550
하이퍼파라미터입니다.

306
00:14:49,550 --> 00:14:52,310
그래서 우리는 훈련 세트와 검증 세트를 사용해

307
00:14:52,310 --> 00:14:55,070
이 lambda의 최적 값을 설정할 수 있습니다.

308
00:14:55,070 --> 00:14:57,390
기본 아이디어는 이 값을 0과 무한대

309
00:14:57,390 --> 00:14:59,910
사이의 부동 소수점으로 설정할 수

310
00:14:59,910 --> 00:15:02,870
있다는 겁니다. 0이면 사실상 정규화가

311
00:15:02,870 --> 00:15:06,170
전혀 없는 상태이고, 무한대까지 갈 수 있습니다.

312
00:15:06,170 --> 00:15:09,770
점점 더 강한 정규화를 적용하는 거죠.

313
00:15:09,770 --> 00:15:12,910
즉, 모델이 훈련 데이터에

314
00:15:12,910 --> 00:15:14,790
과적합되는 것을 얼마나

315
00:15:14,790 --> 00:15:19,750
막을지 조절할 수 있는 조절기 같은 겁니다.

316
00:15:19,750 --> 00:15:23,410
이제 정규화의 간단한 예제를 몇 가지 살펴보겠습니다.

317
00:15:23,410 --> 00:15:26,670
여기 L2 정규화가 있는데,

318
00:15:26,670 --> 00:15:30,910
기본적으로 가중치 행렬의 각 항목을

319
00:15:30,910 --> 00:15:33,370
제곱한 후 모두

320
00:15:33,370 --> 00:15:35,390
더하는 방식입니다.

321
00:15:35,390 --> 00:15:38,790
그 합을 람다(lambda)와 곱해서

322
00:15:38,790 --> 00:15:40,830
전체 손실에 더하는 거죠.

323
00:15:40,830 --> 00:15:42,760
이것이 L2 정규화입니다.

324
00:15:42,760 --> 00:15:44,263
L1 정규화도 매우 비슷하지만,

325
00:15:44,263 --> 00:15:46,680
제곱하는 대신 절댓값을 취하는 차이가 있습니다.

326
00:15:46,680 --> 00:15:48,920
실제로 이 두 가지 정규화

327
00:15:48,920 --> 00:15:53,800
방법이 모델을 훈련할 때 어떻게 다르게 작용하는지에

328
00:15:53,800 --> 00:15:55,180
차이가 있습니다.

329
00:15:55,180 --> 00:15:57,580
L2 정규화에서 일어나는 일 중 하나는,

330
00:15:57,580 --> 00:15:59,663
각 값을 제곱하기 때문에,

331
00:15:59,663 --> 00:16:02,080
아주 작은 값이 있을 때 예를 들어 0.

332
00:16:02,080 --> 00:16:04,020
001 같은 값이 제곱됩니다.

333
00:16:04,020 --> 00:16:06,080
제곱하면 값이 더 작아지죠.

334
00:16:06,080 --> 00:16:09,960
그래서 L2 정규화는 0에 아주 가까운 작은 값들을 허용합니다.

335
00:16:09,960 --> 00:16:11,813
제곱하기 때문에 값이

336
00:16:11,813 --> 00:16:12,980
더 작아지니까요.

337
00:16:12,980 --> 00:16:16,740
그래서 L2에서는 이런 아주 작은 값들이 있으면

338
00:16:16,740 --> 00:16:19,000
패널티가 매우 낮습니다.

339
00:16:19,000 --> 00:16:20,860
반면에 L1은 값을 제곱하지 않습니다.

340
00:16:20,860 --> 00:16:23,503
그냥 원래 값 그대로입니다.

341
00:16:23,503 --> 00:16:24,920
정규화 항을

342
00:16:24,920 --> 00:16:28,580
계산하기 전에 값이 작아지지 않는다는 뜻이죠.

343
00:16:28,580 --> 00:16:31,580
그래서 실제로는 L1 정규화에서는

344
00:16:31,580 --> 00:16:34,320
가중치 행렬에서 0이거나 0에 아주

345
00:16:34,320 --> 00:16:36,900
가까운 값들이 훨씬 더 많이 생깁니다.

346
00:16:36,900 --> 00:16:39,880
반면 L2는 값이 작지만 0은

347
00:16:39,880 --> 00:16:44,140
아닌 값들이 더 넓게 퍼져 있는 경향이 있습니다.

348
00:16:44,140 --> 00:16:46,820
패널티가 아주 작아지니까요.

349
00:16:46,820 --> 00:16:50,340
L2가 모든 가중치를 작고 고르게 분포시키는 이유는

350
00:16:50,340 --> 00:16:51,480
꽤 명확합니다.

351
00:16:51,480 --> 00:16:55,040
그런데 왜 L1은 희소한 벡터를 선호할까요?

352
00:16:55,040 --> 00:16:59,380
생각해볼 방법은, 만약 어떤 값이 0이 되어도 성능이

353
00:16:59,380 --> 00:17:01,980
거의 같다면, L1은 그 값을

354
00:17:01,980 --> 00:17:05,560
0으로 만드는 쪽으로 밀어붙인다는 겁니다.

355
00:17:05,560 --> 00:17:07,540
반면 L2는 그 값이 아주

356
00:17:07,540 --> 00:17:10,140
작지만 0은 아닌 상태로 남게

357
00:17:10,140 --> 00:17:12,020
됩니다. 제곱 때문이죠.

358
00:17:12,020 --> 00:17:14,780
그렇다면 0 값으로 밀어붙인다는 것이

359
00:17:14,780 --> 00:17:16,940
무슨 의미인지 이야기해볼까요?

360
00:17:16,940 --> 00:17:19,660
그래서 이제 이 loss 항을 어떻게 사용하는지에

361
00:17:19,660 --> 00:17:21,740
대해 더 이야기해 보겠습니다.

362
00:17:21,740 --> 00:17:24,780
기본적인 아이디어는 이 값을 최소화하려고 한다는 겁니다.

363
00:17:24,780 --> 00:17:26,500
즉, 모델의 loss나

364
00:17:26,500 --> 00:17:29,160
오류를 최소화하려고 하는 거죠.

365
00:17:29,160 --> 00:17:31,940
여기서 양수 값을 주는 항이

366
00:17:31,940 --> 00:17:35,540
있다면, 그것이 모델 성능이나 데이터

367
00:17:35,540 --> 00:17:37,700
loss에 영향을 주지

368
00:17:37,700 --> 00:17:41,510
않는다면 최적화 과정에서 제거할 것입니다.

369
00:17:41,510 --> 00:17:42,370
이것은 일종의 균형 조절입니다.

370
00:17:42,370 --> 00:17:44,950
정규화 항과 데이터 loss 항의

371
00:17:44,950 --> 00:17:47,490
합을 최적화하려고 하는 거죠.

372
00:17:47,490 --> 00:17:49,550
만약 데이터 loss가 크게 변하지

373
00:17:49,550 --> 00:17:52,913
않지만 정규화 항을 더 낮출 수 있다면, 더 최적화된

374
00:17:52,913 --> 00:17:54,330
모델을 얻을 수 있습니다.

375
00:17:54,330 --> 00:17:57,430
그래서 전체 항을 최소화하려는

376
00:17:57,430 --> 00:18:00,630
관점에서 더 선호되는 모델이 됩니다.

377
00:18:00,630 --> 00:18:03,510
나중에 강의에서 더 복잡한 형태의 정규화도

378
00:18:03,510 --> 00:18:06,810
다룰 텐데, 모두 기본적으로는 훈련 데이터에서 더

379
00:18:06,810 --> 00:18:09,350
잘 맞추어 테스트 데이터에서 더

380
00:18:09,350 --> 00:18:11,870
좋은 성능을 내도록 하는 아이디어입니다.

381
00:18:11,870 --> 00:18:15,370
그중 일부는 모델의 레이어까지 변경하기도

382
00:18:15,370 --> 00:18:17,850
해서 꽤 복잡해집니다.

383
00:18:17,850 --> 00:18:21,830
모델을 정규화하는 방법은 계속 연구가 진행 중인 분야입니다.

384
00:18:21,830 --> 00:18:24,910
매년 새로운 논문이 나오고 있어서 이 분야에 많은 내용이 있습니다.

385
00:18:24,910 --> 00:18:29,310
이번 강의에서는 그중 일부만 다룰 것입니다.

386
00:18:29,310 --> 00:18:32,670
요약하자면, 왜 모델을 정규화하느냐 하면,

387
00:18:32,670 --> 00:18:35,870
첫째, 가중치에 대해 어떤 선호도를 표현할 수 있기

388
00:18:35,870 --> 00:18:36,570
때문입니다.

389
00:18:36,570 --> 00:18:38,740
예를 들어, 문제에 따라

390
00:18:38,740 --> 00:18:40,760
해답이 넓게 퍼져 있거나

391
00:18:40,760 --> 00:18:42,372
가중치 행렬의 많은

392
00:18:42,372 --> 00:18:44,580
값이 0인 희소성을 가져야

393
00:18:44,580 --> 00:18:48,480
한다고 생각한다면, L2 정규화보다 L1 정규화를

394
00:18:48,480 --> 00:18:50,040
선호할 수 있습니다.

395
00:18:50,040 --> 00:18:52,740
정규화 방법에 따라 모델을 더 단순하게

396
00:18:52,740 --> 00:18:56,420
만들어서 테스트 데이터에서 더 잘 작동하게 할 수도 있습니다.

397
00:18:56,420 --> 00:18:58,960
예를 들어, 앞서 보여드린 것처럼

398
00:18:58,960 --> 00:19:02,980
모델에서 아주 높은 차수의 다항항을 강하게

399
00:19:02,980 --> 00:19:06,203
정규화하면 모델을 단순화할 수 있습니다.

400
00:19:06,203 --> 00:19:08,120
자세히 다루지는 않겠지만,

401
00:19:08,120 --> 00:19:10,720
특히 L2 정규화는 최적화

402
00:19:10,720 --> 00:19:12,460
과정을 개선할 수도

403
00:19:12,460 --> 00:19:17,240
있습니다. 왜냐하면 제곱 함수는 포물선과 같기 때문입니다.

404
00:19:17,240 --> 00:19:20,300
y = x 제곱을 그래프로 그리면 포물선이 됩니다.

405
00:19:20,300 --> 00:19:23,720
이 함수들은 볼록(convex) 형태라서 전역 최솟값이 존재하는

406
00:19:23,720 --> 00:19:25,517
좋은 최적화 특성을 가집니다.

407
00:19:25,517 --> 00:19:27,100
이 부분은 이번 강의에서 다루지 않겠습니다.

408
00:19:27,100 --> 00:19:28,260
범위를 벗어나기 때문입니다.

409
00:19:28,260 --> 00:19:30,960
하지만 특정 최적화 유형에서는 정규화가 모델 학습을 더 빠르게

410
00:19:30,960 --> 00:19:33,340
하는 데 도움을 준다는 점은 알아두시기 바랍니다.

411
00:19:36,480 --> 00:19:38,470
여러분께 질문이 있습니다.

412
00:19:38,470 --> 00:19:44,050
W1이면 손가락 하나, W2이면 손가락 두 개를

413
00:19:44,050 --> 00:19:45,690
들어주세요.

414
00:19:45,690 --> 00:19:51,290
이 두 가중치 중에서 L2 정규화기가 선호할

415
00:19:51,290 --> 00:19:54,770
가중치는 어느 쪽일까요?

416
00:19:54,770 --> 00:19:57,267
입력 x가 있습니다.

417
00:19:57,267 --> 00:19:59,850
가중치와 내적을 하면 점수가 같아서,

418
00:19:59,850 --> 00:20:02,690
어느 쪽이든 점수는 1입니다.

419
00:20:02,690 --> 00:20:05,210
데이터 손실도 동일한 상황입니다.

420
00:20:05,210 --> 00:20:08,370
그럼 정규화기는 어느 가중치를

421
00:20:08,370 --> 00:20:10,450
선호할지 판단해 보세요.

422
00:20:10,450 --> 00:20:15,610
W1이라고 생각하면 1, W2라고 생각하면 2를 들어주세요.

423
00:20:15,610 --> 00:20:16,110
좋습니다.

424
00:20:16,110 --> 00:20:16,710
2가 많이 있네요.

425
00:20:16,710 --> 00:20:19,150
네, W2인 이유는 말씀하신 대로 더 퍼져 있기 때문입니다.

426
00:20:19,150 --> 00:20:21,775
각 항목을 제곱할 거니까 1/4를 제곱하면

427
00:20:21,775 --> 00:20:22,830
1/16이 됩니다.

428
00:20:22,830 --> 00:20:27,050
모두 합치면 전체 정규화 항은 1/4이고,

429
00:20:27,050 --> 00:20:29,990
여기서는 제곱하니까 1이 됩니다.

430
00:20:29,990 --> 00:20:32,770
그래서 정규화 손실 측면에서

431
00:20:32,770 --> 00:20:35,003
4배 낮은 거죠.

432
00:20:35,003 --> 00:20:36,670
그리고 말씀드렸듯이, 직관적으로는

433
00:20:36,670 --> 00:20:38,535
더 퍼져 있는 가중치를 선호하는 겁니다.

434
00:20:38,535 --> 00:20:39,910
그럼 이제 또 다른

435
00:20:39,910 --> 00:20:42,190
질문인데, L1은 지금 어떤 걸 선호할까요?

436
00:20:42,190 --> 00:20:45,760
가중치가 1이면 1, 2면 2를 더하는 거죠.

437
00:20:49,390 --> 00:20:51,110
1이 많이 나왔네요.

438
00:20:51,110 --> 00:20:53,170
이건 사실 약간 트릭 질문입니다.

439
00:20:53,170 --> 00:20:56,210
L1 정규화는 각 항목을 합산하는 거라서

440
00:20:56,210 --> 00:20:58,262
둘 다 합이 1이 됩니다.

441
00:20:58,262 --> 00:21:00,470
실제로는 희소성을 고려해 이쪽이 더

442
00:21:00,470 --> 00:21:01,610
많이 쓰이긴 하죠.

443
00:21:01,610 --> 00:21:03,230
하지만 손실 관점에서는

444
00:21:03,230 --> 00:21:07,110
이 두 가중치가 L1 기준으로는 사실상

445
00:21:07,110 --> 00:21:12,210
동등합니다. 1은 0.25를 네 번 더한 값이고, 다른

446
00:21:12,210 --> 00:21:14,090
쪽은 그냥 1이니까요.

447
00:21:14,090 --> 00:21:15,850
그래서 둘 다 합이 1입니다.

448
00:21:15,850 --> 00:21:21,070
따라서 실제 정규화 항은 동일합니다.

449
00:21:21,070 --> 00:21:21,690
네, 알겠습니다.

450
00:21:21,690 --> 00:21:24,190
그럼 예를 들어 0.9라면

451
00:21:24,190 --> 00:21:27,670
L1이 선호되는 경우는 어떤 게 있을까요?

452
00:21:27,670 --> 00:21:32,950
정리하자면, 우리는 x,y 쌍으로 된 데이터셋이 있습니다.

453
00:21:32,950 --> 00:21:35,320
그리고 각 클래스에 대한

454
00:21:35,320 --> 00:21:38,960
점수를 계산하는 방법이 있는데, 여기서는

455
00:21:38,960 --> 00:21:40,320
선형 모델입니다.

456
00:21:40,320 --> 00:21:43,200
행렬 곱셈을 수행하죠.

457
00:21:43,200 --> 00:21:47,402
각 훈련 예제에 대한 Softmax 손실은,

458
00:21:47,402 --> 00:21:48,860
지난

459
00:21:48,860 --> 00:21:53,040
시간에 다뤘듯이, 점수들을 지수화하고

460
00:21:53,040 --> 00:21:56,280
전체 점수 합으로 나누는 겁니다.

461
00:21:56,280 --> 00:21:59,040
지수화를 해서 모두 양수로 만들고,

462
00:21:59,040 --> 00:22:01,260
합산해서 확률 분포를 얻는 거죠.

463
00:22:01,260 --> 00:22:05,360
그래서 최종 값들은 모두 합쳐서 1이 됩니다.

464
00:22:05,360 --> 00:22:06,860
각 클래스에 대한 점수가 있고,

465
00:22:06,860 --> 00:22:09,900
정답 레이블의 마이너스 로그를 취합니다.

466
00:22:09,900 --> 00:22:12,700
이게 바로 정답

467
00:22:12,700 --> 00:22:14,880
레이블의 확률입니다.

468
00:22:14,880 --> 00:22:16,520
전체 손실은 모든 훈련

469
00:22:16,520 --> 00:22:19,320
예제에 대해 이걸 계산해서 Li를

470
00:22:19,320 --> 00:22:21,120
구하는 겁니다. 그리고?

471
00:22:21,120 --> 00:22:24,160
그 다음 모델 가중치에

472
00:22:24,160 --> 00:22:27,640
따라 정규화 항을 더합니다.

473
00:22:27,640 --> 00:22:29,600
왜 일반적으로 Softmax를 사용할까요?

474
00:22:29,600 --> 00:22:32,440
Softmax는 어떤 실수

475
00:22:32,440 --> 00:22:35,450
집합도 확률 분포로 변환해서

476
00:22:35,450 --> 00:22:38,650
합이 1이 되도록 해주기 때문에

477
00:22:38,650 --> 00:22:40,830
매우 유용합니다.

478
00:22:40,830 --> 00:22:43,610
그리고 점수의 값에 따라, 그것은

479
00:22:43,610 --> 00:22:45,930
그 값의 상대적인 확률로

480
00:22:45,930 --> 00:22:46,870
변환됩니다.

481
00:22:46,870 --> 00:22:48,850
그래서 만약 정말 높은 양수 값이

482
00:22:48,850 --> 00:22:51,970
있고 나머지는 매우 낮은 음수라면, Softmax에서는

483
00:22:51,970 --> 00:22:54,950
거의 1에 가깝고 다른 값들은 거의 0이 될 것입니다.

484
00:22:54,950 --> 00:22:59,930
이것이 좋은 점은 어떤 부동 소수점 숫자 리스트도 그

485
00:22:59,930 --> 00:23:01,970
리스트의 값에 기반한

486
00:23:01,970 --> 00:23:05,430
확률 리스트로 변환해 준다는 겁니다.

487
00:23:05,430 --> 00:23:07,930
이것이 Softmax의 주요 용도입니다.

488
00:23:07,930 --> 00:23:10,970
그래서 질문은 우리가 이야기한 정규화, 즉

489
00:23:10,970 --> 00:23:14,930
L1, L2를 가중치 크기에 기반한 정규화 방법으로

490
00:23:14,930 --> 00:23:17,770
볼 수 있다는 점인데, 이는 맞습니다.

491
00:23:17,770 --> 00:23:20,110
그것이 어떻게 더 단순한 모델로 이어질까요?

492
00:23:20,110 --> 00:23:22,510
L1의 설명은 사실 꽤 간단한데,

493
00:23:22,510 --> 00:23:28,110
예를 들어 0이 많은 항들을 선호한다면, 이는 기본적으로 계수가

494
00:23:28,110 --> 00:23:30,910
적은 선형 모델이라는 겁니다.

495
00:23:30,910 --> 00:23:32,390
그래서 이것은 사실 상대적으로

496
00:23:32,390 --> 00:23:33,515
직관적입니다.

497
00:23:33,515 --> 00:23:35,230
하지만 일반적으로 정규화가

498
00:23:35,230 --> 00:23:38,010
항상 더 단순한 모델을 만들어 주는 것은 아닙니다.

499
00:23:38,010 --> 00:23:39,390
그것이 어떻게 사용되느냐에 달려 있습니다.

500
00:23:39,390 --> 00:23:43,430
예를 들어, 여기 처음에 보여준

501
00:23:43,430 --> 00:23:48,430
도표에서, 함수의 고차 다항항에 더 큰 페널티를

502
00:23:48,430 --> 00:23:51,050
주는 L2 또는 L1

503
00:23:51,050 --> 00:23:53,850
정규화를 상상할 수 있습니다.

504
00:23:53,850 --> 00:23:55,310
그런 의미에서, 정규화를

505
00:23:55,310 --> 00:23:56,768
설계해 더 단순한 모델을 선호하도록

506
00:23:56,768 --> 00:23:58,410
하는 것은 꽤 명확합니다.

507
00:23:58,410 --> 00:24:00,250
하지만 항상 그렇게 될 필요는 없습니다.

508
00:24:00,250 --> 00:24:03,510
실제로는 훈련 데이터에서 성능을 일부 희생해 테스트 데이터에서

509
00:24:03,510 --> 00:24:05,170
더 좋은 성능을 내는 개념이고,

510
00:24:05,170 --> 00:24:07,735
이것이 항상 더 단순한 모델을 의미하지는 않습니다.

511
00:24:07,735 --> 00:24:09,110
사실, dropout

512
00:24:09,110 --> 00:24:13,030
같은 여러 정규화 기법은 모델을 더 복잡하게

513
00:24:13,030 --> 00:24:14,870
만들면서도 테스트 데이터에서

514
00:24:14,870 --> 00:24:17,460
더 좋은 성능을 제공합니다.

515
00:24:22,950 --> 00:24:24,170
좋습니다.

516
00:24:24,170 --> 00:24:25,870
이제 우리가 주어진

517
00:24:25,870 --> 00:24:30,150
W가 훈련 데이터와 정규화 항을 기반으로 얼마나 좋은지

518
00:24:30,150 --> 00:24:34,740
계산하는 방법에 대해 이야기했으니, 다음 질문은 실제로

519
00:24:34,740 --> 00:24:37,920
최적의 W를 어떻게 찾느냐는 것입니다.

520
00:24:37,920 --> 00:24:39,860
이것이 바로 최적화이며,

521
00:24:39,860 --> 00:24:42,600
오늘 강의의 후반부 내용입니다.

522
00:24:42,600 --> 00:24:45,920
사람들이 최적화를 설명할 때 보통 손실 지형(loss

523
00:24:45,920 --> 00:24:48,960
landscape)이라는 개념을 사용하는데,

524
00:24:48,960 --> 00:24:51,340
이는 지구상의 일반적인

525
00:24:51,340 --> 00:24:54,960
지형처럼 생각할 수 있습니다. 여기서 수직 방향, 즉

526
00:24:54,960 --> 00:24:56,680
z축 방향이 손실 값입니다.

527
00:24:56,680 --> 00:24:58,900
이 손실 값이 바로 우리가 최소화하려는 값입니다.

528
00:24:58,900 --> 00:25:00,358
그리고 예를 들어, 이

529
00:25:00,358 --> 00:25:02,420
예제에서는 모델에 두 개의 파라미터가

530
00:25:02,420 --> 00:25:04,240
있는데, 그것은 이 지형에서 당신이

531
00:25:04,240 --> 00:25:05,780
있는 x와 y 방향입니다.

532
00:25:05,780 --> 00:25:08,140
그리고 기본적으로 당신은 그냥 사람인 거죠.

533
00:25:08,140 --> 00:25:09,640
이 지형을 걸어

534
00:25:09,640 --> 00:25:12,223
다니면서 가장 낮거나 최저 지점을

535
00:25:12,223 --> 00:25:13,760
찾으려고 하는 겁니다.

536
00:25:13,760 --> 00:25:15,740
이 비유가 조금 약간 어긋나는

537
00:25:15,740 --> 00:25:17,920
이유 중 하나는—이 비유가 매우

538
00:25:17,920 --> 00:25:20,400
흔히 사용되지만—사람은 시각적으로 멀리

539
00:25:20,400 --> 00:25:22,360
볼 수 있어서 계곡의 가장

540
00:25:22,360 --> 00:25:24,633
낮은 지점을 볼 수 있기 때문입니다.

541
00:25:24,633 --> 00:25:26,800
하지만 이 비유는 사실 사람이 눈가리개를

542
00:25:26,800 --> 00:25:28,980
하고 있다고 생각하면 꽤 정확하다고 봅니다.

543
00:25:28,980 --> 00:25:31,390
그들은 어떤 시각 정보도 얻을 수 없습니다.

544
00:25:31,390 --> 00:25:34,130
지금 서 있는 지점의 땅을 만져보고

545
00:25:34,130 --> 00:25:36,192
그 지점의 경사가 어떤지

546
00:25:36,192 --> 00:25:38,150
이해할 수 있을 뿐입니다.

547
00:25:38,150 --> 00:25:39,650
그런 관점에서 보면

548
00:25:39,650 --> 00:25:43,570
이 비유가 우리가 최적의 모델을 찾으려는 방식에 대해

549
00:25:43,570 --> 00:25:44,830
매우 정확해집니다.

550
00:25:44,830 --> 00:25:47,810
그리고 우리는 모델의 파라미터에 따라 달라지는

551
00:25:47,810 --> 00:25:50,530
다양한 손실 값들의 복잡한 지형을

552
00:25:50,530 --> 00:25:55,290
가지고 있는데, 이 지형에서 사람의 위치가 바로 파라미터에 해당합니다.

553
00:25:55,290 --> 00:25:58,170
그럼 어떻게 최적의 지점을 찾을 수 있을까요?

554
00:25:58,170 --> 00:26:03,626
정말 단순한 아이디어를 생각해볼 수 있는데, 아마 별로 좋은 방법은 아니지만

555
00:26:03,626 --> 00:26:05,553
작동할 수도 있습니다.

556
00:26:05,553 --> 00:26:07,970
여기서는 기본적으로 1,000개의 W

557
00:26:07,970 --> 00:26:11,010
값을 무작위로 시도하는 for 루프를 돌면서

558
00:26:11,010 --> 00:26:13,090
가장 좋은 값을 선택하는 겁니다.

559
00:26:13,090 --> 00:26:15,910
분명히 수학적으로 엄밀하지는 않지만,

560
00:26:15,910 --> 00:26:19,830
무작위 기준보다는 더 잘할 수 있습니다.

561
00:26:19,830 --> 00:26:24,330
다른 방법이 없다면, 이 정도도 나쁘지 않을 수 있습니다.

562
00:26:24,330 --> 00:26:29,580
앞서 보여드린 개구리, 자동차 등 10가지 카테고리가 있는

563
00:26:29,580 --> 00:26:32,460
CIFAR-10 데이터셋에서 15.

564
00:26:32,460 --> 00:26:35,465
5% 정확도를 얻을 수 있습니다.

565
00:26:35,465 --> 00:26:36,840
하지만 성능이 아주 좋지는 않습니다.

566
00:26:36,840 --> 00:26:37,960
이 데이터셋의

567
00:26:37,960 --> 00:26:40,500
최신 상태는 현대 딥러닝으로 거의

568
00:26:40,500 --> 00:26:42,840
해결되어 99.7% 정확도를 얻습니다.

569
00:26:42,840 --> 00:26:46,340
그래서 분명 나쁘지는 않지만, 특별히

570
00:26:46,340 --> 00:26:48,940
좋다고도 할 수 없습니다.

571
00:26:48,940 --> 00:26:54,120
두 번째 전략은 아마 조금 전에 설명한 경사를

572
00:26:54,120 --> 00:26:56,540
따라가는 아이디어입니다.

573
00:26:56,540 --> 00:27:00,512
이 경우, 여러분은 눈가리개를 하고 손실 지형 위에 있다고

574
00:27:00,512 --> 00:27:02,220
상상할 수 있습니다. 발밑의

575
00:27:02,220 --> 00:27:03,580
땅을 느끼는 거죠.

576
00:27:03,580 --> 00:27:06,620
그리고 생각합니다. '어느 방향이 지형의

577
00:27:06,620 --> 00:27:09,020
경사면이 나를 가리키는가?' 하고요.

578
00:27:09,020 --> 00:27:12,120
항상 그 방향으로 걸어야 합니다.

579
00:27:12,120 --> 00:27:14,500
이 기본 아이디어는 이 강의에서

580
00:27:14,500 --> 00:27:16,615
다루는 모든 모델, 그리고

581
00:27:16,615 --> 00:27:18,740
기본적으로 모든 딥러닝 모델을

582
00:27:18,740 --> 00:27:20,900
훈련하는 방법입니다. 현재 위치의

583
00:27:20,900 --> 00:27:22,780
손실 지형을 느끼면서

584
00:27:22,780 --> 00:27:24,683
경사를 따라 내려가는 거죠.

585
00:27:24,683 --> 00:27:26,600
이것이 매우 직관적인 설명입니다.

586
00:27:26,600 --> 00:27:29,580
이제 그 뒤에 있는 수학을 좀 더

587
00:27:29,580 --> 00:27:34,000
살펴보겠지만, 머릿속에 이렇게 그려야 합니다.

588
00:27:34,000 --> 00:27:36,380
그럼 실제로 경사를 어떻게 따라가나요?

589
00:27:36,380 --> 00:27:38,360
1차원에서는 미분 개념에

590
00:27:38,360 --> 00:27:41,840
익숙하실 겁니다. 미적분에서 아주

591
00:27:41,840 --> 00:27:45,680
작은 수 h를 현재 위치에 더하고, 그

592
00:27:45,680 --> 00:27:48,740
위치에서 함수 값을 계산한 뒤,

593
00:27:48,740 --> 00:27:51,500
현재 위치의 함수 값을 빼고,

594
00:27:51,500 --> 00:27:53,580
그 차이를 h로 나누는

595
00:27:53,580 --> 00:27:55,300
극한 정의입니다.

596
00:27:55,300 --> 00:27:58,620
그리고 h가 0에 가까워질

597
00:27:58,620 --> 00:28:05,160
때 이 값이 그 점에서 함수의 미분값이 됩니다.

598
00:28:05,160 --> 00:28:08,040
이것은 1차원에 대한 것이지만,

599
00:28:08,040 --> 00:28:11,240
다차원에서는 gradient를 사용합니다.

600
00:28:11,240 --> 00:28:16,263
gradient는 각 값에 대해 이 극한 정의를 따로 계산하는 것입니다.

601
00:28:16,263 --> 00:28:17,680
그래서 각 값마다

602
00:28:17,680 --> 00:28:20,280
다른 미분값을 갖게 됩니다.

603
00:28:20,280 --> 00:28:23,280
그리고 벡터를 얻게 되죠.

604
00:28:23,280 --> 00:28:26,810
이 벡터는 각 차원에 따른 방향을 알려줍니다.

605
00:28:26,810 --> 00:28:29,610
그래서 gradient와 방향

606
00:28:29,610 --> 00:28:33,890
벡터의 내적을 취하면 그 차원에서의 기울기를 계산할

607
00:28:33,890 --> 00:28:36,890
수 있습니다. 특히 가장 가파른 하강

608
00:28:36,890 --> 00:28:40,012
방향은 음의 gradient입니다.

609
00:28:40,012 --> 00:28:41,470
gradient는 언덕 위를 가리키고,

610
00:28:41,470 --> 00:28:43,750
음의 gradient는 언덕 아래를 가리킵니다.

611
00:28:43,750 --> 00:28:45,187
그래서 우리가 이

612
00:28:45,187 --> 00:28:47,770
손실 함수의 최저점에 도달하려면

613
00:28:47,770 --> 00:28:50,210
이 방향으로 가야 합니다.

614
00:28:50,210 --> 00:28:53,047
그럼 미분을 계산하는 방법에는 어떤 것들이 있을까요?

615
00:28:53,047 --> 00:28:55,130
아주 간단한 방법은 극한 h

616
00:28:55,130 --> 00:28:58,210
정의를 아주 작은 h 값으로 실제로 사용하는

617
00:28:58,210 --> 00:29:01,170
것입니다. 예를 들어 0.0001을 더하는 거죠.

618
00:29:01,170 --> 00:29:04,182
손실 함수 값이 아주 조금 변하는 것을 실제로

619
00:29:04,182 --> 00:29:05,390
계산할 수 있습니다.

620
00:29:05,390 --> 00:29:07,110
그래서 차이를 계산하고,

621
00:29:07,110 --> 00:29:09,970
그걸 스텝 크기로 나누면 미분값의 근사치를

622
00:29:09,970 --> 00:29:11,390
얻을 수 있습니다.

623
00:29:11,390 --> 00:29:15,090
이 방법을 W의 각 값에 대해

624
00:29:15,090 --> 00:29:19,010
실제로 적용할 수 있습니다. 이 절차를 계속 반복하는 거죠.

625
00:29:19,010 --> 00:29:20,350
하지만 몇 가지 문제가 있습니다.

626
00:29:20,350 --> 00:29:21,850
각 값을 모두 반복해야

627
00:29:21,850 --> 00:29:23,330
하므로 매우 느립니다.

628
00:29:23,330 --> 00:29:26,060
이것도 근사값이기 때문에 실제 미분값을 계산하는

629
00:29:26,060 --> 00:29:27,160
것은 아닙니다.

630
00:29:27,160 --> 00:29:29,280
특히 부동소수점 연산에서는

631
00:29:29,280 --> 00:29:32,240
상당한 오차가 발생할 수 있어서, 이

632
00:29:32,240 --> 00:29:34,120
방법은 선호되지 않습니다.

633
00:29:34,120 --> 00:29:37,260
하지만 우리가 할 수 있는 기본적인 아이디어나

634
00:29:37,260 --> 00:29:40,400
직관은 이렇게 미분을 계산하는 것입니다.

635
00:29:40,400 --> 00:29:45,060
하지만 실제로는 손실 함수가 W의 함수입니다.

636
00:29:45,060 --> 00:29:49,100
그래서 우리는 모델 함수로부터 점수를

637
00:29:49,100 --> 00:29:53,180
계산해서 손실을 구하는 방법을 알고 있습니다.

638
00:29:53,180 --> 00:29:55,220
그리고 정규화 항을 포함한

639
00:29:55,220 --> 00:29:57,680
전체 손실도 계산할 수 있습니다.

640
00:29:57,680 --> 00:30:03,580
이 전체 손실은 기본적으로 W, Xi,

641
00:30:03,580 --> 00:30:05,762
yi의 함수입니다.

642
00:30:05,762 --> 00:30:07,220
즉, W 행렬과 Xi,

643
00:30:07,220 --> 00:30:09,762
yi가 있고, 로그나 지수 함수가 포함된

644
00:30:09,762 --> 00:30:10,960
수식이 있습니다.

645
00:30:10,960 --> 00:30:16,700
근본적으로 이것은 W, X, y의 함수입니다.

646
00:30:16,700 --> 00:30:19,860
그리고 우리는 특히 손실 함수에 대한 가중치의

647
00:30:19,860 --> 00:30:22,740
기울기, 즉 그리스 문자 nabla로

648
00:30:22,740 --> 00:30:25,130
표현된 기울기를 계산하고자 합니다.

649
00:30:25,130 --> 00:30:29,810
Xi와 yi는 고정되어 있다고 가정할 수 있습니다.

650
00:30:29,810 --> 00:30:32,310
그리고 가중치에 대해서만

651
00:30:32,310 --> 00:30:34,910
미분을 계산하려고 합니다.

652
00:30:34,910 --> 00:30:38,445
이를 위해 미적분학, 연쇄법칙,

653
00:30:38,445 --> 00:30:40,070
복잡하거나

654
00:30:40,070 --> 00:30:45,230
덜 복잡한 방정식의 미분 방법을 사용할 수

655
00:30:45,230 --> 00:30:46,250
있습니다.

656
00:30:46,250 --> 00:30:48,750
여기에는 로그, 지수,

657
00:30:48,750 --> 00:30:51,030
연쇄법칙이 필요합니다.

658
00:30:51,030 --> 00:30:53,250
이것은 숙제 문제로 나올 것입니다.

659
00:30:53,250 --> 00:30:54,970
그래서 지금 단계별로 설명하지는 않겠습니다.

660
00:30:54,970 --> 00:30:56,090
하지만 개념적으로 여러분 모두 이해할 수 있을 거라 생각합니다.

661
00:30:56,090 --> 00:30:58,150
X와 y가 고정되어 있다고

662
00:30:58,150 --> 00:30:58,650
가정합니다.

663
00:30:58,650 --> 00:31:00,442
그다음 W가 변할 때 미분값을 구합니다.

664
00:31:00,442 --> 00:31:04,170
이제 우리는 데이터와 현재 W, 그리고 손실 함수에 따라 W의 기울기 dW를
계산할 수 있는 방법이 있습니다.

665
00:31:04,170 --> 00:31:06,270
요약하자면

666
00:31:06,270 --> 00:31:10,310
이

667
00:31:10,310 --> 00:31:13,550
렇

668
00:31:13,550 --> 00:31:17,550
습니다.

669
00:31:17,550 --> 00:31:20,630
습니다.

670
00:31:20,630 --> 00:31:23,130
수치적 기울기를 사용할 수 있지만 근사값이고 느립니다.

671
00:31:23,130 --> 00:31:25,510
장점은 코드를 작성하기 매우

672
00:31:25,510 --> 00:31:27,110
쉽다는 점입니다.

673
00:31:27,110 --> 00:31:29,710
아주 작은 h를 더하고 차이를 h로 나누면 됩니다.

674
00:31:29,710 --> 00:31:33,970
해석적 기울기는 정확하고 빠릅니다.

675
00:31:33,970 --> 00:31:36,390
하지만 만약 새로운 코드를 처음부터 작성한다면 오류가 있을 수 있습니다.

676
00:31:36,390 --> 00:31:37,850
그래서 보통은 기울기

677
00:31:37,850 --> 00:31:40,390
검사를 하는데, 아주 작은 h 값을 사용한

678
00:31:40,390 --> 00:31:41,910
수치적 방법과 비교해서

679
00:31:41,910 --> 00:31:43,160
비슷한지 확인합니다.

680
00:31:43,160 --> 00:31:44,930
이것은

681
00:31:44,930 --> 00:31:46,730
코드에 버그가

682
00:31:46,730 --> 00:31:50,410
없는지 확인하는

683
00:31:50,410 --> 00:31:52,262
좋은

684
00:31:52,262 --> 00:31:53,470
방법입니다.

685
00:31:53,470 --> 00:31:55,303
여러분 숙제에도 구현이 올바른지

686
00:31:55,303 --> 00:31:57,357
확인하는 기울기 검사가 포함될 것입니다.

687
00:31:57,357 --> 00:31:59,690
네, 질문은 우리가 보통 미분 가능한 손실 함수를

688
00:31:59,690 --> 00:32:01,565
원한다고 하는데, 그래야 기울기를 계산할

689
00:32:01,565 --> 00:32:02,610
수 있기 때문입니다.

690
00:32:02,610 --> 00:32:05,250
만약 더 좋은 손실

691
00:32:05,250 --> 00:32:07,957
함수가 있는데 해석적으로

692
00:32:07,957 --> 00:32:09,790
기울기를

693
00:32:09,790 --> 00:32:12,690
계산할 수 없고, 대신

694
00:32:12,690 --> 00:32:16,310
이 작은 h를 이용한 수치적

695
00:32:16,310 --> 00:32:20,670
방법을 쓸 수 있다면, 그게

696
00:32:20,670 --> 00:32:21,780
가능할까요?

697
00:32:21,780 --> 00:32:25,900
일반적으로는 비미분 가능하면서

698
00:32:25,900 --> 00:32:33,080
더 좋은 손실 함수를 만드는 것은 어렵다고 생각합니다.

699
00:32:33,080 --> 00:32:34,660
그럴 수도 있습니다.

700
00:32:34,660 --> 00:32:36,140
당신의 경우에 가장 좋은 진짜

701
00:32:36,140 --> 00:32:38,720
손실 함수가 있지만, 그것은 미분 가능하지 않습니다.

702
00:32:38,720 --> 00:32:42,440
이 방법을 사용할 수 있고, 작동할 수도 있습니다.

703
00:32:42,440 --> 00:32:44,880
예를 들어, 손실 함수가 모든

704
00:32:44,880 --> 00:32:48,340
지점에서 진짜로 미분 불가능하고, 기본적으로

705
00:32:48,340 --> 00:32:50,300
연결되지 않은 점들의

706
00:32:50,300 --> 00:32:56,140
집합이라면, 가장 가파른 하강법으로 이동하는 것이 반드시 최적의 해에
도달하지

707
00:32:56,140 --> 00:32:57,980
못할 것입니다. 왜냐하면

708
00:32:57,980 --> 00:33:00,580
점들이 잘 연결되어 지형을

709
00:33:00,580 --> 00:33:02,280
형성하지 않기 때문입니다.

710
00:33:02,280 --> 00:33:03,960
그래서 작동할 수도 있습니다.

711
00:33:03,960 --> 00:33:06,100
하지만 손실 함수가 대부분

712
00:33:06,100 --> 00:33:08,940
영역에서 미분 불가능하다면,

713
00:33:08,940 --> 00:33:12,552
아마도 이런 방법들로 최저점을 찾는

714
00:33:12,552 --> 00:33:14,460
것은 어려울 것입니다.

715
00:33:14,460 --> 00:33:17,140
설명의 요약은, 함수가

716
00:33:17,140 --> 00:33:19,000
볼록(convex)하다면 이

717
00:33:19,000 --> 00:33:23,030
그래디언트 하강법이나 가장 가파른 하강법이 매우 잘

718
00:33:23,030 --> 00:33:24,770
작동한다는 겁니다.

719
00:33:24,770 --> 00:33:27,230
하지만 미분 불가능하고 비볼록 함수라면,

720
00:33:27,230 --> 00:33:28,950
이 방법이 잘 작동하지 않을

721
00:33:28,950 --> 00:33:31,270
가능성이 큽니다. 왜냐하면 올바른 방향으로

722
00:33:31,270 --> 00:33:32,910
이동하지 않을 테니까요.

723
00:33:32,910 --> 00:33:36,290
코드가 완벽하다면 반드시 오류가 발생하는 것은 아니지만,

724
00:33:36,290 --> 00:33:38,230
코드에 실수가 있을 수도 있고

725
00:33:38,230 --> 00:33:40,190
바로 알기 어려울 수 있습니다.

726
00:33:40,190 --> 00:33:42,590
하지만 h, 즉 극한 h 정의는 코딩하기

727
00:33:42,590 --> 00:33:43,670
매우 쉽습니다.

728
00:33:43,670 --> 00:33:45,575
h를 아주 작은 값으로 설정하고,

729
00:33:45,575 --> 00:33:46,950
함수를 실행한 뒤 아주

730
00:33:46,950 --> 00:33:48,283
작은 값을 더하면 됩니다.

731
00:33:48,283 --> 00:33:51,070
그래서 구현 시 오류 가능성이 적습니다.

732
00:33:51,070 --> 00:33:52,550
[잘 들리지 않음]

733
00:33:52,550 --> 00:33:53,750
구현 시.

734
00:33:53,750 --> 00:33:54,550
네.

735
00:33:54,550 --> 00:33:56,170
그럴 가치가 있다면 더 오류가 발생하지 않습니다.

736
00:33:56,170 --> 00:33:57,750
맞습니다.

737
00:33:57,750 --> 00:33:58,250
좋습니다.

738
00:33:58,250 --> 00:34:01,670
이제 최적화를 위한 기본 알고리즘인 gradient

739
00:34:01,670 --> 00:34:03,950
descent에 대해 이야기하겠습니다.

740
00:34:03,950 --> 00:34:06,450
기본적인 직관은 우리가 이미 설명한 내용과 같습니다.

741
00:34:06,450 --> 00:34:08,389
우리는 loss landscape

742
00:34:08,389 --> 00:34:09,847
위의 각 지점에서 기울기를

743
00:34:09,847 --> 00:34:13,110
계산하고, loss landscape의 바닥을 향해 아래

744
00:34:13,110 --> 00:34:14,449
방향으로 한 걸음 내딛습니다.

745
00:34:14,449 --> 00:34:18,190
즉, loss 함수, 데이터, 그리고

746
00:34:18,190 --> 00:34:22,510
현재 가중치 값을 기준으로 가중치의 기울기를

747
00:34:22,510 --> 00:34:23,870
계산합니다.

748
00:34:23,870 --> 00:34:26,328
이것은 각 가중치를 얼마나 변경해야 경사를 따라 내려갈 수 있는지

749
00:34:26,328 --> 00:34:29,030
알려줍니다. 그리고 우리는 한 걸음의 크기, 즉 step size가
필요합니다.

750
00:34:29,030 --> 00:34:32,489
얼마나 멀리 경사면을 따라 내려갈지를 결정하는 거죠.

751
00:34:32,489 --> 00:34:34,770
그래서 경사면을 따라 내려가므로

752
00:34:34,770 --> 00:34:38,790
step size에 기울기를 곱한 값에 마이너스 부호가 붙습니다.

753
00:34:38,790 --> 00:34:41,969
이것이 기본적으로 gradient descent입니다.

754
00:34:41,969 --> 00:34:44,330
각 단계에서 기울기를 계산하고,

755
00:34:44,330 --> 00:34:46,770
음의 기울기 방향, 즉

756
00:34:46,770 --> 00:34:51,810
경사면 아래 방향으로 고정된 크기만큼 이동하는 겁니다.

757
00:34:51,810 --> 00:34:53,870
구체적인 예를 들어보겠습니다.

758
00:34:53,870 --> 00:34:56,030
3D loss landscape

759
00:34:56,030 --> 00:34:58,530
대신, 사람들이 종종 이렇게 위에서 내려다보는

760
00:34:58,530 --> 00:35:00,190
식으로 시각화합니다.

761
00:35:00,190 --> 00:35:02,250
보라색은 가장 높은 지점을,

762
00:35:02,250 --> 00:35:05,690
빨간색은 바닥이나 계곡을 나타냅니다.

763
00:35:05,690 --> 00:35:07,630
그리고 원래의 W가 있다고 상상할 수 있습니다.

764
00:35:07,630 --> 00:35:08,790
우리는 loss를 계산할 수 있습니다.

765
00:35:08,790 --> 00:35:11,930
우리는 기울기의 방향, 즉 음의 그래디언트 방향을

766
00:35:11,930 --> 00:35:12,670
알고 있습니다.

767
00:35:12,670 --> 00:35:16,223
그리고 이 화살표는 이전에 이야기한 고정된 스텝 크기를

768
00:35:16,223 --> 00:35:17,390
나타낼 수 있습니다.

769
00:35:17,390 --> 00:35:20,960
우리는 그 방향으로 고정된 스텝 크기만큼 이동하고 있습니다.

770
00:35:25,420 --> 00:35:27,440
네, 고정된 스텝 크기인 것을 볼 수 있습니다.

771
00:35:27,440 --> 00:35:30,435
하지만 그래디언트가 작아질수록 여전히 이 고정된

772
00:35:30,435 --> 00:35:32,560
스텝 크기로 곱하고 있습니다.

773
00:35:32,560 --> 00:35:34,420
그래서 실제로 효과적인

774
00:35:34,420 --> 00:35:36,620
스텝 크기는 그래디언트가 평평한

775
00:35:36,620 --> 00:35:39,220
끝부분 근처에서 작아지기 때문에

776
00:35:39,220 --> 00:35:40,280
작아집니다.

777
00:35:40,280 --> 00:35:42,038
이것이 우리가 항상 가장

778
00:35:42,038 --> 00:35:43,580
가파른 하강 방향으로

779
00:35:43,580 --> 00:35:45,580
이동할 때의 모습입니다.

780
00:35:45,580 --> 00:35:47,218
그렇다면, 내려갈 때 언제

781
00:35:47,218 --> 00:35:48,760
멈출지 어떻게 알 수 있을까요?

782
00:35:48,760 --> 00:35:52,940
음, 이 공식에서는 그냥 무한히 반복해서 멈추지

783
00:35:52,940 --> 00:35:54,183
않는 것 같네요.

784
00:35:54,183 --> 00:35:55,600
그래서 이 방법은 아마 최선은 아니었을 겁니다.

785
00:35:55,600 --> 00:35:57,308
보통은 미리 정해진

786
00:35:57,308 --> 00:35:59,900
반복 횟수만큼 실행합니다.

787
00:35:59,900 --> 00:36:03,940
또는 손실이 고정된 양만큼 크게 변하지 않는지를

788
00:36:03,940 --> 00:36:05,480
확인할 수도 있습니다.

789
00:36:05,480 --> 00:36:08,443
손실이 계속 감소할 것으로 기대하는 허용 오차를 설정할

790
00:36:08,443 --> 00:36:09,360
수 있습니다.

791
00:36:09,360 --> 00:36:11,100
만약 더 이상 감소하지

792
00:36:11,100 --> 00:36:14,823
않고 1E-5나 1E-9 정도만 감소한다면, 충분히 좋은

793
00:36:14,823 --> 00:36:16,740
상태이므로 멈출 수 있습니다.

794
00:36:16,740 --> 00:36:19,073
그래서 멈추는 방법은

795
00:36:19,073 --> 00:36:23,770
고정된 반복 횟수이거나 더 이상 크게 개선되지

796
00:36:23,770 --> 00:36:26,020
않는다는 기준입니다.

797
00:36:29,670 --> 00:36:33,070
이제 가장 인기 있는 그래디언트 하강법

798
00:36:33,070 --> 00:36:35,950
변형인 확률적 그래디언트 하강법에

799
00:36:35,950 --> 00:36:37,630
대해 이야기하겠습니다.

800
00:36:37,630 --> 00:36:39,430
이전에 그래디언트

801
00:36:39,430 --> 00:36:41,110
하강법을 이야기할 때,

802
00:36:41,110 --> 00:36:44,790
전체 훈련 세트에 대해 각 i에 대한

803
00:36:44,790 --> 00:36:50,230
손실 Li를 모두 합산하여 가중치의 손실을 계산한다고 했습니다.

804
00:36:50,230 --> 00:36:53,030
하지만 데이터 세트가 매우 크면

805
00:36:53,030 --> 00:36:55,630
계산량이 많아질 수 있습니다.

806
00:36:55,630 --> 00:36:59,253
그래서 확률적 그래디언트 하강법은 전체 데이터

807
00:36:59,253 --> 00:37:01,170
세트 대신 매번 일부

808
00:37:01,170 --> 00:37:03,350
데이터만 보는 방법입니다. 이를

809
00:37:03,350 --> 00:37:05,990
미니 배치 또는 배치라고 부릅니다.

810
00:37:05,990 --> 00:37:08,110
코드를 보면 데이터

811
00:37:08,110 --> 00:37:12,370
세트에서 256개의 데이터를 샘플링하는데,

812
00:37:12,370 --> 00:37:15,135
이때 배치 크기는 256입니다.

813
00:37:15,135 --> 00:37:20,360
이 256개의 데이터 부분집합에 대해 그래디언트를 평가하고 이전과

814
00:37:20,360 --> 00:37:22,340
같은 작업을 수행합니다.

815
00:37:22,340 --> 00:37:24,840
확률적 그래디언트 하강법이라고 부르는

816
00:37:24,840 --> 00:37:27,520
이유는 알고리즘의 각 단계마다

817
00:37:27,520 --> 00:37:29,920
데이터 세트에서 무작위 부분집합을

818
00:37:29,920 --> 00:37:31,480
샘플링하기 때문입니다.

819
00:37:31,480 --> 00:37:33,740
이것이 확률적 그래디언트 하강법입니다.

820
00:37:33,740 --> 00:37:37,680
즉, 매번 무작위 부분집합에 대해 실행하는 것이죠.

821
00:37:37,680 --> 00:37:39,760
실제로는 완전히 무작위로

822
00:37:39,760 --> 00:37:41,540
샘플링하지 않고,

823
00:37:41,540 --> 00:37:45,400
데이터 세트의 모든 예제를 한 번씩 다 보고 다시

824
00:37:45,400 --> 00:37:47,220
반복하는 방식을 사용합니다.

825
00:37:47,220 --> 00:37:49,460
이것을 한 번의 에포크(epoch)라고

826
00:37:49,460 --> 00:37:51,560
하며, 무작위 순서로 모든 데이터를 한

827
00:37:51,560 --> 00:37:52,580
번 훑는 것입니다.

828
00:37:55,370 --> 00:37:57,920
그래디언트 하강법이나 확률적 그래디언트 하강법에는

829
00:37:57,920 --> 00:37:59,340
몇 가지 문제가 있습니다.

830
00:37:59,340 --> 00:38:02,760
이 시각화는 이전에 보여준 컬러 버전과 같은

831
00:38:02,760 --> 00:38:05,560
유형으로, 손실 지형을 위에서 내려다보는

832
00:38:05,560 --> 00:38:06,540
모습입니다.

833
00:38:06,540 --> 00:38:08,900
하지만 이 곡선들은 레벨 셋(level

834
00:38:08,900 --> 00:38:11,060
set)이라고 불리며, 손실이 모두 같은

835
00:38:11,060 --> 00:38:12,060
점들의 집합입니다.

836
00:38:12,060 --> 00:38:14,240
즉, 색 없이 손실을

837
00:38:14,240 --> 00:38:17,040
위에서 내려다보는 매우 인기

838
00:38:17,040 --> 00:38:19,460
있는 시각화 방법입니다.

839
00:38:19,460 --> 00:38:24,060
여기서 좁고 가파른 계곡이 있고, 그 계곡의

840
00:38:24,060 --> 00:38:25,783
중앙을 따라

841
00:38:25,783 --> 00:38:27,200
이동하려는

842
00:38:27,200 --> 00:38:30,260
현상을 상상할 수 있습니다.

843
00:38:30,260 --> 00:38:34,540
그리고 실제로 gradient descent는 여기서 문제가 발생할 수
있습니다.

844
00:38:34,540 --> 00:38:38,715
무엇이 잘못될 수 있을지 아이디어가 있으신가요?

845
00:38:38,715 --> 00:38:40,340
네, 한 가지 가능한

846
00:38:40,340 --> 00:38:43,700
문제는 이 방향을 따라 위아래로 움직일 때

847
00:38:43,700 --> 00:38:45,620
과도하게 이동하는 것입니다.

848
00:38:45,620 --> 00:38:48,920
만약 경사가 충분히 가파르고 스텝 크기가 크다면, 실제로

849
00:38:48,920 --> 00:38:51,120
계곡 밖으로 진동할 수도 있습니다.

850
00:38:51,120 --> 00:38:53,432
스텝 크기가 매우 크고 경사가

851
00:38:53,432 --> 00:38:55,140
정말 가파르다고

852
00:38:55,140 --> 00:38:57,300
상상해보면, 매번 고정된

853
00:38:57,300 --> 00:39:00,300
스텝 크기로 계속 밖으로 나가게 되는

854
00:39:00,300 --> 00:39:00,800
거죠.

855
00:39:00,800 --> 00:39:03,780
그래서 경사가 충분히 가파르면 계곡 밖으로

856
00:39:03,780 --> 00:39:05,400
튕겨 나갈 수 있습니다.

857
00:39:05,400 --> 00:39:07,960
이것은 학습률이 너무 크면 실제로 발생하는 현상입니다.

858
00:39:07,960 --> 00:39:09,460
이것이 발생할 수 있는 한 가지 문제입니다.

859
00:39:09,460 --> 00:39:12,880
그리고 학습률이 너무 크지 않고 스텝

860
00:39:12,880 --> 00:39:16,488
크기도 적당해도, 경사가 더 가파른

861
00:39:16,488 --> 00:39:18,030
방향에서 그래디언트가

862
00:39:18,030 --> 00:39:20,430
훨씬 크기 때문에 약간

863
00:39:20,430 --> 00:39:23,492
떨리는 현상이 있을 수 있습니다.

864
00:39:23,492 --> 00:39:24,950
그래서 떨리긴 하지만

865
00:39:24,950 --> 00:39:26,430
실제 중심을 향해 의미

866
00:39:26,430 --> 00:39:27,990
있는 진전은 거의 없는데,

867
00:39:27,990 --> 00:39:30,198
위아래로 진동하면서 시간을 보내기

868
00:39:30,198 --> 00:39:30,750
때문입니다.

869
00:39:30,750 --> 00:39:36,630
이것은 기본적인 SGD에서 꽤 큰 문제입니다.

870
00:39:36,630 --> 00:39:39,333
그리고 수학적으로 잠깐 설명하자면, 여기서

871
00:39:39,333 --> 00:39:40,750
고려하는 손실 함수는

872
00:39:40,750 --> 00:39:42,510
조건수가 높습니다. 조건수는

873
00:39:42,510 --> 00:39:45,990
헤시안 행렬의 가장 큰 특이값과 가장 작은 특이값의

874
00:39:45,990 --> 00:39:48,650
비율인데, 헤시안 행렬은 이차 미분입니다.

875
00:39:48,650 --> 00:39:51,190
그래서 위아래 방향의 이차

876
00:39:51,190 --> 00:39:53,210
미분은 매우 크지만,

877
00:39:53,210 --> 00:39:56,390
옆으로는 매우 낮아서 평평하기

878
00:39:56,390 --> 00:40:00,310
때문에 이런 현상이 발생하는 겁니다.

879
00:40:00,310 --> 00:40:01,830
좋습니다.

880
00:40:01,830 --> 00:40:06,550
또한 SGD에서 문제가 될 수 있는 것 중 하나는

881
00:40:06,550 --> 00:40:11,670
손실 함수에 국소 최소값이나, 국소 최소점 또는 새들

882
00:40:11,670 --> 00:40:14,000
포인트가 있을 때입니다.

883
00:40:14,000 --> 00:40:19,920
예를 들어 여기, 이 곡선의 아주 끝부분은 완전히

884
00:40:19,920 --> 00:40:21,260
평평합니다.

885
00:40:21,260 --> 00:40:26,200
그래서 우리가 여기 언덕을 내려간다고 상상하면, 평평해서

886
00:40:26,200 --> 00:40:28,180
그냥 멈춰버릴 겁니다.

887
00:40:28,180 --> 00:40:30,180
여기서 기울기(gradient)가

888
00:40:30,180 --> 00:40:32,960
0이기 때문에 더 이상 진행할 수 없습니다.

889
00:40:32,960 --> 00:40:36,082
이것은 실제로 꽤 큰 문제인데, 국소

890
00:40:36,082 --> 00:40:38,040
최소값(local minimum)에

891
00:40:38,040 --> 00:40:41,218
갇히게 되기 때문입니다. 여기 도달하면 갈

892
00:40:41,218 --> 00:40:42,260
방향이 없거든요.

893
00:40:42,260 --> 00:40:43,223
기울기가 0입니다.

894
00:40:43,223 --> 00:40:44,640
또는 기울기가 매우 작아서 여기서

895
00:40:44,640 --> 00:40:45,920
계속 왔다 갔다 진동할 수 있습니다.

896
00:40:45,920 --> 00:40:48,960
그리고 여기서는 기울기가 0이라서 이 아래쪽

897
00:40:48,960 --> 00:40:50,657
예시에서 실제로 멈출 수도

898
00:40:50,657 --> 00:40:52,240
있습니다. 조금만 더

899
00:40:52,240 --> 00:40:55,000
가면 훨씬 더 내려갈 수 있는데도 말이죠.

900
00:40:55,000 --> 00:40:58,400
그래서 질문은, 아마도 우리가 스텝을 진행하는 방식을

901
00:40:58,400 --> 00:40:59,900
바꿀 수 있을까요?

902
00:40:59,900 --> 00:41:01,692
아마도 Hessian을 사용해서 우리가 가야 할

903
00:41:01,692 --> 00:41:02,840
방향을 결정할 수 있을 겁니다.

904
00:41:02,840 --> 00:41:05,640
사실 맨 마지막에 Hessian 방식에 대해

905
00:41:05,640 --> 00:41:07,740
간단히 설명하는 슬라이드가 있습니다.

906
00:41:07,740 --> 00:41:09,700
하지만 딥러닝에서는 그리 흔하게 사용되지는 않습니다.

907
00:41:09,700 --> 00:41:11,277
하지만 짧게 대답하자면 그렇습니다.

908
00:41:11,277 --> 00:41:13,610
사실 이걸 고려할 수 있는 여러 방법이

909
00:41:13,610 --> 00:41:14,890
있는데, 5분 후쯤에

910
00:41:14,890 --> 00:41:16,190
다룰 예정입니다.

911
00:41:16,190 --> 00:41:17,430
좋은 질문입니다.

912
00:41:17,430 --> 00:41:18,370
네.

913
00:41:18,370 --> 00:41:19,430
곧 다룰 거예요.

914
00:41:24,050 --> 00:41:27,170
제가 생각하기에 여러분이 잘 모를 수도 있는 다른 점

915
00:41:27,170 --> 00:41:29,450
중 하나는, 경험적으로 고차원 모델로

916
00:41:29,450 --> 00:41:32,030
갈수록 안장점(saddle points)이

917
00:41:32,030 --> 00:41:35,472
훨씬 더 흔하다는 겁니다. 즉, 가중치 행렬이 커질수록 이런

918
00:41:35,472 --> 00:41:37,430
안장점을 만날 가능성이 높아집니다.

919
00:41:37,430 --> 00:41:38,805
이 빈도에 대해

920
00:41:38,805 --> 00:41:40,233
설명하는 논문도 있습니다.

921
00:41:40,233 --> 00:41:41,650
안장점이 뭔지

922
00:41:41,650 --> 00:41:43,567
모른다면—말 안장 모양이라서

923
00:41:43,567 --> 00:41:45,850
안장점이라고 부릅니다.

924
00:41:45,850 --> 00:41:48,090
이 안장의 중심에서는 모든

925
00:41:48,090 --> 00:41:51,510
방향으로 기울기(gradient)가 0입니다.

926
00:41:51,510 --> 00:41:53,690
그래서 이 부분은 곡률의 바닥이자

927
00:41:53,690 --> 00:41:55,670
꼭대기 같은 지점입니다.

928
00:41:55,670 --> 00:41:58,910
x축과 y축 방향 모두에서 기울기가 0이라서,

929
00:41:58,910 --> 00:42:00,930
손실 함수의 양쪽으로 크게

930
00:42:00,930 --> 00:42:03,570
내려갈 수 있는 지점에 매우 가까워도

931
00:42:03,570 --> 00:42:05,330
여기서 멈출 수 있습니다.

932
00:42:05,330 --> 00:42:08,370
이것은 SGD에서 꽤 흔한 문제이기도

933
00:42:08,370 --> 00:42:09,870
합니다. 고차원

934
00:42:09,870 --> 00:42:12,170
공간, 즉 파라미터가

935
00:42:12,170 --> 00:42:15,070
많은 모델로 갈수록 이런 현상이

936
00:42:15,070 --> 00:42:16,870
더 자주 발생합니다.

937
00:42:16,870 --> 00:42:18,750
이 문제는 매우 심각합니다.

938
00:42:18,750 --> 00:42:22,150
SGD의 마지막 문제는 매번

939
00:42:22,150 --> 00:42:26,880
데이터의 일부만 샘플링한다는 점입니다.

940
00:42:26,880 --> 00:42:28,790
전체 데이터에 대한 손실을

941
00:42:28,790 --> 00:42:31,735
나타내는 이 값 전체를 보지 않고,

942
00:42:31,735 --> 00:42:33,610
매번 일부 데이터만 봅니다.

943
00:42:33,610 --> 00:42:36,230
그래서 전체 데이터셋을 보지 않기 때문에

944
00:42:36,230 --> 00:42:39,350
업데이트 단계에 다소 노이즈가 섞이게 됩니다.

945
00:42:39,350 --> 00:42:42,990
우리는 여기서 도달하려는 국소 최소점(local

946
00:42:42,990 --> 00:42:47,130
minimum) 쪽으로 나아가고 있지만,

947
00:42:47,130 --> 00:42:50,010
각 단계가 정확히 그 방향으로 가지는 않습니다.

948
00:42:50,010 --> 00:42:52,510
데이터셋을 부분 샘플링하기

949
00:42:52,510 --> 00:42:56,390
때문에 진행 과정에 노이즈가 있는 거죠.

950
00:42:56,390 --> 00:42:58,550
좋습니다.

951
00:42:58,550 --> 00:43:02,870
요약하자면, 이게 주요 문제들입니다.

952
00:43:02,870 --> 00:43:06,110
그리고 모멘텀을 추가하는 꽤 멋진

953
00:43:06,110 --> 00:43:07,370
방법이 있습니다.

954
00:43:07,370 --> 00:43:09,560
이것은 마치 공이 언덕을 굴러 내려가면서

955
00:43:09,560 --> 00:43:11,400
모멘텀을 얻는 것과 같은 방식으로

956
00:43:11,400 --> 00:43:12,400
생각할 수 있습니다.

957
00:43:12,400 --> 00:43:13,880
물리적 특성 측면에서

958
00:43:13,880 --> 00:43:15,922
모델링된 방식과 실제로 매우 유사합니다.

959
00:43:15,922 --> 00:43:17,840
그래서 적어도 직관을 얻는 데

960
00:43:17,840 --> 00:43:19,120
좋은 방법입니다.

961
00:43:19,120 --> 00:43:21,708
로컬 미니멈이 있어도 충분한

962
00:43:21,708 --> 00:43:24,000
속도로 굴러 내려가면

963
00:43:24,000 --> 00:43:25,417
빠져나올 수 있다고

964
00:43:25,417 --> 00:43:27,120
상상할 수 있습니다.

965
00:43:27,120 --> 00:43:31,077
새들 포인트나 평평한 지점이 있어도 모델이 언덕 전체를

966
00:43:31,077 --> 00:43:33,160
굴러 내려왔기 때문에 여기서 더

967
00:43:33,160 --> 00:43:34,618
이상 멈추지 않습니다.

968
00:43:34,618 --> 00:43:36,160
계속 진행할 것입니다.

969
00:43:36,160 --> 00:43:40,860
조건이 좋지 않은 값이 있어도 약간의

970
00:43:40,860 --> 00:43:44,060
진동은 있을 수 있습니다.

971
00:43:44,060 --> 00:43:47,720
하지만 좋은 점은 여러 단계가 계속 같은 방향으로

972
00:43:47,720 --> 00:43:49,680
진행되면서 이 방향으로

973
00:43:49,680 --> 00:43:51,763
속도가 누적된다는 겁니다.

974
00:43:51,763 --> 00:43:54,460
그래서 중심 쪽으로 점점 더 빠르게 이동하게 됩니다.

975
00:43:54,460 --> 00:43:56,640
이 문제에도 도움이 됩니다.

976
00:43:56,640 --> 00:43:58,800
마지막으로, 그래디언트의

977
00:43:58,800 --> 00:44:00,880
노이즈를 평균화하는 데도 도움이

978
00:44:00,880 --> 00:44:04,880
됩니다. 왜냐하면 모두 공통된 방향, 즉 이 미니멈

979
00:44:04,880 --> 00:44:07,460
쪽을 향하고 있기 때문입니다.

980
00:44:07,460 --> 00:44:12,310
그래서 모멘텀을 계산할 때 스스로

981
00:44:12,310 --> 00:44:18,650
누적되며, 노이즈가 모두 공유하는 방향을

982
00:44:18,650 --> 00:44:20,730
반영하기

983
00:44:20,730 --> 00:44:24,842
때문에 더 빠르게 수렴합니다.

984
00:44:24,842 --> 00:44:26,550
이제 실제로 어떻게 하는지

985
00:44:26,550 --> 00:44:28,570
보여드리겠습니다만, 이것이

986
00:44:28,570 --> 00:44:31,450
모멘텀 작동 원리에 대한 일반적인 직관입니다.

987
00:44:31,450 --> 00:44:33,650
여기 SGD가 있습니다.

988
00:44:33,650 --> 00:44:37,170
미니 배치 x가 있습니다.

989
00:44:37,170 --> 00:44:39,570
우리는 기울기, 즉 dx를 계산하고 있습니다.

990
00:44:39,570 --> 00:44:42,190
학습률 또는 스텝 크기가 있는데, 이것을 곱한 후에 음수로

991
00:44:42,190 --> 00:44:43,170
바꾸는 이유는 경사를

992
00:44:43,170 --> 00:44:44,410
내려가야 하기 때문입니다.

993
00:44:44,410 --> 00:44:47,890
이것이 새로운 x를 주는데, 이것이 바로 SGD입니다.

994
00:44:47,890 --> 00:44:52,550
모멘텀을 사용할 때는 이제 이 속도(velocity) 항으로 업데이트합니다.

995
00:44:52,550 --> 00:44:55,310
즉, 특정 지점의 기울기로 업데이트하는

996
00:44:55,310 --> 00:44:57,330
대신 속도로 업데이트하는 거죠.

997
00:44:57,330 --> 00:44:59,050
주어진 시간 단계에서

998
00:44:59,050 --> 00:45:03,870
속도는 이전 속도에 현재 기울기를 더한 값입니다.

999
00:45:03,870 --> 00:45:05,230
이렇게 계산하는 겁니다.

1000
00:45:05,230 --> 00:45:06,647
그리고 rho 값이 있는데,

1001
00:45:06,647 --> 00:45:10,560
이것이 모멘텀, 즉 얼마나 많은 모멘텀을 가질지 결정합니다.

1002
00:45:10,560 --> 00:45:13,020
만약 이 값이 매우 높으면, 새로운

1003
00:45:13,020 --> 00:45:16,340
속도는 이전 시간 단계의 속도에 더 의존하게 됩니다.

1004
00:45:16,340 --> 00:45:21,980
여기 모멘텀 항에서 지난 기울기들의 가중 평균을 구하는데,

1005
00:45:21,980 --> 00:45:24,500
이것이 과거와 현재를 얼마나

1006
00:45:24,500 --> 00:45:26,170
가중할지 결정합니다.

1007
00:45:26,170 --> 00:45:27,420
그래서 이제 우리는 이 값으로 업데이트합니다.

1008
00:45:27,420 --> 00:45:29,680
그리고 여전히 스텝 크기인 알파(alpha)가 있습니다.

1009
00:45:29,680 --> 00:45:31,600
사실 매우 간단한 변화입니다.

1010
00:45:31,600 --> 00:45:34,600
현재 속도와 기울기의

1011
00:45:34,600 --> 00:45:37,800
함수인 속도를 이제 계산하는

1012
00:45:37,800 --> 00:45:39,580
것뿐입니다.

1013
00:45:39,580 --> 00:45:42,340
여기서 질문을 받겠습니다.

1014
00:45:42,340 --> 00:45:44,640
이것이 모멘텀에 대한 설명입니다.

1015
00:45:44,640 --> 00:45:47,340
그리고 아마도 이 문제들을 어떻게 해결하는지 간단히

1016
00:45:47,340 --> 00:45:49,320
다시 정리할 수 있을 것 같습니다.

1017
00:45:49,320 --> 00:45:51,940
이제 과거의 그래디언트 단계들에

1018
00:45:51,940 --> 00:45:55,060
모멘텀을 더하면, 이 방향을 계속

1019
00:45:55,060 --> 00:45:57,640
따라가게 되는 것을 볼 수 있습니다.

1020
00:45:57,640 --> 00:46:00,040
그리고 rho 값에 따라

1021
00:46:00,040 --> 00:46:03,660
모멘텀이 매우 크면, 여기 국소 최소값의 큰

1022
00:46:03,660 --> 00:46:05,740
고비도 넘을 수 있습니다.

1023
00:46:05,740 --> 00:46:07,840
또한 이 방법은 새들 포인트에서 매우

1024
00:46:07,840 --> 00:46:10,400
효과적인데, 이전에 가던 방향을 상당 시간

1025
00:46:10,400 --> 00:46:12,920
계속 유지하기 때문입니다. 그리고 조건이

1026
00:46:12,920 --> 00:46:14,810
좋지 않은 경우에도 마찬가지입니다.

1027
00:46:14,810 --> 00:46:17,520
만약 매 단계마다 누적해서

1028
00:46:17,520 --> 00:46:21,120
오른쪽으로 간다면, 모멘텀도 일관되게 쌓일

1029
00:46:21,120 --> 00:46:21,780
것입니다.

1030
00:46:21,780 --> 00:46:27,280
그리고 여기서 진동이 심하면, 값들이 서로 상쇄되어

1031
00:46:27,280 --> 00:46:29,800
방향으로의 이동이

1032
00:46:29,800 --> 00:46:30,840
줄어듭니다.

1033
00:46:30,840 --> 00:46:34,228
현재 방향과 속도는 반대 방향을

1034
00:46:34,228 --> 00:46:36,020
가리키기 때문에

1035
00:46:36,020 --> 00:46:37,850
최소화됩니다.

1036
00:46:37,850 --> 00:46:39,600
그렇다면 새들 위를 계속

1037
00:46:39,600 --> 00:46:41,540
굴러간다면 어떻게 될까요?

1038
00:46:41,540 --> 00:46:43,540
사실 실무에서는 매우 드문 일이라고 생각합니다.

1039
00:46:43,540 --> 00:46:45,800
하지만 그런 경우라면,

1040
00:46:45,800 --> 00:46:48,000
새들에 갇히게 될 겁니다.

1041
00:46:48,000 --> 00:46:51,000
초기 조건, 즉 어디서 시작하느냐가 매우

1042
00:46:51,000 --> 00:46:53,180
불운한 경우라고 생각합니다.

1043
00:46:53,180 --> 00:46:55,300
그래서 가끔 그런 일이 일어날 수도 있지만,

1044
00:46:55,300 --> 00:46:56,690
가능성은 매우 낮습니다.

1045
00:46:56,690 --> 00:46:58,440
그리고 실제로 사람들이 한

1046
00:46:58,440 --> 00:47:01,970
번의 모델 학습만 하지 않는 이유이기도 합니다.

1047
00:47:01,970 --> 00:47:04,470
보통은 여러 번, 다른 랜덤 시드로 여러 번

1048
00:47:04,470 --> 00:47:06,990
실행해서 그런 일이 발생할 가능성에 대비합니다.

1049
00:47:06,990 --> 00:47:09,532
또한 확률적 경사 하강법을 사용하면, 적어도

1050
00:47:09,532 --> 00:47:12,198
약간의 노이즈가 있어서 새들에서 바로 왔다

1051
00:47:12,198 --> 00:47:14,610
갔다 하는 상황에서 벗어날 가능성이 훨씬

1052
00:47:14,610 --> 00:47:17,090
높습니다. 그래서 기본적으로는 무작위성

1053
00:47:17,090 --> 00:47:19,470
때문에 그런 일이 거의 일어나지 않습니다.

1054
00:47:19,470 --> 00:47:22,550
하지만 가설적으로는 그런 일이 발생할 수도 있다고 생각합니다.

1055
00:47:22,550 --> 00:47:23,490
네.

1056
00:47:23,490 --> 00:47:26,290
그래서 질문은 왜 saddle point 문제가 SGD에서만 발생하고
일반적인

1057
00:47:26,290 --> 00:47:27,745
최적화에서는 문제가 되지 않느냐는 거죠?

1058
00:47:27,745 --> 00:47:29,870
전체 데이터 세트에서도 문제가 될 수 있습니다.

1059
00:47:29,870 --> 00:47:32,860
오히려 전체 데이터 세트에서 더 흔할 수도 있습니다.

1060
00:47:32,860 --> 00:47:35,810
그래서 이것은 SGD가 직면하는 문제이지만,

1061
00:47:35,810 --> 00:47:38,090
추가적인 장치 없이 단순히

1062
00:47:38,090 --> 00:47:41,410
gradient descent에 의존하는 다른

1063
00:47:41,410 --> 00:47:43,090
최적화 알고리즘들도 같은

1064
00:47:43,090 --> 00:47:44,490
문제를 겪을 겁니다.

1065
00:47:44,490 --> 00:47:45,730
네.

1066
00:47:45,730 --> 00:47:46,230
네.

1067
00:47:46,230 --> 00:47:49,450
그래서 질문은, momentum을 추가하면 수렴이 더

1068
00:47:49,450 --> 00:47:51,770
어려워지는가, 즉 overshoot해서 다시

1069
00:47:51,770 --> 00:47:53,410
돌아와야 하는가 하는 거죠?

1070
00:47:53,410 --> 00:47:54,830
짧게 말하면 네, 그렇습니다.

1071
00:47:54,830 --> 00:47:57,890
수렴하는 데 도움이 되지 않을 수

1072
00:47:57,890 --> 00:47:58,810
있지만,

1073
00:47:58,810 --> 00:48:02,010
평균적으로 더 좋은 최소점을 찾는 데는

1074
00:48:02,010 --> 00:48:03,320
도움이 됩니다.

1075
00:48:03,320 --> 00:48:06,580
그래서 momentum이 없으면 국소 최소점에 갇혀서 빨리 수렴할 수
있지만, momentum이

1076
00:48:06,580 --> 00:48:08,892
있으면 더 천천히 수렴하더라도 더 좋은 지점에 도달할 수 있습니다.

1077
00:48:08,892 --> 00:48:11,100
만약 momentum이 없으면 여기서 그냥 수렴할 거고, momentum이

1078
00:48:11,100 --> 00:48:12,640
있으면 overshoot가 발생하는 거죠.

1079
00:48:12,640 --> 00:48:16,040
이런 내용들은 경험적으로 많이 증명되었고,

1080
00:48:16,040 --> 00:48:16,540
특정

1081
00:48:16,540 --> 00:48:18,460
신경망 종류에서 그런 경향이

1082
00:48:18,460 --> 00:48:19,280
있습니다.

1083
00:48:19,280 --> 00:48:21,440
momentum은 학습에 도움이 됩니다.

1084
00:48:21,440 --> 00:48:25,320
이게 momentum을 선호하는 직관적인 이유입니다.

1085
00:48:25,320 --> 00:48:30,038
솔직히 말하면, 사람들은 가장 잘 작동하는 방법을 사용합니다.

1086
00:48:30,038 --> 00:48:31,580
어떤 경우에는

1087
00:48:31,580 --> 00:48:34,180
momentum 없는 SGD가 특정 모델에서

1088
00:48:34,180 --> 00:48:36,580
더 좋은 성능을 보이기도 했습니다.

1089
00:48:36,580 --> 00:48:39,205
그래서 momentum이 더 잘 작동할 수 있는 이유에 대한 직관을 말씀드린
거고,

1090
00:48:39,205 --> 00:48:40,580
실제로는 여러 방법을

1091
00:48:40,580 --> 00:48:43,400
시도해 보고 가장 잘 맞는 걸 선택합니다.

1092
00:48:43,400 --> 00:48:46,200
지금 말씀드리는 건 사람들이 가장 흔히 시도하는 방법들입니다.

1093
00:48:46,200 --> 00:48:46,722
네.

1094
00:48:46,722 --> 00:48:47,680
맞아요.

1095
00:48:47,680 --> 00:48:49,680
수렴에 방해가 될 수도 있습니다.

1096
00:48:54,270 --> 00:48:55,000
좋습니다.

1097
00:48:55,000 --> 00:48:56,740
그럼 계속하겠습니다.

1098
00:48:56,740 --> 00:49:01,580
네, 이 부분은 이미 다뤘다고 생각합니다.

1099
00:49:01,580 --> 00:49:03,320
그리고 한 가지 더 말씀드리고 싶은

1100
00:49:03,320 --> 00:49:05,765
건, 이걸 표현하는 방법이 여러 가지 있다는 겁니다.

1101
00:49:05,765 --> 00:49:07,140
이 방정식들은

1102
00:49:07,140 --> 00:49:09,660
동일하지만, 구현에 따라 다르게 쓰이는

1103
00:49:09,660 --> 00:49:11,220
경우가 있습니다.

1104
00:49:11,220 --> 00:49:13,720
하지만 결국 같은 일을 하는 거죠.

1105
00:49:13,720 --> 00:49:16,780
시간 관계상 왜 동일한지 설명은 생략하겠습니다만,

1106
00:49:16,780 --> 00:49:18,280
슬라이드에서 직접

1107
00:49:18,280 --> 00:49:20,880
확인해 보시면 본질적으로 같은

1108
00:49:20,880 --> 00:49:22,920
공식임을 증명할 수 있습니다.

1109
00:49:22,920 --> 00:49:24,200
네.

1110
00:49:24,200 --> 00:49:26,040
다음으로 이야기할 것은

1111
00:49:26,040 --> 00:49:27,900
다른 옵티마이저입니다.

1112
00:49:27,900 --> 00:49:29,760
그래서 우리는 momentum에 대해 이야기했습니다.

1113
00:49:29,760 --> 00:49:32,400
이제 RMSProp이라는 것에 대해 이야기할 겁니다.

1114
00:49:32,400 --> 00:49:37,760
RMSProp은 2012년에 나온 다소 오래된 방법인데, Geoffrey

1115
00:49:37,760 --> 00:49:40,880
Hinton 그룹에서 나왔습니다.

1116
00:49:40,880 --> 00:49:44,040
기본 아이디어는 momentum이

1117
00:49:44,040 --> 00:49:48,160
포착하는 이 달리는 속도(velocity)

1118
00:49:48,160 --> 00:49:51,600
대신에, 기울기를 원소별로

1119
00:49:51,600 --> 00:49:54,060
스케일링하는 것입니다.

1120
00:49:54,060 --> 00:50:00,530
이렇게 하는 방법은 기울기 제곱 항을 사용하는 것입니다.

1121
00:50:00,530 --> 00:50:04,410
여기서 감쇠율(decay rate)은 이전에 설명한 momentum

1122
00:50:04,410 --> 00:50:07,630
항과 매우 비슷하지만, 이제는 제곱된 기울기에 적용됩니다.

1123
00:50:07,630 --> 00:50:10,850
그래서 이전 항인 기울기 제곱을 취해서 달리는

1124
00:50:10,850 --> 00:50:12,030
평균을 구합니다.

1125
00:50:12,030 --> 00:50:13,670
그리고 1 빼기 곱하기를 합니다.

1126
00:50:13,670 --> 00:50:16,610
여기서는 문자 그대로 기울기 제곱입니다.

1127
00:50:16,610 --> 00:50:19,790
이것은 제곱된 기울기의 달리는 평균입니다.

1128
00:50:19,790 --> 00:50:22,390
큰 값은 훨씬 커지고, 작은

1129
00:50:22,390 --> 00:50:24,270
값은 훨씬 작아집니다.

1130
00:50:24,270 --> 00:50:26,810
특정 값에서 일관되게 큰

1131
00:50:26,810 --> 00:50:28,910
기울기가 있다면, 달리는

1132
00:50:28,910 --> 00:50:32,090
평균이 계속되면서 매우 커집니다.

1133
00:50:32,090 --> 00:50:33,770
그리고 실제로 여기서 나눗셈을 합니다.

1134
00:50:33,770 --> 00:50:36,950
업데이트 단계에서 그것의 제곱근으로 나누는 겁니다.

1135
00:50:36,950 --> 00:50:39,863
기본 아이디어는 이제 우리가 실제로

1136
00:50:39,863 --> 00:50:42,030
스텝을 밟는 방향을 바꾸는

1137
00:50:42,030 --> 00:50:44,490
것에 관한 질문이 있었는데,

1138
00:50:44,490 --> 00:50:46,550
이것이 바로 여러분이 할 수 있는 종류의 작업입니다.

1139
00:50:46,550 --> 00:50:48,690
그리고 이것이 바로 제곱된 그래디언트

1140
00:50:48,690 --> 00:50:50,290
항으로 나누는 작업입니다.

1141
00:50:50,290 --> 00:50:55,290
그래디언트가 매우 큰 값들, 즉 도함수가 매우

1142
00:50:55,290 --> 00:50:59,722
큰 W 값들에 대해서는 더 큰 값으로

1143
00:50:59,722 --> 00:51:01,360
나누게 됩니다.

1144
00:51:01,360 --> 00:51:03,243
그래서 그 방향으로 한 걸음 덜 나아가게 되는 거죠.

1145
00:51:03,243 --> 00:51:05,660
그리고 더 평평한 영역에서는 더 작은

1146
00:51:05,660 --> 00:51:08,340
값으로 나누기 때문에 더 멀리 나아가게 됩니다.

1147
00:51:08,340 --> 00:51:10,608
이것이 바로 기본적인 직관입니다.

1148
00:51:10,608 --> 00:51:12,900
그리고 이것은 누군가가 이전에 질문했던, 우리가

1149
00:51:12,900 --> 00:51:14,860
나아가는 방향을 바꿀 수 있느냐는 질문에

1150
00:51:14,860 --> 00:51:15,760
아주 잘 답해줍니다.

1151
00:51:15,760 --> 00:51:17,040
바로 이것이 여기서 하는 일입니다.

1152
00:51:17,040 --> 00:51:18,457
그래서 여전히

1153
00:51:18,457 --> 00:51:20,660
학습률이 있지만, 누적된 제곱

1154
00:51:20,660 --> 00:51:23,940
그래디언트의 제곱근으로 나누기 때문에

1155
00:51:23,940 --> 00:51:28,020
손실 함수의 평평한 영역에서는 더 큰 걸음, 가파른

1156
00:51:28,020 --> 00:51:31,340
영역에서는 더 짧은 걸음을 내딛게 됩니다.

1157
00:51:31,340 --> 00:51:33,680
누군가 설명할 수 있나요? 제가 간단히

1158
00:51:33,680 --> 00:51:37,200
요약했는데, 이 코드의 특정 줄에서 무슨 일이 일어나는지요?

1159
00:51:39,820 --> 00:51:42,480
그래디언트 스텝 방향에 무슨 일이 일어나나요?

1160
00:51:42,480 --> 00:51:44,220
어떻게 변하나요?

1161
00:51:44,220 --> 00:51:45,785
우리는 현재 그래디언트와

1162
00:51:45,785 --> 00:51:47,660
과거 그래디언트에

1163
00:51:47,660 --> 00:51:50,340
의존하는 이 값으로 나누고 있습니다.

1164
00:51:50,340 --> 00:51:52,060
이 값들이 매우 클

1165
00:51:52,060 --> 00:51:54,440
때, 이들은 벡터 연산입니다.

1166
00:51:54,440 --> 00:51:57,520
여기 도함수 집합이 있고,

1167
00:51:57,520 --> 00:52:00,470
우리는 또 다른 제곱된 그래디언트 값

1168
00:52:00,470 --> 00:52:02,810
집합으로 요소별로 나누고 있습니다.

1169
00:52:02,810 --> 00:52:06,170
그래서 분모가 매우 크면 분모가 매우 커지고, 그

1170
00:52:06,170 --> 00:52:07,830
방향으로의 스텝이 사실상

1171
00:52:07,830 --> 00:52:10,270
작아집니다. 큰 값으로 나누기 때문입니다.

1172
00:52:10,270 --> 00:52:12,030
그리고 값이 매우 작으면 스텝이

1173
00:52:12,030 --> 00:52:15,570
훨씬 커집니다. 왜냐하면 그래디언트 제곱 항이 작기 때문입니다.

1174
00:52:15,570 --> 00:52:18,470
그래서 분모에 있고, 효과적인

1175
00:52:18,470 --> 00:52:21,310
스텝 크기를 증가시키는 겁니다.

1176
00:52:21,310 --> 00:52:21,930
네, 맞습니다.

1177
00:52:21,930 --> 00:52:25,390
그래서 특히 여기처럼 아주 좁은

1178
00:52:25,390 --> 00:52:28,910
계곡이 있을 때, 더 평평한 방향으로

1179
00:52:28,910 --> 00:52:33,390
더 많이 움직이고 싶을 때 사용하는 겁니다.

1180
00:52:33,390 --> 00:52:35,910
네, 질문은 이 맥락에서 작은 그래디언트가 무엇을

1181
00:52:35,910 --> 00:52:37,210
의미하는가 하는 거죠?

1182
00:52:37,210 --> 00:52:39,990
그리고 이것이 가파른 방향으로는 덜

1183
00:52:39,990 --> 00:52:43,310
움직이고 평평한 방향으로는 더 움직이도록 어떻게

1184
00:52:43,310 --> 00:52:45,430
도와주는가 하는 겁니다.

1185
00:52:45,430 --> 00:52:46,050
네, 네.

1186
00:52:46,050 --> 00:52:48,110
사실 이게 세 가지 다른 접근법을

1187
00:52:48,110 --> 00:52:50,810
비교하는 훌륭한 시각 자료인 것 같습니다.

1188
00:52:50,810 --> 00:52:52,950
그래서 모멘텀을 사용하면, 아까 질문이

1189
00:52:52,950 --> 00:52:55,730
있었던 것처럼 과도하게 움직이는 것을 볼 수 있습니다.

1190
00:52:55,730 --> 00:52:57,730
하지만 다시 돌아옵니다.

1191
00:52:57,730 --> 00:53:01,312
SGD는 항상 고정된 방향으로 움직이기 때문에

1192
00:53:01,312 --> 00:53:02,270
더 느립니다.

1193
00:53:02,270 --> 00:53:04,450
그리고 방금 언급한 RMSProp이 있습니다.

1194
00:53:04,450 --> 00:53:07,890
RMSProp이 작동하는 방식은, 제가

1195
00:53:07,890 --> 00:53:10,170
마우스를 움직이는 방향의

1196
00:53:10,170 --> 00:53:15,390
그래디언트가 더 크기 때문에, 그래디언트 제곱 항이 더

1197
00:53:15,390 --> 00:53:18,950
커서 그 방향으로 덜 움직인다는 겁니다.

1198
00:53:18,950 --> 00:53:21,970
그래서 실제로 여기서 중심 쪽으로 빠르게 방향을 틀기

1199
00:53:21,970 --> 00:53:24,950
시작하는데, 그 지점이 더 평평한 지형이고 그

1200
00:53:24,950 --> 00:53:26,910
방향으로 더 많이 이동하고 있습니다.

1201
00:53:26,910 --> 00:53:29,835
즉, 가파른 방향으로는 덜 가고 평평한 방향으로는

1202
00:53:29,835 --> 00:53:31,210
더 가도록 방향을 실제로

1203
00:53:31,210 --> 00:53:32,550
바꾸고 있는 겁니다.

1204
00:53:32,550 --> 00:53:33,050
이렇게 세

1205
00:53:33,050 --> 00:53:34,083
가지가 있습니다.

1206
00:53:34,083 --> 00:53:35,750
그리고 하나 더 논의할

1207
00:53:35,750 --> 00:53:38,970
것이 있는데, 현대 딥러닝에서 가장

1208
00:53:38,970 --> 00:53:41,130
인기 있는 옵티마이저입니다.

1209
00:53:41,130 --> 00:53:46,090
이것은 SGD 모멘텀과 RMSProp의 조합일 뿐입니다.

1210
00:53:46,090 --> 00:53:49,250
여기 거의 Adam 옵티마이저와 같은 것이 있는데,

1211
00:53:49,250 --> 00:53:51,923
딥러닝에서 가장 인기 있는 옵티마이저입니다.

1212
00:53:51,923 --> 00:53:54,090
그리고 이제 이를 이해할 수 있는

1213
00:53:54,090 --> 00:53:56,340
모든 사전 지식을 갖추셨습니다.

1214
00:53:56,340 --> 00:53:57,560
한번 살펴보겠습니다.

1215
00:53:57,560 --> 00:53:59,660
빨간색으로 표시된 첫

1216
00:53:59,660 --> 00:54:03,060
번째 항은 기본적으로 이전에

1217
00:54:03,060 --> 00:54:09,120
설명한 모멘텀으로, beta 1이 모멘텀 항이고 여기서

1218
00:54:09,120 --> 00:54:11,360
velocity가 있습니다.

1219
00:54:11,360 --> 00:54:14,060
그리고 우리는 이동 평균을 취하고 있습니다.

1220
00:54:14,060 --> 00:54:17,140
두 번째 모멘트는 RMSProp의

1221
00:54:17,140 --> 00:54:20,100
그래디언트 제곱 항입니다.

1222
00:54:20,100 --> 00:54:21,860
여기서도 학습률을 스텝 크기

1223
00:54:21,860 --> 00:54:24,660
대신 velocity에 곱하는 같은 작업을

1224
00:54:24,660 --> 00:54:25,682
하고 있습니다.

1225
00:54:25,682 --> 00:54:27,140
하지만 이제도 여전히

1226
00:54:27,140 --> 00:54:29,740
제곱근을 취하는 작업을 하고 있습니다.

1227
00:54:29,740 --> 00:54:31,280
그리고 이것이 두 번째 모멘트입니다.

1228
00:54:31,280 --> 00:54:33,363
첫 번째 모멘트와 두 번째

1229
00:54:33,363 --> 00:54:35,860
모멘트를 사용하는 이유는 물리학과 역학과의

1230
00:54:35,860 --> 00:54:38,580
연관성 때문인데, 기본적으로는 앞서

1231
00:54:38,580 --> 00:54:41,820
설명한 두 가지를 결합한 것으로, 평탄한 방향에서는

1232
00:54:41,820 --> 00:54:43,700
가속을 하고 가파른

1233
00:54:43,700 --> 00:54:45,260
방향에서는 감쇠하며, 모멘텀과

1234
00:54:45,260 --> 00:54:47,680
속도의 개념을 추가하는 것입니다.

1235
00:54:47,680 --> 00:54:50,220
그래서 같은 방향으로 계속

1236
00:54:50,220 --> 00:54:52,660
움직이면 점차 속도가 붙습니다.

1237
00:54:52,660 --> 00:54:55,150
지금 적힌 대로라면, 실제로 첫

1238
00:54:55,150 --> 00:54:59,110
번째 시간 단계에서 문제가 발생할 수 있습니다.

1239
00:54:59,110 --> 00:55:03,070
왜 그런지 조금 불분명할 수도 있는데, 누군가

1240
00:55:03,070 --> 00:55:05,450
추측해 보길 기다리겠습니다.

1241
00:55:05,450 --> 00:55:08,510
한 가지 주목할 점은 이 베타들, beta1과

1242
00:55:08,510 --> 00:55:13,230
beta2가 보통 1에 아주 가깝게 초기화된다는 겁니다, 예를 들어 0.9,
0.

1243
00:55:13,230 --> 00:55:16,430
999이고, 이 두 값도 0으로 초기화된다는 점입니다.

1244
00:55:16,430 --> 00:55:19,030
그래서 첫 번째 시간 단계에서 Adam의

1245
00:55:19,030 --> 00:55:20,750
이 공식을 그대로

1246
00:55:20,750 --> 00:55:24,430
사용하면 원하지 않는 동작이 발생할 수 있습니다.

1247
00:55:24,430 --> 00:55:28,350
또 다른 문제 중 하나는 두 번째 모멘트 계산과

1248
00:55:28,350 --> 00:55:29,470
관련이 있습니다.

1249
00:55:29,470 --> 00:55:32,010
여기가 주요 문제입니다.

1250
00:55:32,010 --> 00:55:33,830
두 번째 모멘트를

1251
00:55:33,830 --> 00:55:38,110
계산하고 다음 줄에서 사용하면 문제가 발생합니다.

1252
00:55:38,110 --> 00:55:39,922
네, 분모가 기본적으로 0이라는 겁니다.

1253
00:55:39,922 --> 00:55:41,130
네, 그게 바로 문제의 핵심입니다.

1254
00:55:41,130 --> 00:55:44,190
처음에 0에서 시작하니까 이 항이 0이 됩니다.

1255
00:55:44,190 --> 00:55:46,670
베타 값이 매우 큽니다.

1256
00:55:46,670 --> 00:55:48,730
그래서 이 값은 매우 작아집니다.

1257
00:55:48,730 --> 00:55:51,190
그리고 첫 번째 단계에서 그래디언트가 크지

1258
00:55:51,190 --> 00:55:54,300
않으면 이 전체 항이 거의 0에 가까워질 수 있습니다.

1259
00:55:54,300 --> 00:55:56,480
이제 거의 0에 가까운 값으로 나누게

1260
00:55:56,480 --> 00:55:59,000
되니, 그래디언트가 작았음에도 불구하고 매우

1261
00:55:59,000 --> 00:56:00,760
큰 초기 스텝이 만들어집니다.

1262
00:56:00,760 --> 00:56:03,020
이건 아마 우리가 원하지 않는 상황일 겁니다.

1263
00:56:03,020 --> 00:56:05,800
그래서 Adam이 마지막으로 하는 것은 여기

1264
00:56:05,800 --> 00:56:08,080
편향 보정(bias terms)을 추가하는

1265
00:56:08,080 --> 00:56:10,560
건데, 이는 훈련 시간 단계에 의존하는

1266
00:56:10,560 --> 00:56:12,780
이 문제를 해결하기 위한 것입니다.

1267
00:56:12,780 --> 00:56:15,168
이 부분은 숙제에서도 다룰

1268
00:56:15,168 --> 00:56:16,460
거라고 생각합니다.

1269
00:56:16,460 --> 00:56:18,880
저는 단순 구현이 왜 작동하지 않는지,

1270
00:56:18,880 --> 00:56:21,980
즉 너무 큰 초기 스텝이 생기는 이유에 대한

1271
00:56:21,980 --> 00:56:24,292
기본 직관만 설명하고 싶었습니다.

1272
00:56:24,292 --> 00:56:26,500
그리고 숙제에서 이것을 구현해 보실 겁니다.

1273
00:56:26,500 --> 00:56:28,620
그리고 시간 단계가 어떻게 사용되는지 보실 수 있습니다.

1274
00:56:28,620 --> 00:56:31,080
하지만 기본 아이디어는 아주 큰 초기 단계를

1275
00:56:31,080 --> 00:56:32,220
보정하는 것입니다.

1276
00:56:32,220 --> 00:56:36,040
시간 단계가 커질수록 이런 바이어스

1277
00:56:36,040 --> 00:56:38,760
항은 덜 필요해집니다.

1278
00:56:38,760 --> 00:56:40,240
좋습니다.

1279
00:56:40,240 --> 00:56:43,640
이것들은 보통 사람들이 사용하는 좋은 기본값들입니다.

1280
00:56:43,640 --> 00:56:47,080
Adam으로 모델을 훈련할 때 이 값을 사용하면, 잘 될

1281
00:56:47,080 --> 00:56:49,068
수도 있고 아닐 수도 있습니다.

1282
00:56:49,068 --> 00:56:50,360
하지만 좋은 출발점입니다.

1283
00:56:50,360 --> 00:56:54,297
그리고 남은 슬라이드에서 학습률이 적절한지

1284
00:56:54,297 --> 00:56:56,880
어떻게 아는지 이야기할 겁니다.

1285
00:56:56,880 --> 00:56:59,200
다른 값들이 적절한지 어떻게 아는지도요.

1286
00:56:59,200 --> 00:57:02,600
시간 관계상 조금 속도를 내겠습니다.

1287
00:57:02,600 --> 00:57:04,340
하지만 기본 아이디어는 다양한

1288
00:57:04,340 --> 00:57:07,262
옵티마이저들이 수렴하는 모습을 볼 수 있다는 겁니다.

1289
00:57:07,262 --> 00:57:08,720
각각 다른 특성을 가지고 있습니다.

1290
00:57:08,720 --> 00:57:12,260
Adam이 RMSProp과 모멘텀을 가진 SGD의 조합이라는 것을

1291
00:57:12,260 --> 00:57:14,860
시각적으로 볼 수 있는데, 두 가지 특성을

1292
00:57:14,860 --> 00:57:16,800
모두 가진다는 점이 매우 흥미롭습니다.

1293
00:57:16,800 --> 00:57:20,220
이것은 우리의 직관과도 일치합니다.

1294
00:57:20,220 --> 00:57:22,500
Adam과 관련된 마지막 주제는

1295
00:57:22,500 --> 00:57:25,660
정규화가 옵티마이저와 어떻게 상호작용하는지

1296
00:57:25,660 --> 00:57:27,080
살펴보는 것입니다.

1297
00:57:27,080 --> 00:57:30,320
예를 들어, L2 정규화가 있다면, 이것이

1298
00:57:30,320 --> 00:57:33,240
옵티마이저 작동 방식에 어떤 영향을 미칠까요?

1299
00:57:33,240 --> 00:57:35,420
사실 답은 바로 명확하지

1300
00:57:35,420 --> 00:57:36,260
않습니다.

1301
00:57:36,260 --> 00:57:37,760
그리고 여러 가지 방법으로 할 수 있습니다.

1302
00:57:37,760 --> 00:57:41,340
기본 Adam에서는 그라디언트를 계산할 때 L2를

1303
00:57:41,340 --> 00:57:42,480
함께 계산합니다.

1304
00:57:42,480 --> 00:57:45,420
그래서 그라디언트를 보면, 손실 부분이

1305
00:57:45,420 --> 00:57:48,380
있었죠—데이터 손실 부분과 정규화 손실

1306
00:57:48,380 --> 00:57:49,700
부분이 있었습니다.

1307
00:57:49,700 --> 00:57:51,430
Adam은 그라디언트를 계산할

1308
00:57:51,430 --> 00:57:53,350
때 이 두 가지를 모두 사용합니다.

1309
00:57:53,350 --> 00:57:56,550
하지만 AdamW는 기본적으로 모멘트

1310
00:57:56,550 --> 00:57:59,230
계산과 모든 단계에서 데이터

1311
00:57:59,230 --> 00:58:02,350
손실만 보고, 정규화 항은

1312
00:58:02,350 --> 00:58:03,830
마지막에 더합니다.

1313
00:58:03,830 --> 00:58:05,892
즉, 제가 말씀드리고 싶은

1314
00:58:05,892 --> 00:58:07,350
것은 정규화를 옵티마이저에

1315
00:58:07,350 --> 00:58:10,390
포함하는 방법에 유연성이 있다는 겁니다.

1316
00:58:10,390 --> 00:58:12,110
Weight decay는

1317
00:58:12,110 --> 00:58:14,770
보통 마지막에 L2

1318
00:58:14,770 --> 00:58:17,670
정규화를 더하는 방식이고, 실제로

1319
00:58:17,670 --> 00:58:19,470
속도나 모멘텀 계산에는

1320
00:58:19,470 --> 00:58:21,158
포함하지 않습니다.

1321
00:58:21,158 --> 00:58:22,450
이것이 주요 차이점입니다.

1322
00:58:22,450 --> 00:58:25,570
그리고 많은 경우에 AdamW가 조금 더 잘 작동하는

1323
00:58:25,570 --> 00:58:26,770
경우가 있습니다.

1324
00:58:26,770 --> 00:58:31,550
Meta의 Llama 시리즈도 모두 AdamW를 사용하는데, 아마도

1325
00:58:31,550 --> 00:58:34,630
조금 더 성능이 좋아서 그런 것 같습니다.

1326
00:58:34,630 --> 00:58:36,210
우리는 하나의 함수인 optimizer를 가지고 있습니다.

1327
00:58:36,210 --> 00:58:37,950
왜 두 개로 나누는 걸까요?

1328
00:58:37,950 --> 00:58:42,510
네, 하나로 합치면 그게 Adam입니다.

1329
00:58:42,510 --> 00:58:45,890
AdamW는 특별히 두 개로 분리하는 거죠.

1330
00:58:45,890 --> 00:58:48,870
그렇게 하는 이유는 속도와

1331
00:58:48,870 --> 00:58:52,080
모멘텀이 가중치의 함수가 아니라

1332
00:58:52,080 --> 00:58:53,880
손실의 함수가 되길

1333
00:58:53,880 --> 00:58:55,620
원하기 때문입니다.

1334
00:58:55,620 --> 00:58:58,200
즉, 실제 가중치 값과 독립적으로

1335
00:58:58,200 --> 00:59:00,832
손실 지형을 탐색하고 싶을

1336
00:59:00,832 --> 00:59:02,540
때 분리하는 겁니다.

1337
00:59:02,540 --> 00:59:04,580
하지만 정규화 항은 여전히 필요하죠.

1338
00:59:04,580 --> 00:59:06,663
단, 모멘트 계산에 방해가 되지 않게 하려는

1339
00:59:06,663 --> 00:59:07,220
겁니다.

1340
00:59:07,220 --> 00:59:09,535
이것이 바로 그렇게 하는 구체적인 이유입니다.

1341
00:59:09,535 --> 00:59:11,160
결국 경험적으로 두 가지를 모두 시도해 보고

1342
00:59:11,160 --> 00:59:12,618
어느 쪽이 더 나은지 확인하는 겁니다.

1343
00:59:12,618 --> 00:59:15,560
이해되셨죠?

1344
00:59:15,560 --> 00:59:16,082
좋습니다.

1345
00:59:16,082 --> 00:59:17,540
이제 학습률에 대해 이야기해 보겠습니다.

1346
00:59:17,540 --> 00:59:21,240
학습률을 선택하는 다양한

1347
00:59:21,240 --> 00:59:23,300
방법이 있습니다.

1348
00:59:23,300 --> 00:59:25,640
때로는 너무 높은 학습률을

1349
00:59:25,640 --> 00:59:27,098
쓰면, 앞서 설명한

1350
00:59:27,098 --> 00:59:30,200
것처럼 손실이 매우 커지면서 손실

1351
00:59:30,200 --> 00:59:32,958
지형 밖으로 진동하게 됩니다.

1352
00:59:32,958 --> 00:59:34,500
반대로 너무 낮은

1353
00:59:34,500 --> 00:59:36,440
학습률은 수렴이 매우 느립니다.

1354
00:59:36,440 --> 00:59:39,300
높은 학습률인데 진동하지는

1355
00:59:39,300 --> 00:59:42,000
않지만, 국소 최소점 주변에서

1356
00:59:42,000 --> 00:59:44,260
튕기면서 실제로 더

1357
00:59:44,260 --> 00:59:46,380
낮은 값으로 수렴하지

1358
00:59:46,380 --> 00:59:48,660
못할 수도 있습니다.

1359
00:59:48,660 --> 00:59:50,060
이상적인 학습률은

1360
00:59:50,060 --> 00:59:52,860
시간이 지남에 따라 손실을

1361
00:59:52,860 --> 00:59:55,020
빠르게 감소시키고, 계속

1362
00:59:55,020 --> 00:59:59,900
훈련하면서도 지속적인 개선이 이루어지는 특성을 가집니다.

1363
00:59:59,900 --> 01:00:02,900
실제로는 상황에 따라 이런 여러 학습률이 모두

1364
01:00:02,900 --> 01:00:04,740
괜찮을 수 있고, 훈련 단계에

1365
01:00:04,740 --> 01:00:06,200
따라서도 달라질 수

1366
01:00:06,200 --> 01:00:09,500
있습니다. 오늘 강의의 마지막 주제가 바로 이것입니다.

1367
01:00:09,500 --> 01:00:11,740
즉, 모델을 훈련하면서 학습률을

1368
01:00:11,740 --> 01:00:13,322
바꿀 수 있습니다.

1369
01:00:13,322 --> 01:00:15,780
항상 고정된 학습률이나 스텝 크기를 가질 필요는

1370
01:00:15,780 --> 01:00:16,280
없습니다.

1371
01:00:16,280 --> 01:00:19,820
그리고 거의 모든 최신 딥러닝, 즉 최고의

1372
01:00:19,820 --> 01:00:21,680
모델들은 학습 중에 학습률을

1373
01:00:21,680 --> 01:00:23,540
다양하게 조절하는

1374
01:00:23,540 --> 01:00:25,340
방법을 가지고 있습니다.

1375
01:00:25,340 --> 01:00:28,980
가장 간단한 방법 중 하나는 고정된

1376
01:00:28,980 --> 01:00:33,740
반복 횟수 후에 학습률을 1/10로 줄이고 계속 학습하는

1377
01:00:33,740 --> 01:00:35,000
것입니다.

1378
01:00:35,000 --> 01:00:39,460
이 방법은 학습률이 너무 높아 더 이상 수렴하지 못하는

1379
01:00:39,460 --> 01:00:42,320
문제를 해결할 수 있어서, 학습률을 줄이면

1380
01:00:42,320 --> 01:00:44,580
손실 함수의 낮은 지점까지

1381
01:00:44,580 --> 01:00:46,220
도달할 수 있습니다.

1382
01:00:46,220 --> 01:00:48,990
이 방법은 ResNet을 학습할 때 매우 흔히 사용됩니다.

1383
01:00:48,990 --> 01:00:51,850
ResNet은 나중에 강의에서 다룰 매우

1384
01:00:51,850 --> 01:00:54,310
인기 있는 합성곱 신경망 종류입니다.

1385
01:00:54,310 --> 01:00:57,810
또 다른 방법은 코사인 학습률 감소(cosine learning rate
decay)입니다.

1386
01:00:57,810 --> 01:01:00,410
이 방법도 매우 인기가 많습니다.

1387
01:01:00,410 --> 01:01:03,870
여기서는 기본적으로 코사인

1388
01:01:03,870 --> 01:01:05,870
파형의 절반처럼,

1389
01:01:05,870 --> 01:01:09,170
최대 학습률에서 시작합니다.

1390
01:01:09,170 --> 01:01:11,570
그리고 끝까지 0으로 내려갑니다.

1391
01:01:11,570 --> 01:01:15,210
이것은 1.2 코사인 형태를 따르며, 계산

1392
01:01:15,210 --> 01:01:17,130
공식도 여기 있습니다.

1393
01:01:17,130 --> 01:01:18,665
자세한 내용은 다루지 않겠지만,

1394
01:01:18,665 --> 01:01:21,290
기본 아이디어는 다양한 방법이 있다는 것입니다.

1395
01:01:21,290 --> 01:01:24,490
코사인 학습률 스케줄러를 사용할 때는

1396
01:01:24,490 --> 01:01:25,990
학습 중간에 좋은

1397
01:01:25,990 --> 01:01:29,310
성능 향상이 나타나는 이런 형태를 자주

1398
01:01:29,310 --> 01:01:30,670
볼 수 있습니다.

1399
01:01:30,670 --> 01:01:32,910
하지만 기본적으로 학습 중 손실 함수의

1400
01:01:32,910 --> 01:01:34,870
형태는 어떤 스케줄러를

1401
01:01:34,870 --> 01:01:36,250
쓰느냐에 크게 좌우됩니다.

1402
01:01:36,250 --> 01:01:38,330
이것이 제가 전달하려는 기본 아이디어입니다.

1403
01:01:38,330 --> 01:01:40,497
예를 들어, 학습 중에

1404
01:01:40,497 --> 01:01:42,710
학습률을 1/10로 줄이는 이

1405
01:01:42,710 --> 01:01:45,763
방법과는 손실 곡선 모양이 매우 다릅니다.

1406
01:01:45,763 --> 01:01:48,180
또 다른 방법은 단순히 선형 학습률 감소를 하는 것입니다.

1407
01:01:48,180 --> 01:01:50,003
즉, 직선을 따라 감소하는 거죠.

1408
01:01:50,003 --> 01:01:52,420
역제곱근 등 다양한 방법도 가능합니다.

1409
01:01:52,420 --> 01:01:54,640
사실 학습 중에 학습률을

1410
01:01:54,640 --> 01:01:57,277
조절하는 방법은 무한히 많습니다.

1411
01:01:57,277 --> 01:01:59,360
그리고 훈련하는 모델 종류나 가장

1412
01:01:59,360 --> 01:02:00,735
잘 맞는 방법에 따라

1413
01:02:00,735 --> 01:02:02,500
최적의 방법을 선택하면 됩니다.

1414
01:02:02,500 --> 01:02:05,080
여기 여러분 환경에서 잘 작동할 수 있는

1415
01:02:05,080 --> 01:02:06,720
몇 가지 방법을 소개합니다.

1416
01:02:06,720 --> 01:02:09,360
또한 매우 인기 있는 전략은 선형

1417
01:02:09,360 --> 01:02:10,980
워밍업을 하는 것입니다.

1418
01:02:10,980 --> 01:02:13,580
최대 학습률에서 바로 시작하는

1419
01:02:13,580 --> 01:02:15,520
대신, 일정한 반복 횟수

1420
01:02:15,520 --> 01:02:20,000
동안 선형적으로 최대값까지 워밍업한 후, 이후에는 원하는

1421
01:02:20,000 --> 01:02:22,620
스케줄러를 적용하는 방식입니다.

1422
01:02:22,620 --> 01:02:24,880
예를 들어 선형 워밍업 후 역제곱근

1423
01:02:24,880 --> 01:02:27,080
스케줄러를 사용하는 경우가 있습니다.

1424
01:02:27,080 --> 01:02:28,640
또는 선형 워밍업 후 코사인

1425
01:02:28,640 --> 01:02:32,480
스케줄러를 사용하는데, 코사인은 모델 훈련에 매우 인기 있는 설정입니다.

1426
01:02:32,480 --> 01:02:38,040
마지막으로, 경험적으로 알려진 규칙 중에 선형

1427
01:02:38,040 --> 01:02:43,320
스케일링 가설 또는 선형 스케일링 법칙 같은 것이

1428
01:02:43,320 --> 01:02:44,560
있습니다.

1429
01:02:44,560 --> 01:02:45,310
정확한 명칭은 기억이 나지 않네요.

1430
01:02:45,310 --> 01:02:46,893
아마 선형 스케일링

1431
01:02:46,893 --> 01:02:49,850
법칙일 텐데, 배치 크기나

1432
01:02:49,850 --> 01:02:53,770
업데이트 당 훈련 샘플 수를 n배 늘리면 학습률도

1433
01:02:53,770 --> 01:02:57,070
n배로 늘려야 한다는 내용입니다.

1434
01:02:57,070 --> 01:02:58,910
즉, 배치 크기를

1435
01:02:58,910 --> 01:03:02,770
늘릴 때 학습률도 비례해서 증가시켜야

1436
01:03:02,770 --> 01:03:03,970
한다는 거죠.

1437
01:03:03,970 --> 01:03:06,670
이 이론의 수학적 배경은 다소 복잡하고,

1438
01:03:06,670 --> 01:03:09,150
경험적인 규칙에 가깝습니다.

1439
01:03:09,150 --> 01:03:12,262
그래서 사람들이 왜 이 방법이 유용할 수 있는지 수학적 증명을

1440
01:03:12,262 --> 01:03:13,470
시도해 본 경우도 있습니다.

1441
01:03:13,470 --> 01:03:17,530
하지만 배치 내 그래디언트의 변동과 배치당 계산하는

1442
01:03:17,530 --> 01:03:21,530
그래디언트 수 등에 따라 다르지만, 실제로 많은

1443
01:03:21,530 --> 01:03:23,090
문제에서 경험적으로 이

1444
01:03:23,090 --> 01:03:25,590
방법이 효과적임이 입증되었습니다.

1445
01:03:25,590 --> 01:03:26,990
그래서 이것은 좋은 경험적 규칙입니다.

1446
01:03:26,990 --> 01:03:28,690
만약 좋은 방법을

1447
01:03:28,690 --> 01:03:31,130
찾았는데 배치 크기를 늘리고

1448
01:03:31,130 --> 01:03:35,130
싶다면, 학습률도 같은 비율로 늘리면 됩니다.

1449
01:03:35,130 --> 01:03:35,810
좋죠.

1450
01:03:35,810 --> 01:03:39,250
그리고 마지막으로 아주 간단히 언급할

1451
01:03:39,250 --> 01:03:44,070
것은, 누군가가 앞서 질문했던 Hessian을

1452
01:03:44,070 --> 01:03:45,630
사용하는 2차 최적화

1453
01:03:45,630 --> 01:03:46,990
방법입니다.

1454
01:03:46,990 --> 01:03:49,028
이 부분을 깊게 다루지는 않겠지만,

1455
01:03:49,028 --> 01:03:50,570
존재한다는 것만 알려드리겠습니다.

1456
01:03:50,570 --> 01:03:53,550
이 과정에서 많이 다루지는 않습니다.

1457
01:03:53,550 --> 01:03:56,470
기본 아이디어는 지금 우리가

1458
01:03:56,470 --> 01:03:59,170
그래디언트를 사용해 손실

1459
01:03:59,170 --> 01:04:00,670
함수의 하강

1460
01:04:00,670 --> 01:04:03,630
방향을 선형 근사하는 겁니다.

1461
01:04:03,630 --> 01:04:05,815
그 방향을 보고 일반적인 한

1462
01:04:05,815 --> 01:04:07,690
걸음을 내딛는 거죠.

1463
01:04:07,690 --> 01:04:12,470
그리고 모멘텀이나 RMSProp 같은 기법을 추가해 가파른

1464
01:04:12,470 --> 01:04:15,085
방향에서는 속도를 줄이기도 했습니다.

1465
01:04:15,085 --> 01:04:16,210
이게 기본적인 아이디어입니다.

1466
01:04:16,210 --> 01:04:21,270
매 시간마다 이 그래디언트를 사용하고 있죠.

1467
01:04:21,270 --> 01:04:24,530
Hessian의 아이디어는

1468
01:04:24,530 --> 01:04:33,030
그래디언트 대신 그 지점에서의 도함수나 Hessian을 이용해 함수에

1469
01:04:33,030 --> 01:04:36,390
2차 다항식, 즉 이차

1470
01:04:36,390 --> 01:04:38,750
곡선을 맞추는 겁니다.

1471
01:04:38,750 --> 01:04:41,910
그리고 그 방법으로 최소값을 찾으려 하는 거죠.

1472
01:04:41,910 --> 01:04:44,640
특정 최적화 문제에서는 이 방법이

1473
01:04:44,640 --> 01:04:46,260
매우 잘 작동합니다.

1474
01:04:46,260 --> 01:04:49,120
하지만 일반적으로 딥러닝에서는 두 가지 이유

1475
01:04:49,120 --> 01:04:51,220
때문에 잘 사용하지 않습니다.

1476
01:04:51,220 --> 01:04:53,620
우선, 테일러 급수 전개를

1477
01:04:53,620 --> 01:04:55,920
해야 하는데, 지금은

1478
01:04:55,920 --> 01:04:57,420
미분만 하고 있지만

1479
01:04:57,420 --> 01:05:00,720
두 번째 혼합 미분도 계산할 수

1480
01:05:00,720 --> 01:05:05,100
있어야 합니다. 이게 이미 어려울 수 있습니다. 게다가 모델의 모든
파라미터에 대한 혼합

1481
01:05:05,100 --> 01:05:10,588
미분이 다른 모든 파라미터에 대해 계산되면, 수백만

1482
01:05:10,588 --> 01:05:12,880
또는 수십억 개의 파라미터를

1483
01:05:12,880 --> 01:05:16,040
가진 신경망에서는 매우 커질

1484
01:05:16,040 --> 01:05:17,520
수 있습니다.

1485
01:05:17,520 --> 01:05:24,520
그래서 실제로는 이 행렬들이 너무 커지기 때문에 사용하지

1486
01:05:24,520 --> 01:05:25,180
않습니다.

1487
01:05:25,180 --> 01:05:27,060
만약 시도하면 컴퓨터 메모리가

1488
01:05:27,060 --> 01:05:27,935
부족해질 겁니다.

1489
01:05:27,935 --> 01:05:29,900
특히 GPU 메모리에서 그렇습니다.

1490
01:05:29,900 --> 01:05:31,760
하지만 작은 모델을

1491
01:05:31,760 --> 01:05:36,400
훈련하거나 더 나은 최적화 단계를 위해 더 많은 시간을 투자할

1492
01:05:36,400 --> 01:05:39,160
수 있다면 이 방법을 고려해볼

1493
01:05:39,160 --> 01:05:40,167
수 있습니다.

1494
01:05:40,167 --> 01:05:42,250
문제에 따라 다르지만, 작은 모델에서는

1495
01:05:42,250 --> 01:05:43,510
실제로 꽤 잘 작동합니다.

1496
01:05:43,510 --> 01:05:45,270
하지만 우리가 훈련하는 대규모

1497
01:05:45,270 --> 01:05:47,950
신경망에서는 메모리 제한 때문에 거의 사용하지 않습니다.

1498
01:05:47,950 --> 01:05:50,410
그리고 헤시안 행렬 계산 등에

1499
01:05:50,410 --> 01:05:52,933
소요되는 계산 시간을 데이터 더 많이

1500
01:05:52,933 --> 01:05:55,100
보는 데 쓰는 게 낫습니다.

1501
01:05:58,170 --> 01:05:58,670
좋습니다.

1502
01:05:58,670 --> 01:06:03,690
여러분께 도움이 될 만한 몇 가지 마무리 생각을

1503
01:06:03,690 --> 01:06:04,910
말씀드리겠습니다.

1504
01:06:04,910 --> 01:06:08,250
Adam이나 AdamW는 새로운 문제에 대해 첫

1505
01:06:08,250 --> 01:06:11,650
모델을 훈련할 때 정말 좋은 기본 선택입니다.

1506
01:06:11,650 --> 01:06:14,032
특정 도메인에서 추천합니다.

1507
01:06:14,032 --> 01:06:16,490
그리고 일정한 학습률로도 괜찮게 작동할 수

1508
01:06:16,490 --> 01:06:16,990
있습니다.

1509
01:06:16,990 --> 01:06:19,610
보통 사람들은 일정한 학습률이나 선형 워밍업,

1510
01:06:19,610 --> 01:06:21,610
그리고 코사인 감쇠를 조합해서

1511
01:06:21,610 --> 01:06:25,450
Adam이나 AdamW를 시도합니다. 이 조합이 매우 인기가 많습니다.

1512
01:06:25,450 --> 01:06:30,030
또한, SGD와 모멘텀은 때때로 Adam보다 더 좋은 성능을 낼 수
있습니다.

1513
01:06:30,030 --> 01:06:34,330
하지만 까다로운 점은 일반적으로 값을 더 많이

1514
01:06:34,330 --> 01:06:36,850
조정해야 한다는 겁니다. RMSProp

1515
01:06:36,850 --> 01:06:40,300
항이 없기 때문에 급격한 방향을 고려할

1516
01:06:40,300 --> 01:06:43,340
수 없어서 더 많은 학습률을

1517
01:06:43,340 --> 01:06:44,422
시도해야 합니다.

1518
01:06:44,422 --> 01:06:46,880
또한 다른 스케줄링 값을 시도해야 할 수도 있는데,

1519
01:06:46,880 --> 01:06:48,658
실제로는 Adam이 테스트에서 가장 좋습니다.

1520
01:06:48,658 --> 01:06:50,700
사람들이 여러 분야에서 많이 시도해봤고

1521
01:06:50,700 --> 01:06:51,658
아주 잘 작동합니다.

1522
01:06:51,658 --> 01:06:55,100
손실 함수 지형에 매우 적응적입니다.

1523
01:06:55,100 --> 01:06:58,280
만약 전체 배치 업데이트를 하고

1524
01:06:58,280 --> 01:06:59,780
있고, 각

1525
01:06:59,780 --> 01:07:03,380
단계에서 기본적으로 전체 훈련 세트를

1526
01:07:03,380 --> 01:07:06,180
배치 크기에 맞출 수 있다면,

1527
01:07:06,180 --> 01:07:09,180
1차 최적화보다 2차 이상

1528
01:07:09,180 --> 01:07:11,760
최적화를 고려하는 게 좋습니다.

1529
01:07:11,760 --> 01:07:13,340
데이터 세트가

1530
01:07:13,340 --> 01:07:17,260
크지 않거나 모델이 크지 않은 경우,

1531
01:07:17,260 --> 01:07:22,060
비선형 업데이트 단계와 더 정교한 하강 전략을

1532
01:07:22,060 --> 01:07:25,260
계산하는 것이 이득일 수 있습니다.

1533
01:07:25,260 --> 01:07:27,243
네, 사실상 강의는

1534
01:07:27,243 --> 01:07:28,160
여기까지입니다.

1535
01:07:28,160 --> 01:07:31,400
앞으로의 내용을 다룬 슬라이드를 보여드리겠습니다.

1536
01:07:31,400 --> 01:07:34,220
이번 강의에서 다룬 선형 모델보다 더

1537
01:07:34,220 --> 01:07:37,080
복잡한 함수를 어떻게 최적화할 수 있을까요?

1538
01:07:37,080 --> 01:07:39,160
다음 강의에서는

1539
01:07:39,160 --> 01:07:44,640
특히 신경망을 다룰 텐데, 매우 흥미로운 주제입니다.

1540
01:07:44,640 --> 01:07:47,360
수업에서 다룰 신경망은 기본적으로 두

1541
01:07:47,360 --> 01:07:49,680
개의 가중치 행렬이 있고, 각

1542
01:07:49,680 --> 01:07:51,480
층마다 하나씩 있습니다.

1543
01:07:51,480 --> 01:07:55,320
그리고 그 사이에 비선형성이라는 것이 끼어

1544
01:07:55,320 --> 01:07:55,940
있습니다.

1545
01:07:55,940 --> 01:07:58,282
가장 흔한, 아니 가장 단순한

1546
01:07:58,282 --> 01:08:00,240
비선형 함수는 ReLU

1547
01:08:00,240 --> 01:08:03,410
함수인데, 나중에 더 배우게 될 겁니다.

1548
01:08:03,410 --> 01:08:05,660
기본 아이디어는 이제 두

1549
01:08:05,660 --> 01:08:07,560
개의 가중치 행렬이 있고,

1550
01:08:07,560 --> 01:08:11,280
그 사이에 추가 함수가 계산된다는 겁니다.

1551
01:08:11,280 --> 01:08:13,500
이게 좋은 점은, 말씀드렸듯이 비선형이라는 겁니다.

1552
01:08:13,500 --> 01:08:16,840
만약 이런 데이터를 선형 분류기로 분류하려

1553
01:08:16,840 --> 01:08:18,800
하면, 파란 점과 빨간

1554
01:08:18,800 --> 01:08:20,840
점이 선형적으로 분리되지

1555
01:08:20,840 --> 01:08:22,359
않는 문제가 생깁니다.

1556
01:08:22,359 --> 01:08:24,600
하지만 어떤 변환을 하거나

1557
01:08:24,600 --> 01:08:26,380
여러 층을 거치면서

1558
01:08:26,380 --> 01:08:28,640
데이터를 선형으로 분리 가능한

1559
01:08:28,640 --> 01:08:31,200
형태로 변환할 수 있습니다. 그게

1560
01:08:31,200 --> 01:08:33,680
모델의 마지막 층이 되는 거죠.

1561
01:08:33,680 --> 01:08:36,730
[AUDIO LOGO]
