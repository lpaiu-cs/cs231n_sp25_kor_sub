1
00:00:05,480 --> 00:00:09,200
오늘 강의 주제는 정규화와 최적화에 관한

2
00:00:09,200 --> 00:00:12,680
것으로, 이는 딥러닝과 머신러닝에서

3
00:00:12,680 --> 00:00:15,160
매우 중요한 개념이며, 특히

4
00:00:15,160 --> 00:00:17,520
컴퓨터 비전에서 중요합니다.

5
00:00:17,520 --> 00:00:20,040
우리는 지난 주의 내용을 요약하고

6
00:00:20,040 --> 00:00:24,400
지난 시간에 논의했던 주제들에 대해 이야기할 것입니다.

7
00:00:24,400 --> 00:00:27,360
우리는 이미지 분류라는 컴퓨터

8
00:00:27,360 --> 00:00:30,960
비전의 핵심 작업에 대해 집중했습니다.

9
00:00:30,960 --> 00:00:34,560
이 작업은 입력으로 주어진 이미지를

10
00:00:34,560 --> 00:00:40,660
사용하여 이 이미지를 레이블 집합 내의 레이블에 매핑하는 것입니다.

11
00:00:40,660 --> 00:00:44,400
여기에는 고양이, 개, 새, 사슴, 트럭의 다섯 가지 레이블이

12
00:00:44,400 --> 00:00:45,060
있습니다.

13
00:00:45,060 --> 00:00:47,440
목표는 입력 이미지에 올바른 레이블을

14
00:00:47,440 --> 00:00:48,720
할당하는 것입니다.

15
00:00:48,720 --> 00:00:50,760
이미지를 입력으로 받아

16
00:00:50,760 --> 00:00:55,360
특정 레이블을 출력하는 모델이나 함수를 생성하고 있습니다.

17
00:00:55,360 --> 00:00:58,360
우리는 분류와 관련된 많은 도전 과제에

18
00:00:58,360 --> 00:01:00,050
대해서도 이야기했습니다.

19
00:01:00,050 --> 00:01:04,670
주요 도전 과제 중 하나는 여기 왼쪽

20
00:01:04,670 --> 00:01:07,730
상단에 나타나 있으며,

21
00:01:07,730 --> 00:01:10,370
이는 우리가 인간으로서

22
00:01:10,370 --> 00:01:12,850
이미지에서 인식하는

23
00:01:12,850 --> 00:01:16,610
것(고양이)과 컴퓨터에서 실제로

24
00:01:16,610 --> 00:01:19,370
표현되는 것(픽셀

25
00:01:19,370 --> 00:01:24,313
값의 그리드) 사이의 의미적 간극입니다.

26
00:01:24,313 --> 00:01:26,730
이는 우리가 이미지를 인식하고 이것이 고양이라는

27
00:01:26,730 --> 00:01:28,950
결정을 내리는 방식과 매우 다릅니다.

28
00:01:28,950 --> 00:01:33,690
복잡한 숫자 표현에서 우리가 이해할 수 있는 표현으로

29
00:01:33,690 --> 00:01:35,570
매핑하는 것이 여기서 핵심

30
00:01:35,570 --> 00:01:36,870
도전 과제입니다.

31
00:01:36,870 --> 00:01:39,730
하지만 이미지 자체와 관련된 도전 과제도

32
00:01:39,730 --> 00:01:40,430
있습니다.

33
00:01:40,430 --> 00:01:44,670
예를 들어 장면의 조명을 살펴보면, 여기서는 조명

34
00:01:44,670 --> 00:01:46,850
위치에 따라 서로 다른

35
00:01:46,850 --> 00:01:49,090
픽셀 강도가 있을 것입니다.

36
00:01:49,090 --> 00:01:52,410
또한 물체의 특정 부분이 그늘에

37
00:01:52,410 --> 00:01:55,130
있어 보기 어려울 수 있습니다.

38
00:01:55,130 --> 00:01:56,615
고양이는 본질적으로 매우 변형 가능하므로,

39
00:01:56,615 --> 00:01:58,990
다양한 방식으로 움직이고 비틀고 구부릴 수 있는

40
00:01:58,990 --> 00:02:00,730
변형 가능한 물체에 대해 이야기합니다.

41
00:02:00,730 --> 00:02:02,490
따라서 항상 같은 형태를 가지지는 않습니다.

42
00:02:02,490 --> 00:02:04,157
이는 물체를 감지하는 알고리즘을

43
00:02:04,157 --> 00:02:06,510
설계하려고 할 때 도전이 될 수 있습니다.

44
00:02:06,510 --> 00:02:08,530
가려짐의 문제도 있습니다.

45
00:02:08,530 --> 00:02:11,510
여기 쿠션 아래에 숨은 고양이가 있을 수

46
00:02:11,510 --> 00:02:13,150
있지만, 우리는 꼬리

47
00:02:13,150 --> 00:02:16,143
덕분에 분명히 고양이라는 것을 알 수

48
00:02:16,143 --> 00:02:16,810
있습니다.

49
00:02:16,810 --> 00:02:18,810
그리고 고양이의 행동 방식으로 우리는

50
00:02:18,810 --> 00:02:20,990
이것이 고양이라는 것을 추론할 수 있습니다.

51
00:02:20,990 --> 00:02:23,270
배경 잡동사니와 같은 것도 있을 수

52
00:02:23,270 --> 00:02:25,570
있는데, 물체가 배경에 섞일 수 있습니다.

53
00:02:25,570 --> 00:02:27,950
그래서 우리는 이것도 어떻게든 고려해야 합니다.

54
00:02:27,950 --> 00:02:31,830
마지막으로, 같은 범주 내에서 서로 매우 다르게 보일 수 있는

55
00:02:31,830 --> 00:02:33,630
객체들 간의 내부 클래스 변동성이라는

56
00:02:33,630 --> 00:02:35,690
개념이 있습니다. 하지만 우리는

57
00:02:35,690 --> 00:02:38,690
여전히 이들을 모두 같은 범주로 묶어야 합니다.

58
00:02:38,690 --> 00:02:41,490
여기 인식의 많은 도전 과제가 있으며,

59
00:02:41,490 --> 00:02:45,030
모든 것을 설명하기 위해 단순히 if else 규칙을

60
00:02:45,030 --> 00:02:48,070
작성하고 단순한 논리로 분류할 수 있는

61
00:02:48,070 --> 00:02:50,590
간단한 문제가 아니라는 이유입니다.

62
00:02:50,590 --> 00:02:52,330
논리가 무시된다면, 이러한

63
00:02:52,330 --> 00:02:54,270
논리 규칙을 만들 수 없다면,

64
00:02:54,270 --> 00:02:56,770
실제로 분류기를 어떻게 만들까요?

65
00:02:56,770 --> 00:02:59,620
여기서 우리는 데이터 기반 접근 방식에 대해 이야기했습니다.

66
00:02:59,620 --> 00:03:02,480
그리고 기본적으로 가장 간단한 머신 러닝 모델인

67
00:03:02,480 --> 00:03:05,560
k-최근접 이웃 모델에 대해 이야기했습니다.

68
00:03:05,560 --> 00:03:10,280
아이디어는 주어진 데이터 포인트에 대해, 새로운

69
00:03:10,280 --> 00:03:13,360
데이터 포인트와 거리가 매우

70
00:03:13,360 --> 00:03:18,320
가까운 훈련 세트의 기존 데이터 포인트를 찾는

71
00:03:18,320 --> 00:03:19,240
것입니다.

72
00:03:19,240 --> 00:03:23,840
1-최근접 이웃의 경우, 가장 가까운 데이터 포인트를

73
00:03:23,840 --> 00:03:26,960
찾아 해당 클래스 레이블을 할당합니다.

74
00:03:26,960 --> 00:03:29,840
또한 여러 최근접 이웃을 살펴보면서 그들

75
00:03:29,840 --> 00:03:33,440
중 가장 일반적인 클래스 레이블을 할당할 수

76
00:03:33,440 --> 00:03:34,057
있습니다.

77
00:03:34,057 --> 00:03:36,140
우리는 이 두 가지 접근 방식에 대해 이야기했습니다.

78
00:03:36,140 --> 00:03:39,200
이상적으로는 데이터 세트를 훈련과 테스트로

79
00:03:39,200 --> 00:03:41,260
나누고 싶지 않지만,

80
00:03:41,260 --> 00:03:44,120
훈련, 검증 및 테스트로 나눌 수 있어

81
00:03:44,120 --> 00:03:46,560
이 검증 세트를 사용하여

82
00:03:46,560 --> 00:03:50,120
하이퍼파라미터를 선택하는 데 도움을 줄 수 있습니다.

83
00:03:50,120 --> 00:03:52,920
k-최근접 이웃의 주요 하이퍼파라미터는

84
00:03:52,920 --> 00:03:56,107
이 k이며, 이 예제에서는 1 또는 5입니다.

85
00:03:56,107 --> 00:03:57,690
우리가 보여준 것은

86
00:03:57,690 --> 00:04:00,850
여기에서 다양한 k 값에 대한 이 검증

87
00:04:00,850 --> 00:04:03,910
세트에서의 정확도를 플로팅하는 예입니다.

88
00:04:03,910 --> 00:04:07,640
가장 높은 정확도를 가진 것을 선택합니다.

89
00:04:07,640 --> 00:04:09,390
이것이 검증 세트를 사용하는 방법이며,

90
00:04:09,390 --> 00:04:11,010
이 테스트 세트는 완전히

91
00:04:11,010 --> 00:04:13,230
새로운 데이터에서 모델이 어떻게 작동하는지

92
00:04:13,230 --> 00:04:14,350
확인하기 위해 남겨둡니다.

93
00:04:14,350 --> 00:04:16,450
이것이 테스트 세트의 목적입니다.

94
00:04:16,450 --> 00:04:18,450
이것은 모두 요약입니다.

95
00:04:18,450 --> 00:04:20,829
거리 측정에 대한 약간의 혼란이 있었습니다.

96
00:04:20,829 --> 00:04:24,330
이것을 더 자세히 설명하는 게시물을 Ed에 올렸습니다.

97
00:04:24,330 --> 00:04:27,410
하지만 우리는 두 가지 다른 거리 측정에 대해 이야기했습니다.

98
00:04:27,410 --> 00:04:30,190
기계 학습에서 가장 일반적으로

99
00:04:30,190 --> 00:04:34,530
사용되는 두 가지는 맨해튼 거리 또는 L1 거리와 L2

100
00:04:34,530 --> 00:04:36,770
거리 또는 유클리드 거리입니다.

101
00:04:36,770 --> 00:04:40,610
L2 거리는 직선 거리로 생각할 수 있으며,

102
00:04:40,610 --> 00:04:42,430
우리가 일상적으로

103
00:04:42,430 --> 00:04:44,490
사용하는 거리 개념을

104
00:04:44,490 --> 00:04:46,790
기하학적으로 표현한 것입니다.

105
00:04:46,790 --> 00:04:49,863
맨해튼 거리는 이 도표에서 좌우 및 상하로만

106
00:04:49,863 --> 00:04:52,390
이동할 수 있다는 아이디어이며, 대각선으로

107
00:04:52,390 --> 00:04:53,830
이동할 수 없습니다.

108
00:04:53,830 --> 00:04:57,260
여기에서 하나의 간단한 예를 살펴보겠습니다.

109
00:04:57,260 --> 00:04:59,060
이 선상의 모든 점이

110
00:04:59,060 --> 00:05:01,180
원점에서 같은 거리에 있는 이유는

111
00:05:01,180 --> 00:05:03,960
대각선으로 이동할 수 없기 때문에

112
00:05:03,960 --> 00:05:07,920
이 경우 위로 0.5, 오른쪽으로 0.5 이동해야

113
00:05:07,920 --> 00:05:10,460
하므로 총 거리는 1입니다. 여기서는

114
00:05:10,460 --> 00:05:13,140
직선으로 가지만 그것도 1입니다.

115
00:05:13,140 --> 00:05:14,480
여기서도 같은 거리입니다.

116
00:05:14,480 --> 00:05:18,020
L2 거리에서는 원점에서 등거리인 모든 점이

117
00:05:18,020 --> 00:05:20,420
원을 형성합니다. 왜냐하면

118
00:05:20,420 --> 00:05:23,660
여기에서 직선으로 이동할 수 있기 때문입니다.

119
00:05:23,660 --> 00:05:26,700
그래서 이것은 아마 간단한 설명일 것입니다.

120
00:05:26,700 --> 00:05:30,060
마지막으로 우리가 지난 시간에 집중했던

121
00:05:30,060 --> 00:05:32,980
것은 선형 분류기의 개념이었습니다.

122
00:05:32,980 --> 00:05:36,940
우리가 했던 기본 설정의 기본 아이디어는

123
00:05:36,940 --> 00:05:41,660
너비 32, 높이 32의 이미지가 있고,

124
00:05:41,660 --> 00:05:44,700
이미지의 각 공간 위치에 대해

125
00:05:44,700 --> 00:05:47,700
빨강, 초록, 파랑 강도를

126
00:05:47,700 --> 00:05:52,500
나타내는 세 개의 픽셀 값이 있다는 것입니다.

127
00:05:52,500 --> 00:05:55,900
아이디어는 이 숫자 배열을 가져와서 3,

128
00:05:55,900 --> 00:05:59,440
000개의 서로 다른 숫자, 즉 3,072로

129
00:05:59,440 --> 00:06:01,400
평탄화하는 것입니다.

130
00:06:01,400 --> 00:06:06,640
그런 다음 이 벡터를 가중치 행렬 W와 곱합니다.

131
00:06:06,640 --> 00:06:11,440
기본 아이디어는 높이가 10이고 너비가

132
00:06:11,440 --> 00:06:18,300
3,071인 가중치 행렬 W가 있을 때, 각 행을

133
00:06:18,300 --> 00:06:22,960
입력 샘플 x와 곱하는 것입니다.

134
00:06:22,960 --> 00:06:26,680
이렇게 하면 10개의 결과 클래스 점수가 생성됩니다.

135
00:06:26,680 --> 00:06:29,040
그래서 종종 각 클래스에

136
00:06:29,040 --> 00:06:32,620
대해 1개의 바이어스 항을 추가합니다.

137
00:06:32,620 --> 00:06:35,680
따라서 이것은 크기 10의 벡터입니다.

138
00:06:35,680 --> 00:06:37,960
우리는 또한 이러한 선형 모델을

139
00:06:37,960 --> 00:06:41,008
보는 세 가지 다른 방법에 대해 이야기했습니다.

140
00:06:41,008 --> 00:06:43,300
하나는 제가 여기 설명한

141
00:06:43,300 --> 00:06:47,382
대수적 관점으로, 각 행이 독립적으로 클래스를

142
00:06:47,382 --> 00:06:48,340
나타냅니다.

143
00:06:48,340 --> 00:06:53,410
입력 벡터 x와 곱하면 점수를 얻고 바이어스를

144
00:06:53,410 --> 00:06:56,210
추가하여 최종 점수를 얻습니다.

145
00:06:56,210 --> 00:06:58,410
각 행을 독립적으로 처리합니다.

146
00:06:58,410 --> 00:07:04,930
이 학습된 클래스 가중치를 템플릿으로 볼 수도 있습니다.

147
00:07:04,930 --> 00:07:08,850
원래 이미지 형태로 벡터를

148
00:07:08,850 --> 00:07:13,090
다시 펼치면 강도를 플로팅하고 각

149
00:07:13,090 --> 00:07:18,130
클래스의 템플릿이 무엇인지 이해할 수

150
00:07:18,130 --> 00:07:21,970
있습니다. 이것이 이 시각화가

151
00:07:21,970 --> 00:07:24,010
나타내는 것입니다.

152
00:07:24,010 --> 00:07:25,570
마지막으로

153
00:07:25,570 --> 00:07:29,130
생각할 수 있는 방법은 기하학적

154
00:07:29,130 --> 00:07:32,930
관점으로, 가중치 행렬의

155
00:07:32,930 --> 00:07:38,570
각 행이 입력 공간의 이 선들로 표현됩니다.

156
00:07:38,570 --> 00:07:40,690
특히, 이 선은 이

157
00:07:40,690 --> 00:07:45,070
방정식을 0으로 설정하는 곳으로, 결정 경계입니다.

158
00:07:45,070 --> 00:07:48,485
따라서 이 선 위에서는 긍정적인 점수를 가질 수

159
00:07:48,485 --> 00:07:50,610
있고, 선 아래에서는 클래스에

160
00:07:50,610 --> 00:07:54,820
대해 부정적인 점수를 가질 수 있는 지점을 형성합니다.

161
00:07:54,820 --> 00:07:57,380
이것들은 이러한 선형 모델을 볼 수

162
00:07:57,380 --> 00:07:59,240
있는 다양한 관점입니다.

163
00:07:59,240 --> 00:08:00,780
모두 같은 일을 하고 있습니다.

164
00:08:00,780 --> 00:08:03,620
기하학적 관점의 좋은 점은 데이터를

165
00:08:03,620 --> 00:08:06,180
시각화할 때, 예를 들어 여기서 파란색과

166
00:08:06,180 --> 00:08:09,360
빨간색을 분류하고자 할 때, 데이터를

167
00:08:09,360 --> 00:08:12,300
완벽하게 분리하는 선을 그릴 수 없다는 것을

168
00:08:12,300 --> 00:08:14,520
쉽게 알 수 있다는 것입니다.

169
00:08:14,520 --> 00:08:17,500
그래서 선형 모델이 할 수 있는

170
00:08:17,500 --> 00:08:21,340
것에 대한 직관을 얻는 좋은 방법입니다.

171
00:08:21,340 --> 00:08:22,180
좋습니다.

172
00:08:22,180 --> 00:08:24,500
지난 시간에 논의한 내용을

173
00:08:24,500 --> 00:08:27,220
고수준으로 요약한 것입니다.

174
00:08:27,220 --> 00:08:30,020
이제 이 강의의 새로운 내용에 대해

175
00:08:30,020 --> 00:08:34,043
좀 더 자세히 들어가겠지만, 지난 시간이나 이 강의

176
00:08:34,043 --> 00:08:35,460
시작 부분에서

177
00:08:35,460 --> 00:08:37,820
논의한 내용에 대해 질문이 있는

178
00:08:37,820 --> 00:08:41,039
분이 있다면 잠깐 멈추고 질문해 주세요.

179
00:08:41,039 --> 00:08:41,539
네.

180
00:08:41,539 --> 00:08:45,720
온라인에서의 질문은 이 시각적 관점이 k-최근접

181
00:08:45,720 --> 00:08:49,720
이웃을 실행하는 것과 같은지에 대한 것입니다.

182
00:08:49,720 --> 00:08:52,070
그리고 이것은 아마도 비교하고 있는

183
00:08:52,070 --> 00:08:53,590
이웃 중 하나일 것입니다.

184
00:08:53,590 --> 00:08:55,350
수학적으로 동등한가요?

185
00:08:55,350 --> 00:08:58,790
아니요, 이 템플릿은 이 선에서 형성되기

186
00:08:58,790 --> 00:09:02,910
때문에 동일하지 않습니다. 특정 데이터 포인트가

187
00:09:02,910 --> 00:09:03,750
아닙니다.

188
00:09:03,750 --> 00:09:09,070
하지만 여전히 이 템플릿을 기반으로 계산할 수

189
00:09:09,070 --> 00:09:11,990
있습니다. 이 다이어그램에서

190
00:09:11,990 --> 00:09:15,330
클래스 방향을 가리키는 선이 보이므로

191
00:09:15,330 --> 00:09:18,390
이 점을 나타낼 것입니다.

192
00:09:18,390 --> 00:09:22,950
네, 그래서 질문은 우리가 이 3,072 숫자를 어떻게 얻었는가입니다.

193
00:09:22,950 --> 00:09:27,830
여기서 아이디어는 이미지의 높이가 32픽셀이고 너비가 32픽셀이며,

194
00:09:27,830 --> 00:09:31,510
이미지의 각 위치가 빨강, 초록, 파랑 픽셀 강도의

195
00:09:31,510 --> 00:09:34,310
세 값으로 표현된다는 것입니다. 그러면

196
00:09:34,310 --> 00:09:38,950
전체 이미지를 나타내기 위해 32 곱하기 32 곱하기 3의 총

197
00:09:38,950 --> 00:09:40,450
값을 얻게 됩니다.

198
00:09:40,450 --> 00:09:44,190
그래서 이렇게 3,072 숫자를 얻습니다.

199
00:09:44,190 --> 00:09:47,790
여기 매우 구체적인 선형

200
00:09:47,790 --> 00:09:50,730
모델의 예가 있습니다.

201
00:09:50,730 --> 00:09:56,270
입력 x에 가중치 행렬 W를 곱하면 이 다양한 클래스에

202
00:09:56,270 --> 00:09:59,530
대한 결과 점수를 얻습니다.

203
00:09:59,530 --> 00:10:01,530
고양이에 대한 점수가 낮고 자동차가 더

204
00:10:01,530 --> 00:10:04,130
높은 점수를 받기 때문에 잘 작동하지 않습니다.

205
00:10:04,130 --> 00:10:07,570
우리는 올바른 클래스에 대해 가장 높은 점수를 원합니다.

206
00:10:07,570 --> 00:10:09,508
두 번째 예에서는 올바르게

207
00:10:09,508 --> 00:10:11,550
작동하므로 꽤 잘합니다.

208
00:10:11,550 --> 00:10:14,370
하지만 개구리 예제에서는 세 개 중 가장

209
00:10:14,370 --> 00:10:16,770
낮은 점수로 완전히 잘못되었습니다.

210
00:10:16,770 --> 00:10:20,210
직관적으로 이 점수가 그리 좋지 않다는 것을 알 수 있습니다.

211
00:10:20,210 --> 00:10:23,670
그런데 이 직관을 수학적으로 어떻게 형식화할 수 있을까요?

212
00:10:23,670 --> 00:10:26,690
주어진 모델이 얼마나 좋은지를 어떻게 판단할 수 있을까요?

213
00:10:26,690 --> 00:10:30,090
이것이 손실 함수의 개념으로, 분류기가 얼마나

214
00:10:30,090 --> 00:10:34,490
좋은지 또는 구체적으로 얼마나 나쁜지를 알려줍니다.

215
00:10:34,490 --> 00:10:39,530
예제 데이터 세트가 있고, 여기서 i로 인덱싱하면 Xi가

216
00:10:39,530 --> 00:10:43,490
각 훈련 예제, yi가 각 훈련

217
00:10:43,490 --> 00:10:45,930
레이블입니다. 전체 데이터 세트에

218
00:10:45,930 --> 00:10:49,360
대한 손실을 계산할 수 있습니다.

219
00:10:49,360 --> 00:10:53,900
각 훈련 예제를 모델을 통해 보내면서 이 손실을

220
00:10:53,900 --> 00:10:57,820
계산합니다. 이 모델은 XiW의 f입니다.

221
00:10:57,820 --> 00:10:59,080
레이블을 얻습니다.

222
00:10:59,080 --> 00:11:02,820
그리고 이를 실제 레이블 yi와 비교하여 계산합니다.

223
00:11:02,820 --> 00:11:05,060
그리고 전체 데이터 세트에 대해 평균을 취합니다.

224
00:11:05,060 --> 00:11:07,280
이렇게 진행합니다.

225
00:11:07,280 --> 00:11:10,700
지난 강의에서 분류를 위한 가장 일반적으로

226
00:11:10,700 --> 00:11:13,500
사용되는 손실인 소프트맥스 손실 또는 교차

227
00:11:13,500 --> 00:11:16,300
엔트로피 손실에 대해 이야기했습니다.

228
00:11:16,300 --> 00:11:18,800
그래서 여기서 다시 그렇게 자세히 논의하지는 않을 것입니다.

229
00:11:18,800 --> 00:11:22,880
기본적으로, 올바른 클래스의 확률이

230
00:11:22,880 --> 00:11:26,360
낮을 때 손실이 매우 큽니다.

231
00:11:26,360 --> 00:11:29,460
올바른 클래스를 매우 높은 확률로

232
00:11:29,460 --> 00:11:32,300
예측할 때 손실이 매우 낮습니다.

233
00:11:32,300 --> 00:11:37,620
제가 방금 설명한 내용은 우리가 데이터 손실이라고 부르는

234
00:11:37,620 --> 00:11:39,940
것에 모두 포함됩니다.

235
00:11:39,940 --> 00:11:43,300
이 손실은 모델 예측이 우리의 훈련 데이터와

236
00:11:43,300 --> 00:11:45,880
얼마나 잘 일치하는지를 알려줍니다.

237
00:11:45,880 --> 00:11:47,790
그리고 분명히, 우리는 이것이 매우 낮기를 원합니다.

238
00:11:47,790 --> 00:11:49,582
매우 낮다면, 우리의 모델이 훈련

239
00:11:49,582 --> 00:11:51,390
데이터를 잘 맞추고 있다는 의미입니다.

240
00:11:51,390 --> 00:11:54,770
하지만 오늘 논의할 두 번째

241
00:11:54,770 --> 00:11:58,670
요소는 손실 함수의 정규화 항입니다.

242
00:11:58,670 --> 00:12:02,110
이것은 모델이 훈련 데이터에서 너무

243
00:12:02,110 --> 00:12:05,852
잘하는 것을 방지하기 위한 것입니다.

244
00:12:05,852 --> 00:12:07,810
실제로 훈련 데이터에서 더 나쁜 성능을

245
00:12:07,810 --> 00:12:10,470
보이지만, 목표는 새로운 테스트 데이터나 보지 못한

246
00:12:10,470 --> 00:12:11,810
데이터에서 더 잘하는 것입니다.

247
00:12:11,810 --> 00:12:14,410
훈련 데이터에서는 더 나쁘고 테스트 세트에서는 더 좋습니다.

248
00:12:14,410 --> 00:12:16,403
이것이 정규화의 요점입니다.

249
00:12:16,403 --> 00:12:18,070
그리고 다음 슬라이드에서

250
00:12:18,070 --> 00:12:20,850
그것에 대한 많은 직관을 다룰 것입니다.

251
00:12:20,850 --> 00:12:23,730
하지만 고수준의 목표는 훈련 데이터에서 더 나쁘게 하고,

252
00:12:23,730 --> 00:12:26,850
테스트 데이터나 보지 못한 데이터에서 더 잘하는 것입니다.

253
00:12:26,850 --> 00:12:29,270
이것이 정규화의 요점입니다.

254
00:12:29,270 --> 00:12:31,110
네, 그래서 우리는 각 i번째

255
00:12:31,110 --> 00:12:33,870
훈련 예제에서 손실을 계산하고 있습니다.

256
00:12:33,870 --> 00:12:37,630
네, i번째 예제의 손실은 Xi와 yi를 사용합니다.

257
00:12:40,183 --> 00:12:40,850
이해가 되나요?

258
00:12:40,850 --> 00:12:42,490
여기서 i가 없을 수도 있습니다.

259
00:12:42,490 --> 00:12:46,910
하지만 이것은 i번째 손실을 말하는 것입니다.

260
00:12:46,910 --> 00:12:47,890
네.

261
00:12:47,890 --> 00:12:50,382
네, 당신이 묻는다면 보통 각 i에 대해 다른

262
00:12:50,382 --> 00:12:51,590
손실을 갖지 않습니다.

263
00:12:51,590 --> 00:12:52,770
네, 네.

264
00:12:52,770 --> 00:12:55,090
그래서 그냥-- 우리는 Li를 i번째

265
00:12:55,090 --> 00:12:57,703
훈련 예제의 손실로 설명했으므로 여기서

266
00:12:57,703 --> 00:12:58,870
사용하고 있습니다.

267
00:12:58,870 --> 00:13:02,370
하지만 네, i일 수도 있습니다.

268
00:13:02,370 --> 00:13:05,090
정규화에 대해 사람들은 보통 이렇게

269
00:13:05,090 --> 00:13:07,070
생각하는 직관을 가지고

270
00:13:07,070 --> 00:13:09,470
있습니다. 이것은 장난감 예제입니다.

271
00:13:09,470 --> 00:13:13,330
아이디어는 입력이 x이고 출력이 y인 이 점들에 어떤

272
00:13:13,330 --> 00:13:15,810
함수를 맞추고 싶다는 것입니다.

273
00:13:15,810 --> 00:13:20,930
그리고 당신은 f1과 f2라는 두 가지 다른 유형의 모델을 가지고 있다고
가정합니다.

274
00:13:20,930 --> 00:13:23,330
그리고 이 중 어떤 것이 더 나은지 결정하려고 합니다.

275
00:13:23,330 --> 00:13:26,230
f1은 모든 데이터 포인트를 통과하므로 훈련

276
00:13:26,230 --> 00:13:29,650
또는 데이터 손실이 매우 낮습니다. 왜냐하면 기본적으로

277
00:13:29,650 --> 00:13:33,530
완벽하게 수행하고 있기 때문입니다. 반면 f2는 모든 점을

278
00:13:33,530 --> 00:13:35,370
완벽하게 통과하지 않습니다.

279
00:13:35,370 --> 00:13:38,690
하지만 직관적으로, 아마도 f2가 우리가 전에 본

280
00:13:38,690 --> 00:13:42,330
적이 없는 새로운 데이터에서 테스트할 때 더 나은

281
00:13:42,330 --> 00:13:43,650
모델인 것 같습니다.

282
00:13:43,650 --> 00:13:47,260
그래서 정규화는 데이터를 너무 과도하게 맞추고

283
00:13:47,260 --> 00:13:49,680
싶지 않다는 직관을 포착하며,

284
00:13:49,680 --> 00:13:51,900
데이터에 덜 맞지만 더

285
00:13:51,900 --> 00:13:55,380
간단하거나 더 나은 선택이 되도록 하는

286
00:13:55,380 --> 00:13:59,060
다른 속성을 가진 모델이 더 나을 수 있습니다.

287
00:13:59,060 --> 00:14:02,220
그렇다면, 같은 분포 내의 새로운 데이터에 대해

288
00:14:02,220 --> 00:14:04,840
이 모델들이 어떻게 작동할지 물어본다면?

289
00:14:04,840 --> 00:14:07,680
f2가 모델링을 훨씬 더 잘 수행한다는 것을 알게 될 것입니다.

290
00:14:07,680 --> 00:14:09,810
그래서 여기서는 보이지 않는 데이터에서 더 잘 작동하고 있습니다.

291
00:14:12,420 --> 00:14:14,800
이전 예제에서 간단한 모델을

292
00:14:14,800 --> 00:14:17,300
선호하는 직관이 잘

293
00:14:17,300 --> 00:14:19,780
드러나는데, 이는 여러 경쟁

294
00:14:19,780 --> 00:14:23,642
가설이 있을 때 가장 간단한 것을 먼저

295
00:14:23,642 --> 00:14:25,100
선택하고, 그게 틀린

296
00:14:25,100 --> 00:14:27,552
것이 확실하다면 더 복잡한

297
00:14:27,552 --> 00:14:29,260
것을 시도해보라는

298
00:14:29,260 --> 00:14:31,240
오컴의 면도날과 같은

299
00:14:31,240 --> 00:14:33,440
철학적 아이디어입니다.

300
00:14:33,440 --> 00:14:34,982
하지만 이것은

301
00:14:34,982 --> 00:14:40,400
정규화가 유용할 수 있는 이유에 대한 직관일 수도 있습니다.

302
00:14:40,400 --> 00:14:42,260
그리고 제가 아직 언급하지 않은 이

303
00:14:42,260 --> 00:14:45,190
방정식에 대한 마지막 사항은 이 람다 매개변수입니다.

304
00:14:45,190 --> 00:14:47,790
이것은 정규화 강도로, 또 다른

305
00:14:47,790 --> 00:14:49,550
하이퍼파라미터입니다.

306
00:14:49,550 --> 00:14:52,310
따라서 최적의 람다를 설정하기 위해 훈련

307
00:14:52,310 --> 00:14:55,070
및 검증 세트를 사용할 수 있습니다.

308
00:14:55,070 --> 00:14:57,390
기본 아이디어는 0과 무한대 사이의

309
00:14:57,390 --> 00:14:59,910
부동 소수점으로 설정할 수 있다는

310
00:14:59,910 --> 00:15:02,870
것입니다. 여기서 0은 기본적으로

311
00:15:02,870 --> 00:15:06,170
정규화가 없음을 의미하며, 무한대까지 가능합니다.

312
00:15:06,170 --> 00:15:09,770
정규화가 점점 더 강해집니다.

313
00:15:09,770 --> 00:15:12,910
모델이 훈련 데이터에 맞추는

314
00:15:12,910 --> 00:15:14,790
것을 얼마나

315
00:15:14,790 --> 00:15:19,750
방지할지를 결정하는 조정 가능한 노브입니다.

316
00:15:19,750 --> 00:15:23,410
이제 정규화의 간단한 예를 살펴보겠습니다.

317
00:15:23,410 --> 00:15:26,670
여기 L2 정규화가 있습니다.

318
00:15:26,670 --> 00:15:30,910
기본적으로 가중치 행렬이 있고, 가중치

319
00:15:30,910 --> 00:15:33,370
행렬의 각 항을 제곱한

320
00:15:33,370 --> 00:15:35,390
후 모두 합칩니다.

321
00:15:35,390 --> 00:15:38,790
그것이 여기서 점수를 제공하고, 이를 lambda와

322
00:15:38,790 --> 00:15:40,830
곱한 후 총 손실에 추가합니다.

323
00:15:40,830 --> 00:15:42,760
이것이 L2 정규화입니다.

324
00:15:42,760 --> 00:15:44,263
L1 정규화는 매우

325
00:15:44,263 --> 00:15:46,680
유사하지만 제곱 대신 절대값을 취합니다.

326
00:15:46,680 --> 00:15:48,920
실제로 모델을 훈련할

327
00:15:48,920 --> 00:15:53,800
때 이 두 가지 정규화가 수행되는 방식에는 차이가

328
00:15:53,800 --> 00:15:55,180
있습니다.

329
00:15:55,180 --> 00:15:57,580
L2 정규화의 경우, 각 값을 제곱하기

330
00:15:57,580 --> 00:15:59,663
때문에 매우 작은 값, 예를

331
00:15:59,663 --> 00:16:02,080
들어 0.001과 같은 값이 있을

332
00:16:02,080 --> 00:16:04,020
때, 제곱하면 더 작아집니다.

333
00:16:04,020 --> 00:16:06,080
제곱하면 더 작아집니다.

334
00:16:06,080 --> 00:16:09,960
따라서 L2 정규화는 0에 가까운 매우 작은 값을

335
00:16:09,960 --> 00:16:11,813
허용합니다. 제곱하면 더

336
00:16:11,813 --> 00:16:12,980
작아지기 때문입니다.

337
00:16:12,980 --> 00:16:16,740
따라서 L2에서는 이러한 매우 작은 값이 있을

338
00:16:16,740 --> 00:16:19,000
경우 벌점이 매우 낮습니다.

339
00:16:19,000 --> 00:16:20,860
반면 L1은 제곱하지 않습니다.

340
00:16:20,860 --> 00:16:23,503
기본값이 그대로입니다.

341
00:16:23,503 --> 00:16:24,920
정규화 항을

342
00:16:24,920 --> 00:16:28,580
계산하기 전에 더 작아지는 것이 아닙니다.

343
00:16:28,580 --> 00:16:31,580
실제로 이것이 L1 정규화로 이어집니다.

344
00:16:31,580 --> 00:16:34,320
가중치 행렬에서 0인 값이 훨씬 더

345
00:16:34,320 --> 00:16:36,900
많고, 실제로 0에 매우 가깝습니다.

346
00:16:36,900 --> 00:16:39,880
반면 L2는 일반적으로 작지만

347
00:16:39,880 --> 00:16:44,140
0이 아닌 값이 더 퍼져 있습니다. 벌점이

348
00:16:44,140 --> 00:16:46,820
매우 작아지기 때문입니다.

349
00:16:46,820 --> 00:16:50,340
L2가 모두 작고 퍼져 있는 가중치를 선호하는 이유는

350
00:16:50,340 --> 00:16:51,480
명확해 보입니다.

351
00:16:51,480 --> 00:16:55,040
하지만 L1이 희소 벡터를 선호하는 이유는 무엇인가요?

352
00:16:55,040 --> 00:16:59,380
값이 0이 될 수 있고 성능이 대체로 동일하다면,

353
00:16:59,380 --> 00:17:01,980
이는 해당 값을 0으로

354
00:17:01,980 --> 00:17:05,560
만드는 방향으로 밀어줄 것입니다.

355
00:17:05,560 --> 00:17:07,540
반면 L2의 경우, 값이

356
00:17:07,540 --> 00:17:10,140
제곱으로 인해 매우 작지만 0이 아닌

357
00:17:10,140 --> 00:17:12,020
상태가 될 수 있습니다.

358
00:17:12,020 --> 00:17:14,780
따라서 질문은 0 값으로 밀어주는 것이 무엇을

359
00:17:14,780 --> 00:17:16,940
의미하는지에 대해 이야기할 수 있을까요?

360
00:17:16,940 --> 00:17:19,660
그래서 우리는 이 손실 항을 어떻게 사용하는지에

361
00:17:19,660 --> 00:17:21,740
대해 더 이야기할 것입니다.

362
00:17:21,740 --> 00:17:24,780
하지만 기본 아이디어는 이를 최소화하려고 한다는 것입니다.

363
00:17:24,780 --> 00:17:26,500
그래서 우리는 손실을

364
00:17:26,500 --> 00:17:29,160
최소화하거나 모델의 오류를 최소화하려고 합니다.

365
00:17:29,160 --> 00:17:31,940
그리고 여기에서 긍정적인 값을

366
00:17:31,940 --> 00:17:35,540
주는 항이 모델 성능과 데이터 손실에 영향을

367
00:17:35,540 --> 00:17:37,700
미치지 않는다면, 우리는

368
00:17:37,700 --> 00:17:41,510
최적화 절차를 통해 그것들을 제거할 것입니다.

369
00:17:41,510 --> 00:17:42,370
이것은 트레이드오프입니다.

370
00:17:42,370 --> 00:17:44,950
당신은 정규화 항과 데이터 손실

371
00:17:44,950 --> 00:17:47,490
항의 합을 최적화하려고 합니다.

372
00:17:47,490 --> 00:17:49,550
그래서 데이터 손실이 크게 변하지

373
00:17:49,550 --> 00:17:52,913
않지만 정규화 항을 더 낮출 수 있다면, 더 최적화된

374
00:17:52,913 --> 00:17:54,330
모델을 얻을 수 있습니다.

375
00:17:54,330 --> 00:17:57,430
그래서 전체 항을 최소화하려고

376
00:17:57,430 --> 00:18:00,630
하는 것에 기반하여 선호될 것입니다.

377
00:18:00,630 --> 00:18:03,510
그래서 저는 이 과정에서 훈련 데이터에서

378
00:18:03,510 --> 00:18:06,810
최악의 성능을 내고 테스트 데이터에서 더 잘하기 위한

379
00:18:06,810 --> 00:18:09,350
정규화의 훨씬 더 복잡한 형태에

380
00:18:09,350 --> 00:18:11,870
대해서도 나중에 다룰 것이라고 생각합니다.

381
00:18:11,870 --> 00:18:15,370
하지만 그 중 일부는 모델의 레이어를 변경하기도

382
00:18:15,370 --> 00:18:17,850
하여 실제로 꽤 복잡해질 수 있습니다.

383
00:18:17,850 --> 00:18:21,830
이것은 모델을 정규화하는 방법에 대한 지속적인 연구 분야입니다.

384
00:18:21,830 --> 00:18:24,910
매년 새로운 논문이 나오므로 여기에 많은 내용이 있습니다.

385
00:18:24,910 --> 00:18:29,310
이 과정에서는 작은 부분만 다룰 것입니다.

386
00:18:29,310 --> 00:18:32,670
그래서 요약하자면, 왜 우리는 모델을 정규화합니까?

387
00:18:32,670 --> 00:18:35,870
첫 번째는 우리가 가중치에 대한 어떤 선호를 표현할 수 있게

388
00:18:35,870 --> 00:18:36,570
해준다는 것입니다.

389
00:18:36,570 --> 00:18:38,740
그래서 우리의 문제에서 어떤

390
00:18:38,740 --> 00:18:40,760
이유로 해결책이 퍼져 있어야 하거나

391
00:18:40,760 --> 00:18:42,372
가중치 행렬의 많은

392
00:18:42,372 --> 00:18:44,580
값이 0인 많은 희소성을 포함해야

393
00:18:44,580 --> 00:18:48,480
한다고 생각한다면, 우리는 L2 정규화 세트를 L1보다

394
00:18:48,480 --> 00:18:50,040
선호할 수 있습니다.

395
00:18:50,040 --> 00:18:52,740
또한 정규화 방법에 따라 모델을 단순화하여

396
00:18:52,740 --> 00:18:56,420
테스트 데이터에서 더 잘 작동하도록 만들 수 있습니다.

397
00:18:56,420 --> 00:18:58,960
예를 들어, 제가 이전에 보여준

398
00:18:58,960 --> 00:19:02,980
것처럼 모델에서 매우 높은 다항식 항을 강하게

399
00:19:02,980 --> 00:19:06,203
정규화한다면 모델을 단순화할 수 있습니다.

400
00:19:06,203 --> 00:19:08,120
우리가 너무 자세히 다루지

401
00:19:08,120 --> 00:19:10,720
않을 내용 중 하나는 특히 L2 정규화가

402
00:19:10,720 --> 00:19:12,460
최적화 과정을 실제로 개선할

403
00:19:12,460 --> 00:19:17,240
수 있다는 것입니다. 왜냐하면 이 제곱이 포물선과 같다고 상상해 보세요.

404
00:19:17,240 --> 00:19:20,300
y = x 제곱을 플로팅하면 포물선이 됩니다.

405
00:19:20,300 --> 00:19:23,720
그리고 이것들은 볼록하므로 전역 최소값이 있는 많은

406
00:19:23,720 --> 00:19:25,517
좋은 최적화 속성을 얻습니다.

407
00:19:25,517 --> 00:19:27,100
이 과정에서는 그 내용을 다루지 않을 것입니다.

408
00:19:27,100 --> 00:19:28,260
그것은 범위를 넘어섭니다.

409
00:19:28,260 --> 00:19:30,960
하지만 특정 유형의 최적화에서는 정규화가 모델을 더 빠르게

410
00:19:30,960 --> 00:19:33,340
훈련하는 데 실제로 도움이 된다는 것을 알아두세요.

411
00:19:36,480 --> 00:19:38,470
여러분에게 질문이 있습니다.

412
00:19:38,470 --> 00:19:44,050
W1이면 1, W2이면 손으로 2를

413
00:19:44,050 --> 00:19:45,690
하세요.

414
00:19:45,690 --> 00:19:51,290
L2 정규화기가 선호하는 두 가중치 W1과

415
00:19:51,290 --> 00:19:54,770
W2 중 어느 것인가요?

416
00:19:54,770 --> 00:19:57,267
입력 x가 있습니다.

417
00:19:57,267 --> 00:19:59,850
곱할 때 가중치와의 내적을 수행하면 동일한

418
00:19:59,850 --> 00:20:02,690
점수를 얻으므로 어떤 방식으로든 점수 1을 얻습니다.

419
00:20:02,690 --> 00:20:05,210
여기서 데이터 손실은 동일할 것입니다.

420
00:20:05,210 --> 00:20:08,370
우리는 정규화기가 선호할 가중치를

421
00:20:08,370 --> 00:20:10,450
결정하려고 합니다.

422
00:20:10,450 --> 00:20:15,610
W1이라고 생각하면 1로 가고 W2라고 생각하면 2로 가세요.

423
00:20:15,610 --> 00:20:16,110
좋아요.

424
00:20:16,110 --> 00:20:16,710
2가 많습니다.

425
00:20:16,710 --> 00:20:19,150
네, W2인 이유는 더 퍼져 있기 때문입니다.

426
00:20:19,150 --> 00:20:21,775
각 턴을 제곱할 것이므로 1/4를 제곱하면

427
00:20:21,775 --> 00:20:22,830
1/16이 됩니다.

428
00:20:22,830 --> 00:20:27,050
모두 합치면 여기서 총 정규화 항은 1/4이고,

429
00:20:27,050 --> 00:20:29,990
여기서 제곱하므로 1이 됩니다.

430
00:20:29,990 --> 00:20:32,770
따라서 정규화 손실

431
00:20:32,770 --> 00:20:35,003
측면에서 4배 낮습니다.

432
00:20:35,003 --> 00:20:36,670
그리고 우리가 말했듯이 직관은 더

433
00:20:36,670 --> 00:20:38,535
퍼진 가중치를 선호한다는 것입니다.

434
00:20:38,535 --> 00:20:39,910
그리고 여기 또 다른 질문이

435
00:20:39,910 --> 00:20:42,190
있습니다. L1은 이제 어떤 것을 선호할까요?

436
00:20:42,190 --> 00:20:45,760
가중치가 1이면 1, 가중치가 2이면 2를 합니다.

437
00:20:49,390 --> 00:20:51,110
우리는 1이 많습니다.

438
00:20:51,110 --> 00:20:53,170
그래서 이건 사실 약간의 트릭 질문입니다.

439
00:20:53,170 --> 00:20:56,210
L1 정규화는 각 항을 합산하므로

440
00:20:56,210 --> 00:20:58,262
둘 다 1로 합산됩니다.

441
00:20:58,262 --> 00:21:00,470
실제로는 우리가 말했듯이 희소성을 고려할 때 이

442
00:21:00,470 --> 00:21:01,610
경우를 볼 수 있을 것입니다.

443
00:21:01,610 --> 00:21:03,230
하지만 손실 관점에서

444
00:21:03,230 --> 00:21:07,110
이 두 가중치는 L1 측면에서 실제로 동등합니다.

445
00:21:07,110 --> 00:21:12,210
왜냐하면 1은 0.25를 네 번 더한 것이고, 다른 하나는

446
00:21:12,210 --> 00:21:14,090
단순히 1이기 때문입니다.

447
00:21:14,090 --> 00:21:15,850
그래서 둘 다 1로 합산됩니다.

448
00:21:15,850 --> 00:21:21,070
따라서 실제 정규화 항은 동일합니다.

449
00:21:21,070 --> 00:21:21,690
네, 알겠습니다.

450
00:21:21,690 --> 00:21:24,190
그렇다면 L1이 선호되는

451
00:21:24,190 --> 00:21:27,670
예는 0.9와 같은 경우일까요?

452
00:21:27,670 --> 00:21:32,950
요약하자면, 우리는 x,y 쌍의 데이터 세트를 가지고 있습니다.

453
00:21:32,950 --> 00:21:35,320
그리고 우리는 각 클래스에 대한

454
00:21:35,320 --> 00:21:38,960
점수를 계산할 방법이 있으며, 우리 경우는 단순히

455
00:21:38,960 --> 00:21:40,320
선형 모델입니다.

456
00:21:40,320 --> 00:21:43,200
행렬 곱셈을 하고 있습니다.

457
00:21:43,200 --> 00:21:47,402
우리가 지난 시간에 논의한 Softmax

458
00:21:47,402 --> 00:21:48,860
손실에서 각 훈련

459
00:21:48,860 --> 00:21:53,040
예제의 손실은 각 점수를 지수화한 다음

460
00:21:53,040 --> 00:21:56,280
점수의 총합으로 나누는 것입니다.

461
00:21:56,280 --> 00:21:59,040
그래서 모두 양수로 만들기 위해 지수화합니다.

462
00:21:59,040 --> 00:22:01,260
그리고 확률 분포를 얻기 위해 합산합니다.

463
00:22:01,260 --> 00:22:05,360
따라서 이 모든 최종 값은 1로 합산됩니다.

464
00:22:05,360 --> 00:22:06,860
각 클래스에 대한 점수가 있습니다.

465
00:22:06,860 --> 00:22:09,900
그리고 올바른 레이블의 마이너스 로그를 취합니다.

466
00:22:09,900 --> 00:22:12,700
따라서 이것은 여기 주어진 올바른

467
00:22:12,700 --> 00:22:14,880
레이블의 확률입니다.

468
00:22:14,880 --> 00:22:16,520
전체 손실은 각 훈련

469
00:22:16,520 --> 00:22:19,320
예제에 대해 이 작업을 수행하고, 각

470
00:22:19,320 --> 00:22:21,120
Li를 계산한 다음,?

471
00:22:21,120 --> 00:22:24,160
그런 다음 모델의 가중치에 따라

472
00:22:24,160 --> 00:22:27,640
여기에서 정규화 항을 추가합니다.

473
00:22:27,640 --> 00:22:29,600
일반적으로 Softmax를 사용하는 이유는 무엇인가요?

474
00:22:29,600 --> 00:22:32,440
Softmax는 어떤 부동

475
00:22:32,440 --> 00:22:35,450
소수점 숫자 집합을 1로 합산되는

476
00:22:35,450 --> 00:22:38,650
확률 분포로 변환하는 함수이기

477
00:22:38,650 --> 00:22:40,830
때문에 훌륭합니다.

478
00:22:40,830 --> 00:22:43,610
점수의 값에 따라 그

479
00:22:43,610 --> 00:22:45,930
값의 상대적 확률로

480
00:22:45,930 --> 00:22:46,870
변환됩니다.

481
00:22:46,870 --> 00:22:48,850
그래서 만약 정말 높은 양수와

482
00:22:48,850 --> 00:22:51,970
나머지가 매우 낮은 음수라면, Softmax는

483
00:22:51,970 --> 00:22:54,950
거의 1에 가깝고 다른 값들은 거의 0이 됩니다.

484
00:22:54,950 --> 00:22:59,930
그래서 이는 리스트의 값에 따라 부동 소수점 숫자의

485
00:22:59,930 --> 00:23:01,970
리스트를 확률의

486
00:23:01,970 --> 00:23:05,430
리스트로 변환하기 때문에 좋습니다.

487
00:23:05,430 --> 00:23:07,930
이것이 Softmax의 주요 유용성입니다.

488
00:23:07,930 --> 00:23:10,970
그래서 질문은 우리가 이야기한 정규화,

489
00:23:10,970 --> 00:23:14,930
즉 L1, L2를 가중치의 크기를 기반으로 정규화하는

490
00:23:14,930 --> 00:23:17,770
방법으로 볼 수 있다는 것입니다.

491
00:23:17,770 --> 00:23:20,110
그렇다면 이것이 더 간단한 모델로 어떻게 연결될까요?

492
00:23:20,110 --> 00:23:22,510
L1의 설명은 사실 꽤

493
00:23:22,510 --> 00:23:28,110
간단하다고 생각하는데, 많은 0을 포함하는 항을 선호한다면, 기본적으로

494
00:23:28,110 --> 00:23:30,910
계수가 적은 선형 모델입니다.

495
00:23:30,910 --> 00:23:32,390
그래서 그것은 사실 상대적으로

496
00:23:32,390 --> 00:23:33,515
간단하다고 생각합니다.

497
00:23:33,515 --> 00:23:35,230
하지만 일반적으로 정규화는

498
00:23:35,230 --> 00:23:38,010
항상 더 간단한 모델을 제공하지는 않을 것입니다.

499
00:23:38,010 --> 00:23:39,390
어떻게 사용되느냐에 따라 다릅니다.

500
00:23:39,390 --> 00:23:43,430
예를 들어, 여기 처음에 보여준 다이어그램에서

501
00:23:43,430 --> 00:23:48,430
L2 정규화 또는 L1 정규화가 있을 수 있으며, 함수의

502
00:23:48,430 --> 00:23:51,050
고차 다항식 항에 대해 더

503
00:23:51,050 --> 00:23:53,850
많은 패널티를 부여하고 있습니다.

504
00:23:53,850 --> 00:23:55,310
그런 의미에서, 더 간단한

505
00:23:55,310 --> 00:23:56,768
모델을 선호하도록 정규화를

506
00:23:56,768 --> 00:23:58,410
설계할 수 있는 방법이 명확합니다.

507
00:23:58,410 --> 00:24:00,250
하지만 항상 그렇게 될 필요는 없습니다.

508
00:24:00,250 --> 00:24:03,510
실제로, 훈련 데이터에서 더 나쁘게 수행하여 테스트 데이터에서

509
00:24:03,510 --> 00:24:05,170
더 잘 수행하는 아이디어이며,

510
00:24:05,170 --> 00:24:07,735
이는 항상 더 간단한 모델을 제공하지는 않습니다.

511
00:24:07,735 --> 00:24:09,110
사실, 드롭아웃과

512
00:24:09,110 --> 00:24:13,030
같은 많은 유형의 정규화가 있어 모델을 더 복잡하게

513
00:24:13,030 --> 00:24:14,870
만들지만 테스트 데이터에서

514
00:24:14,870 --> 00:24:17,460
더 나은 성능을 제공합니다.

515
00:24:22,950 --> 00:24:24,170
좋습니다.

516
00:24:24,170 --> 00:24:25,870
이제 주어진 W의

517
00:24:25,870 --> 00:24:30,150
품질을 훈련 데이터와 이 정규화 항을 기반으로

518
00:24:30,150 --> 00:24:34,740
계산하는 방법에 대해 이야기했으니, 이제 질문은 실제로

519
00:24:34,740 --> 00:24:37,920
최상의 W를 어떻게 찾느냐입니다.

520
00:24:37,920 --> 00:24:39,860
이것이 최적화이며,

521
00:24:39,860 --> 00:24:42,600
오늘 강의의 두 번째 부분입니다.

522
00:24:42,600 --> 00:24:45,920
사람들이 최적화를 설명할 때, 일반적으로

523
00:24:45,920 --> 00:24:48,960
손실 경관이라는 아이디어를 사용합니다.

524
00:24:48,960 --> 00:24:51,340
이는 지구의 일반적인

525
00:24:51,340 --> 00:24:54,960
경관처럼 생각할 수 있으며, 수직 또는 z축

526
00:24:54,960 --> 00:24:56,680
방향이 손실입니다.

527
00:24:56,680 --> 00:24:58,900
이것이 최소화하려는 값입니다.

528
00:24:58,900 --> 00:25:00,358
그리고 이 예에서

529
00:25:00,358 --> 00:25:02,420
모델에 두 개의 매개변수가 있으며,

530
00:25:02,420 --> 00:25:04,240
이는 이 경관에서의

531
00:25:04,240 --> 00:25:05,780
x 및 y 방향입니다.

532
00:25:05,780 --> 00:25:08,140
아이디어는 기본적으로 당신이 사람이라는 것입니다.

533
00:25:08,140 --> 00:25:09,640
당신은 이 경관을

534
00:25:09,640 --> 00:25:12,223
돌아다니며 경관에서 가장 작거나 낮은

535
00:25:12,223 --> 00:25:13,760
지점을 찾으려고 합니다.

536
00:25:13,760 --> 00:25:15,740
이 비유가 흔히 사용되는

537
00:25:15,740 --> 00:25:17,920
비유 중 하나인 이유 중 하나는

538
00:25:17,920 --> 00:25:20,400
인간으로서 우리는 시각적으로

539
00:25:20,400 --> 00:25:22,360
멀리서 계곡의 가장 낮은

540
00:25:22,360 --> 00:25:24,633
지점을 볼 수 있기 때문입니다.

541
00:25:24,633 --> 00:25:26,800
하지만 이 비유는 사실 그 사람이 눈이

542
00:25:26,800 --> 00:25:28,980
가려진 상태라고 생각하면 꽤 정확합니다.

543
00:25:28,980 --> 00:25:31,390
그들은 어떤 시각적 정보에도 접근할 수 없습니다.

544
00:25:31,390 --> 00:25:34,130
그들은 지금 서 있는

545
00:25:34,130 --> 00:25:36,192
지점에서 땅의 기울기만

546
00:25:36,192 --> 00:25:38,150
느낄 수 있습니다.

547
00:25:38,150 --> 00:25:39,650
그 관점에서 보면,

548
00:25:39,650 --> 00:25:43,570
이 비유는 우리가 최상의 모델을 찾으려는 방식에 대해

549
00:25:43,570 --> 00:25:44,830
매우 정확해집니다.

550
00:25:44,830 --> 00:25:47,810
우리는 모델의 매개변수에 따라 다양한

551
00:25:47,810 --> 00:25:50,530
손실 값의 복잡한 경관을 가지고

552
00:25:50,530 --> 00:25:55,290
있으며, 이는 이 경관에서 사람의 위치로 변환됩니다.

553
00:25:55,290 --> 00:25:58,170
그렇다면 최상의 지점을 어떻게 찾을 수 있을까요?

554
00:25:58,170 --> 00:26:03,626
우리는 정말 간단한 아이디어를 사용할 수 있습니다. 아마도 정말 나쁜
아이디어일 수

555
00:26:03,626 --> 00:26:05,553
있지만, 작동할 수 있습니다.

556
00:26:05,553 --> 00:26:07,970
여기서는 기본적으로 1,000개의 서로

557
00:26:07,970 --> 00:26:11,010
다른 W 값을 무작위로 시도하고 가장 좋은

558
00:26:11,010 --> 00:26:13,090
값을 선택하는 for 루프입니다.

559
00:26:13,090 --> 00:26:15,910
명백히 수학적으로 엄밀하지는

560
00:26:15,910 --> 00:26:19,830
않지만, 무작위 기준선보다는 나을 것입니다.

561
00:26:19,830 --> 00:26:24,330
다른 선택지가 없다면, 이 정도도 나쁘지 않을 수 있습니다.

562
00:26:24,330 --> 00:26:29,580
CIFAR-10 데이터 세트에서 15.5%의 정확도를 얻을 수 있습니다. 이
데이터

563
00:26:29,580 --> 00:26:32,460
세트는 제가 이전에 개구리와 자동차 등으로 보여준

564
00:26:32,460 --> 00:26:35,465
10개의 서로 다른 카테고리로 구성되어 있습니다.

565
00:26:35,465 --> 00:26:36,840
하지만 성능이 좋지 않습니다.

566
00:26:36,840 --> 00:26:37,960
이 데이터 세트의

567
00:26:37,960 --> 00:26:40,500
현재 상태는 현대 딥러닝을 통해 기본적으로

568
00:26:40,500 --> 00:26:42,840
해결되었으며, 99.7%의 정확도를 얻습니다.

569
00:26:42,840 --> 00:26:46,340
분명히 나쁘지는 않지만, 특히

570
00:26:46,340 --> 00:26:48,940
좋다고는 할 수 없습니다.

571
00:26:48,940 --> 00:26:54,120
전략 2는 제가 아마 조금 전에 설명한 것처럼 경사를

572
00:26:54,120 --> 00:26:56,540
따르는 아이디어입니다.

573
00:26:56,540 --> 00:27:00,512
이를 위해, 당신은 잃어버린 풍경에서 눈이 가려진 상태로

574
00:27:00,512 --> 00:27:02,220
땅을 느끼고 있다고

575
00:27:02,220 --> 00:27:03,580
상상할 수 있습니다.

576
00:27:03,580 --> 00:27:06,620
그리고 당신은 '지구의 경사가 나를 어느 방향으로

577
00:27:06,620 --> 00:27:09,020
가리키고 있는가?'라고 생각합니다.

578
00:27:09,020 --> 00:27:12,120
그리고 항상 그 방향으로 걸어야 합니다.

579
00:27:12,120 --> 00:27:14,500
이 기본 아이디어는 이 과정에서 모든

580
00:27:14,500 --> 00:27:16,615
모델을 훈련시키는 근본적인 방법이며,

581
00:27:16,615 --> 00:27:18,740
기본적으로 모든 딥러닝 모델이

582
00:27:18,740 --> 00:27:20,900
훈련되는 방식입니다. 현재

583
00:27:20,900 --> 00:27:22,780
위치에서 손실 풍경의 위치를 느끼고

584
00:27:22,780 --> 00:27:24,683
언덕을 내려가는 것입니다.

585
00:27:24,683 --> 00:27:26,600
그래서 이것은 매우 직관적인 설명 방법입니다.

586
00:27:26,600 --> 00:27:29,580
이제 그 뒤에 있는 수학을 더 살펴보겠지만,

587
00:27:29,580 --> 00:27:34,000
이것이 당신이 머릿속에 시각화해야 할 것입니다.

588
00:27:34,000 --> 00:27:36,380
그렇다면 실제로 경사를 어떻게 따를 수 있을까요?

589
00:27:36,380 --> 00:27:38,360
1차원에서는 미분의 개념에

590
00:27:38,360 --> 00:27:41,840
익숙할 것이라고 확신합니다. 미적분학에서

591
00:27:41,840 --> 00:27:45,680
우리는 현재 위치에 아주 작은 수를 더하고, 그

592
00:27:45,680 --> 00:27:48,740
새로운 위치에서 함수의 값을 계산한

593
00:27:48,740 --> 00:27:51,500
다음, 현재 위치를 빼고, 단계 크기로

594
00:27:51,500 --> 00:27:53,580
나누는 한계 h 정의로

595
00:27:53,580 --> 00:27:55,300
생각할 수 있습니다.

596
00:27:55,300 --> 00:27:58,620
h가 0에 접근할 때 이

597
00:27:58,620 --> 00:28:05,160
한계는 그 점에서 함수의 미분을 제공합니다.

598
00:28:05,160 --> 00:28:08,040
이것은 1D에 대한 것이지만,

599
00:28:08,040 --> 00:28:11,240
다차원에서는 기울기를 사용하여

600
00:28:11,240 --> 00:28:16,263
각 값에 대해 이 한계 정의를 별도로 계산합니다.

601
00:28:16,263 --> 00:28:17,680
따라서 각 값에

602
00:28:17,680 --> 00:28:20,280
대해 다른 미분이 있습니다.

603
00:28:20,280 --> 00:28:23,280
그리고 대신 벡터를 얻습니다.

604
00:28:23,280 --> 00:28:26,810
이것은 각 차원에 따른 방향을 제공합니다.

605
00:28:26,810 --> 00:28:29,610
따라서 기울기를 계산할 수

606
00:28:29,610 --> 00:28:33,890
있습니다. 기울기는 기울기와 방향의 내적을 취하여

607
00:28:33,890 --> 00:28:36,890
계산할 수 있으며, 특히 가장 가파른

608
00:28:36,890 --> 00:28:40,012
하강 방향은 음의 기울기입니다.

609
00:28:40,012 --> 00:28:41,470
기울기는 언덕을 향해 가리킵니다.

610
00:28:41,470 --> 00:28:43,750
음의 기울기는 언덕 아래를 가리킵니다.

611
00:28:43,750 --> 00:28:45,187
따라서 우리는 이

612
00:28:45,187 --> 00:28:47,770
잃어버린 풍경의 바닥에 도달하려고

613
00:28:47,770 --> 00:28:50,210
할 때 이동해야 할 방향입니다.

614
00:28:50,210 --> 00:28:53,047
그렇다면 미분을 계산할 수 있는 방법은 무엇일까요?

615
00:28:53,047 --> 00:28:55,130
정말 간단한 방법은 아주 작은

616
00:28:55,130 --> 00:28:58,210
h를 사용하여 한계 h 정의를 실제로 시도하는 것입니다.

617
00:28:58,210 --> 00:29:01,170
예를 들어 0.0001을 더하는 것입니다.

618
00:29:01,170 --> 00:29:04,182
실제로 손실이 약간 변한 마지막 두 자리를

619
00:29:04,182 --> 00:29:05,390
계산할 수 있습니다.

620
00:29:05,390 --> 00:29:07,110
따라서 차이를 계산하고

621
00:29:07,110 --> 00:29:09,970
단계 크기로 나누면 여기서 미분의 근사값을

622
00:29:09,970 --> 00:29:11,390
얻을 수 있습니다.

623
00:29:11,390 --> 00:29:15,090
그리고 W의 각 값에 대해 실제로

624
00:29:15,090 --> 00:29:19,010
이 작업을 수행할 수 있습니다. 이 절차를 반복해서 수행하면 됩니다.

625
00:29:19,010 --> 00:29:20,350
하지만 몇 가지 문제가 있습니다.

626
00:29:20,350 --> 00:29:21,850
각 값을 반복해야

627
00:29:21,850 --> 00:29:23,330
하므로 매우 느립니다.

628
00:29:23,330 --> 00:29:26,060
이것은 또한 근사값이므로 실제 도함수를 계산하는

629
00:29:26,060 --> 00:29:27,160
것이 아닙니다.

630
00:29:27,160 --> 00:29:29,280
특히 부동 소수점 산술에서는 꽤

631
00:29:29,280 --> 00:29:32,240
중요한 오류가 발생할 수 있으므로, 이는

632
00:29:32,240 --> 00:29:34,120
실제로 선호되지 않습니다.

633
00:29:34,120 --> 00:29:37,260
하지만 우리가 할 수 있는 기본적인 아이디어나

634
00:29:37,260 --> 00:29:40,400
직관은 이렇게 도함수를 계산하는 것입니다.

635
00:29:40,400 --> 00:29:45,060
하지만 실제로 우리는 W의 함수로서 손실을 가지고 있습니다.

636
00:29:45,060 --> 00:29:49,100
따라서 모델의 함수에 의해 주어진 손실을 얻기

637
00:29:49,100 --> 00:29:53,180
위해 점수를 계산하는 방법을 알고 있습니다.

638
00:29:53,180 --> 00:29:55,220
그런 다음 정규화 항을 포함하여

639
00:29:55,220 --> 00:29:57,680
총 손실을 계산할 수 있습니다.

640
00:29:57,680 --> 00:30:03,580
이 전체 손실은 기본적으로 W, Xi 및

641
00:30:03,580 --> 00:30:05,762
yi의 함수입니다.

642
00:30:05,762 --> 00:30:07,220
따라서 W 행렬이 있고,

643
00:30:07,220 --> 00:30:09,762
Xi와 yi가 있으며, 로그와 지수가 포함된 이

644
00:30:09,762 --> 00:30:10,960
공식을 가지고 있습니다.

645
00:30:10,960 --> 00:30:16,700
근본적으로 이것은 W, X 및 y의 함수입니다.

646
00:30:16,700 --> 00:30:19,860
우리는 특히 가중치에

647
00:30:19,860 --> 00:30:22,740
대한 손실의 그라디언트를

648
00:30:22,740 --> 00:30:25,130
계산하고자 합니다.

649
00:30:25,130 --> 00:30:29,810
따라서 Xi와 yi가 일정하게 유지된다고 상상할 수 있습니다.

650
00:30:29,810 --> 00:30:32,310
그리고 우리는 가중치에 대해서만

651
00:30:32,310 --> 00:30:34,910
도함수를 계산하려고 합니다.

652
00:30:34,910 --> 00:30:38,445
이를 위해 우리는 미적분학을 사용하고, 연쇄 법칙을

653
00:30:38,445 --> 00:30:40,070
사용하며, 복잡한 방정식이나

654
00:30:40,070 --> 00:30:45,230
덜 복잡한 방정식에 따라 도함수를 계산하기 위해 배운 다양한 방법을 사용할

655
00:30:45,230 --> 00:30:46,250
수 있습니다.

656
00:30:46,250 --> 00:30:48,750
하지만 이를 해결하기 위해서는 로그와

657
00:30:48,750 --> 00:30:51,030
지수 및 연쇄 법칙이 필요합니다.

658
00:30:51,030 --> 00:30:53,250
이것은 숙제의 연습이 될 것입니다.

659
00:30:53,250 --> 00:30:54,970
따라서 지금 단계별로 어떻게 하는지 설명하지는 않겠습니다.

660
00:30:54,970 --> 00:30:56,090
하지만 상대적으로 간단합니다.

661
00:30:56,090 --> 00:30:58,150
개념적으로 여러분 모두가 이것을 어떻게 하는지 이해할 수 있을

662
00:30:58,150 --> 00:30:58,650
것이라고 생각합니다.

663
00:30:58,650 --> 00:31:00,442
X와 y가 상수라고 가정합니다.

664
00:31:00,442 --> 00:31:04,170
그런 다음 W가 변할 때의 미분을 구합니다.

665
00:31:04,170 --> 00:31:06,270
그래서 이제 우리는

666
00:31:06,270 --> 00:31:10,310
데이터와 현재 W에 따라 W, 즉 dW,

667
00:31:10,310 --> 00:31:13,550
W의 기울기를 계산할 수 있는

668
00:31:13,550 --> 00:31:17,550
방법이 있습니다. 그리고 우리의 손실 함수가

669
00:31:17,550 --> 00:31:20,630
오류를 계산하는 방법입니다.

670
00:31:20,630 --> 00:31:23,130
그래서 이것은 요약이라고 할 수 있습니다.

671
00:31:23,130 --> 00:31:25,510
수치적 기울기를 구할 수 있지만

672
00:31:25,510 --> 00:31:27,110
근사적이고 느립니다.

673
00:31:27,110 --> 00:31:29,710
그리고 좋은 점은 작성하기가 매우 쉽다는 것입니다.

674
00:31:29,710 --> 00:31:33,970
정말 작은 h를 추가하고 차이를 구한 후 h로 나누기만 하면 됩니다.

675
00:31:33,970 --> 00:31:36,390
해석적 기울기는 정확하기 때문에 좋습니다.

676
00:31:36,390 --> 00:31:37,850
빠르지만, 만약 처음부터 새로운

677
00:31:37,850 --> 00:31:40,390
기울기를 만드는 경우, 즉 처음부터 계산하는

678
00:31:40,390 --> 00:31:41,910
새로운 코드를 작성하는 경우,

679
00:31:41,910 --> 00:31:43,160
오류가 발생할 수 있습니다.

680
00:31:43,160 --> 00:31:44,930
그래서 이렇게 하는 경우,

681
00:31:44,930 --> 00:31:46,730
사람들은 일반적으로 기울기 검사를

682
00:31:46,730 --> 00:31:50,410
수행하는데, 이는 매우 작은 h 값을 가진 h 버전을 시도하고,

683
00:31:50,410 --> 00:31:52,262
그 값이 비슷한 이웃에 있는지

684
00:31:52,262 --> 00:31:53,470
확인하는 것입니다.

685
00:31:53,470 --> 00:31:55,303
이것은 코드에 버그가 없도록

686
00:31:55,303 --> 00:31:57,357
확인하는 좋은 방법입니다.

687
00:31:57,357 --> 00:31:59,690
숙제에도 구현이 올바른지 확인하기

688
00:31:59,690 --> 00:32:01,565
위해 기울기 검사가

689
00:32:01,565 --> 00:32:02,610
있을 것입니다.

690
00:32:02,610 --> 00:32:05,250
네, 질문은 우리가 종종

691
00:32:05,250 --> 00:32:07,957
미분 가능한 손실 함수를

692
00:32:07,957 --> 00:32:09,790
원한다고 말하지만,

693
00:32:09,790 --> 00:32:12,690
더 나은 손실 함수가 있고

694
00:32:12,690 --> 00:32:16,310
이를 해석적으로 기울기를 계산할 수

695
00:32:16,310 --> 00:32:20,670
없다면, 이 h 수치적 방법을 사용할 수

696
00:32:20,670 --> 00:32:21,780
있을까요?

697
00:32:21,780 --> 00:32:25,900
일반적으로 비미분 가능한 더

698
00:32:25,900 --> 00:32:33,080
나은 손실 함수를 구성하는 것은 어렵다고 생각합니다.

699
00:32:33,080 --> 00:32:34,660
그럴 수도 있습니다.

700
00:32:34,660 --> 00:32:36,140
당신의 경우에 가장 좋은 진짜

701
00:32:36,140 --> 00:32:38,720
손실 함수가 있지만, 그것은 미분 가능하지 않습니다.

702
00:32:38,720 --> 00:32:42,440
이 접근 방식을 사용할 수 있으며, 효과가 있을 수 있습니다.

703
00:32:42,440 --> 00:32:44,880
예를 들어, 손실이 모든 점에서

704
00:32:44,880 --> 00:32:48,340
진정으로 비미분 가능하다면 어려움을 겪을 것이라고

705
00:32:48,340 --> 00:32:50,300
생각합니다. 비연결된

706
00:32:50,300 --> 00:32:56,140
점들의 클러스터와 같아서, 가장 가파른 하강 방향으로 이동하는 것이 잘
연결되어

707
00:32:56,140 --> 00:32:57,980
있지 않고 이 지형을

708
00:32:57,980 --> 00:33:00,580
형성하지 않는다면 최상의 솔루션에

709
00:33:00,580 --> 00:33:02,280
도달하지 못할 것입니다.

710
00:33:02,280 --> 00:33:03,960
그래서 효과가 있을 수 있습니다.

711
00:33:03,960 --> 00:33:06,100
하지만 손실이 대부분의

712
00:33:06,100 --> 00:33:08,940
영역에서 미분 가능하지 않다면 아마도

713
00:33:08,940 --> 00:33:12,552
이러한 접근 방식을 사용하여 최저점을

714
00:33:12,552 --> 00:33:14,460
찾을 수 없을 것입니다.

715
00:33:14,460 --> 00:33:17,140
네, 그래서 설명의 요약은 함수가

716
00:33:17,140 --> 00:33:19,000
볼록하다면 이 경량

717
00:33:19,000 --> 00:33:23,030
하강법 또는 가장 가파른 하강법 접근 방식이 잘

718
00:33:23,030 --> 00:33:24,770
작동한다는 것입니다.

719
00:33:24,770 --> 00:33:27,230
하지만 비미분 가능하고 비볼록 함수가 있다면

720
00:33:27,230 --> 00:33:28,950
아마도 이 접근 방식은 잘 작동하지

721
00:33:28,950 --> 00:33:31,270
않을 것입니다. 왜냐하면 올바른 방향으로

722
00:33:31,270 --> 00:33:32,910
나아가지 않을 것이기 때문입니다.

723
00:33:32,910 --> 00:33:36,290
코드가 완벽하다면 반드시 오류가 발생하는 것은 아니지만,

724
00:33:36,290 --> 00:33:38,230
코드에 실수가 있을 수 있고

725
00:33:38,230 --> 00:33:40,190
즉시 알기 어려울 수 있습니다.

726
00:33:40,190 --> 00:33:42,590
하지만 h, 한계 h 정의는 매우 쉽게

727
00:33:42,590 --> 00:33:43,670
코딩할 수 있습니다.

728
00:33:43,670 --> 00:33:45,575
h를 매우 작은 값으로 설정하고,

729
00:33:45,575 --> 00:33:46,950
함수를 실행한 다음, 매우

730
00:33:46,950 --> 00:33:48,283
작은 양을 추가하면 됩니다.

731
00:33:48,283 --> 00:33:51,070
그래서 구현 시 오류가 덜 발생합니다.

732
00:33:51,070 --> 00:33:52,550
[듣기 어려움]

733
00:33:52,550 --> 00:33:53,750
구현을 위해.

734
00:33:53,750 --> 00:33:54,550
좋습니다.

735
00:33:54,550 --> 00:33:56,170
가치가 있다면 더 많은 오류가 발생하지 않습니다.

736
00:33:56,170 --> 00:33:57,750
맞습니다.

737
00:33:57,750 --> 00:33:58,250
좋습니다.

738
00:33:58,250 --> 00:34:01,670
이제 경량 하강법이라고 하는 최적화를 위한 기본

739
00:34:01,670 --> 00:34:03,950
알고리즘에 대해 이야기하겠습니다.

740
00:34:03,950 --> 00:34:06,450
기본 직관은 우리가 이전에 설명한 것입니다.

741
00:34:06,450 --> 00:34:08,389
손실 경관에 있을 때 각

742
00:34:08,389 --> 00:34:09,847
점에서 기울기를

743
00:34:09,847 --> 00:34:13,110
계산하고 손실 경관의 바닥을 향해 아래쪽으로 한

744
00:34:13,110 --> 00:34:14,449
걸음을 내딛습니다.

745
00:34:14,449 --> 00:34:18,190
우리가 하는 것은 손실 함수, 데이터 및

746
00:34:18,190 --> 00:34:22,510
현재 가중치 값을 고려하여 가중치의 기울기를

747
00:34:22,510 --> 00:34:23,870
계산하는 것입니다.

748
00:34:23,870 --> 00:34:26,328
이것은 우리가 기울기를 따라 내려가기 위해 각 가중치를 얼마나

749
00:34:26,328 --> 00:34:29,030
변화시켜야 하는지를 알려줍니다. 그리고 우리는 스텝 크기를 가져야 합니다.

750
00:34:29,030 --> 00:34:32,489
언덕에서 얼마나 멀리 내려가는지를 결정하는 것입니다.

751
00:34:32,489 --> 00:34:34,770
그래서 언덕을 내려가며,

752
00:34:34,770 --> 00:34:38,790
스텝 크기에서 기울기에 마이너스 기호가 곱해집니다.

753
00:34:38,790 --> 00:34:41,969
그래서 이것이 기본적으로 경량 하강법입니다.

754
00:34:41,969 --> 00:34:44,330
각 단계에서 기울기를 계산하고 있습니다.

755
00:34:44,330 --> 00:34:46,770
그리고 음의 기울기

756
00:34:46,770 --> 00:34:51,810
방향으로 고정된 방향으로 이동하고 있습니다.

757
00:34:51,810 --> 00:34:53,870
여기서 구체적인 예를 들어보겠습니다.

758
00:34:53,870 --> 00:34:56,030
이것이 3D 손실 경관이 아니라,

759
00:34:56,030 --> 00:34:58,530
종종 사람들이 경관을 내려다보는

760
00:34:58,530 --> 00:35:00,190
것처럼 시각화합니다.

761
00:35:00,190 --> 00:35:02,250
보라색은 가장 높은 점을

762
00:35:02,250 --> 00:35:05,690
나타내고 빨간색은 여기서 바닥 또는 계곡을 나타냅니다.

763
00:35:05,690 --> 00:35:07,630
그리고 우리는 원래 W를 가지고 있다고 상상할 수 있습니다.

764
00:35:07,630 --> 00:35:08,790
손실을 계산할 수 있습니다.

765
00:35:08,790 --> 00:35:11,930
우리는 경사의 방향, 즉 음의 기울기 방향을 알고

766
00:35:11,930 --> 00:35:12,670
있습니다.

767
00:35:12,670 --> 00:35:16,223
그리고 이 화살표는 우리가 이전에 이야기했던 고정 단계 크기를

768
00:35:16,223 --> 00:35:17,390
나타낼 수 있습니다.

769
00:35:17,390 --> 00:35:20,960
우리는 그 방향으로 고정 단계 크기를 취하고 있습니다.

770
00:35:25,420 --> 00:35:27,440
네, 고정 단계 크기를 볼 수 있습니다.

771
00:35:27,440 --> 00:35:30,435
하지만 기울기가 작아짐에 따라 여전히 이 고정

772
00:35:30,435 --> 00:35:32,560
단계 크기로 곱하고 있습니다.

773
00:35:32,560 --> 00:35:34,420
그래서 실제 단계 크기는

774
00:35:34,420 --> 00:35:36,620
기울기가 평평해지는 끝

775
00:35:36,620 --> 00:35:39,220
부분에서 작아지기 때문에 실제로

776
00:35:39,220 --> 00:35:40,280
작아집니다.

777
00:35:40,280 --> 00:35:42,038
그래서 우리는 항상

778
00:35:42,038 --> 00:35:43,580
가장 가파른 하강

779
00:35:43,580 --> 00:35:45,580
방향으로 나아가고 있습니다.

780
00:35:45,580 --> 00:35:47,218
그래서 질문은, 우리가 내려갈 때

781
00:35:47,218 --> 00:35:48,760
언제 멈출지를 아는 방법입니다.

782
00:35:48,760 --> 00:35:52,940
음, 이 공식에서는 계속해서 무한히 반복하므로 결코

783
00:35:52,940 --> 00:35:54,183
멈추지 않습니다.

784
00:35:54,183 --> 00:35:55,600
그래서 이것은 아마도 최선이 아니었습니다.

785
00:35:55,600 --> 00:35:57,308
보통은 미리 정해진

786
00:35:57,308 --> 00:35:59,900
반복 횟수가 있습니다.

787
00:35:59,900 --> 00:36:03,940
또는 손실이 고정된 양으로 크게 변하지 않는지

788
00:36:03,940 --> 00:36:05,480
확인할 수 있습니다.

789
00:36:05,480 --> 00:36:08,443
손실이 얼마나 줄어들 것으로 기대하는지에 대한 허용 오차를

790
00:36:08,443 --> 00:36:09,360
설정할 수 있습니다.

791
00:36:09,360 --> 00:36:11,100
그리고 더 이상 줄어들지

792
00:36:11,100 --> 00:36:14,823
않는다면-- 1E-5 또는 1E-9만큼 줄어든다면, 거기서 멈출

793
00:36:14,823 --> 00:36:16,740
수 있습니다. 충분히 좋습니다.

794
00:36:16,740 --> 00:36:19,073
그래서 멈추는 시점을 결정하는

795
00:36:19,073 --> 00:36:23,770
두 가지 방법은 고정된 반복 횟수 또는 더 이상 크게

796
00:36:23,770 --> 00:36:26,020
개선되지 않는 기준입니다.

797
00:36:29,670 --> 00:36:33,070
이제 가장 인기 있는 기울기 하강

798
00:36:33,070 --> 00:36:35,950
변형인 확률적 경량 하강에 대해

799
00:36:35,950 --> 00:36:37,630
이야기하겠습니다.

800
00:36:37,630 --> 00:36:39,430
그리고 이전에 기울기

801
00:36:39,430 --> 00:36:41,110
하강에 대해 이야기할

802
00:36:41,110 --> 00:36:44,790
때, 우리는 전체 훈련 세트에 대해 각 i에

803
00:36:44,790 --> 00:36:50,230
대한 손실 Li를 합산하여 가중치의 손실을 계산한다고 이야기했습니다.

804
00:36:50,230 --> 00:36:53,030
하지만 데이터 세트가 매우 크면

805
00:36:53,030 --> 00:36:55,630
이는 많은 계산이 될 수 있습니다.

806
00:36:55,630 --> 00:36:59,253
그래서 확률적 경량 하강은 기본적으로 전체 데이터

807
00:36:59,253 --> 00:37:01,170
세트를 보는 대신 매번

808
00:37:01,170 --> 00:37:03,350
서브셋을 보는 것입니다. 이를 미니

809
00:37:03,350 --> 00:37:05,990
배치 또는 데이터 배치라고 부릅니다.

810
00:37:05,990 --> 00:37:08,110
그래서 여기서 코드를 보면

811
00:37:08,110 --> 00:37:12,370
데이터 세트에서 256개의 데이터 포인트를 샘플링하는 것과

812
00:37:12,370 --> 00:37:15,135
같습니다. 배치 크기는 256입니다.

813
00:37:15,135 --> 00:37:20,360
우리는 데이터 세트의 이 256 서브셋의 기울기를 평가하고, 이전과

814
00:37:20,360 --> 00:37:22,340
같은 작업을 수행합니다.

815
00:37:22,340 --> 00:37:24,840
확률적 경량 하강이라고 불리는

816
00:37:24,840 --> 00:37:27,520
이유는 알고리즘의 각 단계에서 매번

817
00:37:27,520 --> 00:37:29,920
데이터 세트의 무작위 서브셋을

818
00:37:29,920 --> 00:37:31,480
샘플링하기 때문입니다.

819
00:37:31,480 --> 00:37:33,740
그래서 이것이 확률적 경량 하강입니다.

820
00:37:33,740 --> 00:37:37,680
매번 무작위 서브셋에서 실행하는 것입니다.

821
00:37:37,680 --> 00:37:39,760
실제로 사람들은 완전히 무작위로

822
00:37:39,760 --> 00:37:41,540
샘플링하지 않습니다.

823
00:37:41,540 --> 00:37:45,400
그들은 데이터 세트의 모든 예제를 통과하고

824
00:37:45,400 --> 00:37:47,220
다시 반복하도록 합니다.

825
00:37:47,220 --> 00:37:49,460
그리고 이것을 훈련의 한 에폭이라고 부르며,

826
00:37:49,460 --> 00:37:51,560
무작위 순서로 모든 데이터 샘플을

827
00:37:51,560 --> 00:37:52,580
한 번 반복합니다.

828
00:37:55,370 --> 00:37:57,920
기울기 하강 또는 확률적 기울기 하강에는 몇

829
00:37:57,920 --> 00:37:59,340
가지 문제가 있습니다.

830
00:37:59,340 --> 00:38:02,760
그래서 이 시각화는 이전에 보여준 색상 있는 것과

831
00:38:02,760 --> 00:38:05,560
같은 유형으로 손실 경관을 아래에서

832
00:38:05,560 --> 00:38:06,540
보고 있습니다.

833
00:38:06,540 --> 00:38:08,900
하지만 이 곡선은 레벨 세트라고 불리며,

834
00:38:08,900 --> 00:38:11,060
모든 점에서 손실이 동일한

835
00:38:11,060 --> 00:38:12,060
점들의 집합입니다.

836
00:38:12,060 --> 00:38:14,240
그래서 이것은 손실을 위에서 내려다보는

837
00:38:14,240 --> 00:38:17,040
매우 인기 있는 시각화 방법의 또 다른

838
00:38:17,040 --> 00:38:19,460
방법입니다. 하지만 색상은 없습니다.

839
00:38:19,460 --> 00:38:24,060
그리고 당신은 정말 좁은 계곡이 있고, 양쪽이 정말 가파른

840
00:38:24,060 --> 00:38:25,783
현상을 상상할 수 있습니다.

841
00:38:25,783 --> 00:38:27,200
그리고 당신은

842
00:38:27,200 --> 00:38:30,260
계곡의 중심을 가로지르려고 하고 있습니다.

843
00:38:30,260 --> 00:38:34,540
그리고 경량 하강법은 실제로 여기서 문제에 직면합니다.

844
00:38:34,540 --> 00:38:38,715
누구든지 잘못될 수 있는 것에 대한 아이디어가 있나요?

845
00:38:38,715 --> 00:38:40,340
네, 할 수 있는 것

846
00:38:40,340 --> 00:38:43,700
중 하나는 이 방향으로 위아래로 이동할 때

847
00:38:43,700 --> 00:38:45,620
지나치게 나아가는 것입니다.

848
00:38:45,620 --> 00:38:48,920
그리고 경사가 충분히 가파르고 스텝 크기가 충분히 크면,

849
00:38:48,920 --> 00:38:51,120
실제로 계곡에서 진동할 수 있습니다.

850
00:38:51,120 --> 00:38:53,432
스텝 크기가 매우 크고 경사가

851
00:38:53,432 --> 00:38:55,140
정말 가파르다면, 매번

852
00:38:55,140 --> 00:38:57,300
고정된 스텝 크기로 인해

853
00:38:57,300 --> 00:39:00,300
점점 더 멀어지는 것을 경험하게 될

854
00:39:00,300 --> 00:39:00,800
것입니다.

855
00:39:00,800 --> 00:39:03,780
그래서 경사가 충분히 가파르면 계곡에서

856
00:39:03,780 --> 00:39:05,400
튕겨 나올 수 있습니다.

857
00:39:05,400 --> 00:39:07,960
실제로 학습률이 너무 크면 그런 일이 발생합니다.

858
00:39:07,960 --> 00:39:09,460
그래서 그게 발생할 수 있는 한 가지입니다.

859
00:39:09,460 --> 00:39:12,880
그리고 학습률이 너무 크지 않더라도,

860
00:39:12,880 --> 00:39:16,488
스텝 크기가 너무 크지 않더라도,

861
00:39:16,488 --> 00:39:18,030
경사가 가파른

862
00:39:18,030 --> 00:39:20,430
방향에서 훨씬 더 큰

863
00:39:20,430 --> 00:39:23,492
현상이 발생할 수 있습니다.

864
00:39:23,492 --> 00:39:24,950
그래서 진동하고 있지만

865
00:39:24,950 --> 00:39:26,430
실제 중심으로 의미 있는

866
00:39:26,430 --> 00:39:27,990
진전을 이루지 못하고,

867
00:39:27,990 --> 00:39:30,198
위아래로 진동하는 데 시간을 다 쓰고

868
00:39:30,198 --> 00:39:30,750
있습니다.

869
00:39:30,750 --> 00:39:36,630
그래서 이것은 기본 SGD와 관련된 꽤 큰 문제입니다.

870
00:39:36,630 --> 00:39:39,333
그리고 수학적으로, 여기에

871
00:39:39,333 --> 00:39:40,750
고려하는 손실 함수는

872
00:39:40,750 --> 00:39:42,510
헤시안 행렬의

873
00:39:42,510 --> 00:39:45,990
최대 고유값과 최소 고유값의 비율인

874
00:39:45,990 --> 00:39:48,650
높은 조건수를 가지고 있습니다.

875
00:39:48,650 --> 00:39:51,190
그래서 이 위아래 방향의 두 번째 도함수가

876
00:39:51,190 --> 00:39:53,210
매우 높다고 상상할 수 있습니다.

877
00:39:53,210 --> 00:39:56,390
하지만 옆으로는 매우 평평하기 때문에

878
00:39:56,390 --> 00:40:00,310
매우 낮습니다. 그래서 이것이 이 현상을 일으킵니다.

879
00:40:00,310 --> 00:40:01,830
좋습니다.

880
00:40:01,830 --> 00:40:06,550
그래서 SGD와 관련하여 우리가 가질 수 있는

881
00:40:06,550 --> 00:40:11,670
문제 중 하나는 손실 함수에 국소 최소값 또는

882
00:40:11,670 --> 00:40:14,000
안장점이 있는 경우입니다.

883
00:40:14,000 --> 00:40:19,920
예를 들어, 이 곡선의 끝 부분은 완전히

884
00:40:19,920 --> 00:40:21,260
평평합니다.

885
00:40:21,260 --> 00:40:26,200
그래서 우리가 여기서 언덕을 내려간다고 상상하면, 우리는 여기서

886
00:40:26,200 --> 00:40:28,180
막히게 될 것입니다.

887
00:40:28,180 --> 00:40:30,180
그리고 우리는 더 이상 진행할 수 없을

888
00:40:30,180 --> 00:40:32,960
것입니다. 왜냐하면 여기서의 경계가 0이기 때문입니다.

889
00:40:32,960 --> 00:40:36,082
그래서 이것은 실제로 꽤 큰 문제로,

890
00:40:36,082 --> 00:40:38,040
국소 최소값에 갇히게

891
00:40:38,040 --> 00:40:41,218
됩니다. 여기 도달하면 더 이상 갈 방향이

892
00:40:41,218 --> 00:40:42,260
없습니다.

893
00:40:42,260 --> 00:40:43,223
경계가 0입니다.

894
00:40:43,223 --> 00:40:44,640
아니면 매우 작아서 여기서

895
00:40:44,640 --> 00:40:45,920
위아래로 진동하게 됩니다.

896
00:40:45,920 --> 00:40:48,960
그리고 여기서 경계가 0이기 때문에 이 하단

897
00:40:48,960 --> 00:40:50,657
예제에서 실제로 막힐

898
00:40:50,657 --> 00:40:52,240
수 있습니다. 조금 더

899
00:40:52,240 --> 00:40:55,000
나아가면 훨씬 더 내려갈 수 있습니다.

900
00:40:55,000 --> 00:40:58,400
네, 그래서 질문은 아마도 우리가 스텝을 수행하는 방식을

901
00:40:58,400 --> 00:40:59,900
변경할 수 있을까요?

902
00:40:59,900 --> 00:41:01,692
아마도 헤시안을 사용하여 우리가 가는 방향을

903
00:41:01,692 --> 00:41:02,840
결정할 수 있을 것입니다.

904
00:41:02,840 --> 00:41:05,640
실제로 우리는 마지막에 헤시안 스타일 접근법에

905
00:41:05,640 --> 00:41:07,740
대한 간단한 슬라이드를 가지고 있습니다.

906
00:41:07,740 --> 00:41:09,700
그것은 딥러닝에서 매우 일반적으로 사용되지 않습니다.

907
00:41:09,700 --> 00:41:11,277
하지만 짧은 대답은 예입니다.

908
00:41:11,277 --> 00:41:13,610
실제로 이것을 고려할 수 있는 여러 가지 방법이

909
00:41:13,610 --> 00:41:14,890
있으며, 우리는 5분 후에

910
00:41:14,890 --> 00:41:16,190
그것에 대해 이야기할 것입니다.

911
00:41:16,190 --> 00:41:17,430
그래서 좋은 질문입니다.

912
00:41:17,430 --> 00:41:18,370
네.

913
00:41:18,370 --> 00:41:19,430
그것에 대해 이야기할 것입니다.

914
00:41:24,050 --> 00:41:27,170
그래서 제가 생각하기에 여러분이 모를 수도 있는 다른

915
00:41:27,170 --> 00:41:29,450
것 중 하나는 경험적으로 안장점이 실제로

916
00:41:29,450 --> 00:41:32,030
더 높은 차원 모델로 갈수록 훨씬 더 흔하다는

917
00:41:32,030 --> 00:41:35,472
것입니다. 즉, 가중치 행렬이 점점 커질수록 이러한

918
00:41:35,472 --> 00:41:37,430
안장점을 찾을 가능성이 더 높아집니다.

919
00:41:37,430 --> 00:41:38,805
그리고 이 안장점의 빈도를

920
00:41:38,805 --> 00:41:40,233
설명하는 논문이 있습니다.

921
00:41:40,233 --> 00:41:41,650
안장점을 모른다면,

922
00:41:41,650 --> 00:41:43,567
그것은 말의 안장처럼 생겼기

923
00:41:43,567 --> 00:41:45,850
때문에 안장점이라고 불립니다.

924
00:41:45,850 --> 00:41:48,090
이 안장의 중심에서는

925
00:41:48,090 --> 00:41:51,510
모든 방향에서 기울기가 실제로 0이므로,

926
00:41:51,510 --> 00:41:53,690
이것의 바닥과 이 곡선의

927
00:41:53,690 --> 00:41:55,670
꼭대기와 같습니다.

928
00:41:55,670 --> 00:41:58,910
그래서 x 방향과 y 방향 모두에서 기울기가

929
00:41:58,910 --> 00:42:00,930
0이므로, 양쪽에서 손실 경관을

930
00:42:00,930 --> 00:42:03,570
크게 낮추는 것에 매우 가까워도

931
00:42:03,570 --> 00:42:05,330
여기에서 갇힐 수 있습니다.

932
00:42:05,330 --> 00:42:08,370
그래서 이것은 SGD와 관련된 꽤 일반적인

933
00:42:08,370 --> 00:42:09,870
문제이며, 우리가 더

934
00:42:09,870 --> 00:42:12,170
높은 차원 공간으로 이동할수록,

935
00:42:12,170 --> 00:42:15,070
즉 매개변수가 더 많은 모델과 동등하게,

936
00:42:15,070 --> 00:42:16,870
점점 더 흔해집니다.

937
00:42:16,870 --> 00:42:18,750
이것은 큰 문제입니다.

938
00:42:18,750 --> 00:42:22,150
그리고 SGD의 마지막 문제는

939
00:42:22,150 --> 00:42:26,880
매번 데이터의 하위 집합을 샘플링한다는 것입니다.

940
00:42:26,880 --> 00:42:28,790
그래서 우리는 전체를 보지 않고--

941
00:42:28,790 --> 00:42:31,735
이것은 모든 데이터에 대한 전체 손실을 나타냅니다.

942
00:42:31,735 --> 00:42:33,610
하지만 우리는 매번 하위 집합만 보고 있습니다.

943
00:42:33,610 --> 00:42:36,230
그래서 우리는 전체 데이터 세트를 보지 않기

944
00:42:36,230 --> 00:42:39,350
때문에 다소 시끄러운 업데이트 단계를 가지게 될 것입니다.

945
00:42:39,350 --> 00:42:42,990
그래서 우리는 여기서 도달하려고 하는 이

946
00:42:42,990 --> 00:42:47,130
지역 최소값을 향해 중심으로 나아가고 있습니다.

947
00:42:47,130 --> 00:42:50,010
하지만 각 단계는 그 방향으로 직접 가지 않습니다.

948
00:42:50,010 --> 00:42:52,510
그래서 우리는 데이터 세트를 하위 샘플링하고

949
00:42:52,510 --> 00:42:56,390
있기 때문에 진행하는 방식에 약간의 노이즈가 있습니다.

950
00:42:56,390 --> 00:42:58,550
좋아요, 멋지네요.

951
00:42:58,550 --> 00:43:02,870
요약하자면, 이것이 주요 문제입니다.

952
00:43:02,870 --> 00:43:06,110
그리고 여러분이 기본적으로 모멘텀을 추가하는 꽤

953
00:43:06,110 --> 00:43:07,370
멋진 트릭이 있습니다.

954
00:43:07,370 --> 00:43:09,560
그리고 여러분은 이것을 언덕을 굴러 내려가는

955
00:43:09,560 --> 00:43:11,400
공처럼 생각할 수 있습니다. 공은

956
00:43:11,400 --> 00:43:12,400
모멘텀을 얻습니다.

957
00:43:12,400 --> 00:43:13,880
실제로 물리적 특성 측면에서

958
00:43:13,880 --> 00:43:15,922
모델링되는 방식과 매우 유사합니다.

959
00:43:15,922 --> 00:43:17,840
그래서 적어도 직관을 얻는

960
00:43:17,840 --> 00:43:19,120
좋은 방법입니다.

961
00:43:19,120 --> 00:43:21,708
그래서 여러분은 이것이 지역 최소값이 있을 때

962
00:43:21,708 --> 00:43:24,000
도움이 될 수 있다고 상상할 수 있습니다.

963
00:43:24,000 --> 00:43:25,417
충분한 속도로 굴러내리면

964
00:43:25,417 --> 00:43:27,120
그것을 벗어날 수 있습니다.

965
00:43:27,120 --> 00:43:31,077
안장점이나 단순히 평평한 점이 있다면, 모델은 전체 언덕을

966
00:43:31,077 --> 00:43:33,160
굴러내렸기 때문에 더 이상 여기에서

967
00:43:33,160 --> 00:43:34,618
갇히지 않을 것입니다.

968
00:43:34,618 --> 00:43:36,160
계속 진행할 것입니다.

969
00:43:36,160 --> 00:43:40,860
또한 이 나쁜 조건 값이 있으면 여전히 약간의

970
00:43:40,860 --> 00:43:44,060
진동이 있을 수 있습니다.

971
00:43:44,060 --> 00:43:47,720
하지만 좋은 점은 여러 단계가 계속 그 방향으로

972
00:43:47,720 --> 00:43:49,680
나아가면서 이 방향으로

973
00:43:49,680 --> 00:43:51,763
속도가 쌓일 것입니다.

974
00:43:51,763 --> 00:43:54,460
그래서 여기 중심으로 더 빠르게 나아갈 것입니다.

975
00:43:54,460 --> 00:43:56,640
그래서 이 문제에도 도움이 됩니다.

976
00:43:56,640 --> 00:43:58,800
마지막으로, 이것은 기울기의

977
00:43:58,800 --> 00:44:00,880
노이즈를 평균화하는 데도 도움이

978
00:44:00,880 --> 00:44:04,880
될 수 있습니다. 왜냐하면 모두 이 최소값을 향한

979
00:44:04,880 --> 00:44:07,460
공통 방향을 가지고 있기 때문입니다.

980
00:44:07,460 --> 00:44:12,310
그래서 모멘텀을 계산할 때, 그것은

981
00:44:12,310 --> 00:44:18,650
스스로 쌓이고, 모두가 공유하는 방향을 살펴보므로

982
00:44:18,650 --> 00:44:20,730
노이즈가 고려되어

983
00:44:20,730 --> 00:44:24,842
더 빠르게 수렴할 것입니다.

984
00:44:24,842 --> 00:44:26,550
그래서 실제로 어떻게 하는지

985
00:44:26,550 --> 00:44:28,570
보여드리겠습니다. 하지만 이것이

986
00:44:28,570 --> 00:44:31,450
모멘텀 작동 방식에 대한 일반적인 직관입니다.

987
00:44:31,450 --> 00:44:33,650
여기 SGD가 있습니다.

988
00:44:33,650 --> 00:44:37,170
우리의 미니 배치 x가 있습니다.

989
00:44:37,170 --> 00:44:39,570
우리는 기울기를 계산하고 있으며, 이는 dx입니다.

990
00:44:39,570 --> 00:44:42,190
우리는 학습률 또는 단계 크기를 가지고 있으며, 이를 곱한 후 음수로

991
00:44:42,190 --> 00:44:43,170
만들어야 합니다. 왜냐하면 우리는

992
00:44:43,170 --> 00:44:44,410
언덕을 내려가야 하기 때문입니다.

993
00:44:44,410 --> 00:44:47,890
이것이 우리의 새로운 x를 제공합니다. 그래서 이것이 SGD입니다.

994
00:44:47,890 --> 00:44:52,550
모멘텀을 사용하면 이제 이 속도 항으로 업데이트하고 있습니다.

995
00:44:52,550 --> 00:44:55,310
따라서 특정 지점에서 기울기로 업데이트하는

996
00:44:55,310 --> 00:44:57,330
대신 속도로 업데이트하고 있습니다.

997
00:44:57,330 --> 00:44:59,050
주어진 시간 단계에서의

998
00:44:59,050 --> 00:45:03,870
속도는 이전 속도와 현재 기울기를 더한 것입니다.

999
00:45:03,870 --> 00:45:05,230
이렇게 계산합니다.

1000
00:45:05,230 --> 00:45:06,647
그리고 이 rho 값이

1001
00:45:06,647 --> 00:45:10,560
있는데, 이는 모멘텀으로, 실제로 얼마나 많은 모멘텀을 원하느냐입니다.

1002
00:45:10,560 --> 00:45:13,020
만약 이 값이 매우 높으면, 새로운

1003
00:45:13,020 --> 00:45:16,340
속도는 이전 시간 단계의 속도에 더 의존하게 됩니다.

1004
00:45:16,340 --> 00:45:21,980
그리고 여기 모멘텀 항의 마지막 기울기들의 이동 평균이 과거와

1005
00:45:21,980 --> 00:45:24,500
현재를 얼마나 가중치를 두어야

1006
00:45:24,500 --> 00:45:26,170
하는지를 제공합니다.

1007
00:45:26,170 --> 00:45:27,420
그래서 이제 우리는 이것으로 업데이트하고 있습니다.

1008
00:45:27,420 --> 00:45:29,680
그리고 여전히 이 alpha가 있으며, 이는 단계 크기입니다.

1009
00:45:29,680 --> 00:45:31,600
그래서 사실 매우 간단한 변화입니다.

1010
00:45:31,600 --> 00:45:34,600
이제 현재 속도와 우리의

1011
00:45:34,600 --> 00:45:37,800
기울기의 함수인 속도를

1012
00:45:37,800 --> 00:45:39,580
계산하고 있습니다.

1013
00:45:39,580 --> 00:45:42,340
여기서 질문을 위해 잠시 멈추겠습니다.

1014
00:45:42,340 --> 00:45:44,640
이것이 모멘텀에 대한 설명입니다.

1015
00:45:44,640 --> 00:45:47,340
그리고 아마도 우리가 본 모든 문제를 어떻게 해결하는지

1016
00:45:47,340 --> 00:45:49,320
간단히 요약할 수 있을 것 같습니다.

1017
00:45:49,320 --> 00:45:51,940
그래서 이제 과거의 기울기에 모멘텀을

1018
00:45:51,940 --> 00:45:55,060
추가하고 있다면, 그것이 이 방향으로 계속

1019
00:45:55,060 --> 00:45:57,640
나아가는 것을 볼 수 있을 것입니다.

1020
00:45:57,640 --> 00:46:00,040
그리고 당신의 로가 높다면,

1021
00:46:00,040 --> 00:46:03,660
모멘텀은 계속 진행되어 여기의 국소 최소값에 대한 큰

1022
00:46:03,660 --> 00:46:05,740
범위를 고려할 수 있습니다.

1023
00:46:05,740 --> 00:46:07,840
또한 이 안장점에서 매우

1024
00:46:07,840 --> 00:46:10,400
잘 작동하는데, 이전에 가던

1025
00:46:10,400 --> 00:46:12,920
방향으로 상당한 시간 동안 계속

1026
00:46:12,920 --> 00:46:14,810
나아가기 때문입니다.

1027
00:46:14,810 --> 00:46:17,520
각 단계에서 누적적으로 오른쪽으로

1028
00:46:17,520 --> 00:46:21,120
가고 있다면, 모멘텀도 거기에서 일관되게 쌓일

1029
00:46:21,120 --> 00:46:21,780
것입니다.

1030
00:46:21,780 --> 00:46:27,280
그리고 만약 여기에서 크게 진동하고 있다면, 값들이 상쇄되기

1031
00:46:27,280 --> 00:46:29,800
때문에 방향으로 덜 움직일

1032
00:46:29,800 --> 00:46:30,840
것입니다.

1033
00:46:30,840 --> 00:46:34,228
현재 방향과 속도 측면에서, 그들은

1034
00:46:34,228 --> 00:46:36,020
반대 방향을 가리키므로

1035
00:46:36,020 --> 00:46:37,850
최소화될 것입니다.

1036
00:46:37,850 --> 00:46:39,600
그래서 질문은, 안장을 따라

1037
00:46:39,600 --> 00:46:41,540
굴러가면 어떻게 되는가입니다.

1038
00:46:41,540 --> 00:46:43,540
제 생각에는 실제로는 매우 가능성이 낮습니다.

1039
00:46:43,540 --> 00:46:45,800
하지만 그런 경우라면, 네,

1040
00:46:45,800 --> 00:46:48,000
안장에 갇히게 될 것입니다.

1041
00:46:48,000 --> 00:46:51,000
당신의 초기 조건이 어디서 시작하든

1042
00:46:51,000 --> 00:46:53,180
매우 불행하다고 생각합니다.

1043
00:46:53,180 --> 00:46:55,300
그래서 가끔 그런 일이 발생할 수 있다고 생각하지만,

1044
00:46:55,300 --> 00:46:56,690
매우 가능성이 낮습니다.

1045
00:46:56,690 --> 00:46:58,440
네, 그리고 그것이 실제로

1046
00:46:58,440 --> 00:47:01,970
사람들이 단일 모델 훈련을 하지 않는 이유이기도 합니다.

1047
00:47:01,970 --> 00:47:04,470
종종 그들은 무작위 시드를 다르게 하여 여러 번

1048
00:47:04,470 --> 00:47:06,990
실행합니다. 그런 일이 발생할 경우를 대비해서입니다.

1049
00:47:06,990 --> 00:47:09,532
또 다른 점은 확률적 경량 하강법을 사용하고 있다면,

1050
00:47:09,532 --> 00:47:12,198
적어도 약간의 노이즈가 있어 안장에서 직접

1051
00:47:12,198 --> 00:47:14,610
왔다 갔다 하는 것을 벗어날 가능성이 훨씬 더

1052
00:47:14,610 --> 00:47:17,090
높다는 것입니다. 그래서 기본적으로 무작위성

1053
00:47:17,090 --> 00:47:19,470
때문에 그런 일은 절대 일어나지 않을 것입니다.

1054
00:47:19,470 --> 00:47:22,550
하지만 가정적으로는 그런 일이 발생할 수 있다고 생각합니다.

1055
00:47:22,550 --> 00:47:23,490
응.

1056
00:47:23,490 --> 00:47:26,290
그래서 질문은 왜 안장이 SGD와 관련된 문제일 뿐, 일반적인 최적화와는

1057
00:47:26,290 --> 00:47:27,745
관계가 없는가 하는 것입니다?

1058
00:47:27,745 --> 00:47:29,870
전체 데이터 세트와도 관련된 문제일 것입니다.

1059
00:47:29,870 --> 00:47:32,860
전체 데이터 세트와 함께 더 흔할 수도 있습니다.

1060
00:47:32,860 --> 00:47:35,810
그래서 이것은 SGD가 직면하는

1061
00:47:35,810 --> 00:47:38,090
문제이지만, 단순히 기울기

1062
00:47:38,090 --> 00:47:41,410
하강법에 의존하는 다른 최적화 알고리즘도

1063
00:47:41,410 --> 00:47:43,090
같은 문제에

1064
00:47:43,090 --> 00:47:44,490
직면할 것입니다.

1065
00:47:44,490 --> 00:47:45,730
응.

1066
00:47:45,730 --> 00:47:46,230
응.

1067
00:47:46,230 --> 00:47:49,450
그래서 질문은 모멘텀을 추가하면 과도하게 지나쳐서

1068
00:47:49,450 --> 00:47:51,770
다시 돌아와야 하므로 수렴하기 더

1069
00:47:51,770 --> 00:47:53,410
어려워지는가 하는 것입니다.

1070
00:47:53,410 --> 00:47:54,830
짧은 대답은 네, 그렇다고 생각합니다.

1071
00:47:54,830 --> 00:47:57,890
수렴에 도움이 되지 않을 수도 있지만,

1072
00:47:57,890 --> 00:47:58,810
평균적으로

1073
00:47:58,810 --> 00:48:02,010
더 나은 최소점을 찾는 데 도움이

1074
00:48:02,010 --> 00:48:03,320
될 것입니다.

1075
00:48:03,320 --> 00:48:06,580
지역 최소점에 갇히지 않기 때문에

1076
00:48:06,580 --> 00:48:08,892
수렴이 더 느릴 수 있습니다.

1077
00:48:08,892 --> 00:48:11,100
모멘텀 없이 여기서

1078
00:48:11,100 --> 00:48:12,640
수렴할 것입니다.

1079
00:48:12,640 --> 00:48:16,040
이런 것들은 경험적으로 보여지며,

1080
00:48:16,040 --> 00:48:16,540
특정

1081
00:48:16,540 --> 00:48:18,460
신경망 클래스와 관련이

1082
00:48:18,460 --> 00:48:19,280
있습니다.

1083
00:48:19,280 --> 00:48:21,440
모멘텀은 훈련에 도움이 됩니다.

1084
00:48:21,440 --> 00:48:25,320
하지만 우리가 이를 선호하는 이유에 대한 직관입니다.

1085
00:48:25,320 --> 00:48:30,038
솔직히 말하면, 사람들은 가장 잘 작동하는 것을 사용할 것입니다.

1086
00:48:30,038 --> 00:48:31,580
모멘텀 없이 확률적

1087
00:48:31,580 --> 00:48:34,180
경사 하강법이 특정 모델에서 더 나은

1088
00:48:34,180 --> 00:48:36,580
성능을 보이는 경우도 있습니다.

1089
00:48:36,580 --> 00:48:39,205
그래서 왜 더 나은 성능을 낼 수 있는지에 대한 직관입니다.

1090
00:48:39,205 --> 00:48:40,580
하지만 실제로는 사람들은 다양한

1091
00:48:40,580 --> 00:48:43,400
방법을 시도해보고 어떤 것이 가장 잘 작동하는지 확인합니다.

1092
00:48:43,400 --> 00:48:46,200
현재 사람들이 시도하는 가장 일반적인 방법들을 살펴보겠습니다.

1093
00:48:46,200 --> 00:48:46,722
네.

1094
00:48:46,722 --> 00:48:47,680
하지만 맞아요.

1095
00:48:47,680 --> 00:48:49,680
수렴에 해를 끼칠 수 있습니다, 잠재적으로.

1096
00:48:54,270 --> 00:48:55,000
좋습니다.

1097
00:48:55,000 --> 00:48:56,740
그럼 계속하겠습니다.

1098
00:48:56,740 --> 00:49:01,580
네, 이 부분은 우리가 다뤘던 것 같습니다.

1099
00:49:01,580 --> 00:49:03,320
그리고 제가 지적하고 싶은 또 다른

1100
00:49:03,320 --> 00:49:05,765
점은 이를 공식화하는 방법이 다르다는 것입니다.

1101
00:49:05,765 --> 00:49:07,140
이 방정식들은

1102
00:49:07,140 --> 00:49:09,660
동일하지만, 구현에 따라 다르게 쓰이는

1103
00:49:09,660 --> 00:49:11,220
경우도 있습니다.

1104
00:49:11,220 --> 00:49:13,720
하지만 같은 작업을 수행하고 있습니다.

1105
00:49:13,720 --> 00:49:16,780
시간을 절약하기 위해 이들이 동일한 이유는 건너뛰겠습니다.

1106
00:49:16,780 --> 00:49:18,280
하지만 슬라이드를

1107
00:49:18,280 --> 00:49:20,880
통해 이들이 본질적으로 동일한 공식임을

1108
00:49:20,880 --> 00:49:22,920
스스로 증명할 수 있습니다.

1109
00:49:22,920 --> 00:49:24,200
좋습니다.

1110
00:49:24,200 --> 00:49:26,040
다음에 이야기할 것은

1111
00:49:26,040 --> 00:49:27,900
다른 최적화 기법입니다.

1112
00:49:27,900 --> 00:49:29,760
우리는 모멘텀에 대해 이야기했습니다.

1113
00:49:29,760 --> 00:49:32,400
이제 RMSProp이라는 것에 대해 이야기하겠습니다.

1114
00:49:32,400 --> 00:49:37,760
RMSProp은 이제 2012년의 다소 오래된 방법이지만, Geoffrey

1115
00:49:37,760 --> 00:49:40,880
Hinton의 그룹에서 나왔습니다.

1116
00:49:40,880 --> 00:49:44,040
기본 아이디어는 모멘텀으로

1117
00:49:44,040 --> 00:49:48,160
포착하는 단순한 실행 속도 대신,

1118
00:49:48,160 --> 00:49:51,600
실제로 기울기의 요소별 스케일링을

1119
00:49:51,600 --> 00:49:54,060
추가하는 것입니다.

1120
00:49:54,060 --> 00:50:00,530
이를 수행하는 방법은 기울기 제곱 항을 갖는 것입니다.

1121
00:50:00,530 --> 00:50:04,410
여기서 감쇠율은 우리가 이전에 설명한 모멘텀 항과 매우

1122
00:50:04,410 --> 00:50:07,630
유사하지만, 이제는 제곱 기울기에 적용됩니다.

1123
00:50:07,630 --> 00:50:10,850
우리는 이전 항인 기울기 제곱의 실행

1124
00:50:10,850 --> 00:50:12,030
평균을 갖습니다.

1125
00:50:12,030 --> 00:50:13,670
그리고 그 다음에 1에서 곱합니다.

1126
00:50:13,670 --> 00:50:16,610
여기서 그것은 문자 그대로 기울기 제곱입니다.

1127
00:50:16,610 --> 00:50:19,790
따라서 이것은 우리의 제곱 기울기의 실행 평균입니다.

1128
00:50:19,790 --> 00:50:22,390
더 큰 값은 훨씬 더 커지고, 더 작은

1129
00:50:22,390 --> 00:50:24,270
값은 훨씬 더 작아집니다.

1130
00:50:24,270 --> 00:50:26,810
특정 값에서 지속적으로 큰 기울기가

1131
00:50:26,810 --> 00:50:28,910
있다면, 우리는 여기서 실행

1132
00:50:28,910 --> 00:50:32,090
평균을 계속하면서 매우 크게 될 것입니다.

1133
00:50:32,090 --> 00:50:33,770
실제로 우리는 여기서 나눕니다.

1134
00:50:33,770 --> 00:50:36,950
업데이트 단계에서 우리는 그것의 제곱근으로 나눕니다.

1135
00:50:36,950 --> 00:50:39,863
여기서 기본 아이디어는 이제 우리가 단계적으로 나아가고 있다는

1136
00:50:39,863 --> 00:50:42,030
것입니다. 누군가가 이전에 질문을 했던 것 같은데,

1137
00:50:42,030 --> 00:50:44,490
우리가 나아가는 방향을 바꾼다면 어떻게 될까요?

1138
00:50:44,490 --> 00:50:46,550
이것이 바로 우리가 할 수 있는 유형입니다.

1139
00:50:46,550 --> 00:50:48,690
그리고 이것이 우리가 제곱 기울기

1140
00:50:48,690 --> 00:50:50,290
항으로 나누고 있는 것입니다.

1141
00:50:50,290 --> 00:50:55,290
따라서 매우 큰 제곱 기울기가 있는 값에 대해,

1142
00:50:55,290 --> 00:50:59,722
도함수가 매우 큰 W의 값에 대해 우리는 더 큰

1143
00:50:59,722 --> 00:51:01,360
값으로 나눕니다.

1144
00:51:01,360 --> 00:51:03,243
그래서 우리는 그 방향으로 멀리 나아가지 않을 것입니다.

1145
00:51:03,243 --> 00:51:05,660
더 평평한 영역에서는 더 작은 항으로

1146
00:51:05,660 --> 00:51:08,340
나누기 때문에 더 멀리 나아갈 것입니다.

1147
00:51:08,340 --> 00:51:10,608
이것이 기본적인 직관입니다.

1148
00:51:10,608 --> 00:51:12,900
그리고 이는 누군가가 이전에 질문했던, 우리가 방향으로

1149
00:51:12,900 --> 00:51:14,860
나아가는 방식을 바꿀 수 있는지에 대한 질문을

1150
00:51:14,860 --> 00:51:15,760
매우 잘 해결합니다.

1151
00:51:15,760 --> 00:51:17,040
그리고 이것이 바로 여기서 하고 있는 것입니다.

1152
00:51:17,040 --> 00:51:18,457
여전히 학습률이

1153
00:51:18,457 --> 00:51:20,660
있지만, 누적된 제곱 기울기의

1154
00:51:20,660 --> 00:51:23,940
제곱근으로 나누고 있어, 손실 경관의

1155
00:51:23,940 --> 00:51:28,020
평평한 영역에서는 더 큰 단계로, 매우 가파른

1156
00:51:28,020 --> 00:51:31,340
영역에서는 더 짧은 단계로 나아갑니다.

1157
00:51:31,340 --> 00:51:33,680
누군가 설명해줄 수 있나요? 저는 간단한

1158
00:51:33,680 --> 00:51:37,200
요약을 했지만, 코드의 이 특정 줄에서 무슨 일이 일어나는지요?

1159
00:51:39,820 --> 00:51:42,480
우리의 기울기 단계 방향에 무슨 일이 일어납니까?

1160
00:51:42,480 --> 00:51:44,220
어떻게 바뀌나요?

1161
00:51:44,220 --> 00:51:45,785
우리는 현재 기울기와

1162
00:51:45,785 --> 00:51:47,660
과거 기울기에

1163
00:51:47,660 --> 00:51:50,340
의존하는 이 값으로 나누고 있습니다.

1164
00:51:50,340 --> 00:51:52,060
이 값들이 매우

1165
00:51:52,060 --> 00:51:54,440
크면, 이는 벡터 연산입니다.

1166
00:51:54,440 --> 00:51:57,520
우리는 여기에서 도함수 집합을 가지고 있습니다.

1167
00:51:57,520 --> 00:52:00,470
그리고 우리는 또 다른 제곱 기울기 값

1168
00:52:00,470 --> 00:52:02,810
집합으로 요소별로 나누고 있습니다.

1169
00:52:02,810 --> 00:52:06,170
그래서 매우 커지면 분모가 매우 커지고, 그 방향으로의

1170
00:52:06,170 --> 00:52:07,830
단계가 효과적으로 줄어듭니다.

1171
00:52:07,830 --> 00:52:10,270
왜냐하면 큰 값으로 나누고 있기 때문입니다.

1172
00:52:10,270 --> 00:52:12,030
그리고 매우 작은 값일 때는 단계가

1173
00:52:12,030 --> 00:52:15,570
훨씬 커집니다. 왜냐하면 기울기 제곱 항이 작기 때문입니다.

1174
00:52:15,570 --> 00:52:18,470
그래서 분모에 있고 효과적인

1175
00:52:18,470 --> 00:52:21,310
단계 크기를 증가시키고 있습니다.

1176
00:52:21,310 --> 00:52:21,930
오, 맞아요.

1177
00:52:21,930 --> 00:52:25,390
그래서 아마도 매우 좁은 계곡이 있는

1178
00:52:25,390 --> 00:52:28,910
이런 유형의 예를 위해서입니다.

1179
00:52:28,910 --> 00:52:33,390
그곳에서 더 평평한 방향으로 이동하고 싶습니다.

1180
00:52:33,390 --> 00:52:35,910
네, 질문은 이 맥락에서 작은 기울기가

1181
00:52:35,910 --> 00:52:37,210
무엇을 의미하는가입니다.

1182
00:52:37,210 --> 00:52:39,990
그리고 이것이 어떻게 우리가 가파른

1183
00:52:39,990 --> 00:52:43,310
방향으로는 덜 이동하고 평평한 방향으로는 더 많이

1184
00:52:43,310 --> 00:52:45,430
이동하는 데 도움이 되는가입니다.

1185
00:52:45,430 --> 00:52:46,050
네, 네.

1186
00:52:46,050 --> 00:52:48,110
그래서 사실 이건 아마도 세 가지 다른

1187
00:52:48,110 --> 00:52:50,810
접근 방식을 비교하는 훌륭한 시각적 자료라고 생각합니다.

1188
00:52:50,810 --> 00:52:52,950
그래서 우리는 모멘텀을 사용한 것을 가지고 있는데,

1189
00:52:52,950 --> 00:52:55,730
이전에 질문이 있었던 것처럼 초과하는 것을 볼 수 있습니다.

1190
00:52:55,730 --> 00:52:57,730
하지만 그 후 다시 돌아옵니다.

1191
00:52:57,730 --> 00:53:01,312
SGD는 항상 고정된 방향으로만 이동하기 때문에

1192
00:53:01,312 --> 00:53:02,270
느립니다.

1193
00:53:02,270 --> 00:53:04,450
그리고 우리가 방금 언급한 RMSProp이 있습니다.

1194
00:53:04,450 --> 00:53:07,890
RMSProp의 작동 방식은 제가 여기서

1195
00:53:07,890 --> 00:53:10,170
마우스를 움직이는 방향의 기울기가

1196
00:53:10,170 --> 00:53:15,390
더 높기 때문에 기울기 제곱 항이 더 커져서 그 방향으로 덜 이동합니다.

1197
00:53:15,390 --> 00:53:18,950
죄송합니다, 그 방향으로 덜 이동합니다.

1198
00:53:18,950 --> 00:53:21,970
그래서 실제로 여기서 중앙으로 빠르게 방향을 바꾸기 시작하는

1199
00:53:21,970 --> 00:53:24,950
것을 볼 수 있습니다. 이 시점에서 평평한 경관이지만 그

1200
00:53:24,950 --> 00:53:26,910
방향으로 더 많이 이동하고 있습니다.

1201
00:53:26,910 --> 00:53:29,835
그래서 우리는 실제로 가파른 방향으로는 덜 가고 평평한

1202
00:53:29,835 --> 00:53:31,210
방향으로는 더 많이 가는 기반으로

1203
00:53:31,210 --> 00:53:32,550
방향을 변경하고 있습니다.

1204
00:53:32,550 --> 00:53:33,050
그래서 이

1205
00:53:33,050 --> 00:53:34,083
세 가지입니다.

1206
00:53:34,083 --> 00:53:35,750
그리고 우리가 논의할

1207
00:53:35,750 --> 00:53:38,970
또 하나가 있는데, 현대 딥러닝에서 가장

1208
00:53:38,970 --> 00:53:41,130
인기 있는 최적화기입니다.

1209
00:53:41,130 --> 00:53:46,090
그래서 SGD 모멘텀과 RMSProp의 조합입니다.

1210
00:53:46,090 --> 00:53:49,250
그래서 여기서 아담 최적화기가 거의 무엇인지,

1211
00:53:49,250 --> 00:53:51,923
딥러닝에서 가장 인기 있는 최적화기입니다.

1212
00:53:51,923 --> 00:53:54,090
그리고 이제 그것을 이해하는 데 필요한

1213
00:53:54,090 --> 00:53:56,340
모든 전제 지식을 가지고 있습니다.

1214
00:53:56,340 --> 00:53:57,560
그래서 그것을 봅니다.

1215
00:53:57,560 --> 00:53:59,660
그리고 여기 빨간색의

1216
00:53:59,660 --> 00:54:03,060
첫 번째 항은 기본적으로 우리가 이전에

1217
00:54:03,060 --> 00:54:09,120
설명한 모멘텀으로, 현재가 있고, 베타 1이 모멘텀 항이며,

1218
00:54:09,120 --> 00:54:11,360
여기 속도가 있습니다.

1219
00:54:11,360 --> 00:54:14,060
그리고 우리는 실행 평균을 취하고 있습니다.

1220
00:54:14,060 --> 00:54:17,140
여기 두 번째 모멘트는

1221
00:54:17,140 --> 00:54:20,100
RMSProp의 기울기 제곱 항입니다.

1222
00:54:20,100 --> 00:54:21,860
그리고 우리는 여기서 학습률을

1223
00:54:21,860 --> 00:54:24,660
단계 크기 대신 속도로 곱하는 동일한 작업을

1224
00:54:24,660 --> 00:54:25,682
하고 있습니다.

1225
00:54:25,682 --> 00:54:27,140
하지만 이제 우리는 여전히

1226
00:54:27,140 --> 00:54:29,740
제곱근을 취하는 작업을 하고 있습니다.

1227
00:54:29,740 --> 00:54:31,280
그리고 여기 두 번째 모멘트입니다.

1228
00:54:31,280 --> 00:54:33,363
그들이 첫 번째 모멘트와 두 번째

1229
00:54:33,363 --> 00:54:35,860
모멘트를 사용하는 이유는 물리학과

1230
00:54:35,860 --> 00:54:38,580
역학과의 관계와 같지만, 기본적으로 우리가

1231
00:54:38,580 --> 00:54:41,820
이전에 설명한 두 가지의 조합입니다. 평평한 방향으로의

1232
00:54:41,820 --> 00:54:43,700
이동을 가속하고 가파른

1233
00:54:43,700 --> 00:54:45,260
방향으로는 감쇠하며,

1234
00:54:45,260 --> 00:54:47,680
모멘텀과 속도의 개념을 추가하고 있습니다.

1235
00:54:47,680 --> 00:54:50,220
그래서 같은 방향으로 계속

1236
00:54:50,220 --> 00:54:52,660
이동하면 점차 속도가 증가합니다.

1237
00:54:52,660 --> 00:54:55,150
현재 작성된 대로라면, 이것은

1238
00:54:55,150 --> 00:54:59,110
실제로 첫 번째 시간 단계에서 문제를 일으킬 것입니다.

1239
00:54:59,110 --> 00:55:03,070
그리고 왜 그런지 조금 불확실할 수 있지만, 누군가

1240
00:55:03,070 --> 00:55:05,450
추측할 때까지 기다리겠습니다.

1241
00:55:05,450 --> 00:55:08,510
하나 주목할 점은 이 베타들, beta1과

1242
00:55:08,510 --> 00:55:13,230
beta2가 보통 1에 매우 가깝게 초기화된다는 것입니다. 즉, 0.9,
0.

1243
00:55:13,230 --> 00:55:16,430
999로 초기화되며, 이 두 값은 0으로 초기화됩니다.

1244
00:55:16,430 --> 00:55:19,030
따라서 첫 번째 시간 단계에서

1245
00:55:19,030 --> 00:55:20,750
Adam의 이 공식을

1246
00:55:20,750 --> 00:55:24,430
사용하면 원하지 않는 동작이 발생할 수 있습니다.

1247
00:55:24,430 --> 00:55:28,350
또 다른 점은 두 번째 모멘트 계산과 관련이

1248
00:55:28,350 --> 00:55:29,470
있습니다.

1249
00:55:29,470 --> 00:55:32,010
이것이 여기서의 주요 문제입니다.

1250
00:55:32,010 --> 00:55:33,830
두 번째 모멘트를

1251
00:55:33,830 --> 00:55:38,110
계산한 후 다음 줄에서 사용하면 문제가 발생합니다.

1252
00:55:38,110 --> 00:55:39,922
네, 분모가 기본적으로 0입니다.

1253
00:55:39,922 --> 00:55:41,130
네, 그게 정확한 문제입니다.

1254
00:55:41,130 --> 00:55:44,190
0에서 시작하므로 이 항은 0입니다.

1255
00:55:44,190 --> 00:55:46,670
당신은 매우 큰 베타를 가지고 있습니다.

1256
00:55:46,670 --> 00:55:48,730
이 값은 매우 작습니다.

1257
00:55:48,730 --> 00:55:51,190
그리고 첫 번째 단계에서 기울기가 그리 크지 않다면

1258
00:55:51,190 --> 00:55:54,300
이 전체 항이 기본적으로 0에 매우 가깝게 될 수 있습니다.

1259
00:55:54,300 --> 00:55:56,480
이제 우리는 0에 매우 가까운 것으로 나누고

1260
00:55:56,480 --> 00:55:59,000
있으며, 이는 기울기가 작았음에도 불구하고

1261
00:55:59,000 --> 00:56:00,760
매우 큰 초기 단계를 생성합니다.

1262
00:56:00,760 --> 00:56:03,020
그래서 아마도 우리가 정말로 원하는 것은 아닙니다.

1263
00:56:03,020 --> 00:56:05,800
Adam의 마지막 요소는 여기에서 이러한

1264
00:56:05,800 --> 00:56:08,080
편향 항을 추가하는 것으로, 이는

1265
00:56:08,080 --> 00:56:10,560
이제 훈련의 시간 단계에 의존하는 이

1266
00:56:10,560 --> 00:56:12,780
문제를 해결하기 위한 것입니다.

1267
00:56:12,780 --> 00:56:15,168
이것은 숙제에서도 다룰 내용이라고

1268
00:56:15,168 --> 00:56:16,460
생각합니다.

1269
00:56:16,460 --> 00:56:18,880
나는 Adam의 기본 직관을 제공하고

1270
00:56:18,880 --> 00:56:21,980
싶습니다. 왜 순진한 구현이 작동하지 않는지,

1271
00:56:21,980 --> 00:56:24,292
즉 매우 큰 초기 단계 때문입니다.

1272
00:56:24,292 --> 00:56:26,500
그리고 숙제에서 이를 구현해 볼 것입니다.

1273
00:56:26,500 --> 00:56:28,620
그리고 시간 단계가 어떻게 사용되는지 볼 것입니다.

1274
00:56:28,620 --> 00:56:31,080
하지만 기본 아이디어는 매우 큰 초기 단계를

1275
00:56:31,080 --> 00:56:32,220
고려하는 것입니다.

1276
00:56:32,220 --> 00:56:36,040
시간 단계가 증가함에 따라 이러한 바이어스

1277
00:56:36,040 --> 00:56:38,760
항은 그리 필요하지 않습니다.

1278
00:56:38,760 --> 00:56:40,240
좋아요, 멋지네요.

1279
00:56:40,240 --> 00:56:43,640
이것들은 사람들이 일반적으로 사용하는 좋은 기본값입니다.

1280
00:56:43,640 --> 00:56:47,080
Adam으로 모델을 훈련시키면 이 값을 사용하고, 아마도 잘

1281
00:56:47,080 --> 00:56:49,068
작동할 것이고, 아닐 수도 있습니다.

1282
00:56:49,068 --> 00:56:50,360
하지만 좋은 출발점입니다.

1283
00:56:50,360 --> 00:56:54,297
그리고 나머지 슬라이드에서, 학습률이 올바른지 어떻게 알

1284
00:56:54,297 --> 00:56:56,880
수 있는지에 대해 이야기할 것입니다.

1285
00:56:56,880 --> 00:56:59,200
이 다른 값들이 올바른지 어떻게 알 수 있는지.

1286
00:56:59,200 --> 00:57:02,600
시간을 고려하여 조금 더 빠르게 진행하겠습니다.

1287
00:57:02,600 --> 00:57:04,340
하지만 기본 아이디어는 이러한 다양한

1288
00:57:04,340 --> 00:57:07,262
최적화 알고리즘이 수렴하는 것을 볼 수 있다는 것입니다.

1289
00:57:07,262 --> 00:57:08,720
그들은 모두 다른 특성을 가지고 있습니다.

1290
00:57:08,720 --> 00:57:12,260
Adam이 RMSProp과 모멘텀을 가진 SGD의 조합이라는

1291
00:57:12,260 --> 00:57:14,860
것을 볼 수 있으며, 두 가지 특성을 모두 가지고

1292
00:57:14,860 --> 00:57:16,800
있어 시각적으로 매우 흥미롭습니다.

1293
00:57:16,800 --> 00:57:20,220
이는 우리의 직관과 일치합니다.

1294
00:57:20,220 --> 00:57:22,500
Adam과 관련된 마지막 주제는

1295
00:57:22,500 --> 00:57:25,660
정규화가 최적화 알고리즘과 어떻게 상호작용하는지를 살펴볼

1296
00:57:25,660 --> 00:57:27,080
수 있다는 것입니다.

1297
00:57:27,080 --> 00:57:30,320
예를 들어, L2 정규화를 사용하면 최적화기가

1298
00:57:30,320 --> 00:57:33,240
어떻게 작동하는지에 영향을 미칩니까?

1299
00:57:33,240 --> 00:57:35,420
그리고 그 답은 사실 즉시 명확하지 않다고

1300
00:57:35,420 --> 00:57:36,260
생각합니다.

1301
00:57:36,260 --> 00:57:37,760
그리고 여러 가지 방법으로 할 수 있습니다.

1302
00:57:37,760 --> 00:57:41,340
기본 Adam에서는 그래디언트를 계산할 때 L2를

1303
00:57:41,340 --> 00:57:42,480
계산합니다.

1304
00:57:42,480 --> 00:57:45,420
그래디언트를 살펴보았고, 우리의 손실 부분이

1305
00:57:45,420 --> 00:57:48,380
있었습니다. 즉, 데이터 손실 부분과

1306
00:57:48,380 --> 00:57:49,700
정규화 손실입니다.

1307
00:57:49,700 --> 00:57:51,430
Adam은 그래디언트를 계산할

1308
00:57:51,430 --> 00:57:53,350
때 두 가지를 모두 사용합니다.

1309
00:57:53,350 --> 00:57:56,550
하지만 AdamW는 기본적으로 모든

1310
00:57:56,550 --> 00:57:59,230
모멘트 계산과 모든 단계에 대해

1311
00:57:59,230 --> 00:58:02,350
데이터 손실만을 보고, 마지막에 정규화

1312
00:58:02,350 --> 00:58:03,830
항을 추가합니다.

1313
00:58:03,830 --> 00:58:05,892
기본적으로 제가 여러분에게 설명하려는

1314
00:58:05,892 --> 00:58:07,350
것은 정규화를

1315
00:58:07,350 --> 00:58:10,390
최적화기에 통합하는 방법에 유연성이 있다는 것입니다.

1316
00:58:10,390 --> 00:58:12,110
가중치 감소는 일반적으로

1317
00:58:12,110 --> 00:58:14,770
마지막에 L2 정규화를

1318
00:58:14,770 --> 00:58:17,670
추가하고, 속도와 모멘텀을 계산하는 실제

1319
00:58:17,670 --> 00:58:19,470
최적화기에 포함하지

1320
00:58:19,470 --> 00:58:21,158
않을 때 발생합니다.

1321
00:58:21,158 --> 00:58:22,450
이것이 주요 차이점입니다.

1322
00:58:22,450 --> 00:58:25,570
때때로 많은 설정에서 AdamW가 약간 더

1323
00:58:25,570 --> 00:58:26,770
잘 작동합니다.

1324
00:58:26,770 --> 00:58:31,550
Meta의 Llama 시리즈는 모두 AdamW를 사용하며, 그들이

1325
00:58:31,550 --> 00:58:34,630
약간 더 잘 작동하기 때문이라고 추측합니다.

1326
00:58:34,630 --> 00:58:36,210
우리는 하나의 함수 최적화기가 있습니다.

1327
00:58:36,210 --> 00:58:37,950
왜 두 개로 나누나요?

1328
00:58:37,950 --> 00:58:42,510
네, 하나의 함수로 혼합하면 Adam이 하는 것입니다.

1329
00:58:42,510 --> 00:58:45,890
AdamW는 특별히 두 개로 분리합니다.

1330
00:58:45,890 --> 00:58:48,870
그렇게 하고 싶은 이유는 속도와

1331
00:58:48,870 --> 00:58:52,080
모멘텀을 실제 가중치의 함수가 아니라

1332
00:58:52,080 --> 00:58:53,880
손실의 함수로 만들고

1333
00:58:53,880 --> 00:58:55,620
싶기 때문입니다.

1334
00:58:55,620 --> 00:58:58,200
실제 가중치 값과 독립적으로 손실

1335
00:58:58,200 --> 00:59:00,832
경관을 탐색하려고 한다면, 그래서

1336
00:59:00,832 --> 00:59:02,540
분리하고 싶을 수 있습니다.

1337
00:59:02,540 --> 00:59:04,580
하지만 여전히 정규화 항이 필요할 수 있습니다.

1338
00:59:04,580 --> 00:59:06,663
그러나 그것이 모멘트 계산에 간섭하지 않기를

1339
00:59:06,663 --> 00:59:07,220
원합니다.

1340
00:59:07,220 --> 00:59:09,535
그래서 이것이 그들이 그렇게 하는 구체적인 이유입니다.

1341
00:59:09,535 --> 00:59:11,160
궁극적으로는 경험적입니다. 두 가지를 시도해보고

1342
00:59:11,160 --> 00:59:12,618
어떤 것이 더 잘 작동하는지 확인합니다.

1343
00:59:12,618 --> 00:59:15,560
하지만 이것이 그렇게 하는 이유입니다.

1344
00:59:15,560 --> 00:59:16,082
좋아요, 멋지네요.

1345
00:59:16,082 --> 00:59:17,540
이제 학습률에 대해 이야기하겠습니다.

1346
00:59:17,540 --> 00:59:21,240
학습률을 선택하는 방법에는 여러

1347
00:59:21,240 --> 00:59:23,300
가지가 있습니다.

1348
00:59:23,300 --> 00:59:25,640
때때로 매우 높은 학습률을

1349
00:59:25,640 --> 00:59:27,098
얻을 수 있으며,

1350
00:59:27,098 --> 00:59:30,200
이 경우 손실이 손실 경관에서

1351
00:59:30,200 --> 00:59:32,958
진동하면서 매우 커집니다.

1352
00:59:32,958 --> 00:59:34,500
매우 낮은 학습률이 있으면

1353
00:59:34,500 --> 00:59:36,440
문제는 매우 느리게 수렴한다는 것입니다.

1354
00:59:36,440 --> 00:59:39,300
높은 학습률이 있지만 진동하지

1355
00:59:39,300 --> 00:59:42,000
않으면 지역 최소값 주위에서

1356
00:59:42,000 --> 00:59:44,260
부딪히고 있을 수 있지만,

1357
00:59:44,260 --> 00:59:46,380
학습률이 너무 높아 실제로

1358
00:59:46,380 --> 00:59:48,660
더 낮아질 수 없습니다.

1359
00:59:48,660 --> 00:59:50,060
이상적으로 좋은

1360
00:59:50,060 --> 00:59:52,860
학습률은 시간이 지남에 따라 손실이

1361
00:59:52,860 --> 00:59:55,020
빠르게 감소하게 하고, 모델을

1362
00:59:55,020 --> 00:59:59,900
계속 훈련하면서 지속적인 개선을 보여주는 특성을 가져야 합니다.

1363
00:59:59,900 --> 01:00:02,900
실제로 상황에 따라 이러한 많은 것들이 좋은

1364
01:00:02,900 --> 01:00:04,740
학습률이 될 수 있으며, 훈련

1365
01:00:04,740 --> 01:00:06,200
단계에 따라 다르기도

1366
01:00:06,200 --> 01:00:09,500
합니다. 오늘 강의에서 마지막으로 논의할 사항입니다.

1367
01:00:09,500 --> 01:00:11,740
모델을 훈련하면서 학습률을 실제로

1368
01:00:11,740 --> 01:00:13,322
변경할 수 있습니다.

1369
01:00:13,322 --> 01:00:15,780
고정된 학습률이나 스텝 크기를 항상 가질 필요는

1370
01:00:15,780 --> 01:00:16,280
없습니다.

1371
01:00:16,280 --> 01:00:19,820
그리고 최신 딥러닝 모델들은 훈련 중

1372
01:00:19,820 --> 01:00:21,680
학습률을 다르게

1373
01:00:21,680 --> 01:00:23,540
조절하는 다양한 방법을

1374
01:00:23,540 --> 01:00:25,340
가지고 있습니다.

1375
01:00:25,340 --> 01:00:28,980
그래서 정말 간단한 방법 중 하나는 고정된

1376
01:00:28,980 --> 01:00:33,740
반복 수 이후에 학습률의 1/10을 취하고 계속

1377
01:00:33,740 --> 01:00:35,000
훈련하는 것입니다.

1378
01:00:35,000 --> 01:00:39,460
이렇게 하면 학습률이 너무 높아 더 이상 수렴할 수 없는

1379
01:00:39,460 --> 01:00:42,320
문제를 해결할 수 있으며, 학습률을

1380
01:00:42,320 --> 01:00:44,580
줄이면 손실 경관에서 더 낮은

1381
01:00:44,580 --> 01:00:46,220
값을 얻을 수 있습니다.

1382
01:00:46,220 --> 01:00:48,990
이 방법은 ResNet 훈련 시 매우 일반적으로 사용됩니다.

1383
01:00:48,990 --> 01:00:51,850
ResNet은 매우 인기 있는 합성곱 신경망

1384
01:00:51,850 --> 01:00:54,310
유형으로, 강의 후반에 다룰 것입니다.

1385
01:00:54,310 --> 01:00:57,810
또 다른 방법은 코사인 학습률 감소입니다.

1386
01:00:57,810 --> 01:01:00,410
이 방법도 매우 인기가 많습니다.

1387
01:01:00,410 --> 01:01:03,870
여기서 기본적으로 최대 학습률에서

1388
01:01:03,870 --> 01:01:05,870
시작하여 0으로 내려가는

1389
01:01:05,870 --> 01:01:09,170
코사인 파형의 절반과 같습니다.

1390
01:01:09,170 --> 01:01:11,570
그리고 끝까지 0으로 내려갑니다.

1391
01:01:11,570 --> 01:01:15,210
이것은 1.2 코사인 형태를 따르며, 이를

1392
01:01:15,210 --> 01:01:17,130
계산하는 공식이 있습니다.

1393
01:01:17,130 --> 01:01:18,665
너무 많은 세부사항에 들어가지는

1394
01:01:18,665 --> 01:01:21,290
않겠지만, 기본 아이디어는 다양한 방법이 있다는 것입니다.

1395
01:01:21,290 --> 01:01:24,490
손실이 코사인 학습률 스케줄러를 사용할 때,

1396
01:01:24,490 --> 01:01:25,990
훈련 중간 부분에서

1397
01:01:25,990 --> 01:01:29,310
꽤 좋은 지속적인 이득을 얻는 형태를 자주

1398
01:01:29,310 --> 01:01:30,670
볼 수 있습니다.

1399
01:01:30,670 --> 01:01:32,910
하지만 기본 아이디어는 훈련 중 손실의

1400
01:01:32,910 --> 01:01:34,870
실제 형태가 사용하는 스케줄러에

1401
01:01:34,870 --> 01:01:36,250
크게 의존한다는 것입니다.

1402
01:01:36,250 --> 01:01:38,330
이것이 제가 전달하고자 하는 기본 아이디어입니다.

1403
01:01:38,330 --> 01:01:40,497
예를 들어, 훈련 중 학습률의

1404
01:01:40,497 --> 01:01:42,710
1/10을 취하는 것을 실제로

1405
01:01:42,710 --> 01:01:45,763
볼 수 있는 이와는 매우 다르게 보입니다.

1406
01:01:45,763 --> 01:01:48,180
또 다른 방법은 선형 학습률 감소입니다.

1407
01:01:48,180 --> 01:01:50,003
그냥 직선을 따릅니다.

1408
01:01:50,003 --> 01:01:52,420
역제곱근 등을 사용할 수 있습니다.

1409
01:01:52,420 --> 01:01:54,640
훈련 중 학습률을 조정하는

1410
01:01:54,640 --> 01:01:57,277
방법은 사실상 무한정입니다.

1411
01:01:57,277 --> 01:01:59,360
훈련하는 모델의 유형과 가장 잘

1412
01:01:59,360 --> 01:02:00,735
작동하는 것에 따라 가장 잘

1413
01:02:00,735 --> 01:02:02,500
작동하는 것을 선택하면 됩니다.

1414
01:02:02,500 --> 01:02:05,080
하지만 여기에서 여러분의 설정에서 잘 수행할 수

1415
01:02:05,080 --> 01:02:06,720
있는 몇 가지 방법을 제안합니다.

1416
01:02:06,720 --> 01:02:09,360
또한 정말 인기 있는 전략은 선형

1417
01:02:09,360 --> 01:02:10,980
워밍업을 사용하는 것입니다.

1418
01:02:10,980 --> 01:02:13,580
최대 학습률에서 시작하는 대신,

1419
01:02:13,580 --> 01:02:15,520
고정된 반복 수를

1420
01:02:15,520 --> 01:02:20,000
사용하여 최대 값까지 선형적으로 워밍업한 후, 이후에

1421
01:02:20,000 --> 01:02:22,620
원하는 스케줄러를 사용합니다.

1422
01:02:22,620 --> 01:02:24,880
예를 들어 선형 워밍업 후

1423
01:02:24,880 --> 01:02:27,080
역제곱근을 사용할 수 있습니다.

1424
01:02:27,080 --> 01:02:28,640
또는 선형 워밍업 후

1425
01:02:28,640 --> 01:02:32,480
코사인을 사용하는 것이 모델 훈련에 매우 인기 있는 설정입니다.

1426
01:02:32,480 --> 01:02:38,040
마지막으로, 선형 스케일링 가설 또는 선형

1427
01:02:38,040 --> 01:02:43,320
스케일링 법칙이라고 불리는 경험적 규칙이

1428
01:02:43,320 --> 01:02:44,560
있습니다.

1429
01:02:44,560 --> 01:02:45,310
이름을 잊어버렸습니다.

1430
01:02:45,310 --> 01:02:46,893
아마 선형 스케일링

1431
01:02:46,893 --> 01:02:49,850
법칙일 것입니다. 배치 크기나

1432
01:02:49,850 --> 01:02:53,770
업데이트당 훈련 샘플 수를 n만큼 증가시키면 학습률도

1433
01:02:53,770 --> 01:02:57,070
n만큼 스케일해야 한다고 보여줍니다.

1434
01:02:57,070 --> 01:02:58,910
배치 크기를

1435
01:02:58,910 --> 01:03:02,770
증가시키면 학습률도 비례하여 증가시켜야

1436
01:03:02,770 --> 01:03:03,970
합니다.

1437
01:03:03,970 --> 01:03:06,670
이와 관련된 수학은 다소 복잡하며,

1438
01:03:06,670 --> 01:03:09,150
경험적 규칙에 가깝습니다.

1439
01:03:09,150 --> 01:03:12,262
사람들은 이것이 유용할 수 있는 이유에 대한 수학적 증명을

1440
01:03:12,262 --> 01:03:13,470
시도해 보았습니다.

1441
01:03:13,470 --> 01:03:17,530
하지만 배치의 그래디언트 변동과 배치당 계산하는

1442
01:03:17,530 --> 01:03:21,530
그래디언트 수에 따라 다르지만, 실제로 이는

1443
01:03:21,530 --> 01:03:23,090
많은 문제에 대해

1444
01:03:23,090 --> 01:03:25,590
경험적으로 사실로 입증되었습니다.

1445
01:03:25,590 --> 01:03:26,990
그래서 이것은 좋은 경험 법칙입니다.

1446
01:03:26,990 --> 01:03:28,690
승리하는 레시피가

1447
01:03:28,690 --> 01:03:31,130
있지만 배치 크기를 늘리고

1448
01:03:31,130 --> 01:03:35,130
싶다면, 학습률도 같은 비율로 늘리세요.

1449
01:03:35,130 --> 01:03:35,810
좋아요.

1450
01:03:35,810 --> 01:03:39,250
그리고 제가 간단히 언급할 마지막

1451
01:03:39,250 --> 01:03:44,070
사항은 해시안에 대한 아이디어로, 이전에

1452
01:03:44,070 --> 01:03:45,630
누군가 질문한

1453
01:03:45,630 --> 01:03:46,990
내용입니다.

1454
01:03:46,990 --> 01:03:49,028
그래서 이 부분에 대해 깊이 있게 이야기하지는 않겠지만, 이

1455
01:03:49,028 --> 01:03:50,570
개념이 존재한다는 것을 알려드리고 싶습니다.

1456
01:03:50,570 --> 01:03:53,550
이것은 우리가 강의에서 많이 다루는 내용은 아닙니다.

1457
01:03:53,550 --> 01:03:56,470
기본 아이디어는 현재 그래디언트를

1458
01:03:56,470 --> 01:03:59,170
사용하여 손실 경관을 탐색할

1459
01:03:59,170 --> 01:04:00,670
하강 방향의

1460
01:04:00,670 --> 01:04:03,630
선형 근사를 형성하는 것입니다.

1461
01:04:03,630 --> 01:04:05,815
우리는 방향을 보고 그 방향으로

1462
01:04:05,815 --> 01:04:07,690
일반적인 단계를 밟습니다.

1463
01:04:07,690 --> 01:04:12,470
그리고 우리는 모멘텀과 RMSProp과 같은 멋진 것들을 추가하여

1464
01:04:12,470 --> 01:04:15,085
가파른 방향에서 감속하고 있습니다.

1465
01:04:15,085 --> 01:04:16,210
하지만 이것이 기본 아이디어입니다.

1466
01:04:16,210 --> 01:04:21,270
우리는 각 시간 단계에서 이 그래디언트를 사용하고 있습니다.

1467
01:04:21,270 --> 01:04:24,530
해시안의 아이디어는 그래디언트를

1468
01:04:24,530 --> 01:04:33,030
사용하는 대신, 해당 지점의 도함수나 해시안을 기반으로 함수에 대해

1469
01:04:33,030 --> 01:04:36,390
2차 또는 2차 다항식을 맞추려고

1470
01:04:36,390 --> 01:04:38,750
시도하는 것입니다.

1471
01:04:38,750 --> 01:04:41,910
그리고 이렇게 최소값을 찾으려고 합니다.

1472
01:04:41,910 --> 01:04:44,640
특정 최적화 문제에서는 실제로

1473
01:04:44,640 --> 01:04:46,260
매우 잘 작동합니다.

1474
01:04:46,260 --> 01:04:49,120
하지만 일반적으로 우리는 딥러닝에서 사용하지 않습니다.

1475
01:04:49,120 --> 01:04:51,220
왜냐하면 두 가지가 필요하기 때문입니다.

1476
01:04:51,220 --> 01:04:53,620
하나는 테일러 급수 전개를

1477
01:04:53,620 --> 01:04:55,920
수행해야 하며, 현재 우리는 도함수의

1478
01:04:55,920 --> 01:04:57,420
첫 번째 부분만

1479
01:04:57,420 --> 01:05:00,720
수행하고 있지만, 두 번째 혼합 도함수를

1480
01:05:00,720 --> 01:05:05,100
계산할 수 있어야 하며, 이는 이미 어려울 수 있습니다. 그리고 그 위에,
모델의 모든 매개변수에

1481
01:05:05,100 --> 01:05:10,588
대한 이 혼합 도함수는 수백만 또는 수십억

1482
01:05:10,588 --> 01:05:12,880
개의 매개변수를 가진

1483
01:05:12,880 --> 01:05:16,040
신경망을 가질 때 매우 커질

1484
01:05:16,040 --> 01:05:17,520
수 있습니다.

1485
01:05:17,520 --> 01:05:24,520
그래서 실제로 우리는 이 행렬들이 너무 커지기 때문에 사용하지

1486
01:05:24,520 --> 01:05:25,180
않습니다.

1487
01:05:25,180 --> 01:05:27,060
따라서 이를 실행하려고 하면 컴퓨터의

1488
01:05:27,060 --> 01:05:27,935
메모리가 부족해집니다.

1489
01:05:27,935 --> 01:05:29,900
특히 GPU 메모리에서 그렇습니다.

1490
01:05:29,900 --> 01:05:31,760
하지만 더 작은 모델을

1491
01:05:31,760 --> 01:05:36,400
훈련시키거나 최소값에 더 나은 단계를 얻기 위해 훨씬 더 많은 시간을

1492
01:05:36,400 --> 01:05:39,160
투자하는 것이 괜찮다면, 이 방법을 고려할

1493
01:05:39,160 --> 01:05:40,167
수 있습니다.

1494
01:05:40,167 --> 01:05:42,250
문제 세트에 따라 다르지만, 더 작은 모델의

1495
01:05:42,250 --> 01:05:43,510
경우 실제로 잘 작동합니다.

1496
01:05:43,510 --> 01:05:45,270
하지만 우리가 훈련하는 이러한 대형 신경망의

1497
01:05:45,270 --> 01:05:47,950
경우, 메모리 제한으로 인해 기본적으로 이 방법을 사용하지 않습니다.

1498
01:05:47,950 --> 01:05:50,410
해시안을 계산하려고 소비한 모든

1499
01:05:50,410 --> 01:05:52,933
시간보다 훈련 중 더 많은 데이터를

1500
01:05:52,933 --> 01:05:55,100
보는 것이 더 낫습니다.

1501
01:05:58,170 --> 01:05:58,670
좋습니다.

1502
01:05:58,670 --> 01:06:03,690
그래서 여러분에게 유용할 수 있는 몇 가지 결론적인 생각이

1503
01:06:03,690 --> 01:06:04,910
있습니다.

1504
01:06:04,910 --> 01:06:08,250
새로운 문제를 다룰 때 첫 번째 모델을 훈련시키기 위한

1505
01:06:08,250 --> 01:06:11,650
정말 좋은 기본 선택은 Adam 또는 AdamW입니다.

1506
01:06:11,650 --> 01:06:14,032
저는 이 도메인에서 추천합니다.

1507
01:06:14,032 --> 01:06:16,490
그리고 일정한 학습률을 사용하더라도 괜찮게 작동할 수

1508
01:06:16,490 --> 01:06:16,990
있습니다.

1509
01:06:16,990 --> 01:06:19,610
그래서 보통 사람들은 일정한 학습률이나 선형

1510
01:06:19,610 --> 01:06:21,610
워밍업 후 코사인 감소와 함께

1511
01:06:21,610 --> 01:06:25,450
Adam 또는 AdamW를 시도합니다. 이 조합은 정말 인기 있습니다.

1512
01:06:25,450 --> 01:06:30,030
또한, SGD와 모멘텀이 때때로 Adam보다 더 나은 성능을 보일 수 있다고
생각합니다.

1513
01:06:30,030 --> 01:06:34,330
하지만 까다로운 점은 일반적으로 값을 더 조정해야

1514
01:06:34,330 --> 01:06:36,850
하므로, 이 RMSProp

1515
01:06:36,850 --> 01:06:40,300
항이 가파른 방향을 고려하지 않기 때문에

1516
01:06:40,300 --> 01:06:43,340
더 많은 학습률을 시도해야 한다는

1517
01:06:43,340 --> 01:06:44,422
것입니다.

1518
01:06:44,422 --> 01:06:46,880
또한 다른 스케줄링 값을 시도해야 할 수도 있지만,

1519
01:06:46,880 --> 01:06:48,658
실제로는 Adam이 테스트에서 가장 좋습니다.

1520
01:06:48,658 --> 01:06:50,700
사람들은 다양한 도메인에서 이것을 시도했으며,

1521
01:06:50,700 --> 01:06:51,658
매우 잘 작동합니다.

1522
01:06:51,658 --> 01:06:55,100
손실 경관에 매우 적응적입니다.

1523
01:06:55,100 --> 01:06:58,280
전체 배치 업데이트를 수행하는 경우,

1524
01:06:58,280 --> 01:06:59,780
각 단계에서

1525
01:06:59,780 --> 01:07:03,380
기본적으로 전체 훈련 세트를 배치 크기에

1526
01:07:03,380 --> 01:07:06,180
맞출 수 있으므로, 1차 최적화를

1527
01:07:06,180 --> 01:07:09,180
넘어 2차 및 그 이상을 살펴보는

1528
01:07:09,180 --> 01:07:11,760
것이 좋습니다. 데이터 세트가

1529
01:07:11,760 --> 01:07:13,340
그리 크지 않거나

1530
01:07:13,340 --> 01:07:17,260
모델이 그리 크지 않을 수 있으며, 비선형

1531
01:07:17,260 --> 01:07:22,060
업데이트 단계와 더 정교한 전략을 계산하여 최소값을

1532
01:07:22,060 --> 01:07:25,260
찾는 데 이점을 얻을 수 있습니다.

1533
01:07:25,260 --> 01:07:27,243
네, 그래서 우리는 본질적으로 강의를

1534
01:07:27,243 --> 01:07:28,160
마친 것 같습니다.

1535
01:07:28,160 --> 01:07:31,400
앞으로의 내용을 다룬 슬라이드를 제공하겠습니다.

1536
01:07:31,400 --> 01:07:34,220
우리가 이 강의에서 다룬 선형 모델보다

1537
01:07:34,220 --> 01:07:37,080
더 복잡한 함수를 어떻게 최적화할까요?

1538
01:07:37,080 --> 01:07:39,160
다음 강의에서는

1539
01:07:39,160 --> 01:07:44,640
신경망을 살펴볼 것이며, 이는 매우 흥미로운 주제입니다.

1540
01:07:44,640 --> 01:07:47,360
수업에서 논의할 신경망은 기본적으로

1541
01:07:47,360 --> 01:07:49,680
이제 각 층마다 두 개의

1542
01:07:49,680 --> 01:07:51,480
가중치 행렬이 있습니다.

1543
01:07:51,480 --> 01:07:55,320
그리고 그 사이에 비선형성을 가진 무언가가

1544
01:07:55,320 --> 01:07:55,940
있습니다.

1545
01:07:55,940 --> 01:07:58,282
이 경우 가장 일반적인 것은 아닙니다만,

1546
01:07:58,282 --> 01:08:00,240
가장 간단한 것은 바로 이

1547
01:08:00,240 --> 01:08:03,410
ReLU 함수입니다. 이에 대해 더 배우게 될 것입니다.

1548
01:08:03,410 --> 01:08:05,660
기본 아이디어는 이제 두 개의

1549
01:08:05,660 --> 01:08:07,560
가중치 행렬이 있고, 가중치

1550
01:08:07,560 --> 01:08:11,280
행렬 계산 사이에 추가적인 함수가 있다는 것입니다.

1551
01:08:11,280 --> 01:08:13,500
이것은 비선형적이기 때문에 좋습니다.

1552
01:08:13,500 --> 01:08:16,840
이런 데이터를 분류하기 위해 선형 분류기를 구축하려고

1553
01:08:16,840 --> 01:08:18,800
하면, 파란 점과 빨간

1554
01:08:18,800 --> 01:08:20,840
점이 선형적으로 분리될 수 없는

1555
01:08:20,840 --> 01:08:22,359
문제에 직면하게 됩니다.

1556
01:08:22,359 --> 01:08:24,600
하지만 우리가 할 수 있는 몇 가지

1557
01:08:24,600 --> 01:08:26,380
변환이 있거나 모델의 여러

1558
01:08:26,380 --> 01:08:28,640
층을 통해 데이터를 결국 선으로 분리할

1559
01:08:28,640 --> 01:08:31,200
수 있는 방식으로 변환할 수 있습니다.

1560
01:08:31,200 --> 01:08:33,680
이것이 모델의 최종 층이 될 것입니다.

1561
01:08:33,680 --> 01:08:36,730
[오디오 로고]
