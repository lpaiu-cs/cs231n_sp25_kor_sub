1
00:00:05,510 --> 00:00:10,320
오늘은 이미지 분류에 대해 이야기할 텐데요,

2
00:00:10,320 --> 00:00:13,160
기본적으로 지난 강의에서 다룬

3
00:00:13,160 --> 00:00:15,290
이미지 분류 주제를

4
00:00:15,290 --> 00:00:17,430
계속 이어가는 겁니다.

5
00:00:17,430 --> 00:00:23,750
그리고 신경망, 궁극적으로는 합성곱 신경망에 좀

6
00:00:23,750 --> 00:00:26,990
더 가까워지는 주제들도

7
00:00:26,990 --> 00:00:30,020
조금 다뤄보겠습니다.

8
00:00:30,020 --> 00:00:35,780
먼저 선형 분류기부터 시작하겠습니다.

9
00:00:35,780 --> 00:00:43,280
다음 슬라이드로 넘어가면, 지난 강의에서

10
00:00:43,280 --> 00:00:48,350
이야기했던 강의 계획서입니다. 세

11
00:00:48,350 --> 00:00:52,520
가지 주요 주제, 즉 딥러닝

12
00:00:52,520 --> 00:00:57,660
기초, 시각 세계 인지 및

13
00:00:57,660 --> 00:01:02,350
이해, 시각 세계 재구성과

14
00:01:02,350 --> 00:01:05,620
상호작용을 다루었고,

15
00:01:05,620 --> 00:01:08,590
수업에서 다룰 몇

16
00:01:08,590 --> 00:01:14,540
가지 하위 주제도 포함되어 있습니다.

17
00:01:14,540 --> 00:01:17,860
마지막에는 인간 중심 AI 측면에

18
00:01:17,860 --> 00:01:21,100
관한 토론도 할 예정입니다.

19
00:01:21,100 --> 00:01:27,880
오늘 목표는 첫 세 가지 항목, 즉 데이터 기반

20
00:01:27,880 --> 00:01:31,600
접근법(이게 무슨 의미인지

21
00:01:31,600 --> 00:01:38,230
설명드리겠습니다), 선형 분류, 그리고 k-최근접

22
00:01:38,230 --> 00:01:43,120
이웃 알고리즘을 다루는 것입니다.

23
00:01:43,120 --> 00:01:48,050
지난 강의처럼, 이미지

24
00:01:48,050 --> 00:01:55,750
분류라는 핵심 과제부터 시작하겠습니다.

25
00:01:55,750 --> 00:01:58,930
다시 말하지만, 이것은 컴퓨터 비전에서 핵심 과제입니다.

26
00:01:58,930 --> 00:02:02,070
그리고 우리는 이 과제를 이번 학기 내내 자주 다시

27
00:02:02,070 --> 00:02:03,840
다루게 됩니다. 왜냐하면 아주

28
00:02:03,840 --> 00:02:05,440
좋은 벤치마크이기 때문입니다.

29
00:02:05,440 --> 00:02:09,030
그리고 알고리즘이 어떻게 작동하는지 설명할

30
00:02:09,030 --> 00:02:11,080
몇 가지 예시가 있습니다.

31
00:02:11,080 --> 00:02:17,100
그래서 이것은 우리가 자주 다시 돌아오는 주제 중 하나입니다.

32
00:02:17,100 --> 00:02:21,660
오늘은 이미지 분류 과제를 정의하고, 이미지

33
00:02:21,660 --> 00:02:27,930
분류를 위한 두 가지 데이터 기반 접근법을 소개하려고 합니다.

34
00:02:27,930 --> 00:02:30,480
하나는 최근접 이웃이고,

35
00:02:30,480 --> 00:02:34,240
다른 하나는 선형 분류기입니다.

36
00:02:34,240 --> 00:02:38,580
다른 몇 가지 접근법도 백업 슬라이드에

37
00:02:38,580 --> 00:02:40,660
나와 있습니다.

38
00:02:40,660 --> 00:02:45,120
수업 후에 자유롭게 살펴보셔도 됩니다.

39
00:02:45,120 --> 00:02:49,620
하지만 오늘 다룰 내용은 이것입니다.

40
00:02:49,620 --> 00:02:53,130
그럼 이미지 분류란 무엇일까요?

41
00:02:53,130 --> 00:03:00,610
주어진 이미지와 미리 정해진 여러 레이블, 예를 들어 이 예시처럼

42
00:03:00,610 --> 00:03:04,110
dog, cat, truck,

43
00:03:04,110 --> 00:03:08,170
plane 같은 가능한 레이블 집합이

44
00:03:08,170 --> 00:03:14,880
있을 때, 시스템의 역할은 이 이미지에 그 중 하나의

45
00:03:14,880 --> 00:03:18,000
레이블을 할당하는 것입니다.

46
00:03:18,000 --> 00:03:23,280
우리에게는 이 작업이 매우, 매우 쉬운 일입니다.

47
00:03:23,280 --> 00:03:26,850
왜냐하면 우리의 뇌, 인지 시스템은

48
00:03:26,850 --> 00:03:32,640
이 이미지를 전체적으로 이해하고 레이블을 부여하도록 설계되어

49
00:03:32,640 --> 00:03:34,630
있기 때문입니다.

50
00:03:34,630 --> 00:03:37,950
하지만 이것을 코딩하고 컴퓨터가

51
00:03:37,950 --> 00:03:41,320
이 이미지를 이해하도록 만드는

52
00:03:41,320 --> 00:03:45,000
것은 완전히 다른 이야기입니다.

53
00:03:45,000 --> 00:03:52,120
우리는 기계가 이런 데이터를 어떻게 이해할 수 있는지 보고자 합니다.

54
00:03:52,120 --> 00:03:59,930
그래서 이미지는 종종 데이터 행렬, 더 넓게는

55
00:03:59,930 --> 00:04:04,430
데이터 텐서로 정의됩니다.

56
00:04:04,430 --> 00:04:06,530
그리고 각 픽셀

57
00:04:06,530 --> 00:04:11,130
값은 보통 0에서 255 사이의 숫자로,

58
00:04:11,130 --> 00:04:15,030
8비트 데이터 구조입니다.

59
00:04:15,030 --> 00:04:22,010
이것이 컬러 이미지라고 가정하면, 해상도가

60
00:04:22,010 --> 00:04:25,910
800x600이고 RGB

61
00:04:25,910 --> 00:04:30,080
이미지이므로 빨강, 초록, 파랑

62
00:04:30,080 --> 00:04:33,000
세 채널이 있어

63
00:04:33,000 --> 00:04:37,850
800x600x3 크기의 텐서가 됩니다.

64
00:04:37,850 --> 00:04:43,100
이 슬라이드에서 보실 수 있죠.

65
00:04:43,100 --> 00:04:49,850
아마 짐작하셨겠지만, 이것이 바로 우리가 이 이미지를

66
00:04:49,850 --> 00:04:54,610
인지하는 방식과 기계가 이 이미지를

67
00:04:54,610 --> 00:04:59,520
인식하는 방식 사이의 의미적 간극입니다.

68
00:04:59,520 --> 00:05:04,600
이것이 얼마나 어려울 수 있는지 이해하기 위해,

69
00:05:04,600 --> 00:05:07,820
이런 종류의 영상 데이터에서

70
00:05:07,820 --> 00:05:11,410
발생하는 몇 가지 도전 과제와

71
00:05:11,410 --> 00:05:14,180
변화를 살펴보겠습니다.

72
00:05:14,180 --> 00:05:16,250
예를 들어,

73
00:05:16,250 --> 00:05:21,070
카메라를 움직인다고 가정해 보겠습니다.

74
00:05:21,070 --> 00:05:23,360
카메라를 움직여서

75
00:05:23,360 --> 00:05:28,660
예를 들어 팬닝을 한다면,

76
00:05:28,660 --> 00:05:33,650
고양이가 완전히 가만히 있어도

77
00:05:33,650 --> 00:05:40,540
800x600x3 모든 픽셀 값이 바뀌게 됩니다.

78
00:05:40,540 --> 00:05:44,650
그래서 모든 픽셀들이 새로운 값을 가지게 되는 거죠.

79
00:05:44,650 --> 00:05:50,450
사람 입장에서는 같은 객체입니다.

80
00:05:50,450 --> 00:05:52,600
전혀 다르지 않습니다.

81
00:05:52,600 --> 00:05:54,870
하지만 컴퓨터 입장에서는 완전히

82
00:05:54,870 --> 00:05:57,510
새로운 데이터 포인트가 되는 겁니다.

83
00:05:57,510 --> 00:06:01,120
이것이 한 가지 도전 과제이고,

84
00:06:01,120 --> 00:06:05,920
이 외에도 여러 가지가 있습니다.

85
00:06:05,920 --> 00:06:12,060
예를 들어, 조명은 또 다른 도전 과제입니다.

86
00:06:12,060 --> 00:06:15,900
그래픽스나 다른 비전 강의,

87
00:06:15,900 --> 00:06:20,220
혹은 공학 응용을 위한 디지털

88
00:06:20,220 --> 00:06:23,910
이미지 처리 강의를 들어보셨다면,

89
00:06:23,910 --> 00:06:31,110
각 RGB 픽셀의 값, 즉 RGB 값은 표면

90
00:06:31,110 --> 00:06:37,590
재질, 색상, 그리고 광원에 따라 달라진다는

91
00:06:37,590 --> 00:06:39,870
것을 아실 겁니다.

92
00:06:39,870 --> 00:06:46,350
그래서 같은 고양이, 같은 객체라도 조명

93
00:06:46,350 --> 00:06:48,840
조건이 다르면

94
00:06:48,840 --> 00:06:55,020
숫자 값이 다르게 나타날 수 있습니다.

95
00:06:55,020 --> 00:07:00,180
이 점을 염두에 두면, 고양이가 어두운 방에 있든 햇빛 아래

96
00:07:00,180 --> 00:07:02,500
있든 여전히 고양이입니다.

97
00:07:02,500 --> 00:07:03,910
한 마리 고양이인 거죠.

98
00:07:03,910 --> 00:07:08,440
하지만 이것이 기계에게는 도전 과제를 만듭니다.

99
00:07:08,440 --> 00:07:11,490
조명과 시점 변화 외에

100
00:07:11,490 --> 00:07:16,890
픽셀 값에 영향을 주고 기계가 객체를

101
00:07:16,890 --> 00:07:22,440
인식하는 데 문제를 일으킬 수 있는 다른 도전

102
00:07:22,440 --> 00:07:24,570
과제들을 말해볼

103
00:07:24,570 --> 00:07:26,400
수 있을까요?

104
00:07:26,400 --> 00:07:30,990
배경 혼잡, 배경 객체들이 있겠죠, 사실 이게

105
00:07:30,990 --> 00:07:33,120
다음 슬라이드 주제입니다.

106
00:07:33,120 --> 00:07:38,100
네, 배경 혼잡도 또 다른 도전 과제입니다.

107
00:07:38,100 --> 00:07:40,590
다른 것은요?

108
00:07:40,590 --> 00:07:44,430
확대와 축소, 네, 즉 이미지 내

109
00:07:44,430 --> 00:07:46,630
객체의 크기 변화입니다.

110
00:07:46,630 --> 00:07:49,070
네, 또 무엇이 있을까요?

111
00:07:49,070 --> 00:07:52,100
이미지 해상도도 고려할

112
00:07:52,100 --> 00:07:54,810
수 있는데, 확실히

113
00:07:54,810 --> 00:07:56,700
도전 과제입니다.

114
00:07:56,700 --> 00:08:01,100
하지만 머신러닝 모델이나 객체 인식을

115
00:08:01,100 --> 00:08:05,580
위한 어떤 모델이든 이미지 크기를 정규화하기

116
00:08:05,580 --> 00:08:09,210
때문에, 해상도는 크게 중요하지

117
00:08:09,210 --> 00:08:11,720
않을 수 있습니다,

118
00:08:11,720 --> 00:08:15,230
객체의 확대 효과가 없는 한요.

119
00:08:15,230 --> 00:08:18,120
가림 현상(occlusion)은 주요 문제 중 하나입니다.

120
00:08:18,120 --> 00:08:21,960
다시 말해, 인간에게는 이것이 고양이라는 것을 쉽게 알 수 있죠.

121
00:08:21,960 --> 00:08:23,430
이것들은 고양이들입니다.

122
00:08:23,430 --> 00:08:25,895
마지막 것조차도 실제로 매우 어려운

123
00:08:25,895 --> 00:08:28,050
문제인데, 오른쪽에 있는 것입니다.

124
00:08:28,050 --> 00:08:32,179
오른쪽에는 꼬리와 아마도 발

125
00:08:32,179 --> 00:08:34,239
일부만 보입니다.

126
00:08:37,280 --> 00:08:41,690
누군가는 저게 호랑이일 수도 있고, 아니면 작은

127
00:08:41,690 --> 00:08:46,070
꼬리를 가진 너구리일 수도 있다고 말할 수 있습니다.

128
00:08:46,070 --> 00:08:48,910
하지만 이건—문맥상, 거실 소파

129
00:08:48,910 --> 00:08:52,460
안이라는 걸 알기 때문에, 대부분 고양이일

130
00:08:52,460 --> 00:08:53,890
가능성이 큽니다.

131
00:08:53,890 --> 00:08:54,740
고양이입니다.

132
00:08:54,740 --> 00:08:59,170
그래서 다시 말하지만, 우리 인간에게는 그렇게 어렵지 않습니다.

133
00:08:59,170 --> 00:09:02,720
그 밖에도 많은 문제들이 있습니다.

134
00:09:02,720 --> 00:09:03,310
변형입니다.

135
00:09:03,310 --> 00:09:05,930
고양이는 매우 변형이 심합니다.

136
00:09:05,930 --> 00:09:16,300
그래서 알고리즘이 이를 감지하고 인식하는 데 어려움을

137
00:09:16,300 --> 00:09:17,890
줍니다.

138
00:09:17,890 --> 00:09:19,750
물론 오늘날의 알고리즘은 아닙니다.

139
00:09:19,750 --> 00:09:24,910
일반적으로 단계별로 객체를 감지하는 알고리즘을 만드는

140
00:09:24,910 --> 00:09:26,480
데 있어서요.

141
00:09:26,480 --> 00:09:31,240
그래서 변형은 또 다른 주요 도전 과제 중 하나입니다.

142
00:09:31,240 --> 00:09:37,000
그리고 그 외에도, 클래스 내 변이(intraclass

143
00:09:37,000 --> 00:09:44,220
variation)가 또 하나 중요한 도전 과제입니다.

144
00:09:44,220 --> 00:09:48,340
고양이는 크기, 색상, 무늬가 다양하고, 심지어

145
00:09:48,340 --> 00:09:51,310
품종도 다르다는 것을 알고 있습니다.

146
00:09:51,310 --> 00:09:55,090
그리고 이 모든 것들이 여전히 고양이입니다.

147
00:09:55,090 --> 00:09:59,340
하지만 기계에게는 같은 클래스 내

148
00:09:59,340 --> 00:10:04,530
변형을 인식하는 것이 그렇게 쉽지 않습니다.

149
00:10:04,530 --> 00:10:09,540
또 다른 흥미로운 도전 과제는 문맥입니다.

150
00:10:09,540 --> 00:10:17,260
만약 오른쪽 이미지의 그 부분만 본다면, 혹은 알고리즘이

151
00:10:17,260 --> 00:10:23,170
문맥을 고려하지 않고 본다면, 이것을 호랑이나

152
00:10:23,170 --> 00:10:28,270
다른 동물로 분류하기 매우 쉽습니다.

153
00:10:28,270 --> 00:10:31,110
하지만 문맥과 그림자 효과

154
00:10:31,110 --> 00:10:34,500
등을 알고 있기 때문에,

155
00:10:34,500 --> 00:10:39,876
아마도 이것을 올바르게 분류할 수 있을 것입니다.

156
00:10:45,717 --> 00:10:49,200
하지만 오늘날 우리가 가진

157
00:10:49,200 --> 00:10:52,020
분류기들은 ImageNet과

158
00:10:52,020 --> 00:10:57,400
같은 노력 덕분에 이미지 분류, 이미지 내 객체

159
00:10:57,400 --> 00:11:00,760
인식에서 정말 훌륭한 성과를

160
00:11:00,760 --> 00:11:04,920
내고 있습니다. 그리고 더 큰 규모의

161
00:11:04,920 --> 00:11:10,950
모델을 훈련하기 위한 대규모 벤치마크를 만든 후속

162
00:11:10,950 --> 00:11:12,640
연구들도 있습니다.

163
00:11:12,640 --> 00:11:22,050
이 수업에서 우리가 하고자 하는 것은 활동을 인식하고, 객체를

164
00:11:22,050 --> 00:11:27,600
인식하며, 이미지 내 다른 측면들도 인식할

165
00:11:27,600 --> 00:11:32,200
수 있는 모델을 만드는 것입니다.

166
00:11:32,200 --> 00:11:34,770
이 수업의 나머지 시간 동안

167
00:11:34,770 --> 00:11:38,460
우리는 단계별로 큰 알고리즘을 만드는 데

168
00:11:38,460 --> 00:11:40,460
필요한 기본 구성 요소들을

169
00:11:40,460 --> 00:11:42,750
구축해 나갈 것입니다.

170
00:11:42,750 --> 00:11:51,200
그 전에, 이미지 분류의 가장 기본적인 구성

171
00:11:51,200 --> 00:11:58,260
요소, 즉 이런 함수를 구현하는 것부터

172
00:11:58,260 --> 00:12:01,110
살펴봐야 합니다.

173
00:12:01,110 --> 00:12:06,020
컴퓨터 과학이나 공학 수업에서 알고리즘을

174
00:12:06,020 --> 00:12:11,000
통해 프레임워크를 만드는 과정을 배웠다면,

175
00:12:11,000 --> 00:12:16,130
예를 들어 정렬 알고리즘 같은 경우, 보통

176
00:12:16,130 --> 00:12:18,020
if-then-else

177
00:12:18,020 --> 00:12:21,200
규칙과 for 루프

178
00:12:21,200 --> 00:12:24,000
등이 포함되어 있습니다.

179
00:12:24,000 --> 00:12:31,770
그래서 정렬 알고리즘을 만드는 명확한 작업 흐름도와

180
00:12:31,770 --> 00:12:35,670
if-then-else 단계가 있습니다.

181
00:12:35,670 --> 00:12:39,160
하지만 이미지와 시각적 세계를 이해하는

182
00:12:39,160 --> 00:12:45,230
데 있어서는 그런 방식이 통하지 않고, 이미지 분류를 위한

183
00:12:45,230 --> 00:12:50,660
단계를 하드코딩하는 방법이 없다는 것이 도전 과제입니다.

184
00:12:50,660 --> 00:12:56,000
물론 이 분야에서도 몇 가지 시도가 있었습니다.

185
00:12:56,000 --> 00:13:04,180
객체를 인식하기 위한 알고리즘과 단계를 고안하려는

186
00:13:04,180 --> 00:13:06,590
논문들이 있었죠.

187
00:13:06,590 --> 00:13:13,510
그 중 하나는 에지 디텍터를 기반으로 했는데, 먼저

188
00:13:13,510 --> 00:13:19,130
이미지에서 가장자리를 찾고, 그런 다음

189
00:13:19,130 --> 00:13:23,930
모든 패턴을 만든 후, 예를 들어

190
00:13:23,930 --> 00:13:29,480
코너 같은 중요한 패턴을 보고, 코너

191
00:13:29,480 --> 00:13:32,630
주변의 특징을 추출하거나

192
00:13:32,630 --> 00:13:36,360
특정 유형의 코너 개수를

193
00:13:36,360 --> 00:13:44,110
세고, 이를 바탕으로 출력 클래스로 매핑하려고 했습니다.

194
00:13:44,110 --> 00:13:48,840
이것은 흥미로운 시도였고 제한된

195
00:13:48,840 --> 00:13:55,590
변형의 이미지에서는 어느 정도 성공했지만, 첫째로

196
00:13:55,590 --> 00:13:59,200
이런 알고리즘을 확장하는

197
00:13:59,200 --> 00:14:01,900
것은 매우 어렵습니다.

198
00:14:01,900 --> 00:14:06,660
작동하더라도 인식하려는 모든 객체마다

199
00:14:06,660 --> 00:14:09,300
규칙을 만들어야

200
00:14:09,300 --> 00:14:11,190
하므로 확장하기

201
00:14:11,190 --> 00:14:12,670
힘듭니다.

202
00:14:12,670 --> 00:14:17,100
둘째로, 각각의 논리를 찾는 데에도

203
00:14:17,100 --> 00:14:20,920
많은 노력이 필요합니다.

204
00:14:20,920 --> 00:14:23,040
이러한 이유로

205
00:14:23,040 --> 00:14:27,210
객체 감지나 이미지 분류를

206
00:14:27,210 --> 00:14:34,350
위한 논리와 절차를 만드는 이런 유형의 알고리즘은

207
00:14:34,350 --> 00:14:38,380
크게 성공하지 못했습니다.

208
00:14:38,380 --> 00:14:42,850
기계 학습은 데이터 기반 접근법을 제시합니다.

209
00:14:42,850 --> 00:14:46,320
이 새로운 패러다임, 즉

210
00:14:46,320 --> 00:14:50,220
데이터 기반 관점에서 이

211
00:14:50,220 --> 00:14:56,070
문제를 바라보는 또 다른 패러다임으로서,

212
00:14:56,070 --> 00:15:00,070
세 단계 절차를 정의합니다.

213
00:15:00,070 --> 00:15:06,420
첫 번째는 이미지와 그 레이블이 포함된 데이터

214
00:15:06,420 --> 00:15:09,400
세트를 수집하는 것입니다.

215
00:15:09,400 --> 00:15:11,850
특정 객체나

216
00:15:11,850 --> 00:15:17,040
객체 유형을 인식하고 싶다면,

217
00:15:17,040 --> 00:15:20,550
인터넷에서 데이터 세트나

218
00:15:20,550 --> 00:15:26,130
단일 데이터 포인트를 찾아서 각

219
00:15:26,130 --> 00:15:32,040
예제에서 많은 샘플을 만들 수 있습니다.

220
00:15:32,040 --> 00:15:35,340
10년, 20년 전에는 인터넷의

221
00:15:35,340 --> 00:15:41,270
검색 엔진과 이미지 검색 엔진을 사용해 이런 데이터

222
00:15:41,270 --> 00:15:44,220
세트를 만들곤 했습니다.

223
00:15:44,220 --> 00:15:47,902
이제는 모든 데이터 세트가 이미 존재합니다.

224
00:15:47,902 --> 00:15:51,350
두 번째 단계는 기계 학습 알고리즘을 사용해

225
00:15:51,350 --> 00:15:53,610
분류기를 훈련하는 것입니다.

226
00:15:53,610 --> 00:15:57,680
기본적으로 train 함수를

227
00:15:57,680 --> 00:16:01,730
만들어 훈련 데이터의 이미지와

228
00:16:01,730 --> 00:16:04,500
레이블을 받아, 이미지와

229
00:16:04,500 --> 00:16:08,240
레이블을 연관 짓는 모델을

230
00:16:08,240 --> 00:16:10,170
만드는 겁니다.

231
00:16:10,170 --> 00:16:15,050
마지막 단계는 새로운 이미지에 대해

232
00:16:15,050 --> 00:16:20,570
분류기를 평가하는 것으로, predict

233
00:16:20,570 --> 00:16:23,630
함수를 구현해 모델과

234
00:16:23,630 --> 00:16:28,830
테스트 이미지를 받아, 훈련에

235
00:16:28,830 --> 00:16:32,470
포함되지 않은 테스트 이미지의

236
00:16:32,470 --> 00:16:39,080
레이블을 예측하고 결과를 반환하는 것입니다.

237
00:16:39,080 --> 00:16:42,110
아주 간단한 절차입니다.

238
00:16:42,110 --> 00:16:46,300
하지만 논리를 만드는 대신 데이터

239
00:16:46,300 --> 00:16:50,110
기반 접근법을 구축하는 겁니다.

240
00:16:50,110 --> 00:16:55,390
앞서 말했듯이, 우리는 두 가지 인기 있는 방법과 분류기에 대해

241
00:16:55,390 --> 00:16:56,570
이야기하려고 합니다.

242
00:16:56,570 --> 00:16:59,080
그 중 하나가 최근접 이웃 분류기입니다.

243
00:16:59,080 --> 00:17:03,220
이것은 가장 쉬운 형태의

244
00:17:03,220 --> 00:17:09,579
분류이며, 분류기 구축에 관한 개념을

245
00:17:09,579 --> 00:17:15,099
배우기 위해 특히 다루고자

246
00:17:15,099 --> 00:17:16,160
합니다.

247
00:17:16,160 --> 00:17:22,190
그리고 몇 가지 세부 사항을 설명하기가 더 쉽습니다.

248
00:17:22,190 --> 00:17:26,089
그다음에 선형 분류(linear classification) 주제로
넘어가겠습니다.

249
00:17:26,089 --> 00:17:30,870
그렇게 하기 위해서, 우리는 최근접 이웃 분류기(nearest
neighbor

250
00:17:30,870 --> 00:17:31,990
classifier)를 만듭니다.

251
00:17:31,990 --> 00:17:37,720
말씀드렸듯이, train 함수와 predict 함수를 만들어야 합니다.

252
00:17:37,720 --> 00:17:40,890
train 함수는 모든 데이터와 레이블을

253
00:17:40,890 --> 00:17:43,270
단순히 기억해야 합니다.

254
00:17:43,270 --> 00:17:44,940
그래서 train 함수는 기본적으로

255
00:17:44,940 --> 00:17:47,070
모든 것을 메모리에 저장하는 것 외에는

256
00:17:47,070 --> 00:17:48,130
아무것도 하지 않습니다.

257
00:17:48,130 --> 00:17:50,320
그리고 예측 함수, 즉

258
00:17:50,320 --> 00:17:56,680
predict 함수는 가장 유사한 훈련 이미지를 찾습니다.

259
00:17:56,680 --> 00:18:00,300
기본적으로 모든 이미지와 그 레이블의 조회 테이블(lookup

260
00:18:00,300 --> 00:18:02,140
table)을 만듭니다.

261
00:18:02,140 --> 00:18:08,070
그리고 예측 또는 테스트 시에는 가장 가까운,

262
00:18:08,070 --> 00:18:12,910
가장 유사한 이미지를 찾아서

263
00:18:12,910 --> 00:18:17,620
그 이미지의 레이블을 출력합니다.

264
00:18:17,620 --> 00:18:19,060
예제를 살펴보겠습니다.

265
00:18:19,060 --> 00:18:26,430
훈련 데이터로 이 다섯 개가 있다고 가정하면, 네,

266
00:18:26,430 --> 00:18:27,420
제

267
00:18:27,420 --> 00:18:29,830
커서가 보이시죠.

268
00:18:29,830 --> 00:18:33,280
그리고 이것이 쿼리 이미지, 즉 예측을

269
00:18:33,280 --> 00:18:35,830
위한 입력 이미지입니다.

270
00:18:35,830 --> 00:18:39,540
우리가 하고 싶은 것은 이 훈련 데이터 중에서 이

271
00:18:39,540 --> 00:18:43,480
이미지와 가장 유사한 것이 무엇인지 보는 겁니다.

272
00:18:43,480 --> 00:18:46,210
그걸 위해서는 거리 함수(distance function)가 필요합니다.

273
00:18:46,210 --> 00:18:49,560
이 거리 함수는 두 이미지,

274
00:18:49,560 --> 00:18:53,280
즉 쿼리 이미지와 비교하는

275
00:18:53,280 --> 00:18:58,950
각각의 이미지 쌍을 받아서, 이 두 입력,

276
00:18:58,950 --> 00:19:05,370
두 이미지 간의 유사도를 정의하는 값을 반환해야

277
00:19:05,370 --> 00:19:06,600
합니다.

278
00:19:06,600 --> 00:19:09,790
그 방법에는 여러 가지가 있습니다.

279
00:19:09,790 --> 00:19:14,560
가장 인기 있는 거리 측정 방법 중 하나는 L1

280
00:19:14,560 --> 00:19:19,680
거리입니다. 이는 두 이미지, 즉 이미지 I1과

281
00:19:19,680 --> 00:19:25,980
I2 사이의 픽셀 차이 절대값을 모두 더한 값으로 정의됩니다.

282
00:19:25,980 --> 00:19:28,940
예를 들어, 이 이미지가

283
00:19:28,940 --> 00:19:34,460
테스트 이미지라면, 이 이미지와 학습

284
00:19:34,460 --> 00:19:38,450
데이터에 있는 이미지 간의

285
00:19:38,450 --> 00:19:43,580
거리를 계산하려면 픽셀별로 값을 빼고 그

286
00:19:43,580 --> 00:19:46,100
차이의 절대값을

287
00:19:46,100 --> 00:19:54,470
모두 더합니다. 이렇게 해서 이 값이 두 이미지 간의 거리로

288
00:19:54,470 --> 00:19:55,860
정의됩니다.

289
00:19:55,860 --> 00:20:01,710
이것이 가장 기본적인 거리 함수이지만, 실제로

290
00:20:01,710 --> 00:20:06,030
많은 응용 분야에서 매우 유용합니다.

291
00:20:06,030 --> 00:20:10,310
수업에서 이 L1 거리와 다른 거리 변형들을

292
00:20:10,310 --> 00:20:13,280
자주 다시 다룰 예정입니다.

293
00:20:13,280 --> 00:20:15,080
이 아주 간단한 정의를 가지고, 어떻게

294
00:20:15,080 --> 00:20:16,530
구현할 수 있는지 살펴보겠습니다.

295
00:20:16,530 --> 00:20:17,940
이것을 구현하는 거죠.

296
00:20:17,940 --> 00:20:22,860
말씀드렸듯이, 첫 번째 단계는 학습 데이터를 그냥 기억하는 것입니다.

297
00:20:22,860 --> 00:20:28,390
그래서 train 함수는 데이터를 메모리에 저장합니다.

298
00:20:28,390 --> 00:20:32,920
그리고 predict 함수는 실제로

299
00:20:32,920 --> 00:20:39,380
파이썬 라이브러리와 NumPy 등을 사용해서 단 4줄로

300
00:20:39,380 --> 00:20:42,080
구현할 수 있습니다.

301
00:20:42,080 --> 00:20:46,720
테스트 샘플 각각과 학습 데이터

302
00:20:46,720 --> 00:20:54,760
간의 거리를 계산하고, 각 테스트 샘플마다 최소

303
00:20:54,760 --> 00:20:59,530
거리를 찾은 후, 그 최소 거리

304
00:20:59,530 --> 00:21:05,000
인덱스에 해당하는 레이블을 출력합니다.

305
00:21:05,000 --> 00:21:10,090
이것이 predict 함수의

306
00:21:10,090 --> 00:21:12,370
구현 방법입니다.

307
00:21:12,370 --> 00:21:21,160
네, 픽셀 값은 제가 설명한 것처럼 가장 단순한 형태로

308
00:21:21,160 --> 00:21:28,120
800x600x3 크기의 텐서이며, 3개 채널은 각 픽셀

309
00:21:28,120 --> 00:21:32,200
위치의 RGB 값을 나타냅니다.

310
00:21:32,200 --> 00:21:35,850
네, 온라인 학생들을 위해 질문을

311
00:21:35,850 --> 00:21:37,630
다시 반복해야겠네요.

312
00:21:37,630 --> 00:21:41,520
질문은 픽셀 값이 무엇을 나타내느냐는 것이었습니다.

313
00:21:41,520 --> 00:21:46,290
네, 다음 질문은 왜 픽셀 값이 0과 255 사이인지에 관한 것입니다.

314
00:21:46,290 --> 00:21:52,320
이미지를 저장하는 데는 여러 가지 표준이 존재합니다.

315
00:21:52,320 --> 00:21:57,690
우리가 온라인에서 보거나 여기서 보는 거의 모든 이미지에

316
00:21:57,690 --> 00:22:01,920
사용하는 가장 인기 있는 형식은 RGB입니다.

317
00:22:01,920 --> 00:22:05,580
RGB는 24비트 형식이고, 때로는 알파 채널이

318
00:22:05,580 --> 00:22:08,070
추가되어 32비트가 되기도 합니다.

319
00:22:08,070 --> 00:22:10,540
그 부분은 깊게 들어가진 않겠습니다.

320
00:22:10,540 --> 00:22:16,000
하지만 24비트 형식은 빨강, 초록,

321
00:22:16,000 --> 00:22:19,060
파랑 세 채널

322
00:22:19,060 --> 00:22:22,590
각각에 8비트를 할당한다는

323
00:22:22,590 --> 00:22:24,340
뜻입니다.

324
00:22:24,340 --> 00:22:27,940
이것이 정의된 표준입니다.

325
00:22:27,940 --> 00:22:30,220
다른 프레임워크도

326
00:22:30,220 --> 00:22:33,900
있지만, 이것이 가장 널리 쓰입니다.

327
00:22:33,900 --> 00:22:40,820
그럼, 코드로 돌아가서 질문을 드리겠습니다.

328
00:22:46,080 --> 00:22:53,010
대부분의 학생들이 공학 배경과 약간의 컴퓨터 과학 지식을 가지고

329
00:22:53,010 --> 00:22:55,680
있다고 알고 있는데,

330
00:22:55,680 --> 00:23:01,200
훈련 데이터에 n개의 샘플, 즉 n개의 예제가

331
00:23:01,200 --> 00:23:04,380
있을 때 훈련과 예측이 얼마나

332
00:23:04,380 --> 00:23:08,333
빠르게 이루어지는지 보고 싶습니다.

333
00:23:11,550 --> 00:23:15,420
여러분이 계산 복잡도나 때로는

334
00:23:15,420 --> 00:23:21,170
공간 복잡도를 나타내는 빅오 표기법에

335
00:23:21,170 --> 00:23:23,310
익숙하길 바랍니다.

336
00:23:23,310 --> 00:23:27,260
여기서 알고리즘을 보면, 훈련 데이터를 기준으로

337
00:23:27,260 --> 00:23:28,850
설명하겠습니다.

338
00:23:28,850 --> 00:23:31,790
훈련 함수에서요-- 그리고

339
00:23:31,790 --> 00:23:36,890
예측에 대한 답변은 여러분이 도와주시길 바랍니다.

340
00:23:36,890 --> 00:23:40,730
훈련 단계는 O(1)입니다, 왜냐하면

341
00:23:40,730 --> 00:23:44,550
실제로 아무 작업도 하지 않기 때문입니다.

342
00:23:44,550 --> 00:23:46,770
데이터도 이동하지 않고,

343
00:23:46,770 --> 00:23:50,010
그냥 메모리에 데이터를 복사해서 보관만 합니다.

344
00:23:50,010 --> 00:23:53,340
즉, 연산이 없으므로, 연산

345
00:23:53,340 --> 00:23:56,600
복잡도가 1인 상태에서

346
00:23:56,600 --> 00:24:00,780
훈련 단계를 완료할 수 있습니다.

347
00:24:00,780 --> 00:24:03,650
그렇다면 테스트

348
00:24:03,650 --> 00:24:12,570
데이터의 각 단일 예제에 대한 예측 단계는 어떻게 될까요?

349
00:24:12,570 --> 00:24:15,590
몇 번의 연산을 해야 할까요?

350
00:24:15,590 --> 00:24:17,370
N입니다, 맞습니다.

351
00:24:17,370 --> 00:24:19,990
만약 n개의 학습 데이터가

352
00:24:19,990 --> 00:24:23,590
있다면, 모든 테스트 이미지와

353
00:24:23,590 --> 00:24:26,960
학습 데이터의 모든 이미지 간 거리를

354
00:24:26,960 --> 00:24:33,490
계산해야 하므로 최소한 n번의 연산이 필요하다는 뜻입니다.

355
00:24:33,490 --> 00:24:43,480
그래서 이 방법은 별로 좋지 않습니다. 왜냐하면 학습 단계에서는 아무

356
00:24:43,480 --> 00:24:45,890
것도 하지 않지만,

357
00:24:45,890 --> 00:24:49,190
테스트, 즉 예측할 때는

358
00:24:49,190 --> 00:24:51,700
데이터와 단일 데이터

359
00:24:51,700 --> 00:24:54,220
포인트, 학습 예제

360
00:24:54,220 --> 00:25:00,010
간 비교에 너무 많은 시간을 쓰기 때문입니다.

361
00:25:00,010 --> 00:25:03,880
이것은 마치 ChatGPT에게

362
00:25:03,880 --> 00:25:07,840
질문할 때마다 인터넷상의 모든 가능한

363
00:25:07,840 --> 00:25:11,650
답변과 비교해서 답을 찾는 것과

364
00:25:11,650 --> 00:25:15,510
비슷한데, 그러면 몇 년이 걸리고

365
00:25:15,510 --> 00:25:19,270
나서야 응답을 받을 수 있겠죠.

366
00:25:19,270 --> 00:25:22,710
그래서 아주 간단한 문제에도

367
00:25:22,710 --> 00:25:24,520
확장하기 어렵습니다.

368
00:25:24,520 --> 00:25:26,850
예전에는 이런 방식을 사용했었습니다.

369
00:25:26,850 --> 00:25:34,920
그래서 우리가 원하는 것은 예측 시에 빠른 분류기를

370
00:25:34,920 --> 00:25:37,090
만드는 것입니다.

371
00:25:37,090 --> 00:25:40,860
예측은 훨씬 빠르게 하지만, 학습 시에는 시간이 오래 걸려도

372
00:25:40,860 --> 00:25:42,960
괜찮습니다. 왜냐하면 학습은

373
00:25:42,960 --> 00:25:44,890
오프라인에서 할 수 있기 때문입니다.

374
00:25:44,890 --> 00:25:49,350
이 점을 염두에 두고, 최근에는 GPU

375
00:25:49,350 --> 00:25:55,530
등을 이용해 최근접 이웃 알고리즘을 훨씬 빠르게 만드는 많은

376
00:25:55,530 --> 00:25:59,320
노력이 있었지만, 이 수업 범위를

377
00:25:59,320 --> 00:26:01,545
벗어나므로 관심 있으면

378
00:26:01,545 --> 00:26:03,670
찾아보시면 됩니다.

379
00:26:03,670 --> 00:26:08,460
그럼 이제 이 알고리즘이 어떻게 작동하는지

380
00:26:08,460 --> 00:26:11,580
시각화와 함께 살펴보겠습니다.

381
00:26:11,580 --> 00:26:15,630
이 공간에서 빨강, 파랑,

382
00:26:15,630 --> 00:26:19,050
초록, 보라, 노랑 다섯

383
00:26:19,050 --> 00:26:26,880
클래스가 있고, 각 점은 해당 클래스의 학습 샘플 하나를

384
00:26:26,880 --> 00:26:32,880
나타냅니다. 이 공간을 각 점마다 나누면,

385
00:26:32,880 --> 00:26:37,440
다섯 개 또는 이 경우 여섯 개의

386
00:26:37,440 --> 00:26:41,580
영역으로 분할할 수 있습니다.

387
00:26:41,580 --> 00:26:48,210
만약 테스트 샘플이 특정 영역에 있다면, 그 영역의

388
00:26:48,210 --> 00:26:51,780
색깔이 그 샘플의 최근접

389
00:26:51,780 --> 00:26:57,010
이웃이 어떤 클래스인지 보여줍니다.

390
00:26:57,010 --> 00:27:02,670
이것이 바로 최근접 이웃 알고리즘, 1-최근접 이웃

391
00:27:02,670 --> 00:27:04,530
알고리즘이 이 설정에서

392
00:27:04,530 --> 00:27:07,330
공간을 분할하는 방식입니다.

393
00:27:07,330 --> 00:27:12,500
그런데 이 예제에서 문제점이 보이시나요?

394
00:27:12,500 --> 00:27:14,750
노란 점은 정확히 모든

395
00:27:14,750 --> 00:27:17,220
초록 점들의 가운데에 있습니다.

396
00:27:17,220 --> 00:27:20,370
이것은 아마도 이상치라는 뜻입니다.

397
00:27:20,370 --> 00:27:21,750
아마도 노이즈일 겁니다.

398
00:27:21,750 --> 00:27:24,200
이것은 우리가 해결해야 할 많은 문제들에서

399
00:27:24,200 --> 00:27:25,500
흔히 있는 경우입니다.

400
00:27:25,500 --> 00:27:32,300
그리고 그 이유로 가운데에 큰 노란 영역이

401
00:27:32,300 --> 00:27:35,090
있는 것은 단지 이

402
00:27:35,090 --> 00:27:38,010
한 점 때문입니다.

403
00:27:38,010 --> 00:27:41,880
그리고 우리가 단 하나의 최근접 이웃만 사용하기 때문에 이런 일이

404
00:27:41,880 --> 00:27:42,600
발생합니다.

405
00:27:42,600 --> 00:27:44,760
그래서 좀 더 견고하게 만들기

406
00:27:44,760 --> 00:27:48,450
위해, 우리는 취하는 최근접 이웃의 수를 늘릴 수 있는데,

407
00:27:48,450 --> 00:27:50,750
이것이 최근접 이웃 알고리즘을 k-최근접

408
00:27:50,750 --> 00:27:52,650
이웃으로 바꾸는 겁니다.

409
00:27:52,650 --> 00:28:00,810
우리는 종종 한 점 이상, 여러 샘플을 선택합니다.

410
00:28:00,810 --> 00:28:04,370
그리고 테스트 샘플, 테스트

411
00:28:04,370 --> 00:28:10,450
이미지의 레이블을 식별하기 위해 다수결

412
00:28:10,450 --> 00:28:13,750
투표를 자주 사용합니다.

413
00:28:13,750 --> 00:28:16,840
하지만 여기서 볼 수 있는 문제는

414
00:28:16,840 --> 00:28:19,970
이제 흰색 영역들이 있다는 겁니다.

415
00:28:19,970 --> 00:28:23,480
그 흰색 영역들은 완전한 결정을

416
00:28:23,480 --> 00:28:28,150
내릴 수 없는 영역인데, 그 이유는 그

417
00:28:28,150 --> 00:28:34,240
영역들이 세 가지 다른 클래스의 이웃 샘플 수가

418
00:28:34,240 --> 00:28:36,350
동일하기 때문입니다.

419
00:28:36,350 --> 00:28:39,130
그래서 그 흰색 영역

420
00:28:39,130 --> 00:28:43,970
안의 예제 레이블을 식별할 방법이 없습니다.

421
00:28:43,970 --> 00:28:47,080
여러분이 이런 유형의 공간을 여러분

422
00:28:47,080 --> 00:28:49,750
문제에 대해 만든다면, 이

423
00:28:49,750 --> 00:28:52,730
공간들을 보면 그 영역들이 더 많은

424
00:28:52,730 --> 00:28:55,540
데이터를 수집하기 좋은 영역이라는

425
00:28:55,540 --> 00:28:56,600
뜻입니다.

426
00:28:56,600 --> 00:28:59,300
그래서 그것들은 불명확한 공간입니다.

427
00:28:59,300 --> 00:29:01,840
따라서 더 많은 데이터

428
00:29:01,840 --> 00:29:05,380
수집을 위해 중요한 영역을 찾는 좋은

429
00:29:05,380 --> 00:29:06,650
방법입니다.

430
00:29:06,650 --> 00:29:10,870
그래서 k 값을 더 크게 할 수 있습니다.

431
00:29:10,870 --> 00:29:14,690
하지만 우리가 선택할 수 있는 것 중 하나는, 하나는

432
00:29:17,940 --> 00:29:22,270
중요한 역할을 하는 요소는 k 값입니다.

433
00:29:22,270 --> 00:29:25,290
하지만 기억하신다면, 우리는 또 다른 결정을

434
00:29:25,290 --> 00:29:27,960
내려야 했는데, 그것이 거리 함수였습니다.

435
00:29:27,960 --> 00:29:31,290
우리는 L1 거리에 대해

436
00:29:31,290 --> 00:29:37,320
이야기했는데, 이는 픽셀 간 쌍별 차이의

437
00:29:37,320 --> 00:29:41,130
절대값을 모두 더한 것입니다.

438
00:29:41,130 --> 00:29:47,640
그리고 L1 거리, 때때로 맨해튼 거리라고도

439
00:29:47,640 --> 00:29:52,440
부르는 이 거리를 시각화하면, 거리

440
00:29:52,440 --> 00:29:57,090
함수는 이런 식으로 표현됩니다.

441
00:29:59,620 --> 00:30:05,860
이 공간에 있는 이 정사각형을 보면, 그 정사각형

442
00:30:05,860 --> 00:30:10,815
위의 모든 점들은 원점, 즉 중심점으로부터

443
00:30:10,815 --> 00:30:13,620
같은 거리를

444
00:30:13,620 --> 00:30:15,400
가지고 있습니다.

445
00:30:15,400 --> 00:30:19,560
이것이 L1 거리 함수가 어떻게 작동하는지

446
00:30:19,560 --> 00:30:22,930
시각화하고 이해하는 좋은 방법입니다.

447
00:30:22,930 --> 00:30:26,280
또 다른 인기 있는

448
00:30:26,280 --> 00:30:29,040
거리 함수는 L2인데,

449
00:30:29,040 --> 00:30:34,050
절대값 대신 차이의 제곱을

450
00:30:34,050 --> 00:30:36,910
계산하고 모두 더합니다.

451
00:30:36,910 --> 00:30:39,900
하지만 제곱을 했기 때문에, 우리는 다시 제곱근을 계산합니다.

452
00:30:39,900 --> 00:30:49,320
그리고 이를 시각화하면, 원 위의 각 점들이 중심, 즉 원점으로부터

453
00:30:49,320 --> 00:30:52,770
같은 거리를 갖는

454
00:30:52,770 --> 00:30:56,590
원 시각화를 얻게 됩니다.

455
00:30:56,590 --> 00:30:58,530
이 시각화는 이러한 거리들

456
00:30:58,530 --> 00:31:01,750
간의 차이도 이해하는 데 실제로 도움이 됩니다.

457
00:31:01,750 --> 00:31:04,590
이것들이 우리가 사용할 수 있는 가장 기본적이고 가장 쉬운

458
00:31:04,590 --> 00:31:05,370
거리 함수들입니다.

459
00:31:05,370 --> 00:31:08,250
물론 훨씬 더 많은 거리 함수들이 있습니다.

460
00:31:08,250 --> 00:31:12,710
이 시각화가 유용한 이유는, 때때로 x와 y를

461
00:31:12,710 --> 00:31:16,430
회전시키면—이 두 시각화에서 x와

462
00:31:16,430 --> 00:31:18,410
y는 기본적으로 특징들이기

463
00:31:18,410 --> 00:31:20,000
때문입니다.

464
00:31:20,000 --> 00:31:23,280
두 개의 픽셀 값, 두 개의 특징이 있다면, 우리는

465
00:31:23,280 --> 00:31:25,140
이 2D 공간을 갖게 됩니다.

466
00:31:25,140 --> 00:31:29,450
그리고 이 x와 y는 종종 그런 특징들입니다.

467
00:31:29,450 --> 00:31:33,830
그래서 만약 제가 이 특징들을 회전시킨다면, 즉 다른

468
00:31:33,830 --> 00:31:36,210
종류의 특징을 사용한다면, L1

469
00:31:36,210 --> 00:31:40,140
거리는 다른 형태, 다른 값을 갖게 되지만, L2

470
00:31:40,140 --> 00:31:42,330
거리는 달라지지 않습니다.

471
00:31:42,330 --> 00:31:46,950
그래서 이것이 L1과 L2 사이의 큰 차이점입니다.

472
00:31:46,950 --> 00:31:50,930
그리고 때로는 우리의 특징들이 매우

473
00:31:50,930 --> 00:31:53,040
구체적이고 의미가 있으며

474
00:31:53,040 --> 00:31:56,510
그 정보를 보존하고 싶을 때,

475
00:31:56,510 --> 00:32:03,020
L1이 더 중요하고 더 좋습니다. 왜냐하면 보시다시피, 특징에 기반한

476
00:32:03,020 --> 00:32:07,690
거리를 보존하고 강제하는 형태를 가지고

477
00:32:07,690 --> 00:32:09,140
있기 때문입니다.

478
00:32:09,140 --> 00:32:12,880
하지만 그 특징들이 더 임의적이라면, L2

479
00:32:12,880 --> 00:32:15,250
거리가 더 합리적입니다.

480
00:32:15,250 --> 00:32:19,250
거리를 계산하고 싶다면—이

481
00:32:19,250 --> 00:32:22,630
도형 위의 모든 점들이

482
00:32:22,630 --> 00:32:29,350
원점에서의 거리가 L1 거리를 사용하면 정확히

483
00:32:29,350 --> 00:32:34,300
같습니다. 하지만 L2 거리의 경우,

484
00:32:34,300 --> 00:32:38,020
이 원 위의 점들은 중심

485
00:32:38,020 --> 00:32:43,370
또는 원점에서 같은 거리를 가집니다.

486
00:32:43,370 --> 00:32:47,003
그래서 기본적으로 이 두 이미지가 보여주는

487
00:32:51,200 --> 00:32:52,720
주된 내용입니다.

488
00:32:52,720 --> 00:32:57,760
이 도형 위의 어떤 점이든 L1 거리를 사용할 때 원점에서

489
00:32:57,760 --> 00:33:00,080
같은 거리를 가집니다.

490
00:33:00,080 --> 00:33:03,180
그리고 원 위의 어떤 점이든

491
00:33:03,180 --> 00:33:05,340
L2 거리를 사용하면

492
00:33:05,340 --> 00:33:08,490
원점에서 같은 거리를 갖습니다.

493
00:33:08,490 --> 00:33:11,280
네, 특징을 보존하고 싶을 때

494
00:33:11,280 --> 00:33:15,630
L1을 사용하는 것이 왜 중요한지, 더 좋은지입니다.

495
00:33:15,630 --> 00:33:21,210
그 질문에 답하자면, 특징 축을 회전시키면

496
00:33:21,210 --> 00:33:24,780
거리와 이 거리 함수가

497
00:33:24,780 --> 00:33:27,540
완전히 바뀝니다.

498
00:33:27,540 --> 00:33:33,040
반면에 여기서 같은 작업을 하면 아무것도 변하지 않습니다.

499
00:33:33,040 --> 00:33:37,404
특징 거리 값이 정확히

500
00:33:37,404 --> 00:33:38,200
같습니다.

501
00:33:38,200 --> 00:33:38,700
죄송합니다.

502
00:33:38,700 --> 00:33:44,490
이 경우 L1은 특징 값에 매우 민감한 반면, L2는

503
00:33:44,490 --> 00:33:46,710
그렇지 않습니다.

504
00:33:46,710 --> 00:33:49,440
같은 공간에서 다른 특징을

505
00:33:49,440 --> 00:33:52,920
선택하면 다른 형태가 만들어지고,

506
00:33:52,920 --> 00:33:57,210
그러면 거리 함수도 바뀝니다.

507
00:33:57,210 --> 00:34:01,270
그래서 여기 선을 다시 그리면, 온라인 학생들에게

508
00:34:01,270 --> 00:34:05,200
질문입니다. 왜 회전시키면 변하는지요?

509
00:34:05,200 --> 00:34:09,914
만약 이쪽에서 다른 특징을 선택하면,

510
00:34:09,914 --> 00:34:13,199
선들이 다르게 보일 것입니다.

511
00:34:13,199 --> 00:34:20,559
그래서 이걸 회전시켜도, 그 모양에 대해서는 상관없습니다.

512
00:34:20,559 --> 00:34:21,059
그래서 이걸 회전시켜도, 그 모양에 대해서는 상관없습니다.

513
00:34:23,610 --> 00:34:27,370
우리가 이야기한 두 거리 함수로

514
00:34:27,370 --> 00:34:33,840
공간을 시각화하면, k가 1일 때, 즉 가장 가까운 이웃

515
00:34:33,840 --> 00:34:38,920
한 개를 기준으로 L1과 L2에서 이런

516
00:34:38,920 --> 00:34:41,949
공간 분할을 볼 수 있습니다.

517
00:34:41,949 --> 00:34:44,340
여기서 흥미로운

518
00:34:44,340 --> 00:34:51,870
점은 L1 함수에서는 경계 대부분이 두 축, 즉 두

519
00:34:51,870 --> 00:34:57,180
특징 x1과 x2에 평행하다는 겁니다.

520
00:34:57,180 --> 00:35:01,345
특징에 매우 민감하죠.

521
00:35:01,345 --> 00:35:02,720
반면에 L2에서는

522
00:35:02,720 --> 00:35:07,820
좀 더 부드러운 경계 분리가 있습니다.

523
00:35:07,820 --> 00:35:11,660
랩 웹사이트에 온라인 도구가 있어서,

524
00:35:11,660 --> 00:35:16,640
다양한 거리 함수와 k 값으로 직접

525
00:35:16,640 --> 00:35:18,570
실험해볼 수 있습니다.

526
00:35:18,570 --> 00:35:21,630
다양한 설정을 만들어 볼 수 있죠.

527
00:35:21,630 --> 00:35:23,760
직접 가지고 놀아보시면 됩니다.

528
00:35:23,760 --> 00:35:28,050
그런데 왜 처음에 최근접 이웃에 대해 이야기했을까요?

529
00:35:28,050 --> 00:35:33,560
첫째, 네, 가장 풀기 쉽고, 가장 간단한

530
00:35:33,560 --> 00:35:39,230
데이터 기반 접근법이기 때문에 시작하기

531
00:35:39,230 --> 00:35:40,080
좋습니다.

532
00:35:40,080 --> 00:35:45,950
하지만 최근접 이웃을 반복해서 논의하는

533
00:35:45,950 --> 00:35:49,250
주된 이유 중 하나는

534
00:35:49,250 --> 00:35:55,080
하이퍼파라미터 주제를 다룰 수 있기 때문입니다.

535
00:35:55,080 --> 00:36:01,430
하이퍼파라미터는 알고리즘을 실행하기 위해

536
00:36:01,430 --> 00:36:03,380
결정해야

537
00:36:03,380 --> 00:36:05,820
하는 변수들입니다.

538
00:36:05,820 --> 00:36:12,320
이 경우, k 값, 즉 최근접 이웃의 수가

539
00:36:12,320 --> 00:36:14,850
하이퍼파라미터입니다.

540
00:36:14,850 --> 00:36:19,770
최근접 이웃의 수에 따라 결과가

541
00:36:19,770 --> 00:36:21,960
달라지죠.

542
00:36:21,960 --> 00:36:24,500
그리고 또 하나 선택할 수

543
00:36:24,500 --> 00:36:27,050
있는 것이 거리 함수입니다.

544
00:36:27,050 --> 00:36:33,320
하이퍼파라미터 선택은 종종 데이터셋과

545
00:36:33,320 --> 00:36:39,830
문제에 따라 크게 달라집니다.

546
00:36:39,830 --> 00:36:43,700
각 문제에 맞게 최적화할

547
00:36:43,700 --> 00:36:50,270
수 있도록 이를 식별하는 방법이 필요합니다.

548
00:36:50,270 --> 00:36:52,370
이것이 머신러닝 알고리즘,

549
00:36:52,370 --> 00:36:56,080
딥러닝 알고리즘 등에서 흔히 하이퍼파라미터

550
00:36:56,080 --> 00:36:58,200
튜닝이라고 불리는 것입니다.

551
00:36:58,200 --> 00:36:59,560
그럼 어떻게 하죠?

552
00:36:59,560 --> 00:37:03,220
하이퍼파라미터를 어떻게 설정하나요?

553
00:37:03,220 --> 00:37:04,720
여러 가지 방법이 있습니다.

554
00:37:04,720 --> 00:37:08,520
그중 하나는 훈련 데이터에 가장 잘 맞는

555
00:37:08,520 --> 00:37:10,980
하이퍼파라미터를 선택하는 것입니다.

556
00:37:10,980 --> 00:37:14,140
즉, 이미지나 데이터 세트가 있다고 합시다.

557
00:37:14,140 --> 00:37:19,290
훈련 데이터에서 가장 좋은 훈련 결과,

558
00:37:19,290 --> 00:37:23,940
즉 최소 훈련 손실을 내는

559
00:37:23,940 --> 00:37:27,700
하이퍼파라미터 세트를 찾는 거죠.

560
00:37:27,700 --> 00:37:30,700
훈련 데이터에는 효과적일 수 있지만,

561
00:37:30,700 --> 00:37:34,710
특히 최근접 이웃 알고리즘에서는 전혀 좋은 방법이

562
00:37:34,710 --> 00:37:39,660
아닙니다. 왜냐하면 k가 1일 때 항상 최적이기 때문입니다.

563
00:37:39,660 --> 00:37:43,620
훈련 데이터를 외우는 것이기 때문에 k가

564
00:37:43,620 --> 00:37:47,280
1이면 항상 100% 정확도를 내게 됩니다.

565
00:37:47,280 --> 00:37:51,580
그래서 이것이 좋은 방법이 아니라는 것을 알 수 있습니다.

566
00:37:51,580 --> 00:37:56,520
두 번째 방법은 보류된 테스트 세트에서 가장 잘

567
00:37:56,520 --> 00:38:00,570
작동하는 하이퍼파라미터를 선택하는 것입니다.

568
00:38:00,570 --> 00:38:05,250
첫 번째 방법보다는 낫지만, 여기에도

569
00:38:05,250 --> 00:38:07,630
큰 문제가 있습니다.

570
00:38:07,630 --> 00:38:11,610
왜 이것이 문제인지 아시는 분 있나요?

571
00:38:11,610 --> 00:38:15,390
맞습니다, 이건 일종의 부정행위입니다. 테스트 데이터에서

572
00:38:15,390 --> 00:38:18,910
가장 잘 작동하는 하이퍼파라미터를 찾으려

573
00:38:18,910 --> 00:38:24,030
하기 때문이죠. 그리고 테스트 세트에 없는 다른 데이터에 대해 모델이

574
00:38:24,030 --> 00:38:26,200
어떻게 작동할지 알 수 없습니다.

575
00:38:26,200 --> 00:38:30,630
정확히 그렇습니다.

576
00:38:30,630 --> 00:38:33,450
모델이 어떻게 일반화될지 모르기 때문에

577
00:38:33,450 --> 00:38:35,260
좋은 방법이 아닙니다.

578
00:38:35,260 --> 00:38:39,070
절대 이렇게 하지 마세요.

579
00:38:39,070 --> 00:38:41,620
앞서 말했듯이, 이건 일종의 부정행위입니다.

580
00:38:41,620 --> 00:38:47,040
더 나은 방법은 항상 훈련 데이터의 일부를

581
00:38:47,040 --> 00:38:51,020
검증 세트로 분리하는 것입니다.

582
00:38:51,020 --> 00:38:54,530
그리고 훈련 데이터의 나머지 부분,

583
00:38:54,530 --> 00:38:59,060
즉 train 부분에서 모델을 학습시키고,

584
00:38:59,060 --> 00:39:04,160
그 다음 검증 세트에서 하이퍼파라미터를 찾거나

585
00:39:04,160 --> 00:39:06,230
최적화하는 것입니다.

586
00:39:06,230 --> 00:39:10,710
최적의 하이퍼파라미터를 찾은 후에는 그

587
00:39:10,710 --> 00:39:13,820
하이퍼파라미터를 사용해 테스트 세트에 대한

588
00:39:13,820 --> 00:39:16,370
결과를 재현하고 예측을

589
00:39:16,370 --> 00:39:17,280
수행합니다.

590
00:39:17,280 --> 00:39:20,460
이 방법이 훨씬 낫지만,

591
00:39:20,460 --> 00:39:25,763
자체적으로도 몇 가지 어려움이 있습니다.

592
00:39:28,760 --> 00:39:32,340
가끔 선택한 검증 세트가 전체 데이터

593
00:39:32,340 --> 00:39:34,220
분포를 잘 대표하지

594
00:39:34,220 --> 00:39:37,850
못할 수 있기 때문입니다. 검증 세트는

595
00:39:37,850 --> 00:39:40,770
거의 항상 훨씬 작으니까요.

596
00:39:40,770 --> 00:39:44,780
그래서 더 나은 방법 중 하나는

597
00:39:44,780 --> 00:39:52,160
하이퍼파라미터 설정에 교차 검증을 사용하는 것입니다.

598
00:39:52,160 --> 00:39:54,940
기본적으로 훈련 데이터를 여러

599
00:39:54,940 --> 00:39:59,350
개의 폴드, 즉 여러 개의 파티션으로 나누는 거죠.

600
00:39:59,350 --> 00:40:01,120
여기서는 다섯 개입니다.

601
00:40:01,120 --> 00:40:08,140
각 폴드는 한 번씩 검증 세트 역할을 합니다.

602
00:40:08,140 --> 00:40:10,660
그리고 다섯 번 반복해서 다섯 폴드

603
00:40:10,660 --> 00:40:12,530
교차 검증을 수행합니다.

604
00:40:12,530 --> 00:40:17,810
이 과정을 다섯 번 수행하고 정확도를 평균 내는 거죠.

605
00:40:17,810 --> 00:40:20,510
하이퍼파라미터 값을 설정하고,

606
00:40:20,510 --> 00:40:25,460
이 다섯 세트 모두에 대해 실행한 후 검증

607
00:40:25,460 --> 00:40:29,690
세트에서 정확도를 계산하고 평균을 냅니다.

608
00:40:29,690 --> 00:40:31,600
그리고 나서 하이퍼파라미터의 최적

609
00:40:31,600 --> 00:40:34,250
설정을 찾기 위해 이 과정을 여러 번 반복합니다.

610
00:40:34,250 --> 00:40:36,620
하이퍼파라미터 설정을 찾은 후에는

611
00:40:36,620 --> 00:40:39,160
이를 테스트 세트에 적용합니다.

612
00:40:39,160 --> 00:40:41,500
이 방법이 좀 더 신뢰할

613
00:40:41,500 --> 00:40:44,270
수 있고 훨씬 더 좋은 결과를

614
00:40:44,270 --> 00:40:46,360
내지만, 대규모 딥러닝에서는

615
00:40:46,360 --> 00:40:52,860
데이터가 방대해서 이 과정을 여러 번, 심지어 다섯 번 반복하는

616
00:40:52,860 --> 00:40:56,290
것이 매우 어렵기 때문에 덜 사용됩니다.

617
00:40:56,290 --> 00:41:00,040
그래서 우리는 종종 직관에 의존해 하이퍼파라미터를

618
00:41:00,040 --> 00:41:03,720
설정하고, 단일 검증 세트를 사용하는

619
00:41:03,720 --> 00:41:05,200
경우가 많습니다.

620
00:41:05,200 --> 00:41:09,150
하지만 이 방법이 권장되는 편입니다.

621
00:41:09,150 --> 00:41:13,570
다시 말해, 컴퓨터 비전이나 대규모 데이터

622
00:41:13,570 --> 00:41:16,200
세트 외부에서는 연구

623
00:41:16,200 --> 00:41:22,710
논문에서 결과가 테스트 세트에서 재현 가능하도록 하기 위해 이런

624
00:41:22,710 --> 00:41:27,780
종류의 교차 검증과 통계적 프레임워크를 요구하는

625
00:41:27,780 --> 00:41:29,650
경우가 많습니다.

626
00:41:29,650 --> 00:41:32,590
어쨌든, 다양한 접근법이 있습니다.

627
00:41:32,590 --> 00:41:36,420
이제 최근접 이웃 주제를

628
00:41:36,420 --> 00:41:42,720
마무리하고 몇 가지 예제와 결과를

629
00:41:42,720 --> 00:41:45,540
살펴보겠습니다.

630
00:41:45,540 --> 00:41:50,268
CIFAR10 데이터 세트를 소개하겠습니다.

631
00:41:50,268 --> 00:41:51,810
이 데이터 세트는

632
00:41:51,810 --> 00:41:56,470
과제에서 자주 사용하게 될 데이터 세트 중 하나입니다.

633
00:41:56,470 --> 00:42:01,260
10개의 클래스가 있고, 각각 훈련 이미지와 테스트 이미지가

634
00:42:01,260 --> 00:42:02,170
있습니다.

635
00:42:02,170 --> 00:42:03,960
10개 클래스 중 일부

636
00:42:03,960 --> 00:42:07,590
예제가 테스트 이미지 각각에 대해 최근접 이웃과

637
00:42:07,590 --> 00:42:09,580
함께 여기 표시되어 있습니다.

638
00:42:09,580 --> 00:42:14,790
최근접 이웃을 실행하고 상위

639
00:42:14,790 --> 00:42:22,980
10개의 최근접 이웃을 선택하면 모두 시각화됩니다.

640
00:42:22,980 --> 00:42:26,880
상상할 수 있듯이, 가장 먼저 답해야 할 질문 중 하나는 k 값,

641
00:42:26,880 --> 00:42:28,260
즉 몇 개의 최근접

642
00:42:31,290 --> 00:42:32,930
이웃을 선택할 것인가입니다.

643
00:42:32,930 --> 00:42:34,680
몇 개의 최근접 이웃을 선택해야 할까요?

644
00:42:34,680 --> 00:42:39,960
그리고 k 값 각각에 대해 5-폴드 교차 검증 실험

645
00:42:39,960 --> 00:42:42,660
중 하나를 살펴보겠습니다.

646
00:42:42,660 --> 00:42:47,960
각 점은 5-폴드 중 하나의 폴드를 나타내며, 여기서

647
00:42:47,960 --> 00:42:52,080
다양한 k 값에 따른 결과를 보여줍니다.

648
00:42:52,080 --> 00:42:56,880
여기서 보시다시피, k가 7일 때

649
00:42:56,880 --> 00:43:01,440
정확도가 가장 좋습니다. 약

650
00:43:01,440 --> 00:43:03,740
28~29%

651
00:43:03,740 --> 00:43:10,190
정확도를 내는데, 10개 클래스 분류 문제

652
00:43:10,190 --> 00:43:14,700
치고는 나쁘지 않은 결과입니다.

653
00:43:14,700 --> 00:43:17,190
10개 클래스 분류 문제에서는

654
00:43:17,190 --> 00:43:21,600
무작위 추측이 보통 10% 정확도를 내죠.

655
00:43:21,600 --> 00:43:24,630
그래서 이 결과는 무작위 추측보다 훨씬 낫습니다.

656
00:43:24,630 --> 00:43:25,680
즉, 작동하고 있습니다.

657
00:43:25,680 --> 00:43:30,980
어떤 결과를 내고 있지만 개선할 여지가 많다는 뜻입니다.

658
00:43:30,980 --> 00:43:35,700
예제로 다시 돌아가 보면, 특히 가장 가까운

659
00:43:35,700 --> 00:43:38,900
예에서 실수가 많다는 걸

660
00:43:38,900 --> 00:43:41,040
알 수 있습니다.

661
00:43:41,040 --> 00:43:44,150
예를 들어 네 번째 줄을 보면, 개구리인데,

662
00:43:44,150 --> 00:43:47,690
첫 번째 예는 고양이처럼 보입니다.

663
00:43:47,690 --> 00:43:48,770
아, 죄송합니다, 개입니다.

664
00:43:48,770 --> 00:43:54,340
왜 이런 일이 일어나는지 짐작할 수 있죠? 거리가

665
00:43:54,340 --> 00:43:57,580
픽셀 단위로 계산되기 때문입니다.

666
00:43:57,580 --> 00:44:01,970
픽셀 단위로 보면 서로 비슷해 보입니다.

667
00:44:01,970 --> 00:44:05,570
대부분 픽셀에서 색깔이 비슷해서 거리가

668
00:44:05,570 --> 00:44:08,330
더 가깝게 나오는 거죠.

669
00:44:08,330 --> 00:44:10,390
이 예와 다른 여러

670
00:44:10,390 --> 00:44:15,250
예는 픽셀 값에 기반한 거리가 최선의 선택이

671
00:44:15,250 --> 00:44:16,880
아님을 보여줍니다.

672
00:44:16,880 --> 00:44:18,860
우리는 이런 방식을 실제로 사용하지 않습니다.

673
00:44:18,860 --> 00:44:21,370
더 나은 접근법들이

674
00:44:21,370 --> 00:44:29,420
있는데, 앞으로 강의 후반부에서 더 자세히 다룰 예정입니다.

675
00:44:29,420 --> 00:44:34,340
마지막으로 이 주제를 정리하며, 또 다른 예를 보여드리겠습니다.

676
00:44:34,340 --> 00:44:37,820
이 원본 이미지를 보면, 세

677
00:44:37,820 --> 00:44:40,090
개의 이미지가

678
00:44:40,090 --> 00:44:46,770
색상이나 가림 현상 측면에서 매우 다르게 보이지만, 왼쪽에서

679
00:44:46,770 --> 00:44:51,000
세 번째 이미지는 단지 한 픽셀

680
00:44:51,000 --> 00:44:55,660
오른쪽으로 이동한 같은 이미지입니다.

681
00:44:55,660 --> 00:44:58,590
사람의 눈으로 보면 전혀 차이가

682
00:44:58,590 --> 00:45:00,640
없다고 할 수 있습니다.

683
00:45:00,640 --> 00:45:05,310
하지만 그 이미지와 원본 이미지 사이의 거리는 여기

684
00:45:05,310 --> 00:45:08,590
보이는 다른 두 예와 동일합니다.

685
00:45:08,590 --> 00:45:11,680
질문을 몇 개 받겠습니다. 그리고 지금까지

686
00:45:11,680 --> 00:45:13,870
논의한 내용을 요약한 것입니다.

687
00:45:13,870 --> 00:45:16,680
그래서 질문은, 어떻게 결정을 내리느냐 하는 겁니다.

688
00:45:16,680 --> 00:45:20,700
그럴 때는 보통 상위 후보 중에서 무작위로 하나를 선택하는

689
00:45:20,700 --> 00:45:21,750
경우가 많습니다.

690
00:45:21,750 --> 00:45:27,450
더 많은 데이터를 수집해야 한다면, 예를

691
00:45:27,450 --> 00:45:30,180
들어 지금 유전학

692
00:45:30,180 --> 00:45:33,450
문제를 해결하거나 의료

693
00:45:33,450 --> 00:45:40,200
영상 문제를 다룰 때, 예제나 특징을 시각화해

694
00:45:40,200 --> 00:45:41,020
보면,

695
00:45:41,020 --> 00:45:43,570
최근접 이웃

696
00:45:43,570 --> 00:45:47,610
공간에서 좋은 샘플이 없거나

697
00:45:47,610 --> 00:45:52,920
모호한 영역이 보이면, 그 공간에

698
00:45:52,920 --> 00:45:55,440
속하는 다른 샘플을

699
00:45:55,440 --> 00:45:58,890
찾아보려고 노력합니다.

700
00:45:58,890 --> 00:46:03,960
좋습니다, 그래서 k-최근접 이웃에 대해

701
00:46:03,960 --> 00:46:06,330
이야기한 내용을 요약하자면,

702
00:46:06,330 --> 00:46:12,900
가장 쉬운 알고리즘인 데이터 기반 접근법을 이해하는

703
00:46:12,900 --> 00:46:17,250
것이었고, 하이퍼파라미터 튜닝과 거리 측정

704
00:46:17,250 --> 00:46:19,080
방법, 그리고 k

705
00:46:19,080 --> 00:46:23,040
값이 얼마나 중요한 역할을 하는지에

706
00:46:23,040 --> 00:46:26,070
대해 조금 이야기했습니다.

707
00:46:26,070 --> 00:46:30,570
다음 주제로 넘어가겠습니다, 바로 선형 분류기입니다.

708
00:46:30,570 --> 00:46:35,640
이 주제를 다루는 데 25분 정도 시간이 필요합니다,

709
00:46:35,640 --> 00:46:40,190
이번 강의의 남은 시간을 이 매우

710
00:46:40,190 --> 00:46:43,770
중요한 주제에 할애하고자 합니다.

711
00:46:43,770 --> 00:46:50,960
이것은 거의 모든 것의 가장 중요한 기본 요소입니다.

712
00:46:50,960 --> 00:46:53,930
딥러닝의 전부입니다.

713
00:46:53,930 --> 00:47:04,200
그리고 이 접근법이 어떻게 다른지 살펴볼 필요가 있습니다.

714
00:47:04,200 --> 00:47:07,740
먼저 이 방법이 최근접 이웃과 어떻게 다른지 보겠습니다.

715
00:47:07,740 --> 00:47:09,900
이것은 파라메트릭

716
00:47:09,900 --> 00:47:14,180
접근법인데, 즉 입력 이미지를 출력

717
00:47:14,180 --> 00:47:17,900
클래스, 출력 숫자로 매핑하는

718
00:47:17,900 --> 00:47:22,850
파라미터 w, 즉 가중치를 학습하는

719
00:47:22,850 --> 00:47:23,850
것입니다.

720
00:47:23,850 --> 00:47:26,720
이 경우, 입력을 출력으로

721
00:47:26,720 --> 00:47:30,290
매핑하는 함수 f를 만들

722
00:47:30,290 --> 00:47:35,830
때, 출력은 종종 이미지가 10개의 출력 클래스

723
00:47:35,830 --> 00:47:41,900
레이블 각각에 속할 확률 점수 같은 것입니다.

724
00:47:41,900 --> 00:47:49,060
이렇게 구축한 설정에서, 선형 분류기는

725
00:47:49,060 --> 00:47:53,830
먼저 파라미터 w를 사용해

726
00:47:53,830 --> 00:48:01,010
각 입력 x를 출력 y 값으로 매핑합니다.

727
00:48:01,010 --> 00:48:04,640
이 과정은 매우 간단합니다.

728
00:48:04,640 --> 00:48:10,250
이 이미지는 기본적으로 32x32x3, 즉 3,

729
00:48:10,250 --> 00:48:14,150
072개의 숫자로 이루어져 있습니다.

730
00:48:14,150 --> 00:48:23,000
이것이 우리의 x를 정의하며, 3,072x1 벡터입니다.

731
00:48:23,000 --> 00:48:26,360
그리고 우리는 10개의 출력 클래스가 있다는 것을 알고 있습니다.

732
00:48:26,360 --> 00:48:28,430
그래서 10개의 서로 다른 점수가 필요합니다.

733
00:48:28,430 --> 00:48:29,650
출력

734
00:48:29,650 --> 00:48:34,690
점수는 10x1 벡터 형태가 됩니다.

735
00:48:34,690 --> 00:48:38,910
이는 x를 출력 점수로 매핑하는

736
00:48:38,910 --> 00:48:47,520
10x3,072 크기의 가중치 행렬 w를 찾아야 한다는 의미입니다.

737
00:48:47,520 --> 00:48:50,020
이 선형 함수를 완성하기

738
00:48:50,020 --> 00:48:56,380
위해, 우리는 종종 편향(bias) 항도 사용합니다.

739
00:48:56,380 --> 00:49:01,620
이 값은 입력과 무관하며, 실제로 다양한

740
00:49:01,620 --> 00:49:03,400
용도가 있습니다.

741
00:49:03,400 --> 00:49:07,990
기하학적 시각화를 할 때 설명할 수

742
00:49:07,990 --> 00:49:13,500
있는데, 편향은 때때로 클래스 점수에 변화를

743
00:49:13,500 --> 00:49:20,470
주어 각 클래스를 더 잘 분리하는 데 도움을 줍니다.

744
00:49:20,470 --> 00:49:23,730
앞서 말했듯이, 이러한 선형

745
00:49:23,730 --> 00:49:28,270
함수들은 신경망을 구성하는 기본 빌딩 블록입니다.

746
00:49:28,270 --> 00:49:32,700
이 선형 분류기들, 즉 선형

747
00:49:32,700 --> 00:49:35,170
함수들이 차례대로

748
00:49:35,170 --> 00:49:39,240
연결되면 큰 신경망을 만듭니다.

749
00:49:39,240 --> 00:49:42,480
여기에 추가해야 할 다른 요소들도

750
00:49:42,480 --> 00:49:45,720
많지만, 이것이 가장 중요한

751
00:49:45,720 --> 00:49:48,190
구성 요소 중 하나입니다.

752
00:49:48,190 --> 00:49:56,200
인기 있는 신경망들을 살펴보면, 아키텍처 곳곳에

753
00:49:56,200 --> 00:50:00,570
선형 함수가 존재하는 것을

754
00:50:00,570 --> 00:50:03,600
볼 수 있습니다.

755
00:50:03,600 --> 00:50:08,040
이 매핑과 함수가 무엇을 하는지 더 잘 이해하기

756
00:50:08,040 --> 00:50:11,880
위해, CIFAR10 예제와 훈련 및

757
00:50:11,880 --> 00:50:15,580
테스트 샘플로 다시 돌아가서, 좀 더

758
00:50:15,580 --> 00:50:18,160
간단하게 만들어 보겠습니다.

759
00:50:18,160 --> 00:50:22,120
32x32 크기의 큰 이미지 대신에, 2x2

760
00:50:22,120 --> 00:50:25,680
크기의 네 개 픽셀로 이루어진 입력

761
00:50:25,680 --> 00:50:27,540
이미지를 보겠습니다.

762
00:50:27,540 --> 00:50:34,560
즉, 입력 이미지가 벡터로 변환된다는 의미입니다.

763
00:50:34,560 --> 00:50:42,260
여기서 보시다시피, w와 b 값을 찾아야 합니다.

764
00:50:42,260 --> 00:50:49,200
그래서 입력 이미지는 출력으로 어떤 점수들로 매핑됩니다.

765
00:50:49,200 --> 00:50:54,710
대수학적 관점에서 선형 함수가

766
00:50:54,710 --> 00:50:58,050
이렇게 생겼습니다.

767
00:50:58,050 --> 00:51:01,310
여기 출력 점수는 고양이, 개, 배 세 가지

768
00:51:01,310 --> 00:51:02,940
클래스를 고려하고 있습니다.

769
00:51:02,940 --> 00:51:10,040
이 함수가 이미지, 즉 이미지를 나타내는 벡터를

770
00:51:10,040 --> 00:51:13,520
그 점수들로 매핑하는

771
00:51:13,520 --> 00:51:16,820
것을 볼 수 있습니다.

772
00:51:16,820 --> 00:51:21,780
선형 분류의 대수학적 관점입니다.

773
00:51:21,780 --> 00:51:26,000
이제 이 선형 분류기의 시각적 관점을

774
00:51:26,000 --> 00:51:27,910
살펴보겠습니다.

775
00:51:27,910 --> 00:51:33,190
보시다시피, 이 이미지에 대해 이야기한

776
00:51:33,190 --> 00:51:38,540
것처럼, 각 이미지를 자주 만듭니다.

777
00:51:38,540 --> 00:51:45,310
각 클래스마다, 행렬 w의 행

778
00:51:45,310 --> 00:51:46,330
하나를

779
00:51:46,330 --> 00:51:48,506
정의합니다.

780
00:51:48,506 --> 00:51:51,070
이 행은 특정 클래스에 대한 템플릿

781
00:51:51,070 --> 00:51:52,760
같은 역할을 합니다.

782
00:51:52,760 --> 00:51:57,370
이렇게 분리하면, 이 이미지는 w와

783
00:51:57,370 --> 00:52:01,030
b에 곱해지고, w는 고양이,

784
00:52:01,030 --> 00:52:03,940
개, 배 세 클래스

785
00:52:03,940 --> 00:52:06,140
각각의 템플릿입니다.

786
00:52:06,140 --> 00:52:12,740
CIFAR 데이터셋으로 모델을 훈련하거나 구축한 후,

787
00:52:12,740 --> 00:52:18,880
선형 분류기의 시각적 관점에서 각 10개

788
00:52:18,880 --> 00:52:21,170
클래스에 대해 학습된

789
00:52:21,170 --> 00:52:23,740
템플릿을 보면,

790
00:52:23,740 --> 00:52:26,120
이런 템플릿들을 볼

791
00:52:26,120 --> 00:52:27,910
수 있습니다.

792
00:52:27,910 --> 00:52:30,570
흥미로운 점은, 예를 들어

793
00:52:30,570 --> 00:52:34,360
자동차 클래스의 경우, 자동차

794
00:52:34,360 --> 00:52:39,640
앞모습 같은 템플릿이 보인다는 겁니다. 그리고 이

795
00:52:39,640 --> 00:52:45,870
모든 것이 단 하나의 선형 분류기로 이루어졌다는 점입니다.

796
00:52:45,870 --> 00:52:48,900
그래서 선형 분류기의 시각적

797
00:52:48,900 --> 00:52:51,030
측면, 시각적 관점이

798
00:52:51,030 --> 00:52:55,800
있고, 또 다른 측면은 기하학적 관점입니다.

799
00:52:55,800 --> 00:53:01,290
이 선형 분류기가 자주 하는 일은 2D

800
00:53:01,290 --> 00:53:05,910
공간이라면 각 클래스를 다른 클래스와

801
00:53:05,910 --> 00:53:09,240
구분하는 선을 찾는 것입니다.

802
00:53:09,240 --> 00:53:12,570
여기서 보시다시피 빨강,

803
00:53:12,570 --> 00:53:20,260
파랑, 초록이 서로 다른 클래스를 정의하고 있고, 고차원

804
00:53:20,260 --> 00:53:22,990
공간에서는 이 선 대신

805
00:53:22,990 --> 00:53:27,450
왼쪽 예시처럼 초평면이 있습니다.

806
00:53:27,450 --> 00:53:33,240
여기서 편향(bias) 항의 사용도 볼 수 있는데,

807
00:53:33,240 --> 00:53:36,420
만약 편향이 없다면 모든

808
00:53:36,420 --> 00:53:38,250
선이 공간의

809
00:53:38,250 --> 00:53:42,580
중심인 원점을 지나야 해서 의미가 없죠.

810
00:53:42,580 --> 00:53:44,640
하지만 편향이

811
00:53:44,640 --> 00:53:53,220
있으면 더 신뢰할 수 있는 함수와 결정 경계를 만들 수 있습니다.

812
00:53:53,220 --> 00:53:57,190
그래서 선형 함수는 매우 유용합니다.

813
00:53:57,190 --> 00:54:01,150
선형 분류기는 우리가 이야기한 것처럼 많은 응용 분야에

814
00:54:01,150 --> 00:54:02,650
매우 유용합니다.

815
00:54:02,650 --> 00:54:08,050
그리고 더 복잡한 신경망의 기본 구성 요소입니다.

816
00:54:08,050 --> 00:54:10,410
하지만 이 방법에도 자체적인

817
00:54:10,410 --> 00:54:18,250
어려움이 있습니다. 왜냐하면 여러 개의 분리된 데이터 인스턴스를 분류할 수
없기 때문입니다.

818
00:54:18,250 --> 00:54:20,910
예를 들어, 이 경우 클래스 1이

819
00:54:20,910 --> 00:54:24,200
1사분면과 3사분면이고, 클래스 2가

820
00:54:24,200 --> 00:54:26,100
2사분면과 4사분면이라면,

821
00:54:26,100 --> 00:54:28,170
이들을 선형적으로 분리할 방법이 없습니다.

822
00:54:28,170 --> 00:54:34,580
또 다른 예로, 클래스 1과 클래스 2 사이에 이런 식의 분리가

823
00:54:34,580 --> 00:54:37,970
있다면, 원점으로부터 거리가 1과 2

824
00:54:37,970 --> 00:54:41,570
사이인 영역을 클래스 1로, 나머지를

825
00:54:41,570 --> 00:54:44,580
클래스 2로 하는 경우입니다.

826
00:54:44,580 --> 00:54:47,100
마찬가지로, 공간에 세 개의 영역이

827
00:54:47,100 --> 00:54:50,760
하나의 클래스이고, 나머지 전체가 두 번째 클래스인

828
00:54:50,760 --> 00:54:52,260
경우도 있습니다.

829
00:54:52,260 --> 00:54:54,620
이 모든 경우에 분리는

830
00:54:54,620 --> 00:54:57,770
사실 매우 어렵습니다.

831
00:54:57,770 --> 00:55:04,260
그래서 우리가 해야 할 일은—우리는 선형 분류기에 대해

832
00:55:04,260 --> 00:55:06,380
이야기했고, 그것이 입력

833
00:55:06,380 --> 00:55:12,200
이미지를 출력의 어떤 형태의 레이블로 매핑할 수

834
00:55:12,200 --> 00:55:14,580
있다는 것을 알았습니다.

835
00:55:14,580 --> 00:55:20,480
하지만 이제 남은 것은, 각 이미지가 출력으로 각 단일

836
00:55:20,480 --> 00:55:22,690
클래스에 대한 점수로

837
00:55:22,690 --> 00:55:27,130
매핑되도록 하는 값 w를 어떻게 선택할지입니다.

838
00:55:27,130 --> 00:55:31,970
그리고 그렇게 하기 위해서는, 분류기, 즉 모델이

839
00:55:31,970 --> 00:55:34,330
얼마나 잘못 작동하는지를

840
00:55:34,330 --> 00:55:38,960
수치화하는 손실 함수, 때로는 목적 함수라고도

841
00:55:38,960 --> 00:55:42,140
불리는 것을 정의해야 합니다.

842
00:55:42,140 --> 00:55:45,940
즉, 훈련 데이터에 대한 점수에

843
00:55:45,940 --> 00:55:49,210
대한 불만족 정도입니다.

844
00:55:49,210 --> 00:55:54,700
이것들을 정의한 후에는, w의 값을 효율적으로

845
00:55:54,700 --> 00:56:01,730
변경하여 그 불만족을 최소화할 수 있는 방법, 즉 손실 함수를

846
00:56:01,730 --> 00:56:05,180
최소화하는 방법을 찾아야 합니다.

847
00:56:05,180 --> 00:56:09,260
이것이 바로 최적화 과정입니다.

848
00:56:09,260 --> 00:56:12,620
다음 수업, 다음 강의의 주제입니다.

849
00:56:12,620 --> 00:56:18,610
그리고 다시 간단하게 하기 위해, 여기 보시는

850
00:56:18,610 --> 00:56:23,160
것처럼 세 개의 클래스와 선형 함수를

851
00:56:23,160 --> 00:56:27,520
가진 더 쉽고 쉬운 예제를

852
00:56:27,520 --> 00:56:35,430
보겠습니다. 세 클래스는 고양이, 자동차, 그리고 개구리입니다.

853
00:56:35,430 --> 00:56:37,590
현재 분류기가 얼마나 좋은지

854
00:56:37,590 --> 00:56:40,270
알려주는 손실 함수가 필요합니다.

855
00:56:40,270 --> 00:56:45,210
이를 위해 문제를 매개변수화해야 하는데, 입력

856
00:56:45,210 --> 00:56:50,580
이미지와 레이블 이미지, 그리고 해당 레이블을 각각

857
00:56:50,580 --> 00:56:52,900
xi와 yi로 정의합니다.

858
00:56:52,900 --> 00:56:56,340
그리고 나서 손실 함수와

859
00:56:56,340 --> 00:56:59,550
거리 함수를 사용하여,

860
00:56:59,550 --> 00:57:05,190
예측된 점수 fx와 w, 그리고 이미

861
00:57:05,190 --> 00:57:10,170
주어진 실제 값인 yi를 비교하여

862
00:57:10,170 --> 00:57:13,950
차이와 점수가 얼마나 나쁜지

863
00:57:13,950 --> 00:57:15,760
살펴봅니다.

864
00:57:15,760 --> 00:57:19,300
우리는 종종 샘플 수에 따라 정규화하기도 하지만, 그 부분은

865
00:57:19,300 --> 00:57:20,830
그렇게 중요하지 않습니다.

866
00:57:20,830 --> 00:57:23,160
이것이 손실 함수, 즉

867
00:57:23,160 --> 00:57:26,460
목적 함수를 정의하는 것입니다.

868
00:57:26,460 --> 00:57:32,670
그렇다면 어떻게 최적화를

869
00:57:32,670 --> 00:57:40,510
수행하고 w를 실제로 찾을 수 있을까요?

870
00:57:40,510 --> 00:57:44,370
이 l, 즉 li를 정의하는 다양한 방법이

871
00:57:44,370 --> 00:57:44,980
있습니다.

872
00:57:44,980 --> 00:57:50,790
지금은 softmax 분류기에 대해 이야기하고 싶습니다.

873
00:57:50,790 --> 00:57:57,030
예를 들어 고양이 클래스의 점수가 3.2,

874
00:57:57,030 --> 00:57:59,010
5.1,

875
00:57:59,010 --> 00:58:04,710
그리고 -1.7이었다면, 이것들은

876
00:58:04,710 --> 00:58:10,530
우리가 논의한 함수 f, xi와 w의

877
00:58:10,530 --> 00:58:13,290
출력 점수입니다.

878
00:58:13,290 --> 00:58:18,570
이 점수들은 제한이 없고, 선형 함수이기

879
00:58:18,570 --> 00:58:20,210
때문에 값이

880
00:58:20,210 --> 00:58:22,730
크게 제어되지 않으므로,

881
00:58:22,730 --> 00:58:26,870
이 점수들을 변환해야 합니다.

882
00:58:26,870 --> 00:58:34,740
이것들을 점수 함수로 바꾸기 위해 가장 좋은

883
00:58:34,740 --> 00:58:37,130
방법은 각 입력

884
00:58:37,130 --> 00:58:39,080
이미지 xi에

885
00:58:39,080 --> 00:58:45,050
대해 클래스가 k일 확률을 정의하는

886
00:58:45,050 --> 00:58:49,320
확률로 바꾸는 것입니다.

887
00:58:49,320 --> 00:58:53,510
그렇게 하기 위해 먼저 사용하는

888
00:58:53,510 --> 00:58:58,350
함수가 바로 softmax 함수입니다.

889
00:58:58,350 --> 00:59:02,900
우선 점수 값들을 지수화해서 이런

890
00:59:02,900 --> 00:59:05,580
숫자들을 만듭니다.

891
00:59:05,580 --> 00:59:08,190
이 숫자들에 exp를

892
00:59:08,190 --> 00:59:10,110
적용하면 출력값은

893
00:59:10,110 --> 00:59:12,550
항상 양수가 됩니다.

894
00:59:12,550 --> 00:59:15,370
확률은 항상 양수여야 하니까 이 점이

895
00:59:15,370 --> 00:59:16,220
중요합니다.

896
00:59:16,220 --> 00:59:19,660
이 숫자들을 만든 후에는 그냥

897
00:59:19,660 --> 00:59:22,150
정규화하면 됩니다.

898
00:59:22,150 --> 00:59:24,370
즉, 지수화한 후 모든 샘플의

899
00:59:24,370 --> 00:59:26,690
합으로 나누어 정규화하는 거죠.

900
00:59:26,690 --> 00:59:32,930
그래서 모든 샘플의 합으로 나누어 정규화합니다.

901
00:59:32,930 --> 00:59:37,360
이렇게 하면 확률 함수를 정의하는 아주

902
00:59:37,360 --> 00:59:41,060
좋은 값들의 집합이 만들어집니다.

903
00:59:41,060 --> 00:59:43,280
이것이 바로 분포 함수입니다.

904
00:59:43,280 --> 00:59:44,510
이 값들의 합은 1이 됩니다.

905
00:59:44,510 --> 00:59:48,580
이걸 해석하기 아주 쉬운데,

906
00:59:48,580 --> 00:59:56,500
이 파라미터 w 집합이 이 이미지가 고양이일

907
00:59:56,500 --> 01:00:02,260
확률이 13%, 즉 0.13이라고

908
01:00:02,260 --> 01:00:06,224
생각한다는 뜻입니다.

909
01:00:06,224 --> 01:00:11,440
분명 이 예에서는 w가 좋은 설정이 아니라

910
01:00:11,440 --> 01:00:14,230
실수를 하고 있죠.

911
01:00:14,230 --> 01:00:17,410
우리는 이걸 최적화해서 바꿔야 합니다.

912
01:00:17,410 --> 01:00:21,120
이 확률들은 보통 logits라고

913
01:00:21,120 --> 01:00:26,190
불리는 정규화되지 않은 로그 확률의

914
01:00:26,190 --> 01:00:27,850
대응값입니다.

915
01:00:27,850 --> 01:00:33,120
다른 머신러닝 강의를 들었거나, logistic

916
01:00:33,120 --> 01:00:33,960
regression을

917
01:00:33,960 --> 01:00:39,280
사용해본 적이 있다면 이와 비슷한 프레임워크라는

918
01:00:39,280 --> 01:00:41,560
걸 알 수 있습니다.

919
01:00:41,560 --> 01:00:45,660
이것은 로지스틱 회귀와 정확히 같은 프레임워크입니다.

920
01:00:45,660 --> 01:00:49,200
여기서 여러 클래스가

921
01:00:49,200 --> 01:00:54,540
있으므로 다항 로지스틱 회귀입니다.

922
01:00:54,540 --> 01:00:57,553
함수 l을 어떻게 정의할까요?

923
01:00:57,553 --> 01:00:59,220
함수 l을 정의하는

924
01:00:59,220 --> 01:01:01,980
여러 방법이 있다고 말씀드렸습니다.

925
01:01:01,980 --> 01:01:05,590
우리가 정의하고자 하는 손실 함수의 목적은

926
01:01:05,590 --> 01:01:06,750
무엇일까요?

927
01:01:06,750 --> 01:01:13,950
샘플이 올바른 클래스에 속할 확률을 최대화하는

928
01:01:13,950 --> 01:01:15,820
것입니다.

929
01:01:15,820 --> 01:01:21,810
그래서 0.13의 값을 최대화하고 싶습니다.

930
01:01:21,810 --> 01:01:28,150
이제 그 집합에 더 큰 값들도 있습니다.

931
01:01:28,150 --> 01:01:34,600
이 값을 최대화하고 싶다면, 이것은 최대화 문제입니다.

932
01:01:34,600 --> 01:01:38,100
모든 목적 함수는 최소화

933
01:01:38,100 --> 01:01:42,720
문제로 만들기 때문에,

934
01:01:42,720 --> 01:01:49,170
첫 번째 단계는 값을 부정하는 것입니다.

935
01:01:49,170 --> 01:01:52,020
부정을 해서 최대화 문제를 최소화

936
01:01:52,020 --> 01:01:53,860
문제로 바꾸는 거죠.

937
01:01:53,860 --> 01:01:55,980
그리고 숫자를 좀 더

938
01:01:55,980 --> 01:02:00,130
다루기 쉽게 하기 위해 로그를 취합니다.

939
01:02:00,130 --> 01:02:03,570
그래서 그 값의 음의 로그가

940
01:02:03,570 --> 01:02:06,810
이 문제를 푸는 목적 함수,

941
01:02:06,810 --> 01:02:10,050
즉 손실 함수를 정의합니다.

942
01:02:10,050 --> 01:02:10,740
매우 간단합니다.

943
01:02:10,740 --> 01:02:12,440
이것이

944
01:02:12,440 --> 01:02:20,540
softmax와 이 로지스틱 회귀 함수의 목적 함수, 즉 손실 함수입니다.

945
01:02:20,540 --> 01:02:21,270
softmax와 이 로지스틱 회귀 함수의 목적 함수, 즉 손실 함수입니다.

946
01:02:21,270 --> 01:02:28,490
그리고 CS-229 같은 다른 수업을 들었다면, 이것을

947
01:02:28,490 --> 01:02:31,550
최대 우도 추정이라고도

948
01:02:31,550 --> 01:02:32,160
부릅니다.

949
01:02:32,160 --> 01:02:34,040
같은 알고리즘입니다.

950
01:02:34,040 --> 01:02:42,540
이 점을 염두에 두고 말씀드리면, 앞서 논의한 것처럼 올바른

951
01:02:42,540 --> 01:02:45,140
클래스의 확률 로그의

952
01:02:45,140 --> 01:02:49,580
음수가 목적 함수, 손실 함수를

953
01:02:49,580 --> 01:02:50,560
정의합니다.

954
01:02:53,690 --> 01:02:56,130
기본적으로 그렇게 간단합니다.

955
01:02:56,130 --> 01:02:59,930
하지만 이 프레임워크를 해석하는 다른 방법도

956
01:02:59,930 --> 01:03:00,720
있습니다.

957
01:03:00,720 --> 01:03:08,830
이 손실 함수를 재정의하는 한 가지 방법은 추정된 확률과

958
01:03:08,830 --> 01:03:12,560
올바른 확률을 정의하는

959
01:03:12,560 --> 01:03:15,640
확률 함수가 있다고

960
01:03:15,640 --> 01:03:18,200
보는 것입니다.

961
01:03:18,200 --> 01:03:23,080
우리가 하고 싶은 것은 이 두 확률 함수를 일치시키는 것입니다.

962
01:03:23,080 --> 01:03:29,840
이를 위해 KL 발산, 즉 Kullback-Leibler 발산을

963
01:03:29,840 --> 01:03:32,210
최소화하려고 합니다.

964
01:03:32,210 --> 01:03:36,640
이것은 정보 이론적 관점에서 이 손실 함수를

965
01:03:36,640 --> 01:03:39,140
바라보는 방법입니다.

966
01:03:39,140 --> 01:03:43,760
그리고 다시 말하지만, 이 둘은 정확히 같습니다.

967
01:03:43,760 --> 01:03:49,360
이 설정에서 KL 발산은 우리가 정의한

968
01:03:49,360 --> 01:03:53,290
음의 로그 함수로

969
01:03:53,290 --> 01:03:54,410
단순화됩니다.

970
01:03:54,410 --> 01:04:01,660
더 나아가 이것은 정확히 교차 엔트로피 함수입니다.

971
01:04:01,660 --> 01:04:09,750
왜냐하면 p의 엔트로피, 즉 올바른 확률의 엔트로피에

972
01:04:09,750 --> 01:04:14,500
같은 KL 발산을 더하면,

973
01:04:14,500 --> 01:04:19,210
다시 같은 음의 로그 함수로

974
01:04:19,210 --> 01:04:22,300
단순화되기 때문입니다.

975
01:04:22,300 --> 01:04:26,160
그리고 클래스에 대해 원-핫 인코딩을

976
01:04:26,160 --> 01:04:29,920
사용할 때 엔트로피가 0이기 때문입니다.

977
01:04:29,920 --> 01:04:31,920
그래서 이 함수를 교차 엔트로피

978
01:04:31,920 --> 01:04:34,650
또는 이진 교차 엔트로피 함수라고 부르는

979
01:04:34,650 --> 01:04:36,160
이유 중 하나입니다.

980
01:04:36,160 --> 01:04:39,870
딥러닝 전반에서, 신경망 프레임워크를 사용해봤다면

981
01:04:39,870 --> 01:04:42,790
BCE, 즉 Binary Cross

982
01:04:42,790 --> 01:04:45,240
Entropy에 대해 들어봤거나

983
01:04:45,240 --> 01:04:47,980
앞으로 많이 듣게 될 겁니다.

984
01:04:47,980 --> 01:04:50,850
이것이 바로 같은 프레임워크입니다.

985
01:04:50,850 --> 01:04:57,600
아주 간단한 것부터 시작했지만, 각각의 유사점과

986
01:04:57,600 --> 01:05:00,490
차이점을 알게 되었습니다.

987
01:05:00,490 --> 01:05:03,810
그래서 목적 함수-- 죄송합니다, 손실

988
01:05:03,810 --> 01:05:06,250
함수는 이 확률의 음의 로그로

989
01:05:06,250 --> 01:05:09,600
정의되었고, 확률은 우리가 이야기한

990
01:05:09,600 --> 01:05:11,650
softmax로 정의되었습니다.

991
01:05:11,650 --> 01:05:15,600
그리고 다음 세션 주제인 이것을 최적화하면

992
01:05:15,600 --> 01:05:19,450
올바른 w 값을 얻을 수 있습니다.

993
01:05:19,450 --> 01:05:23,790
하지만 마치기 전에 여기 보이는 정의에 대해 몇

994
01:05:23,790 --> 01:05:26,140
가지 질문을 드리고 싶습니다.

995
01:05:26,140 --> 01:05:29,550
손실 함수 li의

996
01:05:29,550 --> 01:05:33,450
평균값과 최대값은 무엇일까요?

997
01:05:33,450 --> 01:05:37,090
네, 0이고, 이는 음의 무한대로 변합니다.

998
01:05:37,090 --> 01:05:41,470
하지만 음수가 있으니 무한대가 됩니다.

999
01:05:41,470 --> 01:05:42,520
맞습니다.

1000
01:05:42,520 --> 01:05:44,285
그리고

1001
01:05:47,430 --> 01:05:52,240
또한-- 네, 확실히 맞습니다.

1002
01:05:52,240 --> 01:05:56,740
그리고 두 번째 질문을 살펴보겠습니다.

1003
01:05:56,740 --> 01:05:59,850
네, 이 질문입니다.

1004
01:05:59,850 --> 01:06:05,550
초기화할 때 모든 si, 즉 기본적으로 w 값들은

1005
01:06:05,550 --> 01:06:07,990
거의 무작위입니다.

1006
01:06:07,990 --> 01:06:11,730
그래서 각 클래스의

1007
01:06:11,730 --> 01:06:18,030
확률은 거의 같아집니다.

1008
01:06:18,030 --> 01:06:22,403
클래스가 c개일 때 softmax li는 무엇일까요?

1009
01:06:30,435 --> 01:06:31,560
특히 c가 10일 때 말입니다.

1010
01:06:34,140 --> 01:06:37,780
확률이 같으니

1011
01:06:37,780 --> 01:06:40,740
모든 확률은

1012
01:06:40,740 --> 01:06:47,110
대략 1/C가 됩니다. 그리고 이는 log C로 정의됩니다.

1013
01:06:47,110 --> 01:06:49,080
클래스가 10개라면

1014
01:06:49,080 --> 01:06:54,730
log 또는 ln 10은 2.3이고, 이는 지수 함수입니다.

1015
01:06:54,730 --> 01:06:57,020
우리는 그것에 대해 알고 있습니다.
