1
00:00:05,510 --> 00:00:10,320
오늘 우리는 이미지 분류에 대해 이야기할 것이며,

2
00:00:10,320 --> 00:00:13,160
기본적으로 지난 강의에서의

3
00:00:13,160 --> 00:00:15,290
이미지 분류 주제를

4
00:00:15,290 --> 00:00:17,430
계속 이어갈 것입니다.

5
00:00:17,430 --> 00:00:23,750
그리고 우리는 신경망, 궁극적으로는 합성곱 신경망과

6
00:00:23,750 --> 00:00:26,990
관련된 주제들로 조금 더

7
00:00:26,990 --> 00:00:30,020
들어가게 될 것입니다.

8
00:00:30,020 --> 00:00:35,780
우리는 선형 분류기부터 시작할 것입니다.

9
00:00:35,780 --> 00:00:43,280
다음 슬라이드로 넘어가면, 이것은 지난 강의에서

10
00:00:43,280 --> 00:00:48,350
이야기했던 강의 계획서입니다.

11
00:00:48,350 --> 00:00:52,520
우리는 심층 학습 기초,

12
00:00:52,520 --> 00:00:57,660
시각 세계 인식 및 이해, 시각

13
00:00:57,660 --> 00:01:02,350
세계 재구성 및 상호작용이라는

14
00:01:02,350 --> 00:01:05,620
세 가지 주요 주제와

15
00:01:05,620 --> 00:01:08,590
수업에서 다룰 몇

16
00:01:08,590 --> 00:01:14,540
가지 하위 주제에 대해 이야기했습니다.

17
00:01:14,540 --> 00:01:17,860
그리고 마지막에는 인간 중심 AI

18
00:01:17,860 --> 00:01:21,100
측면에 대한 논의가 있을 것입니다.

19
00:01:21,100 --> 00:01:27,880
오늘의 목표는 데이터 기반 접근 방식,

20
00:01:27,880 --> 00:01:31,600
이 의미가 무엇인지

21
00:01:31,600 --> 00:01:38,230
설명하고, 선형 분류기와 k-최근접 이웃

22
00:01:38,230 --> 00:01:43,120
알고리즘을 다루는 것입니다.

23
00:01:43,120 --> 00:01:48,050
지난 강의와 마찬가지로

24
00:01:48,050 --> 00:01:55,750
이미지 분류라는 핵심 작업부터 시작합시다.

25
00:01:55,750 --> 00:01:58,930
다시 말해, 이것은 컴퓨터 비전의 핵심 작업입니다.

26
00:01:58,930 --> 00:02:02,070
우리는 이 작업을 분기 내내 자주 되돌아보게

27
00:02:02,070 --> 00:02:03,840
되는데, 이는 매우 좋은

28
00:02:03,840 --> 00:02:05,440
벤치마크이기 때문입니다.

29
00:02:05,440 --> 00:02:09,030
그리고 알고리즘이 어떻게 작동하는지 설명할

30
00:02:09,030 --> 00:02:11,080
몇 가지 예시가 있습니다.

31
00:02:11,080 --> 00:02:17,100
그래서 이것은 우리가 자주 되돌아보는 항목 중 하나입니다.

32
00:02:17,100 --> 00:02:21,660
오늘 우리는 이미지 분류 작업을 정의하고, 이미지

33
00:02:21,660 --> 00:02:27,930
분류를 위한 두 가지 데이터 기반 접근 방식을 소개하고자

34
00:02:27,930 --> 00:02:30,480
합니다. 하나는 최근접

35
00:02:30,480 --> 00:02:34,240
이웃이고, 다른 하나는 선형 분류기입니다.

36
00:02:34,240 --> 00:02:38,580
우리가 백업 슬라이드에 나열한 다른 접근

37
00:02:38,580 --> 00:02:40,660
방식도 있습니다.

38
00:02:40,660 --> 00:02:45,120
수업 후에 그것들을 살펴보셔도 좋습니다.

39
00:02:45,120 --> 00:02:49,620
하지만 이것이 우리가 다룰 내용입니다.

40
00:02:49,620 --> 00:02:53,130
그렇다면 이미지 분류란 무엇인가요?

41
00:02:53,130 --> 00:03:00,610
이미지와 여러 개의 미리 정의된 레이블, 즉 이 예시에서 보이는

42
00:03:00,610 --> 00:03:04,110
개, 고양이, 트럭, 비행기

43
00:03:04,110 --> 00:03:08,170
등의 가능한 레이블 집합이 주어졌을

44
00:03:08,170 --> 00:03:14,880
때, 시스템의 작업은 이 이미지에 이러한 레이블 중

45
00:03:14,880 --> 00:03:18,000
하나를 할당하는 것입니다.

46
00:03:18,000 --> 00:03:23,280
우리에게는 이것이 매우 쉬운 작업입니다. 왜냐하면

47
00:03:23,280 --> 00:03:26,850
우리의 뇌, 즉 인지 시스템이 이

48
00:03:26,850 --> 00:03:32,640
이미지를 전체적으로 이해하고 레이블을 할당하도록 연결되어

49
00:03:32,640 --> 00:03:34,630
있기 때문입니다.

50
00:03:34,630 --> 00:03:37,950
하지만 이를 코드로 작성하고 컴퓨터가

51
00:03:37,950 --> 00:03:41,320
이 이미지를 어떻게 이해할 수 있는지

52
00:03:41,320 --> 00:03:45,000
살펴보는 것은 완전히 다른 이야기입니다.

53
00:03:45,000 --> 00:03:52,120
우리는 기계가 이러한 데이터를 어떻게 이해할 수 있는지 보고자 합니다.

54
00:03:52,120 --> 00:03:59,930
이미지는 종종 데이터의 행렬, 더 넓게는

55
00:03:59,930 --> 00:04:04,430
데이터의 텐서로 정의됩니다.

56
00:04:04,430 --> 00:04:06,530
그리고 각 픽셀

57
00:04:06,530 --> 00:04:11,130
값은 0에서 255 사이의 숫자이며,

58
00:04:11,130 --> 00:04:15,030
이는 8비트 데이터 구조입니다.

59
00:04:15,030 --> 00:04:22,010
이것이 이미지이므로, 해상도가 800x600이라고

60
00:04:22,010 --> 00:04:25,910
가정할 때, RGB

61
00:04:25,910 --> 00:04:30,080
이미지이므로 빨강,

62
00:04:30,080 --> 00:04:33,000
초록, 파랑의 세

63
00:04:33,000 --> 00:04:37,850
가지 채널이 있으며, 따라서 이는

64
00:04:37,850 --> 00:04:43,100
800x600x3의 텐서입니다.

65
00:04:43,100 --> 00:04:49,850
그래서 아마도 여러분은 이것이 우리의 이미지 인식과 기계가

66
00:04:49,850 --> 00:04:54,610
이미지를 인식하고 보는 방식 간의 의미적

67
00:04:54,610 --> 00:04:59,520
간극이라는 것을 추측할 수 있을 것입니다.

68
00:04:59,520 --> 00:05:04,600
이것이 얼마나 도전적일 수 있는지 이해하기

69
00:05:04,600 --> 00:05:07,820
위해, 이러한 유형의 이미징

70
00:05:07,820 --> 00:05:11,410
데이터에서 몇 가지 도전과

71
00:05:11,410 --> 00:05:14,180
변화를 살펴보겠습니다.

72
00:05:14,180 --> 00:05:16,250
예를 들어,

73
00:05:16,250 --> 00:05:21,070
카메라를 이동한다고 가정해 보겠습니다.

74
00:05:21,070 --> 00:05:23,360
카메라가 이동하면,

75
00:05:23,360 --> 00:05:28,660
예를 들어 카메라를 팬할 경우,

76
00:05:28,660 --> 00:05:33,650
고양이가 완전히 정지해 있어도

77
00:05:33,650 --> 00:05:40,540
800x600x3의 모든 픽셀 값이 변경됩니다.

78
00:05:40,540 --> 00:05:44,650
따라서 모든 픽셀은 새로운 값을 가지게 됩니다.

79
00:05:44,650 --> 00:05:50,450
다시 말해, 우리 인간에게는 동일한 객체입니다.

80
00:05:50,450 --> 00:05:52,600
전혀 차이가 없습니다.

81
00:05:52,600 --> 00:05:54,870
하지만 컴퓨터의 관점에서는

82
00:05:54,870 --> 00:05:57,510
완전히 새로운 데이터 포인트입니다.

83
00:05:57,510 --> 00:06:01,120
이것이 하나의 도전 과제이지만,

84
00:06:01,120 --> 00:06:05,920
다른 도전 과제도 많이 있습니다.

85
00:06:05,920 --> 00:06:12,060
예를 들어, 조명은 또 다른 도전 과제입니다.

86
00:06:12,060 --> 00:06:15,900
그래서 그래픽스나 다른 비전

87
00:06:15,900 --> 00:06:20,220
과정, 또는 공학 응용을 위한 디지털

88
00:06:20,220 --> 00:06:23,910
이미지 처리 과정을 수강한

89
00:06:23,910 --> 00:06:31,110
적이 있다면, 각 RGB 픽셀의 값, RGB 값이 표면

90
00:06:31,110 --> 00:06:37,590
재질, 색상 및 광원에 따라 달라진다는 것을

91
00:06:37,590 --> 00:06:39,870
알고 있을 것입니다.

92
00:06:39,870 --> 00:06:46,350
그래서 같은 고양이, 같은 물체라도 서로 다른

93
00:06:46,350 --> 00:06:48,840
조명 조건에서 사진이

94
00:06:48,840 --> 00:06:55,020
찍힐 때 숫자적으로 다르게 보일 수 있습니다.

95
00:06:55,020 --> 00:07:00,180
그 점을 염두에 두고, 고양이가 어두운 방에 있든 햇볕 아래에

96
00:07:00,180 --> 00:07:02,500
있든 여전히 고양이입니다.

97
00:07:02,500 --> 00:07:03,910
하나의 고양이입니다.

98
00:07:03,910 --> 00:07:08,440
하지만 이것은 기계에 도전 과제를 만들어냅니다.

99
00:07:08,440 --> 00:07:11,490
조명과 관점 변화 외에

100
00:07:11,490 --> 00:07:16,890
픽셀 값을 변경하고 기계가 물체를 인식하는

101
00:07:16,890 --> 00:07:22,440
데 문제를 일으킬 수 있는 다른 도전 과제를

102
00:07:22,440 --> 00:07:24,570
몇 가지 말해줄

103
00:07:24,570 --> 00:07:26,400
수 있나요?

104
00:07:26,400 --> 00:07:30,990
배경 혼잡, 배경 물체, 네, 그것이 실제로 우리의

105
00:07:30,990 --> 00:07:33,120
다음 슬라이드입니다.

106
00:07:33,120 --> 00:07:38,100
네, 배경 혼잡은 또 다른 도전 과제입니다.

107
00:07:38,100 --> 00:07:40,590
다른 것은 없나요?

108
00:07:40,590 --> 00:07:44,430
확대 및 축소, 네, 즉 이미지에서

109
00:07:44,430 --> 00:07:46,630
물체의 크기입니다.

110
00:07:46,630 --> 00:07:49,070
네, 또 다른 것은 무엇인가요?

111
00:07:49,070 --> 00:07:52,100
이미지의 해상도는-- 그것은

112
00:07:52,100 --> 00:07:54,810
도전 과제로 간주될

113
00:07:54,810 --> 00:07:56,700
수 있습니다.

114
00:07:56,700 --> 00:08:01,100
하지만 종종 기계 학습 모델이나 행동 물체와

115
00:08:01,100 --> 00:08:05,580
이미지를 인식하고자 하는 모델에서는 이미지의

116
00:08:05,580 --> 00:08:09,210
크기를 정규화하기 때문에 해상도는

117
00:08:09,210 --> 00:08:11,720
그리 중요하지 않을 수

118
00:08:11,720 --> 00:08:15,230
있습니다, 물체의 확대 효과가 없는 한.

119
00:08:15,230 --> 00:08:18,120
가림은 주요 문제 중 하나입니다.

120
00:08:18,120 --> 00:08:21,960
다시 말해, 인간으로서 이것이 고양이라는 것을 말하는 것은 매우 쉽습니다.

121
00:08:21,960 --> 00:08:23,430
이것들은 고양이들입니다.

122
00:08:23,430 --> 00:08:25,895
마지막 것, 실제로 매우 도전적인

123
00:08:25,895 --> 00:08:28,050
것, 오른쪽에 있는 것입니다.

124
00:08:28,050 --> 00:08:32,179
오른쪽에서 꼬리와 아마도 발의 일부만

125
00:08:32,179 --> 00:08:34,239
볼 수 있습니다.

126
00:08:37,280 --> 00:08:41,690
누군가는 말할 수 있습니다, 네, 그것은 호랑이일 수도 있고,

127
00:08:41,690 --> 00:08:46,070
아니면-- 잘 모르겠지만-- 작은 꼬리를 가진 너구리일 수도 있습니다.

128
00:08:46,070 --> 00:08:48,910
하지만 이것은-- 맥락 때문에, 이것이 거실의

129
00:08:48,910 --> 00:08:52,460
소파 위에 있다는 것을 알고 있기 때문에, 대부분

130
00:08:52,460 --> 00:08:53,890
아마도 고양이입니다.

131
00:08:53,890 --> 00:08:54,740
고양이입니다.

132
00:08:54,740 --> 00:08:59,170
그래서 다시 말해, 우리 인간에게는 그렇게 어렵지 않습니다.

133
00:08:59,170 --> 00:09:02,720
그 이상으로 많은 다른 문제가 있습니다.

134
00:09:02,720 --> 00:09:03,310
변형.

135
00:09:03,310 --> 00:09:05,930
고양이는 매우 변형 가능성이 높습니다.

136
00:09:05,930 --> 00:09:16,300
그래서 알고리즘이 탐지되고 인식되는 데 도전 과제를

137
00:09:16,300 --> 00:09:17,890
만듭니다.

138
00:09:17,890 --> 00:09:19,750
오늘날의 알고리즘이 아니라.

139
00:09:19,750 --> 00:09:24,910
일반적으로 물체를 탐지할 수 있는 단계별 알고리즘을

140
00:09:24,910 --> 00:09:26,480
구축하는 데.

141
00:09:26,480 --> 00:09:31,240
그래서 변형은 또 다른 주요 도전 과제 중 하나입니다.

142
00:09:31,240 --> 00:09:37,000
그 이상으로, 클래스 내 변이는

143
00:09:37,000 --> 00:09:44,220
또 하나의 중요한 도전 과제입니다.

144
00:09:44,220 --> 00:09:48,340
고양이가 다양한 크기, 색상, 패턴으로 올 수 있다는 것을 알고 있습니다,

145
00:09:48,340 --> 00:09:51,310
또는 심지어 다양한 품종을 가질 수 있습니다.

146
00:09:51,310 --> 00:09:55,090
그리고 그 모든 것들은 여전히 고양이입니다.

147
00:09:55,090 --> 00:09:59,340
하지만 기계에게는 클래스 내 변이를

148
00:09:59,340 --> 00:10:04,530
인식하는 것이 그렇게 쉽지 않습니다.

149
00:10:04,530 --> 00:10:09,540
또 다른 흥미로운 도전 과제는 맥락입니다.

150
00:10:09,540 --> 00:10:17,260
만약 당신이 오른쪽의 그 부분만 본다면, 또는 알고리즘이

151
00:10:17,260 --> 00:10:23,170
맥락을 고려하지 않고 이것을 본다면, 이것을 호랑이나

152
00:10:23,170 --> 00:10:28,270
다른 동물로 분류하기가 매우 쉽습니다.

153
00:10:28,270 --> 00:10:31,110
하지만 맥락과 그림자

154
00:10:31,110 --> 00:10:34,500
효과를 알고 있기 때문에,

155
00:10:34,500 --> 00:10:39,876
이것은 아마도 올바르게 분류될 수 있습니다.

156
00:10:45,717 --> 00:10:49,200
하지만 오늘날 우리가 가진

157
00:10:49,200 --> 00:10:52,020
분류기는 이미지 분류와

158
00:10:52,020 --> 00:10:57,400
이미지 내 객체 식별에서 정말 훌륭한 작업을

159
00:10:57,400 --> 00:11:00,760
수행할 수 있습니다.

160
00:11:00,760 --> 00:11:04,920
이는 ImageNet과 대규모

161
00:11:04,920 --> 00:11:10,950
모델 훈련을 위한 대규모 벤치마크를 만든 후속 작업

162
00:11:10,950 --> 00:11:12,640
덕분입니다.

163
00:11:12,640 --> 00:11:22,050
이 수업에서 우리가 하고 싶은 것은 활동을 인식하고, 객체를 인식하며,

164
00:11:22,050 --> 00:11:27,600
이미지 내 다른 측면을 인식할 수

165
00:11:27,600 --> 00:11:32,200
있는 모델을 구축하는 것입니다.

166
00:11:32,200 --> 00:11:34,770
이 수업의 나머지 시간 동안 우리는

167
00:11:34,770 --> 00:11:38,460
이러한 대규모 알고리즘을 구축하는 데 필요한

168
00:11:38,460 --> 00:11:40,460
기본 구성 요소를 단계별로

169
00:11:40,460 --> 00:11:42,750
구축하는 작업을 할 것입니다.

170
00:11:42,750 --> 00:11:51,200
그에 앞서, 이미지를 분류하는 가장 기본적인 구성 요소를

171
00:11:51,200 --> 00:11:58,260
살펴봐야 하며, 그것은 이러한 함수를 구축하고

172
00:11:58,260 --> 00:12:01,110
구현하는 것입니다.

173
00:12:01,110 --> 00:12:06,020
그래서 알고리즘을 통해 프레임워크를 구축하는

174
00:12:06,020 --> 00:12:11,000
컴퓨터 과학 또는 공학 과정을 수강한 적이

175
00:12:11,000 --> 00:12:16,130
있다면, 예를 들어 정렬과 같은 컴퓨터 알고리즘은

176
00:12:16,130 --> 00:12:18,020
종종

177
00:12:18,020 --> 00:12:21,200
if-then-else 규칙과 for

178
00:12:21,200 --> 00:12:24,000
루프 등을 포함합니다.

179
00:12:24,000 --> 00:12:31,770
따라서 정렬 알고리즘을 만드는 작업과 단계, if-then-else

180
00:12:31,770 --> 00:12:35,670
단계의 명확한 흐름도가 있습니다.

181
00:12:35,670 --> 00:12:39,160
하지만 이미지와 시각적 세계를 이해하는

182
00:12:39,160 --> 00:12:45,230
데 있어서는 그런 일이 발생하지 않으며, 이는 도전 과제입니다.

183
00:12:45,230 --> 00:12:50,660
이미지를 분류하기 위한 단계를 하드코딩할 방법이 없습니다.

184
00:12:50,660 --> 00:12:56,000
이 분야에서 몇 가지 노력이 있었지만.

185
00:12:56,000 --> 00:13:04,180
객체를 인식하기 위한 알고리즘과 단계를 제안한

186
00:13:04,180 --> 00:13:06,590
논문들이 있습니다.

187
00:13:06,590 --> 00:13:13,510
그 중 하나는 가장자리를 감지하는 데 기반을 두었으며,

188
00:13:13,510 --> 00:13:19,130
이미지를 첫 번째 단계로 가장자리를 찾고,

189
00:13:19,130 --> 00:13:23,930
이러한 패턴을 생성한 후, 중요한

190
00:13:23,930 --> 00:13:29,480
패턴, 예를 들어 모서리를 살펴보고, 모서리

191
00:13:29,480 --> 00:13:32,630
주변의 일부 특징을

192
00:13:32,630 --> 00:13:36,360
추출하거나 특정 유형의 모서리

193
00:13:36,360 --> 00:13:44,110
수를 세고, 이를 바탕으로 출력 클래스에 매핑하려고 했습니다.

194
00:13:44,110 --> 00:13:48,840
이러한 노력은 흥미로웠고 매우 제한된

195
00:13:48,840 --> 00:13:55,590
변동성 유형의 이미지에서 일부 성공을 거두었지만, 이러한

196
00:13:55,590 --> 00:13:59,200
유형의 알고리즘을 확장하는

197
00:13:59,200 --> 00:14:01,900
것은 매우 어렵습니다.

198
00:14:01,900 --> 00:14:06,660
작동하더라도, 인식하려는 각 객체에 대해 이러한

199
00:14:06,660 --> 00:14:09,300
규칙과 모든 것을 만들어야

200
00:14:09,300 --> 00:14:11,190
하므로 확장하기가

201
00:14:11,190 --> 00:14:12,670
매우 어렵습니다.

202
00:14:12,670 --> 00:14:17,100
둘째, 각 객체에 대한 논리를 찾는

203
00:14:17,100 --> 00:14:20,920
것도 많은 노력이 필요합니다.

204
00:14:20,920 --> 00:14:23,040
이러한 도전 과제로

205
00:14:23,040 --> 00:14:27,210
인해, 객체를 감지하거나 이미지를

206
00:14:27,210 --> 00:14:34,350
분류하기 위한 논리와 절차를 만드는 기반의 알고리즘은 그리

207
00:14:34,350 --> 00:14:38,380
성공적이지 않았다고 생각합니다.

208
00:14:38,380 --> 00:14:42,850
그리고 머신러닝은 데이터 기반 접근 방식을 제공합니다.

209
00:14:42,850 --> 00:14:46,320
따라서 데이터 기반 관점에서

210
00:14:46,320 --> 00:14:50,220
이 문제를 바라보는 새로운

211
00:14:50,220 --> 00:14:56,070
패러다임과 또 다른 패러다임을 통해, 우리는

212
00:14:56,070 --> 00:15:00,070
3단계 프로세스를 정의합니다.

213
00:15:00,070 --> 00:15:06,420
첫 번째 단계는 이미지와 해당 레이블의 데이터

214
00:15:06,420 --> 00:15:09,400
세트를 수집하는 것입니다.

215
00:15:09,400 --> 00:15:11,850
특정 유형의 객체나

216
00:15:11,850 --> 00:15:17,040
특정 유형의 객체를 인식하고 싶다면, 우리는

217
00:15:17,040 --> 00:15:20,550
데이터 세트나 인터넷에서

218
00:15:20,550 --> 00:15:26,130
단일 데이터 포인트를 찾아 데이터를 생성할 수

219
00:15:26,130 --> 00:15:32,040
있습니다. 각 예제에서 많은 샘플을 수집합니다.

220
00:15:32,040 --> 00:15:35,340
10, 20년 전에는 인터넷에서

221
00:15:35,340 --> 00:15:41,270
검색 엔진과 이미지 검색 엔진을 사용하여 이러한 데이터

222
00:15:41,270 --> 00:15:44,220
세트를 만들곤 했습니다.

223
00:15:44,220 --> 00:15:47,902
이제 우리는 모든 데이터 세트를 가지고 있습니다.

224
00:15:47,902 --> 00:15:51,350
그리고 두 번째 단계는 머신러닝 알고리즘을 사용하여

225
00:15:51,350 --> 00:15:53,610
분류기를 훈련하는 것입니다.

226
00:15:53,610 --> 00:15:57,680
기본적으로, 훈련 데이터의 이미지를

227
00:15:57,680 --> 00:16:01,730
가져오고 그에 연결된 레이블을

228
00:16:01,730 --> 00:16:04,500
사용하여 이미지를 레이블과

229
00:16:04,500 --> 00:16:08,240
연관시키는 모델을 구축하는 함수

230
00:16:08,240 --> 00:16:10,170
훈련을 만듭니다.

231
00:16:10,170 --> 00:16:15,050
마지막 단계는 새로운 이미지에서 분류기를

232
00:16:15,050 --> 00:16:20,570
평가하는 것입니다. 이는 모델과 일부 테스트

233
00:16:20,570 --> 00:16:23,630
이미지를 가져오는 predict라는

234
00:16:23,630 --> 00:16:28,830
함수를 구현하는 것을 의미하며, 훈련

235
00:16:28,830 --> 00:16:32,470
이미지의 일부가 아닌 테스트

236
00:16:32,470 --> 00:16:39,080
이미지에 대해 레이블을 예측하고 이를 출력으로 반환합니다.

237
00:16:39,080 --> 00:16:42,110
매우 간단한 절차입니다.

238
00:16:42,110 --> 00:16:46,300
하지만 논리를 구축하는 대신, 우리는 데이터

239
00:16:46,300 --> 00:16:50,110
기반 접근 방식을 구축하고 있습니다.

240
00:16:50,110 --> 00:16:55,390
앞서 말했듯이, 우리는 두 가지 인기 있는 방법과 분류기에 대해

241
00:16:55,390 --> 00:16:56,570
이야기하고 싶습니다.

242
00:16:56,570 --> 00:16:59,080
그 중 하나는 최근접 이웃 분류기입니다.

243
00:16:59,080 --> 00:17:03,220
이것은 가장 쉬운 형태의 분류이며,

244
00:17:03,220 --> 00:17:09,579
우리는 이러한 분류기를 구축하는 개념을 배울

245
00:17:09,579 --> 00:17:15,099
수 있기 때문에 특히 이 부분을 다루고

246
00:17:15,099 --> 00:17:16,160
싶습니다.

247
00:17:16,160 --> 00:17:22,190
그리고 일부 세부 사항을 설명하는 것이 더 쉽습니다.

248
00:17:22,190 --> 00:17:26,089
그런 다음 선형 분류 주제로 넘어가겠습니다.

249
00:17:26,089 --> 00:17:30,870
가장 가까운 이웃 분류기를 구축하기 위해 우리가

250
00:17:30,870 --> 00:17:31,990
하는 일입니다.

251
00:17:31,990 --> 00:17:37,720
말씀드린 대로, 우리는 학습 및 예측 함수를 구축해야 합니다.

252
00:17:37,720 --> 00:17:40,890
학습 함수는 모든 데이터와

253
00:17:40,890 --> 00:17:43,270
레이블을 기억해야 합니다.

254
00:17:43,270 --> 00:17:44,940
따라서 학습 함수는 기본적으로

255
00:17:44,940 --> 00:17:47,070
모든 것을 메모리에 유지하는 것 외에는

256
00:17:47,070 --> 00:17:48,130
아무것도 하지 않습니다.

257
00:17:48,130 --> 00:17:50,320
그리고 예측

258
00:17:50,320 --> 00:17:56,680
함수는 가장 유사한 학습 이미지를 찾습니다.

259
00:17:56,680 --> 00:18:00,300
기본적으로 모든 이미지와 그 레이블의 조회

260
00:18:00,300 --> 00:18:02,140
테이블을 생성합니다.

261
00:18:02,140 --> 00:18:08,070
예측 또는 테스트 시간 동안, 가장 가까운

262
00:18:08,070 --> 00:18:12,910
것, 가장 유사한 이미지를 찾고

263
00:18:12,910 --> 00:18:17,620
해당 이미지의 레이블을 출력합니다.

264
00:18:17,620 --> 00:18:19,060
예제를 살펴보겠습니다.

265
00:18:19,060 --> 00:18:26,430
이 다섯 개가 우리의 학습 데이터라고 가정하면,

266
00:18:26,430 --> 00:18:27,420
네, 내

267
00:18:27,420 --> 00:18:29,830
커서를 보세요.

268
00:18:29,830 --> 00:18:33,280
그리고 이것이 쿼리 이미지, 예측을

269
00:18:33,280 --> 00:18:35,830
위한 입력 이미지입니다.

270
00:18:35,830 --> 00:18:39,540
우리가 하고 싶은 것은 이 학습 데이터와 학습 이미지

271
00:18:39,540 --> 00:18:43,480
중에서 이 이미지와 가장 유사한 것을 찾는 것입니다.

272
00:18:43,480 --> 00:18:46,210
그것을 위해 우리는 거리 함수가 필요합니다.

273
00:18:46,210 --> 00:18:49,560
이 거리 함수는 두 이미지를

274
00:18:49,560 --> 00:18:53,280
가져와서, 각 이미지 쌍을

275
00:18:53,280 --> 00:18:58,950
쿼리 이미지와 비교하고, 이 두 입력, 이

276
00:18:58,950 --> 00:19:05,370
두 이미지 간의 유사성을 정의하는 값을 반환해야

277
00:19:05,370 --> 00:19:06,600
합니다.

278
00:19:06,600 --> 00:19:09,790
이를 수행하는 방법은 여러 가지가 있습니다.

279
00:19:09,790 --> 00:19:14,560
가장 인기 있는 방법 중 하나는 L1

280
00:19:14,560 --> 00:19:19,680
거리로, 두 이미지 간의 픽셀 차이의

281
00:19:19,680 --> 00:19:25,980
절대값 합으로 정의됩니다, 이미지 I1과 I2.

282
00:19:25,980 --> 00:19:28,940
예를 들어, 이것이

283
00:19:28,940 --> 00:19:34,460
테스트 이미지라면, 이 이미지를 학습

284
00:19:34,460 --> 00:19:38,450
데이터의 이미지와 비교하여 거리를

285
00:19:38,450 --> 00:19:43,580
계산하려면, 픽셀 단위로 뺄셈을 하고

286
00:19:43,580 --> 00:19:46,100
픽셀 값의 차이를

287
00:19:46,100 --> 00:19:54,470
합산하여 이 두 이미지 간의 거리로 정의되는 이 새로운 값을

288
00:19:54,470 --> 00:19:55,860
얻습니다.

289
00:19:55,860 --> 00:20:01,710
이것은 가장 기본적인 거리 함수이지만, 실제로 많은

290
00:20:01,710 --> 00:20:06,030
응용 프로그램에서 매우 유용합니다.

291
00:20:06,030 --> 00:20:10,310
우리는 수업에서 이 L1 및 다른 거리 변형에

292
00:20:10,310 --> 00:20:13,280
대해 자주 돌아올 것입니다.

293
00:20:13,280 --> 00:20:15,080
이 매우 간단한 정의로, 우리가 어떻게

294
00:20:15,080 --> 00:20:16,530
구현할 수 있는지 보고자 합니다.

295
00:20:16,530 --> 00:20:17,940
구현해 보겠습니다.

296
00:20:17,940 --> 00:20:22,860
말씀드린 대로, 첫 번째 단계는 학습 데이터를 기억하는 것입니다.

297
00:20:22,860 --> 00:20:28,390
따라서 학습 함수는 데이터를 메모리에 유지합니다.

298
00:20:28,390 --> 00:20:32,920
그리고 예측 함수는 실제로 일부

299
00:20:32,920 --> 00:20:39,380
Python 라이브러리와 NumPy 등을 사용하여 단 4줄로

300
00:20:39,380 --> 00:20:42,080
구현할 수 있습니다.

301
00:20:42,080 --> 00:20:46,720
각 테스트 샘플과 학습 데이터

302
00:20:46,720 --> 00:20:54,760
간의 거리를 계산하고, 각 테스트 샘플에 대해

303
00:20:54,760 --> 00:20:59,530
최소값을 취한 다음, 평균

304
00:20:59,530 --> 00:21:05,000
인덱스에 해당하는 레이블을 출력합니다.

305
00:21:05,000 --> 00:21:10,090
따라서 이것이 예측 함수의 구현이

306
00:21:10,090 --> 00:21:12,370
될 것입니다.

307
00:21:12,370 --> 00:21:21,160
네, 픽셀 값은 제가 설명한 대로 가장 간단한 형태로, 이것은 800 x

308
00:21:21,160 --> 00:21:28,120
600 x 3의 텐서이며, 세 개의 채널이 있고, 각

309
00:21:28,120 --> 00:21:32,200
픽셀 위치에 대한 RGB 값입니다.

310
00:21:32,200 --> 00:21:35,850
그래서 네, 실제로 온라인 학생들을 위해

311
00:21:35,850 --> 00:21:37,630
질문을 반복해야 합니다.

312
00:21:37,630 --> 00:21:41,520
질문은 픽셀 값이 무엇을 나타내는가였습니다.

313
00:21:41,520 --> 00:21:46,290
네, 다음 질문은 왜 0과 255 사이인지입니다.

314
00:21:46,290 --> 00:21:52,320
이미지를 저장하는 다양한 표준이 있습니다.

315
00:21:52,320 --> 00:21:57,690
우리가 온라인에서 보는 거의 모든 이미지에서

316
00:21:57,690 --> 00:22:01,920
사용하는 가장 인기 있는 것은 RGB입니다.

317
00:22:01,920 --> 00:22:05,580
RGB는 24비트 형식이며, 때때로 알파 채널이

318
00:22:05,580 --> 00:22:08,070
있어 32비트가 될 수 있습니다.

319
00:22:08,070 --> 00:22:10,540
그 부분에 대해서는 깊이 들어가고 싶지 않습니다.

320
00:22:10,540 --> 00:22:16,000
하지만 24비트 형식은 빨강, 초록, 파랑의

321
00:22:16,000 --> 00:22:19,060
세 채널 각각에

322
00:22:19,060 --> 00:22:22,590
대해 8비트를 가질 수 있다는

323
00:22:22,590 --> 00:22:24,340
의미입니다.

324
00:22:24,340 --> 00:22:27,940
그래서 이것이 정의된 표준입니다.

325
00:22:27,940 --> 00:22:30,220
다른 프레임워크도 있지만,

326
00:22:30,220 --> 00:22:33,900
이것이 가장 인기 있는 것입니다.

327
00:22:33,900 --> 00:22:40,820
그럼 이제 코드로 돌아가서 질문을 드리겠습니다.

328
00:22:46,080 --> 00:22:53,010
저는 일부 학생들이 대부분 공학 배경과 약간의 컴퓨터 과학

329
00:22:53,010 --> 00:22:55,680
지식을 가지고 있다는

330
00:22:55,680 --> 00:23:01,200
것을 알고 있지만, 훈련 데이터에 있는 n개의

331
00:23:01,200 --> 00:23:04,380
샘플로 훈련과 예측이 얼마나

332
00:23:04,380 --> 00:23:08,333
빠르게 이루어지는지 보고 싶습니다.

333
00:23:11,550 --> 00:23:15,420
여러분이 종종 계산 복잡성과

334
00:23:15,420 --> 00:23:21,170
때때로 공간 복잡성을 나타내는 빅 오 표기법에

335
00:23:21,170 --> 00:23:23,310
익숙하길 바랍니다.

336
00:23:23,310 --> 00:23:27,260
하지만 여기서 알고리즘을 살펴보면, 저는 훈련

337
00:23:27,260 --> 00:23:28,850
데이터로 진행하겠습니다.

338
00:23:28,850 --> 00:23:31,790
훈련 함수에서-- 그리고

339
00:23:31,790 --> 00:23:36,890
예측에 대한 답변을 도와주셨으면 합니다.

340
00:23:36,890 --> 00:23:40,730
훈련 단계에서는 실제로 아무것도

341
00:23:40,730 --> 00:23:44,550
하지 않기 때문에 훈련은 O1입니다.

342
00:23:44,550 --> 00:23:46,770
데이터를 이동조차 하지 않습니다.

343
00:23:46,770 --> 00:23:50,010
우리는 단지 메모리에 데이터의 복사본을 유지하고 있습니다.

344
00:23:50,010 --> 00:23:53,340
따라서 연산이 없다는 것은

345
00:23:53,340 --> 00:23:56,600
연산이 1차의 순서로 훈련

346
00:23:56,600 --> 00:24:00,780
단계를 완료할 수 있다는 의미입니다.

347
00:24:00,780 --> 00:24:03,650
훈련 데이터의

348
00:24:03,650 --> 00:24:12,570
각 단일 예제에 대한 예측 단계는 어떻습니까?

349
00:24:12,570 --> 00:24:15,590
몇 개의 연산이 필요할까요?

350
00:24:15,590 --> 00:24:17,370
N, 맞습니다.

351
00:24:17,370 --> 00:24:19,990
n개의 훈련 데이터가

352
00:24:19,990 --> 00:24:23,590
있다면, 모든 훈련 이미지와

353
00:24:23,590 --> 00:24:26,960
각 테스트 이미지 간의 거리를

354
00:24:26,960 --> 00:24:33,490
계산해야 하므로 최소한 n개의 연산이 필요합니다.

355
00:24:33,490 --> 00:24:43,480
그래서 이것은 좋지 않습니다. 왜냐하면 우리가 종종 하고 싶어하는 것은
훈련이 아무것도

356
00:24:43,480 --> 00:24:45,890
하지 않기 때문입니다.

357
00:24:45,890 --> 00:24:49,190
하지만 테스트 중, 예측 시간

358
00:24:49,190 --> 00:24:51,700
동안 우리는 데이터와 단일

359
00:24:51,700 --> 00:24:54,220
데이터 포인트 및 훈련

360
00:24:54,220 --> 00:25:00,010
예제 간의 비교를 위해 너무 많은 시간을 소비하고 있습니다.

361
00:25:00,010 --> 00:25:03,880
이것은 여러분이 ChatGPT에게

362
00:25:03,880 --> 00:25:07,840
질문할 때마다 매번 답변을 찾고

363
00:25:07,840 --> 00:25:11,650
인터넷의 모든 가능한 답변과 비교하는

364
00:25:11,650 --> 00:25:15,510
것과 비슷하며, 이는 수년이 걸려서

365
00:25:15,510 --> 00:25:19,270
여러분의 응답을 반환할 것입니다.

366
00:25:19,270 --> 00:25:22,710
그래서 아주 간단한 문제에 대해 확장하려고 할 때는

367
00:25:22,710 --> 00:25:24,520
작동하지 않을 것입니다.

368
00:25:24,520 --> 00:25:26,850
우리는 이러한 유형의 접근 방식을 사용하곤 했습니다.

369
00:25:26,850 --> 00:25:34,920
그래서 우리가 종종 원하는 것은 예측 중에 빠른 분류기를

370
00:25:34,920 --> 00:25:37,090
만드는 것입니다.

371
00:25:37,090 --> 00:25:40,860
그들은 훨씬 더 빠르게 수행하지만, 훈련 중에 많은 시간을 소요해도

372
00:25:40,860 --> 00:25:42,960
괜찮습니다. 왜냐하면 그것은

373
00:25:42,960 --> 00:25:44,890
오프라인에서 수행될 수 있기 때문입니다.

374
00:25:44,890 --> 00:25:49,350
그 점을 염두에 두고, GPU 등을 사용하여

375
00:25:49,350 --> 00:25:55,530
최근접 이웃을 훨씬 더 빠르게 만드는 많은 노력이 있었지만,

376
00:25:55,530 --> 00:25:59,320
이는 이 수업의 범위를 넘어서는

377
00:25:59,320 --> 00:26:01,545
것이므로 관심이 있다면

378
00:26:01,545 --> 00:26:03,670
살펴보실 수 있습니다.

379
00:26:03,670 --> 00:26:08,460
하지만 그와 함께, 저는 몇 가지 시각화와 이 알고리즘이 일반적으로

380
00:26:08,460 --> 00:26:11,580
어떻게 작동하는지 살펴보고 싶습니다.

381
00:26:11,580 --> 00:26:15,630
이 공간에서 빨강, 파랑, 초록,

382
00:26:15,630 --> 00:26:19,050
보라, 노랑의 다섯 가지

383
00:26:19,050 --> 00:26:26,880
클래스가 주어지면, 각 점은 해당 클래스의 하나의 훈련 샘플을

384
00:26:26,880 --> 00:26:32,880
나타냅니다. 각 점에 대해 공간을 분할하면,

385
00:26:32,880 --> 00:26:37,440
이 다섯 개의 분할, 즉 이 경우

386
00:26:37,440 --> 00:26:41,580
여섯 개의 서로 다른 분할을 만들

387
00:26:41,580 --> 00:26:48,210
수 있습니다. 특정 영역에 있는 테스트 샘플이 있다면,

388
00:26:48,210 --> 00:26:51,780
그 영역의 색상이 해당

389
00:26:51,780 --> 00:26:57,010
샘플의 최근접 이웃이 무엇인지 보여줍니다.

390
00:26:57,010 --> 00:27:02,670
그래서 이것은 최근접 이웃 알고리즘으로, 하나의 최근접

391
00:27:02,670 --> 00:27:04,530
이웃 알고리즘이 이

392
00:27:04,530 --> 00:27:07,330
설정에서 공간을 분할합니다.

393
00:27:07,330 --> 00:27:12,500
하지만 이 예제에서 문제가 보이나요?

394
00:27:12,500 --> 00:27:14,750
노란색 점은 모든 초록색 점의

395
00:27:14,750 --> 00:27:17,220
중앙에 정확히 위치해 있습니다.

396
00:27:17,220 --> 00:27:20,370
이는 아마도 이상치라는 의미입니다.

397
00:27:20,370 --> 00:27:21,750
아마도 노이즈일 것입니다.

398
00:27:21,750 --> 00:27:24,200
이는 우리가 해결해야 할 많은

399
00:27:24,200 --> 00:27:25,500
문제들에 해당합니다.

400
00:27:25,500 --> 00:27:32,300
그리고 중앙에 큰 노란색 영역이 있는

401
00:27:32,300 --> 00:27:35,090
이유는 바로 이

402
00:27:35,090 --> 00:27:38,010
단일 점 때문입니다.

403
00:27:38,010 --> 00:27:41,880
우리가 단 하나의 최근접 이웃만 사용하기 때문에 이런 일이

404
00:27:41,880 --> 00:27:42,600
발생합니다.

405
00:27:42,600 --> 00:27:44,760
조금 더 견고하게 만들기 위해,

406
00:27:44,760 --> 00:27:48,450
우리가 취하는 최근접 이웃의 수를 늘릴 수 있으며,

407
00:27:48,450 --> 00:27:50,750
이는 최근접 이웃 알고리즘을

408
00:27:50,750 --> 00:27:52,650
k-최근접 이웃으로 바꿉니다.

409
00:27:52,650 --> 00:28:00,810
우리는 종종 하나 이상의 점이나 샘플을 선택합니다.

410
00:28:00,810 --> 00:28:04,370
우리는 종종 주어진 테스트

411
00:28:04,370 --> 00:28:10,450
샘플, 테스트 이미지의 레이블을 식별하기 위해

412
00:28:10,450 --> 00:28:13,750
다수결 투표를 사용합니다.

413
00:28:13,750 --> 00:28:16,840
하지만 여기서 볼 수 있는 문제는

414
00:28:16,840 --> 00:28:19,970
이제 흰색 영역이 있다는 것입니다.

415
00:28:19,970 --> 00:28:23,480
그 흰색 영역은 세 가지

416
00:28:23,480 --> 00:28:28,150
다른 클래스의 이웃으로부터 샘플

417
00:28:28,150 --> 00:28:34,240
수가 동일하기 때문에 완전한 결정을 내릴 수

418
00:28:34,240 --> 00:28:36,350
없는 영역입니다.

419
00:28:36,350 --> 00:28:39,130
그리고 그 흰색 영역의

420
00:28:39,130 --> 00:28:43,970
예제 레이블을 식별할 방법이 없습니다.

421
00:28:43,970 --> 00:28:47,080
당신이 이러한 유형의 공간을 문제에

422
00:28:47,080 --> 00:28:49,750
대해 만든다면, 이는 이러한

423
00:28:49,750 --> 00:28:52,730
공간을 보면 더 많은 데이터를

424
00:28:52,730 --> 00:28:55,540
수집하기에 좋은 영역이라는 것을

425
00:28:55,540 --> 00:28:56,600
의미합니다.

426
00:28:56,600 --> 00:28:59,300
그래서 그것들은 불확실한 공간입니다.

427
00:28:59,300 --> 00:29:01,840
따라서 데이터 수집에

428
00:29:01,840 --> 00:29:05,380
중요한 영역을 찾는 좋은

429
00:29:05,380 --> 00:29:06,650
방법입니다.

430
00:29:06,650 --> 00:29:10,870
그래서 k의 값을 더 크게 설정할 수 있습니다.

431
00:29:10,870 --> 00:29:14,690
하지만 우리가 가진 선택 중 하나는, 하나의.

432
00:29:17,940 --> 00:29:22,270
중요한 역할을 하는 요소는 k의 값입니다.

433
00:29:22,270 --> 00:29:25,290
하지만 기억하신다면, 우리는 거리 함수라는

434
00:29:25,290 --> 00:29:27,960
또 다른 결정을 내려야 했습니다.

435
00:29:27,960 --> 00:29:31,290
우리는 L1 거리, 즉

436
00:29:31,290 --> 00:29:37,320
픽셀 간의 쌍별 차이의 절대값의 합에

437
00:29:37,320 --> 00:29:41,130
대해 다시 이야기했습니다.

438
00:29:41,130 --> 00:29:47,640
L1 거리를 시각화하면, 때때로 우리는

439
00:29:47,640 --> 00:29:52,440
이를 맨해튼 거리라고 부르며,

440
00:29:52,440 --> 00:29:57,090
거리 함수는 이렇게 시각화됩니다.

441
00:29:59,620 --> 00:30:05,860
이 공간에 있는 이 정사각형을 보면, 그

442
00:30:05,860 --> 00:30:10,815
정사각형의 모든 점은 원점, 즉 중심점에서

443
00:30:10,815 --> 00:30:13,620
같은 거리를 가지고

444
00:30:13,620 --> 00:30:15,400
있습니다.

445
00:30:15,400 --> 00:30:19,560
그래서 이것은 L1 거리 함수가 어떻게 작동하는지를

446
00:30:19,560 --> 00:30:22,930
시각화하고 보는 좋은 방법입니다.

447
00:30:22,930 --> 00:30:26,280
우리가 사용하는 또 다른

448
00:30:26,280 --> 00:30:29,040
인기 있는 거리 함수는

449
00:30:29,040 --> 00:30:34,050
L2로, 절대값 대신 차이의 제곱을

450
00:30:34,050 --> 00:30:36,910
계산하고 이를 합산합니다.

451
00:30:36,910 --> 00:30:39,900
하지만 제곱 때문에 우리는 제곱근도 취합니다.

452
00:30:39,900 --> 00:30:49,320
이를 시각화하면, 각 점이 중심, 즉 원점에서 같은

453
00:30:49,320 --> 00:30:52,770
거리를 가지는 원

454
00:30:52,770 --> 00:30:56,590
시각화를 얻게 됩니다.

455
00:30:56,590 --> 00:30:58,530
따라서 이 시각화는 이러한

456
00:30:58,530 --> 00:31:01,750
거리 간의 차이를 이해하는 데 도움이 됩니다.

457
00:31:01,750 --> 00:31:04,590
이것들은 우리가 사용할 수 있는 가장 기본적이고 쉬운

458
00:31:04,590 --> 00:31:05,370
거리 함수입니다.

459
00:31:05,370 --> 00:31:08,250
그래서 다시 말해, 훨씬 더 많은 것들이 있습니다.

460
00:31:08,250 --> 00:31:12,710
이 시각화가 유용한 이유는 때때로 x와

461
00:31:12,710 --> 00:31:16,430
y를 회전시키면, 이 두 시각화에서

462
00:31:16,430 --> 00:31:18,410
x와 y는 기본적으로

463
00:31:18,410 --> 00:31:20,000
특징입니다.

464
00:31:20,000 --> 00:31:23,280
두 개의 픽셀 값, 두 개의 특징이 있다면, 우리는

465
00:31:23,280 --> 00:31:25,140
이 2D 공간을 갖습니다.

466
00:31:25,140 --> 00:31:29,450
그리고 이 x와 y는 종종 그 특징입니다.

467
00:31:29,450 --> 00:31:33,830
따라서 이러한 특징을 회전시키면, 즉 다른 유형의

468
00:31:33,830 --> 00:31:36,210
특징을 사용하면, 이 L1은

469
00:31:36,210 --> 00:31:40,140
다른 프레임워크, 다른 값을 가지게 되지만

470
00:31:40,140 --> 00:31:42,330
L2는 다르지 않습니다.

471
00:31:42,330 --> 00:31:46,950
그래서 이것이 L1과 L2의 큰 차이점인 이유입니다.

472
00:31:46,950 --> 00:31:50,930
때때로 우리의 특징이 매우 구체적이고

473
00:31:50,930 --> 00:31:53,040
의미가 있으며 그 정보를

474
00:31:53,040 --> 00:31:56,510
보존하고 싶을 때, L1이 더

475
00:31:56,510 --> 00:32:03,020
중요하고 더 나은 경우가 많습니다. 왜냐하면 L1은 특징에 기반하여

476
00:32:03,020 --> 00:32:07,690
거리를 보존하고 강제하는 형태를 가지고

477
00:32:07,690 --> 00:32:09,140
있기 때문입니다.

478
00:32:09,140 --> 00:32:12,880
하지만 그 특징들이 더 임의적이라면 L2

479
00:32:12,880 --> 00:32:15,250
거리가 더 의미가 있습니다.

480
00:32:15,250 --> 00:32:19,250
거리를 계산하고 싶다면, 이

481
00:32:19,250 --> 00:32:22,630
형태의 모든 점이 원점에서

482
00:32:22,630 --> 00:32:29,350
정확히 같은 거리를 가지며, L1 거리를 사용할 경우

483
00:32:29,350 --> 00:32:34,300
그렇습니다. 그러나 L2 거리의 경우,

484
00:32:34,300 --> 00:32:38,020
이 원 위의 점들은 이 공간의

485
00:32:38,020 --> 00:32:43,370
중심 또는 원점에서 같은 거리를 가집니다.

486
00:32:43,370 --> 00:32:47,003
그래서 이것이 기본적으로 이 두 이미지가 보여주는

487
00:32:51,200 --> 00:32:52,720
주요 내용입니다.

488
00:32:52,720 --> 00:32:57,760
이 형태의 어떤 점이든 L1 거리를 사용할 때 원점에서

489
00:32:57,760 --> 00:33:00,080
같은 거리를 가집니다.

490
00:33:00,080 --> 00:33:03,180
그리고 원의 경우, 원 위의 어떤

491
00:33:03,180 --> 00:33:05,340
점이든 L2 거리를 사용할

492
00:33:05,340 --> 00:33:08,490
때 원점에서 같은 거리를 가집니다.

493
00:33:08,490 --> 00:33:11,280
네, 왜 중요한지, 특징을

494
00:33:11,280 --> 00:33:15,630
보존하고 싶다면 L1을 사용하는 것이 더 좋습니다.

495
00:33:15,630 --> 00:33:21,210
그 질문에 답하기 위해, 특징 축을

496
00:33:21,210 --> 00:33:24,780
회전시키면 거리와 이 거리 함수가

497
00:33:24,780 --> 00:33:27,540
완전히 변합니다.

498
00:33:27,540 --> 00:33:33,040
여기서 같은 작업을 하면 아무것도 변하지 않습니다.

499
00:33:33,040 --> 00:33:37,404
특징의 값은 정확히 동일합니다.

500
00:33:37,404 --> 00:33:38,200
거리입니다.

501
00:33:38,200 --> 00:33:38,700
죄송합니다.

502
00:33:38,700 --> 00:33:44,490
이 경우 L1은 특징 값에 매우 민감한 반면, L2는

503
00:33:44,490 --> 00:33:46,710
그렇지 않습니다.

504
00:33:46,710 --> 00:33:49,440
같은 공간에서 다른 특징을

505
00:33:49,440 --> 00:33:52,920
선택하면 다른 형태가 생성되므로

506
00:33:52,920 --> 00:33:57,210
L 함수, 거리 함수도 변합니다.

507
00:33:57,210 --> 00:34:01,270
여기서 선을 그리면, 온라인 학생들에게

508
00:34:01,270 --> 00:34:05,200
질문은 왜 회전하면 변하는지입니다.

509
00:34:05,200 --> 00:34:09,914
이쪽에서부터 가는 다른 특징을 선택하면

510
00:34:09,914 --> 00:34:13,199
선이 다르게 보일 것입니다.

511
00:34:13,199 --> 00:34:20,559
이것을 회전시키면, 그러나 그 형태에 대해서는 무관합니다.

512
00:34:20,559 --> 00:34:21,059
이것을 회전시키면, 그러나 그 형태에 대해서는 무관합니다.

513
00:34:23,610 --> 00:34:27,370
우리가 이야기한 이 두 거리

514
00:34:27,370 --> 00:34:33,840
함수로 공간을 시각화하면, k가 1일 때 L1과

515
00:34:33,840 --> 00:34:38,920
L2의 가장 가까운 이웃으로 공간

516
00:34:38,920 --> 00:34:41,949
분할을 볼 수 있습니다.

517
00:34:41,949 --> 00:34:44,340
여기서 흥미로운

518
00:34:44,340 --> 00:34:51,870
점은 L1 함수로 대부분의 경계가 두 축, 두 특징인

519
00:34:51,870 --> 00:34:57,180
x1과 x2에 평행하다는 것입니다.

520
00:34:57,180 --> 00:35:01,345
매우 특징에 민감합니다.

521
00:35:01,345 --> 00:35:02,720
반면에, 우리는

522
00:35:02,720 --> 00:35:07,820
조금 더 부드러운 경계 분리를 가지고 있습니다.

523
00:35:07,820 --> 00:35:11,660
그래서 실험실 웹사이트에 온라인 도구가

524
00:35:11,660 --> 00:35:16,640
있어 다양한 거리 함수와 k의 수로 이것을 가지고

525
00:35:16,640 --> 00:35:18,570
놀 수 있습니다.

526
00:35:18,570 --> 00:35:21,630
다른 설정을 만들 수 있습니다.

527
00:35:21,630 --> 00:35:23,760
그래서 이것을 가지고 놀 수 있습니다.

528
00:35:23,760 --> 00:35:28,050
하지만 왜 처음에 가장 가까운 이웃에 대해 이야기했나요?

529
00:35:28,050 --> 00:35:33,560
첫째, 네, 이것은 해결하기 가장 쉬운 문제이며, 가장 쉬운

530
00:35:33,560 --> 00:35:39,230
솔루션, 가장 쉬운 데이터 기반 접근 방식이며 시작하기에

531
00:35:39,230 --> 00:35:40,080
좋습니다.

532
00:35:40,080 --> 00:35:45,950
하지만 우리가 가장 가까운 이웃을 반복하고 논의하고자

533
00:35:45,950 --> 00:35:49,250
하는 주요 이유 중 하나는

534
00:35:49,250 --> 00:35:55,080
하이퍼파라미터 주제를 살펴볼 수 있기 때문입니다.

535
00:35:55,080 --> 00:36:01,430
하이퍼파라미터는 종종 알고리즘을 실행하기 위해

536
00:36:01,430 --> 00:36:03,380
결정해야 하는

537
00:36:03,380 --> 00:36:05,820
변수 중 일부입니다.

538
00:36:05,820 --> 00:36:12,320
이 경우 k의 값, 가장 가까운 이웃의 수는

539
00:36:12,320 --> 00:36:14,850
하이퍼파라미터로 정의됩니다.

540
00:36:14,850 --> 00:36:19,770
가장 가까운 이웃의 수에 따라 출력이

541
00:36:19,770 --> 00:36:21,960
달라집니다.

542
00:36:21,960 --> 00:36:24,500
그리고 여기서 또 다른

543
00:36:24,500 --> 00:36:27,050
선택은 거리 함수입니다.

544
00:36:27,050 --> 00:36:33,320
하이퍼파라미터의 선택은 종종 데이터

545
00:36:33,320 --> 00:36:39,830
세트에 의존하며 때때로 문제에 의존합니다.

546
00:36:39,830 --> 00:36:43,700
그리고 우리는 각 문제에 대해

547
00:36:43,700 --> 00:36:50,270
최적화할 수 있도록 이를 식별할 방법이 필요합니다.

548
00:36:50,270 --> 00:36:52,370
이것은 머신러닝 알고리즘,

549
00:36:52,370 --> 00:36:56,080
딥러닝 알고리즘 등에서 하이퍼파라미터 튜닝이라고

550
00:36:56,080 --> 00:36:58,200
흔히 불리는 것입니다.

551
00:36:58,200 --> 00:36:59,560
그걸 어떻게 하나요?

552
00:36:59,560 --> 00:37:03,220
하이퍼파라미터를 어떻게 설정하나요?

553
00:37:03,220 --> 00:37:04,720
여러 가지 접근 방식이 있습니다.

554
00:37:04,720 --> 00:37:08,520
그 중 하나는 훈련 데이터에 가장 잘 맞는

555
00:37:08,520 --> 00:37:10,980
하이퍼파라미터를 선택하는 것입니다.

556
00:37:10,980 --> 00:37:14,140
그래서 이미지나 데이터 세트가 있습니다.

557
00:37:14,140 --> 00:37:19,290
훈련 데이터에서 최적의 하이퍼파라미터

558
00:37:19,290 --> 00:37:23,940
세트를 찾아 최저

559
00:37:23,940 --> 00:37:27,700
훈련 손실을 생성합니다.

560
00:37:27,700 --> 00:37:30,700
훈련 데이터에는 잘 작동하지만,

561
00:37:30,700 --> 00:37:34,710
특히 최근접 이웃에서는 k가 1일 때

562
00:37:34,710 --> 00:37:39,660
항상 최상의 값이기 때문에 좋은 생각이 아닙니다.

563
00:37:39,660 --> 00:37:43,620
훈련 데이터를 암기하고 있기 때문에 k가

564
00:37:43,620 --> 00:37:47,280
1이면 항상 100% 정확도를 제공합니다.

565
00:37:47,280 --> 00:37:51,580
그래서 이것이 좋은 생각이 아니라는 것을 알고 있습니다.

566
00:37:51,580 --> 00:37:56,520
두 번째 방법은 보류된 테스트 세트에 가장 잘

567
00:37:56,520 --> 00:38:00,570
맞는 하이퍼파라미터를 선택하는 것입니다.

568
00:38:00,570 --> 00:38:05,250
이것은 첫 번째 방법보다 조금 나은 방법이지만,

569
00:38:05,250 --> 00:38:07,630
여기에도 큰 문제가 있습니다.

570
00:38:07,630 --> 00:38:11,610
누군가 이게 왜 문제가 되는지 말해줄 수 있나요?

571
00:38:11,610 --> 00:38:15,390
정확히 그렇습니다. 테스트 데이터에서 작동하는 최상의

572
00:38:15,390 --> 00:38:18,910
하이퍼파라미터를 찾으려는 것이기 때문에 일종의

573
00:38:18,910 --> 00:38:24,030
속임수입니다. 모델이 테스트 세트가 아닌 다른 데이터 포인트에서

574
00:38:24,030 --> 00:38:26,200
어떻게 작동할지 알지 못합니다.

575
00:38:26,200 --> 00:38:30,630
그래서 맞습니다.

576
00:38:30,630 --> 00:38:33,450
모델이 어떻게 일반화될지 모르기 때문에

577
00:38:33,450 --> 00:38:35,260
좋은 생각이 아닙니다.

578
00:38:35,260 --> 00:38:39,070
절대 이렇게 해서는 안 됩니다.

579
00:38:39,070 --> 00:38:41,620
우리가 이야기한 것처럼, 이것은 일종의 속임수입니다.

580
00:38:41,620 --> 00:38:47,040
더 나은 방법은 항상 훈련 데이터의 일부를

581
00:38:47,040 --> 00:38:51,020
검증 세트로 분리하는 것입니다.

582
00:38:51,020 --> 00:38:54,530
그리고 새로운 부분의 훈련

583
00:38:54,530 --> 00:38:59,060
데이터에서 모델을 훈련시킵니다.

584
00:38:59,060 --> 00:39:04,160
그런 다음 검증 세트에서 하이퍼파라미터를

585
00:39:04,160 --> 00:39:06,230
찾거나 최적화합니다.

586
00:39:06,230 --> 00:39:10,710
최상의 하이퍼파라미터 세트를 찾은 후, 해당

587
00:39:10,710 --> 00:39:13,820
하이퍼파라미터를 사용하여 테스트 세트에

588
00:39:13,820 --> 00:39:16,370
대한 결과를 복제하고 예측을

589
00:39:16,370 --> 00:39:17,280
수행합니다.

590
00:39:17,280 --> 00:39:20,460
이것은 훨씬 더 나은 접근

591
00:39:20,460 --> 00:39:25,763
방식이지만, 그 자체로도 몇 가지 도전 과제가 있습니다.

592
00:39:28,760 --> 00:39:32,340
때때로 선택한 검증 세트가 전체 풍경을

593
00:39:32,340 --> 00:39:34,220
잘 대표하지 않을

594
00:39:34,220 --> 00:39:37,850
수 있습니다. 왜냐하면 검증 세트는

595
00:39:37,850 --> 00:39:40,770
거의 항상 훨씬 작기 때문입니다.

596
00:39:40,770 --> 00:39:44,780
그래서 하이퍼파라미터 설정을 위한

597
00:39:44,780 --> 00:39:52,160
더 나은 접근 방식 중 하나는 교차 검증을 사용하는 것입니다.

598
00:39:52,160 --> 00:39:54,940
기본적으로 훈련 데이터를 여러

599
00:39:54,940 --> 00:39:59,350
개의 폴드, 여러 개의 파티션으로 나눕니다.

600
00:39:59,350 --> 00:40:01,120
여기서는 다섯 개입니다.

601
00:40:01,120 --> 00:40:08,140
각 폴드는 한 번 검증 세트 역할을 합니다.

602
00:40:08,140 --> 00:40:10,660
그리고 반복적으로 이 과정을 다섯 번 수행하여

603
00:40:10,660 --> 00:40:12,530
5-폴드 교차 검증을 합니다.

604
00:40:12,530 --> 00:40:17,810
이 과정을 다섯 번 수행하고 정확도를 평균냅니다.

605
00:40:17,810 --> 00:40:20,510
그래서 하이퍼파라미터의 값을 설정합니다.

606
00:40:20,510 --> 00:40:25,460
이 다섯 세트에 대해 실행하고, 정확도를 정의하고,

607
00:40:25,460 --> 00:40:29,690
검증 세트에서 정확도를 계산하여 평균을 냅니다.

608
00:40:29,690 --> 00:40:31,600
그런 다음 하이퍼파라미터에 대한 최적의

609
00:40:31,600 --> 00:40:34,250
설정을 찾기 위해 이 과정을 여러 번 반복합니다.

610
00:40:34,250 --> 00:40:36,620
하이퍼파라미터 설정을 찾은 후에는

611
00:40:36,620 --> 00:40:39,160
이를 테스트 세트에 적용합니다.

612
00:40:39,160 --> 00:40:41,500
이 방법은 조금 더 신뢰할

613
00:40:41,500 --> 00:40:44,270
수 있으며 훨씬 더 나은 결과를

614
00:40:44,270 --> 00:40:46,360
생성하지만, 대규모

615
00:40:46,360 --> 00:40:52,860
딥러닝에서는 여러 번 반복하고 방대한 데이터 세트로 다섯 번 수행하는

616
00:40:52,860 --> 00:40:56,290
것이 매우 어렵기 때문에 덜 사용됩니다.

617
00:40:56,290 --> 00:41:00,040
그래서 우리는 종종 하이퍼파라미터 설정을 위해 직관을

618
00:41:00,040 --> 00:41:03,720
사용하며, 단일 검증 세트가 때때로 우리가 선택하는

619
00:41:03,720 --> 00:41:05,200
접근 방식입니다.

620
00:41:05,200 --> 00:41:09,150
하지만 이는 상당히 권장됩니다.

621
00:41:09,150 --> 00:41:13,570
다시 말해, 컴퓨터 비전이나 대규모 데이터

622
00:41:13,570 --> 00:41:16,200
세트 외부에서는 종종

623
00:41:16,200 --> 00:41:22,710
연구 논문에서 이러한 유형의 교차 검증과 통계적 프레임워크를

624
00:41:22,710 --> 00:41:27,780
요구하여 결과가 테스트 세트에서 재현

625
00:41:27,780 --> 00:41:29,650
가능하도록 합니다.

626
00:41:29,650 --> 00:41:32,590
어쨌든 다양한 접근 방식이 있습니다.

627
00:41:32,590 --> 00:41:36,420
주제의 마무리를 짓고 최근접

628
00:41:36,420 --> 00:41:42,720
이웃 주제를 정리하며 몇 가지 예제와

629
00:41:42,720 --> 00:41:45,540
결과를 살펴보겠습니다.

630
00:41:45,540 --> 00:41:50,268
CIFAR10 데이터 세트를 소개하겠습니다.

631
00:41:50,268 --> 00:41:51,810
이 데이터 세트는

632
00:41:51,810 --> 00:41:56,470
과제에서 자주 사용할 데이터 세트 중 하나입니다.

633
00:41:56,470 --> 00:42:01,260
10개의 클래스와 여러 개의 훈련 이미지 및 테스트 이미지가

634
00:42:01,260 --> 00:42:02,170
있습니다.

635
00:42:02,170 --> 00:42:03,960
10개의 클래스 중 일부

636
00:42:03,960 --> 00:42:07,590
예제가 여기에서 각 테스트 이미지에 대한

637
00:42:07,590 --> 00:42:09,580
최근접 이웃으로 표시됩니다.

638
00:42:09,580 --> 00:42:14,790
최근접 이웃을 실행하고 상위

639
00:42:14,790 --> 00:42:22,980
10개의 최근접 이웃을 선택하면 모두 시각화됩니다.

640
00:42:22,980 --> 00:42:26,880
상상할 수 있듯이, 첫 번째 질문 중 하나는

641
00:42:26,880 --> 00:42:28,260
k의 값이

642
00:42:31,290 --> 00:42:32,930
얼마여야 하는가입니다.

643
00:42:32,930 --> 00:42:34,680
몇 개의 최근접 이웃을 선택해야 할까요?

644
00:42:34,680 --> 00:42:39,960
그리고 다섯 겹의 빠른 실험 중 하나를 연구하고

645
00:42:39,960 --> 00:42:42,660
싶습니다. 각 점은

646
00:42:42,660 --> 00:42:47,960
k의 각 값에 대한 다섯 겹의 하나의 겹을 나타내며,

647
00:42:47,960 --> 00:42:52,080
여기에서 다른 값이 표시됩니다.

648
00:42:52,080 --> 00:42:56,880
아마도 여기에서 볼 수 있듯이, k가 7일

649
00:42:56,880 --> 00:43:01,440
때 정확도 측면에서 가장 좋은 결과를

650
00:43:01,440 --> 00:43:03,740
생성하며, 이는 약

651
00:43:03,740 --> 00:43:10,190
29%에서 28%의 정확도에 가깝습니다. 이는 10개 클래스

652
00:43:10,190 --> 00:43:14,700
분류 문제이기 때문에 나쁘지 않습니다.

653
00:43:14,700 --> 00:43:17,190
10개 클래스 분류 문제에서는

654
00:43:17,190 --> 00:43:21,600
종종 무작위 추측으로 10%의 정확도를 얻습니다.

655
00:43:21,600 --> 00:43:24,630
그래서 이는 무작위 추측보다 훨씬 나은 결과입니다.

656
00:43:24,630 --> 00:43:25,680
작동하고 있습니다.

657
00:43:25,680 --> 00:43:30,980
어떤 일이 일어나고 있지만 개선할 여지가 많습니다.

658
00:43:30,980 --> 00:43:35,700
예제로 돌아가서 보면, 특히 가장 가까운

659
00:43:35,700 --> 00:43:38,900
것에서 많은 실수가 있음을

660
00:43:38,900 --> 00:43:41,040
알 수 있습니다.

661
00:43:41,040 --> 00:43:44,150
예를 들어, 네 번째 행을 보면 개구리입니다.

662
00:43:44,150 --> 00:43:47,690
하지만 첫 번째 예제는 고양이처럼 보입니다.

663
00:43:47,690 --> 00:43:48,770
죄송합니다, 개입니다.

664
00:43:48,770 --> 00:43:54,340
왜 이런 일이 발생하는지 추측할 수 있습니다. 왜냐하면

665
00:43:54,340 --> 00:43:57,580
거리가 픽셀에 적용되기 때문입니다.

666
00:43:57,580 --> 00:44:01,970
픽셀 단위로 보면 서로 비슷하게 보입니다.

667
00:44:01,970 --> 00:44:05,570
대부분의 픽셀에서 같은 색상을 가지고

668
00:44:05,570 --> 00:44:08,330
있어 훨씬 더 가깝습니다.

669
00:44:08,330 --> 00:44:10,390
이 예제와 다른 많은 예제는

670
00:44:10,390 --> 00:44:15,250
픽셀 및 픽셀 값에 적용되는 거리 측정이 최선의 선택이

671
00:44:15,250 --> 00:44:16,880
아님을 보여줍니다.

672
00:44:16,880 --> 00:44:18,860
우리는 그것들을 연습하지 않습니다.

673
00:44:18,860 --> 00:44:21,370
우리가 미래의

674
00:44:21,370 --> 00:44:29,420
강의에서 논의할 훨씬 더 나은 접근 방식이 있습니다.

675
00:44:29,420 --> 00:44:34,340
주제를 마무리하기 위해, 이것은 또 다른 예입니다.

676
00:44:34,340 --> 00:44:37,820
이 원본 이미지를 보면, 이

677
00:44:37,820 --> 00:44:40,090
세 이미지는

678
00:44:40,090 --> 00:44:46,770
색상이나 가려짐 면에서 매우 다르게 보이지만, 왼쪽에서

679
00:44:46,770 --> 00:44:51,000
세 번째 이미지는 단지 한 픽셀이

680
00:44:51,000 --> 00:44:55,660
오른쪽으로 이동한 같은 이미지입니다.

681
00:44:55,660 --> 00:44:58,590
하지만 인간의 눈으로 보면,

682
00:44:58,590 --> 00:45:00,640
전혀 차이가 없습니다.

683
00:45:00,640 --> 00:45:05,310
그러나 원본 이미지와의 거리는 여기 보이는

684
00:45:05,310 --> 00:45:08,590
다른 두 예와 동일합니다.

685
00:45:08,590 --> 00:45:11,680
몇 가지 질문을 위해 잠시 멈추겠습니다. 이것이

686
00:45:11,680 --> 00:45:13,870
우리가 논의한 내용의 요약입니다.

687
00:45:13,870 --> 00:45:16,680
그래서 질문은, 어떻게 결정을 내릴 것인가입니다.

688
00:45:16,680 --> 00:45:20,700
그런 경우에는 종종 상위 항목 중 하나를 무작위로

689
00:45:20,700 --> 00:45:21,750
선택합니다.

690
00:45:21,750 --> 00:45:27,450
더 많은 데이터를 수집해야 한다면, 예를

691
00:45:27,450 --> 00:45:30,180
들어 유전학 문제를

692
00:45:30,180 --> 00:45:33,450
해결하거나 의료 이미징

693
00:45:33,450 --> 00:45:40,200
문제를 해결할 때 예제나 특징을 시각화할

694
00:45:40,200 --> 00:45:41,020
때입니다.

695
00:45:41,020 --> 00:45:43,570
그리고 이 최근접 이웃

696
00:45:43,570 --> 00:45:47,610
공간에서, 좋은 샘플이 없는 공간이나

697
00:45:47,610 --> 00:45:52,920
모호함이 있는 공간을 보게 된다면, 종종 그

698
00:45:52,920 --> 00:45:55,440
공간의 같은 영역에 있는

699
00:45:55,440 --> 00:45:58,890
다른 샘플을 찾으려고 합니다.

700
00:45:58,890 --> 00:46:03,960
좋습니다, k-최근접 이웃에 대해

701
00:46:03,960 --> 00:46:06,330
이야기한 내용을 요약하자면,

702
00:46:06,330 --> 00:46:12,900
가장 간단한 알고리즘, 데이터 기반 접근

703
00:46:12,900 --> 00:46:17,250
방식에 대한 이해와 하이퍼파라미터

704
00:46:17,250 --> 00:46:19,080
조정, 거리

705
00:46:19,080 --> 00:46:23,040
측정 및 k 값의 중요성에 대해

706
00:46:23,040 --> 00:46:26,070
이야기하는 것이었습니다.

707
00:46:26,070 --> 00:46:30,570
다음 주제로 넘어가겠습니다. 그것은 선형 분류기입니다.

708
00:46:30,570 --> 00:46:35,640
이 주제를 다루는 데 25분이 소요되며, 이 강의의

709
00:46:35,640 --> 00:46:40,190
남은 시간을 이 매우 중요한 주제에 대해

710
00:46:40,190 --> 00:46:43,770
이야기하는 데 할애하고 싶습니다.

711
00:46:43,770 --> 00:46:50,960
이는 거의 모든 것의 가장 중요한 구성 요소입니다.

712
00:46:50,960 --> 00:46:53,930
딥러닝의 모든 것.

713
00:46:53,930 --> 00:47:04,200
그리고 우리는 이 접근 방식이 어떻게 다른지 알아봐야 합니다.

714
00:47:04,200 --> 00:47:07,740
그래서 먼저 이것이 최근접 이웃과 어떻게 다른지 보고 싶습니다.

715
00:47:07,740 --> 00:47:09,900
이것은 매개변수적

716
00:47:09,900 --> 00:47:14,180
접근 방식으로, 입력 이미지를 출력 클래스,

717
00:47:14,180 --> 00:47:17,900
출력 숫자로 매핑하는 일부

718
00:47:17,900 --> 00:47:22,850
매개변수 w 또는 가중치를 학습하고 찾고 있다는

719
00:47:22,850 --> 00:47:23,850
의미입니다.

720
00:47:23,850 --> 00:47:26,720
이 경우, 입력을 출력으로

721
00:47:26,720 --> 00:47:30,290
매핑하는 함수 f를 생성할 때,

722
00:47:30,290 --> 00:47:35,830
종종 그 출력은 이미지가 10개의 출력 클래스

723
00:47:35,830 --> 00:47:41,900
레이블 각각에 대한 멤버십 점수와 같은 것입니다.

724
00:47:41,900 --> 00:47:49,060
따라서 우리가 구축한 이 설정에서, 선형 분류기는

725
00:47:49,060 --> 00:47:53,830
먼저 w를 사용하여 각 입력

726
00:47:53,830 --> 00:48:01,010
x를 값으로 매핑하는데, 이 값이 출력 y입니다.

727
00:48:01,010 --> 00:48:04,640
이것이 수행되는 방법은 매우 간단합니다.

728
00:48:04,640 --> 00:48:10,250
이 이미지는 기본적으로 32x32x3의 영역으로,

729
00:48:10,250 --> 00:48:14,150
즉 3,072개의 숫자입니다.

730
00:48:14,150 --> 00:48:23,000
이것이 우리의 x를 정의하며, x는 3,072x1 벡터입니다.

731
00:48:23,000 --> 00:48:26,360
우리는 10개의 출력 클래스가 있다는 것을 알고 있습니다.

732
00:48:26,360 --> 00:48:28,430
따라서 10개의 서로 다른 점수가 필요합니다.

733
00:48:28,430 --> 00:48:29,650
점수는

734
00:48:29,650 --> 00:48:34,690
10x1의 벡터 형태로 출력됩니다.

735
00:48:34,690 --> 00:48:38,910
이는 x를 출력 점수로 매핑하는

736
00:48:38,910 --> 00:48:47,520
10x3,072의 가중치 행렬 w를 찾아야 함을 의미합니다.

737
00:48:47,520 --> 00:48:50,020
이 선형 함수를 완성하기

738
00:48:50,020 --> 00:48:56,380
위해, 우리는 종종 이 바이어스 항도 사용합니다.

739
00:48:56,380 --> 00:49:01,620
이는 입력에 독립적인 값으로, 실제로 다양한

740
00:49:01,620 --> 00:49:03,400
용도가 있습니다.

741
00:49:03,400 --> 00:49:07,990
나는 기하학적 시각화를 할 때 이에 대해

742
00:49:07,990 --> 00:49:13,500
이야기할 수 있지만, 때때로 서로 다른 클래스 점수에

743
00:49:13,500 --> 00:49:20,470
대한 변화를 만들어 각 클래스를 더 잘 분리하는 데 도움이 됩니다.

744
00:49:20,470 --> 00:49:23,730
내가 말했듯이, 이러한 선형 함수는

745
00:49:23,730 --> 00:49:28,270
실제로 신경망을 구축하는 기본 요소입니다.

746
00:49:28,270 --> 00:49:32,700
이러한 선형 분류기, 선형

747
00:49:32,700 --> 00:49:35,170
함수가 하나씩

748
00:49:35,170 --> 00:49:39,240
이어져서 큰 신경망을 만듭니다.

749
00:49:39,240 --> 00:49:42,480
여기에 추가해야 할 다른 많은

750
00:49:42,480 --> 00:49:45,720
것들이 있지만, 이것은 가장 중요한

751
00:49:45,720 --> 00:49:48,190
구성 요소 중 하나입니다.

752
00:49:48,190 --> 00:49:56,200
일부 인기 있는 신경망을 살펴보면, 선형 함수가

753
00:49:56,200 --> 00:50:00,570
아키텍처 곳곳에 존재하는 것을

754
00:50:00,570 --> 00:50:03,600
볼 수 있습니다.

755
00:50:03,600 --> 00:50:08,040
따라서 이 매핑과 이 함수가 무엇을 하는지 더 잘

756
00:50:08,040 --> 00:50:11,880
이해하기 위해, CIFAR10의 예와 우리의

757
00:50:11,880 --> 00:50:15,580
훈련 및 테스트 샘플로 돌아가서 조금 더

758
00:50:15,580 --> 00:50:18,160
간단하게 만들어 보겠습니다.

759
00:50:18,160 --> 00:50:22,120
32x32의 큰 이미지를 보는 대신, 2x2의

760
00:50:22,120 --> 00:50:25,680
이미지를 보겠습니다. 즉, 4개의 픽셀을

761
00:50:25,680 --> 00:50:27,540
가진 입력 이미지입니다.

762
00:50:27,540 --> 00:50:34,560
이는 입력 이미지가 벡터로 변환된다는 것을 의미합니다.

763
00:50:34,560 --> 00:50:42,260
여기에서 볼 수 있듯이, 우리는 w와 b의 값을 찾아야 합니다.

764
00:50:42,260 --> 00:50:49,200
따라서 입력 이미지는 출력으로서 어떤 점수로 매핑됩니다.

765
00:50:49,200 --> 00:50:54,710
이것이 대수적 관점에서 본 선형

766
00:50:54,710 --> 00:50:58,050
함수의 모습입니다.

767
00:50:58,050 --> 00:51:01,310
여기에서의 출력 점수는 고양이, 개, 배의 세 가지

768
00:51:01,310 --> 00:51:02,940
클래스를 고려하고 있습니다.

769
00:51:02,940 --> 00:51:10,040
보시다시피, 이 함수는 이미지를 나타내는

770
00:51:10,040 --> 00:51:13,520
벡터를 이러한

771
00:51:13,520 --> 00:51:16,820
점수로 매핑합니다.

772
00:51:16,820 --> 00:51:21,780
선형 분류의 대수적 관점입니다.

773
00:51:21,780 --> 00:51:26,000
이제 이 선형 분류기의 시각적 관점을

774
00:51:26,000 --> 00:51:27,910
살펴보겠습니다.

775
00:51:27,910 --> 00:51:33,190
보시다시피, 우리는 이 이미지에 대해

776
00:51:33,190 --> 00:51:38,540
이야기한 것처럼 각 이미지를 자주 생성합니다.

777
00:51:38,540 --> 00:51:45,310
각 클래스에 대해 어떤 종류의-- 우리는 이 행렬

778
00:51:45,310 --> 00:51:46,330
w의

779
00:51:46,330 --> 00:51:48,506
행을 정의합니다.

780
00:51:48,506 --> 00:51:51,070
따라서 이 행은 특정 클래스에

781
00:51:51,070 --> 00:51:52,760
대한 템플릿과 같습니다.

782
00:51:52,760 --> 00:51:57,370
이렇게 분리하면, 이 이미지는 w와 b로

783
00:51:57,370 --> 00:52:01,030
곱해지고, w는 고양이, 개,

784
00:52:01,030 --> 00:52:03,940
배의 세 가지 클래스

785
00:52:03,940 --> 00:52:06,140
각각의 템플릿입니다.

786
00:52:06,140 --> 00:52:12,740
CIFAR 데이터 세트에서 모델을 훈련하거나

787
00:52:12,740 --> 00:52:18,880
구축한 후, 선형 분류기의 시각적 관점을 살펴보면,

788
00:52:18,880 --> 00:52:21,170
10개 클래스

789
00:52:21,170 --> 00:52:23,740
각각에 대해

790
00:52:23,740 --> 00:52:26,120
학습된 템플릿을 볼

791
00:52:26,120 --> 00:52:27,910
수 있습니다.

792
00:52:27,910 --> 00:52:30,570
예를 들어,

793
00:52:30,570 --> 00:52:34,360
자동차의 경우, 자동차의 앞모습

794
00:52:34,360 --> 00:52:39,640
템플릿 같은 것이 보이고, 이는

795
00:52:39,640 --> 00:52:45,870
모두 하나의 선형 분류기로 수행됩니다.

796
00:52:45,870 --> 00:52:48,900
선형 분류기의 시각적

797
00:52:48,900 --> 00:52:51,030
측면과 기하학적

798
00:52:51,030 --> 00:52:55,800
관점의 또 다른 측면이 있습니다.

799
00:52:55,800 --> 00:53:01,290
이 선형 분류기가 종종 하는 것은 2D

800
00:53:01,290 --> 00:53:05,910
공간에서 각 클래스를 다른 클래스와

801
00:53:05,910 --> 00:53:09,240
분리하는 선을 찾는 것입니다.

802
00:53:09,240 --> 00:53:12,570
여기에서 빨간색, 파란색,

803
00:53:12,570 --> 00:53:20,260
초록색은 서로 다른 클래스를 정의하고 있으며, 고차원

804
00:53:20,260 --> 00:53:22,990
공간에서는 이러한

805
00:53:22,990 --> 00:53:27,450
선 대신 하이퍼플레인이 있습니다.

806
00:53:27,450 --> 00:53:33,240
여기에서 바이어스 항의 사용도 볼 수 있습니다. 바이어스가

807
00:53:33,240 --> 00:53:36,420
없었다면 모든 선은 그 공간의 중심인

808
00:53:36,420 --> 00:53:38,250
원점을 통과해야

809
00:53:38,250 --> 00:53:42,580
했을 것이며, 이는 실제로 말이 되지 않습니다.

810
00:53:42,580 --> 00:53:44,640
하지만 바이어스가

811
00:53:44,640 --> 00:53:53,220
있으면 더 신뢰할 수 있는 함수와 결정 경계를 만들 수 있습니다.

812
00:53:53,220 --> 00:53:57,190
선형 함수는 매우 유용합니다.

813
00:53:57,190 --> 00:54:01,150
선형 분류기는 우리가 이야기한 것처럼 많은 응용 프로그램에

814
00:54:01,150 --> 00:54:02,650
매우 유용합니다.

815
00:54:02,650 --> 00:54:08,050
그리고 더 복잡한 신경망의 빌딩 블록입니다.

816
00:54:08,050 --> 00:54:10,410
하지만 많은 별개의

817
00:54:10,410 --> 00:54:18,250
데이터를 분류할 수 없기 때문에 자체적인 도전 과제가 있습니다.

818
00:54:18,250 --> 00:54:20,910
예를 들어, 이 경우 클래스 1이 첫

819
00:54:20,910 --> 00:54:24,200
번째 및 세 번째 사분면이고 두 번째 클래스가 두

820
00:54:24,200 --> 00:54:26,100
번째 및 네 번째 사분면입니다.

821
00:54:26,100 --> 00:54:28,170
이들은 선형적으로 분리할 수 있는 방법이 없습니다.

822
00:54:28,170 --> 00:54:34,580
또 다른 예는 클래스 1과 클래스 2 사이의 이러한 유형의 분리가

823
00:54:34,580 --> 00:54:37,970
있으며, 원점에서의 거리가 1과

824
00:54:37,970 --> 00:54:41,570
2 사이인 클래스 1과 나머지 모든

825
00:54:41,570 --> 00:54:44,580
것이 클래스 2인 경우입니다.

826
00:54:44,580 --> 00:54:47,100
유사하게, 공간에 세 개의 모드, 즉

827
00:54:47,100 --> 00:54:50,760
하나의 클래스가 있는 세 개의 영역이 있고, 두 번째 클래스는

828
00:54:50,760 --> 00:54:52,260
나머지 모든 것입니다.

829
00:54:52,260 --> 00:54:54,620
따라서 이러한 모든 경우에서

830
00:54:54,620 --> 00:54:57,770
분리가 실제로 매우 어렵습니다.

831
00:54:57,770 --> 00:55:04,260
따라서 우리가 해야 할 일은-- 선형 분류기에 대해

832
00:55:04,260 --> 00:55:06,380
이야기했으며, 이들이 실제로

833
00:55:06,380 --> 00:55:12,200
입력 이미지를 출력의 어떤 형태의 레이블로 매핑할

834
00:55:12,200 --> 00:55:14,580
수 있다는 것입니다.

835
00:55:14,580 --> 00:55:20,480
하지만 이제 남은 것은 각 이미지에 대해 w 값을 선택하는

836
00:55:20,480 --> 00:55:22,690
방법입니다. 이 값이

837
00:55:22,690 --> 00:55:27,130
각 클래스에 대한 점수로 이미지를 매핑합니다.

838
00:55:27,130 --> 00:55:31,970
이를 위해 우리는 분류기가 얼마나 잘

839
00:55:31,970 --> 00:55:34,330
작동하는지를 정량화하는

840
00:55:34,330 --> 00:55:38,960
손실 함수, 때때로 목적 함수라고

841
00:55:38,960 --> 00:55:42,140
불리는 것을 정의해야 합니다.

842
00:55:42,140 --> 00:55:45,940
훈련 데이터에 대한 점수와

843
00:55:45,940 --> 00:55:49,210
관련된 불행의 정도입니다.

844
00:55:49,210 --> 00:55:54,700
그것들을 정의한 후, 우리는 그 불행을 최소화할 수

845
00:55:54,700 --> 00:56:01,730
있도록 w의 값을 효율적으로 변경하는 방법을 찾아야 합니다. 기본적으로

846
00:56:01,730 --> 00:56:05,180
손실 함수를 최소화하는 것입니다.

847
00:56:05,180 --> 00:56:09,260
이것이 최적화 과정입니다.

848
00:56:09,260 --> 00:56:12,620
다음 수업, 다음 강의의 주제입니다.

849
00:56:12,620 --> 00:56:18,610
이를 위해, 다시 단순함을 위해 더

850
00:56:18,610 --> 00:56:23,160
쉽고 쉬운 예를 살펴보겠습니다.

851
00:56:23,160 --> 00:56:27,520
여기 보이는 세 가지 클래스,

852
00:56:27,520 --> 00:56:35,430
고양이, 자동차, 개구리의 선형 함수입니다.

853
00:56:35,430 --> 00:56:37,590
현재 분류기가 얼마나 좋은지를

854
00:56:37,590 --> 00:56:40,270
알려주는 손실 함수가 필요합니다.

855
00:56:40,270 --> 00:56:45,210
이를 위해 문제를 매개변수화해야 합니다. xi와

856
00:56:45,210 --> 00:56:50,580
yi는 입력 이미지, 레이블 이미지 및 해당

857
00:56:50,580 --> 00:56:52,900
레이블을 정의합니다.

858
00:56:52,900 --> 00:56:56,340
그런 다음 손실 함수와

859
00:56:56,340 --> 00:56:59,550
거리 함수가 필요합니다.

860
00:56:59,550 --> 00:57:05,190
이는 예측된 점수 fx와 w,

861
00:57:05,190 --> 00:57:10,170
그리고 이미 주어진 실제 값

862
00:57:10,170 --> 00:57:13,950
yi와 비교하여 차이를

863
00:57:13,950 --> 00:57:15,760
살펴봅니다.

864
00:57:15,760 --> 00:57:19,300
우리는 종종 샘플 수에 따라 정규화하지만, 그리

865
00:57:19,300 --> 00:57:20,830
중요하지는 않습니다.

866
00:57:20,830 --> 00:57:23,160
이것이 손실 함수,

867
00:57:23,160 --> 00:57:26,460
목적 함수를 정의합니다.

868
00:57:26,460 --> 00:57:32,670
최적화를 어떻게 할 수 있는지, 그리고

869
00:57:32,670 --> 00:57:40,510
w를 어떻게 찾을 수 있는지에 대한 것입니다.

870
00:57:40,510 --> 00:57:44,370
이 l-- li를 정의하는 방법에는 여러 가지가

871
00:57:44,370 --> 00:57:44,980
있습니다.

872
00:57:44,980 --> 00:57:50,790
지금 소프트맥스 분류기에 대해 이야기하고 싶습니다.

873
00:57:50,790 --> 00:57:57,030
고양이에 대한 예로, 주어진 점수가

874
00:57:57,030 --> 00:57:59,010
3.2, 5.

875
00:57:59,010 --> 00:58:04,710
1, -1.7이었습니다. 이는

876
00:58:04,710 --> 00:58:10,530
우리가 논의한 함수 f, xi 및

877
00:58:10,530 --> 00:58:13,290
w의 출력입니다.

878
00:58:13,290 --> 00:58:18,570
이 점수를 변환하기 위해-- 이 점수는 제한이

879
00:58:18,570 --> 00:58:20,210
없고 값이

880
00:58:20,210 --> 00:58:22,730
종종 제어하기 어렵기

881
00:58:22,730 --> 00:58:26,870
때문에, 이는 단순한 선형 함수입니다.

882
00:58:26,870 --> 00:58:34,740
이 점수를 어떤 점수 함수로 변환하기 위해, 가장 좋은 방법은

883
00:58:34,740 --> 00:58:37,130
이 점수를 확률로

884
00:58:37,130 --> 00:58:39,080
변환하는

885
00:58:39,080 --> 00:58:45,050
것입니다. 이는 각 입력 이미지 xi에 대해

886
00:58:45,050 --> 00:58:49,320
클래스 k가 될 확률을 정의합니다.

887
00:58:49,320 --> 00:58:53,510
이를 위해 먼저 사용하는

888
00:58:53,510 --> 00:58:58,350
함수는 소프트맥스 함수입니다.

889
00:58:58,350 --> 00:59:02,900
우리는 먼저 점수의 값을 지수화하여

890
00:59:02,900 --> 00:59:05,580
이러한 숫자를 생성합니다.

891
00:59:05,580 --> 00:59:08,190
이 숫자에 exp를

892
00:59:08,190 --> 00:59:10,110
사용하면, 출력은

893
00:59:10,110 --> 00:59:12,550
항상 양수가 됩니다.

894
00:59:12,550 --> 00:59:15,370
확률이 항상 양수인지 확인해야

895
00:59:15,370 --> 00:59:16,220
합니다.

896
00:59:16,220 --> 00:59:19,660
이 숫자를 생성한 후, 우리가 할 수 있는

897
00:59:19,660 --> 00:59:22,150
것은 단순히 정규화하는 것입니다.

898
00:59:22,150 --> 00:59:24,370
따라서 지수화한 후 모든 샘플의

899
00:59:24,370 --> 00:59:26,690
합을 기준으로 정규화합니다.

900
00:59:26,690 --> 00:59:32,930
그런 다음 모든 샘플의 합을 기준으로 정규화합니다.

901
00:59:32,930 --> 00:59:37,360
이것은 확률 함수를 정의하는 매우

902
00:59:37,360 --> 00:59:41,060
좋은 값 집합을 생성합니다.

903
00:59:41,060 --> 00:59:43,280
이것은 분포 함수입니다.

904
00:59:43,280 --> 00:59:44,510
합이 1이 됩니다.

905
00:59:44,510 --> 00:59:48,580
이것을 해석하고 싶다면,

906
00:59:48,580 --> 00:59:56,500
이 w의 집합이 이 이미지가 고양이일 확률이

907
00:59:56,500 --> 01:00:02,260
13% 또는 0.13이라고 말하는

908
01:00:02,260 --> 01:00:06,224
것은 매우 간단합니다.

909
01:00:06,224 --> 01:00:11,440
명백히 이 예에서 w가 좋은 설정이 아니기 때문에

910
01:00:11,440 --> 01:00:14,230
실수를 하고 있습니다.

911
01:00:14,230 --> 01:00:17,410
우리는 이를 최적화하고 변경해야 합니다.

912
01:00:17,410 --> 01:00:21,120
이 확률은 종종 로짓이라고

913
01:00:21,120 --> 01:00:26,190
불리는 비정규화된 로그 확률의

914
01:00:26,190 --> 01:00:27,850
대응물입니다.

915
01:00:27,850 --> 01:00:33,120
다른 머신러닝 과정을 수강했거나, 다른 분야에서

916
01:00:33,120 --> 01:00:33,960
로지스틱

917
01:00:33,960 --> 01:00:39,280
회귀를 사용한 적이 있다면, 이는 유사한 유형의

918
01:00:39,280 --> 01:00:41,560
프레임워크입니다.

919
01:00:41,560 --> 01:00:45,660
이것은 로지스틱 회귀와 정확히 동일한 프레임워크입니다.

920
01:00:45,660 --> 01:00:49,200
그리고 여기 여러 클래스가

921
01:00:49,200 --> 01:00:54,540
있으므로 다항 로지스틱 회귀입니다.

922
01:00:54,540 --> 01:00:57,553
함수 l을 어떻게 정의하나요?

923
01:00:57,553 --> 01:00:59,220
함수 l을 정의하는

924
01:00:59,220 --> 01:01:01,980
방법이 여러 가지 있다고 말씀드렸습니다.

925
01:01:01,980 --> 01:01:05,590
우리는 손실 함수를 정의하고자 합니다. 여기서

926
01:01:05,590 --> 01:01:06,750
목표는 무엇인가요?

927
01:01:06,750 --> 01:01:13,950
우리는 샘플이 올바른 클래스에 속할 확률을 최대화하고자

928
01:01:13,950 --> 01:01:15,820
합니다.

929
01:01:15,820 --> 01:01:21,810
그래서 0.13의 값을 최대화하고자 합니다.

930
01:01:21,810 --> 01:01:28,150
이제 그 집합에 더 큰 값들이 있습니다.

931
01:01:28,150 --> 01:01:34,600
이것을 최대화하고자 한다면, 이는 최대화 문제입니다.

932
01:01:34,600 --> 01:01:38,100
우리가 정의하는 모든 목표는

933
01:01:38,100 --> 01:01:42,720
최소화 목표 함수를 구축하려고

934
01:01:42,720 --> 01:01:49,170
하므로, 첫 번째 단계는 값을 부정하는 것입니다.

935
01:01:49,170 --> 01:01:52,020
값을 부정하여 최대화 문제가 최소화

936
01:01:52,020 --> 01:01:53,860
문제로 바뀌게 합니다.

937
01:01:53,860 --> 01:01:55,980
그리고 숫자를 조금 더

938
01:01:55,980 --> 01:02:00,130
관리하기 쉽게 만들기 위해 값의 로그를 취합니다.

939
01:02:00,130 --> 01:02:03,570
따라서 그 값의 음의 로그가 이

940
01:02:03,570 --> 01:02:06,810
문제를 해결하기 위한 목표

941
01:02:06,810 --> 01:02:10,050
함수, 손실 함수를 정의합니다.

942
01:02:10,050 --> 01:02:10,740
매우 간단합니다.

943
01:02:10,740 --> 01:02:12,440
이것이

944
01:02:12,440 --> 01:02:20,540
소프트맥스와 이 로지스틱 회귀 함수에 대한 목표 또는 손실 함수입니다.

945
01:02:20,540 --> 01:02:21,270
소프트맥스와 이 로지스틱 회귀 함수에 대한 목표 또는 손실 함수입니다.

946
01:02:21,270 --> 01:02:28,490
제가 말씀드린 것처럼 CS-229와 같은 다른 수업을 들었다면, 이는

947
01:02:28,490 --> 01:02:31,550
종종 최대 우도 추정이라고도

948
01:02:31,550 --> 01:02:32,160
불립니다.

949
01:02:32,160 --> 01:02:34,040
같은 알고리즘입니다.

950
01:02:34,040 --> 01:02:42,540
그 점을 염두에 두고 말씀드리자면, 우리가 논의한 대로, 이는 올바른

951
01:02:42,540 --> 01:02:45,140
클래스의 확률 로그의

952
01:02:45,140 --> 01:02:49,580
음수로, 목표 함수 손실 함수를

953
01:02:49,580 --> 01:02:50,560
정의합니다.

954
01:02:53,690 --> 01:02:56,130
기본적으로 그렇게 간단합니다.

955
01:02:56,130 --> 01:02:59,930
하지만 이 프레임워크를 해석하는 다른 유형도

956
01:02:59,930 --> 01:03:00,720
있습니다.

957
01:03:00,720 --> 01:03:08,830
손실 함수를 재정의하는 한 가지 방법은 우리가 일부 추정된

958
01:03:08,830 --> 01:03:12,560
확률을 가지고 있고, 올바른

959
01:03:12,560 --> 01:03:15,640
확률을 정의하는 확률 함수도

960
01:03:15,640 --> 01:03:18,200
있다는 것입니다.

961
01:03:18,200 --> 01:03:23,080
우리가 하고자 하는 것은 이 두 확률 함수를 일치시키는 것입니다.

962
01:03:23,080 --> 01:03:29,840
이를 위해 KL 발산, 쿨백-라이블러 발산을

963
01:03:29,840 --> 01:03:32,210
최소화하고자 합니다.

964
01:03:32,210 --> 01:03:36,640
이는 이 손실 함수를 보는 정보

965
01:03:36,640 --> 01:03:39,140
이론적 관점입니다.

966
01:03:39,140 --> 01:03:43,760
그리고 다시 말하지만, 이들은 정확히 동일합니다.

967
01:03:43,760 --> 01:03:49,360
이 설정에서의 KL 발산은 우리가 정의한

968
01:03:49,360 --> 01:03:53,290
동일한 음의 로그 함수로

969
01:03:53,290 --> 01:03:54,410
단순화됩니다.

970
01:03:54,410 --> 01:04:01,660
더 나아가, 이는 정확히 교차 엔트로피 함수입니다.

971
01:04:01,660 --> 01:04:09,750
왜냐하면 우리가 p의 엔트로피, 즉 올바른 값의 엔트로피를

972
01:04:09,750 --> 01:04:14,500
정의하고, 그 동일한 KL 발산을

973
01:04:14,500 --> 01:04:19,210
더하면, 다시 이 음의 로그 함수로

974
01:04:19,210 --> 01:04:22,300
단순화되기 때문입니다.

975
01:04:22,300 --> 01:04:26,160
이는 클래스에 대해 원-핫 인코딩 설정을

976
01:04:26,160 --> 01:04:29,920
사용할 때 엔트로피가 0이기 때문입니다.

977
01:04:29,920 --> 01:04:31,920
그래서 우리가 이 함수를 교차

978
01:04:31,920 --> 01:04:34,650
엔트로피 또는 이진 교차 엔트로피 함수라고

979
01:04:34,650 --> 01:04:36,160
부르는 이유 중 하나입니다.

980
01:04:36,160 --> 01:04:39,870
딥러닝 전반에 걸쳐, 여러분이 신경망

981
01:04:39,870 --> 01:04:42,790
프레임워크를 사용했다면 BCE,

982
01:04:42,790 --> 01:04:45,240
이진 교차 엔트로피에 대해

983
01:04:45,240 --> 01:04:47,980
들어보았거나 많이 들을 것입니다.

984
01:04:47,980 --> 01:04:50,850
그래서 이것은 동일한 프레임워크입니다.

985
01:04:50,850 --> 01:04:57,600
우리는 매우 간단하게 시작하지만, 각자의 유사점과

986
01:04:57,600 --> 01:05:00,490
차이점에 도달했습니다.

987
01:05:00,490 --> 01:05:03,810
그래서 목표는-- 죄송합니다, 손실 함수는

988
01:05:03,810 --> 01:05:06,250
이 확률의 음의 로그로

989
01:05:06,250 --> 01:05:09,600
정의되었고, 확률은 우리가 이야기한 소프트맥스에

990
01:05:09,600 --> 01:05:11,650
의해 정의되었습니다.

991
01:05:11,650 --> 01:05:15,600
그리고 이것을 최적화하는 것은 다음 세션의

992
01:05:15,600 --> 01:05:19,450
주제이며, 올바른 w를 제공합니다.

993
01:05:19,450 --> 01:05:23,790
하지만 끝내기 전에, 여기 보이는 정의와 관련하여 몇

994
01:05:23,790 --> 01:05:26,140
가지 질문을 하고 싶습니다.

995
01:05:26,140 --> 01:05:29,550
손실 함수 li의

996
01:05:29,550 --> 01:05:33,450
평균값과 최대값은 무엇인가요?

997
01:05:33,450 --> 01:05:37,090
네, 0입니다. 이는 마이너스 무한대로 변합니다.

998
01:05:37,090 --> 01:05:41,470
하지만 우리는 그곳에 음의 부정이 있으므로, 이는 무한대가 됩니다.

999
01:05:41,470 --> 01:05:42,520
정확합니다.

1000
01:05:42,520 --> 01:05:44,285
하지만 우리는

1001
01:05:47,430 --> 01:05:52,240
또한-- 네, 확실히-- 맞습니다.

1002
01:05:52,240 --> 01:05:56,740
그리고 두 번째 질문을 실제로 살펴보겠습니다.

1003
01:05:56,740 --> 01:05:59,850
네, 이 질문입니다.

1004
01:05:59,850 --> 01:06:05,550
모든 si, 즉 w를 초기화할 때, 처음에는

1005
01:06:05,550 --> 01:06:07,990
거의 무작위입니다.

1006
01:06:07,990 --> 01:06:11,730
그래서 각 클래스의

1007
01:06:11,730 --> 01:06:18,030
확률은 대부분 같아집니다.

1008
01:06:18,030 --> 01:06:22,403
c개의 클래스가 있다고 가정할 때, 소프트맥스 li는 무엇인가요?

1009
01:06:30,435 --> 01:06:31,560
특히 c가 10일 경우입니다.

1010
01:06:34,140 --> 01:06:37,780
확률이 같기

1011
01:06:37,780 --> 01:06:40,740
때문에, 모든

1012
01:06:40,740 --> 01:06:47,110
확률은 1/C에 가깝습니다. 그리고 이는 log C로 정의됩니다.

1013
01:06:47,110 --> 01:06:49,080
10개의 클래스가 있다면,

1014
01:06:49,080 --> 01:06:54,730
10의 로그 또는 ln은 2.3이며, 이는 exp입니다.

1015
01:06:54,730 --> 01:06:57,020
우리는 그것에 대해 알고 있습니다.
