1
00:00:05,270 --> 00:00:06,410
안녕하세요, 여러분.

2
00:00:06,410 --> 00:00:07,415
제 이름은 Zain입니다.

3
00:00:07,415 --> 00:00:09,290
사실 제가 처음 강의, 즉 세 번째

4
00:00:09,290 --> 00:00:12,058
강의에서 제 소개를 하지 않았다는 걸 깨달았습니다만,

5
00:00:12,058 --> 00:00:14,100
저는 이 강의의 공동 강사 중 한 명입니다.

6
00:00:14,100 --> 00:00:15,100
제 이름은 Zain Durante입니다.

7
00:00:15,100 --> 00:00:17,040
저는 Ehsan과 Fei-Fei 두 분께 공동 지도받고 있습니다.

8
00:00:17,040 --> 00:00:22,430
저는 Stanford에서 박사 4학년 학생이고, 오늘 강의인

9
00:00:22,430 --> 00:00:24,260
6강에서는 컨볼루션

10
00:00:24,260 --> 00:00:26,330
신경망 훈련과 CNN

11
00:00:26,330 --> 00:00:29,420
아키텍처에 대해 이야기할 것입니다.

12
00:00:29,420 --> 00:00:33,260
이 강의는 크게 두 부분으로 나뉜다고 할

13
00:00:33,260 --> 00:00:34,770
수 있습니다.

14
00:00:34,770 --> 00:00:36,290
첫 번째는 지금까지

15
00:00:36,290 --> 00:00:39,590
배운 컨볼루션 레이어, 선형 레이어 또는 완전

16
00:00:39,590 --> 00:00:42,030
연결 레이어 같은 다양한 구성 요소를

17
00:00:42,030 --> 00:00:43,970
어떻게 조합해 CNN 아키텍처를

18
00:00:43,970 --> 00:00:46,350
만드는지 알려드리는 것입니다.

19
00:00:46,350 --> 00:00:48,170
몇 가지 예시를 살펴보고,

20
00:00:48,170 --> 00:00:50,060
또한 실제로 어떻게

21
00:00:50,060 --> 00:00:53,070
훈련하는지 그 과정에 대해 이야기할 것입니다.

22
00:00:53,070 --> 00:00:54,800
앞서 말씀드렸듯이, 오늘은 기본적으로

23
00:00:54,800 --> 00:00:56,520
두 가지 주제를 다룰 것입니다.

24
00:00:56,520 --> 00:00:58,080
첫 번째는 CNN을 어떻게

25
00:00:58,080 --> 00:00:59,955
구축하는가입니다. 여기서는 CNN 아키텍처를

26
00:00:59,955 --> 00:01:02,630
어떻게 정의하고 훈련 준비를 하는지에 대해 말하는 겁니다.

27
00:01:02,630 --> 00:01:04,090
두 번째 주제는 CNN을

28
00:01:04,090 --> 00:01:05,780
어떻게 훈련시키는가입니다.

29
00:01:05,780 --> 00:01:09,500
먼저 첫 번째 주제부터 시작해서, 컨볼루션

30
00:01:09,500 --> 00:01:12,710
신경망의 레이어들을 살펴보겠습니다.

31
00:01:12,710 --> 00:01:14,840
지난 강의에서 기억하실지 모르겠지만,

32
00:01:14,840 --> 00:01:17,300
이 모델들의 핵심 레이어인 컨볼루션

33
00:01:17,300 --> 00:01:18,830
레이어에 대해 배웠습니다.

34
00:01:18,830 --> 00:01:23,020
이 레이어들은 필터를 가지고 작동합니다.

35
00:01:23,020 --> 00:01:26,110
각 컨볼루션 레이어마다 미리 정해진 필터 수가

36
00:01:26,110 --> 00:01:28,390
있는데, 이 경우에는 6개입니다.

37
00:01:28,390 --> 00:01:31,820
여기서 입력 데이터의 깊이와 일치시킵니다.

38
00:01:31,820 --> 00:01:34,490
이 경우에는 32x32 RGB 이미지이므로

39
00:01:34,490 --> 00:01:35,810
깊이 채널이 세 개입니다.

40
00:01:35,810 --> 00:01:38,920
각 필터가 이미지 위를 슬라이딩하며 각

41
00:01:38,920 --> 00:01:40,600
지점에서 점수를 계산합니다.

42
00:01:40,600 --> 00:01:42,360
이미지의 해당 위치에서

43
00:01:42,360 --> 00:01:44,470
필터 값과 이미지 값의

44
00:01:44,470 --> 00:01:45,893
내적을 구합니다.

45
00:01:45,893 --> 00:01:48,560
모든 값을 곱하고 합산한 후 바이어스

46
00:01:48,560 --> 00:01:49,970
항을 더합니다.

47
00:01:49,970 --> 00:01:54,280
이렇게 해서 오른쪽 출력 활성화 맵의 각

48
00:01:54,280 --> 00:01:56,210
값을 계산합니다.

49
00:01:56,210 --> 00:01:57,970
이미지 위를 지나가는 슬라이딩

50
00:01:57,970 --> 00:01:59,090
윈도우가 있습니다.

51
00:01:59,090 --> 00:02:01,010
각 위치에서 점수를

52
00:02:01,010 --> 00:02:04,360
계산하고 이렇게 활성화 맵을 얻습니다.

53
00:02:04,360 --> 00:02:06,170
필터마다 하나씩 있습니다.

54
00:02:06,170 --> 00:02:08,080
보통 마지막에 ReLU

55
00:02:08,080 --> 00:02:11,420
같은 비선형 활성화 함수를 적용합니다.

56
00:02:11,420 --> 00:02:13,010
이 내용은 지난 강의에서 다뤘습니다.

57
00:02:13,010 --> 00:02:15,160
여기에 너무 시간을 많이 쓰지 않겠습니다.

58
00:02:15,160 --> 00:02:17,530
질문은, 이미지의 깊이는

59
00:02:17,530 --> 00:02:19,880
RGB 채널 수와 같지만 여기

60
00:02:19,880 --> 00:02:23,000
출력의 깊이는 6이라는 점입니다.

61
00:02:23,000 --> 00:02:25,880
만약 두 번째 컨볼루션 층이 있다면,

62
00:02:25,880 --> 00:02:27,580
그 층의 필터는

63
00:02:27,580 --> 00:02:31,400
이 6개의 활성화 맵 전체를 대상으로 해야 하므로

64
00:02:31,400 --> 00:02:34,300
다음 층의 깊이는 6이 됩니다.

65
00:02:34,300 --> 00:02:36,500
네, 그리고 두 번째 층은

66
00:02:36,500 --> 00:02:38,770
컨볼루션 층보다 훨씬 간단한

67
00:02:38,770 --> 00:02:40,310
풀링 층입니다.

68
00:02:40,310 --> 00:02:43,480
여기서는 여전히 2x2 필터를 스트라이드

69
00:02:43,480 --> 00:02:47,330
2로 이미지 위를 슬라이딩하는 방식입니다.

70
00:02:47,330 --> 00:02:48,290
그래서 우리는 건너뛰고 있습니다.

71
00:02:48,290 --> 00:02:49,873
모든 위치를 다 처리하는 것은 아닙니다.

72
00:02:49,873 --> 00:02:51,410
여기에는 max pooling이 있습니다.

73
00:02:51,410 --> 00:02:53,665
각 영역에서 최대값을 취해서 여기서

74
00:02:53,665 --> 00:02:55,040
그 값을 얻는 거죠.

75
00:02:55,040 --> 00:02:57,490
또는 average pooling을 할 수도 있습니다.

76
00:02:57,490 --> 00:02:59,600
이 두 가지 모두 흔히 사용됩니다, 라고 말씀드릴 수 있겠습니다.

77
00:02:59,600 --> 00:03:01,083
그리고 아키텍처에 따라, 새

78
00:03:01,083 --> 00:03:02,500
아키텍처를 만든다면 두 가지를

79
00:03:02,500 --> 00:03:03,970
모두 시도해보고 어느 쪽이

80
00:03:03,970 --> 00:03:05,690
더 성능이 좋은지 확인할 겁니다.

81
00:03:05,690 --> 00:03:07,870
기본 아이디어는 이미지의

82
00:03:07,870 --> 00:03:11,710
높이와 너비 차원에서 정보를 통합하는 것입니다.

83
00:03:11,710 --> 00:03:15,340
좋습니다, 지금까지 이 강의에서 기본적으로

84
00:03:15,340 --> 00:03:17,890
위쪽 행에 있는 합성곱 층,

85
00:03:17,890 --> 00:03:20,290
풀링 층, 그리고 완전 연결

86
00:03:20,290 --> 00:03:21,470
층을 다뤘습니다.

87
00:03:21,470 --> 00:03:22,887
이들은 신경망 강의에서

88
00:03:22,887 --> 00:03:24,940
처음 다룬 층들로, 기본적으로

89
00:03:24,940 --> 00:03:27,370
하나의 행렬 곱셈과 활성화 함수가

90
00:03:27,370 --> 00:03:29,000
이어지는 구조입니다.

91
00:03:29,000 --> 00:03:31,190
이 강의의 나머지 부분에서는

92
00:03:31,190 --> 00:03:32,890
CNN에서

93
00:03:32,890 --> 00:03:36,220
보통 사용되는 나머지 층들, 즉 정규화

94
00:03:36,220 --> 00:03:39,440
층과 드롭아웃에 대해 이야기하겠습니다.

95
00:03:39,440 --> 00:03:42,760
드롭아웃은 모델 아키텍처

96
00:03:42,760 --> 00:03:44,560
자체에서 사용하는 정규화

97
00:03:44,560 --> 00:03:45,860
기법입니다.

98
00:03:45,860 --> 00:03:48,358
마지막으로 활성화 함수들을 다시

99
00:03:48,358 --> 00:03:50,650
살펴보면서, 역사적으로 그리고 현대

100
00:03:50,650 --> 00:03:55,030
딥러닝에서 가장 흔히 쓰이는 활성화 함수들을 알려드리겠습니다.

101
00:03:55,030 --> 00:03:57,410
정규화 층부터 시작하자면,

102
00:03:57,410 --> 00:03:59,410
기본 아이디어는 입력

103
00:03:59,410 --> 00:04:01,900
데이터의 평균과 표준편차

104
00:04:01,900 --> 00:04:05,080
같은 통계량을 계산해서 데이터를

105
00:04:05,080 --> 00:04:06,710
정규화하는 것입니다.

106
00:04:06,710 --> 00:04:10,390
그리고 모델이 그 시점에서 학습하기에

107
00:04:10,390 --> 00:04:13,960
최적의 분포가 무엇인지 배우게

108
00:04:13,960 --> 00:04:15,020
됩니다.

109
00:04:15,020 --> 00:04:18,130
구체적으로는 학습된 평균과 표준편차를

110
00:04:18,130 --> 00:04:19,930
이용해 입력

111
00:04:19,930 --> 00:04:24,290
데이터를 스케일하고 이동시키는 파라미터를 학습합니다.

112
00:04:24,290 --> 00:04:27,640
모든 정규화 층이 작동하는 방식은 두

113
00:04:27,640 --> 00:04:28,700
단계입니다.

114
00:04:28,700 --> 00:04:31,120
첫 번째는 입력 데이터를 단위 가우시안,

115
00:04:31,120 --> 00:04:34,790
즉 평균 0, 표준편차 1로 정규화하는 것입니다.

116
00:04:34,790 --> 00:04:36,920
그 다음에 스케일과 이동을 적용합니다.

117
00:04:36,920 --> 00:04:40,828
즉, 표준편차를 키우거나 줄이기 위해 어떤 값으로 곱하고,

118
00:04:40,828 --> 00:04:42,370
평균 위치를 바꾸기

119
00:04:42,370 --> 00:04:43,880
위해 이동시키는 거죠.

120
00:04:43,880 --> 00:04:46,280
모든 정규화 층이 이 방식을 쓰지만,

121
00:04:46,280 --> 00:04:47,920
차이가 나는 부분은

122
00:04:47,920 --> 00:04:49,520
통계량을 계산하는 방법입니다.

123
00:04:49,520 --> 00:04:52,330
즉, 평균과 표준편차를 어떻게 계산하고,

124
00:04:52,330 --> 00:04:53,840
계산된 통계량을

125
00:04:53,840 --> 00:04:56,900
어떤 값에 적용하느냐가 다릅니다. 하지만

126
00:04:56,900 --> 00:04:59,690
모두 이 큰 틀의 과정을 따릅니다.

127
00:04:59,690 --> 00:05:03,410
가장 흔히 쓰이는 정규화 층인

128
00:05:03,410 --> 00:05:06,590
LayerNorm에 대해

129
00:05:06,590 --> 00:05:07,920
이야기하겠습니다.

130
00:05:07,920 --> 00:05:11,120
특히 트랜스포머에서 매우 자주 사용됩니다.

131
00:05:11,120 --> 00:05:13,970
입력 데이터 x가 있다고 가정하면,

132
00:05:13,970 --> 00:05:16,500
배치 크기가 n입니다.

133
00:05:16,500 --> 00:05:19,110
즉, 모델에 n개의 샘플이 들어오는 거죠.

134
00:05:19,110 --> 00:05:22,130
각 샘플은 차원 D의 벡터입니다.

135
00:05:22,130 --> 00:05:25,250
LayerNorm은 각

136
00:05:25,250 --> 00:05:29,180
샘플별로 평균과 표준편차를 계산합니다.

137
00:05:29,180 --> 00:05:29,940
샘플별로 평균과 표준편차를 계산합니다.

138
00:05:29,940 --> 00:05:32,270
즉, 깊이 방향,

139
00:05:32,270 --> 00:05:35,970
차원 D를 따라 평균과 표준편차를

140
00:05:35,970 --> 00:05:37,770
구하는 겁니다.

141
00:05:37,770 --> 00:05:40,920
그 다음 학습 가능한 파라미터를 학습합니다.

142
00:05:40,920 --> 00:05:42,530
이 파라미터들은

143
00:05:42,530 --> 00:05:45,950
경사 하강법으로 학습되며, 각 샘플에

144
00:05:45,950 --> 00:05:47,460
적용됩니다.

145
00:05:47,460 --> 00:05:49,490
이렇게 각

146
00:05:49,490 --> 00:05:51,900
샘플별로 평균과 표준편차를

147
00:05:51,900 --> 00:05:54,490
계산한 후,

148
00:05:54,490 --> 00:05:59,930
학습된 스케일과 이동 파라미터를 적용합니다.

149
00:05:59,930 --> 00:06:00,430
학습된 스케일과 이동 파라미터를 적용합니다.

150
00:06:00,430 --> 00:06:03,780
입력 데이터에서 평균을 빼고 표준편차로

151
00:06:03,780 --> 00:06:06,370
나누어 정규화한 뒤,

152
00:06:06,370 --> 00:06:09,420
곱셈으로 스케일을 적용하고 이동을

153
00:06:09,420 --> 00:06:10,870
하는 겁니다.

154
00:06:10,870 --> 00:06:13,870
이것이 LayerNorm의 기본 아이디어입니다.

155
00:06:13,870 --> 00:06:19,740
큰 틀에서 보면, 이 모든 다양한 정규화 레이어들은 매우

156
00:06:19,740 --> 00:06:22,990
유사한 것을 계산하지만, 주요

157
00:06:22,990 --> 00:06:25,020
차이점은 평균과

158
00:06:25,020 --> 00:06:28,600
표준편차를 어떻게 계산하느냐입니다.

159
00:06:28,600 --> 00:06:30,840
이것은 Group Normalization이라는

160
00:06:30,840 --> 00:06:33,960
논문에서 나온 아주 좋은 시각화로,

161
00:06:33,960 --> 00:06:35,790
새로운 정규화 방식을 소개합니다.

162
00:06:35,790 --> 00:06:38,095
요즘에는 그렇게 흔히 쓰이지는 않지만,

163
00:06:38,095 --> 00:06:39,720
이 그림은 다양한

164
00:06:39,720 --> 00:06:42,510
정규화 레이어들이 어떻게 다른지 직관을 얻기에

165
00:06:42,510 --> 00:06:43,540
정말 좋습니다.

166
00:06:43,540 --> 00:06:47,610
LayerNorm의 경우, 제가 벡터를 정규화하는

167
00:06:47,610 --> 00:06:50,170
아주 단순한 경우를

168
00:06:50,170 --> 00:06:52,990
설명했지만, 합성곱 신경망에서는

169
00:06:52,990 --> 00:06:56,520
채널 차원, 즉 깊이와 이미지의 높이 및

170
00:06:56,520 --> 00:06:58,770
너비, 즉 공간 차원이

171
00:06:58,770 --> 00:06:59,650
있습니다.

172
00:06:59,650 --> 00:07:02,890
LayerNorm은 각 샘플마다

173
00:07:02,890 --> 00:07:04,780
별도로 처리하면서

174
00:07:04,780 --> 00:07:07,410
모든 채널, 높이, 너비에

175
00:07:07,410 --> 00:07:10,150
걸쳐 평균을 계산합니다.

176
00:07:10,150 --> 00:07:14,970
이 그림을 다시 보면, 모든 값에

177
00:07:14,970 --> 00:07:18,876
대해 하나의 평균과 하나의

178
00:07:18,876 --> 00:07:22,260
표준편차를 계산하는

179
00:07:22,260 --> 00:07:23,770
셈입니다.

180
00:07:23,770 --> 00:07:26,292
각 입력 데이터 포인트마다 모든

181
00:07:26,292 --> 00:07:28,500
채널과 높이, 너비 차원에 걸쳐

182
00:07:28,500 --> 00:07:31,250
하나의 평균과 하나의 표준편차를

183
00:07:31,250 --> 00:07:31,750
계산합니다.

184
00:07:31,750 --> 00:07:33,900
이것이 LayerNorm이 하는 일입니다.

185
00:07:33,900 --> 00:07:36,630
하지만 통계치를 다르게 계산할 수도 있다고

186
00:07:36,630 --> 00:07:38,050
상상할 수 있습니다.

187
00:07:38,050 --> 00:07:41,470
BatchNorm은 각 채널마다 하나의

188
00:07:41,470 --> 00:07:44,070
평균과 하나의 표준편차를

189
00:07:44,070 --> 00:07:45,100
계산합니다.

190
00:07:45,100 --> 00:07:46,930
그리고 그 채널에만 적용하며,

191
00:07:46,930 --> 00:07:49,510
배치 내 모든 데이터에 대해 평균을 냅니다.

192
00:07:49,510 --> 00:07:52,310
InstanceNorm은 더 세분화되어 있고, GroupNorm도
마찬가지입니다.

193
00:07:52,310 --> 00:07:54,400
이 모든 레이어들은 기본적으로

194
00:07:54,400 --> 00:07:57,100
데이터를 정규화하고 학습 가능한

195
00:07:57,100 --> 00:07:58,868
스케일링과 시프팅

196
00:07:58,868 --> 00:08:01,160
파라미터를 갖는 같은 목적을

197
00:08:01,160 --> 00:08:03,285
추구하지만, 입력 데이터의

198
00:08:03,285 --> 00:08:07,390
서로 다른 부분집합을 사용해 통계치를 계산하기 때문에

199
00:08:07,390 --> 00:08:08,770
방식이 다릅니다.

200
00:08:08,770 --> 00:08:10,490
네, 질문은 LayerNorm에서 각

201
00:08:10,490 --> 00:08:12,740
이미지나 입력 데이터마다 하나의 평균과 하나의

202
00:08:12,740 --> 00:08:14,390
표준편차를 계산하느냐는 것입니다.

203
00:08:14,390 --> 00:08:17,710
네, 각각 별도로 계산합니다. 하지만

204
00:08:17,710 --> 00:08:19,770
BatchNorm은 이 예시에서는

205
00:08:19,770 --> 00:08:21,020
그렇지 않습니다.

206
00:08:21,020 --> 00:08:22,060
네.

207
00:08:22,060 --> 00:08:25,590
BatchNorm의 경우 실제로는 미니 배치 내에서 경사 하강법을

208
00:08:25,590 --> 00:08:26,840
할 때 적용됩니다.

209
00:08:26,840 --> 00:08:28,882
작은 배치의 데이터를 보고 있는 거죠.

210
00:08:28,882 --> 00:08:30,580
그 데이터를 모델에 입력합니다.

211
00:08:30,580 --> 00:08:34,330
배치 내 모든 데이터에 기반해 각

212
00:08:34,330 --> 00:08:37,120
채널별 평균과 표준편차를

213
00:08:37,120 --> 00:08:38,340
계산합니다.

214
00:08:41,075 --> 00:08:43,450
네, 이 다이어그램을 이해할 수

215
00:08:43,450 --> 00:08:46,495
있다면 모든 다른 정규화 레이어가 무엇을

216
00:08:46,495 --> 00:08:49,120
하는지 이해하는 거라고 생각합니다. 강의

217
00:08:49,120 --> 00:08:51,078
후에 완전히 이해가 안

218
00:08:51,078 --> 00:08:54,070
되면 다시 한 번 살펴보면서 파란색으로 음영

219
00:08:54,070 --> 00:08:56,080
처리된 부분이 우리가 통계치를

220
00:08:56,080 --> 00:08:58,810
계산하고 평균과 표준편차를 적용하는

221
00:08:58,810 --> 00:09:00,973
값이라는 걸 꼭 확인해 보세요.

222
00:09:00,973 --> 00:09:02,890
네, 마지막 질문 하나만 하고 다음으로 넘어가겠습니다.

223
00:09:02,890 --> 00:09:05,860
채널에 대해서도 레이어가 같은가요?

224
00:09:05,860 --> 00:09:10,430
여기서 채널은 깊이(depth)를 의미합니다.

225
00:09:10,430 --> 00:09:21,790
즉, 각 공간 위치에서 가지는 값의 개수입니다.

226
00:09:21,790 --> 00:09:23,260
좋습니다.

227
00:09:23,260 --> 00:09:27,260
정규화 레이어에 대해 이야기했죠.

228
00:09:27,260 --> 00:09:29,500
핵심은 통계치를 계산하고

229
00:09:29,500 --> 00:09:33,310
이를 입력 데이터에 적용한 다음, 학습 가능한

230
00:09:33,310 --> 00:09:37,160
스케일과 시프트 파라미터를 적용하는 겁니다.

231
00:09:37,160 --> 00:09:41,150
다음으로 이야기할 레이어는 dropout입니다.

232
00:09:41,150 --> 00:09:44,420
이것은 CNN에서 사용하는 정규화 레이어입니다.

233
00:09:44,420 --> 00:09:46,280
그리고 이것이 여러분이 배워야

234
00:09:46,280 --> 00:09:48,360
할 마지막 레이어로, 이제부터는

235
00:09:48,360 --> 00:09:50,443
사람들이 수년간 만든 다양한

236
00:09:50,443 --> 00:09:53,060
CNN 아키텍처들을 살펴볼 수 있습니다.

237
00:09:53,060 --> 00:09:56,030
드롭아웃의 기본 아이디어는 학습

238
00:09:56,030 --> 00:09:59,630
과정에서 무작위성을 추가하고, 테스트 시에는

239
00:09:59,630 --> 00:10:01,572
이를 제거하는 것입니다.

240
00:10:01,572 --> 00:10:03,530
목표는 모델이 학습 데이터를 배우기

241
00:10:03,530 --> 00:10:06,570
어렵게 만들어서 일반화 성능을 높이는 것입니다.

242
00:10:06,570 --> 00:10:09,350
그래서 이것은 일종의 정규화 방법입니다.

243
00:10:09,350 --> 00:10:12,950
구체적으로는 각 레이어의 순전파

244
00:10:12,950 --> 00:10:15,500
때 일부 출력이나

245
00:10:15,500 --> 00:10:19,440
활성화를 무작위로 0으로 만듭니다.

246
00:10:19,440 --> 00:10:24,000
드롭아웃 레이어의 주요 하이퍼파라미터는

247
00:10:24,000 --> 00:10:25,800
값을

248
00:10:25,800 --> 00:10:28,590
드롭아웃할 확률입니다.

249
00:10:28,590 --> 00:10:32,390
0.5가 가장 흔하고 0.25도

250
00:10:32,390 --> 00:10:34,040
자주 사용됩니다.

251
00:10:34,040 --> 00:10:37,310
즉, 고정된 비율의 값을 무작위로 제거하는

252
00:10:37,310 --> 00:10:37,980
거죠.

253
00:10:37,980 --> 00:10:41,790
그래서 다음 레이어로 전달되는 값들은 0이 됩니다.

254
00:10:41,790 --> 00:10:45,950
따라서 이 값들을 굳이 계산할 필요가 없습니다.

255
00:10:45,950 --> 00:10:50,082
사실 이 시점에서 출력값이 모두 0이기 때문에

256
00:10:50,082 --> 00:10:52,540
마스킹 같은 트릭을 써서 계산을

257
00:10:52,540 --> 00:10:54,700
아예 안 해도 됩니다. 0

258
00:10:54,700 --> 00:10:57,350
곱하기 어떤 값도 0이니까요.

259
00:10:57,350 --> 00:11:01,820
일반적으로 왜 이런 방법이 효과가 있는지 궁금할 수 있습니다.

260
00:11:01,820 --> 00:11:07,660
이건 이론적으로 잘 연구된 것보다는 경험적으로

261
00:11:07,660 --> 00:11:11,900
효과가 입증된 방법인데, 드롭아웃이

262
00:11:11,900 --> 00:11:13,810
하는 일을 이해할

263
00:11:13,810 --> 00:11:16,360
수 있는 몇 가지 관점이

264
00:11:16,360 --> 00:11:17,780
있습니다.

265
00:11:17,780 --> 00:11:21,430
기본적으로 네트워크가 중복된 표현을

266
00:11:21,430 --> 00:11:24,530
갖도록 강제하는 겁니다.

267
00:11:24,530 --> 00:11:27,520
예를 들어, 출력 바로

268
00:11:27,520 --> 00:11:31,340
전 레이어에서 학습하는 특징

269
00:11:31,340 --> 00:11:34,960
리스트가 있고, CNN이 귀,

270
00:11:34,960 --> 00:11:37,510
꼬리, 털, 발톱 같은

271
00:11:37,510 --> 00:11:40,070
특징을 추출한다고

272
00:11:40,070 --> 00:11:43,310
합시다. 모델은 고양이일

273
00:11:43,310 --> 00:11:45,270
확률을 출력하죠.

274
00:11:45,270 --> 00:11:47,690
이때 드롭아웃의 유용한 점은

275
00:11:47,690 --> 00:11:49,910
학습 중 일부 특징이 무작위로

276
00:11:49,910 --> 00:11:51,330
사라지기 때문에,

277
00:11:51,330 --> 00:11:53,540
모델이 특정 특징에

278
00:11:53,540 --> 00:11:56,430
과도하게 의존하지 못하고, 더

279
00:11:56,430 --> 00:11:59,450
넓은 범위의 특징과 출력 클래스

280
00:11:59,450 --> 00:12:02,690
간 대응 관계를 배워야 한다는

281
00:12:02,690 --> 00:12:03,270
겁니다.

282
00:12:03,270 --> 00:12:07,560
즉, 귀가 있고 털이

283
00:12:07,560 --> 00:12:11,450
있으면 항상 고양이라는

284
00:12:11,450 --> 00:12:14,190
식으로 특정 특징

285
00:12:14,190 --> 00:12:17,670
조합에만 집중하지

286
00:12:17,670 --> 00:12:20,468
못하게 합니다.

287
00:12:20,468 --> 00:12:22,010
이렇게 하면

288
00:12:22,010 --> 00:12:25,490
데이터셋 내에서 특정 특징과

289
00:12:25,490 --> 00:12:28,040
출력 클래스 간

290
00:12:28,040 --> 00:12:30,680
강한 상관관계가 있어도,

291
00:12:30,680 --> 00:12:31,200
강한 상관관계가 있어도,

292
00:12:31,200 --> 00:12:33,920
드롭아웃 덕분에 모델이 학습

293
00:12:33,920 --> 00:12:35,383
단계에서 항상

294
00:12:35,383 --> 00:12:36,800
그 조합을

295
00:12:36,800 --> 00:12:40,410
보지 못해 일반화 성능이 좋아집니다.

296
00:12:40,410 --> 00:12:43,000
이것은 고양이 예시입니다.

297
00:12:43,000 --> 00:12:46,260
만약 나무(tree)라면 어떤 특징을

298
00:12:46,260 --> 00:12:48,880
드롭아웃할지 어떻게 정할까요?

299
00:12:48,880 --> 00:12:52,600
드롭아웃은 완전히 무작위이기 때문에

300
00:12:52,600 --> 00:12:54,580
선택을 하지 않습니다.

301
00:12:54,580 --> 00:12:59,520
즉, 매 학습 단계마다 50%의 특징이

302
00:12:59,520 --> 00:13:01,800
무작위로 0이 됩니다.

303
00:13:01,800 --> 00:13:04,410
선택을 안 해도 된다는

304
00:13:04,410 --> 00:13:08,785
점이 장점이지만, 완전히 무작위라는 뜻입니다.

305
00:13:08,785 --> 00:13:10,410
꼬리와 발톱 같은

306
00:13:10,410 --> 00:13:13,460
일부 특징만 본다면 모델이 어떻게 알까요?

307
00:13:13,460 --> 00:13:15,960
사실 학습 데이터에서는 일부 특징만

308
00:13:15,960 --> 00:13:18,127
보기 때문에 성능이 떨어집니다.

309
00:13:18,127 --> 00:13:20,190
즉, 모든 정보를

310
00:13:20,190 --> 00:13:22,680
갖지 못해 학습 성능은

311
00:13:22,680 --> 00:13:23,830
나빠지지만,

312
00:13:23,830 --> 00:13:25,205
테스트 시에는

313
00:13:25,205 --> 00:13:27,510
드롭아웃이 없기

314
00:13:27,510 --> 00:13:30,670
때문에 더 좋은 성능을 냅니다.

315
00:13:30,670 --> 00:13:32,830
학습 시에는 최악의

316
00:13:32,830 --> 00:13:35,080
성능, 테스트 시에는

317
00:13:35,080 --> 00:13:38,370
더 좋은 성능을 내는

318
00:13:38,370 --> 00:13:39,230
개념입니다.

319
00:13:39,230 --> 00:13:41,210
마지막으로, 질문 전에 설명했어야 할 부분인데, 테스트 시에는 드롭아웃을

320
00:13:41,210 --> 00:13:41,710
적용하지 않습니다.

321
00:13:41,710 --> 00:13:45,100
즉, 학습 단계에서만 무작위성을 추가하고,

322
00:13:45,100 --> 00:13:49,000
테스트 단계에서는 출력 활성화를 전혀

323
00:13:49,000 --> 00:13:50,480
마스킹하지 않습니다.

324
00:13:50,480 --> 00:13:54,340
중요한 점은 학습 시 50%의

325
00:13:54,340 --> 00:13:57,388
활성화를 드롭아웃했다면, 테스트

326
00:13:57,388 --> 00:13:58,930
시에는 각

327
00:13:58,930 --> 00:14:04,520
레이어에 50% 더 많은 값이 입력된다는 겁니다.

328
00:14:04,520 --> 00:14:06,920
이것을 스케일하지 않으면 문제가 발생할 수 있습니다.

329
00:14:06,920 --> 00:14:09,370
그래서 해야 할 일은 dropout

330
00:14:09,370 --> 00:14:14,440
확률을 곱해서 각 층에 들어오는 값의 크기가 학습과 테스트

331
00:14:14,440 --> 00:14:17,330
시에 유지되도록 하는 것입니다.

332
00:14:17,330 --> 00:14:19,333
만약 50% 값을

333
00:14:19,333 --> 00:14:21,500
dropout하고 테스트 시에 모두

334
00:14:21,500 --> 00:14:24,580
포함하면, 이전보다 훨씬 큰 입력

335
00:14:24,580 --> 00:14:27,970
크기를 보게 되어 이상한 동작이 발생합니다.

336
00:14:27,970 --> 00:14:30,910
그렇다면 역전파는 어떻게 될까요?

337
00:14:30,910 --> 00:14:34,700
역전파에서 0으로 된 값이

338
00:14:34,700 --> 00:14:40,620
있으면, 그 경로를 더 이상 따라갈 필요가

339
00:14:40,620 --> 00:14:41,920
없습니다.

340
00:14:41,920 --> 00:14:43,003
ReLU와 매우 비슷합니다.

341
00:14:43,003 --> 00:14:46,110
그 지점에서 값이 0이면, 그라디언트도

342
00:14:46,110 --> 00:14:47,380
0이 됩니다.

343
00:14:47,380 --> 00:14:54,120
따라서 계산 그래프의 더 뒤쪽에서는 그라디언트가

344
00:14:54,120 --> 00:14:56,790
계산되지 않습니다.

345
00:14:56,790 --> 00:15:01,410
특정 값이나 활성화를 dropout하면,

346
00:15:01,410 --> 00:15:03,660
그 활성화에 연결된

347
00:15:03,660 --> 00:15:06,780
가중치는 경사 하강법 중에

348
00:15:06,780 --> 00:15:08,820
업데이트되지 않습니다.

349
00:15:08,820 --> 00:15:10,560
질문을 다시

350
00:15:10,560 --> 00:15:11,590
해보겠습니다.

351
00:15:11,590 --> 00:15:13,030
테스트 시에는 무엇을 하고 있나요?

352
00:15:13,030 --> 00:15:17,380
테스트 시에는 모든 출력 활성화를 사용합니다.

353
00:15:17,380 --> 00:15:18,880
더 이상 dropout하지

354
00:15:18,880 --> 00:15:21,430
않지만 dropout 확률로 스케일링해야 합니다.

355
00:15:21,430 --> 00:15:23,340
그래서 모든 출력을 사용하므로

356
00:15:23,340 --> 00:15:26,200
각 출력 활성화에 p 값을 곱합니다.

357
00:15:26,200 --> 00:15:29,640
그렇지 않으면 테스트 시 각

358
00:15:29,640 --> 00:15:32,880
노드가 학습 때보다 훨씬 많은

359
00:15:32,880 --> 00:15:35,290
입력을 받게 됩니다.

360
00:15:35,290 --> 00:15:37,780
입력 크기를 유지하려면

361
00:15:37,780 --> 00:15:41,360
p 값을 곱해야 합니다.

362
00:15:41,360 --> 00:15:43,505
이렇게 하면 분산도 동일하게

363
00:15:43,505 --> 00:15:44,630
유지됩니다.

364
00:15:44,630 --> 00:15:47,500
이 방법이 아주 잘 작동합니다.

365
00:15:47,500 --> 00:15:50,300
그렇다면 이미지에 노이즈를 추가하는 것은 어떨까요?

366
00:15:50,300 --> 00:15:51,925
답은 예이고, 다음 슬라이드에서

367
00:15:51,925 --> 00:15:53,420
그 방법을 다루겠습니다.

368
00:15:53,420 --> 00:15:57,400
네, 이미지에 노이즈를 추가하는 것은 좋은 아이디어입니다.

369
00:15:57,400 --> 00:15:59,438
여기 구체적인 코드가 있습니다.

370
00:15:59,438 --> 00:16:01,730
이미 언급했으니 자세히

371
00:16:01,730 --> 00:16:06,790
설명하지 않겠지만, 여기서 p 비율만큼 활성화를

372
00:16:06,790 --> 00:16:11,220
dropout하고 테스트 시에 곱합니다.

373
00:16:11,220 --> 00:16:13,558
다음 주제는 활성화 함수입니다.

374
00:16:13,558 --> 00:16:15,850
여러분은 이제 CNN의 주요 층들을

375
00:16:15,850 --> 00:16:17,057
모두 배웠습니다.

376
00:16:17,057 --> 00:16:19,390
이제 활성화 함수에 대해

377
00:16:19,390 --> 00:16:20,120
이야기하겠습니다.

378
00:16:20,120 --> 00:16:23,020
활성화 함수의 목적은 모델에

379
00:16:23,020 --> 00:16:25,910
비선형성을 도입하는 것입니다.

380
00:16:25,910 --> 00:16:29,452
현재 컨볼루션 연산자와

381
00:16:29,452 --> 00:16:30,910
완전 연결층은

382
00:16:30,910 --> 00:16:35,893
활성화 없이 모두 곱셈과 덧셈으로

383
00:16:35,893 --> 00:16:37,310
이루어진

384
00:16:37,310 --> 00:16:39,623
선형 연산입니다.

385
00:16:39,623 --> 00:16:41,540
활성화 함수는 비선형성을

386
00:16:41,540 --> 00:16:43,230
추가하는 역할을 합니다.

387
00:16:43,230 --> 00:16:49,100
역사적으로 sigmoid가 많이 사용되었지만,

388
00:16:49,100 --> 00:16:52,410
오늘날 사용하지 않는

389
00:16:52,410 --> 00:16:55,110
중요한 이유가 있습니다.

390
00:16:55,110 --> 00:16:57,390
sigmoid 함수는 이렇게 생겼습니다.

391
00:16:57,390 --> 00:17:01,320
슬라이드 오른쪽 위에 식이 있습니다.

392
00:17:01,320 --> 00:17:06,470
주요 문제는 경험적으로 여러 층의 sigmoid를

393
00:17:06,470 --> 00:17:08,819
거치면 역전파 시

394
00:17:08,819 --> 00:17:10,670
그라디언트가 점점 작아진다는

395
00:17:10,670 --> 00:17:12,105
점입니다.

396
00:17:12,105 --> 00:17:13,730
그래서 끝에서부터 시작하면,

397
00:17:13,730 --> 00:17:15,329
그래디언트의 크기가 꽤 큽니다.

398
00:17:15,329 --> 00:17:18,030
그리고 여러 층의 역전파를 거치면서 모델의

399
00:17:18,030 --> 00:17:20,488
초기 초반 층으로 갈수록, 이

400
00:17:20,488 --> 00:17:22,280
과정을 할수록 그래디언트가

401
00:17:22,280 --> 00:17:23,490
점점 작아집니다.

402
00:17:23,490 --> 00:17:27,109
그래서 이 질문을 수업에 열어보겠습니다.

403
00:17:27,109 --> 00:17:30,480
이 현상은 sigmoid에서는 나타나지 않습니다.

404
00:17:30,480 --> 00:17:33,970
그럼 그래프에서 sigmoid가 정말 작은

405
00:17:33,970 --> 00:17:36,870
그래디언트를 가지는 구간은 어디일까요?

406
00:17:36,870 --> 00:17:39,573
네, 아주 음수이거나 아주 양수인 값이 맞습니다.

407
00:17:39,573 --> 00:17:40,990
이것은 사실 큰 문제입니다.

408
00:17:40,990 --> 00:17:42,365
그래프에서 시각적으로도 그래디언트가

409
00:17:42,365 --> 00:17:44,070
매우 평평한 것을 볼 수 있습니다.

410
00:17:44,070 --> 00:17:45,790
여기서 미분을 취하는 거죠.

411
00:17:45,790 --> 00:17:46,920
매우 작습니다.

412
00:17:46,920 --> 00:17:49,050
그래서 기본적으로 음의 무한대부터

413
00:17:49,050 --> 00:17:52,160
양의 무한대까지 거의 모든 입력 공간에서 그래디언트가

414
00:17:52,160 --> 00:17:53,410
매우 작습니다.

415
00:17:53,410 --> 00:17:55,285
그리고 중간의 좁은

416
00:17:55,285 --> 00:17:58,540
구간에서만 0이 아닌 값을 가지므로 양쪽

417
00:17:58,540 --> 00:18:00,750
극단에서는 매우 빠르게

418
00:18:00,750 --> 00:18:02,230
0에 접근합니다.

419
00:18:02,230 --> 00:18:05,400
즉, sigmoid에 들어오는 값이

420
00:18:05,400 --> 00:18:07,140
매우 크거나 매우

421
00:18:07,140 --> 00:18:11,190
작으면 그래디언트가 매우 작아진다는 의미입니다.

422
00:18:11,190 --> 00:18:13,290
이것이 ReLU가 매우 인기를

423
00:18:13,290 --> 00:18:15,930
끈 주요 이유 중 하나입니다. 왜냐하면

424
00:18:15,930 --> 00:18:19,410
양수 영역에서는 이런 현상이 없기 때문입니다.

425
00:18:19,410 --> 00:18:24,120
여기서는 미분값이 1이지만, 실제로는

426
00:18:24,120 --> 00:18:27,240
왼쪽에 그래디언트가

427
00:18:27,240 --> 00:18:31,910
0인 평평한 구간이 여전히 존재합니다.

428
00:18:31,910 --> 00:18:36,360
그래서 이제 입력 영역의 절반 정도가 이런 상태인 셈입니다.

429
00:18:36,360 --> 00:18:38,570
여기서는 1의 그래디언트와 0의 그래디언트가

430
00:18:38,570 --> 00:18:41,060
반반 나오는데, 이는 거의 대부분이 0이거나

431
00:18:41,060 --> 00:18:43,940
0에 아주 가까운 값이고 중간에 작은 영역만 있는 경우보다

432
00:18:43,940 --> 00:18:44,880
훨씬 낫습니다.

433
00:18:44,880 --> 00:18:47,100
그래서 실제로는 이런 방식이 더 잘 작동합니다.

434
00:18:47,100 --> 00:18:48,830
또한, 0과 입력값 중 최대값을

435
00:18:48,830 --> 00:18:51,710
구하는 게 시그모이드 함수를 계산하는 것보다

436
00:18:51,710 --> 00:18:52,830
훨씬 저렴합니다.

437
00:18:52,830 --> 00:18:56,360
이 두 가지 이유로 ReLU가 매우 인기를 끌게 되었습니다.

438
00:18:56,360 --> 00:18:59,810
하지만 여전히 음수 입력에 대해

439
00:18:59,810 --> 00:19:03,650
0 그래디언트가 나오는 문제가 있어서,

440
00:19:03,650 --> 00:19:07,670
최근에는 0 근처에서 활성화 함수가

441
00:19:07,670 --> 00:19:12,710
평평하지 않은 구간을 갖도록 해서 이 문제를

442
00:19:12,710 --> 00:19:17,400
피하는 활성화 함수들이 인기를 얻고 있습니다.

443
00:19:17,400 --> 00:19:20,990
이것이 GELU이고, SELU도 있는데 슬라이드에서

444
00:19:20,990 --> 00:19:23,490
보여드리겠지만 수식은 다루지 않겠습니다.

445
00:19:23,490 --> 00:19:24,860
두 함수는 매우 비슷하게 생겼습니다.

446
00:19:24,860 --> 00:19:31,110
기본 아이디어는 ReLU에서 0에서 1로 급격히 바뀌는

447
00:19:31,110 --> 00:19:35,680
도함수의 불연속점을 부드럽게 만드는 겁니다.

448
00:19:35,680 --> 00:19:41,680
ReLU는 매우 날카롭고 불연속적인 함수지만, GELU의 좋은 점은

449
00:19:41,680 --> 00:19:44,070
여기서 0이 아닌 그래디언트를

450
00:19:44,070 --> 00:19:46,330
실제로 갖는다는 겁니다.

451
00:19:46,330 --> 00:19:49,960
그리고 x가 무한대나 음의 무한대로 갈 때는

452
00:19:49,960 --> 00:19:52,840
ReLU에 수렴하지만, 중간 구간에서는

453
00:19:52,840 --> 00:19:55,510
더 부드러운 동작을 보입니다.

454
00:19:55,510 --> 00:19:57,330
특히 GELU가 계산하는

455
00:19:57,330 --> 00:20:01,360
것은 Gaussian error linear unit입니다.

456
00:20:01,360 --> 00:20:05,250
이것은 가우시안 정규분포의

457
00:20:05,250 --> 00:20:06,940
누적분포함수입니다.

458
00:20:06,940 --> 00:20:09,610
가우시안 곡선 아래 면적을 생각하면, 이것이

459
00:20:09,610 --> 00:20:11,740
임의의 x에서의 phi(x) 값입니다.

460
00:20:11,740 --> 00:20:14,370
매우 음수인 값에서는 0에

461
00:20:14,370 --> 00:20:16,720
가까운 값을 가지므로 ReLU의

462
00:20:16,720 --> 00:20:19,570
0에 수렴하는 이유입니다.

463
00:20:19,570 --> 00:20:22,230
매우 큰 양수 값에서는 1에 매우

464
00:20:22,230 --> 00:20:24,490
가까워져서 곡선 아래 면적이 되고,

465
00:20:24,490 --> 00:20:26,530
따라서 x에 수렴합니다.

466
00:20:26,530 --> 00:20:28,162
이것이 GELU입니다.

467
00:20:28,162 --> 00:20:30,370
이 함수는 이런 좋은 특성들을 가지고 있고,

468
00:20:30,370 --> 00:20:31,870
극한에서는 ReLU로 수렴합니다.

469
00:20:31,870 --> 00:20:35,740
그리고 이것이 오늘날 트랜스포머에서 주로

470
00:20:35,740 --> 00:20:38,380
사용하는 활성화 함수입니다.

471
00:20:38,380 --> 00:20:40,900
모두를 보면, 조금 흐릿하게 보면, 많은 함수들이

472
00:20:40,900 --> 00:20:41,597
비슷해 보입니다.

473
00:20:41,597 --> 00:20:43,430
기본 아이디어는 비교적 평평한 형태입니다.

474
00:20:43,430 --> 00:20:49,870
그리고 극한에서는 f(x) = x에 접근하여

475
00:20:49,870 --> 00:20:53,020
선형 함수가 됩니다.

476
00:20:53,020 --> 00:20:56,410
SELU는 사실 x에 sigmoid(x)를

477
00:20:56,410 --> 00:20:59,170
곱한 형태로, 매우 음수일

478
00:20:59,170 --> 00:21:02,450
때는 0에 가깝고 매우 양수일 때는

479
00:21:02,450 --> 00:21:05,150
1에 가까운 특성을 가집니다.

480
00:21:05,150 --> 00:21:08,500
그래서 이것은 여기서 phi로 나타낸 단위

481
00:21:08,500 --> 00:21:11,128
가우시안의 누적 분포 함수와 비슷합니다.

482
00:21:11,128 --> 00:21:12,670
그래서 모양도 실제로 매우

483
00:21:12,670 --> 00:21:13,795
비슷하게 보이는 이유입니다.

484
00:21:17,236 --> 00:21:20,207
그렇다면 이런 활성화 함수들이 CNN에서 어디에 사용되는지

485
00:21:20,207 --> 00:21:21,790
궁금할 수 있는데, 일반적인

486
00:21:21,790 --> 00:21:24,080
답은 선형 연산자 뒤에 위치한다는 겁니다.

487
00:21:24,080 --> 00:21:27,695
즉, 피드포워드 또는 선형 레이어, 완전

488
00:21:27,695 --> 00:21:29,070
연결 레이어라고

489
00:21:29,070 --> 00:21:31,210
부르는 같은 레이어 뒤에,

490
00:21:31,210 --> 00:21:34,740
행렬 곱셈 다음에 활성화 함수를 둡니다.

491
00:21:34,740 --> 00:21:37,380
또는 합성곱 레이어가 있다면,

492
00:21:37,380 --> 00:21:39,305
거의 항상 합성곱

493
00:21:39,305 --> 00:21:40,680
레이어 뒤,

494
00:21:40,680 --> 00:21:45,300
이런 선형 레이어 뒤에 활성화 함수를 둡니다.

495
00:21:45,300 --> 00:21:47,850
자, 이제 CNN의 모든 구성 요소에

496
00:21:47,850 --> 00:21:49,668
대해 다 배웠습니다.

497
00:21:49,668 --> 00:21:51,210
이제 이들을 어떻게

498
00:21:51,210 --> 00:21:53,280
조합하는지, 그리고 사람들이 어떻게

499
00:21:53,280 --> 00:21:56,010
최첨단 합성곱 신경망 아키텍처를

500
00:21:56,010 --> 00:21:58,650
만들었는지 몇 가지 예를 보여드리겠습니다.

501
00:21:58,650 --> 00:22:00,930
이 슬라이드는 정말 멋진데, 두 가지

502
00:22:00,930 --> 00:22:02,530
다른 값을 플롯하기 때문입니다.

503
00:22:02,530 --> 00:22:04,570
한쪽에는 오류율이 있는데, 파란

504
00:22:04,570 --> 00:22:06,105
막대그래프로 나타납니다.

505
00:22:06,105 --> 00:22:08,730
이것은 시간에 따른 것으로, 사람들이 ImageNet에서

506
00:22:08,730 --> 00:22:09,940
훈련한 다양한 모델들입니다.

507
00:22:09,940 --> 00:22:12,060
그리고 주황색 삼각형은

508
00:22:12,060 --> 00:22:16,330
모델이 가진 레이어 수를 나타냅니다.

509
00:22:16,330 --> 00:22:17,940
같은 시점에서

510
00:22:17,940 --> 00:22:21,420
오류율이 크게 떨어지고, 인간 성능을

511
00:22:21,420 --> 00:22:24,190
처음으로 넘었을 때, 레이어

512
00:22:24,190 --> 00:22:27,710
수가 크게 증가한 것을 볼 수 있습니다.

513
00:22:27,710 --> 00:22:29,753
오늘 수업에서 어떻게 이런

514
00:22:29,753 --> 00:22:31,420
성과를 냈는지,

515
00:22:31,420 --> 00:22:35,920
그리고 어떤 설계 도전과 목표가 있었는지 살펴보겠습니다.

516
00:22:35,920 --> 00:22:38,590
역사적으로 AlexNet은 ImageNet에서 정말

517
00:22:38,590 --> 00:22:41,180
잘 작동한 첫 번째 CNN 기반 논문이었습니다.

518
00:22:41,180 --> 00:22:43,593
GPU를 사용해 훈련할 수 있었죠.

519
00:22:43,593 --> 00:22:45,260
이전 강의에서

520
00:22:45,260 --> 00:22:48,670
다뤘으니 AlexNet의 역사적 관점에

521
00:22:48,670 --> 00:22:51,280
대해서는 자세히 다루지 않겠습니다만,

522
00:22:51,280 --> 00:22:55,600
2010년대에 매우 표준적이고 널리 사용된

523
00:22:55,600 --> 00:22:59,300
아키텍처인 VGG와 비교해보고 싶습니다.

524
00:22:59,300 --> 00:23:05,350
두 CNN 아키텍처를 나란히 플롯할 수 있을 것

525
00:23:05,350 --> 00:23:06,380
같습니다.

526
00:23:06,380 --> 00:23:09,910
일반적으로 AI에서는 모델 아키텍처를 블록

527
00:23:09,910 --> 00:23:13,090
다이어그램으로 그리는데, 각 블록은 서로

528
00:23:13,090 --> 00:23:15,910
다른 레이어나 여러 레이어가 쌓인

529
00:23:15,910 --> 00:23:17,480
그룹을 나타냅니다.

530
00:23:17,480 --> 00:23:19,540
이것은 처음 봤을 때

531
00:23:19,540 --> 00:23:22,000
일반적인 차이를 직관적으로 이해하는

532
00:23:22,000 --> 00:23:23,700
데 도움이 됩니다.

533
00:23:23,700 --> 00:23:26,550
여기서 주황색 블록들은 모두 3x3

534
00:23:26,550 --> 00:23:28,670
합성곱 레이어입니다.

535
00:23:28,670 --> 00:23:30,170
즉, 3x3 크기의

536
00:23:30,170 --> 00:23:33,980
필터가 슬라이딩하는 합성곱 레이어입니다.

537
00:23:33,980 --> 00:23:36,620
스트라이드는 1로, 이미지의 모든

538
00:23:36,620 --> 00:23:39,570
위치를 방문하며 건너뛰지 않습니다.

539
00:23:39,570 --> 00:23:42,020
그리고 가장자리에는 패딩

540
00:23:42,020 --> 00:23:47,400
1을 추가해서 합성곱을 해도 크기가 줄어들지 않게 합니다.

541
00:23:47,400 --> 00:23:52,460
또한 여기저기에 맥스 풀링 레이어도 추가합니다.

542
00:23:52,460 --> 00:23:52,980
또한 여기저기에 맥스 풀링 레이어도 추가합니다.

543
00:23:52,980 --> 00:23:57,320
그리고 모든 풀링 레이어 뒤에는 4,096

544
00:23:57,320 --> 00:24:01,130
차원의 완전 연결 레이어 두 세트와 1,

545
00:24:01,130 --> 00:24:05,940
000 차원의 완전 연결 레이어가 이어집니다.

546
00:24:05,940 --> 00:24:08,810
마지막에 1,000이 있는 이유는 ImageNet이 1,000개의

547
00:24:08,810 --> 00:24:10,890
서로 다른 이미지 카테고리를 가지고 있기 때문입니다.

548
00:24:10,890 --> 00:24:13,530
그래서 우리는 각 카테고리에 대한 점수가 필요합니다.

549
00:24:13,530 --> 00:24:16,160
최종 레이어는 항상

550
00:24:16,160 --> 00:24:21,170
이미지 분류 문제에서 클래스 수와 같습니다.

551
00:24:21,170 --> 00:24:23,500
그래서 실제로 매우 비슷하게 보인다는 것을 알 수 있습니다.

552
00:24:23,500 --> 00:24:26,130
이는 AlexNet을 확장한 버전으로,

553
00:24:26,130 --> 00:24:28,120
여기에는 더 많은 레이어가 있습니다.

554
00:24:28,120 --> 00:24:31,680
그리고 이제는 두 레이어가 아닌 세

555
00:24:31,680 --> 00:24:34,530
그룹의 컨볼루션을 한 번에

556
00:24:34,530 --> 00:24:37,170
수행한 후 풀링을 합니다.

557
00:24:37,170 --> 00:24:39,210
이 모델들에는 기본적으로

558
00:24:39,210 --> 00:24:42,870
세 가지 유형의 레이어만 있지만, 이전에

559
00:24:42,870 --> 00:24:46,320
시도된 어떤 것보다 훨씬 뛰어난 성능을

560
00:24:46,320 --> 00:24:49,470
보인다는 점이 정말 놀랍습니다.

561
00:24:49,470 --> 00:24:52,638
이것들은 오늘 논의할 가장 간단한 모델들이라고

562
00:24:52,638 --> 00:24:54,180
할 수 있는데, 왜

563
00:24:54,180 --> 00:24:57,490
3x3 컨볼루션을 사용하는지 궁금할 수 있습니다.

564
00:24:57,490 --> 00:24:59,740
이 값을 어떻게 선택했을까요?

565
00:24:59,740 --> 00:25:02,790
사실 3x3을 선택한 데에는

566
00:25:02,790 --> 00:25:08,640
직관적인 이유가 있고, 이들은 3개 또는 4개의 그룹으로

567
00:25:08,640 --> 00:25:10,810
묶어 사용합니다.

568
00:25:10,810 --> 00:25:13,470
여러분께 질문을 드리겠습니다.

569
00:25:13,470 --> 00:25:16,060
유효 수용 영역(effective receptive field)이란
무엇일까요?

570
00:25:16,060 --> 00:25:19,480
지난 시간에 수용 영역에 대해 살펴봤는데,

571
00:25:19,480 --> 00:25:22,610
이는 특정 활성화 맵 값이 입력

572
00:25:22,610 --> 00:25:26,550
이미지의 어느 부분을 참조했는지에 관한 개념입니다.

573
00:25:26,550 --> 00:25:30,020
즉, 모델의 여러 레이어를 거친 후 최종 활성화 맵을 계산하는

574
00:25:30,020 --> 00:25:32,130
데 사용된 입력 값들이 무엇인지입니다.

575
00:25:32,130 --> 00:25:34,130
3개의 3x3 컨볼루션

576
00:25:34,130 --> 00:25:36,980
레이어가 있고, 스트라이드 1로 슬라이딩

577
00:25:36,980 --> 00:25:38,280
필터를 적용할 때,

578
00:25:38,280 --> 00:25:39,980
여기 A3 활성화

579
00:25:39,980 --> 00:25:43,740
맵의 각 값의 유효 수용 영역은 얼마일까요?

580
00:25:43,740 --> 00:25:46,340
이것은 세 번째 레이어 이후의 상태입니다.

581
00:25:46,340 --> 00:25:48,990
여기 한 레이어를 보여드리고 있습니다.

582
00:25:48,990 --> 00:25:52,640
A3의 각 값은 A2에서 3x3 격자 값을 보고

583
00:25:52,640 --> 00:25:54,810
계산된다는 것을 알 수 있습니다.

584
00:25:54,810 --> 00:25:57,110
그리고 A2의 각 값도 마찬가지로 A1에서 3x3

585
00:25:57,110 --> 00:25:58,530
격자라고 생각할 수 있습니다.

586
00:25:58,530 --> 00:26:01,680
그리고 그 각각도 입력에서 3x3 격자입니다.

587
00:26:01,680 --> 00:26:05,420
잠시 이 부분을 생각해 보시죠.

588
00:26:05,420 --> 00:26:09,275
다음 레이어를 보면 도움이 될 수도 있습니다.

589
00:26:09,275 --> 00:26:11,400
이걸 시각화하는 게 정말 도움이 됩니다.

590
00:26:11,400 --> 00:26:15,410
A1에서 각 모서리는 새로운

591
00:26:15,410 --> 00:26:21,030
3x3 격자에서 계산되고 있습니다.

592
00:26:21,030 --> 00:26:26,290
입력에서 이 전체 사각형의 크기는 얼마일까요?

593
00:26:26,290 --> 00:26:27,160
7x7입니다.

594
00:26:27,160 --> 00:26:27,940
네, 정확합니다.

595
00:26:27,940 --> 00:26:29,650
첫 번째는 3x3입니다.

596
00:26:29,650 --> 00:26:31,980
다음 것은 5x5이고요.

597
00:26:31,980 --> 00:26:33,370
그 다음은 7x7입니다.

598
00:26:33,370 --> 00:26:36,400
이걸 여기서 쉽게 시각화할 수 있습니다.

599
00:26:36,400 --> 00:26:39,240
스트라이드 1인 3x3 컨볼루션의

600
00:26:39,240 --> 00:26:42,270
좋은 점은 각 레이어마다 수용 영역이

601
00:26:42,270 --> 00:26:44,590
기본적으로 2씩 늘어난다는

602
00:26:44,590 --> 00:26:48,300
겁니다. 왜냐하면 각 점에서 좌우와 위아래를

603
00:26:48,300 --> 00:26:50,050
보기 때문입니다.

604
00:26:50,050 --> 00:26:51,745
그래서 그런 블록을 여러 개 쌓으면,

605
00:26:51,745 --> 00:26:53,120
매번 두 개씩 더하는 겁니다.

606
00:26:57,876 --> 00:27:00,780
즉, 3x3 컨볼루션과 스트라이드 1

607
00:27:00,780 --> 00:27:04,050
레이어 세 개를 쌓은 것이 7x7 레이어

608
00:27:04,050 --> 00:27:08,192
하나와 같은 유효 필드를 가진다는 걸 보여드린 거죠.

609
00:27:08,192 --> 00:27:09,900
그럼 질문은 이게 사후에

610
00:27:09,900 --> 00:27:12,192
합리화한 건지, 아니면

611
00:27:12,192 --> 00:27:16,260
실험 설계에 쓴 직관인지 얼마나 되는가 하는 거죠?

612
00:27:16,260 --> 00:27:18,860
사실 아키텍처에 따라 다를 것 같습니다.

613
00:27:18,860 --> 00:27:20,990
어떤 것들은 직관에 더 집중한 경우도 있고요.

614
00:27:20,990 --> 00:27:23,532
그리고 다음에 다룰 것은, 전체 연구

615
00:27:23,532 --> 00:27:24,340
방향이 어떤

616
00:27:24,340 --> 00:27:27,070
경험적 발견에서 시작된 것 같아요, 일종의

617
00:27:27,070 --> 00:27:28,820
사고 실험 같은 거죠.

618
00:27:28,820 --> 00:27:30,653
그게 ResNet이고,

619
00:27:30,653 --> 00:27:32,320
거기서는 잘 작동할 것이라는

620
00:27:32,320 --> 00:27:35,140
꽤 좋은 직관이 있었던 걸로 알고 있습니다.

621
00:27:35,140 --> 00:27:40,600
하지만 이 경우는, 저자들이 공개적으로 말한

622
00:27:40,600 --> 00:27:43,450
적이 없어서 사후 합리화인지

623
00:27:43,450 --> 00:27:49,360
경험적 발견에 기반한 건지, 설계 선택에

624
00:27:49,360 --> 00:27:51,513
관여했는지 제가

625
00:27:51,513 --> 00:27:53,930
말씀드리기 어렵습니다.

626
00:27:53,930 --> 00:27:58,030
하지만 ResNet의 경우, 이것이 만들어지게 된 가설이라는

627
00:27:58,030 --> 00:28:02,270
것은 알고 있습니다만, 사실 이것은 정말 좋은 특성입니다.

628
00:28:02,270 --> 00:28:04,780
그래서 3개의 3x3 필터가 하나의 7x7 레이어와

629
00:28:04,780 --> 00:28:06,290
같은 유효 수용 영역을 갖습니다.

630
00:28:06,290 --> 00:28:08,330
그리고 실제로 파라미터 수도 더 적습니다.

631
00:28:08,330 --> 00:28:13,570
채널 차원이 동일하다고 가정하면, 입력

632
00:28:13,570 --> 00:28:17,780
채널 수를 가진 3x3 격자가

633
00:28:17,780 --> 00:28:19,470
있습니다.

634
00:28:19,470 --> 00:28:21,788
그래서 3 곱하기 3 곱하기 C가 각 필터의

635
00:28:21,788 --> 00:28:22,830
값 개수가 됩니다.

636
00:28:22,830 --> 00:28:26,180
그리고 이런 필터가 C개 있다면, 3 곱하기 3 곱하기 C 곱하기

637
00:28:26,180 --> 00:28:28,500
C, 즉 3 제곱 곱하기 C 제곱이 됩니다.

638
00:28:28,500 --> 00:28:30,150
그리고 총 3개의 레이어가 있습니다.

639
00:28:30,150 --> 00:28:35,120
이 관점에서 보면, 실제로 파라미터 수가 더 적다는

640
00:28:35,120 --> 00:28:37,080
것을 알 수 있습니다.

641
00:28:37,080 --> 00:28:41,750
우리가 여기서 더 복잡하고 비선형적인 모델을

642
00:28:41,750 --> 00:28:43,650
만들고 있습니다.

643
00:28:43,650 --> 00:28:47,180
그래서 파라미터 수는 적지만 입력 데이터 간의 더

644
00:28:47,180 --> 00:28:49,460
복잡한 관계를 모델링할 수 있습니다.

645
00:28:49,460 --> 00:28:52,610
아마도 그래서 3x3 레이어를 여러 개

646
00:28:52,610 --> 00:28:56,600
쌓는 것이 단순히 더 큰 필터를 사용하는 것보다 나을

647
00:28:56,600 --> 00:28:57,750
수 있습니다.

648
00:28:57,750 --> 00:28:59,630
파라미터 수가 적고 더

649
00:28:59,630 --> 00:29:02,990
복잡한 관계도 모델링할 수 있습니다.

650
00:29:02,990 --> 00:29:05,690
좋습니다, 이제 ResNet에 대해 이야기하겠습니다.

651
00:29:05,690 --> 00:29:07,700
아마 누군가 방금 질문한

652
00:29:07,700 --> 00:29:09,930
사고 실험을 많이 언급할 것 같습니다.

653
00:29:09,930 --> 00:29:13,590
실제로 ResNet 설계에 대한

654
00:29:13,590 --> 00:29:16,530
많은 논의와 생각을 촉발한

655
00:29:16,530 --> 00:29:19,240
경험적 발견이 있었습니다.

656
00:29:19,240 --> 00:29:23,820
아이디어는 평범한 CNN 네트워크에

657
00:29:23,820 --> 00:29:27,497
더 깊은 레이어를 계속 쌓으면

658
00:29:27,497 --> 00:29:29,080
어떻게

659
00:29:29,080 --> 00:29:31,540
되는지를 보여줬습니다.

660
00:29:31,540 --> 00:29:33,870
우리가 발견한 것과 그들이

661
00:29:33,870 --> 00:29:38,040
발견한 것은 20층 모델이 56층 모델보다 테스트

662
00:29:38,040 --> 00:29:40,030
오류가 더 낮다는 것입니다.

663
00:29:40,030 --> 00:29:42,715
이것이 과적합 때문이라고 생각할 수 있지만,

664
00:29:42,715 --> 00:29:45,340
실제로는 그렇지 않습니다. 훈련 오류를

665
00:29:45,340 --> 00:29:48,400
보면 20층 모델의 훈련 오류도 더 낮습니다.

666
00:29:48,400 --> 00:29:51,930
즉, 훈련 오류와 테스트 오류가 모두 낮다는 것은

667
00:29:51,930 --> 00:29:55,410
모델이 모든 면에서 더 잘 수행하고 있다는 뜻입니다.

668
00:29:55,410 --> 00:29:59,640
그렇다면 왜 56층 모델이 20층

669
00:29:59,640 --> 00:30:02,940
모델보다 성능이 떨어질까요?

670
00:30:02,940 --> 00:30:04,540
혼란스러울 수 있습니다.

671
00:30:04,540 --> 00:30:07,080
그리고 앞서 말했듯이, 이것은

672
00:30:07,080 --> 00:30:09,240
과적합 때문이 아닙니다.

673
00:30:09,240 --> 00:30:12,450
더 깊은 모델은 더 큰 표현력을 가지고

674
00:30:12,450 --> 00:30:16,160
있고, 이론적으로는 얕은 네트워크가 표현할

675
00:30:16,160 --> 00:30:20,610
수 있는 어떤 모델도 표현할 수 있어야 합니다.

676
00:30:20,610 --> 00:30:25,370
입력과 출력 값 사이의 가능한 매핑 집합은

677
00:30:25,370 --> 00:30:28,370
더 큰 네트워크가 더 작은

678
00:30:28,370 --> 00:30:31,410
네트워크의 슈퍼셋입니다.

679
00:30:31,410 --> 00:30:35,318
왜냐하면 이론적으로 일부 레이어를 항등

680
00:30:35,318 --> 00:30:36,860
함수로 설정해

681
00:30:36,860 --> 00:30:40,310
아무 작업도 하지 않게 할 수

682
00:30:40,310 --> 00:30:41,880
있기 때문입니다.

683
00:30:41,880 --> 00:30:44,300
레이어의 절반을 아무 작업도 하지

684
00:30:44,300 --> 00:30:46,460
않게 설정하면, 정확히 절반 크기의

685
00:30:46,460 --> 00:30:49,080
모델과 같은 표현력을 가지게 됩니다.

686
00:30:49,080 --> 00:30:53,570
그래서 이 모델들이 표현력 측면에서 더 나쁘다는

687
00:30:53,570 --> 00:30:56,760
게 아니라, 실제로 최적화가 더

688
00:30:56,760 --> 00:30:58,970
어렵다는 겁니다.

689
00:30:58,970 --> 00:31:03,080
왜냐하면 더 깊은 네트워크의 가능한 모델 집합이

690
00:31:03,080 --> 00:31:05,690
더 크고, 얕은 네트워크가

691
00:31:05,690 --> 00:31:11,280
학습할 수 있는 모든 모델을 포함하고 있기 때문입니다.

692
00:31:11,280 --> 00:31:16,290
전에 잠깐 언급했지만, 더 깊은 모델이 어떻게 하면 적어도

693
00:31:16,290 --> 00:31:17,970
얕은 모델만큼 잘 학습할

694
00:31:17,970 --> 00:31:19,330
수 있을까요?

695
00:31:19,330 --> 00:31:20,440
그 방법은 바로 설정하는 겁니다.

696
00:31:20,440 --> 00:31:23,670
예를 들어 두 층짜리 모델과 한 층짜리

697
00:31:23,670 --> 00:31:26,190
모델이 있다고 할 때, 왼쪽은 한

698
00:31:26,190 --> 00:31:29,670
층, 오른쪽은 두 층이라면, 한 층을 사실상

699
00:31:29,670 --> 00:31:32,700
항등 행렬, 즉 항등 함수로 설정하면,

700
00:31:32,700 --> 00:31:36,630
모델은 적어도 얕은 모델만큼은 성능을 낼 수 있습니다.

701
00:31:36,630 --> 00:31:40,290
적어도 얕은 모델만큼은 성능을 낼 수 있습니다.

702
00:31:40,290 --> 00:31:43,720
그럼 이 직관을 실제 모델에 어떻게 반영할 수 있을까요?

703
00:31:43,720 --> 00:31:45,810
최적화 과정에서 얕은 모델만큼

704
00:31:45,810 --> 00:31:48,000
잘 할 수 있도록 모델이

705
00:31:48,000 --> 00:31:50,500
그렇게 할 수 있게 만드는 겁니다.

706
00:31:50,500 --> 00:31:53,100
우리가 하는 방법은 원하는 기본

707
00:31:53,100 --> 00:31:56,070
매핑을 직접 학습하는 대신, 잔차

708
00:31:56,070 --> 00:32:00,580
매핑(residual mapping)을 학습하는 것입니다.

709
00:32:00,580 --> 00:32:04,170
이게 어떻게 생겼냐면, 입력값 x를

710
00:32:04,170 --> 00:32:07,830
복사해서 컨볼루션 레이어를 지나서도 그대로

711
00:32:07,830 --> 00:32:12,310
전달합니다. 그래서 이 시점에서 값은 원래

712
00:32:12,310 --> 00:32:16,810
입력 x와 두 개의 컨볼루션 스택 출력 모두를

713
00:32:16,810 --> 00:32:18,080
받게 됩니다.

714
00:32:18,080 --> 00:32:20,530
즉, 여기서 F(x)라고

715
00:32:20,530 --> 00:32:23,590
부르는 잔차 맵은 모든

716
00:32:23,590 --> 00:32:29,870
컨볼루션 필터에 대해 0 값을 학습할 수도 있습니다.

717
00:32:29,870 --> 00:32:31,670
출력은 0이 되고,

718
00:32:31,670 --> 00:32:35,120
그럼 x를 더하면 결과는 x가 됩니다.

719
00:32:35,120 --> 00:32:37,660
이렇게 하면 레이어가 아무것도 학습할 필요가

720
00:32:37,660 --> 00:32:40,690
없을 때, 모델이 이 레이어들을 쉽게 우회할

721
00:32:40,690 --> 00:32:41,480
수 있습니다.

722
00:32:41,480 --> 00:32:45,190
즉, 우리가 앞서 말한 항등 함수를 아주

723
00:32:45,190 --> 00:32:47,920
쉽게 학습할 수 있게 되는 거죠.

724
00:32:47,920 --> 00:32:51,440
필터가 모두 0 값으로 채워지면 됩니다.

725
00:32:51,440 --> 00:32:53,450
예를 들어 필터가 모두

726
00:32:53,450 --> 00:32:57,725
0 값으로 채워지거나, 실제로는 아주 작은 값만

727
00:32:57,725 --> 00:32:59,350
학습하면 됩니다.

728
00:32:59,350 --> 00:33:03,025
왜냐하면 x에서 h(x)로 전체 매핑을

729
00:33:03,025 --> 00:33:06,310
학습하는 대신, F(x)라는 차이만

730
00:33:06,310 --> 00:33:08,500
학습하면 되기 때문입니다.

731
00:33:08,500 --> 00:33:11,850
즉, 원하는 출력과

732
00:33:11,850 --> 00:33:14,370
복사된 블록 간의

733
00:33:14,370 --> 00:33:18,220
차이만 학습하는 겁니다.

734
00:33:18,220 --> 00:33:22,150
이걸 잔차 블록(residual block) 또는 잔차

735
00:33:22,150 --> 00:33:24,360
연결(residual

736
00:33:24,360 --> 00:33:26,910
connection)이라고 부릅니다. 이전

737
00:33:26,910 --> 00:33:30,720
레이어의 값을 나중 레이어에 복사해서 더하는 구조입니다.

738
00:33:30,720 --> 00:33:32,830
조금 전에 말한 직관은,

739
00:33:32,830 --> 00:33:36,240
더 큰 네트워크가 훈련과 테스트 오류가 더

740
00:33:36,240 --> 00:33:39,810
나쁜 현상은 최적화가 더 어렵기 때문이라는

741
00:33:39,810 --> 00:33:41,560
관찰에서 나왔습니다.

742
00:33:41,560 --> 00:33:44,760
그래서 얕은 네트워크를 쉽게 모델링할 수 있는 모델을 만들어야

743
00:33:44,760 --> 00:33:47,070
한다는 직관이 있었고, 적어도 얕은

744
00:33:47,070 --> 00:33:49,390
모델만큼은 성능을 낼 수 있어야 한다는 겁니다.

745
00:33:49,390 --> 00:33:51,990
이걸 위해 잔차 연결을 추가해서 값을

746
00:33:51,990 --> 00:33:54,683
쉽게 복사할 수 있게 하고, 컨볼루션

747
00:33:54,683 --> 00:33:56,350
레이어 사이에서 항등

748
00:33:56,350 --> 00:33:59,070
매핑을 학습하려고 애쓰는 대신 아키텍처에

749
00:33:59,070 --> 00:34:00,550
직접 넣은 겁니다.

750
00:34:00,550 --> 00:34:04,470
그리고 경험적으로도 이 방법은 매우 잘 작동했습니다.

751
00:34:04,470 --> 00:34:06,500
잔차 블록이 무엇을 하는지 다시 설명드리면,

752
00:34:06,500 --> 00:34:07,940
입력 x가 있고,

753
00:34:07,940 --> 00:34:10,550
두 개의 컨볼루션 레이어를 통과해서

754
00:34:10,550 --> 00:34:12,670
출력 F(x)를 얻습니다.

755
00:34:12,670 --> 00:34:14,900
x는 그대로 복사되어 여기로 전달되고,

756
00:34:14,900 --> 00:34:17,050
이 값과 두

757
00:34:17,050 --> 00:34:21,820
블록의 출력 F(x)를 더합니다.

758
00:34:21,820 --> 00:34:24,679
x는 이전 레이어의 출력이거나,

759
00:34:24,679 --> 00:34:26,739
모델의 첫 번째 레이어라면

760
00:34:26,739 --> 00:34:28,659
이미지가 됩니다.

761
00:34:28,659 --> 00:34:30,460
질문은 데이터가 충분하지 않아서 그런

762
00:34:30,460 --> 00:34:32,627
것일 수도 있고, 충분한 데이터를 추가하면

763
00:34:32,627 --> 00:34:35,440
잔차 블록 없이도 모델을 학습할 수 있지 않냐는 거죠.

764
00:34:35,440 --> 00:34:38,320
저는 이 블록들이 더 많은 데이터를 학습하는

765
00:34:38,320 --> 00:34:40,840
데 정말 큰 도움이 된다고 생각합니다.

766
00:34:40,840 --> 00:34:43,610
문제는 정말 최적화 문제였던 겁니다.

767
00:34:43,610 --> 00:34:45,280
트랜스포머도 같은 이유로

768
00:34:45,280 --> 00:34:47,505
잔차 블록을 사용합니다. 그리고 이게

769
00:34:47,505 --> 00:34:48,880
더 복잡한 모델을 잘

770
00:34:48,880 --> 00:34:51,580
학습하게 도와주고, 더 많은 데이터를 사용할

771
00:34:51,580 --> 00:34:53,000
수 있게 해줍니다.

772
00:34:53,000 --> 00:34:55,427
그래서 저는 이게 매우 좋다고 생각합니다.

773
00:34:55,427 --> 00:34:57,010
Residual block은

774
00:34:57,010 --> 00:35:00,650
더 많은 함수를 더 쉽게 모델링할 수 있기 때문에

775
00:35:00,650 --> 00:35:04,970
더 많은 데이터를 더 효율적으로 사용할 수 있게 도와줍니다.

776
00:35:04,970 --> 00:35:09,260
네, 질문은 아마도 더 오래 훈련하면 성능이

777
00:35:09,260 --> 00:35:13,070
결국 작은 네트워크의 값에 수렴하지

778
00:35:13,070 --> 00:35:15,188
않을까 하는 거죠.

779
00:35:15,188 --> 00:35:17,480
그리고 더 큰 모델을 훈련하는 데 시간이 오래

780
00:35:17,480 --> 00:35:20,000
걸려서 최적화가 더 어려운 것일 수도 있습니다.

781
00:35:20,000 --> 00:35:25,760
제 생각에는 아니라고 봅니다. 훈련 시간을 아무리 늘려도

782
00:35:25,760 --> 00:35:28,507
작은 모델의 성능에 수렴하지

783
00:35:28,507 --> 00:35:30,090
않았습니다.

784
00:35:30,090 --> 00:35:32,390
그 이유는 본질적으로

785
00:35:32,390 --> 00:35:36,110
지역 최소값에 갇히기 때문입니다.

786
00:35:36,110 --> 00:35:39,120
그리고 residual 연결을 추가하면

787
00:35:39,120 --> 00:35:41,430
이런 문제를 피할 수 있습니다.

788
00:35:41,430 --> 00:35:44,970
이 현상의 실제 설명은 아직도 활발히

789
00:35:44,970 --> 00:35:48,950
연구되고 있다고 말씀드릴 수 있습니다.

790
00:35:48,950 --> 00:35:51,350
이 모델들이 왜 전역

791
00:35:51,350 --> 00:35:56,060
최소값이 아니라 지역 최소값을 피하고 최적의 해를 찾지

792
00:35:56,060 --> 00:35:58,830
못하는지, 또는 왜 더 나은

793
00:35:58,830 --> 00:36:03,730
해를 찾지 못하는지 정확히 이해하기는 매우 어렵습니다.

794
00:36:03,730 --> 00:36:06,130
그리고 종종 이것은 경험적인 발견이지만,

795
00:36:06,130 --> 00:36:07,690
그에 대한 직관도 있습니다.

796
00:36:07,690 --> 00:36:09,150
이 경우 직관은, 당시

797
00:36:09,150 --> 00:36:11,612
더 잘 작동하던 얕은 모델만큼은 최소한

798
00:36:11,612 --> 00:36:14,070
성능을 낼 수 있게 모델을 만들고

799
00:36:14,070 --> 00:36:15,300
싶다는 것이었습니다.

800
00:36:15,300 --> 00:36:17,520
즉, 단순히 더 오래 훈련한다고

801
00:36:17,520 --> 00:36:19,920
해서 더 나아지는 게 아니라는 겁니다.

802
00:36:19,920 --> 00:36:23,940
사실 그것은 한계였고, 얕은 모델만큼 좋은

803
00:36:23,940 --> 00:36:26,780
성능을 전혀 낼 수 없었습니다.

804
00:36:31,590 --> 00:36:38,100
좋습니다, 여기 전체 ResNet 아키텍처가 있습니다.

805
00:36:38,100 --> 00:36:41,050
이제 residual block들이 쌓여 있는 구조입니다.

806
00:36:41,050 --> 00:36:44,700
그래서 여기 두 블록이 함께 의미하는 바가 바로 그것입니다.

807
00:36:44,700 --> 00:36:47,500
이것은 residual block입니다.

808
00:36:47,500 --> 00:36:49,860
그래서 3x3 컨볼루션과 ReLU가 있고, 그 다음에

809
00:36:49,860 --> 00:36:51,730
또 다른 3x3 컨볼루션이 있습니다.

810
00:36:51,730 --> 00:36:54,172
그리고 여기서 x 값을 복사해오고 있습니다.

811
00:36:54,172 --> 00:36:55,630
이 값을 출력에 더한 다음,

812
00:36:55,630 --> 00:36:57,297
그 뒤에 ReLU를 적용합니다.

813
00:36:57,297 --> 00:37:00,975
이 블록 쌍 각각이 하나의 residual입니다.

814
00:37:00,975 --> 00:37:03,100
그래서 이 선이 건너뛰는 걸 볼

815
00:37:03,100 --> 00:37:06,370
수 있는데, 값이 앞으로 더해지기 때문입니다.

816
00:37:06,370 --> 00:37:08,620
ResNet의 멋진

817
00:37:08,620 --> 00:37:14,150
점은 다양한 깊이의 모델들을 많이 만들었다는

818
00:37:14,150 --> 00:37:14,900
겁니다.

819
00:37:14,900 --> 00:37:16,608
그래서 작은 모델부터 큰 모델까지

820
00:37:16,608 --> 00:37:17,980
다양한 모델 군을 만들었습니다.

821
00:37:17,980 --> 00:37:20,480
그리고 층 수를 늘릴수록

822
00:37:20,480 --> 00:37:22,100
성능이 향상된다는 걸

823
00:37:22,100 --> 00:37:25,390
보여줬는데, 큰 모델로 갈수록

824
00:37:25,390 --> 00:37:28,010
성능 차이는 점점 작아졌습니다.

825
00:37:28,010 --> 00:37:31,700
즉, 데이터셋에 따라 더 많은 층을

826
00:37:31,700 --> 00:37:35,530
추가해도 큰 폭의 성능 향상은 어려웠지만,

827
00:37:35,530 --> 00:37:38,110
특히 초기 모델들에서는

828
00:37:38,110 --> 00:37:40,658
성능 향상이 꽤 컸다는

829
00:37:40,658 --> 00:37:41,450
겁니다.

830
00:37:41,450 --> 00:37:43,877
그리고 101층에서 152층 사이에서는 성능

831
00:37:43,877 --> 00:37:44,960
변화가 거의 없었습니다.

832
00:37:44,960 --> 00:37:47,860
약간 더 나아지긴 했지만, 그때는 성능

833
00:37:47,860 --> 00:37:49,990
변화가 1% 정도에 불과했습니다.

834
00:37:49,990 --> 00:37:51,800
네, 152라는 숫자는 어떻게 정했나요?

835
00:37:51,800 --> 00:37:54,520
사실 152라는 숫자가 어떻게 나왔는지는 저도 모릅니다.

836
00:37:54,520 --> 00:37:58,190
아마도 여기서 여러 값을 시도해보고 싶었던 것 같습니다.

837
00:37:58,190 --> 00:37:59,170
보시다시피,

838
00:37:59,170 --> 00:38:00,760
정확히 두 배는

839
00:38:00,760 --> 00:38:05,350
아니지만 매번 상당히 증가하는 걸 볼 수 있습니다.

840
00:38:05,350 --> 00:38:07,400
왜 152를 선택했는지는 모르겠습니다.

841
00:38:07,400 --> 00:38:08,620
좋은 질문입니다.

842
00:38:08,620 --> 00:38:11,700
아마도 다른 것들보다 더 잘 작동한다는 것을 보여줬을 겁니다.

843
00:38:11,700 --> 00:38:13,650
사실 저는 잘 모릅니다.

844
00:38:13,650 --> 00:38:16,830
일반적으로 모델의 레이어 수를 여러 개 시도할

845
00:38:16,830 --> 00:38:18,970
때, 예를 들어 시도하려는

846
00:38:18,970 --> 00:38:21,720
레이어 수가 있다면, 가장 작은 모델부터

847
00:38:21,720 --> 00:38:24,940
먼저 학습시키고 성능을 본 다음, 레이어를

848
00:38:24,940 --> 00:38:26,800
더 추가해서 성능이

849
00:38:26,800 --> 00:38:29,230
향상되는지 확인하는 식으로 진행합니다.

850
00:38:29,230 --> 00:38:31,170
아마도 그래서 152에서 멈춘

851
00:38:31,170 --> 00:38:34,410
이유는 성능 향상이 더 이상 크지 않았기 때문일 겁니다.

852
00:38:34,410 --> 00:38:36,670
또한 GPU 메모리 제한도 있어서,

853
00:38:36,670 --> 00:38:38,470
모델이 커질수록 하드웨어 관점에서

854
00:38:38,470 --> 00:38:40,720
학습이 어려워집니다. GPU 메모리에

855
00:38:40,720 --> 00:38:43,450
더 많은 파라미터를 넣어야 하기 때문입니다.

856
00:38:43,450 --> 00:38:47,040
즉, 컴퓨팅 환경에 따라 학습할 수 있는 모델

857
00:38:47,040 --> 00:38:49,080
크기에 한계가 있습니다.

858
00:38:49,080 --> 00:38:51,940
이 모델들은 각각 따로 학습시켜야 한다고

859
00:38:51,940 --> 00:38:55,710
생각합니다. 예를 들어 18 레이어 모델 하나, 34

860
00:38:55,710 --> 00:38:57,790
레이어 모델 하나 이렇게요.

861
00:38:57,790 --> 00:39:03,100
질문은 CNN 블록의 직관을 어떻게 생각할 것인가인데, 우리가
residual

862
00:39:03,100 --> 00:39:07,270
connection을 사용하기 때문에 여전히 더 높은 수준의

863
00:39:07,270 --> 00:39:09,568
추상화로 생각할 수 있습니다.

864
00:39:09,568 --> 00:39:11,360
이것은 레이어에서 실제로도 증명되었습니다.

865
00:39:11,360 --> 00:39:16,020
블록 내에서 단순히 더 높은 수준의 특징을 학습하는

866
00:39:16,020 --> 00:39:18,520
대신, 원본 이미지에서 더

867
00:39:18,520 --> 00:39:20,228
높은 수준의 특징으로

868
00:39:20,228 --> 00:39:23,180
가는 델타를 학습하는 겁니다.

869
00:39:23,180 --> 00:39:24,950
즉, 블록에서 학습하는 것은 델타입니다.

870
00:39:24,950 --> 00:39:27,280
델타를 학습하지만 각 단계에서 여전히

871
00:39:27,280 --> 00:39:29,900
더 높은 수준의 표현을 얻는 것이죠.

872
00:39:29,900 --> 00:39:32,500
그 부분은 같지만

873
00:39:32,500 --> 00:39:38,560
실제 기능적인 방식은 이전 입력에 더하는 F(x)를

874
00:39:38,560 --> 00:39:41,120
학습하는 것입니다.

875
00:39:41,120 --> 00:39:43,270
즉, 델타를 학습하는 거죠.

876
00:39:43,270 --> 00:39:44,785
질문은 덧셈을 할 때

877
00:39:44,785 --> 00:39:46,910
텐서 크기가 같아야 하는가인데,

878
00:39:46,910 --> 00:39:47,690
답은 그렇습니다.

879
00:39:47,690 --> 00:39:51,203
이것이 바로 모든 블록이 3x3

880
00:39:51,203 --> 00:39:52,870
컨볼루션을

881
00:39:52,870 --> 00:39:57,590
스트라이드 1, 패딩 1로 해서 레이어마다

882
00:39:57,590 --> 00:40:01,370
크기를 유지하는 이유 중 하나입니다.

883
00:40:01,370 --> 00:40:01,890
크기를 유지하는 이유 중 하나입니다.

884
00:40:01,890 --> 00:40:05,443
예를 들어 풀링 레이어 이후에는

885
00:40:05,443 --> 00:40:06,860
값들을

886
00:40:06,860 --> 00:40:12,710
풀어내는 방법을 생각할 수 있겠지만, 풀링 후에는

887
00:40:12,710 --> 00:40:14,343
텐서 크기가

888
00:40:14,343 --> 00:40:16,010
달라서 단순

889
00:40:16,010 --> 00:40:18,570
덧셈은 불가능합니다.

890
00:40:18,570 --> 00:40:23,925
그래서 이런 덧셈은 적어도 일반적인 경우 풀링 전에 이루어집니다.

891
00:40:23,925 --> 00:40:25,550
물론 각 값을

892
00:40:25,550 --> 00:40:28,800
여러 값으로 분산시키는 식으로 해결할

893
00:40:28,800 --> 00:40:30,710
수도 있겠지만요.

894
00:40:30,710 --> 00:40:35,390
자, 이것이 ResNet의 주요 핵심 내용입니다.

895
00:40:35,390 --> 00:40:38,180
또 다른 멋진 트릭은 일정

896
00:40:38,180 --> 00:40:40,620
블록 수마다 필터 수를

897
00:40:40,620 --> 00:40:42,770
두 배로 늘리고 공간

898
00:40:42,770 --> 00:40:46,040
차원을 다운샘플링한다는 점입니다.

899
00:40:46,040 --> 00:40:48,170
즉, 아주 평평한

900
00:40:48,170 --> 00:40:53,525
이미지에서 시작해 네트워크를 통과하면서 공간적으로는 작아지지만

901
00:40:53,525 --> 00:40:55,900
깊이는 커지는 구조를

902
00:40:55,900 --> 00:40:57,880
상상할 수 있습니다.

903
00:40:57,880 --> 00:40:59,195
이렇게 생각하면 됩니다.

904
00:40:59,195 --> 00:41:00,570
그리고 마지막에는

905
00:41:00,570 --> 00:41:03,243
분류에 사용하는 벡터가 됩니다.

906
00:41:03,243 --> 00:41:05,160
그래서 네트워크 내 값들의

907
00:41:05,160 --> 00:41:07,650
변화와 형태를 이렇게 시각화하면

908
00:41:07,650 --> 00:41:08,740
됩니다.

909
00:41:08,740 --> 00:41:11,280
그리고 ResNet에 다소 독특한

910
00:41:11,280 --> 00:41:14,110
점 중 하나는, 다른 아키텍처도

911
00:41:14,110 --> 00:41:17,740
하지만, residual 블록 레이어들 전에

912
00:41:17,740 --> 00:41:21,520
상대적으로 큰 컨볼루션 레이어가 있다는 것입니다.

913
00:41:21,520 --> 00:41:23,040
여기서는 경험적으로 이 레이어를

914
00:41:23,040 --> 00:41:25,240
추가했을 때 성능이 더 좋았다고 합니다.

915
00:41:25,240 --> 00:41:29,610
이것은 순전히 경험적인 발견입니다.

916
00:41:29,610 --> 00:41:34,890
네, 기본적으로 강조할 점은 이 큰 모델들이 매우 좋은

917
00:41:34,890 --> 00:41:37,230
성능을 냈다는 것입니다.

918
00:41:37,230 --> 00:41:39,870
그때가 100개 이상의 레이어 모델을 성공적으로 훈련시킨

919
00:41:39,870 --> 00:41:42,280
첫 번째 사례였기 때문에 정말 큰 의미가 있었습니다.

920
00:41:42,280 --> 00:41:45,810
그리고 기본적으로 ResNet은 이후에 다양한 컴퓨터 비전

921
00:41:45,810 --> 00:41:47,320
작업에 사용되었습니다.

922
00:41:47,320 --> 00:41:48,848
당시 거의 모든 컴퓨터 비전

923
00:41:48,848 --> 00:41:50,640
작업에서 ResNet을 사용했는데,

924
00:41:50,640 --> 00:41:54,890
이는 residual 연결 덕분에 성능이 매우 좋았기 때문입니다.

925
00:41:54,890 --> 00:41:59,370
그래서 우리는 몇 가지 CNN 아키텍처에 대해 이야기했는데, 주요한

926
00:41:59,370 --> 00:42:02,710
것은 ResNet이고 역사적으로는 VGG도 있었습니다.

927
00:42:02,710 --> 00:42:05,800
작은 필터 크기가 왜 유용한지, 그리고 이런 필터를 여러 층 쌓는

928
00:42:05,800 --> 00:42:07,780
것이 왜 유용한지에 대해 이야기했습니다.

929
00:42:07,780 --> 00:42:09,780
그래서 CNN을 실제로

930
00:42:09,780 --> 00:42:11,910
어떻게 구성하고 훈련 준비를

931
00:42:11,910 --> 00:42:15,120
시키는지에 대해 마지막으로 이야기할 것은,

932
00:42:15,120 --> 00:42:19,170
개별 레이어의 가중치 값을 어떻게 초기화하느냐입니다.

933
00:42:19,170 --> 00:42:24,010
어떤 값을 선택하느냐에 따라 너무 작거나 너무 큰 값을

934
00:42:24,010 --> 00:42:27,370
넣을 수 있는데, 이는 훈련 중 모델에

935
00:42:27,370 --> 00:42:29,880
심각한 문제를 일으킬 수

936
00:42:29,880 --> 00:42:30,820
있습니다.

937
00:42:30,820 --> 00:42:36,360
여기서는 4,096 차원의 특징을

938
00:42:36,360 --> 00:42:40,000
가진 6층 네트워크입니다.

939
00:42:40,000 --> 00:42:43,600
그리고 이것은 단순히 6개의 완전 연결층으로 구성된 모델이며,

940
00:42:43,600 --> 00:42:45,330
우리는 이를 초기화합니다.

941
00:42:45,330 --> 00:42:48,400
여기서는 단위 가우시안 랜덤 값을 뽑아서

942
00:42:48,400 --> 00:42:50,610
0.01을 곱해 0에

943
00:42:50,610 --> 00:42:53,560
가까운 아주 작은 값을 만들고, 각

944
00:42:53,560 --> 00:42:55,720
층마다 ReLU를 사용합니다.

945
00:42:55,720 --> 00:43:00,340
이 모델의 순전파를 그려보면,

946
00:43:00,340 --> 00:43:02,400
처음에는 ReLU

947
00:43:02,400 --> 00:43:06,280
때문에 모든 평균이 양수여서

948
00:43:06,280 --> 00:43:08,260
비교적 높은

949
00:43:08,260 --> 00:43:10,890
평균과 표준편차를 갖게

950
00:43:10,890 --> 00:43:12,130
됩니다.

951
00:43:12,130 --> 00:43:14,790
하지만 각 층을 지날수록 가중치

952
00:43:14,790 --> 00:43:17,160
초기값이 매우 작았기 때문에

953
00:43:17,160 --> 00:43:20,120
평균과 표준편차가 점점 작아집니다.

954
00:43:20,120 --> 00:43:21,870
사실 이상적으로는 각

955
00:43:21,870 --> 00:43:24,360
층마다 이 값들이 모두 같아야 하는데,

956
00:43:24,360 --> 00:43:26,460
그래야 최적화 문제가

957
00:43:26,460 --> 00:43:28,860
훨씬 더 잘 풀리기 때문입니다.

958
00:43:28,860 --> 00:43:33,690
만약 0.01 대신 0.05를 쓴다면, 너무 큰

959
00:43:33,690 --> 00:43:36,780
값을 설정했을 때 어떤 문제가 생길지

960
00:43:36,780 --> 00:43:38,925
상상할 수 있나요?

961
00:43:38,925 --> 00:43:42,540
너무 작으면 값이 0에 수렴합니다.

962
00:43:42,540 --> 00:43:44,970
너무 크면 어떻게 될까요?

963
00:43:44,970 --> 00:43:47,580
네, 기본적으로 각 레이어에서 활성화 값이

964
00:43:47,580 --> 00:43:48,400
점점 커집니다.

965
00:43:48,400 --> 00:43:51,730
그래서 여기 그래프로 그려보면, 마지막에는 평균과 표준편차가

966
00:43:51,730 --> 00:43:54,050
엄청나게 커지는 걸 볼 수 있습니다.

967
00:43:54,050 --> 00:43:57,910
152층 ResNet을 훈련한다고 상상해보면, 이

968
00:43:57,910 --> 00:44:00,610
문제가 매우 빠르게 심각해질 수 있죠.

969
00:44:00,610 --> 00:44:03,980
그럼 실제로 어떻게 해결할 수 있을까요?

970
00:44:03,980 --> 00:44:07,240
이 경우 최적값은 아마 0.022 정도일 텐데,

971
00:44:07,240 --> 00:44:10,600
그걸 어떻게 알 수 있고, 더 일반적으로 모든

972
00:44:10,600 --> 00:44:13,480
레이어에 대해 어떻게 적용할 수 있을까요?

973
00:44:13,480 --> 00:44:16,610
가중치를 초기화하는 방법은 몇 가지가 있습니다.

974
00:44:16,610 --> 00:44:18,430
오늘 수업에서는 가장 많이 쓰이는 방법을

975
00:44:18,430 --> 00:44:20,510
설명할 텐데, 물론 다른 방법들도 있습니다.

976
00:44:20,510 --> 00:44:22,930
일반적으로 이 값들은

977
00:44:22,930 --> 00:44:27,380
여기 값들의 차원에 따라 결정됩니다.

978
00:44:27,380 --> 00:44:33,100
그래서 4,096 차원의 완전 연결층과 2,048 차원의 완전 연결층은

979
00:44:33,100 --> 00:44:34,880
다른 값을 갖게 됩니다.

980
00:44:34,880 --> 00:44:37,990
우리가 살펴볼 구체적인 공식은 Kaiming

981
00:44:37,990 --> 00:44:40,190
Initialization이라고 합니다.

982
00:44:40,190 --> 00:44:42,980
이 방법은 ResNet을 만든 사람과 같은 사람이 고안한 겁니다.

983
00:44:42,980 --> 00:44:45,818
Kaiming은 매우 유명한 연구자입니다.

984
00:44:45,818 --> 00:44:47,860
지금도 컴퓨터 비전 분야에서 매우 유명한

985
00:44:47,860 --> 00:44:48,440
연구자이고,

986
00:44:48,440 --> 00:44:50,523
지난 10~15년간 가장 많이 인용된 컴퓨터

987
00:44:50,523 --> 00:44:53,560
과학자 중 한 명일 겁니다, 어쩌면 가장 많이 인용된 사람이죠.

988
00:44:53,560 --> 00:44:57,730
그래서 컴퓨터 비전 커뮤니티에서 매우 잘 알려져 있습니다.

989
00:44:57,730 --> 00:45:00,600
그리고 입력 차원 크기 대비

990
00:45:00,600 --> 00:45:05,190
2의 제곱근으로 값을 초기화하는 아이디어도

991
00:45:05,190 --> 00:45:07,390
그가 제안했습니다.

992
00:45:07,390 --> 00:45:09,450
그리고 이 방법을 어떻게 유도했는지,

993
00:45:09,450 --> 00:45:12,977
ReLU 활성화 함수와 함께 사용하면 각 층에서 표준편차와

994
00:45:12,977 --> 00:45:15,060
평균이 상대적으로 일정하게 유지된다는

995
00:45:15,060 --> 00:45:17,680
것을 모두 자세히 설명하지는 않겠습니다.

996
00:45:17,680 --> 00:45:20,297
하지만 실제로 그래프를 그려보면 이 방법이 그런 효과가 있음을 알

997
00:45:20,297 --> 00:45:22,380
수 있어서, 마치 원하는 특성을 얻기 위해 이 공식을

998
00:45:22,380 --> 00:45:24,505
대입하는 마법 같은 공식이라고 생각할 수 있습니다.

999
00:45:24,505 --> 00:45:26,590
유도 과정을 알고 싶으시면 논문을

1000
00:45:26,590 --> 00:45:28,800
여기 링크해 두었으니 자유롭게

1001
00:45:28,800 --> 00:45:31,690
참고하시면 되고, 그냥 저희 말을 믿으셔도 됩니다.

1002
00:45:31,690 --> 00:45:33,190
여기서는 자세히 다루지

1003
00:45:33,190 --> 00:45:36,600
않겠지만, 이 방법은 평균과 표준편차가 변하지 않는

1004
00:45:36,600 --> 00:45:37,990
원하는 효과를 냅니다.

1005
00:45:37,990 --> 00:45:40,180
그리고 어떤 설정이든

1006
00:45:40,180 --> 00:45:42,340
테스트를 통해 이 값이

1007
00:45:42,340 --> 00:45:46,630
무엇인지 찾아볼 수도 있다고 생각할 수 있습니다.

1008
00:45:46,630 --> 00:45:50,140
자, 이렇게 해서 가중치 초기화 방법, 여러

1009
00:45:50,140 --> 00:45:52,270
층을 결합해 CNN

1010
00:45:52,270 --> 00:45:53,980
아키텍처를 구성하는 방법,

1011
00:45:53,980 --> 00:45:58,570
사람들이 사용하는 활성화 함수, 그리고 CNN의 다양한

1012
00:45:58,570 --> 00:46:01,010
층들에 대해 이야기했습니다.

1013
00:46:01,010 --> 00:46:03,260
이미 꽤 많은 주제를 다룬 것 같습니다.

1014
00:46:03,260 --> 00:46:04,990
잠시 멈춰서 이 부분에 대해

1015
00:46:04,990 --> 00:46:06,920
질문이 있는지 확인하겠습니다.

1016
00:46:06,920 --> 00:46:08,980
강의의 두 번째 부분은 첫

1017
00:46:08,980 --> 00:46:11,210
번째 부분보다 훨씬 덜 복잡합니다.

1018
00:46:11,210 --> 00:46:13,120
주로 모델을 훈련할 때

1019
00:46:13,120 --> 00:46:17,158
유용한 실용적인 팁들을 많이 다룰 예정입니다.

1020
00:46:17,158 --> 00:46:19,450
그럼 질문입니다, CNN에서 가중치 초기화는

1021
00:46:19,450 --> 00:46:20,180
어떻게 하나요?

1022
00:46:20,180 --> 00:46:23,120
초기화 방법은 같지만,

1023
00:46:23,120 --> 00:46:26,120
여기서 n은 커널 크기입니다.

1024
00:46:26,120 --> 00:46:29,320
예를 들어 3x3 커널에 채널이 6개라면

1025
00:46:29,320 --> 00:46:33,830
3 곱하기 3 곱하기 6이 되지만, 기본 개념은 같습니다.

1026
00:46:33,830 --> 00:46:36,460
층이나 층의 종류에 따라

1027
00:46:36,460 --> 00:46:40,120
차원을 다르게 계산하는 것뿐입니다.

1028
00:46:40,120 --> 00:46:43,150
각 연산에서 대략 몇 개의 값이 있는지로

1029
00:46:43,150 --> 00:46:46,220
생각할 수 있지만, 층에 따라 다릅니다.

1030
00:46:46,220 --> 00:46:49,590
그리고 어떤 층은 다른 가중치 초기화를

1031
00:46:49,590 --> 00:46:51,260
사용하기도 하지만, 이것은

1032
00:46:51,260 --> 00:46:54,170
CNN에 적용되는 초기화 방법입니다.

1033
00:46:54,170 --> 00:46:56,600
네, 질문은 초기화 값이 너무 크면

1034
00:46:56,600 --> 00:46:59,370
왜 활성화 값이 폭발하는가 하는 거죠?

1035
00:46:59,370 --> 00:47:04,220
초기화된 네트워크의 각 층마다 무작위로

1036
00:47:04,220 --> 00:47:08,070
초기화된 값들이 있다고 상상해 보세요.

1037
00:47:08,070 --> 00:47:13,328
그 값들이 매우 크면, 그 뒤에 ReLU 활성화

1038
00:47:13,328 --> 00:47:15,630
함수를 적용하게 됩니다.

1039
00:47:15,630 --> 00:47:21,330
ReLU는 층의 출력을 제한하지 않습니다.

1040
00:47:21,330 --> 00:47:23,400
ReLU는 출력이 무한대로 갈 수 있습니다.

1041
00:47:23,400 --> 00:47:27,050
그래서 만약 모든 가중치를

1042
00:47:27,050 --> 00:47:29,458
같은 무작위

1043
00:47:29,458 --> 00:47:31,250
큰 값들로

1044
00:47:31,250 --> 00:47:35,370
초기화해서 같은 연산을 반복한다면,

1045
00:47:35,370 --> 00:47:38,120
각 층에서

1046
00:47:38,120 --> 00:47:40,730
큰 값들이 곱해져서

1047
00:47:40,730 --> 00:47:44,620
점점 더 커지게 됩니다.

1048
00:47:44,620 --> 00:47:47,850
이걸 단순한 점화식으로 생각할 수도 있는데,

1049
00:47:47,850 --> 00:47:50,250
처음에 모두 무작위로

1050
00:47:50,250 --> 00:47:53,650
초기화된 값들이 어떤 값에 곱해지는 거죠. 단순

1051
00:47:53,650 --> 00:47:55,860
점화식에서는 그 값이 1이길

1052
00:47:55,860 --> 00:47:56,860
원합니다.

1053
00:47:56,860 --> 00:47:59,100
하지만 벡터 값들이

1054
00:47:59,100 --> 00:48:01,912
행렬과 곱해지기 때문에,

1055
00:48:01,912 --> 00:48:03,870
벡터 차원에

1056
00:48:03,870 --> 00:48:10,530
따라 평균 출력이 달라집니다. ReLU 이후에는 활성화

1057
00:48:10,530 --> 00:48:12,930
값들의 표준편차가

1058
00:48:12,930 --> 00:48:14,440
존재하죠.

1059
00:48:14,440 --> 00:48:16,110
그리고 음수 값들을

1060
00:48:16,110 --> 00:48:18,857
제거하면 그 시점의 출력이 남는데,

1061
00:48:18,857 --> 00:48:20,440
만약 값들이 매우

1062
00:48:20,440 --> 00:48:22,232
크면 표준편차도 커집니다.

1063
00:48:22,232 --> 00:48:26,520
그래서 하위 절반을 제거하면 평균이

1064
00:48:26,520 --> 00:48:29,910
점점 더 양수 쪽으로 이동합니다.

1065
00:48:29,910 --> 00:48:31,380
이해가 되셨나요? 슬라이드가

1066
00:48:31,380 --> 00:48:33,147
없어서 보여드리진 못했지만요.

1067
00:48:33,147 --> 00:48:33,730
대체로 그렇습니다, 죄송합니다.

1068
00:48:33,730 --> 00:48:35,290
네, 논문에서 더 자세한 내용을 보실 수 있습니다.

1069
00:48:35,290 --> 00:48:37,040
읽기에 그렇게 어렵지 않은 편입니다.

1070
00:48:40,950 --> 00:48:43,690
여기서 결론은, 정규화가 활성화

1071
00:48:43,690 --> 00:48:47,297
값이 폭발하는 문제를 해결할 수 있지만,

1072
00:48:47,297 --> 00:48:48,880
최적화는 여전히 어려울

1073
00:48:48,880 --> 00:48:50,470
수 있다는 겁니다.

1074
00:48:50,470 --> 00:48:54,160
아마 이 내용을 좀 더 자세히 설명하는 후속

1075
00:48:54,160 --> 00:48:55,948
글을 써야 할 것 같네요.

1076
00:48:55,948 --> 00:48:58,240
정말 좋은 질문입니다, 네.

1077
00:48:58,240 --> 00:49:00,198
이 문제를 해결할 수는 있겠지만,

1078
00:49:00,198 --> 00:49:03,790
아마도 최적화하기는 여전히 어려울 것 같습니다.

1079
00:49:03,790 --> 00:49:06,125
네, 좋은 질문입니다.

1080
00:49:06,125 --> 00:49:06,625
네.

1081
00:49:09,280 --> 00:49:10,150
좋습니다.

1082
00:49:10,150 --> 00:49:12,573
그럼 이제 이 단계들에 대해 이야기하겠습니다.

1083
00:49:12,573 --> 00:49:14,240
그럼 실제로 모델을 어떻게 학습시키나요?

1084
00:49:14,240 --> 00:49:16,510
데이터 전처리에서 좋은 점은 이미지에

1085
00:49:16,510 --> 00:49:17,930
대해 정말 쉽다는 겁니다.

1086
00:49:17,930 --> 00:49:19,970
거대한 이미지 데이터셋이

1087
00:49:19,970 --> 00:49:23,480
있다면, 일반적인 방법은 빨간색, 초록색, 파란색

1088
00:49:23,480 --> 00:49:25,450
픽셀의 평균과 표준편차를

1089
00:49:25,450 --> 00:49:27,110
계산하는 것입니다.

1090
00:49:27,110 --> 00:49:30,100
입력 이미지를 가져와서 평균을 빼고

1091
00:49:30,100 --> 00:49:32,300
표준편차로 나누는 거죠.

1092
00:49:32,300 --> 00:49:36,220
이것이 이미지 데이터 정규화 방법입니다.

1093
00:49:36,220 --> 00:49:38,500
사실 매우 간단합니다.

1094
00:49:38,500 --> 00:49:40,270
각 픽셀 채널에 대해 평균과

1095
00:49:40,270 --> 00:49:44,100
표준편차를 미리 계산해야 한다는 점만 필요합니다.

1096
00:49:44,100 --> 00:49:45,750
그래서 때때로 이미

1097
00:49:45,750 --> 00:49:48,490
계산된 평균을 사용하는 경우도 있습니다.

1098
00:49:48,490 --> 00:49:51,450
가장 흔한 예가 ImageNet의 평균과

1099
00:49:51,450 --> 00:49:54,715
표준편차를 사용해서, 비록 ImageNet이 아닌

1100
00:49:54,715 --> 00:49:57,090
데이터로 모델을 학습하더라도 입력 이미지에

1101
00:49:57,090 --> 00:49:58,410
적용하는 겁니다.

1102
00:49:58,410 --> 00:50:05,080
이것은 데이터셋에 따라 달라진다고 생각하시면 됩니다.

1103
00:50:05,080 --> 00:50:07,622
그리고 모델마다 데이터셋에 따라 다른 값을

1104
00:50:07,622 --> 00:50:10,080
사용할 수 있지만, 가장 일반적으로

1105
00:50:10,080 --> 00:50:11,940
쓰이는 것은 ImageNet의

1106
00:50:11,940 --> 00:50:14,040
평균과 표준편차를 사용하는 것입니다.

1107
00:50:14,040 --> 00:50:16,980
네, 어떤 입력 이미지든 모델에 넣기

1108
00:50:16,980 --> 00:50:18,815
전에 이 작업을 적용합니다.

1109
00:50:23,580 --> 00:50:25,650
네, 이 부분은 정말 빠르게 넘어갔네요.

1110
00:50:25,650 --> 00:50:27,460
그다음 데이터 증강에

1111
00:50:27,460 --> 00:50:31,170
대해 이야기하겠습니다. 수업 중에 누군가 이미지에

1112
00:50:31,170 --> 00:50:34,350
노이즈를 추가하는 건 어떠냐고 제안했었죠.

1113
00:50:34,350 --> 00:50:35,350
좋은 아이디어입니다.

1114
00:50:35,350 --> 00:50:37,020
여기서 이미지에 노이즈를 추가하는 여러 방법에

1115
00:50:37,020 --> 00:50:37,962
대해 이야기할 겁니다.

1116
00:50:37,962 --> 00:50:40,420
이것은 정규화에 도움이 되고 모델이

1117
00:50:40,420 --> 00:50:42,730
과적합되는 것을 방지합니다.

1118
00:50:42,730 --> 00:50:47,110
앞서 말했듯이, 정규화에서는 학습

1119
00:50:47,110 --> 00:50:49,270
시에 무작위성을

1120
00:50:49,270 --> 00:50:50,810
추가하고,

1121
00:50:50,810 --> 00:50:53,990
테스트 시에는 그 무작위성을 평균내는 패턴이 흔히 쓰입니다.

1122
00:50:53,990 --> 00:50:56,690
때때로 근사적이긴 하지만, 예를

1123
00:50:56,690 --> 00:51:00,070
들어 dropout에서는 학습 시에

1124
00:51:00,070 --> 00:51:02,930
활성화의 50%를 무작위로 제거하고,

1125
00:51:02,930 --> 00:51:05,470
테스트 시에는 모든 활성화를

1126
00:51:05,470 --> 00:51:06,970
사용하지만 dropout

1127
00:51:06,970 --> 00:51:09,490
확률 p로 스케일을 조정합니다.

1128
00:51:09,490 --> 00:51:11,540
이것이 매우 흔한 패턴입니다.

1129
00:51:11,540 --> 00:51:13,760
데이터 증강에도 이 패턴이 사용됩니다.

1130
00:51:13,760 --> 00:51:21,010
이 원통형 모양이 데이터셋이라고 상상해보세요.

1131
00:51:21,010 --> 00:51:22,010
이 원통형 모양이 데이터셋이라고 상상해보세요.

1132
00:51:22,010 --> 00:51:24,140
이미지와 레이블을 불러옵니다.

1133
00:51:24,140 --> 00:51:26,800
고양이 레이블과 데이터셋에서 가져온

1134
00:51:26,800 --> 00:51:28,210
원본 이미지가 있죠.

1135
00:51:28,210 --> 00:51:31,790
모델에 넣기 전에, 현대

1136
00:51:31,790 --> 00:51:35,390
딥러닝에서는 컴퓨터 비전 모델

1137
00:51:35,390 --> 00:51:39,420
학습 시 거의 항상 데이터 증강을

1138
00:51:39,420 --> 00:51:40,600
사용합니다.

1139
00:51:40,600 --> 00:51:42,960
기본 아이디어는 이미지에 변환을

1140
00:51:42,960 --> 00:51:45,660
적용해서 다르게 보이게 하되, 여전히

1141
00:51:45,660 --> 00:51:47,980
카테고리 클래스를 인식할 수 있게

1142
00:51:47,980 --> 00:51:50,370
만든 다음 모델에 전달하고 손실을

1143
00:51:50,370 --> 00:51:51,610
계산하는 것입니다.

1144
00:51:51,610 --> 00:51:53,100
이 방법의 좋은 점 중

1145
00:51:53,100 --> 00:51:55,660
하나는 데이터셋 크기를 효과적으로 늘릴 수

1146
00:51:55,660 --> 00:51:58,228
있다는 겁니다. 같은 이미지를 여러 번 보는

1147
00:51:58,228 --> 00:52:00,270
대신, 서로 다른 변환을 적용한 여러

1148
00:52:00,270 --> 00:52:02,820
버전을 보게 되니까요. 모두 같은 카테고리

1149
00:52:02,820 --> 00:52:04,300
레이블을 갖고 있습니다.

1150
00:52:04,300 --> 00:52:06,450
즉, 더 많은 데이터를 얻을 수

1151
00:52:06,450 --> 00:52:09,310
있고, 따라서 일반화 능력이 향상됩니다. 하지만

1152
00:52:09,310 --> 00:52:12,937
훈련 손실은 더 높아질 텐데, 같은 예제를 반복해서

1153
00:52:12,937 --> 00:52:14,770
보는 게 아니기 때문입니다.

1154
00:52:14,770 --> 00:52:18,060
그래서 모델이 단순히 암기하는 게 더 어려워집니다.

1155
00:52:18,060 --> 00:52:20,770
그럼 가중치 초기화가 적절한지 어떻게 알 수 있을까요?

1156
00:52:20,770 --> 00:52:23,320
이번 경우에는 네트워크 층 전체에서

1157
00:52:23,320 --> 00:52:25,470
평균과 표준편차가 비교적

1158
00:52:25,470 --> 00:52:28,290
일정하기 때문에 적절하다는 걸 알 수

1159
00:52:28,290 --> 00:52:28,990
있습니다.

1160
00:52:28,990 --> 00:52:30,330
그리고 이번

1161
00:52:30,330 --> 00:52:34,570
경우에는 모드 붕괴가 0으로 가는 걸 보지 않았습니다.

1162
00:52:34,570 --> 00:52:38,470
대신 층 수가 늘어날수록 무한대로 발산하는

1163
00:52:38,470 --> 00:52:40,660
현상이 있었습니다.

1164
00:52:40,660 --> 00:52:43,510
항상 적절하게 초기화하려면 이 공식을

1165
00:52:43,510 --> 00:52:44,720
사용하면 됩니다.

1166
00:52:44,720 --> 00:52:46,870
이 공식은 항상 가중치를 잘 초기화해 줍니다.

1167
00:52:46,870 --> 00:52:49,100
실제로 사람들이 이렇게 합니다.

1168
00:52:49,100 --> 00:52:52,690
만약 전에 없던 새로운 종류의 연산을

1169
00:52:52,690 --> 00:52:54,910
하는 층을 만든다면, 여러

1170
00:52:54,910 --> 00:52:56,890
가지 가중치 초기화 방식을

1171
00:52:56,890 --> 00:52:59,390
시도해보고 가장 잘 맞는

1172
00:52:59,390 --> 00:53:01,390
걸 찾아야 할 겁니다.

1173
00:53:01,390 --> 00:53:04,480
하지만 일반적으로는 이

1174
00:53:04,480 --> 00:53:09,530
선형 층이나 컨볼루션 층에 대해 여기

1175
00:53:09,530 --> 00:53:14,110
나온 타이밍 초기화 공식을 사용합니다.

1176
00:53:14,110 --> 00:53:14,980
네.

1177
00:53:14,980 --> 00:53:17,967
데이터 증강으로 다시 돌아가서, 구체적으로

1178
00:53:17,967 --> 00:53:20,300
어떤 증강을 할 수 있을까요?

1179
00:53:20,300 --> 00:53:21,890
그중 하나가 수평 뒤집기입니다.

1180
00:53:21,890 --> 00:53:23,710
이것은 문제에 따라 다릅니다.

1181
00:53:23,710 --> 00:53:25,780
텍스트를 읽는 모델을 원한다면,

1182
00:53:25,780 --> 00:53:28,450
이것은 매우 나쁜 증강 방법입니다. 왜냐하면

1183
00:53:28,450 --> 00:53:30,250
텍스트가 마치 거울을

1184
00:53:30,250 --> 00:53:31,750
통해 보는 것처럼 제대로

1185
00:53:31,750 --> 00:53:33,380
읽을 수 없기 때문입니다.

1186
00:53:33,380 --> 00:53:37,560
그래서 이것은 일상적인 물체에는 가끔 유용합니다.

1187
00:53:37,560 --> 00:53:40,100
대부분의 물체가 대칭적이기

1188
00:53:40,100 --> 00:53:44,330
때문에 이 특성은 보통 꽤 잘 작동합니다.

1189
00:53:44,330 --> 00:53:46,640
그리고 현미경이나 항공 사진 같은

1190
00:53:46,640 --> 00:53:48,907
이미지를 본다면 수직 뒤집기도

1191
00:53:48,907 --> 00:53:50,490
가능하고 그게 의미가

1192
00:53:50,490 --> 00:53:51,573
있을 수 있습니다.

1193
00:53:51,573 --> 00:53:53,990
하지만 일상적인 물체에는 수직

1194
00:53:53,990 --> 00:53:56,240
뒤집기가 별로 의미가 없습니다.

1195
00:53:56,240 --> 00:53:58,160
고양이는 거의 항상 이 자세로

1196
00:53:58,160 --> 00:53:59,330
보이기 때문입니다.

1197
00:53:59,330 --> 00:54:00,440
하지만 고양이가

1198
00:54:00,440 --> 00:54:02,990
다양한 방향으로 있는 데이터셋이라면

1199
00:54:02,990 --> 00:54:06,860
뒤집기나 회전 같은 증강이 데이터셋에 맞을 수 있겠죠.

1200
00:54:06,860 --> 00:54:11,310
또 다른 증강 방법은 크기 조정과 자르기입니다.

1201
00:54:11,310 --> 00:54:16,790
ResNet과 많은 딥러닝 이미지 모델들은

1202
00:54:16,790 --> 00:54:21,140
기본적으로 이미지에서 무작위로 자른

1203
00:54:21,140 --> 00:54:25,400
후 그 부분을 모델 입력 크기로

1204
00:54:25,400 --> 00:54:27,105
다시 조정합니다.

1205
00:54:27,105 --> 00:54:28,980
때로는 또 다른 자르기를 추가로 하기도 합니다.

1206
00:54:28,980 --> 00:54:30,740
가장 일반적인 전략은

1207
00:54:30,740 --> 00:54:33,540
이미지의 짧은 쪽 길이를

1208
00:54:33,540 --> 00:54:35,490
선택하는 것입니다.

1209
00:54:35,490 --> 00:54:40,230
예를 들어 모델 입력 이미지 크기가 224x224

1210
00:54:40,230 --> 00:54:43,980
픽셀이라면, 이보다 큰 값을 먼저

1211
00:54:43,980 --> 00:54:51,150
선택하고 그 크기 L을 포함하는 이미지의 일부를 자릅니다. 이 값들은

1212
00:54:51,150 --> 00:54:53,950
보통 이렇게 사용됩니다.

1213
00:54:53,950 --> 00:54:55,063
이미지를 자릅니다.

1214
00:54:58,163 --> 00:54:59,080
죄송합니다, 자르는 게 아니라,

1215
00:54:59,080 --> 00:55:01,960
이미지를 그 크기로 조정합니다.

1216
00:55:01,960 --> 00:55:04,870
예를 들어 800x600 이미지가

1217
00:55:04,870 --> 00:55:10,350
있고 256을 사용한다면, 짧은 쪽인 600을 256으로 조정하고

1218
00:55:10,350 --> 00:55:12,730
800은 비례해서 조정합니다.

1219
00:55:12,730 --> 00:55:15,990
그래서 이 L 크기로 이미지를 조정하는 겁니다. 짧은 쪽을 L로 스케일링한
다음, 그

1220
00:55:15,990 --> 00:55:19,200
이미지에서 224x224 픽셀 크기의

1221
00:55:19,200 --> 00:55:21,190
랜덤 패치를 자릅니다.

1222
00:55:21,190 --> 00:55:24,070
즉, 상대 해상도를 유지하면서 이미지를

1223
00:55:24,070 --> 00:55:26,350
먼저 스케일링해서 L에 맞게

1224
00:55:26,350 --> 00:55:29,620
작거나 크게 만든 후, 그 이미지에서 랜덤

1225
00:55:29,620 --> 00:55:31,340
크롭을 하는 겁니다.

1226
00:55:31,340 --> 00:55:35,073
이 방법이 가장 널리 쓰이는데, 대부분 라이브러리에서는 random

1227
00:55:35,073 --> 00:55:37,490
resized crop이라고 부릅니다.

1228
00:55:37,490 --> 00:55:40,032
이 방법은 대부분 문제에 사용되는데,

1229
00:55:40,032 --> 00:55:44,902
꽤 잘 작동하고 이미지의 상대 해상도를 유지하기 때문입니다.

1230
00:55:44,902 --> 00:55:46,360
그리고 증강에서 할 수 있는 또

1231
00:55:46,360 --> 00:55:49,150
다른 멋진 기법이 있는데, 바로 테스트 타임 증강입니다.

1232
00:55:49,150 --> 00:55:51,400
최고 성능을 내고 싶다면,

1233
00:55:51,400 --> 00:55:54,520
여러 가지 크롭과 리사이즈를 만들어서 모두

1234
00:55:54,520 --> 00:55:56,860
모델에 통과시키고 마지막에 예측을

1235
00:55:56,860 --> 00:55:58,820
평균 내는 방법입니다.

1236
00:55:58,820 --> 00:56:01,000
ResNet 같은 경우에는

1237
00:56:01,000 --> 00:56:04,240
여러 스케일, 여러 크롭 위치, 심지어는

1238
00:56:04,240 --> 00:56:07,910
좌우 반전까지 시도하는 경우가 많습니다.

1239
00:56:07,910 --> 00:56:10,970
보통은 점점 성능 향상이 줄어들지만, 이런

1240
00:56:10,970 --> 00:56:13,210
테스트 타임 증강으로 1%에서 2%

1241
00:56:13,210 --> 00:56:15,890
정도의 성능 향상을 얻을 수 있습니다.

1242
00:56:15,890 --> 00:56:18,110
그래서 정말 성능이 중요한 상황에서,

1243
00:56:18,110 --> 00:56:20,780
마지막 몇 퍼센트 포인트를 짜내고 싶다면

1244
00:56:20,780 --> 00:56:22,530
거의 모든 컴퓨터 비전 문제에

1245
00:56:22,530 --> 00:56:24,885
쓸 수 있는 아주 좋은 기법입니다.

1246
00:56:27,820 --> 00:56:31,200
자, 마지막으로 몇 가지 증강 기법을 더 살펴보겠습니다.

1247
00:56:31,200 --> 00:56:34,310
첫 번째는 컬러 지터인데, 여기서는 대비와 밝기를

1248
00:56:34,310 --> 00:56:36,320
랜덤하게 조절하고 이미지도

1249
00:56:36,320 --> 00:56:37,920
그에 맞게 스케일링합니다.

1250
00:56:37,920 --> 00:56:40,850
그래서 이미지가 더 흐릿해 보이거나, 색상이

1251
00:56:40,850 --> 00:56:43,830
더 흐릿하거나 더 밝아 보일 수 있는데,

1252
00:56:43,830 --> 00:56:47,250
이런 기법들은 전통적인 이미지 처리 방법입니다.

1253
00:56:47,250 --> 00:56:49,860
이런 다양한 증강 기법을 쓸 때는 여러

1254
00:56:49,860 --> 00:56:52,190
값을 시도해 보면서, 이미지가 여전히

1255
00:56:52,190 --> 00:56:55,133
분포 내에 있고 인간이 보기에도 자연스러운지

1256
00:56:55,133 --> 00:56:56,550
확인하는 게 보통입니다.

1257
00:56:56,550 --> 00:56:58,302
이렇게 해서 얼마나 지터를

1258
00:56:58,302 --> 00:57:00,260
줄지, 밝기 변화량을 얼마로

1259
00:57:00,260 --> 00:57:02,910
할지 결정하는 게 좋은 판단 기준입니다.

1260
00:57:02,910 --> 00:57:04,680
보통 제가 문제를 시작할 때는

1261
00:57:04,680 --> 00:57:06,390
여러 증강 기법을 시도해 보고,

1262
00:57:06,390 --> 00:57:08,090
원본 데이터와 다르면서도

1263
00:57:08,090 --> 00:57:09,840
여전히 인식

1264
00:57:09,840 --> 00:57:13,130
가능하고 쉽게 알아볼 수 있는 정도를

1265
00:57:13,130 --> 00:57:13,800
확인합니다.

1266
00:57:13,800 --> 00:57:18,080
그리고 일반적으로 사용하기 좋은 증강 기법들입니다.

1267
00:57:18,080 --> 00:57:19,970
마지막으로는 이미지의 일부를

1268
00:57:19,970 --> 00:57:21,470
잘라내고 그 부분에

1269
00:57:21,470 --> 00:57:22,970
검은색이나 회색 박스를

1270
00:57:22,970 --> 00:57:25,130
씌우는 것을 상상할 수 있습니다.

1271
00:57:25,130 --> 00:57:27,550
이 방법은 아마 덜 흔하게 쓰이지만,

1272
00:57:27,550 --> 00:57:29,050
문제에 따라 증강 기법을

1273
00:57:29,050 --> 00:57:30,520
창의적으로 활용할 수

1274
00:57:30,520 --> 00:57:32,090
있다는 점을 보여줍니다.

1275
00:57:32,090 --> 00:57:35,000
예를 들어 카메라가 가려져서 물체를

1276
00:57:35,000 --> 00:57:38,403
완전히 볼 수 없는 상황이라면, 이런 방법이

1277
00:57:38,403 --> 00:57:40,570
모델이 물체 일부가 가려져도

1278
00:57:40,570 --> 00:57:42,612
견고해지도록 만드는 멋진

1279
00:57:42,612 --> 00:57:44,230
트릭이 될 수 있습니다.

1280
00:57:44,230 --> 00:57:46,880
그래서 주어진 상황에 맞게 어떤 증강 기법이 적절한지

1281
00:57:46,880 --> 00:57:48,170
상상해볼 수 있습니다.

1282
00:57:48,170 --> 00:57:51,280
입력 데이터를 인간이 인식할 수

1283
00:57:51,280 --> 00:57:53,508
있으면서도 모델이 학습

1284
00:57:53,508 --> 00:57:56,050
예제를 외우기 어렵게 변형하는

1285
00:57:56,050 --> 00:57:58,960
방법이 무엇인지 생각하는 거죠.

1286
00:57:58,960 --> 00:58:02,500
자, 여기서 마지막 주제들은

1287
00:58:02,500 --> 00:58:04,940
매우 실용적입니다.

1288
00:58:04,940 --> 00:58:06,940
프로젝트를 하거나 모델을

1289
00:58:06,940 --> 00:58:11,470
훈련할 때, 예를 들어 여러분의 코스 프로젝트에서 앞으로

1290
00:58:11,470 --> 00:58:14,230
설명할 내용을 꼭 따라 하시길

1291
00:58:14,230 --> 00:58:16,463
권합니다. 하지만 이 내용은

1292
00:58:16,463 --> 00:58:18,130
코스 외에도 모든

1293
00:58:18,130 --> 00:58:21,440
컴퓨터 비전 분야에 적용할 수 있습니다.

1294
00:58:21,440 --> 00:58:25,720
실제로 많은 경우에 데이터가 충분하지 않은 경우가

1295
00:58:25,720 --> 00:58:26,890
많습니다.

1296
00:58:26,890 --> 00:58:29,728
ImageNet 원본은 백만 장의 이미지가 있었죠.

1297
00:58:29,728 --> 00:58:32,020
여러분 문제에 백만 장의 이미지가 없을

1298
00:58:32,020 --> 00:58:33,780
수도 있습니다. 거의 대부분이

1299
00:58:33,780 --> 00:58:36,970
그렇죠, 대규모 팀과 방대한 데이터를 수집하지 않는 이상요.

1300
00:58:36,970 --> 00:58:40,380
데이터가 많지 않아도 CNN을 훈련할 수 있을까요?

1301
00:58:40,380 --> 00:58:42,370
간단히 말하면, 네 할 수

1302
00:58:42,370 --> 00:58:45,870
있습니다. 다만 조금 똑똑하게 해야 합니다.

1303
00:58:45,870 --> 00:58:48,183
지난 강의 중 하나에서

1304
00:58:48,183 --> 00:58:49,600
CNN의 다양한

1305
00:58:49,600 --> 00:58:52,620
필터가 서로 다른 종류의 특징을

1306
00:58:52,620 --> 00:58:56,093
추출하는 방법을 보여드렸습니다.

1307
00:58:56,093 --> 00:58:57,510
이것은 누군가가 컨볼루션

1308
00:58:57,510 --> 00:59:00,240
신경망에서 특징의 계층 구조에 대해 질문했을 때로 거슬러

1309
00:59:00,240 --> 00:59:00,940
올라갑니다.

1310
00:59:00,940 --> 00:59:03,690
처음에는 가장자리나 패턴, 아주 작은

1311
00:59:03,690 --> 00:59:05,740
형태 같은 것들을 추출합니다.

1312
00:59:05,740 --> 00:59:08,115
그리고 가장 높은 수준에서,

1313
00:59:08,115 --> 00:59:11,980
만약 우리가 이미지를 CNN에 넣고

1314
00:59:11,980 --> 00:59:16,380
클래스 점수 바로 직전에 나오는 최종 벡터를

1315
00:59:16,380 --> 00:59:21,570
얻은 다음, 데이터셋 내 다른 이미지들과 비교하면, 이

1316
00:59:21,570 --> 00:59:25,530
이미지 벡터 값들이 실제로 매우 가깝다는

1317
00:59:25,530 --> 00:59:27,910
것을 알 수 있습니다.

1318
00:59:27,910 --> 00:59:31,890
이것을 이전에 했던 최근접 이웃(nearest

1319
00:59:31,890 --> 00:59:34,770
neighbors)과 비슷하게 생각할 수

1320
00:59:34,770 --> 00:59:38,370
있는데, 이번에는 이미지의 픽셀이 아니라 CNN의

1321
00:59:38,370 --> 00:59:41,190
분류 계층 바로 직전, 예를 들어

1322
00:59:41,190 --> 00:59:45,060
4096 또는 2048 차원 벡터를 보는 겁니다.

1323
00:59:45,060 --> 00:59:49,590
여기서 차이를 보면, L2 거리입니다.

1324
00:59:49,590 --> 00:59:52,330
주어진 이미지를 모델에 넣고

1325
00:59:52,330 --> 00:59:53,880
마지막 계층을 제외한

1326
00:59:53,880 --> 00:59:55,890
모든 계층을 거친

1327
00:59:55,890 --> 00:59:57,990
후 이 벡터 공간에서

1328
00:59:57,990 --> 01:00:00,690
가까운 다른 이미지를 보면,

1329
01:00:00,690 --> 01:00:03,600
같은 카테고리에 속한 이미지들이

1330
01:00:03,600 --> 01:00:06,700
매우 가깝다는 것을 알 수 있습니다.

1331
01:00:06,700 --> 01:00:08,760
직관적으로 말하면,

1332
01:00:08,760 --> 01:00:14,400
이 특징들은 실제로 매우 훌륭해서, 이 위에 선형

1333
01:00:14,400 --> 01:00:17,730
분류기나 K-최근접 이웃 분류기를

1334
01:00:17,730 --> 01:00:19,440
만들어서 객체를

1335
01:00:19,440 --> 01:00:23,050
매우 잘 분류할 수 있다는 뜻입니다.

1336
01:00:23,050 --> 01:00:25,700
그럼 이걸 실제로 어떻게 사용할 수 있을까요?

1337
01:00:25,700 --> 01:00:28,480
먼저 ImageNet에서 모델을

1338
01:00:28,480 --> 01:00:30,760
학습시키거나, 누군가가

1339
01:00:30,760 --> 01:00:33,890
ImageNet이나 대규모 인터넷 스케일

1340
01:00:33,890 --> 01:00:37,370
데이터셋에서 학습한 모델을 가져옵니다.

1341
01:00:37,370 --> 01:00:40,930
그리고 이 모든 계층을 고정(freeze)해서

1342
01:00:40,930 --> 01:00:42,350
학습하지 않습니다.

1343
01:00:42,350 --> 01:00:45,760
이전과 똑같이 유지하고, 마지막 계층만

1344
01:00:45,760 --> 01:00:48,822
ImageNet의 1000개 클래스

1345
01:00:48,822 --> 01:00:51,280
대신 여러분 데이터셋의 클래스

1346
01:00:51,280 --> 01:00:52,520
수로 교체합니다.

1347
01:00:52,520 --> 01:00:54,320
그 다음 모델을 학습할 때는

1348
01:00:54,320 --> 01:00:56,150
이 마지막 계층만 학습합니다.

1349
01:00:56,150 --> 01:01:00,220
예전 컴퓨터 비전 패러다임에서 색상

1350
01:01:00,220 --> 01:01:02,385
히스토그램 같은

1351
01:01:02,385 --> 01:01:03,760
미리 정의된

1352
01:01:03,760 --> 01:01:05,830
특징을 추출하는

1353
01:01:05,830 --> 01:01:07,720
feature

1354
01:01:07,720 --> 01:01:10,570
extractor가 있었는데,

1355
01:01:10,570 --> 01:01:13,820
고정된 모델은 거의 이와 비슷하다고 생각할 수 있습니다.

1356
01:01:13,820 --> 01:01:15,700
변경하지 않는 미리 정의된

1357
01:01:15,700 --> 01:01:17,390
특징 추출기 역할을 하면서,

1358
01:01:17,390 --> 01:01:19,780
그 위에 학습할 모델을 위한 특징을

1359
01:01:19,780 --> 01:01:20,975
계산하는 거죠.

1360
01:01:20,975 --> 01:01:23,100
이 패러다임과 매우 유사합니다,

1361
01:01:23,100 --> 01:01:25,150
왜냐하면 여기서는 학습하지 않으니까요.

1362
01:01:25,150 --> 01:01:27,330
더 큰 데이터셋이 있다면,

1363
01:01:27,330 --> 01:01:29,550
실제로는 전체 모델을 학습하는

1364
01:01:29,550 --> 01:01:32,290
것이 가장 잘 작동하는데, 이때 초기값은

1365
01:01:32,290 --> 01:01:35,400
ImageNet이나 다른 대규모 인터넷

1366
01:01:35,400 --> 01:01:38,310
스케일 데이터셋에서 사전 학습된 값으로

1367
01:01:38,310 --> 01:01:39,400
설정합니다.

1368
01:01:39,400 --> 01:01:43,110
제가 다루는 거의 모든 문제에서, 아마

1369
01:01:43,110 --> 01:01:48,150
백만에서 천만 개 정도의 학습 예제가 있어서, 여기서 세

1370
01:01:48,150 --> 01:01:51,400
번째 단계인 이 방법을 사용합니다.

1371
01:01:51,400 --> 01:01:54,180
그래서 저는 수십억 개의 데이터로 학습된 모델로 시작할

1372
01:01:54,180 --> 01:01:55,870
텐데, 저는 그만큼의 연산 능력이

1373
01:01:55,870 --> 01:01:59,400
없기 때문입니다. 그리고 나서 제 상대적으로 작은 데이터셋으로 모델을

1374
01:01:59,400 --> 01:02:00,340
미세 조정할 겁니다.

1375
01:02:00,340 --> 01:02:02,257
그렇게 하면 제가 직접 모델을 학습시키는 것보다

1376
01:02:02,257 --> 01:02:03,640
더 좋은 성능을 얻을 수 있습니다.

1377
01:02:03,640 --> 01:02:05,710
왜냐하면 그 모델은 기본적으로 더 많은 데이터를 본 것이기 때문입니다.

1378
01:02:05,710 --> 01:02:08,400
더 나은 특징 추출기를 만들어낸 거죠.

1379
01:02:08,400 --> 01:02:10,150
그리고 전체를 미세 조정할 때에도

1380
01:02:10,150 --> 01:02:12,370
제 문제에 충분히 특화될 수 있습니다.

1381
01:02:12,370 --> 01:02:14,520
기본적으로 예를 들어 ImageNet에서

1382
01:02:14,520 --> 01:02:16,020
모델을 학습시키는

1383
01:02:16,020 --> 01:02:18,060
아주 구체적인 경우를 생각해 봅시다.

1384
01:02:18,060 --> 01:02:21,370
우리는 이 모델을 가져와서 마지막 레이어를 교체합니다.

1385
01:02:21,370 --> 01:02:25,000
그래서 더 이상 1,000개의 클래스를 출력하지 않게 하는 거죠.

1386
01:02:25,000 --> 01:02:28,670
대신 여러분 데이터셋의 클래스 수를 출력하도록 합니다.

1387
01:02:28,670 --> 01:02:31,870
그리고 앞서 이야기한 Kaiming Initialization을

1388
01:02:31,870 --> 01:02:33,530
사용해 이 레이어를

1389
01:02:33,530 --> 01:02:36,640
무작위로 초기화하지만, 나머지 레이어들은 이전에

1390
01:02:36,640 --> 01:02:38,420
가지고 있던 값을 유지합니다.

1391
01:02:38,420 --> 01:02:40,190
그래서 이 값들은 바꾸지 않고,

1392
01:02:40,190 --> 01:02:41,590
경사 하강법 동안에도 이 값들은

1393
01:02:41,590 --> 01:02:42,950
절대 바뀌지 않습니다.

1394
01:02:42,950 --> 01:02:45,110
즉, 이 값들은 변하지 않는 겁니다.

1395
01:02:45,110 --> 01:02:46,490
우리는 기본적으로 이미지를 가져옵니다.

1396
01:02:46,490 --> 01:02:47,905
그 이미지를 모델에 통과시키고,

1397
01:02:47,905 --> 01:02:50,530
이제 마치 선형 분류기를 학습시키는

1398
01:02:50,530 --> 01:02:54,640
것과 같습니다. 입력은 모델 전체를 통과시켜

1399
01:02:54,640 --> 01:02:57,550
계산한 각 이미지당 4,096차원의

1400
01:02:57,550 --> 01:02:58,550
벡터인 거죠.

1401
01:02:58,550 --> 01:03:00,782
그다음 4,096차원 벡터를 가지고,

1402
01:03:00,782 --> 01:03:02,990
그걸 클래스 수에 매핑하는 겁니다.

1403
01:03:02,990 --> 01:03:05,680
그리고 우리는 이 매핑을 마지막에만 학습합니다.

1404
01:03:05,680 --> 01:03:09,280
네, 질문은 ImageNet으로 학습했기 때문에

1405
01:03:09,280 --> 01:03:10,880
모델에 편향이 있을까요?

1406
01:03:10,880 --> 01:03:12,140
답은 확실히 그렇습니다.

1407
01:03:12,140 --> 01:03:17,740
그래서 두 번째 방법, 이렇게 학습하면 ImageNet과

1408
01:03:17,740 --> 01:03:22,490
매우 비슷한 데이터셋에서 가장 좋은

1409
01:03:22,490 --> 01:03:24,270
성능을 냅니다.

1410
01:03:24,270 --> 01:03:27,230
예를 들어 노트북이나 교실, 사람 같은

1411
01:03:27,230 --> 01:03:31,340
일상적인 사물 사진들이죠. ImageNet은 일상적인

1412
01:03:31,340 --> 01:03:32,640
물체들입니다.

1413
01:03:32,640 --> 01:03:35,850
하지만 만약 화성 사진이라면 훨씬 성능이 떨어질 겁니다.

1414
01:03:35,850 --> 01:03:37,970
즉, 사전 학습된 모델의 학습 데이터에 기반한

1415
01:03:37,970 --> 01:03:39,283
편향이 분명히 존재합니다.

1416
01:03:39,283 --> 01:03:40,700
그래서 같은

1417
01:03:40,700 --> 01:03:42,860
유형의 분포, 즉 비슷한

1418
01:03:42,860 --> 01:03:45,830
종류의 물체나 장소를 보는

1419
01:03:45,830 --> 01:03:47,600
데이터가 필요합니다.

1420
01:03:47,600 --> 01:03:50,390
그렇다면 데이터셋이 분포 밖일 때는

1421
01:03:50,390 --> 01:03:51,650
어떻게 해야 할까요?

1422
01:03:51,650 --> 01:03:55,590
이 부분을 다룬 슬라이드가 있습니다.

1423
01:03:55,590 --> 01:03:57,050
정말 좋은 질문입니다.

1424
01:03:57,050 --> 01:04:00,685
매우 비슷한 데이터셋이지만 데이터가 적다면, 아까 말한

1425
01:04:00,685 --> 01:04:02,060
선형 분류기 전략을

1426
01:04:02,060 --> 01:04:03,313
사용할 수 있습니다.

1427
01:04:03,313 --> 01:04:05,480
비슷한 데이터셋에 데이터가 꽤 많다면, 모든 레이어를

1428
01:04:05,480 --> 01:04:07,320
미세 조정하는 것이 가장 좋은 성능을 냅니다.

1429
01:04:07,320 --> 01:04:08,737
이것이 앞서 언급한 이

1430
01:04:08,737 --> 01:04:10,640
슬라이드의 두 번째와 세 번째 전략입니다.

1431
01:04:10,640 --> 01:04:12,932
그렇다면 아주 다른 데이터셋이라면 어떻게 할까요?

1432
01:04:12,932 --> 01:04:16,280
데이터가 많다면 처음부터 시작하거나,

1433
01:04:16,280 --> 01:04:20,810
초기화만 해도 더 나은 성능을 낼 수

1434
01:04:20,810 --> 01:04:21,810
있습니다.

1435
01:04:21,810 --> 01:04:23,893
여기서 테스트를 하겠지만, 성능이 더

1436
01:04:23,893 --> 01:04:26,190
좋아질지 나빠질지 보장할 방법은 없습니다.

1437
01:04:26,190 --> 01:04:29,430
그리고 데이터가 아주 적거나 데이터 세트가 매우 다르다면,

1438
01:04:29,430 --> 01:04:31,020
아마도 비슷한 데이터로

1439
01:04:31,020 --> 01:04:33,130
훈련된 모델을 찾으려고 할 겁니다.

1440
01:04:33,130 --> 01:04:35,460
연구자들이 도메인 밖 일반화(out-of-domain

1441
01:04:35,460 --> 01:04:38,410
generalization)를 위해 살펴본

1442
01:04:38,410 --> 01:04:41,420
특정 기법들이 있습니다. 기본 아이디어는 하나의 도메인에서

1443
01:04:41,420 --> 01:04:43,170
모델을 훈련시키고, 어떤 면에서

1444
01:04:43,170 --> 01:04:45,430
다른 새로운 도메인을 학습하려는 겁니다.

1445
01:04:45,430 --> 01:04:47,140
이것은 활발한 연구 분야이지만,

1446
01:04:47,140 --> 01:04:50,100
항상 작동하는 일반적인 기법이 있다고는 할

1447
01:04:50,100 --> 01:04:52,690
수 없고, 문제에 따라 다릅니다. 반면에 이

1448
01:04:52,690 --> 01:04:56,150
방법은 여기 오른쪽 위 사분면을 제외한 모든 경우에

1449
01:04:56,150 --> 01:04:56,650
적용됩니다.

1450
01:04:56,650 --> 01:04:58,060
실제로 꽤 잘 작동합니다.

1451
01:04:58,060 --> 01:04:59,950
사실 이런 기법들이 있습니다.

1452
01:04:59,950 --> 01:05:01,510
그리고 꽤 활발한 연구 분야입니다.

1453
01:05:01,510 --> 01:05:02,890
특정 모델들은 더 잘 일반화합니다.

1454
01:05:02,890 --> 01:05:04,307
예를 들어, 언어 모델은

1455
01:05:04,307 --> 01:05:07,020
다양한 도메인을 학습하는 데 꽤 능숙한 편입니다.

1456
01:05:07,020 --> 01:05:10,470
하지만 완전히 새로운 문제이고, 아무도

1457
01:05:10,470 --> 01:05:12,892
다뤄본 적 없으며 데이터도 많지

1458
01:05:12,892 --> 01:05:14,350
않은 상황이 가장

1459
01:05:14,350 --> 01:05:15,340
최악입니다.

1460
01:05:15,340 --> 01:05:17,890
그런 환경에서 모델을 훈련하는 것이 단연코 가장 어렵습니다.

1461
01:05:17,890 --> 01:05:19,860
그래서 질문은, 마지막 한 층만 훈련하는 것과 모든 층을

1462
01:05:19,860 --> 01:05:20,950
훈련하는 것 사이에 뭔가 하시나요?

1463
01:05:20,950 --> 01:05:22,492
네, 실제로 일부

1464
01:05:22,492 --> 01:05:25,770
층만 훈련하는 연구가 많이 이루어졌습니다.

1465
01:05:25,770 --> 01:05:27,825
LoRA라는 기법도 있는데, 트랜스포머

1466
01:05:27,825 --> 01:05:29,950
강의에서 다룰 수도 있습니다.

1467
01:05:29,950 --> 01:05:31,930
올해 다룰지는 확실하지

1468
01:05:31,930 --> 01:05:34,152
않지만, 기본 아이디어는

1469
01:05:34,152 --> 01:05:36,360
모든 층을 미세 조정하되

1470
01:05:36,360 --> 01:05:41,820
모든 값을 정확히 바꾸는 것이 아니라, 각 층 간의 저랭크

1471
01:05:41,820 --> 01:05:44,340
차이만 학습하는 방식입니다. 즉,

1472
01:05:44,340 --> 01:05:47,160
층 자체를 미세 조정하는 대신

1473
01:05:47,160 --> 01:05:49,830
원래 층 간의 차이를 미세

1474
01:05:49,830 --> 01:05:51,040
조정하는 겁니다.

1475
01:05:51,040 --> 01:05:55,410
그래서 네, LoRA 같은 기법을 사용할 수 있습니다.

1476
01:05:55,410 --> 01:05:57,835
더 자세한 설명이 필요하지만,

1477
01:05:57,835 --> 01:06:00,460
기본 아이디어는 실제 값들을 미세

1478
01:06:00,460 --> 01:06:03,480
조정하는 대신 ResNet처럼 값 층

1479
01:06:03,480 --> 01:06:06,143
간의 차이를 미세 조정한다는 겁니다.

1480
01:06:06,143 --> 01:06:07,560
LoRA도 비슷한데, 아주

1481
01:06:07,560 --> 01:06:09,690
적은 수의 파라미터로 이 작업을 합니다.

1482
01:06:09,690 --> 01:06:12,000
질문은 기본적으로 몇 개의 층을 선택할지

1483
01:06:12,000 --> 01:06:13,750
어떻게 결정하느냐인 것 같습니다.

1484
01:06:13,750 --> 01:06:16,180
왜 많은 층을 선택했을까요?

1485
01:06:16,180 --> 01:06:18,720
특히 왜 각 크기별로 두 개의 convolution

1486
01:06:18,720 --> 01:06:20,940
층이 있고 하나가 아닌가요?

1487
01:06:20,940 --> 01:06:23,190
사실 이건 우리가 앞서 VGG

1488
01:06:23,190 --> 01:06:26,130
예제에서 본 것과 매우 비슷합니다. 3개의

1489
01:06:26,130 --> 01:06:30,600
3x3 convolution을 사용하면 7x7 convolution과

1490
01:06:30,600 --> 01:06:32,640
같은 수용 영역을

1491
01:06:32,640 --> 01:06:35,880
가지면서도, 7x7 필터 하나보다 3개의

1492
01:06:35,880 --> 01:06:38,910
활성화 함수 덕분에 더 많은 비선형 관계를

1493
01:06:38,910 --> 01:06:40,390
모델링할 수 있습니다.

1494
01:06:40,390 --> 01:06:42,820
즉, 3x3 필터가 더 표현력이

1495
01:06:42,820 --> 01:06:44,970
뛰어나지만, 충분한 수만 있으면 같은

1496
01:06:44,970 --> 01:06:46,510
값 집합을 보는 거죠.

1497
01:06:46,510 --> 01:06:48,960
그래서 더 작은 필터를 많이 사용하는 게 더 큰

1498
01:06:48,960 --> 01:06:51,455
필터를 적게 사용하는 것보다 표현력이 좋습니다.

1499
01:06:55,320 --> 01:06:57,523
좋습니다, 계속하겠습니다.

1500
01:06:57,523 --> 01:06:59,190
네, 기본적으로 비슷한 데이터를

1501
01:06:59,190 --> 01:07:00,810
가진 큰 데이터셋을 찾으세요.

1502
01:07:00,810 --> 01:07:02,310
그 데이터셋으로 학습된 모델을

1503
01:07:02,310 --> 01:07:04,800
가져와서 자신의 데이터에 맞게 미세 조정하는 겁니다.

1504
01:07:04,800 --> 01:07:06,180
몇 가지 좋은 링크가 있습니다.

1505
01:07:06,180 --> 01:07:08,010
PyTorch image models에는

1506
01:07:08,010 --> 01:07:10,380
ImageNet 등 다양한 데이터셋으로

1507
01:07:10,380 --> 01:07:14,270
학습된 모델들이 많고, PyTorch 버전 GitHub 저장소도 있습니다.

1508
01:07:14,270 --> 01:07:15,260
거기서도 몇 가지 찾으실 수 있을 겁니다.

1509
01:07:15,260 --> 01:07:17,680
좋습니다, 끝부분에서 하이퍼파라미터 선택에

1510
01:07:17,680 --> 01:07:19,190
대해 간단히 말씀드리겠습니다.

1511
01:07:19,190 --> 01:07:22,630
모델 학습이 잘 안 되고 바로 작동하지 않는다면,

1512
01:07:22,630 --> 01:07:24,060
가장 좋은 방법은

1513
01:07:24,060 --> 01:07:27,140
작은 샘플에 과적합을 시도하는 겁니다.

1514
01:07:27,140 --> 01:07:29,650
이건 딥러닝에서 기본 디버깅 전략으로,

1515
01:07:29,650 --> 01:07:31,960
데이터 포인트 하나만 가지고 학습

1516
01:07:31,960 --> 01:07:34,503
손실이 0에 가까워지는지 확인하는 겁니다.

1517
01:07:34,503 --> 01:07:36,920
모델이 하나의 학습 예제를 암기할 수 있어야

1518
01:07:36,920 --> 01:07:38,560
하고, 그렇지 않으면 코드에

1519
01:07:38,560 --> 01:07:41,380
버그가 있거나 문제를 모델링하기에 적절한

1520
01:07:41,380 --> 01:07:42,910
모델을 선택하지 않은 겁니다.

1521
01:07:42,910 --> 01:07:45,980
이건 정말 좋은 학습 문제입니다.

1522
01:07:45,980 --> 01:07:47,980
또 어떤 학습률이 작동하는지, 어떤 게

1523
01:07:47,980 --> 01:07:49,250
안 되는지 알려줍니다.

1524
01:07:49,250 --> 01:07:50,890
그리고 탐색할 학습률 범위에 대한 대략적인

1525
01:07:50,890 --> 01:07:51,975
아이디어를 얻을 수 있습니다.

1526
01:07:51,975 --> 01:07:54,350
이 방법은 모델이 올바른지, 학습률이

1527
01:07:54,350 --> 01:07:57,197
적절한지, 그리고 코드에 영향을 줄

1528
01:07:57,197 --> 01:07:59,530
수 있는 다른 버그가 없는지 확인하는

1529
01:07:59,530 --> 01:08:00,950
좋은 방법입니다.

1530
01:08:00,950 --> 01:08:02,860
항상 첫 번째 단계입니다.

1531
01:08:02,860 --> 01:08:06,350
코드 실행에 문제가 있다면 이렇게

1532
01:08:06,350 --> 01:08:08,140
디버깅하세요.

1533
01:08:08,140 --> 01:08:09,910
이걸 해결한 후 두 번째로 할

1534
01:08:09,910 --> 01:08:12,178
일은 아주 거친 하이퍼파라미터 그리드를

1535
01:08:12,178 --> 01:08:12,970
시도하는 겁니다.

1536
01:08:12,970 --> 01:08:15,540
먼저 다양한 학습률로 시도해보고,

1537
01:08:15,540 --> 01:08:18,347
각 학습률에서 학습 손실이 어떻게

1538
01:08:18,347 --> 01:08:19,930
변하는지 확인하세요.

1539
01:08:19,930 --> 01:08:23,279
한 에폭 동안 학습 손실이 가장 지속적으로 감소하는

1540
01:08:23,279 --> 01:08:25,689
학습률을 선택하는 게 좋습니다.

1541
01:08:25,689 --> 01:08:29,069
이건 꽤 좋은 추정치지만, 더 오래 학습할 수도 있습니다.

1542
01:08:29,069 --> 01:08:30,939
좋은 학습률 집합을 찾으면 다른

1543
01:08:30,939 --> 01:08:33,279
하이퍼파라미터도 살펴볼 수 있습니다.

1544
01:08:33,279 --> 01:08:35,380
특히 손실 외에도

1545
01:08:35,380 --> 01:08:38,189
정확도 곡선을 봐야 합니다.

1546
01:08:38,189 --> 01:08:39,660
학습 정확도와

1547
01:08:39,660 --> 01:08:41,460
검증 정확도가 있죠.

1548
01:08:41,460 --> 01:08:43,380
둘 다 계속 상승한다면 학습을

1549
01:08:43,380 --> 01:08:45,819
계속하는 게 합리적입니다만,

1550
01:08:45,819 --> 01:08:48,270
학습 손실은 증가하는데 검증 손실은

1551
01:08:48,270 --> 01:08:50,979
감소하는 상황도 있을 수 있습니다.

1552
01:08:50,979 --> 01:08:52,899
이것이 과적합입니다.

1553
01:08:52,899 --> 01:08:56,437
그래서 정규화를 늘리거나 더 많은 데이터를 얻을 수

1554
01:08:56,437 --> 01:08:58,479
있다면 그것도 효과적일 수 있지만,

1555
01:08:58,479 --> 01:09:00,776
성능을 이 지점 이상의 최고점까지

1556
01:09:00,776 --> 01:09:02,609
향상시키려면 둘 중 하나를

1557
01:09:02,609 --> 01:09:04,680
해야 합니다. 지금 이 지점이

1558
01:09:04,680 --> 01:09:07,229
아마도 지금까지의 최적 모델일 겁니다.

1559
01:09:07,229 --> 01:09:09,870
여기서 차이가 거의 보이지

1560
01:09:09,870 --> 01:09:14,450
않는다면 아마 더 오래 학습해도 괜찮습니다. 일반적으로 검증

1561
01:09:14,450 --> 01:09:18,680
손실이 최대가 되는 지점까지 학습하는 게 좋기

1562
01:09:18,680 --> 01:09:19,470
때문입니다.

1563
01:09:19,470 --> 01:09:21,649
그래서 계속 학습할 수 있다면 계속

1564
01:09:21,649 --> 01:09:22,649
학습할 수 있습니다.

1565
01:09:22,649 --> 01:09:25,399
여기서 큰 차이가 없거나

1566
01:09:25,399 --> 01:09:31,085
검증 정확도가 학습 정확도와 비슷하다면, 학습

1567
01:09:31,085 --> 01:09:32,460
정확도가 검증

1568
01:09:32,460 --> 01:09:35,029
정확도와 차이가 나기 시작할

1569
01:09:35,029 --> 01:09:39,050
때까지 계속 학습할 수 있습니다.

1570
01:09:39,050 --> 01:09:41,600
그리고 이 과정을 반복해서 계속 진행할

1571
01:09:41,600 --> 01:09:42,410
수 있습니다.

1572
01:09:42,410 --> 01:09:46,529
마지막으로 하이퍼파라미터 탐색에 대해 말씀드리면, 보통 두 개

1573
01:09:46,529 --> 01:09:49,160
이상의 하이퍼파라미터를 탐색할 때

1574
01:09:49,160 --> 01:09:50,930
모든 조합을 시도해야 하는지,

1575
01:09:50,930 --> 01:09:53,015
아니면 더 좋은 방법이 있는지

1576
01:09:53,015 --> 01:09:54,390
궁금해하실 겁니다.

1577
01:09:54,390 --> 01:09:57,620
실제로는 미리 정해진 모든 조합을 시도하는

1578
01:09:57,620 --> 01:10:00,470
그리드 서치보다 하이퍼파라미터

1579
01:10:00,470 --> 01:10:03,810
공간에서 무작위 탐색이 훨씬 더 효과적입니다.

1580
01:10:03,810 --> 01:10:05,780
그 이유는 한 축은 중요하지

1581
01:10:05,780 --> 01:10:08,060
않은 하이퍼파라미터라서 값에 상관없이

1582
01:10:08,060 --> 01:10:09,893
성능이 거의 같고, 다른 축은

1583
01:10:09,893 --> 01:10:11,602
중요한 하이퍼파라미터일 수

1584
01:10:11,602 --> 01:10:12,740
있기 때문입니다.

1585
01:10:12,740 --> 01:10:14,700
무작위로 값을 선택하면 중요한

1586
01:10:14,700 --> 01:10:16,910
하이퍼파라미터 공간을 훨씬

1587
01:10:16,910 --> 01:10:19,280
더 철저히 탐색할 수 있지만,

1588
01:10:19,280 --> 01:10:23,000
그리드 서치는 중요하지 않은 하이퍼파라미터의 여러

1589
01:10:23,000 --> 01:10:25,010
값을 반복해서 확인하느라 시간을

1590
01:10:25,010 --> 01:10:26,520
낭비하게 됩니다.

1591
01:10:26,520 --> 01:10:28,040
그래서 실제로는 시도할

1592
01:10:28,040 --> 01:10:30,950
범위를 정하고 그 범위 내에서 무작위로

1593
01:10:30,950 --> 01:10:34,040
하이퍼파라미터 값을 뽑는 게 가장 좋습니다.

1594
01:10:34,040 --> 01:10:35,790
그리고 그렇게 하면서 최적 모델을 찾을 때까지 계속 진행하면 됩니다.

1595
01:10:35,790 --> 01:10:38,082
네, 여기까지입니다.

1596
01:10:38,082 --> 01:10:39,390
오늘은 CNN의 레이어, 활성화 함수, CNN 아키텍처, 가중치 초기화,
모델을 어떻게 미리 정의하고 구축하는지, 그리고 실제로 어떻게 학습시키는지에
대해 이야기했습니다.

1597
01:10:39,390 --> 01:10:42,210
데이터를 모델

1598
01:10:42,210 --> 01:10:44,160
입력으로

1599
01:10:44,160 --> 01:10:46,710
어떻게

1600
01:10:46,710 --> 01:10:48,960
변환하는지,

1601
01:10:48,960 --> 01:10:51,210
어떻게 증강하는지,

1602
01:10:51,210 --> 01:10:52,640
전이 학습이 성능 향상에 얼마나 좋은 방법인지,

1603
01:10:52,640 --> 01:10:54,320
그리고 최적의

1604
01:10:54,320 --> 01:10:56,778
하이퍼파라미터를 어떻게 선택하는지

1605
01:10:56,778 --> 01:10:57,450
다뤘습니다.

1606
01:10:57,450 --> 01:10:59,610
오늘 강의에서 많은 내용을 다뤘네요.

1607
01:10:59,610 --> 01:11:00,710
모두 정말 감사합니다.

1608
01:11:00,710 --> 01:11:02,260
네.
