1
00:00:05,270 --> 00:00:06,410
안녕하세요, 여러분.

2
00:00:06,410 --> 00:00:07,415
제 이름은 제인입니다.

3
00:00:07,415 --> 00:00:09,290
제가 첫 번째 강의에서 제 소개를 하지

4
00:00:09,290 --> 00:00:12,058
않았다는 것을 깨달았습니다. 그 강의는 세 번째 강의였고,

5
00:00:12,058 --> 00:00:14,100
저는 이 과정의 공동 강사 중 한 명입니다.

6
00:00:14,100 --> 00:00:15,100
제 이름은 제인 두란테입니다.

7
00:00:15,100 --> 00:00:17,040
저는 에산과 페이페이의 공동 지도 아래 있습니다.

8
00:00:17,040 --> 00:00:22,430
저는 스탠포드에서 4학년 박사 과정 학생이며, 오늘

9
00:00:22,430 --> 00:00:24,260
강의에서는 합성곱 신경망

10
00:00:24,260 --> 00:00:26,330
훈련과 CNN

11
00:00:26,330 --> 00:00:29,420
아키텍처에 대해 이야기할 것입니다.

12
00:00:29,420 --> 00:00:33,260
그래서 이 강의는 두 가지 구성 요소로 나뉘어 있다고

13
00:00:33,260 --> 00:00:34,770
할 수 있습니다.

14
00:00:34,770 --> 00:00:36,290
첫 번째는 우리가

15
00:00:36,290 --> 00:00:39,590
배운 다양한 구성 요소, 즉 합성곱 층, 선형

16
00:00:39,590 --> 00:00:42,030
층 또는 완전 연결 층을 어떻게

17
00:00:42,030 --> 00:00:43,970
조합하여 CNN 아키텍처를

18
00:00:43,970 --> 00:00:46,350
만드는지에 대한 것입니다.

19
00:00:46,350 --> 00:00:48,170
몇 가지 예제를 살펴보고, 이러한

20
00:00:48,170 --> 00:00:50,060
네트워크를 실제로 어떻게 훈련하는지와

21
00:00:50,060 --> 00:00:53,070
그 과정에 관련된 모든 단계에 대해서도 이야기할 것입니다.

22
00:00:53,070 --> 00:00:54,800
앞서 언급했듯이, 기본적으로

23
00:00:54,800 --> 00:00:56,520
두 가지 주제가 있습니다.

24
00:00:56,520 --> 00:00:58,080
첫 번째는 CNN을 구축하는

25
00:00:58,080 --> 00:00:59,955
방법이며, 이는 CNN 아키텍처를

26
00:00:59,955 --> 00:01:02,630
정의하여 훈련할 수 있도록 설정하는 방법을 의미합니다.

27
00:01:02,630 --> 00:01:04,090
그리고 오늘의 두 번째 주제는

28
00:01:04,090 --> 00:01:05,780
CNN을 훈련하는 방법입니다.

29
00:01:05,780 --> 00:01:09,500
첫 번째 주제부터 시작하겠습니다. 합성곱

30
00:01:09,500 --> 00:01:12,710
신경망의 층에 대해 살펴보겠습니다.

31
00:01:12,710 --> 00:01:14,840
지난 강의에서 배운 바와

32
00:01:14,840 --> 00:01:17,300
같이, 이 모델의 핵심 층은

33
00:01:17,300 --> 00:01:18,830
합성곱 층입니다.

34
00:01:18,830 --> 00:01:23,020
이 층들이 작동하는 방식은 필터를 가지고 있다는 것입니다.

35
00:01:23,020 --> 00:01:26,110
각 합성곱 층마다 미리 정의된 필터의 수가

36
00:01:26,110 --> 00:01:28,390
있으며, 이 경우에는 여섯 개입니다.

37
00:01:28,390 --> 00:01:31,820
이 필터는 입력 데이터의 깊이에 맞춰져 있습니다.

38
00:01:31,820 --> 00:01:34,490
이 경우, 우리는 32x32 RGB 이미지가 있으므로

39
00:01:34,490 --> 00:01:35,810
깊이 채널이 세 개입니다.

40
00:01:35,810 --> 00:01:38,920
각 필터는 이미지를 가로질러 슬라이드하며 각

41
00:01:38,920 --> 00:01:40,600
지점에서 점수를 계산합니다.

42
00:01:40,600 --> 00:01:42,360
그 위치에서 필터의

43
00:01:42,360 --> 00:01:44,470
값과 이미지의

44
00:01:44,470 --> 00:01:45,893
값을 내적합니다.

45
00:01:45,893 --> 00:01:48,560
그래서 이 모든 값을 곱하고 합산한 다음

46
00:01:48,560 --> 00:01:49,970
편향 항을 추가합니다.

47
00:01:49,970 --> 00:01:54,280
이렇게 해서 오른쪽의 출력 활성화 맵에서

48
00:01:54,280 --> 00:01:56,210
각 값을 계산합니다.

49
00:01:56,210 --> 00:01:57,970
이미지를 가로질러 슬라이드하는

50
00:01:57,970 --> 00:01:59,090
이러한 창이 있습니다.

51
00:01:59,090 --> 00:02:01,010
각 위치에서 점수를 계산하고,

52
00:02:01,010 --> 00:02:04,360
그렇게 해서 이러한 활성화 맵을 얻습니다.

53
00:02:04,360 --> 00:02:06,170
각 필터마다 하나씩 있습니다.

54
00:02:06,170 --> 00:02:08,080
보통 마지막에는 ReLU

55
00:02:08,080 --> 00:02:11,420
또는 비선형 활성화 함수를 적용합니다.

56
00:02:11,420 --> 00:02:13,010
이것은 지난 강의에서 다룬 내용입니다.

57
00:02:13,010 --> 00:02:15,160
너무 많은 시간을 할애하지 않겠습니다.

58
00:02:15,160 --> 00:02:17,530
질문은, 이미지의 깊이는 RGB

59
00:02:17,530 --> 00:02:19,880
채널의 수와 같지만,

60
00:02:19,880 --> 00:02:23,000
여기서 출력의 깊이는 6이라는 것입니다.

61
00:02:23,000 --> 00:02:25,880
따라서 그 후에 두 번째 합성곱 층이

62
00:02:25,880 --> 00:02:27,580
있다면, 이 여섯

63
00:02:27,580 --> 00:02:31,400
개의 활성화 맵을 모두 가로지르는 필터가 필요하므로

64
00:02:31,400 --> 00:02:34,300
다음 층의 깊이는 6이 될 것입니다.

65
00:02:34,300 --> 00:02:36,500
좋습니다, 그리고 우리가 이야기한 두

66
00:02:36,500 --> 00:02:38,770
번째 층은 합성곱 층보다 훨씬 간단한

67
00:02:38,770 --> 00:02:40,310
풀링 층의 개념입니다.

68
00:02:40,310 --> 00:02:43,480
여기서 우리는 여전히 이미지를 가로질러

69
00:02:43,480 --> 00:02:47,330
슬라이드하는 필터, 2x2 필터와 보폭 2를 사용합니다.

70
00:02:47,330 --> 00:02:48,290
그래서 우리는 건너뛰고 있습니다.

71
00:02:48,290 --> 00:02:49,873
모든 위치를 다 다루고 있지는 않습니다.

72
00:02:49,873 --> 00:02:51,410
여기 최대 풀링이 있습니다.

73
00:02:51,410 --> 00:02:53,665
그래서 우리는 이 영역 각각의 최대값을 취하고,

74
00:02:53,665 --> 00:02:55,040
그게 여기서 얻는 값입니다.

75
00:02:55,040 --> 00:02:57,490
또는 평균 풀링을 할 수도 있습니다.

76
00:02:57,490 --> 00:02:59,600
둘 다 일반적으로 사용된다고 할 수 있습니다.

77
00:02:59,600 --> 00:03:01,083
아키텍처에 따라, 새로운 아키텍처를

78
00:03:01,083 --> 00:03:02,500
만들고 있다면, 두 가지를

79
00:03:02,500 --> 00:03:03,970
모두 시도해보고 어떤 것이

80
00:03:03,970 --> 00:03:05,690
더 잘 작동하는지 확인할 것입니다.

81
00:03:05,690 --> 00:03:07,870
기본 아이디어는 이미지의

82
00:03:07,870 --> 00:03:11,710
높이와 너비 차원에서 통합하는 것입니다.

83
00:03:11,710 --> 00:03:15,340
좋습니다, 그래서 우리는 이 시점에서 강의의 모든

84
00:03:15,340 --> 00:03:17,890
상단 행, 즉 컨볼루션 레이어,

85
00:03:17,890 --> 00:03:20,290
풀링 레이어 및 완전 연결 레이어를

86
00:03:20,290 --> 00:03:21,470
다루었습니다.

87
00:03:21,470 --> 00:03:22,887
이것들은 신경망 강의에서

88
00:03:22,887 --> 00:03:24,940
이야기한 첫 번째 레이어로,

89
00:03:24,940 --> 00:03:27,370
기본적으로 하나의 행렬 곱셈과 그 뒤에

90
00:03:27,370 --> 00:03:29,000
활성화 함수가 따릅니다.

91
00:03:29,000 --> 00:03:31,190
이 강의의 나머지 부분에서는 CNN에서

92
00:03:31,190 --> 00:03:32,890
볼 수 있는 나머지 레이어,

93
00:03:32,890 --> 00:03:36,220
적어도 일반적으로 사용되는 레이어에 대해 이야기할 것입니다.

94
00:03:36,220 --> 00:03:39,440
여기에는 정규화 레이어가 포함되며, 이에 대해 자세히

95
00:03:39,440 --> 00:03:42,760
설명하겠습니다. 그리고 드롭아웃도 포함되며, 이는 실제로

96
00:03:42,760 --> 00:03:44,560
모델 아키텍처 자체에서 사용되는

97
00:03:44,560 --> 00:03:45,860
정규화 기법입니다.

98
00:03:45,860 --> 00:03:48,358
마지막으로 활성화 함수에 대해 다시

99
00:03:48,358 --> 00:03:50,650
살펴보고, 역사적으로나 현대의 딥러닝

100
00:03:50,650 --> 00:03:55,030
시대에서 가장 일반적으로 사용되는 것들에 대해 말씀드리겠습니다.

101
00:03:55,030 --> 00:03:57,410
정규화 레이어부터 시작하겠습니다.

102
00:03:57,410 --> 00:03:59,410
여기서 기본 아이디어는 입력

103
00:03:59,410 --> 00:04:01,900
데이터에 대한 평균과 표준 편차와

104
00:04:01,900 --> 00:04:05,080
같은 통계를 계산한 다음 이를 사용하여 데이터를

105
00:04:05,080 --> 00:04:06,710
정규화하는 것입니다.

106
00:04:06,710 --> 00:04:10,390
그리고 우리는 기본적으로 모델이 그

107
00:04:10,390 --> 00:04:13,960
시점에서 학습할 최적의 분포가 무엇인지 배울

108
00:04:13,960 --> 00:04:15,020
것입니다.

109
00:04:15,020 --> 00:04:18,130
구체적으로 우리는 학습된 평균과 학습된

110
00:04:18,130 --> 00:04:19,930
표준 편차로 입력

111
00:04:19,930 --> 00:04:24,290
데이터를 스케일하고 이동시키는 매개변수를 배웁니다.

112
00:04:24,290 --> 00:04:27,640
모든 정규화 레이어가 작동하는 방식은 두

113
00:04:27,640 --> 00:04:28,700
단계입니다.

114
00:04:28,700 --> 00:04:31,120
첫 번째는 들어오는 데이터를 단위 가우시안으로

115
00:04:31,120 --> 00:04:34,790
정규화하는 것입니다. 즉, 평균 0, 표준 편차 1입니다.

116
00:04:34,790 --> 00:04:36,920
그런 다음 우리는 이를 스케일하고 이동시킵니다.

117
00:04:36,920 --> 00:04:40,828
즉, 표준 편차를 증가시키거나 감소시키기 위해 어떤 값으로

118
00:04:40,828 --> 00:04:42,370
곱하고, 평균을 변경하기

119
00:04:42,370 --> 00:04:43,880
위해 이동시킵니다.

120
00:04:43,880 --> 00:04:46,280
모든 정규화 레이어가 이 기술을

121
00:04:46,280 --> 00:04:47,920
수행하지만, 그들이 통계를

122
00:04:47,920 --> 00:04:49,520
계산하는 방식이 다릅니다.

123
00:04:49,520 --> 00:04:52,330
즉, 평균과 표준 편차를 어떻게 계산하고,

124
00:04:52,330 --> 00:04:53,840
이러한 계산된 통계를

125
00:04:53,840 --> 00:04:56,900
어떤 값에 적용하는지에 따라 다르지만, 모든 정규화

126
00:04:56,900 --> 00:04:59,690
레이어는 이 고수준 프로세스를 수행합니다.

127
00:04:59,690 --> 00:05:03,410
그래서 저는 오늘날 딥러닝에서 가장 일반적으로

128
00:05:03,410 --> 00:05:06,590
사용되는 정규화 레이어인 레이어 정규화에 대해

129
00:05:06,590 --> 00:05:07,920
이야기하겠습니다.

130
00:05:07,920 --> 00:05:11,120
특히 트랜스포머에서 매우 일반적으로 사용됩니다.

131
00:05:11,120 --> 00:05:13,970
그래서 들어오는 데이터 x가 있고, 이는

132
00:05:13,970 --> 00:05:16,500
배치 크기 n이라고 상상할 수 있습니다.

133
00:05:16,500 --> 00:05:19,110
따라서 n개의 샘플이 모델에 들어옵니다.

134
00:05:19,110 --> 00:05:22,130
각각은 차원 D의 벡터입니다.

135
00:05:22,130 --> 00:05:25,250
레이어 정규화가 하는 것은 각 샘플에

136
00:05:25,250 --> 00:05:29,180
대해 평균과 표준 편차를 별도로 계산하는

137
00:05:29,180 --> 00:05:29,940
것입니다.

138
00:05:29,940 --> 00:05:32,270
따라서 깊이 또는 차원 D를

139
00:05:32,270 --> 00:05:35,970
따라 평균이 무엇인지, 표준 편차가 무엇인지

140
00:05:35,970 --> 00:05:37,770
계산하고 있습니다.

141
00:05:37,770 --> 00:05:40,920
그런 다음 매개변수를 학습합니다.

142
00:05:40,920 --> 00:05:42,530
이들은 모델에서 경량

143
00:05:42,530 --> 00:05:45,950
하강법을 통해 학습된 학습 가능한 매개변수로,

144
00:05:45,950 --> 00:05:47,460
각 샘플에 적용됩니다.

145
00:05:47,460 --> 00:05:49,490
이렇게 통계를 계산한 후,

146
00:05:49,490 --> 00:05:51,900
각 샘플을 별도로

147
00:05:51,900 --> 00:05:54,490
처리하여 평균과 표준 편차를 계산한

148
00:05:54,490 --> 00:05:59,930
다음, 학습된 스케일과 이동 또는 매개변수를 여기에 적용합니다.

149
00:05:59,930 --> 00:06:00,430
다음, 학습된 스케일과 이동 또는 매개변수를 여기에 적용합니다.

150
00:06:00,430 --> 00:06:03,780
그래서 우리는 입력 데이터 내에서 평균을

151
00:06:03,780 --> 00:06:06,370
빼고 표준 편차로 나누어

152
00:06:06,370 --> 00:06:09,420
정규화한 다음, 곱셈과 이동으로

153
00:06:09,420 --> 00:06:10,870
스케일을 적용합니다.

154
00:06:10,870 --> 00:06:13,870
이것이 LayerNorm의 아이디어입니다.

155
00:06:13,870 --> 00:06:19,740
높은 수준에서 모든 이러한 다양한 정규화 레이어는 매우

156
00:06:19,740 --> 00:06:22,990
유사한 작업을 수행하지만,

157
00:06:22,990 --> 00:06:25,020
주요 차이점은 평균과

158
00:06:25,020 --> 00:06:28,600
표준 편차를 계산하는 방법입니다.

159
00:06:28,600 --> 00:06:30,840
이것은 Group Normalization이라는

160
00:06:30,840 --> 00:06:33,960
논문에서 가져온 정말 멋진 시각화입니다. 이는

161
00:06:33,960 --> 00:06:35,790
정규화하는 새로운 방법을 소개합니다.

162
00:06:35,790 --> 00:06:38,095
요즘에는 그리 일반적으로 사용되지 않지만,

163
00:06:38,095 --> 00:06:39,720
이는 이러한 다양한 정규화

164
00:06:39,720 --> 00:06:42,510
레이어가 어떻게 다른지에 대한 직관을 얻는 정말

165
00:06:42,510 --> 00:06:43,540
좋은 방법입니다.

166
00:06:43,540 --> 00:06:47,610
LayerNorm에 대해 설명할 때, 우리는 단순히

167
00:06:47,610 --> 00:06:50,170
정규화할 벡터가 있는 경우를

168
00:06:50,170 --> 00:06:52,990
설명했지만, 합성곱 신경망의

169
00:06:52,990 --> 00:06:56,520
경우에는 채널 차원, 깊이, 높이 및 너비

170
00:06:56,520 --> 00:06:58,770
또는 이미지의 공간 차원이

171
00:06:58,770 --> 00:06:59,650
있습니다.

172
00:06:59,650 --> 00:07:02,890
LayerNorm이 하는 일은 각 샘플에

173
00:07:02,890 --> 00:07:04,780
대해 여전히 별도로 처리하고

174
00:07:04,780 --> 00:07:07,410
모든 채널, 모든 높이 및 모든

175
00:07:07,410 --> 00:07:10,150
너비에 걸쳐 평균을 계산하는 것입니다.

176
00:07:10,150 --> 00:07:14,970
이 다이어그램을 다시 살펴보면, 기본적으로

177
00:07:14,970 --> 00:07:18,876
이러한 모든 값에 대해 하나의

178
00:07:18,876 --> 00:07:22,260
평균과 하나의 표준 편차를

179
00:07:22,260 --> 00:07:23,770
계산하게 됩니다.

180
00:07:23,770 --> 00:07:26,292
각 입력 데이터 포인트에 대해 모든 채널,

181
00:07:26,292 --> 00:07:28,500
모든 높이 및 너비 차원에 걸쳐

182
00:07:28,500 --> 00:07:31,250
하나의 평균과 하나의 표준 편차를 계산하고

183
00:07:31,250 --> 00:07:31,750
있습니다.

184
00:07:31,750 --> 00:07:33,900
이것이 LayerNorm이 하는 일입니다.

185
00:07:33,900 --> 00:07:36,630
하지만 이러한 통계를 다르게 계산할 수 있다고

186
00:07:36,630 --> 00:07:38,050
상상할 수 있습니다.

187
00:07:38,050 --> 00:07:41,470
BatchNorm에서는 각 채널을 취하고 각

188
00:07:41,470 --> 00:07:44,070
채널이 하나의 평균과 하나의 표준 편차로

189
00:07:44,070 --> 00:07:45,100
계산됩니다.

190
00:07:45,100 --> 00:07:46,930
그리고 그것을 해당 채널에만 적용하므로

191
00:07:46,930 --> 00:07:49,510
배치의 모든 데이터에 걸쳐 평균을 내고 있습니다.

192
00:07:49,510 --> 00:07:52,310
InstanceNorm은 GroupNorm보다 더 세분화되어 있습니다.

193
00:07:52,310 --> 00:07:54,400
모든 이러한 레이어는 기본적으로

194
00:07:54,400 --> 00:07:57,100
데이터를 정규화하고 학습 가능한

195
00:07:57,100 --> 00:07:58,868
스케일 및 이동 매개변수를

196
00:07:58,868 --> 00:08:01,160
가지려는 같은 작업을

197
00:08:01,160 --> 00:08:03,285
시도하고 있지만, 통계를 계산하는

198
00:08:03,285 --> 00:08:07,390
방식이 입력 데이터의 서로 다른 하위 집합을 사용하기

199
00:08:07,390 --> 00:08:08,770
때문에 다릅니다.

200
00:08:08,770 --> 00:08:10,490
네, 그래서 질문은 LayerNorm에

201
00:08:10,490 --> 00:08:12,740
대해 각 이미지 또는 입력 데이터에 대해 하나의 평균과

202
00:08:12,740 --> 00:08:14,390
하나의 표준 편차를 계산하고 있습니까?

203
00:08:14,390 --> 00:08:17,710
네, 모두 별도로 계산되지만, 이 예제에서

204
00:08:17,710 --> 00:08:19,770
BatchNorm은

205
00:08:19,770 --> 00:08:21,020
그렇지 않습니다.

206
00:08:21,020 --> 00:08:22,060
네.

207
00:08:22,060 --> 00:08:25,590
BatchNorm은 실제로 경량 배치 내에서 기울기 하강법을

208
00:08:25,590 --> 00:08:26,840
수행할 때입니다.

209
00:08:26,840 --> 00:08:28,882
당신은 보고 있는 작은 데이터 배치가 있습니다.

210
00:08:28,882 --> 00:08:30,580
모델에 입력합니다.

211
00:08:30,580 --> 00:08:34,330
배치의 모든 데이터를 기반으로 각

212
00:08:34,330 --> 00:08:37,120
채널의 평균과 표준 편차를

213
00:08:37,120 --> 00:08:38,340
계산합니다.

214
00:08:41,075 --> 00:08:43,450
네, 이 다이어그램을 이해할 수

215
00:08:43,450 --> 00:08:46,495
있다면, 모든 다양한 정규화 레이어가

216
00:08:46,495 --> 00:08:49,120
무엇을 하는지 이해할 수 있다고

217
00:08:49,120 --> 00:08:51,078
생각합니다. 그래서 강의

218
00:08:51,078 --> 00:08:54,070
후에도 여전히 완전히 이해하지 못한다면,

219
00:08:54,070 --> 00:08:56,080
통계를 계산하고 평균과

220
00:08:56,080 --> 00:08:58,810
표준 편차를 적용하는 값이 무엇인지

221
00:08:58,810 --> 00:09:00,973
확인하는 것이 좋습니다.

222
00:09:00,973 --> 00:09:02,890
네, 마지막 질문 하나 하고 넘어가겠습니다.

223
00:09:02,890 --> 00:09:05,860
채널은 레이어와 동일합니까?

224
00:09:05,860 --> 00:09:10,430
여기서 채널은 깊이를 의미합니다.

225
00:09:10,430 --> 00:09:21,790
각 공간 위치에서 가지고 있는 값의 수입니다.

226
00:09:21,790 --> 00:09:23,260
좋아요, 멋지네요.

227
00:09:23,260 --> 00:09:27,260
정규화 레이어에 대해 이야기했습니다.

228
00:09:27,260 --> 00:09:29,500
핵심 아이디어는 통계를

229
00:09:29,500 --> 00:09:33,310
계산하고 이를 입력 데이터에 적용한 다음, 학습한

230
00:09:33,310 --> 00:09:37,160
스케일 및 이동 매개변수를 적용하는 것입니다.

231
00:09:37,160 --> 00:09:41,150
다음으로 이야기할 레이어 유형은 드롭아웃이라고 합니다.

232
00:09:41,150 --> 00:09:44,420
이것은 CNN의 정규화 레이어입니다.

233
00:09:44,420 --> 00:09:46,280
그리고 이것은 사람들이

234
00:09:46,280 --> 00:09:48,360
수년 동안 만든 다양한

235
00:09:48,360 --> 00:09:50,443
CNN 아키텍처를 살펴보려면

236
00:09:50,443 --> 00:09:53,060
배워야 할 마지막 레이어입니다.

237
00:09:53,060 --> 00:09:56,030
드롭아웃의 기본 아이디어는 훈련

238
00:09:56,030 --> 00:09:59,630
과정에서 무작위성을 추가하고, 이를 테스트

239
00:09:59,630 --> 00:10:01,572
시에는 제거하는 것입니다.

240
00:10:01,572 --> 00:10:03,530
목표는 모델이 훈련 데이터를 학습하기

241
00:10:03,530 --> 00:10:06,570
어렵게 만드는 것이지만, 그러면 더 잘 일반화됩니다.

242
00:10:06,570 --> 00:10:09,350
이것은 정규화의 한 형태입니다.

243
00:10:09,350 --> 00:10:12,950
구체적으로 우리가 하는 방법은 각 레이어의

244
00:10:12,950 --> 00:10:15,500
순전파에서 실제로 일부 출력이나

245
00:10:15,500 --> 00:10:19,440
활성화를 무작위로 0으로 만드는 것입니다.

246
00:10:19,440 --> 00:10:24,000
이 드롭아웃 레이어의 주요 매개변수는 고정된

247
00:10:24,000 --> 00:10:25,800
하이퍼파라미터인

248
00:10:25,800 --> 00:10:28,590
값이 드롭아웃될 확률입니다.

249
00:10:28,590 --> 00:10:32,390
0.5가 가장 일반적이고 0.25도

250
00:10:32,390 --> 00:10:34,040
자주 사용됩니다.

251
00:10:34,040 --> 00:10:37,310
여기서 고정된 비율의 값을 드롭아웃하고

252
00:10:37,310 --> 00:10:37,980
있습니다.

253
00:10:37,980 --> 00:10:41,790
따라서 다음 레이어로 넘어갈 때 이 값들은 0이 됩니다.

254
00:10:41,790 --> 00:10:45,950
그래서 여기서 값을 계산할 필요가 없습니다.

255
00:10:45,950 --> 00:10:50,082
기본적으로 여기서 모든 출력은 현재 0이므로

256
00:10:50,082 --> 00:10:52,540
마스킹을 사용하여 값을 계산할

257
00:10:52,540 --> 00:10:54,700
필요가 없습니다. 0에 어떤

258
00:10:54,700 --> 00:10:57,350
값을 곱해도 0이기 때문입니다.

259
00:10:57,350 --> 00:11:01,820
그래서 일반적으로 왜 이것이 작동하는지 물어볼 수 있습니다.

260
00:11:01,820 --> 00:11:07,660
이것은 이론적 관점에서 잘 연구된 것보다 경험적인

261
00:11:07,660 --> 00:11:11,900
것이라고 말할 수 있지만, 드롭아웃이

262
00:11:11,900 --> 00:11:13,810
하는 일을

263
00:11:13,810 --> 00:11:16,360
이해하는 몇 가지 방법이

264
00:11:16,360 --> 00:11:17,780
있습니다.

265
00:11:17,780 --> 00:11:21,430
기본적으로 네트워크가 중복

266
00:11:21,430 --> 00:11:24,530
표현을 갖도록 강제합니다.

267
00:11:24,530 --> 00:11:27,520
모델의 출력 바로 앞 레이어에서 학습하고

268
00:11:27,520 --> 00:11:31,340
있는 특징 목록이 있다고 가정해 보겠습니다.

269
00:11:31,340 --> 00:11:34,960
CNN이 이러한 각 특징을 추출하여 이미지에

270
00:11:34,960 --> 00:11:37,510
귀가 있는지, 꼬리가 있는지,

271
00:11:37,510 --> 00:11:40,070
털이 있는지, 발톱이 있는지를 감지할

272
00:11:40,070 --> 00:11:43,310
수 있습니다. 모델이 고양이 점수를

273
00:11:43,310 --> 00:11:45,270
출력하도록 하고 싶습니다.

274
00:11:45,270 --> 00:11:47,690
이것의 유용한 점 중 하나는 훈련 중에

275
00:11:47,690 --> 00:11:49,910
이러한 값 중 일부가 무작위로 드롭아웃될

276
00:11:49,910 --> 00:11:51,330
수 있기 때문입니다.

277
00:11:51,330 --> 00:11:53,540
모델은 특정 클래스에 특정

278
00:11:53,540 --> 00:11:56,430
특징이 존재하는 것에 과도하게

279
00:11:56,430 --> 00:11:59,450
의존할 수 없으며, 특징과 출력 클래스

280
00:11:59,450 --> 00:12:02,690
간의 더 넓은 대응 관계를 학습해야

281
00:12:02,690 --> 00:12:03,270
합니다.

282
00:12:03,270 --> 00:12:07,560
모델은 귀가 있고 털이

283
00:12:07,560 --> 00:12:11,450
있는 경우 항상

284
00:12:11,450 --> 00:12:14,190
고양이인 것처럼

285
00:12:14,190 --> 00:12:17,670
특정 특징에만 집중할

286
00:12:17,670 --> 00:12:20,468
수 없습니다.

287
00:12:20,468 --> 00:12:22,010
따라서 데이터

288
00:12:22,010 --> 00:12:25,490
세트에서 특정 특징과 출력 클래스 간의 상관관계가

289
00:12:25,490 --> 00:12:28,040
매우 강하더라도 새로운 특징에

290
00:12:28,040 --> 00:12:30,680
더 잘 일반화하는 데 도움이

291
00:12:30,680 --> 00:12:31,200
됩니다.

292
00:12:31,200 --> 00:12:33,920
드롭아웃을 통해 모델은 훈련 단계에서

293
00:12:33,920 --> 00:12:35,383
이러한 것들에 의존할

294
00:12:35,383 --> 00:12:36,800
수 없게 됩니다.

295
00:12:36,800 --> 00:12:40,410
왜냐하면 항상 특징 쌍을 함께 보지 않기 때문입니다.

296
00:12:40,410 --> 00:12:43,000
이것은 고양이에 대한 예입니다.

297
00:12:43,000 --> 00:12:46,260
질문은 나무와 같은 것이 있다면 어떤 특징을

298
00:12:46,260 --> 00:12:48,880
드롭아웃할지 어떻게 결정할 것인가입니다.

299
00:12:48,880 --> 00:12:52,600
드롭아웃 부분은 완전히 무작위이므로 이에 대한

300
00:12:52,600 --> 00:12:54,580
선택을 하지 않습니다.

301
00:12:54,580 --> 00:12:59,520
이 경우 주어진 단계에서 50%의 특징이 드롭아웃되고

302
00:12:59,520 --> 00:13:01,800
0으로 설정됩니다.

303
00:13:01,800 --> 00:13:04,410
그래서 선택을 할 필요가

304
00:13:04,410 --> 00:13:08,785
없다는 점이 좋지만, 완전히 무작위입니다.

305
00:13:08,785 --> 00:13:10,410
모델은 꼬리와 발톱과 같은

306
00:13:10,410 --> 00:13:13,460
특징의 하위 집합만 보고 있다면 어떻게 알 수 있을까요?

307
00:13:13,460 --> 00:13:15,960
핵심은 실제로 특징의 하위 집합만 보기 때문에 훈련

308
00:13:15,960 --> 00:13:18,127
데이터에서 더 나쁜 성능을 보인다는 것입니다.

309
00:13:18,127 --> 00:13:20,190
모든 정보를 갖지 않음으로써

310
00:13:20,190 --> 00:13:22,680
모델이 나빠지지만, 테스트 시에는 더

311
00:13:22,680 --> 00:13:23,830
잘 수행됩니다.

312
00:13:23,830 --> 00:13:25,205
훈련 시에는 최악이고

313
00:13:25,205 --> 00:13:27,510
테스트 시에는 더 잘 수행됩니다.

314
00:13:27,510 --> 00:13:30,670
테스트 시에는 드롭아웃이 없기 때문입니다.

315
00:13:30,670 --> 00:13:32,830
여기서 마지막 구성 요소는 아마도 질문을

316
00:13:32,830 --> 00:13:35,080
받기 전에 먼저 설명했어야 할 아이디어로,

317
00:13:35,080 --> 00:13:38,370
테스트 시에는 더 이상 어떤 값도 드롭아웃하지

318
00:13:38,370 --> 00:13:39,230
않는다는 것입니다.

319
00:13:39,230 --> 00:13:41,210
이것은 훈련 단계에서만 추가하는

320
00:13:41,210 --> 00:13:41,710
무작위성입니다.

321
00:13:41,710 --> 00:13:45,100
테스트 시에는 출력 활성화를

322
00:13:45,100 --> 00:13:49,000
마스킹하지 않으며 드롭아웃 개념을 완전히

323
00:13:49,000 --> 00:13:50,480
제거합니다.

324
00:13:50,480 --> 00:13:54,340
우리가 훈련 시간 동안 50%의 활성화를

325
00:13:54,340 --> 00:13:57,388
드롭아웃했다면, 테스트 시간에는

326
00:13:57,388 --> 00:13:58,930
각 레이어에

327
00:13:58,930 --> 00:14:04,520
입력되는 값이 50% 더 많아진다는 점을 주목해야 합니다.

328
00:14:04,520 --> 00:14:06,920
그래서 스케일을 조정하지 않으면 문제가 발생할 수 있습니다.

329
00:14:06,920 --> 00:14:09,370
그래서 해야 할 일은 드롭아웃

330
00:14:09,370 --> 00:14:14,440
확률로 곱하여 각 레이어로 들어오는 값의 크기를 훈련 및

331
00:14:14,440 --> 00:14:17,330
테스트 시간 동안 유지하는 것입니다.

332
00:14:17,330 --> 00:14:19,333
그렇지 않으면 50%의 값을

333
00:14:19,333 --> 00:14:21,500
드롭하고 테스트 시간에 모든

334
00:14:21,500 --> 00:14:24,580
값을 포함하면 이전보다 훨씬 큰 입력 크기를

335
00:14:24,580 --> 00:14:27,970
보게 되어 이상한 동작이 발생할 것입니다.

336
00:14:27,970 --> 00:14:30,910
네, 그럼 역전파는 어떻게 되나요?

337
00:14:30,910 --> 00:14:34,700
역전파에서는 이러한 0으로 설정된

338
00:14:34,700 --> 00:14:40,620
값이 있을 때, 더 이상 방향 그래프의 경로를 탐색할 필요가

339
00:14:40,620 --> 00:14:41,920
없습니다.

340
00:14:41,920 --> 00:14:43,003
ReLU와 매우 유사합니다.

341
00:14:43,003 --> 00:14:46,110
그 지점에서 0으로 설정된 값이 있으면

342
00:14:46,110 --> 00:14:47,380
기울기가 0이 됩니다.

343
00:14:47,380 --> 00:14:54,120
따라서 계산 그래프의 더 뒤쪽에서는 그 지점에서 기울기가

344
00:14:54,120 --> 00:14:56,790
계산되지 않습니다.

345
00:14:56,790 --> 00:15:01,410
특정 값이나 활성화를 드롭하면 해당 활성화와

346
00:15:01,410 --> 00:15:03,660
관련된 가중치는

347
00:15:03,660 --> 00:15:06,780
드롭할 경우 경량 하강 동안

348
00:15:06,780 --> 00:15:08,820
업데이트되지 않습니다.

349
00:15:08,820 --> 00:15:10,560
그래서 질문은-- 아마도

350
00:15:10,560 --> 00:15:11,590
다시 표현할게요.

351
00:15:11,590 --> 00:15:13,030
테스트 시간에 우리는 무엇을 하고 있나요?

352
00:15:13,030 --> 00:15:17,380
테스트 시간에는 모든 출력 활성화를 사용합니다.

353
00:15:17,380 --> 00:15:18,880
더 이상 드롭하지 않지만

354
00:15:18,880 --> 00:15:21,430
드롭아웃 확률로 스케일을 조정해야 합니다.

355
00:15:21,430 --> 00:15:23,340
그래서 이제 모든 활성화를

356
00:15:23,340 --> 00:15:26,200
사용하므로 각 출력 활성화에 이 p 값을 곱합니다.

357
00:15:26,200 --> 00:15:29,640
그렇지 않으면 각 노드가 훈련 중보다

358
00:15:29,640 --> 00:15:32,880
테스트 시간에 훨씬 더 많은 입력을

359
00:15:32,880 --> 00:15:35,290
받는다고 상상할 수 있습니다.

360
00:15:35,290 --> 00:15:37,780
그래서 입력의 동일한 크기를

361
00:15:37,780 --> 00:15:41,360
유지하려면 이 p 값으로 곱해야 합니다.

362
00:15:41,360 --> 00:15:43,505
그리고 분산은 이러한 다양한 속성에서

363
00:15:43,505 --> 00:15:44,630
동일하게 유지됩니다.

364
00:15:44,630 --> 00:15:47,500
이렇게 하면 매우 잘 작동합니다.

365
00:15:47,500 --> 00:15:50,300
그래서 질문은, 대신 이미지에 노이즈를 추가할 수 있나요?

366
00:15:50,300 --> 00:15:51,925
답은 예이며, 향후 슬라이드에서

367
00:15:51,925 --> 00:15:53,420
그 방법을 설명하겠습니다.

368
00:15:53,420 --> 00:15:57,400
네, 이미지에 노이즈를 추가하는 것은 훌륭한 아이디어입니다.

369
00:15:57,400 --> 00:15:59,438
자, 여기 특정 코드가 있습니다.

370
00:15:59,438 --> 00:16:01,730
이미 언급했기 때문에

371
00:16:01,730 --> 00:16:06,790
이 부분은 넘어가겠지만, 여기서 활성화의 p

372
00:16:06,790 --> 00:16:11,220
비율을 드롭하고 테스트 시간에 곱합니다.

373
00:16:11,220 --> 00:16:13,558
다음 주제는 활성화 함수입니다.

374
00:16:13,558 --> 00:16:15,850
여러분은 이제 CNN의 모든 주요 레이어를

375
00:16:15,850 --> 00:16:17,057
기본적으로 배웠습니다.

376
00:16:17,057 --> 00:16:19,390
이제 이러한 활성화 함수에 대해 이야기할

377
00:16:19,390 --> 00:16:20,120
것입니다.

378
00:16:20,120 --> 00:16:23,020
기억하신다면, 이러한 활성화 함수의 전체

379
00:16:23,020 --> 00:16:25,910
목적은 모델에 비선형성을 도입하는 것입니다.

380
00:16:25,910 --> 00:16:29,452
현재 이러한 컨볼루션 연산자와 이미지

381
00:16:29,452 --> 00:16:30,910
위를 슬라이딩하는

382
00:16:30,910 --> 00:16:35,893
커널, 그리고 활성화가 없는 완전 연결 레이어는 모두

383
00:16:35,893 --> 00:16:37,310
곱셈과 덧셈이기

384
00:16:37,310 --> 00:16:39,623
때문에 선형 연산입니다.

385
00:16:39,623 --> 00:16:41,540
활성화 함수의 전체 목적은

386
00:16:41,540 --> 00:16:43,230
비선형성을 추가하는 것입니다.

387
00:16:43,230 --> 00:16:49,100
역사적으로, 시그모이드 함수는 매우 일반적으로 사용된 활성화 함수였지만,

388
00:16:49,100 --> 00:16:52,410
오늘날 더 이상 사용되지 않는 이유가

389
00:16:52,410 --> 00:16:55,110
되는 주요 문제가 있습니다.

390
00:16:55,110 --> 00:16:57,390
그래서 시그모이드를 그래프로 나타내면 이렇게 보입니다.

391
00:16:57,390 --> 00:17:01,320
슬라이드의 오른쪽 상단에서 방정식을 볼 수 있습니다.

392
00:17:01,320 --> 00:17:06,470
주요 문제는 경험적으로 발생한 것으로, 많은 시그모이드

393
00:17:06,470 --> 00:17:08,819
레이어를 거치면 역전파를 계산할

394
00:17:08,819 --> 00:17:10,670
때 기울기가 점점

395
00:17:10,670 --> 00:17:12,105
작아진다는 것입니다.

396
00:17:12,105 --> 00:17:13,730
그래서 끝에서 시작하면, 기울기가

397
00:17:13,730 --> 00:17:15,329
상당히 큰 크기를 가집니다.

398
00:17:15,329 --> 00:17:18,030
그리고 여러 층의 역전파를 거치면서

399
00:17:18,030 --> 00:17:20,488
모델의 초기 층으로 가면, 이 과정을

400
00:17:20,488 --> 00:17:22,280
진행할수록 기울기는

401
00:17:22,280 --> 00:17:23,490
점점 작아집니다.

402
00:17:23,490 --> 00:17:27,109
그래서 이 질문을 클래스에 열어보겠습니다.

403
00:17:27,109 --> 00:17:30,480
이것은 시그모이드에서 발생하는 현상이 아닙니다.

404
00:17:30,480 --> 00:17:33,970
그래서 우리의 그래프에서 시그모이드가 정말

405
00:17:33,970 --> 00:17:36,870
작은 기울기를 가지는 영역은 어디인가요?

406
00:17:36,870 --> 00:17:39,573
네, 매우 부정적인 값과 매우 긍정적인 값이 맞습니다.

407
00:17:39,573 --> 00:17:40,990
그리고 이것은 실제로 큰 문제입니다.

408
00:17:40,990 --> 00:17:42,365
그래프에서 기울기가 매우 평평하다는

409
00:17:42,365 --> 00:17:44,070
것을 시각적으로 볼 수 있습니다.

410
00:17:44,070 --> 00:17:45,790
여기서 미분을 취하고 있습니다.

411
00:17:45,790 --> 00:17:46,920
매우 작습니다.

412
00:17:46,920 --> 00:17:49,050
기본적으로 음의 무한대에서 양의

413
00:17:49,050 --> 00:17:52,160
무한대까지 거의 모든 입력 공간에서 기울기가

414
00:17:52,160 --> 00:17:53,410
매우 작습니다.

415
00:17:53,410 --> 00:17:55,285
그리고 중간의 좁은

416
00:17:55,285 --> 00:17:58,540
범위에서만 비제로인 값을 가지므로 극단의

417
00:17:58,540 --> 00:18:00,750
양쪽 끝에서 0에 매우

418
00:18:00,750 --> 00:18:02,230
빠르게 접근합니다.

419
00:18:02,230 --> 00:18:05,400
따라서 시그모이드에 들어오는 값이

420
00:18:05,400 --> 00:18:07,140
매우 크거나 매우 작으면

421
00:18:07,140 --> 00:18:11,190
기울기가 매우 작아진다는 것을 의미합니다.

422
00:18:11,190 --> 00:18:13,290
이것이 ReLU가 매우 인기를

423
00:18:13,290 --> 00:18:15,930
끌게 된 주요 이유 중 하나입니다.

424
00:18:15,930 --> 00:18:19,410
이제 긍정적인 영역에서는 이러한 행동이 없습니다.

425
00:18:19,410 --> 00:18:24,120
여기서는 미분이 1이지만, 실제로는

426
00:18:24,120 --> 00:18:27,240
기울기가 0인 왼쪽의

427
00:18:27,240 --> 00:18:31,910
평평한 부분이 여전히 있습니다.

428
00:18:31,910 --> 00:18:36,360
그래서 이제 기본적으로 입력 도메인의 절반을 가지고 있습니다.

429
00:18:36,360 --> 00:18:38,570
여기서는 기울기가 1이고 나머지 절반은

430
00:18:38,570 --> 00:18:41,060
0으로, 거의 모든 것이 0이거나 0에

431
00:18:41,060 --> 00:18:43,940
매우 가까운 것보다 나은 것입니다. 중간의 작은

432
00:18:43,940 --> 00:18:44,880
영역을 제외하고요.

433
00:18:44,880 --> 00:18:47,100
그래서 실제로는 이것들이 더 잘 작동합니다.

434
00:18:47,100 --> 00:18:48,830
또한, 시그모이드 함수보다

435
00:18:48,830 --> 00:18:51,710
0과 입력 값 사이의 최대 연산을 계산하는 것이

436
00:18:51,710 --> 00:18:52,830
훨씬 저렴합니다.

437
00:18:52,830 --> 00:18:56,360
그래서 이 두 가지 이유로 ReLU가 매우 인기를 끌었습니다.

438
00:18:56,360 --> 00:18:59,810
하지만 여전히 어떤 부정적인

439
00:18:59,810 --> 00:19:03,650
입력에 대해서는 0

440
00:19:03,650 --> 00:19:07,670
기울기를 얻는 문제를 가지고 있으며,

441
00:19:07,670 --> 00:19:12,710
최근에는 근처 0에 비평평한

442
00:19:12,710 --> 00:19:17,400
활성화 함수가 인기를 끌고 있습니다.

443
00:19:17,400 --> 00:19:20,990
그래서 이것이 GELU입니다. 그리고 SELU도 있으며, 슬라이드에서

444
00:19:20,990 --> 00:19:23,490
보여드리겠지만 공식은 다루지 않겠습니다.

445
00:19:23,490 --> 00:19:24,860
그들은 매우 유사하게 보입니다.

446
00:19:24,860 --> 00:19:31,110
기본 아이디어는 ReLU에서 0에서 1로의 비매끄러운

447
00:19:31,110 --> 00:19:35,680
점프를 부드럽게 만드는 것입니다.

448
00:19:35,680 --> 00:19:41,680
그래서 이것은 ReLU와 함께 매우 날카롭고 비매끄러운 함수입니다. 하지만
GELU의

449
00:19:41,680 --> 00:19:44,070
좋은 점은 여기서 실제로 비제로

450
00:19:44,070 --> 00:19:46,330
기울기를 가진다는 것입니다.

451
00:19:46,330 --> 00:19:49,960
그리고 x가 무한대 또는 음의 무한대에 접근할

452
00:19:49,960 --> 00:19:52,840
때 ReLU로 수렴하지만, 중간에서

453
00:19:52,840 --> 00:19:55,510
더 부드러운 동작을 얻습니다.

454
00:19:55,510 --> 00:19:57,330
특히 GELU가

455
00:19:57,330 --> 00:20:01,360
계산하는 것은 이 가우시안 오류 선형 단위입니다.

456
00:20:01,360 --> 00:20:05,250
그래서 이것은 가우시안 정규의 누적

457
00:20:05,250 --> 00:20:06,940
분포 함수입니다.

458
00:20:06,940 --> 00:20:09,610
가우시안의 곡선 아래 면적을 상상해보면, 그것이

459
00:20:09,610 --> 00:20:11,740
바로 어떤 점 x에서의 phi(x)입니다.

460
00:20:11,740 --> 00:20:14,370
그래서 여기서 정말 부정적인 값이 있으면

461
00:20:14,370 --> 00:20:16,720
0에 가까운 값을 가지게 되며,

462
00:20:16,720 --> 00:20:19,570
그래서 여기서 ReLU 0으로 수렴합니다.

463
00:20:19,570 --> 00:20:22,230
그리고 매우 높은 긍정적인 값에서는

464
00:20:22,230 --> 00:20:24,490
곡선 아래 면적이 1에 매우 가까워지므로

465
00:20:24,490 --> 00:20:26,530
여기서 x로 수렴합니다.

466
00:20:26,530 --> 00:20:28,162
그래서 이것이 GELU입니다.

467
00:20:28,162 --> 00:20:30,370
이것은 이러한 좋은 속성을 가지고 있으며

468
00:20:30,370 --> 00:20:31,870
극단에서 ReLU로 수렴합니다.

469
00:20:31,870 --> 00:20:35,740
그리고 이것은 오늘날 트랜스포머에서 사용되는

470
00:20:35,740 --> 00:20:38,380
주요 활성화 함수입니다.

471
00:20:38,380 --> 00:20:40,900
모두 살펴보면, 많은 것들이 비슷하게

472
00:20:40,900 --> 00:20:41,597
보입니다.

473
00:20:41,597 --> 00:20:43,430
기본 아이디어는 상대적으로 평평한 것입니다.

474
00:20:43,430 --> 00:20:49,870
그리고 한계에서 f(x) = x에 접근하여

475
00:20:49,870 --> 00:20:53,020
선형 선이 됩니다.

476
00:20:53,020 --> 00:20:56,410
그래서 SELU는 실제로 x 곱하기

477
00:20:56,410 --> 00:20:59,170
시그모이드(x)로, 매우

478
00:20:59,170 --> 00:21:02,450
음수 값에서는 0에 가깝고 매우 양수

479
00:21:02,450 --> 00:21:05,150
값에서는 1에 가깝습니다.

480
00:21:05,150 --> 00:21:08,500
그래서 실제로 여기서 phi인 단위 가우시안의

481
00:21:08,500 --> 00:21:11,128
누적 분포 함수와 유사합니다.

482
00:21:11,128 --> 00:21:12,670
그래서 형태가 실제로 매우

483
00:21:12,670 --> 00:21:13,795
비슷하게 보이는 이유입니다.

484
00:21:17,236 --> 00:21:20,207
그래서 CNN에서 이러한 활성화가 어디에 사용되는지

485
00:21:20,207 --> 00:21:21,790
물어볼 수 있으며, 일반적인 대답은

486
00:21:21,790 --> 00:21:24,080
선형 연산자 뒤에 배치된다는 것입니다.

487
00:21:24,080 --> 00:21:27,695
피드포워드 또는 선형 레이어 또는 완전 연결

488
00:21:27,695 --> 00:21:29,070
레이어가 있을 때마다,

489
00:21:29,070 --> 00:21:31,210
이는 모두 같은 레이어에 대한

490
00:21:31,210 --> 00:21:34,740
용어로, 행렬 곱셈 뒤에 활성화 함수가 옵니다.

491
00:21:34,740 --> 00:21:37,380
또는 컨볼루션 레이어가

492
00:21:37,380 --> 00:21:39,305
있는 경우,

493
00:21:39,305 --> 00:21:40,680
거의 항상

494
00:21:40,680 --> 00:21:45,300
이 후에 활성화 함수를 배치합니다.

495
00:21:45,300 --> 00:21:47,850
좋습니다, 이제 CNN의 모든 구성

496
00:21:47,850 --> 00:21:49,668
요소에 대해 배웠습니다.

497
00:21:49,668 --> 00:21:51,210
이제 그것들을 어떻게

498
00:21:51,210 --> 00:21:53,280
결합하고 사람들이 최첨단

499
00:21:53,280 --> 00:21:56,010
컨볼루션 신경망 아키텍처를 어떻게

500
00:21:56,010 --> 00:21:58,650
만들었는지 몇 가지 예를 살펴보겠습니다.

501
00:21:58,650 --> 00:22:00,930
이 슬라이드는 두 가지 다른 값을 플로팅하기

502
00:22:00,930 --> 00:22:02,530
때문에 정말 멋지다고 생각합니다.

503
00:22:02,530 --> 00:22:04,570
한편으로는 오류율이 있으며,

504
00:22:04,570 --> 00:22:06,105
이는 파란색 막대입니다.

505
00:22:06,105 --> 00:22:08,730
그리고 이것은 시간에 따른 것이므로, 이는 사람들이 ImageNet에서

506
00:22:08,730 --> 00:22:09,940
훈련한 다양한 모델입니다.

507
00:22:09,940 --> 00:22:12,060
그리고 모델의 레이어

508
00:22:12,060 --> 00:22:16,330
수를 나타내는 주황색 삼각형이 있습니다.

509
00:22:16,330 --> 00:22:17,940
오류가 크게

510
00:22:17,940 --> 00:22:21,420
감소하고 인간 성능을 처음으로 초과하는

511
00:22:21,420 --> 00:22:24,190
시점에서 레이어 수가

512
00:22:24,190 --> 00:22:27,710
크게 증가하는 것을 볼 수 있습니다.

513
00:22:27,710 --> 00:22:29,753
그래서 오늘 수업에서 그들이

514
00:22:29,753 --> 00:22:31,420
어떻게 이를 달성했는지,

515
00:22:31,420 --> 00:22:35,920
그리고 그들이 어떻게 했는지에 대한 설계 과제와 목표를 살펴보겠습니다.

516
00:22:35,920 --> 00:22:38,590
역사적으로 AlexNet은 ImageNet에서 정말

517
00:22:38,590 --> 00:22:41,180
잘 작동한 첫 번째 CNN 기반 논문이었습니다.

518
00:22:41,180 --> 00:22:43,593
그들은 GPU를 사용하여 이를 훈련할 수 있었습니다.

519
00:22:43,593 --> 00:22:45,260
우리는 강의에서 이것에

520
00:22:45,260 --> 00:22:48,670
대해 이야기했으므로 역사적 관점에서

521
00:22:48,670 --> 00:22:51,280
AlexNet에 대한 세부 사항을 너무 많이

522
00:22:51,280 --> 00:22:55,600
다루지 않겠지만, 2010년대에 정말 표준적이고 일반적으로

523
00:22:55,600 --> 00:22:59,300
사용된 아키텍처인 VGG와 비교하고 싶습니다.

524
00:22:59,300 --> 00:23:05,350
그래서 여기서 두 CNN 아키텍처를 나란히 플로팅할 수 있다고

525
00:23:05,350 --> 00:23:06,380
생각합니다.

526
00:23:06,380 --> 00:23:09,910
일반적으로 AI에서는 각 블록이 서로 다른 레이어

527
00:23:09,910 --> 00:23:13,090
또는 함께 쌓인 레이어 그룹을 나타내는 블록

528
00:23:13,090 --> 00:23:15,910
다이어그램을 사용하여 모델 아키텍처를

529
00:23:15,910 --> 00:23:17,480
플로팅하는 것을 좋아합니다.

530
00:23:17,480 --> 00:23:19,540
그리고 이것은 초기 glance에서

531
00:23:19,540 --> 00:23:22,000
일반적인 차이점에 대한 직관을

532
00:23:22,000 --> 00:23:23,700
얻는 데도 도움이 됩니다.

533
00:23:23,700 --> 00:23:26,550
여기서 일반적인 주황색 블록은 3x3

534
00:23:26,550 --> 00:23:28,670
컨볼루션 레이어입니다.

535
00:23:28,670 --> 00:23:30,170
이들은 3x3 크기의

536
00:23:30,170 --> 00:23:33,980
필터가 슬라이딩하는 컨볼루션 레이어입니다.

537
00:23:33,980 --> 00:23:36,620
스트라이드는 1이므로 이미지를

538
00:23:36,620 --> 00:23:39,570
방문하며 아무 것도 건너뛰지 않습니다.

539
00:23:39,570 --> 00:23:42,020
그리고 컨볼루션 레이어를

540
00:23:42,020 --> 00:23:47,400
수행할 때 축소되지 않도록 외부에 1의 패딩을 추가합니다.

541
00:23:47,400 --> 00:23:52,460
그리고 여기서도 이러한 맥스 풀링 레이어를 추가합니다.

542
00:23:52,460 --> 00:23:52,980
그리고 여기서도 이러한 맥스 풀링 레이어를 추가합니다.

543
00:23:52,980 --> 00:23:57,320
모든 풀링 레이어 이후에는 4,096

544
00:23:57,320 --> 00:24:01,130
차원의 완전 연결 레이어 두 세트를

545
00:24:01,130 --> 00:24:05,940
시작하고 그 뒤에 1,000 차원이 옵니다.

546
00:24:05,940 --> 00:24:08,810
마지막에 1,000이 있는 이유는 ImageNet이 1,000개의

547
00:24:08,810 --> 00:24:10,890
서로 다른 이미지 카테고리였기 때문입니다.

548
00:24:10,890 --> 00:24:13,530
따라서 이러한 각 범주에 대한 점수가 필요합니다.

549
00:24:13,530 --> 00:24:16,160
최종 레이어는 이미지 분류

550
00:24:16,160 --> 00:24:21,170
문제에 대해 가진 클래스 수와 항상 같습니다.

551
00:24:21,170 --> 00:24:23,500
실제로 매우 유사하게 보입니다.

552
00:24:23,500 --> 00:24:26,130
더 많은 레이어가 있는 AlexNet의

553
00:24:26,130 --> 00:24:28,120
확대 버전과 같습니다.

554
00:24:28,120 --> 00:24:31,680
이제는 두 개의 레이어 대신 풀링

555
00:24:31,680 --> 00:24:34,530
후에 한 번에 세 그룹의

556
00:24:34,530 --> 00:24:37,170
컨볼루션을 수행하고 있습니다.

557
00:24:37,170 --> 00:24:39,210
이 모델에는 기본적으로

558
00:24:39,210 --> 00:24:42,870
세 가지 유형의 레이어만 있지만, 이전에

559
00:24:42,870 --> 00:24:46,320
사람들이 시도했던 것들과 비교할 때 매우

560
00:24:46,320 --> 00:24:49,470
잘 작동하는 것은 정말 놀랍습니다.

561
00:24:49,470 --> 00:24:52,638
오늘 논의할 가장 간단한 모델이라고 할 수

562
00:24:52,638 --> 00:24:54,180
있지만, 왜 3x3

563
00:24:54,180 --> 00:24:57,490
컨볼루션을 사용하는지 궁금할 수 있습니다.

564
00:24:57,490 --> 00:24:59,740
이 값을 어떻게 선택했을까요?

565
00:24:59,740 --> 00:25:02,790
실제로 3x3을 선택한 이유에

566
00:25:02,790 --> 00:25:08,640
대한 직관이 있으며, 특히 세 개 또는 네 개의

567
00:25:08,640 --> 00:25:10,810
그룹이 있습니다.

568
00:25:10,810 --> 00:25:13,470
그래서 여러분에게 질문을 드리겠습니다.

569
00:25:13,470 --> 00:25:16,060
효과적인 수용 영역이란 무엇인가요?

570
00:25:16,060 --> 00:25:19,480
지난 시간에 수용 영역을 살펴보았지만,

571
00:25:19,480 --> 00:25:22,610
이는 특정 활성화 맵의 값이

572
00:25:22,610 --> 00:25:26,550
이전에 본 입력 이미지의 부분을 의미합니다.

573
00:25:26,550 --> 00:25:30,020
모델의 여러 레이어 후에 최종 활성화 맵을 계산하는

574
00:25:30,020 --> 00:25:32,130
데 사용된 값은 무엇인가요?

575
00:25:32,130 --> 00:25:34,130
슬라이딩 필터와 보폭

576
00:25:34,130 --> 00:25:36,980
1을 가진 3x3 컨볼루션 레이어가

577
00:25:36,980 --> 00:25:38,280
세 개 있습니다.

578
00:25:38,280 --> 00:25:39,980
여기에서 활성화 맵

579
00:25:39,980 --> 00:25:43,740
A3의 각 값의 효과적인 수용 영역은 무엇인가요?

580
00:25:43,740 --> 00:25:46,340
이것은 세 번째 레이어 이후입니다.

581
00:25:46,340 --> 00:25:48,990
여기에서 레이어 중 하나를 보여주고 있습니다.

582
00:25:48,990 --> 00:25:52,640
A3의 각 값은 A2의 3x3 값

583
00:25:52,640 --> 00:25:54,810
그리드를 보고 계산됩니다.

584
00:25:54,810 --> 00:25:57,110
그리고 A2의 각 값은 A1의

585
00:25:57,110 --> 00:25:58,530
3x3 그리드입니다.

586
00:25:58,530 --> 00:26:01,680
그것들의 각 값은 입력의 3x3 그리드입니다.

587
00:26:01,680 --> 00:26:05,420
여러분이 잠시 생각해 보도록 하겠습니다.

588
00:26:05,420 --> 00:26:09,275
다음 레이어를 보면 도움이 될 수 있습니다.

589
00:26:09,275 --> 00:26:11,400
이를 시각화하는 것이 실제로 매우 도움이 됩니다.

590
00:26:11,400 --> 00:26:15,410
A1의 각 모서리에서 새로운

591
00:26:15,410 --> 00:26:21,030
3x3 그리드에서 계산하고 있습니다.

592
00:26:21,030 --> 00:26:26,290
입력에서 이 전체 정사각형의 크기는 얼마인가요?

593
00:26:26,290 --> 00:26:27,160
7x7입니다.

594
00:26:27,160 --> 00:26:27,940
네, 정확히 그렇습니다.

595
00:26:27,940 --> 00:26:29,650
첫 번째 것은 3x3입니다.

596
00:26:29,650 --> 00:26:31,980
다음 것은 5x5이고?

597
00:26:31,980 --> 00:26:33,370
그 다음 것은 7x7입니다.

598
00:26:33,370 --> 00:26:36,400
여기에서 이를 쉽게 시각화할 수 있습니다.

599
00:26:36,400 --> 00:26:39,240
보폭 1의 3x3 컨볼루션의 좋은

600
00:26:39,240 --> 00:26:42,270
점은 각 레이어에서 수용 영역에 기본적으로

601
00:26:42,270 --> 00:26:44,590
2를 추가하고 있다는 것입니다.

602
00:26:44,590 --> 00:26:48,300
각 지점에서 왼쪽과 오른쪽, 위와 아래를

603
00:26:48,300 --> 00:26:50,050
보고 있기 때문입니다.

604
00:26:50,050 --> 00:26:51,745
그래서 그런 블록이 많이 생기면

605
00:26:51,745 --> 00:26:53,120
매번 두 개씩 더하는 것입니다.

606
00:26:57,876 --> 00:27:00,780
그래서 기본적으로 우리는 3x3 합성곱과

607
00:27:00,780 --> 00:27:04,050
보폭 1 레이어로 구성된 3개의 스택이 7x7

608
00:27:04,050 --> 00:27:08,192
레이어와 동일한 유효 필드를 가진다는 것을 보여주었습니다.

609
00:27:08,192 --> 00:27:09,900
네, 그래서 질문은

610
00:27:09,900 --> 00:27:12,192
이것이 사후 정당화인지 아니면

611
00:27:12,192 --> 00:27:16,260
그들이 실험을 설계하는 데 사용하는 직관인지입니다.

612
00:27:16,260 --> 00:27:18,860
사실 아키텍처에 따라 다를 것 같습니다.

613
00:27:18,860 --> 00:27:20,990
그래서 어떤 것들은 더 직관적인 데 초점을 맞춥니다.

614
00:27:20,990 --> 00:27:23,532
그리고 실제로 우리가 다음에 다룰

615
00:27:23,532 --> 00:27:24,340
것은

616
00:27:24,340 --> 00:27:27,070
전적으로 경험적 발견에서 시작된 연구

617
00:27:27,070 --> 00:27:28,820
방향이라고 생각합니다.

618
00:27:28,820 --> 00:27:30,653
그래서 그것이 ResNet이고,

619
00:27:30,653 --> 00:27:32,320
거기에는 실제로 전체 조사를

620
00:27:32,320 --> 00:27:35,140
이끌어낸 꽤 좋은 직관이 있다고 생각합니다.

621
00:27:35,140 --> 00:27:40,600
하지만 이 경우에는 저자들이 사후 정당화인지 경험적

622
00:27:40,600 --> 00:27:43,450
발견에 기반한 것인지 또는

623
00:27:43,450 --> 00:27:49,360
설계 선택에 관여했는지에 대해 공개적으로 이야기한 것을

624
00:27:49,360 --> 00:27:51,513
보지 못했기 때문에

625
00:27:51,513 --> 00:27:53,930
말씀드릴 수 없습니다.

626
00:27:53,930 --> 00:27:58,030
하지만 ResNet의 경우, 그것이 창조로 이어진 가설이었다는

627
00:27:58,030 --> 00:28:02,270
것은 알고 있습니다. 하지만 이것은 실제로 매우 좋은 속성입니다.

628
00:28:02,270 --> 00:28:04,780
그래서 이 3x3들이 7x7 레이어와 동일한

629
00:28:04,780 --> 00:28:06,290
유효 수용 필드를 가집니다.

630
00:28:06,290 --> 00:28:08,330
그리고 실제로 매개변수도 더 적습니다.

631
00:28:08,330 --> 00:28:13,570
그래서 채널 차원이 동일하게 유지된다고 가정하면,

632
00:28:13,570 --> 00:28:17,780
입력 채널 수가 있는 3x3

633
00:28:17,780 --> 00:28:19,470
그리드가 있습니다.

634
00:28:19,470 --> 00:28:21,788
그래서 3 곱하기 3 곱하기 C가 각 필터의

635
00:28:21,788 --> 00:28:22,830
값의 수가 됩니다.

636
00:28:22,830 --> 00:28:26,180
그리고 이러한 필터가 C개 있다면 3 곱하기 3 곱하기 C

637
00:28:26,180 --> 00:28:28,500
곱하기 C 또는 3 제곱 C 제곱이 됩니다.

638
00:28:28,500 --> 00:28:30,150
그리고 총 3개의 레이어가 있습니다.

639
00:28:30,150 --> 00:28:35,120
그래서 이 관점에서 보면 실제로 매개변수가

640
00:28:35,120 --> 00:28:37,080
더 적습니다.

641
00:28:37,080 --> 00:28:41,750
그리고 우리는 여기서 더 복잡하고 비선형 모델을

642
00:28:41,750 --> 00:28:43,650
구축하고 있습니다.

643
00:28:43,650 --> 00:28:47,180
그래서 매개변수가 더 적고 입력 데이터 간의 더 복잡한

644
00:28:47,180 --> 00:28:49,460
관계를 모델링할 수 있습니다.

645
00:28:49,460 --> 00:28:52,610
그래서 아마도 이러한 3x3 레이어를 쌓는

646
00:28:52,610 --> 00:28:56,600
것이 단순히 더 큰 필터를 사용하는 것보다 나을 수

647
00:28:56,600 --> 00:28:57,750
있는 이유입니다.

648
00:28:57,750 --> 00:28:59,630
그래서 매개변수가 더 적고

649
00:28:59,630 --> 00:29:02,990
또한 더 복잡한 관계를 모델링할 수 있습니다.

650
00:29:02,990 --> 00:29:05,690
좋습니다, 이제 ResNet에 대해

651
00:29:05,690 --> 00:29:07,700
이야기하겠습니다. 누군가 방금 질문한

652
00:29:07,700 --> 00:29:09,930
사고 실험을 언급할 것입니다.

653
00:29:09,930 --> 00:29:13,590
실제로 ResNet 설계에 대한

654
00:29:13,590 --> 00:29:16,530
많은 대화와 사고를 촉발한

655
00:29:16,530 --> 00:29:19,240
경험적 발견이 있었습니다.

656
00:29:19,240 --> 00:29:23,820
그리고 아이디어는 실제로 더 깊은 레이어를 평범한 CNN

657
00:29:23,820 --> 00:29:27,497
네트워크에 계속 쌓으면, 이렇게 계속 레이어를

658
00:29:27,497 --> 00:29:29,080
추가하면 점점 더

659
00:29:29,080 --> 00:29:31,540
커지는데, 그럼 어떻게 될까요?

660
00:29:31,540 --> 00:29:33,870
우리가 발견한 것과 그들이 발견한

661
00:29:33,870 --> 00:29:38,040
것은 20 레이어 모델이 실제로 56 레이어 모델보다 낮은 테스트

662
00:29:38,040 --> 00:29:40,030
오류를 가진다는 것입니다.

663
00:29:40,030 --> 00:29:42,715
이것이 과적합 때문이라고 생각할 수 있지만, 실제로는

664
00:29:42,715 --> 00:29:45,340
그렇지 않습니다. 왜냐하면 훈련 오류를 보면

665
00:29:45,340 --> 00:29:48,400
이 20 레이어 모델의 훈련 오류도 더 낮기 때문입니다.

666
00:29:48,400 --> 00:29:51,930
그래서 낮은 훈련 오류와 낮은 테스트 오류는 기본적으로

667
00:29:51,930 --> 00:29:55,410
모델이 모든 면에서 더 잘하고 있다는 것을 의미합니다.

668
00:29:55,410 --> 00:29:59,640
그렇다면 왜 56 레이어 모델이 20 레이어

669
00:29:59,640 --> 00:30:02,940
모델보다 성능이 떨어지는 걸까요?

670
00:30:02,940 --> 00:30:04,540
혼란스러울 수 있습니다.

671
00:30:04,540 --> 00:30:07,080
그리고 제가 이전에 언급했듯이,

672
00:30:07,080 --> 00:30:09,240
이는 과적합 때문이 아닙니다.

673
00:30:09,240 --> 00:30:12,450
그래서 이러한 더 깊은 모델은 더 많은

674
00:30:12,450 --> 00:30:16,160
표현력을 가지고 있으며, 이론적으로는 더 얕은 네트워크가

675
00:30:16,160 --> 00:30:20,610
모델링할 수 있는 모든 모델을 표현할 수 있어야 합니다.

676
00:30:20,610 --> 00:30:25,370
따라서 입력과 더 큰 네트워크의 다양한 값 간의 가능한

677
00:30:25,370 --> 00:30:28,370
매핑 집합은 더 작은 네트워크의

678
00:30:28,370 --> 00:30:31,410
슈퍼셋입니다. 왜냐하면 이론적으로는

679
00:30:31,410 --> 00:30:35,318
이러한 레이어 중 일부를 항등 함수로

680
00:30:35,318 --> 00:30:36,860
설정하여 아무것도

681
00:30:36,860 --> 00:30:40,310
하지 않도록 할 수 있다고 상상할 수

682
00:30:40,310 --> 00:30:41,880
있기 때문입니다.

683
00:30:41,880 --> 00:30:44,300
그리고 레이어의 절반을 아무것도 하지

684
00:30:44,300 --> 00:30:46,460
않도록 설정하면, 크기가 절반인 모델과

685
00:30:46,460 --> 00:30:49,080
정확히 동일한 표현력을 가지게 됩니다.

686
00:30:49,080 --> 00:30:53,570
이 모델들이 표현력 측면에서 더 나쁘다는

687
00:30:53,570 --> 00:30:56,760
것이 아니라, 더 깊은 네트워크의

688
00:30:56,760 --> 00:30:58,970
가능한 모델 집합이

689
00:30:58,970 --> 00:31:03,080
더 크고, 이는 더 얕은 네트워크가 배울

690
00:31:03,080 --> 00:31:05,690
수 있는 모든 가능한 모델을

691
00:31:05,690 --> 00:31:11,280
포함하기 때문에 최적화하기가 더 어렵다는 것입니다.

692
00:31:11,280 --> 00:31:16,290
그래서 이전에 언급했지만, 더 깊은 모델이 얕은 모델만큼 좋게

693
00:31:16,290 --> 00:31:17,970
학습할 수 있는

694
00:31:17,970 --> 00:31:19,330
방법은 무엇인가요?

695
00:31:19,330 --> 00:31:20,440
그것은 설정에 의해 가능합니다.

696
00:31:20,440 --> 00:31:23,670
두 개의 층을 가진 모델과 한 개의 층을 가진 모델이

697
00:31:23,670 --> 00:31:26,190
있을 때, 여기 한 층이 있고 오른쪽에

698
00:31:26,190 --> 00:31:29,670
두 층이 있을 때, 한 층을 본질적으로 항등 행렬로

699
00:31:29,670 --> 00:31:32,700
설정하면, 이는 단순히 항등 함수입니다. 그?

700
00:31:32,700 --> 00:31:36,630
모델은 적어도 얕은 모델만큼 좋아야 합니다.

701
00:31:36,630 --> 00:31:40,290
적어도 얕은 모델만큼 좋아야 합니다.

702
00:31:40,290 --> 00:31:43,720
그렇다면 실제로 이 직관을 우리의 모델에 어떻게 구축할 수 있을까요?

703
00:31:43,720 --> 00:31:45,810
우리는 최적화 중에 원할

704
00:31:45,810 --> 00:31:48,000
경우 얕은 모델만큼 좋을

705
00:31:48,000 --> 00:31:50,500
수 있도록 하고 싶습니다.

706
00:31:50,500 --> 00:31:53,100
우리가 이를 수행하는 방법은

707
00:31:53,100 --> 00:31:56,070
실제로 원하는 기본 매핑을 직접 맞추는

708
00:31:56,070 --> 00:32:00,580
대신 잔여 매핑이라고 불리는 것을 맞추는 것입니다.

709
00:32:00,580 --> 00:32:04,170
이것은 기본적으로 값 x를 가져와서

710
00:32:04,170 --> 00:32:07,830
우리의 컨볼루션 레이어를 넘어

711
00:32:07,830 --> 00:32:12,310
복사하여 이 지점의 값이 이미 x, 즉 원래 입력과

712
00:32:12,310 --> 00:32:16,810
두 개의 컨볼루션 스택의 출력을 받고

713
00:32:16,810 --> 00:32:18,080
있도록 합니다.

714
00:32:18,080 --> 00:32:20,530
기본적으로 이 시점에서

715
00:32:20,530 --> 00:32:23,590
잔여 맵이라고 불리는 F of

716
00:32:23,590 --> 00:32:29,870
x는 모든 컨볼루션 필터에 대해 0 값을 학습할 수 있습니다.

717
00:32:29,870 --> 00:32:31,670
그럼 출력은 여기서 0이 될 것입니다.

718
00:32:31,670 --> 00:32:35,120
그런 다음 우리는 여기서 x를 더하고 x를 얻습니다.

719
00:32:35,120 --> 00:32:37,660
이것은 모델이 레이어에 대해 학습할 필요가 없을

720
00:32:37,660 --> 00:32:40,690
경우 이러한 레이어를 우회할 수 있는 매우 간단한 방법을

721
00:32:40,690 --> 00:32:41,480
제공합니다.

722
00:32:41,480 --> 00:32:45,190
이것은 이제 필터를 0으로 학습함으로써 우리가

723
00:32:45,190 --> 00:32:47,920
이전에 이야기한 이 항등 함수를

724
00:32:47,920 --> 00:32:51,440
매우 쉽게 학습할 수 있음을 의미합니다.

725
00:32:51,440 --> 00:32:53,450
예를 들어, 모든 필터가

726
00:32:53,450 --> 00:32:57,725
0 값으로 채워지거나, 이 경우 더 실용적으로는

727
00:32:57,725 --> 00:32:59,350
x에서 h of x로의

728
00:32:59,350 --> 00:33:03,025
전체 매핑을 학습하는 대신 이 차이, 즉 F

729
00:33:03,025 --> 00:33:06,310
of x만 학습하면 되기 때문에 매우

730
00:33:06,310 --> 00:33:08,500
작은 값을 학습해야 합니다.

731
00:33:08,500 --> 00:33:11,850
이제 원하는 출력과

732
00:33:11,850 --> 00:33:14,370
복사된 블록 간의

733
00:33:14,370 --> 00:33:18,220
차이를 배우고 있습니다.

734
00:33:18,220 --> 00:33:22,150
이것은 잔차 블록 또는 잔차 연결이라고

735
00:33:22,150 --> 00:33:24,360
하며, 모델의 이전 레이어에서

736
00:33:24,360 --> 00:33:26,910
나중 레이어로 값을

737
00:33:26,910 --> 00:33:30,720
복사하고 그 지점의 값에 더하는 것입니다.

738
00:33:30,720 --> 00:33:32,830
제가 말씀드린 직관은 이러한

739
00:33:32,830 --> 00:33:36,240
더 큰 네트워크가 최적화하기 어려워서

740
00:33:36,240 --> 00:33:39,810
훈련 및 테스트 오류가 더 나빠진다는

741
00:33:39,810 --> 00:33:41,560
관찰된 현상이었습니다.

742
00:33:41,560 --> 00:33:44,760
따라서 직관은 더 얕은 네트워크를 쉽게 모델링할 수 있는 모델을

743
00:33:44,760 --> 00:33:47,070
구축해야 한다는 것이었습니다. 그래야

744
00:33:47,070 --> 00:33:49,390
적어도 더 얕은 모델만큼은 좋을 수 있습니다.

745
00:33:49,390 --> 00:33:51,990
그들이 이를 수행한 방법은 잔차 연결을

746
00:33:51,990 --> 00:33:54,683
추가하여 값을 쉽게 복사하고 아키텍처

747
00:33:54,683 --> 00:33:56,350
자체에 통합하는 것이었습니다.

748
00:33:56,350 --> 00:33:59,070
합성곱 레이어 간의 정체성 매핑을

749
00:33:59,070 --> 00:34:00,550
학습하려고 하지 않고요.

750
00:34:00,550 --> 00:34:04,470
경험적으로도 이것이 매우 잘 작동한다는 것이 입증되었습니다.

751
00:34:04,470 --> 00:34:06,500
네, 잔차 블록은 무엇을 전달하나요?

752
00:34:06,500 --> 00:34:07,940
입력 x가 있습니다.

753
00:34:07,940 --> 00:34:10,550
두 개의 서로 다른 합성곱 레이어를

754
00:34:10,550 --> 00:34:12,670
통과시키고 출력 F(x)를 얻습니다.

755
00:34:12,670 --> 00:34:14,900
x는 여기서 그대로 복사됩니다.

756
00:34:14,900 --> 00:34:17,050
이것은 x와 정확히

757
00:34:17,050 --> 00:34:21,820
동일하며, 두 블록의 출력인 F(x)에 더합니다.

758
00:34:21,820 --> 00:34:24,679
네, x는 이전 레이어 중 하나의

759
00:34:24,679 --> 00:34:26,739
출력이거나 모델의 첫 번째

760
00:34:26,739 --> 00:34:28,659
레이어라면 이미지일 것입니다.

761
00:34:28,659 --> 00:34:30,460
네, 질문은 데이터가 충분하지

762
00:34:30,460 --> 00:34:32,627
않을 수 있고, 충분한 데이터를 추가하면

763
00:34:32,627 --> 00:34:35,440
이러한 블록 없이 모델을 훈련할 수 있을지입니다.

764
00:34:35,440 --> 00:34:38,320
이 블록들이 실제로 더 많은 데이터로부터

765
00:34:38,320 --> 00:34:40,840
학습하는 데 매우 도움이 된다고 생각합니다.

766
00:34:40,840 --> 00:34:43,610
문제는 정말로 최적화 문제였습니다.

767
00:34:43,610 --> 00:34:45,280
변환기들은 정확히 같은 이유로

768
00:34:45,280 --> 00:34:47,505
잔차 블록을 사용했습니다. 그리고 이것이

769
00:34:47,505 --> 00:34:48,880
더 복잡한 모델을 모델링하는

770
00:34:48,880 --> 00:34:51,580
데 도움이 되며, 실제로 더 많은 데이터를

771
00:34:51,580 --> 00:34:53,000
사용할 수 있게 해줍니다.

772
00:34:53,000 --> 00:34:55,427
그래서 저는 매우 좋다고 생각합니다.

773
00:34:55,427 --> 00:34:57,010
잔여 블록은 더 많은

774
00:34:57,010 --> 00:35:00,650
데이터를 더 효율적으로 사용할 수 있도록 도와줍니다.

775
00:35:00,650 --> 00:35:04,970
왜냐하면 더 많은 함수들을 모델링할 수 있기 때문입니다.

776
00:35:04,970 --> 00:35:09,260
그래서 질문은 아마도 우리가 더 오랫동안 훈련하면

777
00:35:09,260 --> 00:35:13,070
성능이 결국 더 작은 네트워크의 값으로

778
00:35:13,070 --> 00:35:15,188
수렴할 것이라는 것입니다.

779
00:35:15,188 --> 00:35:17,480
그리고 더 큰 모델을 훈련하는 데 시간이 더

780
00:35:17,480 --> 00:35:20,000
걸리기 때문에 최적화하기가 더 어려울 수도 있습니다.

781
00:35:20,000 --> 00:35:25,760
그리고 제 생각에는 그렇지 않다는 답이 있습니다. 훈련을 얼마나 오래

782
00:35:25,760 --> 00:35:28,507
하든지 간에 작은 모델의 성능에

783
00:35:28,507 --> 00:35:30,090
수렴하지 않았습니다.

784
00:35:30,090 --> 00:35:32,390
그 이유는 본질적으로

785
00:35:32,390 --> 00:35:36,110
지역 최소값에 갇혀 있기 때문입니다.

786
00:35:36,110 --> 00:35:39,120
이러한 잔여 연결을 추가하면 이러한

787
00:35:39,120 --> 00:35:41,430
문제를 피할 수 있습니다.

788
00:35:41,430 --> 00:35:44,970
이것이 왜 그런지에 대한 실제 설명은 여전히

789
00:35:44,970 --> 00:35:48,950
더 활발한 연구 분야라고 할 수 있습니다.

790
00:35:48,950 --> 00:35:51,350
이 모델들이 글로벌

791
00:35:51,350 --> 00:35:56,060
최소값을 피하고 최적의 솔루션을

792
00:35:56,060 --> 00:35:58,830
찾지 못하는 원인을

793
00:35:58,830 --> 00:36:03,730
정확히 이해하기는 정말 어렵습니다.

794
00:36:03,730 --> 00:36:06,130
그리고 종종 이것은 정말 경험적인 발견이지만, 그

795
00:36:06,130 --> 00:36:07,690
뒤에는 어떤 직관이 있습니다.

796
00:36:07,690 --> 00:36:09,150
이 경우 직관은 우리가

797
00:36:09,150 --> 00:36:11,612
모델이 그 당시 더 잘 수행하고 있었던

798
00:36:11,612 --> 00:36:14,070
얕은 모델만큼은 최소한 잘 수행할 수 있도록

799
00:36:14,070 --> 00:36:15,300
하려는 것입니다.

800
00:36:15,300 --> 00:36:17,520
그래서 단순히 더 오랫동안 훈련하면

801
00:36:17,520 --> 00:36:19,920
더 잘할 수 있는 것은 아닙니다.

802
00:36:19,920 --> 00:36:23,940
실제로 그것은 얕은 모델만큼 잘 수행할

803
00:36:23,940 --> 00:36:26,780
수 없는 한계였습니다.

804
00:36:31,590 --> 00:36:38,100
좋습니다, 그래서 전체 ResNet 아키텍처는 이렇습니다.

805
00:36:38,100 --> 00:36:41,050
이제 우리는 이러한 잔여 블록의 스택이 있습니다.

806
00:36:41,050 --> 00:36:44,700
그래서 이것이 여기 두 블록이 함께 의미하는 것입니다.

807
00:36:44,700 --> 00:36:47,500
잔여 블록입니다.

808
00:36:47,500 --> 00:36:49,860
그래서 우리는 ReLU가 있는 3x3 컨볼루션과

809
00:36:49,860 --> 00:36:51,730
또 다른 3x3 컨볼루션이 있습니다.

810
00:36:51,730 --> 00:36:54,172
그리고 우리는 여기서 이 x 값을 복사하고 있습니다.

811
00:36:54,172 --> 00:36:55,630
우리는 여기서 출력에 추가하고

812
00:36:55,630 --> 00:36:57,297
그 다음에 ReLU를 적용합니다.

813
00:36:57,297 --> 00:37:00,975
그래서 이 블록 쌍 각각이 이러한 잔여 블록 중 하나입니다.

814
00:37:00,975 --> 00:37:03,100
그래서 여기서 이 선이 건너뛰는 것을

815
00:37:03,100 --> 00:37:06,370
볼 수 있는 이유는 값이 앞으로 추가되기 때문입니다.

816
00:37:06,370 --> 00:37:08,620
ResNet의 멋진

817
00:37:08,620 --> 00:37:14,150
점은 기본적으로 그들이 만든 다양한 깊이가 많다는

818
00:37:14,150 --> 00:37:14,900
것입니다.

819
00:37:14,900 --> 00:37:16,608
그래서 그들은 더 작고 더 큰 모델의

820
00:37:16,608 --> 00:37:17,980
전체 패밀리를 만들었습니다.

821
00:37:17,980 --> 00:37:20,480
그리고 그들은 레이어 수를 늘릴수록

822
00:37:20,480 --> 00:37:22,100
성능이 증가한다는

823
00:37:22,100 --> 00:37:25,390
것을 보여주었습니다. 비록 더 큰 모델로

824
00:37:25,390 --> 00:37:28,010
갈수록 성능 차이는 작아졌습니다.

825
00:37:28,010 --> 00:37:31,700
그래서 데이터 세트를 고려할 때, 그들은 더

826
00:37:31,700 --> 00:37:35,530
많은 레이어를 추가함으로써 상당한 양을 확장할

827
00:37:35,530 --> 00:37:38,110
수 없었지만, 특히 초기 모델들

828
00:37:38,110 --> 00:37:40,658
사이에서 성능의 상당한 개선을

829
00:37:40,658 --> 00:37:41,450
보았습니다.

830
00:37:41,450 --> 00:37:43,877
그리고 101에서 152까지는 성능이 실제로

831
00:37:43,877 --> 00:37:44,960
변화하지 않았습니다.

832
00:37:44,960 --> 00:37:47,860
약간 더 나아졌지만, 그 시점에서 성능 변화는

833
00:37:47,860 --> 00:37:49,990
아마도 1%에 불과했습니다.

834
00:37:49,990 --> 00:37:51,800
그래서 그들은 152라는 숫자를 어떻게 얻었나요?

835
00:37:51,800 --> 00:37:54,520
사실 저는 그들이 152라는 숫자를 어떻게 얻었는지 모르겠습니다.

836
00:37:54,520 --> 00:37:58,190
그들은 여기서 다른 값을 시도해보고 싶었던 것 같습니다.

837
00:37:58,190 --> 00:37:59,170
그리고 당신은

838
00:37:59,170 --> 00:38:00,760
그것이-- 정확히 두

839
00:38:00,760 --> 00:38:05,350
배는 아니지만, 매번 상당한 증가가 있다는 것을 볼 수 있습니다.

840
00:38:05,350 --> 00:38:07,400
그들이 152를 어떻게 선택했는지 모르겠어요.

841
00:38:07,400 --> 00:38:08,620
좋은 질문이에요.

842
00:38:08,620 --> 00:38:11,700
아마도 다른 것들보다 더 잘 작동하는 것을 보여줬을 거예요.

843
00:38:11,700 --> 00:38:13,650
사실 저도 잘 모르겠어요.

844
00:38:13,650 --> 00:38:16,830
일반적으로 모델을 위해 여러 다른 층 수를 시도할

845
00:38:16,830 --> 00:38:18,970
때, 예를 들어 시도하고 싶은

846
00:38:18,970 --> 00:38:21,720
층 수가 있다면, 먼저 가장 작은 모델을

847
00:38:21,720 --> 00:38:24,940
훈련시키고 성능을 보고, 그 다음 더 많은

848
00:38:24,940 --> 00:38:26,800
층을 추가하여 성능이 증가하는지

849
00:38:26,800 --> 00:38:29,230
확인하고 계속 진행하는 방식입니다.

850
00:38:29,230 --> 00:38:31,170
그래서 아마도 그들이 152에서 멈춘

851
00:38:31,170 --> 00:38:34,410
이유는 성능이 더 이상 많이 증가하지 않았기 때문일 거예요.

852
00:38:34,410 --> 00:38:36,670
또한 GPU 메모리 제한이 있어서 모델이

853
00:38:36,670 --> 00:38:38,470
커질수록 하드웨어 관점에서

854
00:38:38,470 --> 00:38:40,720
훈련하기가 더 어려워집니다. 더 많은

855
00:38:40,720 --> 00:38:43,450
매개변수를 GPU 메모리에 맞춰야 하니까요.

856
00:38:43,450 --> 00:38:47,040
따라서 컴퓨팅 환경에 따라 훈련할 수 있는 모델의

857
00:38:47,040 --> 00:38:49,080
크기에는 한계가 있습니다.

858
00:38:49,080 --> 00:38:51,940
이 모델들은 별도로 훈련해야 할 것

859
00:38:51,940 --> 00:38:55,710
같아요. 예를 들어 18층 모델 하나와 34층

860
00:38:55,710 --> 00:38:57,790
모델 하나를 두는 식으로요.

861
00:38:57,790 --> 00:39:03,100
질문은 잔여 연결을 사용하고 있으므로 CNN 블록의 직관을 어떻게

862
00:39:03,100 --> 00:39:07,270
생각할 것인가입니다. 여전히 더 높은 수준의

863
00:39:07,270 --> 00:39:09,568
추상화로 생각할 수 있습니다.

864
00:39:09,568 --> 00:39:11,360
이것은 층에서 사실로 입증되었습니다.

865
00:39:11,360 --> 00:39:16,020
블록 내에서 단순히 더 높은 수준의 특징을 학습하는

866
00:39:16,020 --> 00:39:18,520
대신, 원본 이미지에서 더

867
00:39:18,520 --> 00:39:20,228
높은 수준의 특징을

868
00:39:20,228 --> 00:39:23,180
얻기 위해 델타를 학습하고 있습니다.

869
00:39:23,180 --> 00:39:24,950
블록에서 학습하는 것은 그것입니다.

870
00:39:24,950 --> 00:39:27,280
델타를 학습하고 있지만, 각 단계에서 이러한

871
00:39:27,280 --> 00:39:29,900
더 높은 수준의 표현을 여전히 달성하고 있습니다.

872
00:39:29,900 --> 00:39:32,500
그 부분은 동일하지만,

873
00:39:32,500 --> 00:39:38,560
실제로 수행하는 기능적인 방법은 이전 입력에 추가하는 F(x)를

874
00:39:38,560 --> 00:39:41,120
학습하는 것입니다.

875
00:39:41,120 --> 00:39:43,270
그래서 델타를 학습하는 것과 같습니다.

876
00:39:43,270 --> 00:39:44,785
질문은 덧셈을 할 때 같은

877
00:39:44,785 --> 00:39:46,910
텐서 크기가 필요하냐는 것입니다.

878
00:39:46,910 --> 00:39:47,690
답은 예입니다.

879
00:39:47,690 --> 00:39:51,203
그리고 이것이 모든 것이 3x3

880
00:39:51,203 --> 00:39:52,870
컨볼루션을 가지고

881
00:39:52,870 --> 00:39:57,590
있는 이유 중 하나입니다. 보폭 1과 패딩

882
00:39:57,590 --> 00:40:01,370
1로 모든 층에서 동일한 크기를 유지합니다.

883
00:40:01,370 --> 00:40:01,890
1로 모든 층에서 동일한 크기를 유지합니다.

884
00:40:01,890 --> 00:40:05,443
예를 들어 풀링 레이어가 있다면, 값을

885
00:40:05,443 --> 00:40:06,860
풀어내는 방법을

886
00:40:06,860 --> 00:40:12,710
생각해낼 수 있을지 모르지만, 풀링 레이어 이후에는 텐서의

887
00:40:12,710 --> 00:40:14,343
크기가 다르기

888
00:40:14,343 --> 00:40:16,010
때문에 더 이상 단순한

889
00:40:16,010 --> 00:40:18,570
덧셈을 할 수 없습니다.

890
00:40:18,570 --> 00:40:23,925
이들은 적어도 일반적인 풀링 전에 수행됩니다.

891
00:40:23,925 --> 00:40:25,550
각 값을 여러

892
00:40:25,550 --> 00:40:28,800
값으로 분산시켜서 해결할

893
00:40:28,800 --> 00:40:30,710
수는 있지만요.

894
00:40:30,710 --> 00:40:35,390
좋아요, 그래서 이것이 ResNet의 주요 요점입니다.

895
00:40:35,390 --> 00:40:38,180
그들이 하는 또 다른 멋진 트릭은

896
00:40:38,180 --> 00:40:40,620
일정 수의 블록 후에

897
00:40:40,620 --> 00:40:42,770
필터 수를 두 배로 늘리고

898
00:40:42,770 --> 00:40:46,040
공간 차원을 다운샘플링하는 것입니다.

899
00:40:46,040 --> 00:40:48,170
기본적으로 정말 평평한

900
00:40:48,170 --> 00:40:53,525
이미지로 시작한다고 상상할 수 있습니다. 활성화가 네트워크를

901
00:40:53,525 --> 00:40:55,900
통과하면서 공간적으로 작아지지만

902
00:40:55,900 --> 00:40:57,880
깊이는 더 커집니다.

903
00:40:57,880 --> 00:40:59,195
이렇게 생각하면 됩니다.

904
00:40:59,195 --> 00:41:00,570
그리고 마지막에는

905
00:41:00,570 --> 00:41:03,243
분류에 사용할 벡터가 됩니다.

906
00:41:03,243 --> 00:41:05,160
그래서 네트워크 내에서 값이

907
00:41:05,160 --> 00:41:07,650
어떻게 변화하는지와 그 형태를 시각화하는

908
00:41:07,650 --> 00:41:08,740
방법입니다.

909
00:41:08,740 --> 00:41:11,280
그리고 ResNet에 다소 독특한

910
00:41:11,280 --> 00:41:14,110
또 다른 점은, 다른 아키텍처도

911
00:41:14,110 --> 00:41:17,740
이러지만, 잔여 블록이 있는 모든 층 이전에

912
00:41:17,740 --> 00:41:21,520
상대적으로 큰 컨볼루션 레이어가 있다는 것입니다.

913
00:41:21,520 --> 00:41:23,040
여기서는 이를 추가했을 때 더

914
00:41:23,040 --> 00:41:25,240
잘 작동한다는 것이 경험적으로 입증되었습니다.

915
00:41:25,240 --> 00:41:29,610
그래서 이것은 순전히 경험적 발견입니다.

916
00:41:29,610 --> 00:41:34,890
좋아요, 기본적으로 강조하고 싶은 것은 이러한 큰 모델들이

917
00:41:34,890 --> 00:41:37,230
매우 잘 작동했다는 것입니다.

918
00:41:37,230 --> 00:41:39,870
그들은 100개 이상의 레이어 모델을 성공적으로 훈련할 수

919
00:41:39,870 --> 00:41:42,280
있었던 것은 처음이었으므로 정말 큰 사건이었습니다.

920
00:41:42,280 --> 00:41:45,810
기본적으로 ResNet은 이후 다양한 컴퓨터 비전

921
00:41:45,810 --> 00:41:47,320
작업에 사용되었습니다.

922
00:41:47,320 --> 00:41:48,848
그 당시 거의 모든

923
00:41:48,848 --> 00:41:50,640
컴퓨터 비전 작업은 이러한

924
00:41:50,640 --> 00:41:54,890
잔차 연결 덕분에 성능이 뛰어난 ResNet을 사용했습니다.

925
00:41:54,890 --> 00:41:59,370
우리는 CNN 아키텍처에 대해 이야기했으며, 주요 아키텍처는

926
00:41:59,370 --> 00:42:02,710
ResNet과 역사적으로 VGG입니다.

927
00:42:02,710 --> 00:42:05,800
우리는 작은 필터 크기가 유용한 이유와 이러한 레이어를 많이 사용하는

928
00:42:05,800 --> 00:42:07,780
것이 유용한 이유에 대해 이야기했습니다.

929
00:42:07,780 --> 00:42:09,780
CNN을 실제로 구성하고

930
00:42:09,780 --> 00:42:11,910
훈련 준비를 하는 방법에

931
00:42:11,910 --> 00:42:15,120
대해 마지막으로 이야기할 것은 개별

932
00:42:15,120 --> 00:42:19,170
레이어의 가중치 값을 어떻게 초기화하는가입니다.

933
00:42:19,170 --> 00:42:24,010
선택하는 값에 따라 너무 작거나 너무 큰 값을 설정할

934
00:42:24,010 --> 00:42:27,370
수 있으며, 이는 훈련 중 모델에

935
00:42:27,370 --> 00:42:29,880
심각한 문제를 일으킬 수

936
00:42:29,880 --> 00:42:30,820
있습니다.

937
00:42:30,820 --> 00:42:36,360
여기서는 4,096 차원의 특징을

938
00:42:36,360 --> 00:42:40,000
가진 6층 네트워크입니다.

939
00:42:40,000 --> 00:42:43,600
이것은 단순히 6개의 완전 연결 모델 레이어이며, 우리는

940
00:42:43,600 --> 00:42:45,330
그것들을 초기화합니다.

941
00:42:45,330 --> 00:42:48,400
여기서는 단위 가우시안 랜덤 값을 얻고,

942
00:42:48,400 --> 00:42:50,610
이를 0.01로 곱하여 0에

943
00:42:50,610 --> 00:42:53,560
가까운 매우 작은 값을 얻으며, 각

944
00:42:53,560 --> 00:42:55,720
레이어에도 ReLU가 있습니다.

945
00:42:55,720 --> 00:43:00,340
이 모델의 순전파를 플롯하면, 처음에는 상대적으로

946
00:43:00,340 --> 00:43:02,400
높은 값을 얻는 것을 볼

947
00:43:02,400 --> 00:43:06,280
수 있습니다. ReLU가 있기 때문에 모든

948
00:43:06,280 --> 00:43:08,260
평균이 양수가 될 것이지만,

949
00:43:08,260 --> 00:43:10,890
평균과 표준 편차는 상대적으로

950
00:43:10,890 --> 00:43:12,130
높습니다.

951
00:43:12,130 --> 00:43:14,790
하지만 각 레이어가 진행됨에 따라,

952
00:43:14,790 --> 00:43:17,160
매우 작은 가중치 초기화 덕분에

953
00:43:17,160 --> 00:43:20,120
평균과 표준 편차가 점점 작아집니다.

954
00:43:20,120 --> 00:43:21,870
이론적으로 우리는 각

955
00:43:21,870 --> 00:43:24,360
레이어의 모든 값이 동일하기를 원합니다.

956
00:43:24,360 --> 00:43:26,460
이는 최적화 문제를 훨씬

957
00:43:26,460 --> 00:43:28,860
더 쉽게 해결할 수 있게 합니다.

958
00:43:28,860 --> 00:43:33,690
0.01 대신 0.05를 사용한다고 하면, 너무 큰 값으로

959
00:43:33,690 --> 00:43:36,780
설정했을 때 어떤 문제가 발생할지 상상할

960
00:43:36,780 --> 00:43:38,925
수 있는 사람이 있나요?

961
00:43:38,925 --> 00:43:42,540
너무 작으면 기본적으로 0으로 가게 됩니다.

962
00:43:42,540 --> 00:43:44,970
너무 크면 어떻게 될까요?

963
00:43:44,970 --> 00:43:47,580
네, 기본적으로 각 레이어에서 활성화가

964
00:43:47,580 --> 00:43:48,400
점점 커집니다.

965
00:43:48,400 --> 00:43:51,730
여기서 플롯하면, 마지막에는 엄청난 평균과 표준

966
00:43:51,730 --> 00:43:54,050
편차가 있다는 것을 볼 수 있습니다.

967
00:43:54,050 --> 00:43:57,910
152층 ResNet을 훈련하고 있다면, 이것이 매우 빠르게 큰

968
00:43:57,910 --> 00:44:00,610
문제가 될 수 있다는 것을 상상할 수 있습니다.

969
00:44:00,610 --> 00:44:03,980
그럼 실제로 어떻게 해야 할까요?

970
00:44:03,980 --> 00:44:07,240
이 경우 최적의 값은 0.022 정도라고 생각하지만,

971
00:44:07,240 --> 00:44:10,600
이를 어떻게 알 수 있고, 모든 레이어에서

972
00:44:10,600 --> 00:44:13,480
더 일반적으로 어떻게 할 수 있을까요?

973
00:44:13,480 --> 00:44:16,610
가중치를 초기화하는 몇 가지 방법이 있습니다.

974
00:44:16,610 --> 00:44:18,430
오늘 수업에서는 가장 일반적으로 사용되는

975
00:44:18,430 --> 00:44:20,510
방법을 설명하겠지만, 다른 방법도 있습니다.

976
00:44:20,510 --> 00:44:22,930
일반적으로 이들은

977
00:44:22,930 --> 00:44:27,380
값의 차원에 따라 달라집니다.

978
00:44:27,380 --> 00:44:33,100
따라서 4,096 차원의 완전 연결 레이어와 2048 차원의 레이어는

979
00:44:33,100 --> 00:44:34,880
다른 값을 가집니다.

980
00:44:34,880 --> 00:44:37,990
우리가 살펴볼 특정 공식은 Kaiming

981
00:44:37,990 --> 00:44:40,190
초기화라고 합니다.

982
00:44:40,190 --> 00:44:42,980
사실 ResNet을 만든 사람과 동일한 사람입니다.

983
00:44:42,980 --> 00:44:45,818
Kaiming은 매우 유명했습니다.

984
00:44:45,818 --> 00:44:47,860
그는 여전히 매우 유명한 컴퓨터 비전

985
00:44:47,860 --> 00:44:48,440
연구자입니다.

986
00:44:48,440 --> 00:44:50,523
그는 지난 10년 또는 15년 동안 가장

987
00:44:50,523 --> 00:44:53,560
많이 인용된 컴퓨터 과학자 중 한 명이라고 생각합니다.

988
00:44:53,560 --> 00:44:57,730
그는 컴퓨터 비전 커뮤니티에서 매우 잘 알려져 있습니다.

989
00:44:57,730 --> 00:45:00,600
그는 또한 입력 차원

990
00:45:00,600 --> 00:45:05,190
크기의 제곱근으로 값을 초기화하는 아이디어를

991
00:45:05,190 --> 00:45:07,390
제안했습니다.

992
00:45:07,390 --> 00:45:09,450
그리고 그들이 어떻게 이를 도출하고

993
00:45:09,450 --> 00:45:12,977
ReLU 활성화 함수로 인해 표준 편차와 평균이 층 전체에서 상대적으로

994
00:45:12,977 --> 00:45:15,060
일정하게 유지된다는 것을 보여주었는지에

995
00:45:15,060 --> 00:45:17,680
대한 모든 세부 사항을 다루지는 않을 것입니다.

996
00:45:17,680 --> 00:45:20,297
하지만 이를 그래프로 나타내면, 이것이 효과가 있다는 것을 알

997
00:45:20,297 --> 00:45:22,380
수 있으므로, 이를 마치 원하는 속성을 얻기

998
00:45:22,380 --> 00:45:24,505
위해 끼워 넣는 마법의 공식처럼 생각할 수 있습니다.

999
00:45:24,505 --> 00:45:26,590
그리고 유도 과정을 알고 싶다면,

1000
00:45:26,590 --> 00:45:28,800
실제로 논문을 링크해 두었으니 자유롭게

1001
00:45:28,800 --> 00:45:31,690
살펴보셔도 되지만, 우리의 말을 믿으셔도 됩니다.

1002
00:45:31,690 --> 00:45:33,190
여기서 세부 사항을

1003
00:45:33,190 --> 00:45:36,600
다루지는 않겠지만, 평균과 표준 편차가 변하지 않는

1004
00:45:36,600 --> 00:45:37,990
원하는 효과를 줍니다.

1005
00:45:37,990 --> 00:45:40,180
또한 주어진 설정에

1006
00:45:40,180 --> 00:45:42,340
대해 테스트를 통해 이

1007
00:45:42,340 --> 00:45:46,630
값이 무엇인지 찾으려고 할 수도 있습니다.

1008
00:45:46,630 --> 00:45:50,140
좋습니다, 그래서 이것이 가중치를 초기화하는 방법,

1009
00:45:50,140 --> 00:45:52,270
서로 다른 층을 결합하여

1010
00:45:52,270 --> 00:45:53,980
CNN 아키텍처를 형성하는

1011
00:45:53,980 --> 00:45:58,570
방법, 사람들이 사용하는 활성화 함수, 그리고 CNN의 다양한

1012
00:45:58,570 --> 00:46:01,010
층에 대해 논의하는 방법입니다.

1013
00:46:01,010 --> 00:46:03,260
그래서 이미 꽤 많은 주제를 다룬 것 같습니다.

1014
00:46:03,260 --> 00:46:04,990
그래서 이 부분에 대해 질문이

1015
00:46:04,990 --> 00:46:06,920
있는지 잠시 멈추겠습니다.

1016
00:46:06,920 --> 00:46:08,980
강의의 두 번째 부분은 실제로 첫

1017
00:46:08,980 --> 00:46:11,210
번째 부분보다 훨씬 덜 밀집되어 있습니다.

1018
00:46:11,210 --> 00:46:13,120
그래서 우리는 이러한 모델을

1019
00:46:13,120 --> 00:46:17,158
훈련할 때 유용한 실용적인 팁을 많이 다룰 것입니다.

1020
00:46:17,158 --> 00:46:19,450
질문은 CNN의 가중치 초기화를 어떻게

1021
00:46:19,450 --> 00:46:20,180
하느냐입니다.

1022
00:46:20,180 --> 00:46:23,120
여전히 같은 초기화를 사용하지만,

1023
00:46:23,120 --> 00:46:26,120
여기서의 차원 n은 커널의 크기입니다.

1024
00:46:26,120 --> 00:46:29,320
3x3 커널에 채널이 6이라면 3

1025
00:46:29,320 --> 00:46:33,830
곱하기 3 곱하기 6이 되지만, 같은 아이디어입니다.

1026
00:46:33,830 --> 00:46:36,460
단지 층의 종류에

1027
00:46:36,460 --> 00:46:40,120
따라 차원을 다르게 계산합니다.

1028
00:46:40,120 --> 00:46:43,150
각 작업에서 대략적으로 값의 수로 생각할

1029
00:46:43,150 --> 00:46:46,220
수 있지만, 이는 층에 따라 다릅니다.

1030
00:46:46,220 --> 00:46:49,590
일부 층은 다른 가중치 초기화를 사용하지만,

1031
00:46:49,590 --> 00:46:51,260
이는 CNN에

1032
00:46:51,260 --> 00:46:54,170
적용되는 초기화 방법에 대한 것입니다.

1033
00:46:54,170 --> 00:46:56,600
그래서 질문은, 초기화 값이 너무

1034
00:46:56,600 --> 00:46:59,370
크면 왜 활성화가 폭발하는가입니다?

1035
00:46:59,370 --> 00:47:04,220
각 층에서 초기화된 네트워크의 값들이 무작위로

1036
00:47:04,220 --> 00:47:08,070
초기화된 집합이 있다고 상상해 보세요.

1037
00:47:08,070 --> 00:47:13,328
그리고 값이 매우 크면, 그 다음에 ReLU

1038
00:47:13,328 --> 00:47:15,630
활성화를 수행하게 됩니다.

1039
00:47:15,630 --> 00:47:21,330
그것은 실제로 층의 출력을 제한하지 않습니다.

1040
00:47:21,330 --> 00:47:23,400
ReLU로 무한대로 갈 수 있습니다.

1041
00:47:23,400 --> 00:47:27,050
따라서 무작위 값의 동일한 집합에

1042
00:47:27,050 --> 00:47:29,458
모든 가중치를 초기화하기

1043
00:47:29,458 --> 00:47:31,250
때문에 본질적으로

1044
00:47:31,250 --> 00:47:35,370
같은 작업을 반복하게 되면, 각 층에서 큰

1045
00:47:35,370 --> 00:47:38,120
값의 집합을 또 다른 큰 값으로

1046
00:47:38,120 --> 00:47:40,730
초기화된 집합과 곱하게

1047
00:47:40,730 --> 00:47:44,620
되어 이후 반복마다 더 커지게 됩니다.

1048
00:47:44,620 --> 00:47:47,850
이것은 마치 재귀 관계처럼 생각할 수

1049
00:47:47,850 --> 00:47:50,250
있는데, 처음에 모두 무작위로

1050
00:47:50,250 --> 00:47:53,650
초기화되어 값과 곱해지기 때문에 간단한

1051
00:47:53,650 --> 00:47:55,860
재귀 관계에서는 1이 되기를

1052
00:47:55,860 --> 00:47:56,860
원합니다.

1053
00:47:56,860 --> 00:47:59,100
하지만 행렬과

1054
00:47:59,100 --> 00:48:01,912
곱해지는 값의 벡터가 있기

1055
00:48:01,912 --> 00:48:03,870
때문에, 평균

1056
00:48:03,870 --> 00:48:10,530
출력은 벡터의 차원에 따라 달라지며, ReLU 이후에는

1057
00:48:10,530 --> 00:48:12,930
활성화의 표준 편차가

1058
00:48:12,930 --> 00:48:14,440
있습니다.

1059
00:48:14,440 --> 00:48:16,110
그런 다음 모든 음수를

1060
00:48:16,110 --> 00:48:18,857
제거하고 그 시점에서의 출력을 남기면,

1061
00:48:18,857 --> 00:48:20,440
값이 매우 크면

1062
00:48:20,440 --> 00:48:22,232
표준 편차도 매우 커집니다.

1063
00:48:22,232 --> 00:48:26,520
따라서 하반부를 제거하면 평균이 점점 더

1064
00:48:26,520 --> 00:48:29,910
긍정적으로 이동하기 시작합니다.

1065
00:48:29,910 --> 00:48:31,380
이해가 되셨나요? 보여줄

1066
00:48:31,380 --> 00:48:33,147
슬라이드는 없었지만, 괜찮습니다.

1067
00:48:33,147 --> 00:48:33,730
대부분, 죄송합니다.

1068
00:48:33,730 --> 00:48:35,290
논문에서 더 많은 세부 사항을 볼 수 있습니다.

1069
00:48:35,290 --> 00:48:37,040
읽기에는 그렇게 나쁘지 않다고 생각합니다.

1070
00:48:40,950 --> 00:48:43,690
여기서 논의의 결론은 정규화가

1071
00:48:43,690 --> 00:48:47,297
폭발하는 활성화 문제를 해결할 수 있지만, 여전히

1072
00:48:47,297 --> 00:48:48,880
최적화하기 어려울

1073
00:48:48,880 --> 00:48:50,470
수 있다는 것입니다.

1074
00:48:50,470 --> 00:48:54,160
아마도 이 내용을 더 자세히 설명하는 후속 포스트를

1075
00:48:54,160 --> 00:48:55,948
해야 할 것 같은데, 사실

1076
00:48:55,948 --> 00:48:58,240
정말 좋은 질문이라고 생각합니다.

1077
00:48:58,240 --> 00:49:00,198
이 특정 문제를 해결할 수 있을

1078
00:49:00,198 --> 00:49:03,790
것 같지만, 논의에서 최적화하기는 여전히 어려울 수 있습니다.

1079
00:49:03,790 --> 00:49:06,125
네, 좋은 질문입니다.

1080
00:49:06,125 --> 00:49:06,625
좋아요.

1081
00:49:09,280 --> 00:49:10,150
멋지네요.

1082
00:49:10,150 --> 00:49:12,573
이제 이 단계들에 대해 이야기하겠습니다.

1083
00:49:12,573 --> 00:49:14,240
모델을 실제로 어떻게 훈련시키나요?

1084
00:49:14,240 --> 00:49:16,510
데이터 전처리의 좋은 점은 이미지에 대해

1085
00:49:16,510 --> 00:49:17,930
정말 쉽다는 것입니다.

1086
00:49:17,930 --> 00:49:19,970
거대한 이미지 데이터 세트가

1087
00:49:19,970 --> 00:49:23,480
있다면, 표준적인 방법은 평균 빨강, 평균 초록,

1088
00:49:23,480 --> 00:49:25,450
평균 파랑 픽셀과 표준

1089
00:49:25,450 --> 00:49:27,110
편차를 계산하는 것입니다.

1090
00:49:27,110 --> 00:49:30,100
입력 이미지를 가져와서 평균을

1091
00:49:30,100 --> 00:49:32,300
빼고 표준 편차로 나눕니다.

1092
00:49:32,300 --> 00:49:36,220
이것이 이미지에 대한 데이터 정규화 방법입니다.

1093
00:49:36,220 --> 00:49:38,500
사실 매우 간단합니다.

1094
00:49:38,500 --> 00:49:40,270
각 픽셀 채널에 대한

1095
00:49:40,270 --> 00:49:44,100
평균과 표준 편차를 미리 계산해야 합니다.

1096
00:49:44,100 --> 00:49:45,750
그래서 때때로 사람들이

1097
00:49:45,750 --> 00:49:48,490
이미 계산된 평균을 사용할 것입니다.

1098
00:49:48,490 --> 00:49:51,450
매우 일반적인 방법은 ImageNet의 평균과

1099
00:49:51,450 --> 00:49:54,715
표준 편차를 사용하여 입력 이미지에 적용하는 것입니다.

1100
00:49:54,715 --> 00:49:57,090
모델이 ImageNet에서 훈련되지

1101
00:49:57,090 --> 00:49:58,410
않더라도 말이죠.

1102
00:49:58,410 --> 00:50:05,080
이것은 매우 데이터 세트에 의존적이라는 점을 생각해야 합니다.

1103
00:50:05,080 --> 00:50:07,622
다양한 모델이 데이터 세트에 따라 다른 값을

1104
00:50:07,622 --> 00:50:10,080
사용할 수 있지만, 가장 일반적으로

1105
00:50:10,080 --> 00:50:11,940
사용되는 것은 ImageNet의

1106
00:50:11,940 --> 00:50:14,040
평균과 표준 편차를 사용하는 것입니다.

1107
00:50:14,040 --> 00:50:16,980
그래서 모든 입력 이미지에 대해 모델이 보기

1108
00:50:16,980 --> 00:50:18,815
전에 이 작업을 적용합니다.

1109
00:50:23,580 --> 00:50:25,650
그래서 네, 그 부분은 정말 빨랐습니다.

1110
00:50:25,650 --> 00:50:27,460
그리고 데이터 증강

1111
00:50:27,460 --> 00:50:31,170
측면에서, 수업 중에 누군가가 이미지에

1112
00:50:31,170 --> 00:50:34,350
노이즈를 추가하자는 제안을 했습니다.

1113
00:50:34,350 --> 00:50:35,350
정말 좋은 아이디어입니다.

1114
00:50:35,350 --> 00:50:37,020
여기에서 이미지에 노이즈를 추가하는 다양한 방법에

1115
00:50:37,020 --> 00:50:37,962
대해 이야기하겠습니다.

1116
00:50:37,962 --> 00:50:40,420
이것은 정규화에 도움이 되고 모델의

1117
00:50:40,420 --> 00:50:42,730
과적합을 방지하는 데 도움이 됩니다.

1118
00:50:42,730 --> 00:50:47,110
우리가 전에 이야기했듯이, 이것은 정규화와 관련된 일반적인 패턴으로,

1119
00:50:47,110 --> 00:50:49,270
훈련 시간 동안 어떤 종류의

1120
00:50:49,270 --> 00:50:50,810
무작위성을 추가합니다.

1121
00:50:50,810 --> 00:50:53,990
그리고 테스트 시간에는 그 무작위성을 평균화합니다.

1122
00:50:53,990 --> 00:50:56,690
때때로 이것은 근사적이지만, 예를 들어

1123
00:50:56,690 --> 00:51:00,070
드롭아웃의 경우, 훈련 시간 동안 50%의

1124
00:51:00,070 --> 00:51:02,930
활성화를 무작위로 드롭한다고 보았습니다.

1125
00:51:02,930 --> 00:51:05,470
그리고 테스트 시간에는 모든 활성화를

1126
00:51:05,470 --> 00:51:06,970
사용하지만, 이

1127
00:51:06,970 --> 00:51:09,490
드롭아웃 확률 p로 축소해야 합니다.

1128
00:51:09,490 --> 00:51:11,540
이것은 정말 일반적인 패턴입니다.

1129
00:51:11,540 --> 00:51:13,760
그리고 데이터 증강에도 사용됩니다.

1130
00:51:13,760 --> 00:51:21,010
이 실린더가 데이터 세트와 같다고 상상할 수

1131
00:51:21,010 --> 00:51:22,010
있습니다.

1132
00:51:22,010 --> 00:51:24,140
이미지와 레이블을 로드합니다.

1133
00:51:24,140 --> 00:51:26,800
고양이 레이블이 있고 데이터 세트에서

1134
00:51:26,800 --> 00:51:28,210
원본 이미지가 있습니다.

1135
00:51:28,210 --> 00:51:31,790
모델에 실제로 전달하기 전에, 현대

1136
00:51:31,790 --> 00:51:35,390
딥러닝에서는 컴퓨터 비전 모델 훈련을 위해 데이터

1137
00:51:35,390 --> 00:51:39,420
증강을 항상 사용하는 것이 매우 일반적이고

1138
00:51:39,420 --> 00:51:40,600
기본적입니다.

1139
00:51:40,600 --> 00:51:42,960
기본 아이디어는 이미지를 변형하여

1140
00:51:42,960 --> 00:51:45,660
다르게 보이게 하지만 여전히 카테고리

1141
00:51:45,660 --> 00:51:47,980
클래스를 인식할 수 있도록 만든 다음,

1142
00:51:47,980 --> 00:51:50,370
이를 모델에 전달하고 여기서 손실을

1143
00:51:50,370 --> 00:51:51,610
계산하는 것입니다.

1144
00:51:51,610 --> 00:51:53,100
이의 장점 중 하나는

1145
00:51:53,100 --> 00:51:55,660
데이터 세트의 크기를 효과적으로 늘릴 수

1146
00:51:55,660 --> 00:51:58,228
있다는 것입니다. 각 이미지를 여러 번

1147
00:51:58,228 --> 00:52:00,270
보는 대신, 서로 다른 변형의

1148
00:52:00,270 --> 00:52:02,820
이미지를 보게 되어 모두 같은 카테고리

1149
00:52:02,820 --> 00:52:04,300
레이블을 유지합니다.

1150
00:52:04,300 --> 00:52:06,450
따라서 기본적으로 더 많은 데이터를

1151
00:52:06,450 --> 00:52:09,310
얻을 수 있고, 따라서 일반화 능력이

1152
00:52:09,310 --> 00:52:12,937
향상되지만, 같은 예제를 반복해서 보지 않기 때문에 훈련

1153
00:52:12,937 --> 00:52:14,770
손실은 더 높아질 것입니다.

1154
00:52:14,770 --> 00:52:18,060
그래서 모델이 단순히 암기하는 것을 더 어렵게 만듭니다.

1155
00:52:18,060 --> 00:52:20,770
그렇다면 가중치 초기화가 적절한지 어떻게 알 수 있을까요?

1156
00:52:20,770 --> 00:52:23,320
이 경우에는 네트워크의 레이어 전반에

1157
00:52:23,320 --> 00:52:25,470
걸쳐 평균과 표준 편차가 상대적으로

1158
00:52:25,470 --> 00:52:28,290
일정하기 때문에 올바르다는 것을 알 수

1159
00:52:28,290 --> 00:52:28,990
있습니다.

1160
00:52:28,990 --> 00:52:30,330
그리고 이

1161
00:52:30,330 --> 00:52:34,570
경우에는 모드가 0으로 붕괴되는 것을 보았습니다.

1162
00:52:34,570 --> 00:52:38,470
이 경우에는 레이어 수를 늘리면서

1163
00:52:38,470 --> 00:52:40,660
무한대로 폭발했습니다.

1164
00:52:40,660 --> 00:52:43,510
항상 이러한 일이 발생하도록 보장하는 방법은 공식을

1165
00:52:43,510 --> 00:52:44,720
사용하는 것입니다.

1166
00:52:44,720 --> 00:52:46,870
이 공식은 항상 잘 초기화됩니다.

1167
00:52:46,870 --> 00:52:49,100
실제로 사람들이 이렇게 합니다.

1168
00:52:49,100 --> 00:52:52,690
아무도 해본 적이 없는 다른 종류의 작업을 수행하는

1169
00:52:52,690 --> 00:52:54,910
새로운 레이어를 만든다면, 여러

1170
00:52:54,910 --> 00:52:56,890
가지 다른 가중치 초기화 방식을

1171
00:52:56,890 --> 00:52:59,390
시도해보고 어떤 것이 가장 잘

1172
00:52:59,390 --> 00:53:01,390
작동하는지 확인해야 할 것입니다.

1173
00:53:01,390 --> 00:53:04,480
하지만 일반적으로 이러한 선형

1174
00:53:04,480 --> 00:53:09,530
레이어나 합성곱 레이어의 경우, 여기서 '타이밍

1175
00:53:09,530 --> 00:53:14,110
초기화'라고 불리는 이 공식을 사용할 수 있습니다.

1176
00:53:14,110 --> 00:53:14,980
좋습니다.

1177
00:53:14,980 --> 00:53:17,967
데이터 증강으로 돌아가서, 구체적으로 어떤

1178
00:53:17,967 --> 00:53:20,300
다양한 증강을 할 수 있을까요?

1179
00:53:20,300 --> 00:53:21,890
그 중 하나는 수평 뒤집기입니다.

1180
00:53:21,890 --> 00:53:23,710
이는 문제에 따라 다릅니다.

1181
00:53:23,710 --> 00:53:25,780
텍스트를 읽는 모델을 원한다면,

1182
00:53:25,780 --> 00:53:28,450
이는 매우 나쁜 증강이 될 것입니다.

1183
00:53:28,450 --> 00:53:30,250
왜냐하면 텍스트가 이제 거울을

1184
00:53:30,250 --> 00:53:31,750
통해 보는 것처럼 제대로

1185
00:53:31,750 --> 00:53:33,380
읽을 수 없기 때문입니다.

1186
00:53:33,380 --> 00:53:37,560
그래서 이는 일상적인 물체에 가끔 유용합니다.

1187
00:53:37,560 --> 00:53:40,100
대부분의 물체는 대칭적이기

1188
00:53:40,100 --> 00:53:44,330
때문에 이 속성이 실제로 잘 작동합니다.

1189
00:53:44,330 --> 00:53:46,640
그리고 현미경이나 위에서

1190
00:53:46,640 --> 00:53:48,907
본 이미지를 보고 있다면

1191
00:53:48,907 --> 00:53:50,490
수직 뒤집기도 가능할

1192
00:53:50,490 --> 00:53:51,573
것입니다.

1193
00:53:51,573 --> 00:53:53,990
하지만 일상적인 물체의 경우, 수직

1194
00:53:53,990 --> 00:53:56,240
뒤집기는 실제로 의미가 없습니다.

1195
00:53:56,240 --> 00:53:58,160
고양이는 거의 항상 이 위치에서

1196
00:53:58,160 --> 00:53:59,330
보이기 때문입니다.

1197
00:53:59,330 --> 00:54:00,440
그러나 고양이가

1198
00:54:00,440 --> 00:54:02,990
다양한 방향에 있는 데이터 세트가 있다면,

1199
00:54:02,990 --> 00:54:06,860
뒤집거나 회전하는 것이 데이터 세트에 맞을 수 있습니다.

1200
00:54:06,860 --> 00:54:11,310
또 다른 유형의 증강은 크기 조정 및 자르기 아이디어입니다.

1201
00:54:11,310 --> 00:54:16,790
ResNet과 많은 다른 이미지 모델이 하는

1202
00:54:16,790 --> 00:54:21,140
것은 기본적으로 이미지의 무작위 크롭을

1203
00:54:21,140 --> 00:54:25,400
가져와서 이를 이미지 크기로

1204
00:54:25,400 --> 00:54:27,105
조정하는 것입니다.

1205
00:54:27,105 --> 00:54:28,980
그들은 이후에 또 다른 크롭을 가져올 수도 있습니다.

1206
00:54:28,980 --> 00:54:30,740
가장 일반적인 전략은

1207
00:54:30,740 --> 00:54:33,540
이미지의 짧은 변의 길이를

1208
00:54:33,540 --> 00:54:35,490
선택하는 것입니다.

1209
00:54:35,490 --> 00:54:40,230
모델의 입력 이미지 크기가 224x224

1210
00:54:40,230 --> 00:54:43,980
픽셀이라면, 이보다 큰 값을 선택하고 이

1211
00:54:43,980 --> 00:54:51,150
더 큰 스케일 L을 포함하는 이미지의 크롭을 찾습니다. 이러한

1212
00:54:51,150 --> 00:54:53,950
값들이 일반적으로 사용됩니다.

1213
00:54:53,950 --> 00:54:55,063
이미지를 자릅니다.

1214
00:54:58,163 --> 00:54:59,080
죄송합니다, 자르지 않습니다.

1215
00:54:59,080 --> 00:55:01,960
이미지를 해당 스케일로 조정합니다.

1216
00:55:01,960 --> 00:55:04,870
예를 들어, 이것이 800x600 이미지라면,

1217
00:55:04,870 --> 00:55:10,350
여기서 256을 사용하면 짧은 변인 600을 256으로 조정하고, 800은

1218
00:55:10,350 --> 00:55:12,730
그에 맞게 비례적으로 조정됩니다.

1219
00:55:12,730 --> 00:55:15,990
따라서 이를 L로 조정합니다. 짧은 쪽을 L로 조정한 다음, 그

1220
00:55:15,990 --> 00:55:19,200
이미지에서 224x224 픽셀의

1221
00:55:19,200 --> 00:55:21,190
랜덤 패치를 잘라냅니다.

1222
00:55:21,190 --> 00:55:24,070
그래서 이미지를 조정할 때 먼저

1223
00:55:24,070 --> 00:55:26,350
상대 해상도를 유지하고, 이를

1224
00:55:26,350 --> 00:55:29,620
L에 맞게 작게 또는 크게 만든 다음

1225
00:55:29,620 --> 00:55:31,340
랜덤 크롭을 합니다.

1226
00:55:31,340 --> 00:55:35,073
이것은 대부분의 라이브러리에서 '랜덤 리사이즈 크롭'이라고 불리는

1227
00:55:35,073 --> 00:55:37,490
가장 일반적으로 사용되는 방법입니다.

1228
00:55:37,490 --> 00:55:40,032
이 방법은 대부분의 문제에서

1229
00:55:40,032 --> 00:55:44,902
사용되며, 이미지의 상대 해상도를 유지합니다.

1230
00:55:44,902 --> 00:55:46,360
그리고 테스트 타임 증강이라고

1231
00:55:46,360 --> 00:55:49,150
하는 증강 기법을 사용할 수 있는 또 다른 멋진 트릭이 있습니다.

1232
00:55:49,150 --> 00:55:51,400
최고의 성능을 얻고 싶다면,

1233
00:55:51,400 --> 00:55:54,520
다양한 크롭과 리사이즈를 여러 개 얻고,

1234
00:55:54,520 --> 00:55:56,860
이를 모델에 통과시킨 후 예측을

1235
00:55:56,860 --> 00:55:58,820
평균화할 수 있습니다.

1236
00:55:58,820 --> 00:56:01,000
ResNet의 경우,

1237
00:56:01,000 --> 00:56:04,240
사람들은 종종 다양한 스케일과 크롭

1238
00:56:04,240 --> 00:56:07,910
위치를 시도하고, 아마도 뒤집기도 합니다.

1239
00:56:07,910 --> 00:56:10,970
보통은 수익이 감소하기 시작하지만, 이 테스트

1240
00:56:10,970 --> 00:56:13,210
타임 증강을 사용하면 실제로 1%에서

1241
00:56:13,210 --> 00:56:15,890
2%의 성능 향상을 얻을 수 있습니다.

1242
00:56:15,890 --> 00:56:18,110
정말 중요한 상황이라면, 마지막

1243
00:56:18,110 --> 00:56:20,780
퍼센트 포인트를 끌어내려고 한다면, 이는

1244
00:56:20,780 --> 00:56:22,530
거의 모든 컴퓨터 비전 문제에

1245
00:56:22,530 --> 00:56:24,885
사용할 수 있는 훌륭한 트릭입니다.

1246
00:56:27,820 --> 00:56:31,200
좋아요, 마지막 몇 가지 증강입니다.

1247
00:56:31,200 --> 00:56:34,310
하나는 색상 변동으로, 여기서는 대비와

1248
00:56:34,310 --> 00:56:36,320
밝기를 랜덤화하고 이미지의

1249
00:56:36,320 --> 00:56:37,920
크기를 조정합니다.

1250
00:56:37,920 --> 00:56:40,850
그래서 아마도 이미지가 더 부드럽게 보이거나

1251
00:56:40,850 --> 00:56:43,830
색상이 더 부드럽거나 더 밝게 보일 수

1252
00:56:43,830 --> 00:56:47,250
있지만, 이는 매우 전통적인 이미지 처리 기법입니다.

1253
00:56:47,250 --> 00:56:49,860
이러한 다양한 증강을 통해 서로 다른

1254
00:56:49,860 --> 00:56:52,190
값을 시도하고, 어떤 값이 이미지가

1255
00:56:52,190 --> 00:56:55,133
여전히 분포에 맞고 인간에게 정상적으로

1256
00:56:55,133 --> 00:56:56,550
보이는지를 확인합니다.

1257
00:56:56,550 --> 00:56:58,302
이는 얼마나 많은 변동을 가져야

1258
00:56:58,302 --> 00:57:00,260
하는지, 얼마나 많은 밝기 변화를

1259
00:57:00,260 --> 00:57:02,910
가져야 하는지를 판단하는 좋은 방법입니다.

1260
00:57:02,910 --> 00:57:04,680
문제를 시작할 때, 저는 이러한

1261
00:57:04,680 --> 00:57:06,390
다양한 증강을 시도합니다.

1262
00:57:06,390 --> 00:57:08,090
데이터가 원본 데이터와

1263
00:57:08,090 --> 00:57:09,840
어떻게 다른지, 그러나 여전히

1264
00:57:09,840 --> 00:57:13,130
저에게 인식 가능하고 쉽게 인식할 수 있는지를

1265
00:57:13,130 --> 00:57:13,800
확인합니다.

1266
00:57:13,800 --> 00:57:18,080
이것은 일반적으로 사용할 수 있는 좋은 증강 세트입니다.

1267
00:57:18,080 --> 00:57:19,970
마지막으로, 이미지의 일부를

1268
00:57:19,970 --> 00:57:21,470
잘라내고 그 위에

1269
00:57:21,470 --> 00:57:22,970
검은색 또는 회색 상자를

1270
00:57:22,970 --> 00:57:25,130
놓는 것을 상상할 수 있습니다.

1271
00:57:25,130 --> 00:57:27,550
이것은 아마도 덜 일반적으로 사용되지만,

1272
00:57:27,550 --> 00:57:29,050
문제에 따라 증강을

1273
00:57:29,050 --> 00:57:30,520
창의적으로 사용할 수

1274
00:57:30,520 --> 00:57:32,090
있는 방법을 보여줍니다.

1275
00:57:32,090 --> 00:57:35,000
예를 들어, 카메라가 가려져서 물체를

1276
00:57:35,000 --> 00:57:38,403
완전히 볼 수 없는 상황이라면, 이는 모델이

1277
00:57:38,403 --> 00:57:40,570
물체의 일부가 가려지는 것에 더

1278
00:57:40,570 --> 00:57:42,612
강인해지도록 만드는 멋진

1279
00:57:42,612 --> 00:57:44,230
트릭이 될 수 있습니다.

1280
00:57:44,230 --> 00:57:46,880
주어진 상황에 대해 어떤 증강이 의미가 있을지

1281
00:57:46,880 --> 00:57:48,170
상상할 수 있습니다.

1282
00:57:48,170 --> 00:57:51,280
입력 데이터를 어떻게 변형하여 여전히

1283
00:57:51,280 --> 00:57:53,508
인간에게 인식 가능하게

1284
00:57:53,508 --> 00:57:56,050
만들면서 모델이 훈련 예제를

1285
00:57:56,050 --> 00:57:58,960
암기하기 어렵게 만들 수 있을까요?

1286
00:57:58,960 --> 00:58:02,500
좋아요, 여기서 마지막 주제 세트는

1287
00:58:02,500 --> 00:58:04,940
기본적으로 매우 실용적입니다.

1288
00:58:04,940 --> 00:58:06,940
프로젝트를 하거나 모델을

1289
00:58:06,940 --> 00:58:11,470
훈련할 때, 예를 들어 코스 프로젝트를 위해, 우리는

1290
00:58:11,470 --> 00:58:14,230
다음 슬라이드에서 설명할 정확한 것들을

1291
00:58:14,230 --> 00:58:16,463
해야 한다고 생각합니다.

1292
00:58:16,463 --> 00:58:18,130
하지만 이는 코스

1293
00:58:18,130 --> 00:58:21,440
외의 모든 컴퓨터 비전 분야에도 적용됩니다.

1294
00:58:21,440 --> 00:58:25,720
실제로 많은 경우 데이터가 많지

1295
00:58:25,720 --> 00:58:26,890
않습니다.

1296
00:58:26,890 --> 00:58:29,728
ImageNet의 원본 버전은 백만 개의 이미지가 있었습니다.

1297
00:58:29,728 --> 00:58:32,020
아마도 여러분의 문제에 대해 백만 개의 이미지가 없을

1298
00:58:32,020 --> 00:58:33,780
것입니다. 거의 모든 사람이 그렇지 않으며,

1299
00:58:33,780 --> 00:58:36,970
대규모 팀과 함께 방대한 양의 데이터를 수집하지 않는 한 그렇습니다.

1300
00:58:36,970 --> 00:58:40,380
데이터가 많지 않다면 CNN을 여전히 훈련할 수 있을까요?

1301
00:58:40,380 --> 00:58:42,370
짧은 대답은 예, 가능합니다.

1302
00:58:42,370 --> 00:58:45,870
하지만 어떻게 하는지 조금 똑똑해야 합니다.

1303
00:58:45,870 --> 00:58:48,183
지난 강의에서 CNN의

1304
00:58:48,183 --> 00:58:49,600
다양한 필터가

1305
00:58:49,600 --> 00:58:52,620
서로 다른 유형의 특징을

1306
00:58:52,620 --> 00:58:56,093
추출하는 방법을 보여주었습니다.

1307
00:58:56,093 --> 00:58:57,510
이는 합성곱 신경망에서

1308
00:58:57,510 --> 00:59:00,240
특징의 계층 구조에 대해 누군가가 질문한 것과 관련이

1309
00:59:00,240 --> 00:59:00,940
있습니다.

1310
00:59:00,940 --> 00:59:03,690
처음에는 주로 가장자리나 패턴

1311
00:59:03,690 --> 00:59:05,740
또는 매우 작은 형태입니다.

1312
00:59:05,740 --> 00:59:08,115
그리고 가장 높은 수준에서,

1313
00:59:08,115 --> 00:59:11,980
우리가 이미지를 CNN에 넣고 클래스 점수를

1314
00:59:11,980 --> 00:59:16,380
얻기 직전의 이 최종 벡터를 얻는다고 상상해

1315
00:59:16,380 --> 00:59:21,570
보세요. 그리고 그것을 데이터 세트의 다른 이미지와 비교하면,

1316
00:59:21,570 --> 00:59:25,530
실제로 이 이미지의 벡터 값들이 매우

1317
00:59:25,530 --> 00:59:27,910
가깝다는 것을 알 수 있습니다.

1318
00:59:27,910 --> 00:59:31,890
그래서 이것을 우리가 이전에 했던 최근접 이웃 개념으로

1319
00:59:31,890 --> 00:59:34,770
생각할 수 있지만, 여기서 우리가

1320
00:59:34,770 --> 00:59:38,370
보는 벡터는 CNN의 마지막 분류 레이어 바로

1321
00:59:38,370 --> 00:59:41,190
직전의 벡터입니다. 그래서 이것은

1322
00:59:41,190 --> 00:59:45,060
4,096 또는 2,048 레이어와 같은 것입니다.

1323
00:59:45,060 --> 00:59:49,590
그리고 여기서 차이를 보면, 그것은 L2 거리입니다.

1324
00:59:49,590 --> 00:59:52,330
주어진 이미지를 모델에 넣고

1325
00:59:52,330 --> 00:59:53,880
이 벡터

1326
00:59:53,880 --> 00:59:55,890
공간에서 모델과 가까운

1327
00:59:55,890 --> 00:59:57,990
다른 이미지를 살펴보면,

1328
00:59:57,990 --> 01:00:00,690
같은 범주에 있는 항목일

1329
01:00:00,690 --> 01:00:03,600
때 이미지들이 서로 매우

1330
01:00:03,600 --> 01:00:06,700
가깝다는 것을 알 수 있습니다.

1331
01:00:06,700 --> 01:00:08,760
직관적으로, 이것이 기본적으로

1332
01:00:08,760 --> 01:00:14,400
의미하는 것은 이 특징들이 실제로 매우 좋다는 것입니다. 이 위에

1333
01:00:14,400 --> 01:00:17,730
선형 분류기를 구축하거나 K 최근접 이웃

1334
01:00:17,730 --> 01:00:19,440
분류기를 사용할 수

1335
01:00:19,440 --> 01:00:23,050
있으며, 객체를 매우 잘 분류할 수 있습니다.

1336
01:00:23,050 --> 01:00:25,700
그렇다면 이것을 실제로 어떻게 사용할 수 있을까요?

1337
01:00:25,700 --> 01:00:28,480
당신이 할 일은 먼저 ImageNet에서

1338
01:00:28,480 --> 01:00:30,760
모델을 훈련시키거나, 다른 사람이

1339
01:00:30,760 --> 01:00:33,890
ImageNet에서 훈련한 모델이나 매우 큰

1340
01:00:33,890 --> 01:00:37,370
웹 인터넷 규모 데이터 세트를 가져오는 것입니다.

1341
01:00:37,370 --> 01:00:40,930
그리고 이 모든 레이어를 고정하여 훈련하지 않도록

1342
01:00:40,930 --> 01:00:42,350
할 수 있습니다.

1343
01:00:42,350 --> 01:00:45,760
이전과 정확히 동일하게 유지하고, ImageNet의

1344
01:00:45,760 --> 01:00:48,822
경우 1,000 클래스가 아닌 데이터

1345
01:00:48,822 --> 01:00:51,280
세트에 있는 클래스 수로 이 최종

1346
01:00:51,280 --> 01:00:52,520
레이어를 교체합니다.

1347
01:00:52,520 --> 01:00:54,320
그리고 모델을 훈련할 때, 여기

1348
01:00:54,320 --> 01:00:56,150
있는 이 레이어만 훈련합니다.

1349
01:00:56,150 --> 01:01:00,220
우리가 컴퓨터 비전의 구형 패러다임에서 특징 추출기가

1350
01:01:00,220 --> 01:01:02,385
있었던 것처럼, 미리

1351
01:01:02,385 --> 01:01:03,760
정의된 작업 집합을

1352
01:01:03,760 --> 01:01:05,830
통해 색상 히스토그램 및

1353
01:01:05,830 --> 01:01:07,720
기타 미리 정의된 특징을

1354
01:01:07,720 --> 01:01:10,570
얻는 것에 대해 이야기했었습니다.

1355
01:01:10,570 --> 01:01:13,820
고정된 모델이 이 작업을 수행한다고 거의 생각할 수 있습니다.

1356
01:01:13,820 --> 01:01:15,700
변경하지 않는 미리 정의된

1357
01:01:15,700 --> 01:01:17,390
특징 추출기로, 우리는 그것을

1358
01:01:17,390 --> 01:01:19,780
사용하여 특징을 계산하고 그 위에

1359
01:01:19,780 --> 01:01:20,975
모델을 훈련합니다.

1360
01:01:20,975 --> 01:01:23,100
실제로 이 패러다임 하에서는 매우 유사합니다.

1361
01:01:23,100 --> 01:01:25,150
왜냐하면 여기서 훈련하지 않기 때문입니다.

1362
01:01:25,150 --> 01:01:27,330
그리고 더 큰 데이터 세트가 있는

1363
01:01:27,330 --> 01:01:29,550
경우, 실제로 가장 잘 작동하는

1364
01:01:29,550 --> 01:01:32,290
것은 전체 모델을 훈련하는 것이지만,

1365
01:01:32,290 --> 01:01:35,400
ImageNet이나 다른 매우 큰 인터넷 규모

1366
01:01:35,400 --> 01:01:38,310
데이터 세트에서 미리 훈련된 값으로

1367
01:01:38,310 --> 01:01:39,400
초기화하는 것입니다.

1368
01:01:39,400 --> 01:01:43,110
그래서 제가 작업하는 모든 문제에 대해, 저는

1369
01:01:43,110 --> 01:01:48,150
아마도 100만 또는 1,000만 개의 훈련 예제를 가지고 있기

1370
01:01:48,150 --> 01:01:51,400
때문에 이 단계 3을 수행하고 있습니다.

1371
01:01:51,400 --> 01:01:54,180
그래서 저는 수십억 개의 데이터로 훈련된

1372
01:01:54,180 --> 01:01:55,870
모델로 시작하고, 그

1373
01:01:55,870 --> 01:01:59,400
다음 상대적으로 작은 데이터 세트에서 모델을 미세

1374
01:01:59,400 --> 01:02:00,340
조정합니다.

1375
01:02:00,340 --> 01:02:02,257
그리고 제가 스스로 모델을 훈련하려고 했을

1376
01:02:02,257 --> 01:02:03,640
때보다 더 나은 성능을 얻습니다.

1377
01:02:03,640 --> 01:02:05,710
모델이 기본적으로 더 많은 데이터를 보았기 때문입니다.

1378
01:02:05,710 --> 01:02:08,400
더 나은 특징 추출기를 생성했습니다.

1379
01:02:08,400 --> 01:02:10,150
그리고 전체를 미세 조정할 때,

1380
01:02:10,150 --> 01:02:12,370
여전히 제 문제에 충분히 구체적일 수 있습니다.

1381
01:02:12,370 --> 01:02:14,520
기본적으로, 예를 들어 ImageNet에서

1382
01:02:14,520 --> 01:02:16,020
모델을 훈련하는 매우 구체적인

1383
01:02:16,020 --> 01:02:18,060
사례를 사용한다고 가정해 보겠습니다.

1384
01:02:18,060 --> 01:02:21,370
우리는 이 모델을 가져와서 최종 레이어를 교체하여

1385
01:02:21,370 --> 01:02:25,000
더 이상 1,000 클래스를 출력하지 않도록 합니다.

1386
01:02:25,000 --> 01:02:28,670
데이터 세트의 클래스 수를 출력하도록 합니다.

1387
01:02:28,670 --> 01:02:31,870
그리고 우리는 이전에 이야기한 Kaiming

1388
01:02:31,870 --> 01:02:33,530
초기화를 사용하여

1389
01:02:33,530 --> 01:02:36,640
무작위로 초기화하고, 나머지 레이어는

1390
01:02:36,640 --> 01:02:38,420
이전의 값을 유지합니다.

1391
01:02:38,420 --> 01:02:40,190
그래서 우리는 이 값을 변경하지

1392
01:02:40,190 --> 01:02:41,590
않으며, 경량 하강 중에도 이

1393
01:02:41,590 --> 01:02:42,950
값을 변경하지 않습니다.

1394
01:02:42,950 --> 01:02:45,110
그래서 이 값들은 변경되지 않습니다.

1395
01:02:45,110 --> 01:02:46,490
우리는 기본적으로 이미지를 가져옵니다.

1396
01:02:46,490 --> 01:02:47,905
모델을 통해 전달합니다.

1397
01:02:47,905 --> 01:02:50,530
그리고 이제 우리는 마치 선형 분류기를

1398
01:02:50,530 --> 01:02:54,640
훈련하는 것과 같습니다. 입력은 우리가 전체 모델을

1399
01:02:54,640 --> 01:02:57,550
통과시켜 계산한 각 이미지에 대한 4,096

1400
01:02:57,550 --> 01:02:58,550
벡터입니다.

1401
01:02:58,550 --> 01:03:00,782
그런 다음 우리는 4,096의 벡터를 가지고 있습니다.

1402
01:03:00,782 --> 01:03:02,990
그리고 그것을 클래스 수에 매핑하고 있습니다.

1403
01:03:02,990 --> 01:03:05,680
우리는 이 매핑을 마지막에만 훈련하고 있습니다.

1404
01:03:05,680 --> 01:03:09,280
그래서 질문은, ImageNet에서 훈련되었기 때문에

1405
01:03:09,280 --> 01:03:10,880
모델에 편향이 생길까요?

1406
01:03:10,880 --> 01:03:12,140
답은 확실히 그렇습니다.

1407
01:03:12,140 --> 01:03:17,740
이 방법으로 훈련하면 모델은 ImageNet과

1408
01:03:17,740 --> 01:03:22,490
매우 유사한 데이터 세트에서 가장

1409
01:03:22,490 --> 01:03:24,270
잘 작동합니다.

1410
01:03:24,270 --> 01:03:27,230
이런 것들은 노트북이나 교실, 사람과

1411
01:03:27,230 --> 01:03:31,340
같은 일상적인 사물의 사진입니다. ImageNet은

1412
01:03:31,340 --> 01:03:32,640
일상적인 객체입니다.

1413
01:03:32,640 --> 01:03:35,850
하지만 만약 화성의 사진이라면 성능이 훨씬 떨어질 것입니다.

1414
01:03:35,850 --> 01:03:37,970
사전 훈련된 모델의 훈련 데이터에 따라

1415
01:03:37,970 --> 01:03:39,283
확실히 편향이 존재합니다.

1416
01:03:39,283 --> 01:03:40,700
같은 종류의

1417
01:03:40,700 --> 01:03:42,860
객체나 위치를 보는

1418
01:03:42,860 --> 01:03:45,830
같은 유형의 분포에서 무언가를

1419
01:03:45,830 --> 01:03:47,600
얻고 싶습니다.

1420
01:03:47,600 --> 01:03:50,390
그래서 질문은, 데이터 세트가 분포에서 벗어났을 때

1421
01:03:50,390 --> 01:03:51,650
어떻게 해야 할까요?

1422
01:03:51,650 --> 01:03:55,590
사실 이와 관련된 슬라이드가 있습니다.

1423
01:03:55,590 --> 01:03:57,050
정말 좋은 질문입니다.

1424
01:03:57,050 --> 01:04:00,685
매우 유사한 데이터 세트가 있지만 데이터가 적다면, 우리가 방금

1425
01:04:00,685 --> 01:04:02,060
언급한 선형 분류기 전략을

1426
01:04:02,060 --> 01:04:03,313
사용할 수 있습니다.

1427
01:04:03,313 --> 01:04:05,480
유사한 데이터 세트가 많다면, 모든 레이어를 미세

1428
01:04:05,480 --> 01:04:07,320
조정하여 최고의 성능을 얻을 수 있습니다.

1429
01:04:07,320 --> 01:04:08,737
이것은 제가 이전에 언급한

1430
01:04:08,737 --> 01:04:10,640
슬라이드의 전략 두와 세입니다.

1431
01:04:10,640 --> 01:04:12,932
그렇다면 매우 다른 데이터 세트가 있을 때는 어떻게 해야 할까요?

1432
01:04:12,932 --> 01:04:16,280
데이터가 많다면 아예 처음부터

1433
01:04:16,280 --> 01:04:20,810
시작하거나 초기화를 통해 더 나은 성능을 얻을 수

1434
01:04:20,810 --> 01:04:21,810
있습니다.

1435
01:04:21,810 --> 01:04:23,893
여기서는 테스트를 하겠지만 성능이 더

1436
01:04:23,893 --> 01:04:26,190
좋거나 나쁠 것이라는 보장은 없습니다.

1437
01:04:26,190 --> 01:04:29,430
그리고 데이터가 매우 적거나 매우 다른 데이터 세트가 있다면,

1438
01:04:29,430 --> 01:04:31,020
아마도 비슷한 것에

1439
01:04:31,020 --> 01:04:33,130
대해 훈련된 모델을 찾고 싶을 것입니다.

1440
01:04:33,130 --> 01:04:35,460
연구자들이 도메인 외 일반화에

1441
01:04:35,460 --> 01:04:38,410
대해 살펴본 특정 기술이 있으며,

1442
01:04:38,410 --> 01:04:41,420
하나의 도메인에서 모델을 훈련하고

1443
01:04:41,420 --> 01:04:43,170
새로운 도메인을 배우려는

1444
01:04:43,170 --> 01:04:45,430
기본 아이디어가 있습니다.

1445
01:04:45,430 --> 01:04:47,140
이것은 활발한 연구 분야이지만

1446
01:04:47,140 --> 01:04:50,100
항상 작동하는 일반적인 기술이 있다고는 말할 수 없으며,

1447
01:04:50,100 --> 01:04:52,690
그 설정에서는 문제에 따라 다소 의존적입니다.

1448
01:04:52,690 --> 01:04:56,150
반면에 이것은 오른쪽 상단 사분면을 제외한 모든 것에

1449
01:04:56,150 --> 01:04:56,650
해당합니다.

1450
01:04:56,650 --> 01:04:58,060
실제로 꽤 잘 작동합니다.

1451
01:04:58,060 --> 01:04:59,950
실제로 이를 위한 기술이 있습니다.

1452
01:04:59,950 --> 01:05:01,510
그리고 이는 꽤 활발한 연구 분야입니다.

1453
01:05:01,510 --> 01:05:02,890
특정 모델이 더 잘 일반화됩니다.

1454
01:05:02,890 --> 01:05:04,307
예를 들어, 언어 모델은

1455
01:05:04,307 --> 01:05:07,020
다양한 도메인을 배우는 데 꽤 좋습니다.

1456
01:05:07,020 --> 01:05:10,470
하지만 네, 완전히 다른 문제를 가지고 있고

1457
01:05:10,470 --> 01:05:12,892
데이터가 많지 않은 상황에 있는

1458
01:05:12,892 --> 01:05:14,350
것은 확실히 최악의

1459
01:05:14,350 --> 01:05:15,340
시나리오입니다.

1460
01:05:15,340 --> 01:05:17,890
그 설정에서 모델을 훈련하는 것이 가장 어렵습니다.

1461
01:05:17,890 --> 01:05:19,860
그래서 질문은, 마지막 레이어와 모든 레이어를

1462
01:05:19,860 --> 01:05:20,950
훈련하는 사이에 무언가를 하나요?

1463
01:05:20,950 --> 01:05:22,492
네, 실제로 사람들은

1464
01:05:22,492 --> 01:05:25,770
레이어의 일부를 훈련하는 데 많은 작업을 했습니다.

1465
01:05:25,770 --> 01:05:27,825
LoRA라는 기술도 있으며, 이는

1466
01:05:27,825 --> 01:05:29,950
변환기 강의에서 다룰 수 있습니다.

1467
01:05:29,950 --> 01:05:31,930
올해 다룰지는

1468
01:05:31,930 --> 01:05:34,152
확실하지 않지만,

1469
01:05:34,152 --> 01:05:36,360
기본 아이디어는 모든

1470
01:05:36,360 --> 01:05:41,820
레이어를 미세 조정하는 방식으로, 모든

1471
01:05:41,820 --> 01:05:44,340
값을 정확히 변경하지

1472
01:05:44,340 --> 01:05:47,160
않고, 원래 레이어 간의

1473
01:05:47,160 --> 01:05:49,830
차이를 미세 조정하는

1474
01:05:49,830 --> 01:05:51,040
것입니다.

1475
01:05:51,040 --> 01:05:55,410
그래서 네, LoRA를 사용할 수 있는 기술이 있습니다.

1476
01:05:55,410 --> 01:05:57,835
더 많은 설명이 필요하지만 기본

1477
01:05:57,835 --> 01:06:00,460
아이디어는 실제 값을 미세 조정하는

1478
01:06:00,460 --> 01:06:03,480
대신 ResNet처럼 값 레이어

1479
01:06:03,480 --> 01:06:06,143
간의 차이를 미세 조정하는 것입니다.

1480
01:06:06,143 --> 01:06:07,560
LoRA는 그런 방식이지만

1481
01:06:07,560 --> 01:06:09,690
매우 적은 수의 매개변수로 수행합니다.

1482
01:06:09,690 --> 01:06:12,000
질문은 기본적으로 몇 개의 레이어를

1483
01:06:12,000 --> 01:06:13,750
선택할지 어떻게 결정하는가입니다.

1484
01:06:13,750 --> 01:06:16,180
왜 많은 수의 레이어를 선택했나요?

1485
01:06:16,180 --> 01:06:18,720
특히 왜 각 크기마다 두 개의 컨볼루션

1486
01:06:18,720 --> 01:06:20,940
레이어가 있는지 하나가 아닌지요?

1487
01:06:20,940 --> 01:06:23,190
실제로 VGG와 비슷한 예시로,

1488
01:06:23,190 --> 01:06:26,130
3개의 3x3 컨볼루션이 있으면

1489
01:06:26,130 --> 01:06:30,600
7x7 컨볼루션과 같은 수용 영역을 가질 수 있지만, 7x7

1490
01:06:30,600 --> 01:06:32,640
필터에서 하나의 활성화

1491
01:06:32,640 --> 01:06:35,880
함수만 사용하는 것보다 세 개의 활성화

1492
01:06:35,880 --> 01:06:38,910
함수가 있어 더 많은 비선형 관계를

1493
01:06:38,910 --> 01:06:40,390
모델링할 수 있습니다.

1494
01:06:40,390 --> 01:06:42,820
기본적으로 3x3이 더 표현력이 뛰어나지만

1495
01:06:42,820 --> 01:06:44,970
충분한 값을 가지고 있는 한 동일한

1496
01:06:44,970 --> 01:06:46,510
값 집합을 보고 있습니다.

1497
01:06:46,510 --> 01:06:48,960
더 큰 집합의 작은 필터가 더 작은 집합의

1498
01:06:48,960 --> 01:06:51,455
큰 필터보다 더 표현력이 뛰어납니다.

1499
01:06:55,320 --> 01:06:57,523
좋아요, 계속 진행하겠습니다.

1500
01:06:57,523 --> 01:06:59,190
기본적으로 유사한 데이터를 가진 대규모

1501
01:06:59,190 --> 01:07:00,810
데이터 세트를 찾으려고 합니다.

1502
01:07:00,810 --> 01:07:02,310
그 데이터로 훈련된 모델을

1503
01:07:02,310 --> 01:07:04,800
가져와서 자신의 데이터로 미세 조정합니다.

1504
01:07:04,800 --> 01:07:06,180
몇 가지 좋은 링크입니다.

1505
01:07:06,180 --> 01:07:08,010
PyTorch 이미지 모델에는

1506
01:07:08,010 --> 01:07:10,380
ImageNet 및 기타 데이터 세트에서 훈련된

1507
01:07:10,380 --> 01:07:14,270
여러 모델이 있으며, PyTorch 버전의 GitHub 리포지토리도
있습니다.

1508
01:07:14,270 --> 01:07:15,260
여러분도 몇 가지를 찾을 수 있을 것입니다.

1509
01:07:15,260 --> 01:07:17,680
좋아요, 마지막에 하이퍼파라미터 선택에

1510
01:07:17,680 --> 01:07:19,190
대해 간단히 이야기하겠습니다.

1511
01:07:19,190 --> 01:07:22,630
모델 훈련에 어려움이 있고 즉시 작동하지

1512
01:07:22,630 --> 01:07:24,060
않는다면, 가장 좋은

1513
01:07:24,060 --> 01:07:27,140
방법은 작은 샘플에 과적합하는 것입니다.

1514
01:07:27,140 --> 01:07:29,650
이것은 딥러닝에서 기본 디버깅 전략으로, 하나의

1515
01:07:29,650 --> 01:07:31,960
데이터 포인트만 가지고 훈련 손실이

1516
01:07:31,960 --> 01:07:34,503
기본적으로 0으로 가는 것을 보고 싶어합니다.

1517
01:07:34,503 --> 01:07:36,920
모델은 하나의 훈련 예제를 기억할 수 있어야

1518
01:07:36,920 --> 01:07:38,560
하며, 그렇게 할 수 없다면 코드

1519
01:07:38,560 --> 01:07:41,380
어딘가에 버그가 있거나 문제를 모델링하기에 적합한

1520
01:07:41,380 --> 01:07:42,910
모델을 선택하지 않은 것입니다.

1521
01:07:42,910 --> 01:07:45,980
이것은 정말 좋은 훈련 문제입니다.

1522
01:07:45,980 --> 01:07:47,980
또한 어떤 학습률이 작동하는지 또는

1523
01:07:47,980 --> 01:07:49,250
작동하지 않는지를 알려줍니다.

1524
01:07:49,250 --> 01:07:50,890
그래서 탐색해야 할 학습률의 대략적인

1525
01:07:50,890 --> 01:07:51,975
범위를 알 수 있습니다.

1526
01:07:51,975 --> 01:07:54,350
이것은 모델이 올바른지 확인하고

1527
01:07:54,350 --> 01:07:57,197
학습률이 합리적인지 확인하며 코드에 영향을 줄

1528
01:07:57,197 --> 01:07:59,530
수 있는 다른 버그가 없는지

1529
01:07:59,530 --> 01:08:00,950
확인하는 좋은 방법입니다.

1530
01:08:00,950 --> 01:08:02,860
항상 첫 번째 단계입니다.

1531
01:08:02,860 --> 01:08:06,350
코드를 실행하는 데 문제가 있다면,

1532
01:08:06,350 --> 01:08:08,140
이렇게 디버깅합니다.

1533
01:08:08,140 --> 01:08:09,910
이것을 얻은 후 두 번째로 하고

1534
01:08:09,910 --> 01:08:12,178
싶은 것은 매우 거친 하이퍼파라미터 그리드를

1535
01:08:12,178 --> 01:08:12,970
시도하는 것입니다.

1536
01:08:12,970 --> 01:08:15,540
먼저 다양한 학습률로 시도하고, 다른

1537
01:08:15,540 --> 01:08:18,347
학습률로 훈련한 모델의 훈련 손실이 어떻게

1538
01:08:18,347 --> 01:08:19,930
보이는지 확인합니다.

1539
01:08:19,930 --> 01:08:23,279
훈련 손실이 한 에포크 동안 가장 지속적으로

1540
01:08:23,279 --> 01:08:25,689
감소하는 것을 원합니다.

1541
01:08:25,689 --> 01:08:29,069
이는 꽤 좋은 추정이지만 더 오랫동안 훈련할 수 있습니다.

1542
01:08:29,069 --> 01:08:30,939
좋은 학습률 집합을 얻으면 다른

1543
01:08:30,939 --> 01:08:33,279
하이퍼파라미터도 살펴볼 수 있습니다.

1544
01:08:33,279 --> 01:08:35,380
특히 손실 외에도 정확도

1545
01:08:35,380 --> 01:08:38,189
곡선도 살펴봐야 합니다.

1546
01:08:38,189 --> 01:08:39,660
훈련 정확도와

1547
01:08:39,660 --> 01:08:41,460
검증 정확도가 있습니다.

1548
01:08:41,460 --> 01:08:43,380
여전히 상승하고 있다면,

1549
01:08:43,380 --> 01:08:45,819
합리적으로 계속 훈련해야 한다는 의미이지만,

1550
01:08:45,819 --> 01:08:48,270
훈련 손실이 상승하고 검증 손실이

1551
01:08:48,270 --> 01:08:50,979
하락하는 시나리오가 있을 수 있습니다.

1552
01:08:50,979 --> 01:08:52,899
이것은 과적합입니다.

1553
01:08:52,899 --> 01:08:56,437
따라서 정규화를 늘리거나 더 많은 데이터를

1554
01:08:56,437 --> 01:08:58,479
얻을 수 있다면, 성능을

1555
01:08:58,479 --> 01:09:00,776
향상시키기 위해 두 가지

1556
01:09:00,776 --> 01:09:02,609
중 하나를 해야 합니다.

1557
01:09:02,609 --> 01:09:04,680
여기서 최고의 모델이

1558
01:09:04,680 --> 01:09:07,229
지금까지의 모델일 것입니다.

1559
01:09:07,229 --> 01:09:09,870
여기서 간격이 거의 없다면,

1560
01:09:09,870 --> 01:09:14,450
일반적으로 검증 손실이 최대화되는 지점에

1561
01:09:14,450 --> 01:09:18,680
도달하기 위해 더 오랫동안 훈련할 수

1562
01:09:18,680 --> 01:09:19,470
있습니다.

1563
01:09:19,470 --> 01:09:21,649
그래서 계속 훈련할 수 있다면 계속

1564
01:09:21,649 --> 01:09:22,649
훈련할 수 있습니다.

1565
01:09:22,649 --> 01:09:25,399
여기서 큰 간격이 없더라도,

1566
01:09:25,399 --> 01:09:31,085
검증 정확도가 훈련 정확도와 비슷하다면 훈련

1567
01:09:31,085 --> 01:09:32,460
정확도가 검증

1568
01:09:32,460 --> 01:09:35,029
정확도와 다르게 시작할

1569
01:09:35,029 --> 01:09:39,050
때까지 계속 훈련할 수 있습니다.

1570
01:09:39,050 --> 01:09:41,600
이 과정을 반복할 수

1571
01:09:41,600 --> 01:09:42,410
있습니다.

1572
01:09:42,410 --> 01:09:46,529
하이퍼파라미터 검색에 대한 마지막 메모는, 일반적으로 사람들이

1573
01:09:46,529 --> 01:09:49,160
두 개 이상의 하이퍼파라미터를 검색한다고

1574
01:09:49,160 --> 01:09:50,930
생각하는데, 모든 조합을

1575
01:09:50,930 --> 01:09:53,015
시도해야 할까요, 아니면 가장

1576
01:09:53,015 --> 01:09:54,390
좋은 방법이 무엇일까요?

1577
01:09:54,390 --> 01:09:57,620
실제로는 하이퍼파라미터 공간에서 무작위 검색이

1578
01:09:57,620 --> 01:10:00,470
미리 정의된 세트를 시도하는 그리드

1579
01:10:00,470 --> 01:10:03,810
검색보다 훨씬 더 잘 작동한다고 생각합니다.

1580
01:10:03,810 --> 01:10:05,780
그 이유는 주축이 중요하지

1581
01:10:05,780 --> 01:10:08,060
않은 하이퍼파라미터일 때, 값에

1582
01:10:08,060 --> 01:10:09,893
따라 성능이 대략 동일하게

1583
01:10:09,893 --> 01:10:11,602
유지되는 것을 상상할 수

1584
01:10:11,602 --> 01:10:12,740
있기 때문입니다.

1585
01:10:12,740 --> 01:10:14,700
이 모든 것에 대해 무작위 값을

1586
01:10:14,700 --> 01:10:16,910
사용하면 중요한 파라미터의

1587
01:10:16,910 --> 01:10:19,280
하이퍼파라미터 공간을 훨씬 더 철저히 검색할

1588
01:10:19,280 --> 01:10:23,000
수 있지만, 중요하지 않은 하이퍼파라미터에 대해 여러

1589
01:10:23,000 --> 01:10:25,010
값을 재확인하는 그리드 검색에

1590
01:10:25,010 --> 01:10:26,520
시간을 낭비하고 있습니다.

1591
01:10:26,520 --> 01:10:28,040
따라서 실제로는 시도하고

1592
01:10:28,040 --> 01:10:30,950
싶은 범위를 정의한 다음, 그 범위에서 값을

1593
01:10:30,950 --> 01:10:34,040
가진 하이퍼파라미터를 무작위로 수집해야 합니다.

1594
01:10:34,040 --> 01:10:35,790
그것이 아마도 가장 좋은 방법일 것입니다.

1595
01:10:35,790 --> 01:10:38,082
그리고 최고의 모델을 얻을 때까지 계속 실행합니다.

1596
01:10:38,082 --> 01:10:39,390
좋아요, 그게 전부입니다.

1597
01:10:39,390 --> 01:10:42,210
우리는 CNN의 레이어, 활성화 함수, CNN 아키텍처,

1598
01:10:42,210 --> 01:10:44,160
가중치 초기화, 이러한 모델을

1599
01:10:44,160 --> 01:10:46,710
어떻게 미리 정의하고 구축하는지, 그리고 실제로

1600
01:10:46,710 --> 01:10:48,960
어떻게 훈련하는지에 대해 이야기했습니다.

1601
01:10:48,960 --> 01:10:51,210
모델에 입력할 데이터를 어떻게 변경합니까?

1602
01:10:51,210 --> 01:10:52,640
어떻게 증강합니까?

1603
01:10:52,640 --> 01:10:54,320
전이 학습은 성능을 향상시키는

1604
01:10:54,320 --> 01:10:56,778
정말 멋진 트릭이며, 최고의 하이퍼파라미터를 어떻게

1605
01:10:56,778 --> 01:10:57,450
선택합니까?

1606
01:10:57,450 --> 01:10:59,610
그래서 오늘 강의에서 많은 내용을 다뤘습니다.

1607
01:10:59,610 --> 01:11:00,710
여러분 모두 정말 감사합니다.

1608
01:11:00,710 --> 01:11:02,260
네.
