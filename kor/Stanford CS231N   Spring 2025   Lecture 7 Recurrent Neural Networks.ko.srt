1
00:00:05,560 --> 00:00:07,020
안녕하세요 여러분.

2
00:00:07,020 --> 00:00:09,360
7강에 오신 것을 환영합니다.

3
00:00:09,360 --> 00:00:12,205
지난 시간의 몇 가지 설명을 다시 하고 싶습니다.

4
00:00:12,205 --> 00:00:13,580
지난 강의에서 제가

5
00:00:13,580 --> 00:00:17,360
좋다고 생각한 두 개의 게시물이 있었는데, 여러분이 확인하고

6
00:00:17,360 --> 00:00:18,780
싶어 할 것 같습니다.

7
00:00:18,780 --> 00:00:19,760
아직 보지 못하셨다면,

8
00:00:19,760 --> 00:00:21,260
제가 빠르게 설명하겠습니다.

9
00:00:21,260 --> 00:00:23,760
드롭아웃을 설명할 때, 테스트 시간에

10
00:00:23,760 --> 00:00:26,220
확률을 조정하는 방법에 대해

11
00:00:26,220 --> 00:00:28,520
약간 혼란이 있었던 것 같습니다.

12
00:00:28,520 --> 00:00:32,680
기본적으로, 슬라이드에서 제가 말한 내용이 일치하지 않았습니다.

13
00:00:32,680 --> 00:00:36,040
드롭아웃의 각 순전파에서 우리는 이

14
00:00:36,040 --> 00:00:38,960
하이퍼파라미터 p를 가지고 있는데,

15
00:00:38,960 --> 00:00:41,120
이는 드롭아웃하는 뉴런의

16
00:00:41,120 --> 00:00:43,080
양이거나 사용하는 드롭아웃

17
00:00:43,080 --> 00:00:45,320
구현에 따라 유지하는

18
00:00:45,320 --> 00:00:46,500
뉴런의 양입니다.

19
00:00:46,500 --> 00:00:47,620
일반적으로 그렇습니다.

20
00:00:47,620 --> 00:00:50,400
드롭아웃하는 뉴런의 수입니다.

21
00:00:50,400 --> 00:00:52,940
대부분의 라이브러리에서 p는 그렇게 의미합니다.

22
00:00:52,940 --> 00:00:54,860
하지만 기본 아이디어는 테스트

23
00:00:54,860 --> 00:00:58,480
시간에 기대 출력이 훈련 시간과 같아야 한다는 것입니다.

24
00:00:58,480 --> 00:01:03,020
즉, 훈련 시간에 25%의 활성화를 드롭아웃했다면,

25
00:01:03,020 --> 00:01:07,020
테스트 시간에는 0.75로 스케일링하여 기대

26
00:01:07,020 --> 00:01:09,068
출력이 같아야 합니다.

27
00:01:09,068 --> 00:01:10,860
그래서 이 슬라이드에서

28
00:01:10,860 --> 00:01:14,020
구현이 단위를 활성 상태로 유지하는 확률로

29
00:01:14,020 --> 00:01:17,840
p를 사용하기 때문에 약간의 혼란이 있었던 것 같습니다.

30
00:01:17,840 --> 00:01:20,200
조금의 불일치가 있었습니다.

31
00:01:20,200 --> 00:01:21,900
명확히 하기 위해서입니다.

32
00:01:21,900 --> 00:01:25,020
지난 수업에서 정규화가 어떻게 유용할 수 있는지에

33
00:01:25,020 --> 00:01:26,760
대한 질문이 있었습니다.

34
00:01:26,760 --> 00:01:28,220
그리고 잘못 초기화된

35
00:01:28,220 --> 00:01:31,280
가중치로 인해 발생하는 문제를 해결할 수 있습니다.

36
00:01:31,280 --> 00:01:34,340
하지만 우리는 모델에 2D 입력이 있고 ReLU를

37
00:01:34,340 --> 00:01:37,380
사용하는 2층 신경망이 있는 설정을 가지고 있습니다.

38
00:01:37,380 --> 00:01:40,160
기본적으로 이 사분면 함수를 출력합니다.

39
00:01:40,160 --> 00:01:43,580
그래서 점이 오른쪽 상단에 있으면, 점이 위치한

40
00:01:43,580 --> 00:01:47,500
사분면에 따라 1, 2, 3 또는 4를 출력합니다.

41
00:01:47,500 --> 00:01:51,300
우리는 지난 시간에 논의한 Kaiming 초기화를

42
00:01:51,300 --> 00:01:54,760
사용한 좋은 초기화와 표준 편차가 너무 높은 나쁜

43
00:01:54,760 --> 00:01:57,020
초기화에 대한 다양한 훈련 손실과

44
00:01:57,020 --> 00:01:59,280
테스트 손실을 플로팅합니다.

45
00:01:59,280 --> 00:02:02,400
여기에서 파란색 플롯은 나쁜 초기화를 나타내고, 초록색은

46
00:02:02,400 --> 00:02:05,200
LayerNorm을 사용한 나쁜 초기화를 나타냅니다.

47
00:02:05,200 --> 00:02:07,700
그래서 실제로 많은 문제를 해결하는 것을 볼 수 있습니다.

48
00:02:07,700 --> 00:02:09,120
하지만 최상의 성능을 얻으려면

49
00:02:09,120 --> 00:02:10,703
여전히 좋은 가중치 초기화가

50
00:02:10,703 --> 00:02:12,460
필요하며, 이는 이후의 두 선입니다.

51
00:02:12,460 --> 00:02:13,860
그래서 깊이 들어가 볼 수 있습니다.

52
00:02:13,860 --> 00:02:15,840
또한 LayerNorm이 도움이

53
00:02:15,840 --> 00:02:18,080
되는지는 문제에 따라 다릅니다.

54
00:02:18,080 --> 00:02:20,960
이 사분면 I에서는 각 점의 정확한 2D

55
00:02:20,960 --> 00:02:23,963
위치를 알 필요가 없다고 상상할 수 있습니다.

56
00:02:23,963 --> 00:02:25,380
그래서 LayerNorm이 실제로 도움이 되었습니다.

57
00:02:25,380 --> 00:02:28,100
하지만 코드에 있는 다른 함수들 중

58
00:02:28,100 --> 00:02:30,037
일부는 정확한 좌표를 알아야

59
00:02:30,037 --> 00:02:32,620
올바른 출력을 얻을 수 있으며,

60
00:02:32,620 --> 00:02:34,465
LayerNorm은 평균을

61
00:02:34,465 --> 00:02:35,840
빼고 표준 편차로

62
00:02:35,840 --> 00:02:38,060
나누는 과정에서 입력의 정확한

63
00:02:38,060 --> 00:02:39,935
공간 위치에 대한 정보를 잃기

64
00:02:39,935 --> 00:02:42,040
때문에 성능을 저하시킵니다.

65
00:02:42,040 --> 00:02:44,480
그래서 여기 몇 가지 메모가 있습니다.

66
00:02:44,480 --> 00:02:47,740
기본적으로 높은 수준에서 문제를 해결하는 데 도움이 되지만, 여전히

67
00:02:47,740 --> 00:02:48,873
간극이 존재합니다.

68
00:02:48,873 --> 00:02:51,040
그래서 단순한 정규화로는 이 가중치 초기화

69
00:02:51,040 --> 00:02:52,380
문제를 해결할 수 없습니다.

70
00:02:55,040 --> 00:02:57,260
그리고 제가 언급했듯이, 모델링하려는

71
00:02:57,260 --> 00:03:00,380
것에 따라 항상 의미가 있을지는 아닙니다.

72
00:03:00,380 --> 00:03:02,840
그래서 지난 시간에 대해 요약하자면,

73
00:03:02,840 --> 00:03:05,700
지금까지 주로 이러한 일반적인 비순환

74
00:03:05,700 --> 00:03:07,860
신경망에 대해 이야기해왔습니다.

75
00:03:07,860 --> 00:03:12,260
이것은 고정 크기 입력과 고정 크기 출력을 가지고 있습니다.

76
00:03:12,260 --> 00:03:15,020
여기에서 활성화 함수를 설정하는 한 번의

77
00:03:15,020 --> 00:03:15,880
설정이 있습니다.

78
00:03:15,880 --> 00:03:17,980
이미지와 이미지 채널에 대해

79
00:03:17,980 --> 00:03:21,540
고정된 평균과 표준 편차에 따라 데이터 전처리를

80
00:03:21,540 --> 00:03:22,580
수행합니다.

81
00:03:22,580 --> 00:03:25,140
사용하는 가중치 초기화

82
00:03:25,140 --> 00:03:27,660
및 정규화 함수와

83
00:03:27,660 --> 00:03:30,520
전이 학습이 있습니다.

84
00:03:30,520 --> 00:03:33,460
하나의 데이터 세트, 예를 들어 ImageNet이나 다른 대규모

85
00:03:33,460 --> 00:03:35,560
인터넷 데이터 세트에서 사전 훈련을 하면,

86
00:03:35,560 --> 00:03:37,935
가중치를 해당 값으로 초기화할 경우 더 나은 결과를

87
00:03:37,935 --> 00:03:38,740
얻을 수 있습니다.

88
00:03:38,740 --> 00:03:40,573
훈련 동역학에 대해서도

89
00:03:40,573 --> 00:03:42,740
이야기했으며, 좋은 학습률을

90
00:03:42,740 --> 00:03:44,340
선택하고 다양한

91
00:03:44,340 --> 00:03:46,560
하이퍼파라미터를 업데이트하는 방법,

92
00:03:46,560 --> 00:03:48,980
그리고 검증 성능 및 테스트

93
00:03:48,980 --> 00:03:51,860
시간 증강을 기반으로 최적화하는

94
00:03:51,860 --> 00:03:53,640
방법에 대해 논의했습니다.

95
00:03:53,640 --> 00:03:56,500
여기서 2번과 3번에 대한 정말 좋은 도구는 제가

96
00:03:56,500 --> 00:03:59,600
모든 프로젝트에서 사용하는 'weights and biases'라는

97
00:03:59,600 --> 00:04:00,540
것입니다.

98
00:04:00,540 --> 00:04:01,980
이것이 유용할 수 있습니다.

99
00:04:01,980 --> 00:04:05,280
다양한 하이퍼파라미터로 서로 다른 실행을

100
00:04:05,280 --> 00:04:07,760
설정할 수 있는 정말

101
00:04:07,760 --> 00:04:09,060
멋진 방법입니다.

102
00:04:09,060 --> 00:04:12,060
이 경우, 여기에서 드롭아웃 열을 보여줍니다.

103
00:04:12,060 --> 00:04:14,200
이것들은 드롭아웃의 다양한 값들입니다.

104
00:04:14,200 --> 00:04:15,700
색상 코딩이 정말 좋습니다.

105
00:04:15,700 --> 00:04:18,160
일반적으로 드롭아웃의 낮은 값이 더 높은

106
00:04:18,160 --> 00:04:20,100
정확도를 달성하는 것을 볼 수 있습니다.

107
00:04:20,100 --> 00:04:24,800
그래서 검증 세트 성능에 따라 이러한 다양한

108
00:04:24,800 --> 00:04:26,400
하이퍼파라미터를

109
00:04:26,400 --> 00:04:28,680
시각화할 수 있습니다.

110
00:04:28,680 --> 00:04:33,000
여러 번의 실행을 통해 어떤 하이퍼파라미터가 가장 잘

111
00:04:33,000 --> 00:04:35,445
작동하는지 알 수 있습니다.

112
00:04:35,445 --> 00:04:36,320
저는 항상 이것을 사용합니다.

113
00:04:36,320 --> 00:04:37,260
정말 좋다고 생각합니다.

114
00:04:37,260 --> 00:04:38,840
특히 성능을 더 개선하기 위해 반복적으로

115
00:04:38,840 --> 00:04:39,880
실행할 수 있는

116
00:04:39,880 --> 00:04:41,160
컴퓨팅 자원이 있다면, 이는

117
00:04:41,160 --> 00:04:42,500
시각화하는 정말 멋진 방법입니다.

118
00:04:42,500 --> 00:04:43,500
그들은 잘하고 있다고 생각합니다.

119
00:04:43,500 --> 00:04:45,360
TensorBoard와 같은 다른 도구들도 있습니다.

120
00:04:45,360 --> 00:04:49,640
하지만 개인적으로 제가 좋아하는 것은 이것입니다.

121
00:04:49,640 --> 00:04:52,700
좋습니다, 오늘 강의의 나머지 부분에서는

122
00:04:52,700 --> 00:04:55,140
시퀀스 모델링에 대해 논의할 것입니다.

123
00:04:55,140 --> 00:04:59,060
이것은 모델에 대한 고정 크기 입력과 대조적으로,

124
00:04:59,060 --> 00:05:00,860
가변 길이의

125
00:05:00,860 --> 00:05:03,100
시퀀스가 있을 경우입니다.

126
00:05:03,100 --> 00:05:05,380
또한 변환기 시대 이전에

127
00:05:05,380 --> 00:05:07,380
사람들이 사용하는 간단한

128
00:05:07,380 --> 00:05:09,880
신경망에 대해서도 논의할 것입니다.

129
00:05:09,880 --> 00:05:13,300
주로 RNN과 그 변형으로 구성됩니다.

130
00:05:13,300 --> 00:05:15,580
그리고 RNN이 실제로

131
00:05:15,580 --> 00:05:20,620
많은 현대 언어 모델, 즉 상태 공간 모델에 어떻게 유사하고

132
00:05:20,620 --> 00:05:23,060
영감을 주는지를 한 슬라이드에서

133
00:05:23,060 --> 00:05:24,785
설명하겠습니다.

134
00:05:24,785 --> 00:05:26,160
Mamba에 대해 들어본 적이 있을 것입니다.

135
00:05:26,160 --> 00:05:28,785
슬라이드에서 이야기할 다른 것들도 있지만, 기본

136
00:05:28,785 --> 00:05:31,500
아이디어는 RNN의 핵심 개념이 오늘날에도 여전히

137
00:05:31,500 --> 00:05:32,760
사용되고 있다는 것입니다.

138
00:05:32,760 --> 00:05:34,437
그들은 과거에만 사용된

139
00:05:34,437 --> 00:05:36,020
것이 아니며,

140
00:05:36,020 --> 00:05:39,500
변환기보다 많은 장점을 가지고 있습니다.

141
00:05:39,500 --> 00:05:43,100
좋습니다, 이 시퀀스 모델링 작업을 구체적으로 공식화하기

142
00:05:43,100 --> 00:05:45,780
위해, 우리는 고정 크기 입력에서 고정 크기

143
00:05:45,780 --> 00:05:48,700
출력으로 가는 일반적인 신경망을 상상할 수

144
00:05:48,700 --> 00:05:51,680
있습니다. 이는 지금까지 강의에서 논의한 내용입니다.

145
00:05:51,680 --> 00:05:55,760
대조적으로, 하나의 입력에 대해 여러 개의 출력을 생성하는 시퀀스 모델링
작업을 가질 수

146
00:05:55,760 --> 00:05:56,260
있습니다.

147
00:05:56,260 --> 00:05:59,300
여기서 우리는 여전히 이미지와 같은 고정 크기 입력을 가지고

148
00:05:59,300 --> 00:06:01,780
있지만, 가변 길이의 시퀀스를 출력하고자 합니다.

149
00:06:01,780 --> 00:06:04,180
일반적인 예로는 이미지 캡셔닝이 있습니다.

150
00:06:04,180 --> 00:06:06,680
이미지를 입력하고 언어를 모델링하거나

151
00:06:06,680 --> 00:06:08,480
인코딩하는 방식에

152
00:06:08,480 --> 00:06:11,980
따라 단어 또는 문자 시퀀스를 출력하고자 하며,

153
00:06:11,980 --> 00:06:14,360
목표는 이미지에서 발생하는 상황에

154
00:06:14,360 --> 00:06:17,800
대한 가변 길이의 캡션 출력을 갖는 것입니다.

155
00:06:17,800 --> 00:06:21,200
여러 개의 입력에 대해 하나의 출력을 생성하는 시퀀스 모델링 작업도
가능합니다.

156
00:06:21,200 --> 00:06:25,400
여기서 우리는 입력이 비디오라고 가정하고, 이 비디오가

157
00:06:25,400 --> 00:06:27,960
무엇인지 분류하려고 합니다.

158
00:06:27,960 --> 00:06:30,500
비디오 프레임의 시퀀스를 제공합니다.

159
00:06:30,500 --> 00:06:33,800
출력은 이미지 분류 사례와 유사하게 하나의 클래스

160
00:06:33,800 --> 00:06:34,853
레이블입니다.

161
00:06:34,853 --> 00:06:36,520
하지만 이제는 단일 이미지가

162
00:06:36,520 --> 00:06:38,600
아닌 여러 프레임을 입력으로 사용합니다.

163
00:06:38,600 --> 00:06:41,200
이것은 많은 입력에 대해 하나의 출력을 생성하는 예입니다.

164
00:06:41,200 --> 00:06:42,940
그리고 또한 많은 입력에 대해 많은 출력을 생성하는 경우도 있습니다.

165
00:06:42,940 --> 00:06:48,860
시퀀스의 입력과 출력 수가 일치할 필요는 없으므로, 가변 개수의

166
00:06:48,860 --> 00:06:51,300
프레임을 입력으로 사용하고, 이

167
00:06:51,300 --> 00:06:54,617
경우 출력은 가변 길이의 캡션이 될 수

168
00:06:54,617 --> 00:06:56,200
있으며, 반드시

169
00:06:56,200 --> 00:06:58,380
일치할 필요는 없지만 일치할

170
00:06:58,380 --> 00:06:59,760
수도 있습니다.

171
00:06:59,760 --> 00:07:02,540
모든 입력에 대해 하나의 출력이 있습니다.

172
00:07:02,540 --> 00:07:04,460
RNN에 대해 논의할 때 우리는 주로

173
00:07:04,460 --> 00:07:06,400
오른쪽 끝의 설정에 집중할 것입니다.

174
00:07:06,400 --> 00:07:08,740
하지만 문제를 다른 설정에 적용하기

175
00:07:08,740 --> 00:07:12,180
위해 변경하고 재구성할 수 있는 작은 변화가 많이

176
00:07:12,180 --> 00:07:14,847
있지만, 이것이 가장 직관적인 것입니다.

177
00:07:14,847 --> 00:07:16,805
입력이 있을 때마다 출력이

178
00:07:16,805 --> 00:07:18,980
있으며, 수업 시작 부분에서 RNN이

179
00:07:18,980 --> 00:07:22,460
어떻게 작동하는지 이야기하는 데 사용할 것입니다.

180
00:07:22,460 --> 00:07:24,260
여기서 전형적인 예제

181
00:07:24,260 --> 00:07:26,052
문제는 비디오 분류로,

182
00:07:26,052 --> 00:07:28,820
모든 프레임을 분류하는 것입니다.

183
00:07:28,820 --> 00:07:31,060
좋습니다, RNN이란 무엇인가요?

184
00:07:31,060 --> 00:07:36,020
기본 아이디어는 입력 시퀀스 x와 출력 시퀀스 y가

185
00:07:36,020 --> 00:07:37,340
있다는 것입니다.

186
00:07:37,340 --> 00:07:41,180
RNN을 RNN으로 만드는 것은 이 순환적 특성입니다.

187
00:07:41,180 --> 00:07:44,020
사람들은 종종 이 블록으로 피드백되는

188
00:07:44,020 --> 00:07:46,180
화살표로 이를 도식화합니다.

189
00:07:46,180 --> 00:07:48,490
다양한 다이어그램을 읽을 때 이것이 순환

190
00:07:48,490 --> 00:07:50,190
레이어임을 알 수 있는 방법입니다.

191
00:07:50,190 --> 00:07:52,530
하지만 실제로 의미하는 것은 RNN이

192
00:07:52,530 --> 00:07:55,510
시퀀스가 처리될 때 업데이트되는 내부 상태

193
00:07:55,510 --> 00:07:57,570
또는 종종 '숨겨진 상태'를

194
00:07:57,570 --> 00:07:59,190
가지고 있다는 것입니다.

195
00:07:59,190 --> 00:08:01,910
모델에 새로운 입력이 있을 때마다 이를

196
00:08:01,910 --> 00:08:04,570
처리하고 새로운 숨겨진 상태 또는 내부

197
00:08:04,570 --> 00:08:05,910
상태를 계산합니다.

198
00:08:05,910 --> 00:08:07,790
숨겨진 상태가 업데이트되며,

199
00:08:07,790 --> 00:08:11,170
이는 새로운 입력과 이전 내부

200
00:08:11,170 --> 00:08:14,050
또는 숨겨진 상태에 따라 달라집니다.

201
00:08:14,050 --> 00:08:16,118
이 다이어그램은 기울기가 실제로

202
00:08:16,118 --> 00:08:18,410
어떻게 계산되는지와 연산 순서에 대해 생각할

203
00:08:18,410 --> 00:08:21,190
때 때때로 혼란스러울 수 있다고 생각합니다.

204
00:08:21,190 --> 00:08:25,850
사람들은 종종 펼쳐진 RNN의 다이어그램을 그립니다.

205
00:08:25,850 --> 00:08:28,850
여기서 기본적으로 이전과 동일하지만,

206
00:08:28,850 --> 00:08:32,370
현재 숨겨진 상태 계산이 해당 시간

207
00:08:32,370 --> 00:08:35,370
단계의 입력과 이전 RNN 상태에

208
00:08:35,370 --> 00:08:38,669
의존한다는 것을 명시적으로 보여줍니다.

209
00:08:38,669 --> 00:08:41,409
각 출력을 계산하는 데 정확히

210
00:08:41,409 --> 00:08:43,890
필요한 것을 더 명시적으로 모델링하고

211
00:08:43,890 --> 00:08:48,830
있으며, 각 RNN은 계산 그래프에서 뒤로 이동합니다.

212
00:08:48,830 --> 00:08:51,330
지금까지 단어로 이야기해왔습니다.

213
00:08:51,330 --> 00:08:53,990
이제 수학적 방정식으로 이를 정리해 보겠습니다.

214
00:08:53,990 --> 00:08:57,670
기본 아이디어는 벡터 시퀀스 x를 처리하려고

215
00:08:57,670 --> 00:08:59,230
한다는 것입니다.

216
00:08:59,230 --> 00:09:01,430
우리는 이 재귀 공식을 매 시간

217
00:09:01,430 --> 00:09:03,310
단계마다 적용하고 있습니다.

218
00:09:03,310 --> 00:09:06,910
따라서 우리는 이전 숨겨진 상태와 특정 시간

219
00:09:06,910 --> 00:09:10,670
단계의 입력 벡터의 함수로서 새로운 숨겨진 상태를

220
00:09:10,670 --> 00:09:14,350
가지고 있으며, 일반적으로 활성화 함수와

221
00:09:14,350 --> 00:09:15,910
일부 매개변수 W가

222
00:09:15,910 --> 00:09:17,630
있는 함수도 있습니다. 이것은 우리가 배우고

223
00:09:17,630 --> 00:09:21,110
있는 초기 신경망 레이어와 매우 유사하게

224
00:09:21,110 --> 00:09:23,750
생각할 수 있으며, 가중치

225
00:09:23,750 --> 00:09:27,650
행렬을 곱한 다음 활성화 함수로 이어집니다.

226
00:09:27,650 --> 00:09:29,070
여기서도 같은 원리입니다.

227
00:09:29,070 --> 00:09:31,970
유일한 변화는 이제 재귀 공식이라는 것입니다.

228
00:09:31,970 --> 00:09:39,150
우리는 숨겨진 상태를 계산할 때마다 같은 W 집합과 같은

229
00:09:39,150 --> 00:09:43,410
활성화 함수를 사용하고 있습니다.

230
00:09:43,410 --> 00:09:48,450
기본적으로 제가 언급한 바와 같이, 이것은 재귀 공식입니다.

231
00:09:48,450 --> 00:09:50,370
실제 출력을 얻기 위해, 이 파란색

232
00:09:50,370 --> 00:09:51,710
블록을 어떻게 계산합니까?

233
00:09:51,710 --> 00:09:55,370
우리는 숨겨진 차원 상태를 출력

234
00:09:55,370 --> 00:09:57,850
차원으로 변환하는 별도의

235
00:09:57,850 --> 00:10:02,810
매개변수 집합에 의존하는 별도의 함수를 가지고

236
00:10:02,810 --> 00:10:03,550
있습니다.

237
00:10:03,550 --> 00:10:06,410
또한 숨겨진 상태를 출력으로 변환하는 가중치

238
00:10:06,410 --> 00:10:07,350
집합도 있습니다.

239
00:10:07,350 --> 00:10:08,710
따라서 이것은 두 가지 일을 합니다.

240
00:10:08,710 --> 00:10:10,530
우리의 벡터 차원을

241
00:10:10,530 --> 00:10:12,613
숨겨진 상태의 차원 크기에서

242
00:10:12,613 --> 00:10:15,150
출력의 차원 크기로 변경합니다.

243
00:10:15,150 --> 00:10:18,330
그리고 또한 그곳에서 변환을 제공합니다.

244
00:10:18,330 --> 00:10:23,050
W hy는 숨겨진 상태에 곱하여 결과를 얻는 가중치 행렬입니다.

245
00:10:23,050 --> 00:10:24,910
즉, 두 가지

246
00:10:24,910 --> 00:10:25,847
일을 합니다.

247
00:10:25,847 --> 00:10:28,430
숨겨진 상태를 출력의 차원으로 변환합니다.

248
00:10:28,430 --> 00:10:31,013
따라서 숨겨진 상태와 출력은 서로 다른 차원을 가질 수 있습니다.

249
00:10:31,013 --> 00:10:35,070
그리고 또한 배우는 가중치 행렬입니다.

250
00:10:35,070 --> 00:10:36,950
따라서 이 차원 변경을 수행할

251
00:10:36,950 --> 00:10:38,930
뿐만 아니라 숨겨진 상태에

252
00:10:38,930 --> 00:10:40,120
변환을 적용합니다.

253
00:10:40,120 --> 00:10:41,870
그래서 W hy는

254
00:10:41,870 --> 00:10:45,390
숨겨진 상태를 출력으로 변환하는 방법입니다.

255
00:10:45,390 --> 00:10:48,870
이전 슬라이드는 새로운 숨겨진 상태를 계산하는 방법이었습니다.

256
00:10:48,870 --> 00:10:51,190
본질적으로 동일한 아이디어로, 같은

257
00:10:51,190 --> 00:10:53,493
매개변수 집합으로 재귀적으로 수행하고 있지만,

258
00:10:53,493 --> 00:10:54,910
숨겨진 상태를 계산하기

259
00:10:54,910 --> 00:10:56,830
위한 매개변수 집합과 함수가

260
00:10:56,830 --> 00:10:57,620
하나 있습니다.

261
00:10:57,620 --> 00:10:59,870
출력을 계산하기 위한 매개변수

262
00:10:59,870 --> 00:11:02,430
집합과 함수가 또 하나 있으며,

263
00:11:02,430 --> 00:11:06,590
이는 작업 유형과 RNN 모델링 방식에 따라 다릅니다.

264
00:11:06,590 --> 00:11:09,867
네, 그래서 여전히 각 시간 단계마다 같은 가중치를 공유하지만,

265
00:11:09,867 --> 00:11:11,450
여기에는 두 가지 다른 것이 있습니다.

266
00:11:11,450 --> 00:11:13,710
하나는 기본적으로 어떻게 계산하는지입니다.

267
00:11:13,710 --> 00:11:16,210
아마도 더 구체적인 예를 통해 더 명확해질 것입니다.

268
00:11:16,210 --> 00:11:19,010
하지만 새로운 숨겨진 상태를 실제로 어떻게 계산하는지,

269
00:11:19,010 --> 00:11:20,755
즉 RNN의 내부 상태입니다.

270
00:11:20,755 --> 00:11:22,630
그리고 그 숨겨진 상태를

271
00:11:22,630 --> 00:11:26,390
출력으로 변환하는 방법입니다. 이것이 이 슬라이드입니다.

272
00:11:26,390 --> 00:11:31,830
이 펼쳐진 다이어그램을 살펴보면, 숨겨진 상태를

273
00:11:31,830 --> 00:11:35,670
어떤 값으로 초기화해야 하는지 알 수

274
00:11:35,670 --> 00:11:36,730
있습니다.

275
00:11:36,730 --> 00:11:40,890
우리는 보통 이것을 h,0이라고 부르며 원하는 값으로 초기화할

276
00:11:40,890 --> 00:11:42,070
수 있습니다.

277
00:11:42,070 --> 00:11:47,170
원칙적으로, 보통 이것은 학습된 입력 벡터이지만, 이제

278
00:11:47,170 --> 00:11:50,250
우리는 이 펼쳐진 재귀 RNN의 각

279
00:11:50,250 --> 00:11:53,195
단계로 구체적인 예를 통해 실제로

280
00:11:53,195 --> 00:11:55,570
전방 패스를 수행하는 것이

281
00:11:55,570 --> 00:11:58,170
어떤 모습인지 살펴보겠습니다.

282
00:11:58,170 --> 00:12:00,730
이미 몇 가지 질문에서 언급된 바와

283
00:12:00,730 --> 00:12:02,890
같이, 우리는 벡터 x의

284
00:12:02,890 --> 00:12:05,110
시퀀스를 처리하고 있으며, 각 시간

285
00:12:05,110 --> 00:12:07,610
단계에서 이 재귀 공식을 적용하고

286
00:12:07,610 --> 00:12:08,410
있습니다.

287
00:12:08,410 --> 00:12:12,142
숨겨진 상태를 계산할 때마다 같은 함수와 같은 매개변수

288
00:12:12,142 --> 00:12:13,850
집합이 사용되는 것을

289
00:12:13,850 --> 00:12:15,397
정말로 알 수 있습니다.

290
00:12:15,397 --> 00:12:17,730
그리고 숨겨진 상태에서 출력을

291
00:12:17,730 --> 00:12:19,290
예측할 때마다 항상

292
00:12:19,290 --> 00:12:22,850
별도의 함수와 별도의 매개변수 집합이 사용됩니다.

293
00:12:22,850 --> 00:12:25,850
네, 그래서 이전 y 값이 새로운 은닉 상태에 영향을 줄 수 있나요?

294
00:12:25,850 --> 00:12:27,088
어떤 공식화에서는 그렇습니다.

295
00:12:27,088 --> 00:12:29,630
그리고 왜 그렇게 사용되는지에 대한 예제를 하나 살펴보겠습니다.

296
00:12:29,630 --> 00:12:33,770
가장 일반적으로 사용되는 경우는 다음 값을 예측하고자 할 때입니다.

297
00:12:33,770 --> 00:12:36,055
언어 모델링이나 자기 회귀 모델링

298
00:12:36,055 --> 00:12:37,430
작업을 수행할 때 이전

299
00:12:37,430 --> 00:12:40,030
값을 기반으로 하나의 값을 예측하려고

300
00:12:40,030 --> 00:12:43,055
할 때, 사람들은 이전 값을 입력으로 사용합니다.

301
00:12:43,055 --> 00:12:44,430
그래서 일반적으로 사람들이

302
00:12:44,430 --> 00:12:47,150
y가 다음 은닉 상태에 어떻게 영향을 미칠 수

303
00:12:47,150 --> 00:12:49,430
있는지에 대한 명시적 공식을 이렇게 사용합니다.

304
00:12:49,430 --> 00:12:53,390
첫 번째 시간 단계에서 h와 x의 차이는 무엇인가요?

305
00:12:53,390 --> 00:12:57,190
기본적으로 서로 다른 가중치를 사용합니다.

306
00:12:57,190 --> 00:13:02,190
h,0은 모든 은닉 상태를 다음 상태로 업데이트하는

307
00:13:02,190 --> 00:13:06,243
데 사용되는 가중치를 사용하고 있습니다.

308
00:13:06,243 --> 00:13:08,410
가중치가 어떻게 생겼는지 정확히 살펴보겠지만,

309
00:13:08,410 --> 00:13:10,327
기본적으로 서로 다른 가중치를

310
00:13:10,327 --> 00:13:12,290
사용하고 있다는 것이 짧은 대답입니다.

311
00:13:12,290 --> 00:13:15,270
사람들이 바닐라 RNN이라고

312
00:13:15,270 --> 00:13:18,470
말할 때, 일반적으로 tanh 또는

313
00:13:18,470 --> 00:13:21,990
쌍곡선 탄젠트를 활성화 함수로 사용하는

314
00:13:21,990 --> 00:13:26,550
이 유형의 모델을 거의 정확하게 지칭합니다.

315
00:13:26,550 --> 00:13:28,770
이것은 1과 -1 사이로 제한되기

316
00:13:28,770 --> 00:13:30,070
때문에 좋습니다.

317
00:13:30,070 --> 00:13:32,930
그래서 이 작업을 반복할수록 값이

318
00:13:32,930 --> 00:13:35,410
이 범위 내에 유지됩니다.

319
00:13:35,410 --> 00:13:36,870
그래서 이것은 좋은 속성입니다.

320
00:13:36,870 --> 00:13:38,090
또한 0을 중심으로 하고

321
00:13:38,090 --> 00:13:40,030
양수와 음수를 모두 표현할 수 있습니다.

322
00:13:40,030 --> 00:13:43,410
이것이 사람들이 tanh를 사용하는 이유입니다.

323
00:13:43,410 --> 00:13:48,430
또한 여기서 출력 함수 fy가 있을 수 있지만, 가장 간단한

324
00:13:48,430 --> 00:13:50,890
경우에는 출력 yt가 은닉

325
00:13:50,890 --> 00:13:53,830
상태와 곱해진 행렬일 수 있습니다.

326
00:13:53,830 --> 00:13:57,410
그래서 이것이 RNN의 가장 간단한 공식입니다.

327
00:13:57,410 --> 00:14:00,530
오늘 강의에서 구체적인 예제로

328
00:14:00,530 --> 00:14:05,490
다룰 것은 수동으로 순환 신경망을 만드는

329
00:14:05,490 --> 00:14:06,983
아이디어입니다.

330
00:14:06,983 --> 00:14:09,650
그래디언트 하강법이나 다른 방법을 통해 배우지

331
00:14:09,650 --> 00:14:10,750
않을 것입니다.

332
00:14:10,750 --> 00:14:15,030
어떻게 수작업으로 하나를 구성할 수 있는지 보여드리겠습니다.

333
00:14:15,030 --> 00:14:16,490
그리고 그것을 살펴보면서

334
00:14:16,490 --> 00:14:18,010
순전파가 어떻게 이루어지는지,

335
00:14:18,010 --> 00:14:20,550
각 가중치 행렬이 무엇을 하는지,

336
00:14:20,550 --> 00:14:23,610
출력이 어떻게 계산되는지 이해할 수 있을 것입니다.

337
00:14:23,610 --> 00:14:25,290
이 간단한 예제에서는 모든

338
00:14:25,290 --> 00:14:26,450
가중치를 살펴보아야

339
00:14:26,450 --> 00:14:28,690
하므로 꽤 단순해야 합니다.

340
00:14:28,690 --> 00:14:32,330
0과 1의 시퀀스가 주어지고, 목표는 두

341
00:14:32,330 --> 00:14:36,730
개의 연속된 1이 있을 때 1을 출력하는 것입니다.

342
00:14:36,730 --> 00:14:39,750
기본적으로 반복된 1을 감지하고, 그렇지

343
00:14:39,750 --> 00:14:41,250
않으면 0을 출력합니다.

344
00:14:41,250 --> 00:14:45,430
입력 시퀀스가 0, 1, 0, 1로 들어오는 것을 볼 수 있습니다.

345
00:14:45,430 --> 00:14:47,050
지금까지 반복된 1은 없었습니다.

346
00:14:47,050 --> 00:14:48,590
하지만 이제 반복된 1이 있고,

347
00:14:48,590 --> 00:14:50,840
여기서 두 개가 연속으로 있기 때문에 또

348
00:14:50,840 --> 00:14:52,110
다른 반복된 1이 있습니다.

349
00:14:52,110 --> 00:14:54,443
그래서 우리가 만들고 있는 모델의 유형입니다.

350
00:14:54,443 --> 00:14:55,610
이 작업을 수행하려고 합니다.

351
00:14:55,610 --> 00:14:58,430
이것은 입력마다 하나의 출력을

352
00:14:58,430 --> 00:15:01,910
갖는 다대다 시퀀스 모델링 작업입니다.

353
00:15:01,910 --> 00:15:04,610
지금까지 높은 수준에서 이야기했지만,

354
00:15:04,610 --> 00:15:07,310
이를 수행하기 위해 RNN을 만들려고

355
00:15:07,310 --> 00:15:11,265
한다면, 은닉 상태에서 어떤 정보를 캡처해야 할까요?

356
00:15:11,265 --> 00:15:13,390
모델의 내부 상태가 있으므로,

357
00:15:13,390 --> 00:15:17,670
이 작업을 수행하기 위해 어떤 정보가 그곳에 캡처되어야 할까요?

358
00:15:17,670 --> 00:15:20,077
네, 그래서 이전 시간 단계의 입력입니다.

359
00:15:20,077 --> 00:15:22,410
그리고 우리의 출력이 숨겨진 상태에만 의존한다면, 우리는

360
00:15:22,410 --> 00:15:23,690
무엇을 더 알아야 할까요?

361
00:15:23,690 --> 00:15:24,630
현재-- 네,

362
00:15:24,630 --> 00:15:26,070
정확히 그렇습니다.

363
00:15:26,070 --> 00:15:28,030
그래서 이것이 우리가 숨겨진

364
00:15:28,030 --> 00:15:29,770
상태에서 포착해야 할 정보입니다.

365
00:15:29,770 --> 00:15:34,190
이전 입력과 현재 x 값, 즉 0 또는 1입니다.

366
00:15:34,190 --> 00:15:36,410
이것을 하는 방법은 숨겨진 상태

367
00:15:36,410 --> 00:15:38,830
t를 3차원 벡터로 설정하는 것입니다.

368
00:15:38,830 --> 00:15:41,247
3인 이유는 출력 단계 계산을

369
00:15:41,247 --> 00:15:44,190
할 때 유용할 것이기 때문이지만,

370
00:15:44,190 --> 00:15:47,770
여기서 1 없이도 만들 수 있을 것입니다.

371
00:15:47,770 --> 00:15:49,730
오늘 강의의 목적을 위해 수학을

372
00:15:49,730 --> 00:15:51,952
쉽게 하고 간단하게 만들기 위한 것입니다.

373
00:15:51,952 --> 00:15:53,910
그리고 다른 정보는 현재 값입니다.

374
00:15:53,910 --> 00:15:57,130
그래서 이것은 이전 값 0 또는 1과 함께 0 또는

375
00:15:57,130 --> 00:15:58,110
1이 될 것입니다.

376
00:15:58,110 --> 00:16:00,610
우리는 001로 초기화할 것이므로,

377
00:16:00,610 --> 00:16:03,970
기본적으로 이 시점에서 두 개의 0을

378
00:16:03,970 --> 00:16:06,890
연속으로 보고 있다고 가정하는 것입니다.

379
00:16:06,890 --> 00:16:09,970
네, 그래서 이렇게 할 것입니다.

380
00:16:09,970 --> 00:16:11,840
이것이 우리가 숨겨진 상태에서

381
00:16:11,840 --> 00:16:13,590
추적하려는 변수의 유형입니다.

382
00:16:13,590 --> 00:16:15,190
그리고 이것이 h,0을 초기화하는 방법입니다.

383
00:16:15,190 --> 00:16:16,570
다양한 전략으로 초기화할

384
00:16:16,570 --> 00:16:19,030
수 있거나 학습할 수 있다고 이야기했습니다.

385
00:16:19,030 --> 00:16:21,450
이것이 우리가 초기화할

386
00:16:21,450 --> 00:16:24,050
값입니다. 이제 코드를 단계별로

387
00:16:24,050 --> 00:16:25,510
살펴보겠습니다.

388
00:16:25,510 --> 00:16:28,210
지금 화면에 표시하고 있습니다.

389
00:16:28,210 --> 00:16:32,050
하지만 ReLU도 사용할 것이고, 미안합니다, 이 슬라이드에서 놓친 다른 한

390
00:16:32,050 --> 00:16:33,550
가지는 활성화 함수를 ReLU로

391
00:16:33,550 --> 00:16:35,383
설정하고 있다는 것입니다. 수학을

392
00:16:35,383 --> 00:16:36,610
쉽게 하기 위해서입니다.

393
00:16:36,610 --> 00:16:39,387
그래서 0 또는 그 값의 최대값이 될 것입니다.

394
00:16:39,387 --> 00:16:41,470
이 경우 기본적으로 0과 1만 다루고

395
00:16:41,470 --> 00:16:42,090
있습니다.

396
00:16:42,090 --> 00:16:44,490
그래서 생각하기가 꽤 간단합니다.

397
00:16:44,490 --> 00:16:44,990
네, 아마도

398
00:16:44,990 --> 00:16:48,190
tanh와 함께 작동하도록 구성할 수 있을 것입니다.

399
00:16:48,190 --> 00:16:49,630
하지만 이것은 실행

400
00:16:49,630 --> 00:16:52,050
방법에 대한 예제로 제가 만든 것입니다.

401
00:16:52,050 --> 00:16:55,390
그래서 수학을 정말 쉽게 만들기 위해 ReLU를 사용할 것입니다.

402
00:16:55,390 --> 00:16:57,350
하지만 네, tanh로도 할 수

403
00:16:57,350 --> 00:16:59,430
있는 모델을 만들 수 있을 것입니다.

404
00:16:59,430 --> 00:17:02,830
네, 멋지네요.

405
00:17:02,830 --> 00:17:04,569
우리는 ReLU를 가지고 있습니다.

406
00:17:04,569 --> 00:17:06,890
여기 두 개의 특정 가중치가 있습니다.

407
00:17:06,890 --> 00:17:10,390
첫 번째 가중치는 이전

408
00:17:10,390 --> 00:17:13,790
숨겨진 상태를 변환합니다.

409
00:17:13,790 --> 00:17:16,869
이전 숨겨진 상태에 변환을

410
00:17:16,869 --> 00:17:20,130
적용하여 다음 것을 계산합니다.

411
00:17:20,130 --> 00:17:21,849
그리고 이 가중치는

412
00:17:21,849 --> 00:17:25,869
입력 x를 숨겨진 상태의 차원으로

413
00:17:25,869 --> 00:17:28,290
변환하고 변환을 적용합니다.

414
00:17:28,290 --> 00:17:31,030
그래서 우리는 이 두 번째 것을 설정하고 있습니다.

415
00:17:31,030 --> 00:17:32,890
현재의 은닉 상태는

416
00:17:32,890 --> 00:17:36,610
이전 은닉 상태와 현재 시간 단계의

417
00:17:36,610 --> 00:17:37,490
함수입니다.

418
00:17:37,490 --> 00:17:41,250
따라서 시간 단계 t에서 이 은닉

419
00:17:41,250 --> 00:17:46,010
상태를 계산할 때, 먼저 현재 값을 계산하려고

420
00:17:46,010 --> 00:17:46,670
합니다.

421
00:17:46,670 --> 00:17:49,470
여기서 x 값을 사용할 것입니다.

422
00:17:49,470 --> 00:17:55,250
가중치를 1, 0, 0의 값을 가진 3x1 열 벡터로

423
00:17:55,250 --> 00:18:00,790
설정하여 x가 0일 때 행렬 곱을 하면 0 벡터가

424
00:18:00,790 --> 00:18:06,292
되고, x가 1일 때는 1, 0, 0이 되도록 합니다.

425
00:18:06,292 --> 00:18:07,750
그리고 이것을 다른 항에

426
00:18:07,750 --> 00:18:09,667
추가할 것이지만, 기본적으로 현재

427
00:18:09,667 --> 00:18:11,030
값을 계산하는 것입니다.

428
00:18:11,030 --> 00:18:15,110
그래서 위에는 0이거나 1이 될 것입니다.

429
00:18:15,110 --> 00:18:18,490
이것은 여기서 첫 번째 연산을 기반으로 계산됩니다.

430
00:18:21,770 --> 00:18:23,970
그래서 입력을 기반으로

431
00:18:23,970 --> 00:18:26,790
현재 값을 계산하는 방법입니다.

432
00:18:26,790 --> 00:18:31,390
이제 은닉 상태 변환을 어떻게 하는지에 대해

433
00:18:31,390 --> 00:18:32,310
이야기하겠습니다.

434
00:18:32,310 --> 00:18:35,550
위의 이 값에 대해 현재 값을 사용하고자

435
00:18:35,550 --> 00:18:36,050
합니다.

436
00:18:36,050 --> 00:18:38,750
가중치 행렬의 상단 행에는 0만 있을 것입니다.

437
00:18:38,750 --> 00:18:40,230
이는 이전 은닉 상태와

438
00:18:40,230 --> 00:18:41,750
곱할 때 위쪽에 0

439
00:18:41,750 --> 00:18:43,930
값을 얻게 됨을 의미합니다.

440
00:18:43,930 --> 00:18:46,270
그래서 오른쪽에 있는 값에 따라 0과

441
00:18:46,270 --> 00:18:47,490
더해질 것입니다.

442
00:18:47,490 --> 00:18:50,270
이렇게 해서 이전 은닉 상태에 따라 변경되지

443
00:18:50,270 --> 00:18:51,870
않도록 유지할 것입니다.

444
00:18:51,870 --> 00:18:55,550
다음 행은 1, 0, 0으로 설정할 것입니다.

445
00:18:55,550 --> 00:18:56,510
왜 이렇게 하는 걸까요?

446
00:18:56,510 --> 00:18:58,390
우리가 여기에서 이전 시간 단계의 숨겨진

447
00:18:58,390 --> 00:19:00,670
상태를 가지고 있다고 상상할 수 있습니다.

448
00:19:00,670 --> 00:19:05,070
그리고 우리는 이제 이전 값을 이전 현재 시간 단계로

449
00:19:05,070 --> 00:19:06,890
설정하고자 합니다.

450
00:19:06,890 --> 00:19:08,490
그래서 우리는 1, 0, 0을 가지고 있습니다.

451
00:19:08,490 --> 00:19:10,930
이것은 h,t-1과 곱해질 것입니다.

452
00:19:10,930 --> 00:19:14,630
현재 값을 이제 이 시간 단계의 이전

453
00:19:14,630 --> 00:19:16,810
값으로 설정할 것입니다.

454
00:19:16,810 --> 00:19:19,650
기본적으로 이 항은 위에 0이

455
00:19:19,650 --> 00:19:22,870
되고, 두 번째 항은 이전 시간 단계

456
00:19:22,870 --> 00:19:24,650
입력 값이 됩니다.

457
00:19:24,650 --> 00:19:27,730
그리고 마지막 부분은 모든

458
00:19:27,730 --> 00:19:31,170
계산에서 이 1을 유지하도록 합니다.

459
00:19:31,170 --> 00:19:33,290
요약하자면, 우리는

460
00:19:33,290 --> 00:19:36,130
오른쪽 항이 현재 값을

461
00:19:36,130 --> 00:19:39,230
추적하도록 0이 있습니다.

462
00:19:39,230 --> 00:19:42,050
여기에는 이전 시간 단계의

463
00:19:42,050 --> 00:19:43,930
현재 값을

464
00:19:43,930 --> 00:19:46,810
현재 시간 단계의 이전으로

465
00:19:46,810 --> 00:19:50,130
복사하기 위해 1이 있습니다.

466
00:19:50,130 --> 00:19:52,330
우리는 h를 하고 있습니다.

467
00:19:52,330 --> 00:19:57,230
코드에서는 쉬울 수 있지만 h,t 이전은 h,t와 같고,

468
00:19:57,230 --> 00:20:00,090
여기에 해당 값을 아래로 하나

469
00:20:00,090 --> 00:20:01,290
이동하고자 합니다.

470
00:20:01,290 --> 00:20:03,682
그리고 이것은 1의 복사본입니다.

471
00:20:03,682 --> 00:20:05,390
그렇다면 실제로 출력을 어떻게 얻을 수 있을까요?

472
00:20:05,390 --> 00:20:10,010
우리는 Whh와 Wxh에 대해 이야기한 가중치 행렬을

473
00:20:10,010 --> 00:20:15,010
통해 이러한 값을 추적할 수 있는 방법에 대해 이야기했습니다.

474
00:20:15,010 --> 00:20:17,250
숨겨진 상태를 출력 차원으로

475
00:20:17,250 --> 00:20:20,110
변환하는 가중치 행렬이 필요하며,

476
00:20:20,110 --> 00:20:23,010
우리는 1x3이 되기를 원합니다.

477
00:20:23,010 --> 00:20:26,350
숨겨진 차원을 입력으로 할 때 출력되는

478
00:20:26,350 --> 00:20:28,050
단일 값입니다.

479
00:20:28,050 --> 00:20:31,870
그리고 이것은 여기의 값과 여기의 값 간의

480
00:20:31,870 --> 00:20:32,930
내적입니다.

481
00:20:32,930 --> 00:20:35,830
이것은 현재 값과 이전 값에서 1을 빼는

482
00:20:35,830 --> 00:20:39,750
것과 관련이 있습니다. 왜냐하면 여기에서 -1과 곱하기

483
00:20:39,750 --> 00:20:40,250
때문입니다.

484
00:20:40,250 --> 00:20:42,710
여기서 하나가 유용해지고

485
00:20:42,710 --> 00:20:46,570
현재는 하나와 연결됩니다.

486
00:20:46,570 --> 00:20:49,150
그리고 이전도 여기서

487
00:20:49,150 --> 00:20:51,110
하나와 연결됩니다.

488
00:20:51,110 --> 00:20:52,450
그래서 우리가 실제로 이렇게 합니다.

489
00:20:52,450 --> 00:20:57,570
생각해보면 이 일반적인 공식이 작동할 것입니다.

490
00:20:57,570 --> 00:21:02,070
여기서 현재와 이전을 더하면 2에서 1을

491
00:21:02,070 --> 00:21:06,350
빼면 ReLU 내부의 이 왼쪽 항은

492
00:21:06,350 --> 00:21:07,690
1이 됩니다.

493
00:21:07,690 --> 00:21:09,670
1과 0의 최대값은 1입니다.

494
00:21:09,670 --> 00:21:13,390
둘 다 0이라면 -1이 되어 0이 됩니다.

495
00:21:13,390 --> 00:21:14,890
이들은 1과 0입니다.

496
00:21:14,890 --> 00:21:16,510
그럼에도 불구하고 0이 나옵니다.

497
00:21:16,510 --> 00:21:19,770
이렇게 가중치 행렬을 구성할 수 있습니다.

498
00:21:19,770 --> 00:21:23,650
하지만 이 계산의 각 단계에 대해 질문이 있는지 잠시

499
00:21:23,650 --> 00:21:26,450
멈추고 이야기하고 싶었습니다. 왜냐하면

500
00:21:26,450 --> 00:21:29,865
이 수업에서 우리가 실제로 모든 행렬과 벡터 곱셈을

501
00:21:29,865 --> 00:21:32,490
수행하는 유일한 예제이기 때문입니다.

502
00:21:32,490 --> 00:21:34,290
나머지는 사람들이

503
00:21:34,290 --> 00:21:37,770
이러한 레이어를 어떻게 조합하는지에 대한 더 높은

504
00:21:37,770 --> 00:21:39,630
수준의 설명이 될 것입니다.

505
00:21:39,630 --> 00:21:43,610
그래서 행렬과 벡터가 어떻게 추적되고 곱해지고

506
00:21:43,610 --> 00:21:46,170
업데이트되는지에 대한 질문이 있는지

507
00:21:46,170 --> 00:21:47,790
확인하고 싶습니다.

508
00:21:47,790 --> 00:21:49,290
네, 질문은 가중치

509
00:21:49,290 --> 00:21:51,950
행렬을 어떻게 구성하느냐입니다.

510
00:21:51,950 --> 00:21:53,560
정말 좋은 질문이고, 여기

511
00:21:53,560 --> 00:21:55,310
슬라이드에 넣으려고 생각했습니다.

512
00:21:55,310 --> 00:21:56,848
그럼 실제로 어떻게 할까요?

513
00:21:56,848 --> 00:21:58,890
이 수업에서 항상 가중치 행렬을 찾는

514
00:21:58,890 --> 00:21:59,830
방법과 같습니다.

515
00:21:59,830 --> 00:22:01,150
우리는 경량 하강법을 사용할 것입니다.

516
00:22:01,150 --> 00:22:03,570
여러 시간 단계가 있을 때 경량 하강법을 어떻게

517
00:22:03,570 --> 00:22:05,410
수행하는지, 각 시간 단계에서 손실이

518
00:22:05,410 --> 00:22:07,550
계산될 수 있는 방법에 대해 이야기할 것입니다.

519
00:22:07,550 --> 00:22:10,710
그래서 그게 우리가 다음에 다룰 많은 내용이 될 것입니다.

520
00:22:10,710 --> 00:22:13,850
그래서 좋은 질문이고 강의와 매우 관련이 있습니다.

521
00:22:13,850 --> 00:22:16,450
이것은 단지 예제일 뿐이므로

522
00:22:16,450 --> 00:22:20,880
모든 가중치 행렬이 어떻게 곱해지는지 볼 수 있습니다.

523
00:22:20,880 --> 00:22:22,728
기본적으로 만약 당신이

524
00:22:22,728 --> 00:22:24,520
이것으로 초기화하려고 하고,

525
00:22:24,520 --> 00:22:26,320
그런 다음 다른 작업을 수행하도록

526
00:22:26,320 --> 00:22:29,840
훈련하려고 한다면, 그것은 이로 가중치를 초기화하는

527
00:22:29,840 --> 00:22:31,880
전이 학습이 될 것입니다.

528
00:22:31,880 --> 00:22:35,412
하지만 실제로는 잘 작동하지 않을 것이라고 생각합니다. 왜냐하면 당신의

529
00:22:35,412 --> 00:22:37,120
숨겨진 상태가 정말 작고 사람들은

530
00:22:37,120 --> 00:22:40,022
일반적으로 훨씬 더 큰 숨겨진 상태를 사용하기 때문입니다.

531
00:22:40,022 --> 00:22:42,480
여기 슬라이드에 맞출 수 있는 무언가를

532
00:22:42,480 --> 00:22:43,640
하고 싶었습니다.

533
00:22:43,640 --> 00:22:46,160
네, 알겠습니다. 두 번째 행을 다시 설명하겠습니다.

534
00:22:46,160 --> 00:22:51,280
h,t-1을 열 벡터로 상상해보면, 행렬 곱셈을

535
00:22:51,280 --> 00:22:55,560
수행하여 여기 왼쪽의 값을 얻을 때,

536
00:22:55,560 --> 00:22:59,520
이 두 번째 행은 값을 가져와서

537
00:22:59,520 --> 00:23:02,840
회전시키고 여기 있는 값들과

538
00:23:02,840 --> 00:23:04,540
내적을 수행합니다.

539
00:23:04,540 --> 00:23:08,680
계산된 두 번째 행의 항목은

540
00:23:08,680 --> 00:23:12,940
여기 벡터의 맨 위와 같습니다.

541
00:23:12,940 --> 00:23:16,200
그래서 현재를 이전으로 이동하는 방법은 여기

542
00:23:16,200 --> 00:23:17,560
단계에서 이루어집니다.

543
00:23:17,560 --> 00:23:20,420
이 행렬 곱셈의

544
00:23:20,420 --> 00:23:27,100
최종 결과는 두 번째 값이 t-1의 현재

545
00:23:27,100 --> 00:23:30,140
값이 되도록 합니다.

546
00:23:30,140 --> 00:23:33,060
그래서 t-1과 행렬 곱셈을 수행하고,

547
00:23:33,060 --> 00:23:36,660
이 연산의 두 번째 행과 이 연산은 모두

548
00:23:36,660 --> 00:23:40,220
우리의 숨겨진 상태 크기의 벡터를 제공합니다.

549
00:23:40,220 --> 00:23:42,700
그래서 우리는 그것들을 더하고 있습니다.

550
00:23:42,700 --> 00:23:45,540
네, 왼쪽은 이전 이월을 수행하고

551
00:23:45,540 --> 00:23:47,760
오른쪽은 현재를 수행하고 있습니다.

552
00:23:47,760 --> 00:23:51,780
그리고 이것이 장난감 예제를 넘어 RNN에서 어떻게

553
00:23:51,780 --> 00:23:55,060
작동하는지를 보여줍니다. 여기서 이 가중치 행렬은

554
00:23:55,060 --> 00:23:57,620
현재 입력과 곱해지고, 이 다른

555
00:23:57,620 --> 00:24:00,680
가중치 행렬은 이전 은닉 상태와 곱해집니다.

556
00:24:00,680 --> 00:24:03,460
그래서 이것이 이러한 가중치 행렬이 특정

557
00:24:03,460 --> 00:24:06,540
문제보다 더 일반적으로 추적하는 것입니다.

558
00:24:06,540 --> 00:24:11,280
그렇다면 실제로 기울기를 어떻게 계산합니까?

559
00:24:11,280 --> 00:24:13,080
계산 그래프를 살펴보겠습니다.

560
00:24:13,080 --> 00:24:16,720
그래서 이전보다 좀 더 명확하게 그리기 위해,

561
00:24:16,720 --> 00:24:19,580
우리는 x1과 x2가 들어오고, x의

562
00:24:19,580 --> 00:24:20,940
시퀀스가 있습니다.

563
00:24:20,940 --> 00:24:23,480
우리는 각 시간 단계에서 은닉 상태를

564
00:24:23,480 --> 00:24:27,120
계산하고 있으며, 이러한 계산을 위해 동일한

565
00:24:27,120 --> 00:24:30,380
W, 동일한 가중치 행렬을 사용하고 있습니다.

566
00:24:30,380 --> 00:24:31,880
그래서 우리는 기울기를

567
00:24:31,880 --> 00:24:35,440
계산하는 방법을 생각할 때 이것을 고려해야 하며, 많은 입력에서

568
00:24:35,440 --> 00:24:37,580
많은 출력 시나리오로 시작하겠습니다.

569
00:24:37,580 --> 00:24:40,520
각 입력에 대한 출력이 있습니다.

570
00:24:40,520 --> 00:24:43,727
이 시나리오에서는 각 출력에 대한 손실을 계산할 수도 있으며,

571
00:24:43,727 --> 00:24:45,560
이는 해당 단계에서 출력이

572
00:24:45,560 --> 00:24:47,100
얼마나 정확한지를 나타냅니다.

573
00:24:47,100 --> 00:24:53,000
따라서 이 설정에서 각 단계에서 손실이 있으며, 이를 모두 합쳐

574
00:24:53,000 --> 00:24:55,875
총 손실을 얻을 수 있습니다.

575
00:24:55,875 --> 00:24:58,000
이것이 전체 입력 시퀀스에

576
00:24:58,000 --> 00:24:59,640
대한 손실입니다.

577
00:24:59,640 --> 00:25:07,303
그리고 기본적으로 역전파를 수행할 때, 최종 손실을 계산하면

578
00:25:07,303 --> 00:25:08,720
각 시간

579
00:25:08,720 --> 00:25:11,040
단계에 대한 손실도

580
00:25:11,040 --> 00:25:14,280
계산할 수 있습니다.

581
00:25:14,280 --> 00:25:16,720
따라서 시간 단계별로 손실을 계산하는 경우, 이를

582
00:25:16,720 --> 00:25:18,340
독립적으로 처리할 수 있습니다.

583
00:25:18,340 --> 00:25:20,460
때때로 시간 단계별 손실을

584
00:25:20,460 --> 00:25:23,180
기반으로 전체 손실이 있을 수도 있습니다.

585
00:25:23,180 --> 00:25:28,020
우리는 이러한 W 각각에 대한 최종 기울기를 얻을

586
00:25:28,020 --> 00:25:29,180
수 있습니다.

587
00:25:29,180 --> 00:25:32,380
각 시간 단계에 대해 기울기를 별도로 계산한

588
00:25:32,380 --> 00:25:34,400
다음, 이를 모두 합칠 것입니다.

589
00:25:34,400 --> 00:25:36,560
이것이 실제로 작동하는 방식입니다.

590
00:25:36,560 --> 00:25:39,980
각 시간 단계에서 다른 W가 있었다고 상상할 수 있습니다.

591
00:25:39,980 --> 00:25:43,800
각기 다른 W에 대해 다른 기울기를 계산할 수 있도록

592
00:25:43,800 --> 00:25:46,300
계산 그래프가 구조화될 수

593
00:25:46,300 --> 00:25:48,800
있는 방법을 쉽게 볼 수 있습니다.

594
00:25:48,800 --> 00:25:51,620
그래서 우리는 본질적으로 계산 목적으로

595
00:25:51,620 --> 00:25:57,302
단일 W를 서로 다른 W 집합으로 취급하지만, 마지막에는 모든 기울기를

596
00:25:57,302 --> 00:25:59,260
합칩니다. 왜냐하면 곱해지는

597
00:25:59,260 --> 00:26:00,635
것은 동일한 가중치

598
00:26:00,635 --> 00:26:02,040
행렬이기 때문입니다.

599
00:26:02,040 --> 00:26:06,260
개념적으로, 각 시간 단계에 대해 계산하고 있는 것으로

600
00:26:06,260 --> 00:26:08,740
생각할 수 있으며, 머릿속에서 마치

601
00:26:08,740 --> 00:26:10,560
다른 W를 사용하는 것처럼

602
00:26:10,560 --> 00:26:12,740
처리하지만, 가중치의 값이

603
00:26:12,740 --> 00:26:15,240
동일하기 때문에 각 시간 단계에서

604
00:26:15,240 --> 00:26:18,000
계산한 모든 기울기를 합쳐야 합니다.

605
00:26:18,000 --> 00:26:20,000
많은 입력에서 하나의

606
00:26:20,000 --> 00:26:24,080
출력 시나리오에서는 여기에서 단일 손실만 계산됩니다.

607
00:26:24,080 --> 00:26:28,107
때때로 문제 설정에 따라 최종 은닉 상태만

608
00:26:28,107 --> 00:26:30,440
사용하여 값을 계산합니다.

609
00:26:30,440 --> 00:26:32,420
예를 들어 비디오 분류를 시도하는

610
00:26:32,420 --> 00:26:35,420
경우, 분류하는 동안 비디오 전체에 대한 정보를

611
00:26:35,420 --> 00:26:37,503
가질 수 있으므로 각 단계의 은닉

612
00:26:37,503 --> 00:26:40,170
상태를 사용하는 것이 의미가 있을 수 있습니다.

613
00:26:40,170 --> 00:26:42,720
당신은 평균 풀링이나 최대 풀링과

614
00:26:42,720 --> 00:26:46,520
같은 풀링을 수행하여 y 값을 계산할 것입니다.

615
00:26:46,520 --> 00:26:51,800
그리고 이미지 분류나 이미지 캡셔닝에서 이전 y를 통합하는

616
00:26:51,800 --> 00:26:56,073
방법에 대한 질문이 있었던 것처럼, 이러한 일대다

617
00:26:56,073 --> 00:26:58,240
매핑이 있을 경우에도

618
00:26:58,240 --> 00:26:59,620
마찬가지입니다.

619
00:26:59,620 --> 00:27:04,640
따라서 f,w에 대한 입력이 필요합니다. 왜냐하면 두 개의 서로

620
00:27:04,640 --> 00:27:07,460
다른 가중치 행렬이기 때문입니다.

621
00:27:07,460 --> 00:27:09,840
하나는 입력 벡터 x를 기대하고,

622
00:27:09,840 --> 00:27:14,100
다른 하나는 이전 시간 단계의 은닉 상태를 기대합니다.

623
00:27:14,100 --> 00:27:14,600
다른 하나는 이전 시간 단계의 은닉 상태를 기대합니다.

624
00:27:14,600 --> 00:27:19,080
그래서 여기 많은 값을 넣을 수 있다고 상상할 수 있습니다.

625
00:27:19,080 --> 00:27:24,940
당신은 단순히 0을 넣거나 이전 출력을 여기에

626
00:27:24,940 --> 00:27:26,820
넣을 수 있습니다.

627
00:27:26,820 --> 00:27:30,480
나는 역전파를 수행하는 방법을 높은 수준에서 설명했습니다.

628
00:27:30,480 --> 00:27:32,500
하지만 실제로는 GPU

629
00:27:32,500 --> 00:27:35,283
메모리가 부족해지는 것과 같은 특정

630
00:27:35,283 --> 00:27:37,700
문제에 직면하게 될 것입니다.

631
00:27:37,700 --> 00:27:40,842
이는 신경망을 훈련할 때 발생하는

632
00:27:40,842 --> 00:27:43,300
모든 문제의 원인입니다. 그리고,

633
00:27:43,300 --> 00:27:46,360
아마도 NaN 손실 문자열 훈련과

634
00:27:46,360 --> 00:27:47,880
관련이 있습니다.

635
00:27:47,880 --> 00:27:53,180
따라서 각 시간 단계에서 손실을 계산할 때, 매우 긴

636
00:27:53,180 --> 00:27:56,668
입력 시퀀스가 있는 경우 이해하기가

637
00:27:56,668 --> 00:27:57,960
정말 쉽습니다.

638
00:27:57,960 --> 00:28:00,580
각 시간 단계에서 활성화와 기울기를

639
00:28:00,580 --> 00:28:03,880
메모리에 유지하고 모두 합산해야 합니다.

640
00:28:03,880 --> 00:28:07,180
입력 시퀀스가 증가함에 따라 매우 커질

641
00:28:07,180 --> 00:28:07,920
것입니다.

642
00:28:07,920 --> 00:28:11,840
이 문제를 해결하기 위해 실제로 무엇을 할 수 있을까요?

643
00:28:11,840 --> 00:28:13,740
이것은 시간에 따른 역전파라고

644
00:28:13,740 --> 00:28:16,080
하며, 여러 다른 시간 단계에서

645
00:28:16,080 --> 00:28:18,620
적용되는 동일한 가중치 행렬이

646
00:28:18,620 --> 00:28:21,040
있을 때, 각 시간 단계에서 기울기를

647
00:28:21,040 --> 00:28:22,520
합산하는 것입니다.

648
00:28:22,520 --> 00:28:24,593
할 수 있는 것은 잘라진 시간에

649
00:28:24,593 --> 00:28:26,260
따른 역전파라고 합니다.

650
00:28:26,260 --> 00:28:28,640
기본적으로 시간 창을

651
00:28:28,640 --> 00:28:31,680
고정하고, 지금까지 모델이

652
00:28:31,680 --> 00:28:35,400
훈련된 것처럼 가정할 수 있습니다.

653
00:28:35,400 --> 00:28:37,480
h,0에서 시작합니다.

654
00:28:37,480 --> 00:28:41,200
시간 단계 1의 입력과 이전 h 값을

655
00:28:41,200 --> 00:28:43,300
기반으로 계산합니다.

656
00:28:43,300 --> 00:28:47,258
현재 숨겨진 상태 h,1을 계산할 수 있으며, 이를 사용하여

657
00:28:47,258 --> 00:28:49,300
출력을 계산할 수 있습니다.

658
00:28:49,300 --> 00:28:50,640
손실을 얻게 됩니다.

659
00:28:50,640 --> 00:28:52,580
각 예제에 대해 이 작업을 수행할 수 있습니다.

660
00:28:52,580 --> 00:28:54,247
이 설정에서 훈련 중에

661
00:28:54,247 --> 00:28:57,560
우리가 보는 모든 것이 시작 시퀀스처럼 처리하는

662
00:28:57,560 --> 00:29:00,800
것이 얼마나 쉬운지 상상할 수 있습니다.

663
00:29:00,800 --> 00:29:03,620
다음 블록으로 이동하면 이제

664
00:29:03,620 --> 00:29:06,460
h,0이 이전 단계의

665
00:29:06,460 --> 00:29:08,380
출력으로 시작됩니다.

666
00:29:08,380 --> 00:29:11,153
따라서 마지막 단계에서의 출력을 사용하여 숨겨진

667
00:29:11,153 --> 00:29:12,820
상태를 초기화하지만, 기울기는

668
00:29:12,820 --> 00:29:14,400
더 이상 전달되지 않습니다.

669
00:29:14,400 --> 00:29:16,900
기본적으로 시간 단계의 이웃에서

670
00:29:16,900 --> 00:29:19,780
손실만 보고 있는 계산 그래프를

671
00:29:19,780 --> 00:29:21,600
배치하고 있습니다.

672
00:29:21,600 --> 00:29:23,760
이것은 설정하는 고정 창 크기입니다.

673
00:29:23,760 --> 00:29:29,820
이것이 특히 긴 입력 시퀀스가 있을 때 상대적으로

674
00:29:29,820 --> 00:29:32,460
일반적인 문제를 해결하는

675
00:29:32,460 --> 00:29:34,260
방법입니다.

676
00:29:34,260 --> 00:29:37,500
그래서 기본적으로 배치 처리하고

677
00:29:37,500 --> 00:29:42,380
전체 입력 시퀀스에 대해 계속할 수 있습니다.

678
00:29:42,380 --> 00:29:46,375
또 다른 질문은, 마지막에 단일 출력을 가진

679
00:29:46,375 --> 00:29:48,500
경우 어떻게 작동하는지

680
00:29:48,500 --> 00:29:50,500
물어볼 수 있습니다.

681
00:29:50,500 --> 00:29:54,320
각 시간 단계에서 기울기를 여전히

682
00:29:54,320 --> 00:29:59,540
계산할 수 있지만, 더 이상 시간 단계 자체의 출력에

683
00:29:59,540 --> 00:30:02,420
의존하는 손실은 없으며,

684
00:30:02,420 --> 00:30:04,040
대신 상류

685
00:30:04,040 --> 00:30:06,333
기울기에 의존하게 됩니다.

686
00:30:06,333 --> 00:30:09,000
여기 다이어그램의 가장 오른쪽을 보고

687
00:30:09,000 --> 00:30:11,120
있으며, 마지막 시간 단계의

688
00:30:11,120 --> 00:30:13,800
출력을 기반으로 계산한 손실이 있습니다.

689
00:30:13,800 --> 00:30:16,640
현재 숨겨진 상태에 대한

690
00:30:16,640 --> 00:30:19,680
기울기를 계산할 수 있습니다.

691
00:30:19,680 --> 00:30:23,800
그리고 이전 숨겨진 상태가 최종 숨겨진 상태에

692
00:30:23,800 --> 00:30:27,480
어떻게 기여했는지 이해하는 데 도움이

693
00:30:27,480 --> 00:30:30,420
되는 Whh 행렬이 있습니다.

694
00:30:30,420 --> 00:30:34,240
이것을 사용하여 이전 숨겨진 상태와 가중치 행렬을

695
00:30:34,240 --> 00:30:36,680
기반으로 기울기를 계산할

696
00:30:36,680 --> 00:30:37,700
수 있습니다.

697
00:30:37,700 --> 00:30:42,000
손실을 변경할 수 있도록 이 변환 행렬 Whh를

698
00:30:42,000 --> 00:30:44,880
어떻게 변경할 수 있을까요?

699
00:30:44,880 --> 00:30:49,778
그리고 기본적으로 여기서 Whh에 대해 기울기 규칙을 반복적으로

700
00:30:49,778 --> 00:30:50,820
적용합니다.

701
00:30:50,820 --> 00:30:53,112
숨겨진 상태가 다음 숨겨진 상태를 어떻게

702
00:30:53,112 --> 00:30:56,720
변화시켰는지, 그리고 그것이 손실에 어떻게 기여했는지를 보고 있습니다.

703
00:30:56,720 --> 00:30:59,100
여기 마지막 예제를 봅니다.

704
00:30:59,100 --> 00:31:01,080
이것은 숨겨진 상태를 변경하는

705
00:31:01,080 --> 00:31:04,040
것이 손실에 어떻게 의존하는지를 알려주며, 이전

706
00:31:04,040 --> 00:31:06,020
숨겨진 상태가 어떻게 변화했는지,

707
00:31:06,020 --> 00:31:10,220
그것이 현재 숨겨진 상태에 어떻게 영향을 미쳤는지를 보여줍니다.

708
00:31:10,220 --> 00:31:14,100
각 시간 단계에서 서로 다른 W를 사용하면 더

709
00:31:14,100 --> 00:31:17,340
이상 재귀 관계로 모델링하지 않는다는

710
00:31:17,340 --> 00:31:18,520
것을 의미합니다.

711
00:31:18,520 --> 00:31:22,300
기본적으로 각 가능한 시간 단계에 대해 하나의

712
00:31:22,300 --> 00:31:24,900
레이어로 생각할 수 있습니다.

713
00:31:24,900 --> 00:31:28,580
따라서 재귀적으로 시퀀스로 모델링하지

714
00:31:28,580 --> 00:31:33,860
않으면 성능이 저하될 것입니다. 각 입력이

715
00:31:33,860 --> 00:31:35,923
독립적으로 가는 별도의

716
00:31:35,923 --> 00:31:37,340
가중치를

717
00:31:37,340 --> 00:31:38,640
가진 신경망을

718
00:31:38,640 --> 00:31:40,720
훈련한다고 상상해

719
00:31:40,720 --> 00:31:41,420
보세요.

720
00:31:41,420 --> 00:31:42,360
네, 독립적으로요.

721
00:31:42,360 --> 00:31:43,818
순서 모델링 문제가

722
00:31:43,818 --> 00:31:45,960
아닌 문제에 대해서는 이해가

723
00:31:45,960 --> 00:31:49,740
됩니다. 분류하고 싶은 것들의 집합이 있을 뿐입니다.

724
00:31:49,740 --> 00:31:52,920
시퀀스의 양을 미리 알아야 합니다.

725
00:31:52,920 --> 00:31:54,920
그래서 시퀀스가 아니라면

726
00:31:54,920 --> 00:31:59,300
작동할 수 있을 것 같지만, 가변 길이의 시퀀스에

727
00:31:59,300 --> 00:32:03,440
대해서는 잘 작동하지 않을 것 같습니다.

728
00:32:03,440 --> 00:32:06,663
왜냐하면 각 시간 단계마다 하나의

729
00:32:06,663 --> 00:32:08,080
신경망을 훈련시키는

730
00:32:08,080 --> 00:32:10,660
것과 같기 때문입니다. 그래서

731
00:32:10,660 --> 00:32:13,800
그렇게 공식화하는 방법이 아닙니다.

732
00:32:13,800 --> 00:32:15,300
그럼 청킹과는 어떻게 작동하나요?

733
00:32:15,300 --> 00:32:18,960
그래서 여기서 빨간 점이 있는

734
00:32:18,960 --> 00:32:21,560
지점까지 이해하셨나요?

735
00:32:21,560 --> 00:32:24,800
우리는 최종 은닉 상태에 대한

736
00:32:24,800 --> 00:32:27,160
손실의 기울기를 계산할

737
00:32:27,160 --> 00:32:29,000
수 있습니다.

738
00:32:29,000 --> 00:32:32,240
그렇게 할 수 있다면, 우리는 두 번째 최종 은닉

739
00:32:32,240 --> 00:32:35,460
상태에 대한 손실의 기울기를 계산할 수 있습니다.

740
00:32:35,460 --> 00:32:37,280
최종 은닉 상태는 이전

741
00:32:37,280 --> 00:32:40,160
은닉 상태에 이 가중치 행렬 W를 곱한

742
00:32:40,160 --> 00:32:42,300
것에 의존하기 때문입니다.

743
00:32:42,300 --> 00:32:45,200
그래서 우리는 이렇게 할 수 있고 여기까지 왔다 갔다 할 수 있습니다.

744
00:32:45,200 --> 00:32:50,660
이 시점에서 우리가 저장해야 할 것은 이 마지막 단계입니다.

745
00:32:50,660 --> 00:32:56,040
그럼 이 마지막 또는 최종 은닉 상태의 기울기는 무엇인가요? 아마도
'최종'이라는 단어를 사용하는 것이

746
00:32:56,040 --> 00:32:57,580
적절하지 않을 수 있습니다.

747
00:32:57,580 --> 00:33:01,980
하지만 잘라낸 배치 내에서 이 초기 은닉 상태의

748
00:33:01,980 --> 00:33:05,320
기울기는 손실에 대해 무엇인가요?

749
00:33:05,320 --> 00:33:07,280
그리고 역으로 계산할 때,

750
00:33:07,280 --> 00:33:10,420
우리는 그 값을 사용하여 모든 이전 시간 단계를

751
00:33:10,420 --> 00:33:11,140
계산합니다.

752
00:33:11,140 --> 00:33:13,200
그래서 그게 전체 과정입니다.

753
00:33:13,200 --> 00:33:17,220
당신은 은닉 상태가 새로운 은닉 상태를 형성하는 방식만

754
00:33:17,220 --> 00:33:19,820
보고 있으며, 여기서 업데이트되는

755
00:33:19,820 --> 00:33:21,200
유일한 값입니다.

756
00:33:23,940 --> 00:33:28,000
네, 그리고 입력이 은닉 상태를 어떻게 변화시키는지도요.

757
00:33:28,000 --> 00:33:30,860
그래서 두 가지 값을 보고 있습니다. 입력이

758
00:33:30,860 --> 00:33:33,380
그것에 미치는 영향과 입력이 다음 은닉

759
00:33:33,380 --> 00:33:36,240
상태와 이전 은닉 상태에 미치는 영향입니다.

760
00:33:36,240 --> 00:33:38,580
죄송합니다, 두 가지 값이 있습니다.

761
00:33:38,580 --> 00:33:41,500
그래서 학습은 모든 배치에 대해 여전히 발생합니다.

762
00:33:41,500 --> 00:33:44,300
여기 W의 각

763
00:33:44,300 --> 00:33:49,080
매개변수에 대한 손실이 있습니다.

764
00:33:49,080 --> 00:33:53,660
그리고 이전 시간 단계에 대해 계산할 때,

765
00:33:53,660 --> 00:33:57,440
기본적으로 이 하나의 값을 유지합니다.

766
00:33:57,440 --> 00:33:59,580
여기 초기 은닉 상태를 변경하면 손실이

767
00:33:59,580 --> 00:34:00,460
어떻게 변하나요?

768
00:34:00,460 --> 00:34:01,960
그것을 계산할 수 있고,

769
00:34:01,960 --> 00:34:04,722
그러면 이 원래 은닉 상태와 현재

770
00:34:04,722 --> 00:34:06,680
시간 단계가 이 변수에 어떻게

771
00:34:06,680 --> 00:34:08,900
영향을 미칠지 볼 수 있습니다.

772
00:34:08,900 --> 00:34:11,580
하지만 실제로 다음 청크로 이동할 때,

773
00:34:11,580 --> 00:34:14,679
이 은닉 상태가 다음 청크의 은닉 상태에

774
00:34:14,679 --> 00:34:17,620
어떻게 영향을 미치는지만 살펴보면 됩니다.

775
00:34:17,620 --> 00:34:19,659
그래서 이 분할 경계를 보고 있습니다.

776
00:34:19,659 --> 00:34:21,320
추적해야 할 유일한

777
00:34:21,320 --> 00:34:24,960
변수는 청크 이후에 발생하는 은닉 상태의

778
00:34:24,960 --> 00:34:27,560
기울기가 무엇인지입니다.

779
00:34:27,560 --> 00:34:29,199
그런 다음 이를 사용하여

780
00:34:29,199 --> 00:34:31,157
입력 x와 이전에 의존하는 현재

781
00:34:31,157 --> 00:34:33,741
은닉 상태의 기울기를 계산할 수 있습니다.

782
00:34:33,741 --> 00:34:35,699
공식화하는 방법은 여러 가지가

783
00:34:35,699 --> 00:34:38,120
있지만, 우리는 여기 모든 가중치에

784
00:34:38,120 --> 00:34:41,860
업데이트를 적용하고 메모리를 제로화한다고 상상할 수 있습니다.

785
00:34:41,860 --> 00:34:46,620
우리가 추적하는 유일한 것은 바로 이 기울기입니다.

786
00:34:46,620 --> 00:34:49,440
그래서 기울기를 가중치에 적용하는

787
00:34:49,440 --> 00:34:51,550
단계에서 학습률과 최적화 도구에

788
00:34:51,550 --> 00:34:53,800
따라 모든 기울기를 적용할 수

789
00:34:53,800 --> 00:34:54,500
있습니다.

790
00:34:54,500 --> 00:34:56,739
그런 다음 다음 배치를 계산하는 것으로 넘어갑니다.

791
00:34:56,739 --> 00:34:59,020
이것이 완벽한 계산이 아닌 이유는

792
00:34:59,020 --> 00:35:03,380
모든 것을 한 번에 계산하는 것이 아니라 독립적으로 계산하고

793
00:35:03,380 --> 00:35:04,340
있기 때문입니다.

794
00:35:04,340 --> 00:35:07,380
그래서 한 번에 하나의 업데이트가 아니라

795
00:35:07,380 --> 00:35:10,620
세 가지 다른 업데이트가 있습니다.

796
00:35:10,620 --> 00:35:14,620
하지만 여전히 각 단계에서 기울기를 계산해야 합니다.

797
00:35:14,620 --> 00:35:16,340
메모리에 하나의 것을 유지하는데,

798
00:35:16,340 --> 00:35:19,670
그것은 이 숨겨진 상태가 배치에서 첫 번째 것이라는 것입니다.

799
00:35:22,220 --> 00:35:24,580
여기서 손실을 결정하기 위해 숨겨진

800
00:35:24,580 --> 00:35:26,920
상태를 어떻게 업데이트할 수 있습니까?

801
00:35:26,920 --> 00:35:29,460
그리고 나머지는 모두 버립니다.

802
00:35:29,460 --> 00:35:31,500
그래서 메모리에 가중치가 있습니다.

803
00:35:31,500 --> 00:35:34,580
기울기를 적용할 수 있고, 학습률을 곱한

804
00:35:34,580 --> 00:35:36,500
후 가중치에 적용합니다.

805
00:35:36,500 --> 00:35:41,080
분산 학습을 하면 비슷한 것을 볼 수 있습니다.

806
00:35:41,080 --> 00:35:43,740
각 GPU에서 독립적으로

807
00:35:43,740 --> 00:35:47,140
계산된 기울기를 모두 동일한 가중치

808
00:35:47,140 --> 00:35:49,320
집합에 적용합니다.

809
00:35:49,320 --> 00:35:52,720
분산 학습에 대한 강의가 곧 있을 것 같습니다.

810
00:35:52,720 --> 00:35:55,320
모두 같은 메모리에서 동시에

811
00:35:55,320 --> 00:35:57,460
추적하지 않고, 가중치에

812
00:35:57,460 --> 00:36:01,140
하나씩 적용하는 비슷한 상황입니다.

813
00:36:01,140 --> 00:36:03,598
모두 메모리에 맞출 수 있다면 더 좋을 것입니다.

814
00:36:03,598 --> 00:36:06,057
모두 메모리에 맞출 수 있다면 더 좋을 것입니다.

815
00:36:06,057 --> 00:36:07,160
이것은 주로--

816
00:36:07,160 --> 00:36:09,340
이 경우에는 본질적으로 동일합니다.

817
00:36:09,340 --> 00:36:11,400
하지만 이 설정에서는 정보를 명시적으로

818
00:36:11,400 --> 00:36:14,080
잃고 있다는 것이 더 명확할 수 있습니다.

819
00:36:14,080 --> 00:36:19,600
여기서는 한 번에 일부 출력만 보고 있습니다.

820
00:36:19,600 --> 00:36:22,760
계산할 때 모든 손실 집합을 보지

821
00:36:22,760 --> 00:36:26,640
않는 것이 정말 명확합니다. 각 시간

822
00:36:26,640 --> 00:36:28,520
단계에서 손실이

823
00:36:28,520 --> 00:36:30,020
있기 때문입니다.

824
00:36:30,020 --> 00:36:32,620
여기서 정보를 잃지만, 이

825
00:36:32,620 --> 00:36:34,880
경우에는 정보를 잃지 않습니다.

826
00:36:34,880 --> 00:36:37,240
전체 RNN을 슬라이드에 맞출 수

827
00:36:37,240 --> 00:36:39,560
없는 또 다른 실용적인 예는 문자

828
00:36:39,560 --> 00:36:42,040
수준 언어 모델의 아이디어입니다.

829
00:36:42,040 --> 00:36:43,960
10년 전에는 꽤

830
00:36:43,960 --> 00:36:48,480
효과적이라는 것이 정말 재미있습니다.

831
00:36:48,480 --> 00:36:50,160
현재의 언어 모델의

832
00:36:50,160 --> 00:36:53,150
물결이 RNN으로 문자를 예측하는 이

833
00:36:53,150 --> 00:36:56,230
매우 간단한 접근 방식의 축적이라는 것을

834
00:36:56,230 --> 00:36:58,830
볼 수 있어 정말 재미있습니다.

835
00:36:58,830 --> 00:37:01,330
보통 이런 모델을 만들면

836
00:37:01,330 --> 00:37:05,070
문자를 입력하고, 이를 원-핫

837
00:37:05,070 --> 00:37:10,750
인코딩이라고 부르며, 벡터의 한 위치에 1이 있고

838
00:37:10,750 --> 00:37:12,750
나머지 위치에는

839
00:37:12,750 --> 00:37:14,370
0이 있습니다.

840
00:37:14,370 --> 00:37:16,790
여기서 인덱스입니다.

841
00:37:16,790 --> 00:37:18,910
이것을 인덱스로 인코딩할 수

842
00:37:18,910 --> 00:37:22,010
있고, 이를 입력으로 사용할 수 있습니다.

843
00:37:22,010 --> 00:37:23,630
이전 숨겨진 층과

844
00:37:23,630 --> 00:37:25,510
현재 입력을 기반으로

845
00:37:25,510 --> 00:37:28,550
숨겨진 층을 계산할 수 있습니다.

846
00:37:28,550 --> 00:37:30,510
출력층도 동일하게,

847
00:37:30,510 --> 00:37:33,630
여기서 해당 올바른 값에 대한

848
00:37:33,630 --> 00:37:37,390
출력을 볼 수 있습니다. 이는 다음 시간

849
00:37:37,390 --> 00:37:39,290
단계로 취해집니다.

850
00:37:39,290 --> 00:37:42,590
예를 들어 출력이 e가 되기를 원하며,

851
00:37:42,590 --> 00:37:45,012
이를 여기로 매핑하고,

852
00:37:45,012 --> 00:37:46,970
소프트맥스와 같은 것을 상상할

853
00:37:46,970 --> 00:37:50,050
수 있으며, 로짓이 있어 점수입니다.

854
00:37:50,050 --> 00:37:52,670
2.2는 4.1보다 낮습니다.

855
00:37:52,670 --> 00:37:57,290
그래서 네, 이 시간 단계에서 출력이

856
00:37:57,290 --> 00:37:59,290
그리 좋지 않을 수

857
00:37:59,290 --> 00:38:00,550
있습니다.

858
00:38:00,550 --> 00:38:03,650
그래서 이를 시간 단계별 분류 문제로 볼 수

859
00:38:03,650 --> 00:38:04,310
있습니다.

860
00:38:04,310 --> 00:38:09,210
일반적으로 이러한 언어 모델이 하는 것은

861
00:38:09,210 --> 00:38:14,490
소프트맥스를 기반으로 한 시간 단계별 분류입니다.

862
00:38:14,490 --> 00:38:17,410
테스트 시간에 기본 아이디어는 문자를

863
00:38:17,410 --> 00:38:19,668
한 번에 하나씩 샘플링하고

864
00:38:19,668 --> 00:38:21,210
모델에 다시

865
00:38:21,210 --> 00:38:25,170
피드하여 이전 시간 단계에서 생성한 것을 보게

866
00:38:25,170 --> 00:38:29,590
하는 것입니다. 이를 반복하여 단어를 생성합니다.

867
00:38:29,590 --> 00:38:36,930
문자 수준에서 작동하여 이 기본 언어 모델링 작업을 수행하는 RNN을 실제로

868
00:38:36,930 --> 00:38:39,110
만들 수 있으며,

869
00:38:39,110 --> 00:38:40,570
잘 작동합니다.

870
00:38:40,570 --> 00:38:44,570
하나 주목할 점은 이 입력 레이어와 관련하여,

871
00:38:44,570 --> 00:38:47,450
보통 우리는 모델에 원-핫 임베딩을

872
00:38:47,450 --> 00:38:49,110
실제로 입력하지

873
00:38:49,110 --> 00:38:52,310
않고, 대신 임베딩 레이어라는 것을

874
00:38:52,310 --> 00:38:57,310
사용한다는 것입니다. 이는 본질적으로 거대한 행렬입니다.

875
00:38:57,310 --> 00:39:00,290
D는 모델에 대한 서로 다른

876
00:39:00,290 --> 00:39:01,710
입력의 수입니다.

877
00:39:01,710 --> 00:39:04,390
당신이 하는 것은 이 행렬 곱셈을

878
00:39:04,390 --> 00:39:08,165
상상할 수 있는데, 여기서 우리는 입력

879
00:39:08,165 --> 00:39:09,790
샘플에 따라 임베딩

880
00:39:09,790 --> 00:39:14,410
행렬의 첫 번째 행 또는 두 번째 행을 가져옵니다.

881
00:39:14,410 --> 00:39:17,710
그리고 우리는 이것을 행렬 곱셈으로 사용합니다.

882
00:39:17,710 --> 00:39:19,972
사실 이건 틀렸습니다. 이건 더

883
00:39:19,972 --> 00:39:21,430
높은 확률이어야 합니다.

884
00:39:23,737 --> 00:39:26,070
재미있게도, 우리는 이 슬라이드를 몇 년

885
00:39:26,070 --> 00:39:29,350
동안 가지고 있었고 아무도 눈치채지 못한 것 같습니다.

886
00:39:29,350 --> 00:39:34,350
어쨌든, 여기 E가 우리의 목표 문자입니다.

887
00:39:34,350 --> 00:39:37,168
그래서 이 경우, 모델이 실제로 잘못하고 있다는 점에서 당신이

888
00:39:37,168 --> 00:39:38,710
맞습니다. 우리는 이 시간 단계에

889
00:39:38,710 --> 00:39:40,585
대해 강하게 패널티를 주고 싶습니다.

890
00:39:40,585 --> 00:39:43,150
네, 좋은 질문입니다.

891
00:39:43,150 --> 00:39:45,990
네, 이 구현의 좋은 점 중 하나는 정말

892
00:39:45,990 --> 00:39:47,230
간단하다는 것입니다.

893
00:39:47,230 --> 00:39:49,710
파이썬 코드가 112줄이고, 다양한

894
00:39:49,710 --> 00:39:52,870
작업에 대해 이러한 모델을 훈련할 수 있습니다.

895
00:39:52,870 --> 00:39:56,150
이것은 당신이 할 수 있었던 사전 LLM 시대입니다.

896
00:39:56,150 --> 00:39:58,590
윌리엄 셰익스피어의 소네트에 대해 훈련할 수 있습니다.

897
00:39:58,590 --> 00:40:00,410
그리고 제가 언급했듯이, 이

898
00:40:00,410 --> 00:40:03,330
과정의 전 강사인 안드레이 카르파티가 2015년에 쓴

899
00:40:03,330 --> 00:40:07,850
블로그 포스트가 있습니다. 이 포스트는 RNN이 텍스트 생성에서 얼마나

900
00:40:07,850 --> 00:40:09,930
비합리적으로 효과적인지를 다루고 있습니다.

901
00:40:09,930 --> 00:40:10,430
네?

902
00:40:10,430 --> 00:40:14,570
임베딩 레이어를 사용하는 이유를 다시 설명해 주실 수 있나요?

903
00:40:14,570 --> 00:40:16,610
네, 임베딩 레이어의 기본 아이디어는

904
00:40:16,610 --> 00:40:19,290
일반적으로 모델에 대한 입력으로 벡터를 사용하는

905
00:40:19,290 --> 00:40:20,620
것이 더 낫다는 것입니다.

906
00:40:20,620 --> 00:40:22,870
그리고 이러한 임베딩 레이어가 무엇인지 배울 수 있습니다.

907
00:40:22,870 --> 00:40:26,810
우리는 일반적으로 이러한 것을 학습할 때 분산된

908
00:40:26,810 --> 00:40:28,540
가중치를 선호합니다.

909
00:40:28,540 --> 00:40:30,290
그래서 임베딩 레이어를

910
00:40:30,290 --> 00:40:34,217
Kaiming 초기화와 같은 매우 작은 제로 값으로

911
00:40:34,217 --> 00:40:35,550
초기화할 수 있습니다.

912
00:40:35,550 --> 00:40:37,050
그런 다음 입력

913
00:40:37,050 --> 00:40:38,730
벡터로서 한 번에 하나의

914
00:40:38,730 --> 00:40:41,890
행만 보고, 숫자로 입력하는 대신,

915
00:40:41,890 --> 00:40:43,930
기본적으로 많은 제로가 있는

916
00:40:43,930 --> 00:40:47,830
하나로 표현해야 합니다. 최적화는 임베딩이 더 잘

917
00:40:47,830 --> 00:40:48,610
작동합니다.

918
00:40:53,415 --> 00:40:55,790
그래서 네, 112줄의 파이썬 코드로 할 수

919
00:40:55,790 --> 00:40:56,950
있습니다. 꽤 멋지죠.

920
00:40:56,950 --> 00:40:59,070
윌리엄 셰익스피어의 소네트에 대해 훈련할 수

921
00:40:59,070 --> 00:41:00,998
있으며, 실제로 합리적인 텍스트를 출력합니다.

922
00:41:00,998 --> 00:41:02,290
몇 가지 예를 살펴보겠습니다.

923
00:41:02,290 --> 00:41:03,990
재미있는 점 중 하나는 모델을

924
00:41:03,990 --> 00:41:05,448
더 훈련할수록 점점 더

925
00:41:05,448 --> 00:41:06,910
일관성이 생긴다는 것입니다.

926
00:41:06,910 --> 00:41:09,190
처음에는 W에 대한 적절한 값을 배우지 못했기

927
00:41:09,190 --> 00:41:11,750
때문에 기본적으로 그냥 의미 없는 말입니다.

928
00:41:11,750 --> 00:41:15,230
그리고 더 많이 훈련할수록, III 단계처럼

929
00:41:15,230 --> 00:41:17,790
보이게 되고, 적어도 일부 단어는

930
00:41:17,790 --> 00:41:19,085
영어처럼 보입니다.

931
00:41:19,085 --> 00:41:20,710
그리고 더 훈련하면

932
00:41:20,710 --> 00:41:22,750
실제로 잘 작동하기

933
00:41:22,750 --> 00:41:25,230
시작합니다. 이는 AI 시대에 다가올

934
00:41:25,230 --> 00:41:30,310
것에 대한 약간의 예고편이었던 것 같습니다. 꽤 멋지죠.

935
00:41:30,310 --> 00:41:32,550
전체적으로 스타일에 대한 것,

936
00:41:32,550 --> 00:41:36,510
누군가의 이름을 어떻게 가져야 하는지, 그리고 그럴듯하게

937
00:41:36,510 --> 00:41:40,435
보이는 것에 대해 배우는 것을 볼 수 있습니다.

938
00:41:40,435 --> 00:41:41,810
더 많이 생성할수록

939
00:41:41,810 --> 00:41:43,352
점점 덜 의미가

940
00:41:43,352 --> 00:41:45,930
통하지만, 보는 것은 꽤 멋집니다.

941
00:41:45,930 --> 00:41:50,110
코드에 대해 훈련할 수 있습니다. 이 예에서는 리눅스에서

942
00:41:50,110 --> 00:41:51,480
훈련했다고 생각합니다.

943
00:41:51,480 --> 00:41:53,230
리눅스의 소스 코드가 있습니다.

944
00:41:53,230 --> 00:41:55,188
이러한 문자 수준 RNN 중 하나를

945
00:41:55,188 --> 00:41:59,332
훈련했으며, C 코드를 생성하는 것을 볼 수 있습니다. 꽤 좋아 보입니다.

946
00:41:59,332 --> 00:42:00,790
이게 컴파일될지는

947
00:42:00,790 --> 00:42:03,410
모르겠지만, 보기에는 합리적으로 보입니다.

948
00:42:03,410 --> 00:42:07,950
그리고 이 아이디어는 지난 몇 년 동안 정말 인기를 끌었습니다.

949
00:42:07,950 --> 00:42:12,570
그러니까, 여러분 모두 아시겠지만, 많은 분들이 컴퓨터 과학이나

950
00:42:12,570 --> 00:42:14,850
코딩 분야에서 일하시거나 이 분야의

951
00:42:14,850 --> 00:42:18,370
학생들이기 때문에, 이제는 이러한 언어 모델을

952
00:42:18,370 --> 00:42:20,410
위한 다양한 프로그래밍 도구가

953
00:42:20,410 --> 00:42:22,770
있습니다. 이 모델들은 본질적으로

954
00:42:22,770 --> 00:42:26,850
기존 코드의 많은 훈련 데이터를 소비하여 유사한 작업에

955
00:42:26,850 --> 00:42:29,513
대해 훈련되었습니다. 다음 문자를 예측하려고

956
00:42:29,513 --> 00:42:30,930
하는 대신, 다음

957
00:42:30,930 --> 00:42:33,950
토큰, 즉 문자 그룹을 예측하려고 합니다.

958
00:42:33,950 --> 00:42:35,950
토큰을 정의하는 방법은 모델에 따라 다르며,

959
00:42:35,950 --> 00:42:37,670
그에 대한 많은 세부 사항이 있습니다.

960
00:42:37,670 --> 00:42:39,430
하지만 높은 수준에서 보면, 정말

961
00:42:39,430 --> 00:42:41,263
비슷한 작업입니다. 그들은 단지 문자

962
00:42:41,263 --> 00:42:43,113
그룹을 순차적으로 예측하고 있습니다.

963
00:42:43,113 --> 00:42:45,030
최근 몇 년 동안 이러한 기존 도구들

964
00:42:45,030 --> 00:42:46,850
덕분에 폭발적으로 증가했습니다.

965
00:42:46,850 --> 00:42:47,350
네?

966
00:42:51,030 --> 00:42:52,650
모델에 대한 입력은 무엇인가요?

967
00:42:52,650 --> 00:42:54,030
트리거 같은 건가요?

968
00:42:54,030 --> 00:42:55,530
아, 이거에 대한 건가요?

969
00:42:55,530 --> 00:42:56,030
네.

970
00:42:56,030 --> 00:42:58,990
입력을 무작위 문자로

971
00:42:58,990 --> 00:43:01,450
시작할 수 있습니다.

972
00:43:01,450 --> 00:43:03,150
그렇게 하는 방법일 수

973
00:43:03,150 --> 00:43:05,230
있지만, 초기 입력이 필요합니다.

974
00:43:05,230 --> 00:43:08,550
언어 모델에는 일반적으로 미리

975
00:43:08,550 --> 00:43:11,353
정해진 시작 토큰이 있습니다.

976
00:43:11,353 --> 00:43:13,770
이것은 항상 시퀀스의 시작 부분에서 볼 수 있는 것입니다.

977
00:43:13,770 --> 00:43:15,490
그래서 RNN과 비슷한 방식으로 할 수 있습니다.

978
00:43:15,490 --> 00:43:17,573
이 정확한 시나리오에서 그들이 무엇을 했는지는 모르겠습니다.

979
00:43:17,573 --> 00:43:20,288
아마도 그들은 단순히 문자를 사용했을 수도 있지만, 알기 어렵습니다.

980
00:43:20,288 --> 00:43:21,830
그래서 질문은 언어 모델에서 레이블링은

981
00:43:21,830 --> 00:43:22,930
어떻게 작동하나요?

982
00:43:22,930 --> 00:43:25,750
이 순수 언어 모델의 멋진 점은 그들이 단지

983
00:43:25,750 --> 00:43:28,250
다음 토큰을 예측하는 것뿐이라는 것입니다.

984
00:43:28,250 --> 00:43:29,470
레이블을 붙일 필요가 없고,

985
00:43:29,470 --> 00:43:30,970
많은 텍스트만 제공하면 됩니다.

986
00:43:30,970 --> 00:43:32,490
이 모델들이 그렇게 좋은 이유는

987
00:43:32,490 --> 00:43:35,950
인터넷에서 본질적으로 모든 사용 가능한 텍스트를 긁어모으고,

988
00:43:35,950 --> 00:43:38,610
그 모든 것에 대해 모델을 훈련시키기 때문입니다.

989
00:43:38,610 --> 00:43:40,848
그래서 그들이 그렇게 좋은 이유입니다.

990
00:43:40,848 --> 00:43:42,890
그냥 다음 토큰을 생성하고, 레이블을 붙일

991
00:43:42,890 --> 00:43:44,182
필요가 없기 때문입니다.

992
00:43:44,182 --> 00:43:46,610
그래서 언어 모델이 그렇게 좋은 이유입니다.

993
00:43:46,610 --> 00:43:49,170
질문은, 매 시간 단계에서

994
00:43:49,170 --> 00:43:53,130
항상 최대 확률 출력을 취한다면, 우리는 항상

995
00:43:53,130 --> 00:43:56,577
같은 것을 반복해서 생성하게 될 것입니다.

996
00:43:56,577 --> 00:43:57,910
그리고 대답은 실제로 그렇습니다.

997
00:43:57,910 --> 00:44:01,490
그래서 최대 확률을 취하면, 이 예시는

998
00:44:01,490 --> 00:44:03,790
그리 좋지 않습니다.

999
00:44:03,790 --> 00:44:05,770
하지만 확률이 여기서 정확하다고 가정하고,

1000
00:44:05,770 --> 00:44:08,270
매 시간 단계에서 최대 확률을 취한다고 상상해 보세요.

1001
00:44:08,270 --> 00:44:10,062
같은 입력에 대해 항상 같은 출력을

1002
00:44:10,062 --> 00:44:11,070
얻게 될 것입니다.

1003
00:44:11,070 --> 00:44:12,570
실제로 사람들이 하는 것은,

1004
00:44:12,570 --> 00:44:14,693
이것은 탐욕적 디코딩이라고 불리며, 항상

1005
00:44:14,693 --> 00:44:16,610
최대 확률을 선택하는 것이 아닙니다.

1006
00:44:16,610 --> 00:44:18,967
실제로는, 그들은 소프트맥스에서 출력된

1007
00:44:18,967 --> 00:44:21,050
확률에 의해 주어진 분포를 기반으로

1008
00:44:21,050 --> 00:44:22,107
샘플링합니다.

1009
00:44:22,107 --> 00:44:23,690
그래서 최대 확률을

1010
00:44:23,690 --> 00:44:28,790
선택하지 않고, 이 경우에는 0.84 확률을 선택하거나 다른

1011
00:44:28,790 --> 00:44:32,710
출력 변수에 대해 0.13 확률을 선택합니다.

1012
00:44:32,710 --> 00:44:34,910
그리고 각 시퀀스에 대해 그것을 실행합니다.

1013
00:44:34,910 --> 00:44:37,243
그리고 할 수 있는 방법이 여러 가지 있습니다.

1014
00:44:37,243 --> 00:44:38,210
미리 검색할 수 있습니다.

1015
00:44:38,210 --> 00:44:39,750
이것은 빔 검색이라고 불리며,

1016
00:44:39,750 --> 00:44:41,333
여러 가지를 시도하고 시퀀스에

1017
00:44:41,333 --> 00:44:43,930
대해 전체 확률이 가장 높은 것을 찾는 것입니다.

1018
00:44:43,930 --> 00:44:46,810
그래서 많은 연구가 진행 중입니다. 이러한 모델에서

1019
00:44:46,810 --> 00:44:48,490
샘플링하는 방법은 무엇인가요?

1020
00:44:48,490 --> 00:44:50,350
하지만 간단한 대답은 항상 가장

1021
00:44:50,350 --> 00:44:52,430
높은 확률을 선택하지 않는다는 것입니다.

1022
00:44:52,430 --> 00:44:55,390
네, 그래서 질문은 여러 개의 출력이 있는 경우,

1023
00:44:55,390 --> 00:44:57,670
매번 무언가를 출력하는지 아니면

1024
00:44:57,670 --> 00:44:59,410
여기서 볼 것이 있는지입니다.

1025
00:44:59,410 --> 00:45:01,310
실제로 계산을 절약하기 위해 사용되지

1026
00:45:01,310 --> 00:45:03,730
않는 것을 출력하고 싶지 않을 것입니다.

1027
00:45:03,730 --> 00:45:05,880
하지만 각 시간 단계에서 출력할 수 있습니다.

1028
00:45:05,880 --> 00:45:08,130
문제에 따라 흥미로울 수 있으며, 훈련

1029
00:45:08,130 --> 00:45:10,872
과정에서 출력이 수렴하고 있는지 이해하는

1030
00:45:10,872 --> 00:45:12,330
것이 중요할 수 있습니다.

1031
00:45:12,330 --> 00:45:13,163
그런 것 같습니다.

1032
00:45:13,163 --> 00:45:15,250
그래서 살펴보는 것이 유용할 수 있지만, 일반적으로

1033
00:45:15,250 --> 00:45:17,930
사람들은 계산을 절약하기 위해 그렇게 하지는 않을 것입니다.

1034
00:45:17,930 --> 00:45:19,630
하지만 실제로 유용할 수 있습니다.

1035
00:45:19,630 --> 00:45:21,370
모델이 작동하는 방식을 이해하는 데 도움이 될 수 있습니다.

1036
00:45:21,370 --> 00:45:23,590
정확한 답을 예측하는 데

1037
00:45:23,590 --> 00:45:27,630
도움이 되는 특정 트리거나 요소가 있을 수 있습니다.

1038
00:45:27,630 --> 00:45:30,730
좋은 질문입니다.

1039
00:45:30,730 --> 00:45:34,110
그래서 계속 진행하겠습니다.

1040
00:45:34,110 --> 00:45:36,090
우리는 이러한 RNN에 대해 이야기했으며, 문자를 생성하는

1041
00:45:36,090 --> 00:45:37,533
데 얼마나 좋은지에 대해 이야기했습니다.

1042
00:45:37,533 --> 00:45:39,450
우리는 그것들을 현대 코딩 도구와

1043
00:45:39,450 --> 00:45:41,050
연결했으며, 정말 멋집니다.

1044
00:45:41,050 --> 00:45:42,970
RNN의 또 다른 멋진 점은

1045
00:45:42,970 --> 00:45:46,905
활성화 값을 살펴볼 수 있으며, 때때로 모델이

1046
00:45:46,905 --> 00:45:48,530
추적하는 것에 대해 흥미로운

1047
00:45:48,530 --> 00:45:50,697
정보를 제공한다는 것입니다.

1048
00:45:50,697 --> 00:45:55,010
우리의 작은 장난감 예제에서 출력 활성화를 살펴보았고,

1049
00:45:55,010 --> 00:45:57,810
현재 값과 이전 값을 볼 수

1050
00:45:57,810 --> 00:45:58,310
있습니다.

1051
00:45:58,310 --> 00:46:02,090
그것이 RNN 상태 또는 셀에서 추적하는 것이었습니다.

1052
00:46:02,090 --> 00:46:05,722
또한 기본적으로 여기에 시퀀스를 제공할 수 있습니다.

1053
00:46:05,722 --> 00:46:07,430
슬라이드에서 보여줄

1054
00:46:07,430 --> 00:46:11,970
모델은 tanh 활성화를 사용하고 있으며, 이는 1에서

1055
00:46:11,970 --> 00:46:16,090
-1까지이며, -1은 여기서 빨간색으로 시각화되고,

1056
00:46:16,090 --> 00:46:18,450
1에 가까운 것은 파란색입니다.

1057
00:46:18,450 --> 00:46:20,050
여기에서 전체 스펙트럼을 얻습니다.

1058
00:46:20,050 --> 00:46:21,730
각 문자가 들어올 때,

1059
00:46:21,730 --> 00:46:23,250
그 시간 단계에서 해당

1060
00:46:23,250 --> 00:46:26,500
셀의 활성화는 무엇인지 살펴볼 수 있습니다.

1061
00:46:26,500 --> 00:46:28,750
그래서 그들이 이 플롯을 색상 코드화하는 방식입니다.

1062
00:46:28,750 --> 00:46:30,470
이것은 실제로 아무것도 보여주지 않으며, 무작위입니다.

1063
00:46:30,470 --> 00:46:32,070
많은 것들이 해석할 수 없을 것입니다.

1064
00:46:32,070 --> 00:46:34,750
하지만 일부는 추적할 수 있는 꽤 멋진 것들이 있습니다.

1065
00:46:34,750 --> 00:46:37,410
예를 들어, 이것은 인용 탐지기이며,

1066
00:46:37,410 --> 00:46:40,110
인용이 시작되면 기본적으로

1067
00:46:40,110 --> 00:46:42,410
켜지고 인용이 끝나면 꺼집니다.

1068
00:46:42,410 --> 00:46:44,610
그래서 기본적으로 이것은 RNN 추적의 일부로,

1069
00:46:44,610 --> 00:46:46,750
어느 시점에 끝 인용부호가 필요합니다.

1070
00:46:46,750 --> 00:46:50,550
그리고 그것을 언제 넣을지는 모델이 알아내려고

1071
00:46:50,550 --> 00:46:53,190
하는 것이지만, 추적하고 있습니다.

1072
00:46:53,190 --> 00:46:57,650
또 다른 멋진 것은 줄 길이 추적 셀입니다.

1073
00:46:57,650 --> 00:47:02,930
그래서 매우 높은 값에서 시작합니다.

1074
00:47:02,930 --> 00:47:05,010
그리고 모델이 새 줄 문자가 있을

1075
00:47:05,010 --> 00:47:06,510
것이라고 생각하는 곳에

1076
00:47:06,510 --> 00:47:08,190
가까워질수록 매우 낮은 값이 됩니다.

1077
00:47:08,190 --> 00:47:13,370
그래서 이것은 다른 값을 보는 방법으로도 멋집니다.

1078
00:47:13,370 --> 00:47:15,430
그리고 이것들은 우리가 보고

1079
00:47:15,430 --> 00:47:18,910
있는 이 모델의 레이어에서 단일 활성화이며,

1080
00:47:18,910 --> 00:47:20,690
각 문자에 매핑됩니다.

1081
00:47:20,690 --> 00:47:23,270
그래서 매우 해석 가능합니다.

1082
00:47:23,270 --> 00:47:25,450
그들은 이 if 문 셀을 가지고 있습니다.

1083
00:47:25,450 --> 00:47:27,150
그래서 if 문 내의 모든 것이

1084
00:47:27,150 --> 00:47:29,670
여기에서 추적되고 있으며, 이것도 꽤 멋집니다.

1085
00:47:29,670 --> 00:47:33,290
그리고 인용부호나 주석을 감지하는 것과 같은

1086
00:47:33,290 --> 00:47:36,850
것들도 필요합니다. 주석 끝 문자를 출력해야

1087
00:47:36,850 --> 00:47:37,670
하니까요.

1088
00:47:37,670 --> 00:47:39,750
그래서 이것은 추적해야 할 것입니다.

1089
00:47:39,750 --> 00:47:42,310
그래서 당신은 이 멋진 해석 가능한 셀도 가지고 있습니다.

1090
00:47:42,310 --> 00:47:44,710
그리고 마지막으로 이 코드 깊이 셀입니다.

1091
00:47:44,710 --> 00:47:47,870
코드에 중첩이 있을 때,

1092
00:47:47,870 --> 00:47:51,610
각 시간 단계마다,

1093
00:47:51,610 --> 00:47:53,690
각 들여쓰기

1094
00:47:53,690 --> 00:47:58,170
단계마다 더 많이 활성화됩니다.

1095
00:47:58,170 --> 00:47:59,750
그래서 네, 이것은 꽤 멋집니다.

1096
00:47:59,750 --> 00:48:01,530
실제로 활성화를 보고 입력에

1097
00:48:01,530 --> 00:48:03,130
직접 매핑할 수 있으며,

1098
00:48:03,130 --> 00:48:06,770
특별한 트릭이 필요하지 않습니다. 이것은 RNN의 일부

1099
00:48:06,770 --> 00:48:08,490
숨겨진 상태가 얼마나 해석

1100
00:48:08,490 --> 00:48:10,953
가능한지를 생각하면 정말 놀랍습니다.

1101
00:48:10,953 --> 00:48:13,370
실제로 우리가 수동으로 할당하던

1102
00:48:13,370 --> 00:48:14,830
것과 다소 유사합니다.

1103
00:48:14,830 --> 00:48:19,410
하지만 RNN은 내부적으로 매우 유사한 과정을 수행하고 있습니다.

1104
00:48:19,410 --> 00:48:22,050
좋습니다, 이제 RNN을 사용하고

1105
00:48:22,050 --> 00:48:25,530
싶을 때의 트레이드오프에 대해 이야기하겠습니다.

1106
00:48:25,530 --> 00:48:29,390
좋은 점은 어떤 길이의 입력도 처리할 수 있다는 것입니다.

1107
00:48:29,390 --> 00:48:31,563
그래서 현대의 많은 언어 모델은

1108
00:48:31,563 --> 00:48:33,230
변환기에 의존하며,

1109
00:48:33,230 --> 00:48:36,070
최대 컨텍스트 길이 또는 컨텍스트 창이라고

1110
00:48:36,070 --> 00:48:37,510
하는 것이 있습니다.

1111
00:48:37,510 --> 00:48:38,530
RNN은 이것이 없습니다.

1112
00:48:38,530 --> 00:48:40,890
본질적으로 무한 길이의 시퀀스를 받아들일 수

1113
00:48:40,890 --> 00:48:43,710
있습니다. 모델을 계속 실행할 수 있는 한 말이죠.

1114
00:48:43,710 --> 00:48:46,350
그래서 컨텍스트 길이 제한이 없습니다.

1115
00:48:46,350 --> 00:48:50,030
시간 단계 t에 대한 계산은 이론적으로 숨겨진 상태에

1116
00:48:50,030 --> 00:48:52,190
캡처된 경우 여러 단계

1117
00:48:52,190 --> 00:48:54,470
뒤의 정보를 사용할 수 있습니다.

1118
00:48:54,470 --> 00:48:56,430
모델이 입력 시퀀스의 모든 동적을

1119
00:48:56,430 --> 00:48:58,030
숨겨진 상태에서 효과적으로

1120
00:48:58,030 --> 00:48:59,670
캡처하고 있다면, 이론적으로

1121
00:48:59,670 --> 00:49:02,350
매우 오래된 값도 사용할 수 있습니다.

1122
00:49:02,350 --> 00:49:04,750
하지만 실제로는 이와 관련된 몇 가지

1123
00:49:04,750 --> 00:49:08,990
문제가 있을 수 있으며, 그에 대한 세부 사항을 다룰 것입니다.

1124
00:49:08,990 --> 00:49:13,490
또한, 모델 크기는 더 긴 입력에 대해 증가하지 않습니다.

1125
00:49:13,490 --> 00:49:16,230
그래서 각 입력 시간 단계마다

1126
00:49:16,230 --> 00:49:20,170
다른 레이어가 있다고 가정해 보겠습니다.

1127
00:49:20,170 --> 00:49:22,230
이 문제는 없으므로 좋습니다.

1128
00:49:22,230 --> 00:49:25,130
그리고 각 시간 단계에서 동일한 가중치를 적용하고 있습니다.

1129
00:49:25,130 --> 00:49:27,325
기본적으로, 출력을 계산하는 업데이트

1130
00:49:27,325 --> 00:49:29,950
규칙은 매번 동일하다는 것을 알고 있습니다.

1131
00:49:29,950 --> 00:49:32,848
여기에는 멋진 대칭성이 있습니다.

1132
00:49:32,848 --> 00:49:35,390
문제를 개념적으로 생각할 때, 매

1133
00:49:35,390 --> 00:49:36,290
시간

1134
00:49:36,290 --> 00:49:38,930
단계마다 항상 같은 작업을 수행하는데,

1135
00:49:38,930 --> 00:49:42,570
이는 개념적으로 좋고 구현에도 도움이 됩니다.

1136
00:49:42,570 --> 00:49:44,270
주요 단점은 무엇인가요?

1137
00:49:44,270 --> 00:49:48,690
다음 숨겨진 상태를 계산하기 위해 매번 이전 숨겨진

1138
00:49:48,690 --> 00:49:50,970
상태를 계산해야 합니다.

1139
00:49:50,970 --> 00:49:55,830
필요하다면 느릴 수 있습니다.

1140
00:49:55,830 --> 00:49:58,930
각 숨겨진 상태는 이전 상태들에

1141
00:49:58,930 --> 00:50:02,410
의존하여 결정되므로, 이 재귀 계산은 실제로

1142
00:50:02,410 --> 00:50:05,550
많은 시간을 소모할 수 있습니다.

1143
00:50:05,550 --> 00:50:08,690
추론 시간에는 문제가

1144
00:50:08,690 --> 00:50:10,750
되지 않지만,

1145
00:50:10,750 --> 00:50:12,570
훈련

1146
00:50:12,570 --> 00:50:16,150
시간에는 손실을 계산하기

1147
00:50:16,150 --> 00:50:17,970
위해 이전

1148
00:50:17,970 --> 00:50:21,890
숨겨진 상태를 계산해야 하므로

1149
00:50:21,890 --> 00:50:24,470
모든 것을

1150
00:50:24,470 --> 00:50:27,440
배치하기가 어렵습니다.

1151
00:50:27,440 --> 00:50:32,400
이는 많은 데이터로 확장하는 데 어려움을 초래할 수 있습니다.

1152
00:50:32,400 --> 00:50:34,800
실제로는 고정 크기의 숨겨진

1153
00:50:34,800 --> 00:50:37,400
상태를 가지고 있기 때문에

1154
00:50:37,400 --> 00:50:40,120
여러 시간 단계 뒤의

1155
00:50:40,120 --> 00:50:42,730
정보를 접근하기가 어렵습니다.

1156
00:50:42,730 --> 00:50:44,480
따라서 시퀀스가 길어질수록

1157
00:50:44,480 --> 00:50:46,270
일부 정보를 잃게 됩니다.

1158
00:50:49,160 --> 00:50:52,560
좋습니다, RNN이 성공을 거둔 컴퓨터 비전과

1159
00:50:52,560 --> 00:50:56,200
관련된 몇 가지 응용 프로그램에 대해 이야기하겠습니다.

1160
00:50:56,200 --> 00:50:58,103
그 중 하나는 우리가 이야기한 이미지

1161
00:50:58,103 --> 00:50:59,020
캡셔닝입니다.

1162
00:50:59,020 --> 00:51:02,040
여기서 기본적인 것은 시퀀스를

1163
00:51:02,040 --> 00:51:05,000
시작하는 시작 토큰 또는 시작 문자가

1164
00:51:05,000 --> 00:51:06,300
있다는 것입니다.

1165
00:51:06,300 --> 00:51:08,800
그리고 종료 문자나 종료 토큰이 있을

1166
00:51:08,800 --> 00:51:09,940
때 종료됩니다.

1167
00:51:09,940 --> 00:51:13,040
이 경우, 단어 수준의 토큰처럼 보입니다.

1168
00:51:13,040 --> 00:51:16,840
이런 모델을 가질 수 있습니다.

1169
00:51:16,840 --> 00:51:19,080
가장 기본적인 방법은

1170
00:51:19,080 --> 00:51:22,080
본질적으로 이미지를 인코딩하는 시각적

1171
00:51:22,080 --> 00:51:24,580
인코더인 CNN을 가지고, 이를

1172
00:51:24,580 --> 00:51:28,780
우리의 순환 신경망에 입력으로 사용하고 이전에

1173
00:51:28,780 --> 00:51:32,220
생성된 텍스트와 함께 사용하는 것입니다.

1174
00:51:32,220 --> 00:51:33,840
여기에는 두 단계가 있습니다.

1175
00:51:33,840 --> 00:51:36,180
그리고 CNN과 RNN을 어떻게 결합할 것인지

1176
00:51:36,180 --> 00:51:37,680
더 구체적으로 설명하겠습니다.

1177
00:51:37,680 --> 00:51:39,680
테스트 이미지를 상상해 보세요.

1178
00:51:39,680 --> 00:51:43,780
이미지가 들어오면, 모델은 위쪽의 첫 번째

1179
00:51:43,780 --> 00:51:45,580
레이어에서 시작하여

1180
00:51:45,580 --> 00:51:47,020
아래로 내려갑니다.

1181
00:51:47,020 --> 00:51:49,140
이것은 ImageNet에서 훈련된

1182
00:51:49,140 --> 00:51:51,420
것이라고 상상할 수 있습니다.

1183
00:51:51,420 --> 00:51:53,960
클래스 레이블은 사용하지 않지만, 마지막에서

1184
00:51:53,960 --> 00:51:56,120
두 번째 레이어를 사용할 것입니다.

1185
00:51:56,120 --> 00:51:58,220
이는 전이 학습에서 본

1186
00:51:58,220 --> 00:52:00,180
일반적인 전략이며,

1187
00:52:00,180 --> 00:52:04,420
이미지의 좋은 시각적 표현을 얻는 방법입니다.

1188
00:52:04,420 --> 00:52:06,820
따라서 마지막에서 두 번째 레이어를 사용합니다.

1189
00:52:06,820 --> 00:52:11,560
그리고 이제 이것을 우리의 숨겨진 상태에 대한 입력으로 사용할 수 있습니다.

1190
00:52:11,560 --> 00:52:13,660
이제 우리의 숨겨진

1191
00:52:13,660 --> 00:52:18,680
상태는 이 Wih 값의 함수이기도 합니다.

1192
00:52:18,680 --> 00:52:23,340
단순히 숨겨진 상태만 있는 것이 아니라, 여기서

1193
00:52:23,340 --> 00:52:27,100
시각적 요소도 추적하고 있습니다.

1194
00:52:27,100 --> 00:52:28,760
하지만 이는 과제에

1195
00:52:28,760 --> 00:52:31,860
포함되지 않으므로 너무 많은 시간을

1196
00:52:31,860 --> 00:52:33,640
할애하지 않겠습니다.

1197
00:52:33,640 --> 00:52:35,740
RNN이 역사적으로

1198
00:52:35,740 --> 00:52:37,820
CNN과 함께 어떻게

1199
00:52:37,820 --> 00:52:39,487
사용되었는지에 대한

1200
00:52:39,487 --> 00:52:41,680
감을 주기 위해서입니다.

1201
00:52:41,680 --> 00:52:44,480
우리는 샘플링 프로세스를 사용하여

1202
00:52:44,480 --> 00:52:46,720
각 시간 단계에서

1203
00:52:46,720 --> 00:52:48,220
토큰을 계산합니다.

1204
00:52:48,220 --> 00:52:50,745
우리는 종료 토큰이 있을 때 종료합니다.

1205
00:52:50,745 --> 00:52:52,120
종료 토큰을 샘플링할 때마다, 우리는

1206
00:52:52,120 --> 00:52:53,720
언제 끝내야 하는지를 알 수 있습니다.

1207
00:52:53,720 --> 00:52:56,180
그리고 이 모델들은 당시 매우 잘 작동했습니다.

1208
00:52:56,180 --> 00:52:58,860
그들은 많은 훌륭한 성공을 거두었다고 생각합니다.

1209
00:52:58,860 --> 00:53:02,040
여기에서 모델이 입력 이미지에 기반하여

1210
00:53:02,040 --> 00:53:05,400
매우 합리적인 캡션을 출력하는 많은

1211
00:53:05,400 --> 00:53:10,480
좋은 예를 볼 수 있지만, 모델이 많은 시나리오에서 어려움을

1212
00:53:10,480 --> 00:53:11,960
겪기도 했습니다.

1213
00:53:11,960 --> 00:53:16,520
이러한 많은 문제는 훈련 데이터에서 이러한 이미지가 일반적으로

1214
00:53:16,520 --> 00:53:19,300
나타나는 분포와 관련이 있습니다.

1215
00:53:19,300 --> 00:53:22,700
예를 들어, 누군가가 이렇게 손을 모아 무언가를

1216
00:53:22,700 --> 00:53:27,040
들고 있는 모습은 마우스를 잡는 모습과 매우 유사합니다.

1217
00:53:27,040 --> 00:53:31,860
하지만 분명히 그들이 들고 있는 것은 평평한 물체이므로 이것이 전화기라는
것을

1218
00:53:31,860 --> 00:53:34,880
알 수 있으며, 그들의 손은 아래가 아니라

1219
00:53:34,880 --> 00:53:36,080
위를 향하고 있습니다.

1220
00:53:36,080 --> 00:53:39,260
이것은 흥미롭게 보입니다.

1221
00:53:39,260 --> 00:53:41,380
또한, 그들은 여성이 털

1222
00:53:41,380 --> 00:53:46,700
옷을 입고 있는 모습에서 고양이를 들고 있다고 생각하는 것 같습니다.

1223
00:53:46,700 --> 00:53:49,760
그들은 해변을 보고 서핑 보드가 있다고 가정합니다.

1224
00:53:49,760 --> 00:53:51,520
이런 종류의 환각은 오늘날

1225
00:53:51,520 --> 00:53:54,260
비전 언어 모델에서 여전히 매우

1226
00:53:54,260 --> 00:53:58,340
흔하며, 특정 장면에서 일반적으로 존재하는 물체가 있다고

1227
00:53:58,340 --> 00:54:00,080
생각하지만, 당신이

1228
00:54:00,080 --> 00:54:03,140
보고 있는 특정 장면에는 존재하지 않습니다.

1229
00:54:03,140 --> 00:54:05,340
또한, 나무에 앉아 있는 새나 공을 던지는

1230
00:54:05,340 --> 00:54:07,960
것과 같은 것들이 있지만, 그는 공을 잡고 있습니다.

1231
00:54:07,960 --> 00:54:11,080
이 모든 것은 본질적으로 데이터 세트의 편향에 기반하고

1232
00:54:11,080 --> 00:54:14,340
있으며, 모델이 훈련 중에 이를 학습하면서 실제

1233
00:54:14,340 --> 00:54:16,140
이미지에서는 그렇지 않은 경우에도

1234
00:54:16,140 --> 00:54:17,900
특정 물체나 특정 행동이

1235
00:54:17,900 --> 00:54:20,800
수행되고 있다고 가장 가능성이 높다고 학습합니다.

1236
00:54:20,800 --> 00:54:25,160
따라서 데이터 세트에서는 이러한 행동이나 물체가 특정 장면과

1237
00:54:25,160 --> 00:54:27,577
높은 동시 발생을 보입니다.

1238
00:54:27,577 --> 00:54:29,160
모델은 이를 연관 짓는 법을 배우지만,

1239
00:54:29,160 --> 00:54:30,900
이를 분리하는 법은 배우지 않습니다.

1240
00:54:30,900 --> 00:54:33,828
이 장면에서 발생하는 이유는-- 이 장면에서는

1241
00:54:33,828 --> 00:54:36,120
장갑이 여기 있고, 공이 장갑으로 들어가고

1242
00:54:36,120 --> 00:54:38,780
있으므로 던지고 있지 않다는 것을 알고

1243
00:54:38,780 --> 00:54:40,723
있지만, 그렇게 설명해야 합니다.

1244
00:54:40,723 --> 00:54:42,140
그리고 이러한 모델을 훈련하는

1245
00:54:42,140 --> 00:54:44,180
방식은 캡션을 출력하도록 훈련하는 것입니다.

1246
00:54:44,180 --> 00:54:46,020
그래서 우리는 거기서 어떤

1247
00:54:46,020 --> 00:54:47,840
설명도 하지 않으며, 이것이

1248
00:54:47,840 --> 00:54:51,680
이러한 동시 발생 문제를 보는 이유 중 하나입니다.

1249
00:54:51,680 --> 00:54:55,300
비주얼 질문 응답을 위해, 이는 RNN이 사용된

1250
00:54:55,300 --> 00:54:57,820
또 다른 매우 일반적인 작업이며,

1251
00:54:57,820 --> 00:55:01,520
비주얼 질문 응답에 대해 일반적으로 사용된 두

1252
00:55:01,520 --> 00:55:03,220
가지 공식이 있습니다.

1253
00:55:03,220 --> 00:55:05,880
하나는 기본적으로 캡션 모델이 있다고

1254
00:55:05,880 --> 00:55:08,160
말하고, 그것이 질문에 얼마나

1255
00:55:08,160 --> 00:55:12,020
잘 대답할 수 있는지를 보고 싶어하는 것입니다.

1256
00:55:12,020 --> 00:55:14,658
할 수 있는 한 가지는 이 질문을 주고,

1257
00:55:14,658 --> 00:55:16,200
텍스트를 출력하게 한

1258
00:55:16,200 --> 00:55:19,520
다음 각 답변 시퀀스의 확률을 살펴보는 것입니다.

1259
00:55:19,520 --> 00:55:21,920
그래서 각 문자나 토큰에 대한 확률이

1260
00:55:21,920 --> 00:55:23,420
있으며, 이를 곱하여 전체

1261
00:55:23,420 --> 00:55:25,295
답변의 확률을 얻을 수 있습니다.

1262
00:55:25,295 --> 00:55:29,860
이것은 이러한 RNN 스타일 모델 중 하나를 사용하여 질문 응답을

1263
00:55:29,860 --> 00:55:31,940
수행하는 한 가지 방법입니다.

1264
00:55:31,940 --> 00:55:36,460
사람들이 더 일반적으로 사용한 방법은 기본적으로 모델에

1265
00:55:36,460 --> 00:55:40,260
대한 입력으로 질문을 주고, 여러 다른 답변을

1266
00:55:40,260 --> 00:55:42,180
모델에 대한 별도의 입력으로

1267
00:55:42,180 --> 00:55:43,577
주는 것입니다.

1268
00:55:43,577 --> 00:55:45,660
그런 다음 본질적으로 질문당 확률을

1269
00:55:45,660 --> 00:55:46,280
출력합니다.

1270
00:55:46,280 --> 00:55:50,020
이 경우, 네 가지 클래스가 있는 네 방향 분류기가 되어,

1271
00:55:50,020 --> 00:55:52,628
답변 1, 답변 2, 답변 3, 답변 4가

1272
00:55:52,628 --> 00:55:54,920
있으며, 확률을 출력하는 것입니다.

1273
00:55:54,920 --> 00:55:56,962
형식화할 수 있는 다양한 방법이

1274
00:55:56,962 --> 00:56:00,433
있지만, 이는 언어를 사용해야 하고 시퀀스

1275
00:56:00,433 --> 00:56:02,100
모델링이 도움이 되는

1276
00:56:02,100 --> 00:56:04,900
컴퓨터 비전에서 매우 일반적인 작업입니다.

1277
00:56:04,900 --> 00:56:06,940
또한 비주얼 대화.

1278
00:56:06,940 --> 00:56:10,520
당시에는 이 모든 것이 매우 별개의 작업으로 간주되었습니다.

1279
00:56:10,520 --> 00:56:13,580
요즘에는 거의 모든 작업을 수행할 수 있는 하나의

1280
00:56:13,580 --> 00:56:17,117
모델이 있지만, 이미지에 대해 어떻게 대화할 수 있을까요?

1281
00:56:17,117 --> 00:56:19,200
지난 2년 동안 이러한 종류의

1282
00:56:19,200 --> 00:56:22,960
모델의 능력이 폭발적으로 증가하는 것을 보았습니다.

1283
00:56:22,960 --> 00:56:27,800
RNN이 일반적으로 사용된 또 다른 유형의 모델은 이 비주얼

1284
00:56:27,800 --> 00:56:30,140
내비게이션 작업을 위한 것입니다.

1285
00:56:30,140 --> 00:56:33,800
그래서 여러분은 이러한 이미지가 들어오고, 2D

1286
00:56:33,800 --> 00:56:37,420
평면에서 이동할 방향의 시퀀스를 출력하고 싶어합니다.

1287
00:56:37,420 --> 00:56:40,942
목표 목적지에 어떻게 도달하나요?

1288
00:56:40,942 --> 00:56:42,400
이 시퀀스

1289
00:56:42,400 --> 00:56:48,480
모델이 사용된 또 다른 응용 프로그램이 있으니 알아두세요.

1290
00:56:48,480 --> 00:56:53,680
제가 이전에 명시적으로 언급하지 않았던 한 가지는,

1291
00:56:53,680 --> 00:56:56,440
다층 CNN이나 다층 밀집

1292
00:56:56,440 --> 00:57:02,600
또는 완전 연결 계층을 가질 수 있는 것과 같은 방식으로, 다층

1293
00:57:02,600 --> 00:57:06,060
RNN도 가질 수 있다는 점입니다.

1294
00:57:06,060 --> 00:57:07,920
실제로 제가 보여준 대부분의

1295
00:57:07,920 --> 00:57:10,280
RNN은 다층 RNN이었습니다.

1296
00:57:10,280 --> 00:57:16,280
주요 차이점은 각 층을 별도로 처리한다는 것입니다.

1297
00:57:16,280 --> 00:57:19,300
예를 들어, 1층의 은닉 상태는 1층의

1298
00:57:19,300 --> 00:57:21,300
이전 시간 단계의

1299
00:57:21,300 --> 00:57:23,220
은닉 상태에 의존합니다.

1300
00:57:23,220 --> 00:57:26,320
깊이 방향 차원에서는,

1301
00:57:26,320 --> 00:57:31,473
각 층에서 이전 시간 단계의 은닉

1302
00:57:31,473 --> 00:57:33,140
상태만을

1303
00:57:33,140 --> 00:57:35,280
보고 있습니다.

1304
00:57:35,280 --> 00:57:40,300
그리고 시간 차원에서 윈도우를 보는 측면에서, 첫 번째

1305
00:57:40,300 --> 00:57:43,060
층은 실제 입력 x를 입력으로

1306
00:57:43,060 --> 00:57:45,220
가지지만, 두 번째

1307
00:57:45,220 --> 00:57:49,600
층은 이전 층의 출력 y를 입력으로 사용합니다.

1308
00:57:49,600 --> 00:57:51,860
이렇게 쌓을 수 있으며,

1309
00:57:51,860 --> 00:57:57,700
각 층은 그 층의 이전 은닉 상태에 대해서만

1310
00:57:57,700 --> 00:58:00,200
작동하지만, 층 간의

1311
00:58:00,200 --> 00:58:02,140
입력 출력 전달도

1312
00:58:02,140 --> 00:58:03,460
있습니다.

1313
00:58:03,460 --> 00:58:06,580
따라서 이 오른쪽 상단 값을 계산하기

1314
00:58:06,580 --> 00:58:10,782
위해서는 이 전체 계산 그래프의 다양한 값,

1315
00:58:10,782 --> 00:58:13,240
다양한 은닉 상태를 미리

1316
00:58:13,240 --> 00:58:14,380
계산해야 합니다.

1317
00:58:14,380 --> 00:58:17,540
그래서 훈련을 시작하면 이 과정이

1318
00:58:17,540 --> 00:58:22,720
매우 복잡해지고 비효율적이라는 느낌을 받을 수 있습니다.

1319
00:58:22,720 --> 00:58:27,680
좋습니다, 제가 1990년대에 실제로 제안된 RNN의 주요

1320
00:58:27,680 --> 00:58:30,900
변형 중 하나에 대해 이야기하겠습니다.

1321
00:58:30,900 --> 00:58:34,400
이 변형은 LSTM이라고 하며, 변환기 혁명

1322
00:58:34,400 --> 00:58:37,400
이전에 상당한 성공을 거두었습니다.

1323
00:58:37,400 --> 00:58:41,500
LSTM의 작동 방식에 대한 세부 사항을 알

1324
00:58:41,500 --> 00:58:44,360
필요는 없지만, RNN이 LSTM이

1325
00:58:44,360 --> 00:58:49,300
완화하려고 하는 몇 가지 주요 단점이 있다는 것을

1326
00:58:49,300 --> 00:58:51,680
배우기를 바랍니다. 많은 현대

1327
00:58:51,680 --> 00:58:53,840
상태 공간 모델도 RNN이

1328
00:58:53,840 --> 00:58:57,640
직면하는 동일한 문제를 완화하려고 합니다.

1329
00:58:57,640 --> 00:59:01,720
기본적으로 tanh가 정말 일반적으로 사용되는

1330
00:59:01,720 --> 00:59:04,580
활성화 함수라는 것을 이야기했습니다.

1331
00:59:04,580 --> 00:59:06,560
그리고 이전 은닉 상태를

1332
00:59:06,560 --> 00:59:09,200
새로운 상태로 변환하는 Whh

1333
00:59:09,200 --> 00:59:11,920
행렬이 있다는 것도 이야기했습니다.

1334
00:59:11,920 --> 00:59:17,180
이것은 현재 시간 단계에서 입력 벡터 xt를

1335
00:59:17,180 --> 00:59:19,420
은닉 상태 차원으로

1336
00:59:19,420 --> 00:59:22,860
변환하는 Wxh 행렬과 합산됩니다.

1337
00:59:22,860 --> 00:59:28,320
이것을 이렇게 벡터를 쌓는 것으로

1338
00:59:28,320 --> 00:59:31,300
공식화할 수도 있습니다.

1339
00:59:31,300 --> 00:59:33,100
그래서 사람들은 때때로

1340
00:59:33,100 --> 00:59:37,540
이 두 개의 W를 결합하여 하나의 큰 W를 형성합니다.

1341
00:59:37,540 --> 00:59:42,700
하지만 이 두 블록은 대각선으로 배치되어 있다는 점에 유의해야

1342
00:59:42,700 --> 00:59:45,540
합니다. 이렇게 공식화하면 W에

1343
00:59:45,540 --> 00:59:48,280
많은 0이 존재합니다. 왜냐하면

1344
00:59:48,280 --> 00:59:50,200
Whh는 xt와 전혀

1345
00:59:50,200 --> 00:59:53,680
상호작용하지 않기 때문입니다. 하지만 이것은

1346
00:59:53,680 --> 00:59:55,700
생각하고 수학을 기록하는

1347
00:59:55,700 --> 00:59:57,820
것을 쉽게 하기 위한 약식

1348
00:59:57,820 --> 00:59:58,760
표기법입니다.

1349
00:59:58,760 --> 01:00:00,940
그래서 여기서 세 가지 변형을 모두 볼 수 있습니다.

1350
01:00:00,940 --> 01:00:02,420
이것은 실제 값, 비영

1351
01:00:02,420 --> 01:00:05,480
값 및 가중치 행렬이 어디에 있는지 가장

1352
01:00:05,480 --> 01:00:07,100
명시적일 수 있습니다.

1353
01:00:07,100 --> 01:00:09,680
그래서 이것을 생각하는 한 가지 방법은 이러한

1354
01:00:09,680 --> 01:00:11,240
벡터를 함께 쌓는 것입니다.

1355
01:00:11,240 --> 01:00:13,980
우리는 이 W로 곱하고 tanh h를 통과시킵니다.

1356
01:00:13,980 --> 01:00:18,243
이것은 다음 RNN에 전달하는 출력 h,t를 제공합니다.

1357
01:00:18,243 --> 01:00:19,660
이것들이 쌓여 있다고 상상할 수 있습니다.

1358
01:00:19,660 --> 01:00:23,320
그리고 출력 yt를 직접 가질 수도

1359
01:00:23,320 --> 01:00:27,760
있고, ht에 가중치 행렬을 곱한 후 활성화 함수가

1360
01:00:27,760 --> 01:00:31,040
있는 층을 가질 수도 있습니다.

1361
01:00:31,040 --> 01:00:32,960
네, 질문입니다.

1362
01:00:32,960 --> 01:00:35,680
아, 물론입니다.

1363
01:00:35,680 --> 01:00:40,320
여기 다층 RNN이 있는데, [INAUDIBLE] 행렬은 거의

1364
01:00:40,320 --> 01:00:41,320
[INAUDIBLE]인가요?

1365
01:00:41,320 --> 01:00:43,640
네, 다층 RNN의 경우 가중치는 층

1366
01:00:43,640 --> 01:00:44,700
내에서 공유됩니다.

1367
01:00:44,700 --> 01:00:49,600
따라서 모든 은닉 상태 업데이트는 동일한

1368
01:00:49,600 --> 01:00:51,300
가중치를 사용합니다.

1369
01:00:51,300 --> 01:00:54,360
그리고 이 다이어그램에서 수직으로

1370
01:00:54,360 --> 01:00:58,440
쌓인 각 층은 별도의 가중치 집합을 가집니다.

1371
01:00:58,440 --> 01:01:04,420
알겠어요, 그래서 이게 작동하는 방식입니다.

1372
01:01:04,420 --> 01:01:06,700
그리고 역전파가 있을 때, 우리가

1373
01:01:06,700 --> 01:01:07,640
이야기했던 것입니다.

1374
01:01:07,640 --> 01:01:11,060
각 시간 단계에 대한 손실이 없다면,

1375
01:01:11,060 --> 01:01:14,380
왜 출력 h,t의 손실을 기반으로

1376
01:01:14,380 --> 01:01:18,060
손실을 계산해야 하는지, 그래서 이

1377
01:01:18,060 --> 01:01:20,920
역전파를 수행할 때 W로 곱하고

1378
01:01:20,920 --> 01:01:23,380
tanh의 미분도 취합니다.

1379
01:01:23,380 --> 01:01:25,560
이 두 가지 모두 실제로 문제가 있을 수 있습니다.

1380
01:01:25,560 --> 01:01:28,860
특히 수학적으로 우리의 숨겨진

1381
01:01:28,860 --> 01:01:32,300
상태 t의 각 구성 요소를

1382
01:01:32,300 --> 01:01:36,820
h,t-1에 대해 변경할 때의 기울기를

1383
01:01:36,820 --> 01:01:38,340
살펴보면.

1384
01:01:38,340 --> 01:01:40,880
죄송합니다, h,t-1의 각 구성 요소를

1385
01:01:40,880 --> 01:01:43,010
변경하면 h,t에 어떤 영향을 미칠까요?

1386
01:01:43,010 --> 01:01:44,760
이 기울기가 계산하는 것입니다.

1387
01:01:44,760 --> 01:01:47,900
tanh의 미분이 필요합니다. 왜냐하면 이것이 우리의 활성화 함수이기

1388
01:01:47,900 --> 01:01:48,520
때문입니다.

1389
01:01:48,520 --> 01:01:52,500
그리고 이전 숨겨진 상태를 다음 상태로 변환하기 위해

1390
01:01:52,500 --> 01:01:55,357
여기에서 곱하는 Whh가 있습니다.

1391
01:01:55,357 --> 01:01:57,440
그래서 이것이 실제로 기울기를 계산하는 방법입니다.

1392
01:01:57,440 --> 01:01:58,920
여기에서 문제에 직면할 수 있습니다.

1393
01:01:58,920 --> 01:02:04,260
각 시간 단계에서 손실을 계산하고, 여기에서

1394
01:02:04,260 --> 01:02:06,980
총 손실을 각

1395
01:02:06,980 --> 01:02:09,760
가중치에 대해 합산합니다.

1396
01:02:09,760 --> 01:02:11,200
총 손실은 이

1397
01:02:11,200 --> 01:02:13,600
재사용된 W 행렬에

1398
01:02:13,600 --> 01:02:18,560
대해 각 시간 단계의 손실의 합입니다.

1399
01:02:18,560 --> 01:02:23,280
그래서 L,t에 대한 최종 단계에서 L의

1400
01:02:26,640 --> 01:02:32,200
손실을 계산하기 위해, 각 중간 숨겨진 상태와

1401
01:02:32,200 --> 01:02:35,560
그것이 W에 미치는 영향을 계산해야

1402
01:02:35,560 --> 01:02:39,880
이 최종 손실을 체인 룰을 사용하여

1403
01:02:39,880 --> 01:02:42,400
계산할 수 있습니다.

1404
01:02:42,400 --> 01:02:44,400
여기에서 예를 언급했으며, 이것이

1405
01:02:44,400 --> 01:02:46,640
문제인 이유를 지적하고자 합니다.

1406
01:02:46,640 --> 01:02:48,540
이 개별 항목을

1407
01:02:48,540 --> 01:02:50,840
살펴보면, 현재 숨겨진

1408
01:02:50,840 --> 01:02:55,360
상태를 변경하는 것이 다음 상태에

1409
01:02:55,360 --> 01:02:58,560
어떻게 영향을 미치는지에 대한

1410
01:02:58,560 --> 01:03:00,640
이 측면에 집중하면,

1411
01:03:00,640 --> 01:03:04,860
이 곱셈 항에 포함된 계산의

1412
01:03:04,860 --> 01:03:09,100
대부분과 동일한 것을 언급했던 것입니다.

1413
01:03:09,100 --> 01:03:10,860
그래서 이것이 문제인 이유는 무엇인가요?

1414
01:03:10,860 --> 01:03:15,560
우선, 이것은 여기에서 플롯된 tanh의 미분입니다.

1415
01:03:15,560 --> 01:03:19,220
최대 값은 1이며, 거의 항상

1416
01:03:19,220 --> 01:03:20,920
1보다 작습니다.

1417
01:03:20,920 --> 01:03:25,540
그래서 이 항에서 소실 기울기가 발생할 수 있습니다.

1418
01:03:25,540 --> 01:03:30,540
하지만 비선형성이 없다고 가정하거나 이 문제가

1419
01:03:30,540 --> 01:03:33,140
없는 다른 활성화 함수를

1420
01:03:33,140 --> 01:03:34,620
선택하더라도.

1421
01:03:34,620 --> 01:03:38,860
각 시간 단계에서 곱하는 이 가중치

1422
01:03:38,860 --> 01:03:41,980
행렬을 살펴보면, 큰 특이값이

1423
01:03:41,980 --> 01:03:45,160
있을 것이고, 그래서

1424
01:03:45,160 --> 01:03:49,440
벡터가 들어올 때 최대한 늘어나는 정도가

1425
01:03:49,440 --> 01:03:52,640
무엇인지 알 수 있습니다.

1426
01:03:52,640 --> 01:03:55,980
단위 벡터의 특이값이 행렬에 의해

1427
01:03:55,980 --> 01:03:58,060
얼마나 늘어날 수

1428
01:03:58,060 --> 01:03:59,700
있는지를 알려줍니다.

1429
01:03:59,700 --> 01:04:02,220
그래서 만약 매우 크면 기울기가 폭발할

1430
01:04:02,220 --> 01:04:04,720
수 있고, 매우 작으면 소실 기울기

1431
01:04:04,720 --> 01:04:06,280
문제가 발생할 수 있습니다.

1432
01:04:06,280 --> 01:04:08,440
폭발하는 기울기가 있다면, 이를

1433
01:04:08,440 --> 01:04:11,300
해결하는 방법은 기울기를 스케일링하는 것입니다.

1434
01:04:11,300 --> 01:04:14,920
그래서 그냥 나누거나 클립하여 너무 큰 기울기가

1435
01:04:14,920 --> 01:04:15,460
문제

1436
01:04:15,460 --> 01:04:17,793
되지 않도록 할 수 있습니다.

1437
01:04:17,793 --> 01:04:21,240
하지만 이 정말 작은 소실 기울기

1438
01:04:21,240 --> 01:04:23,640
문제는 사람들이 실제로 긴

1439
01:04:23,640 --> 01:04:26,880
RNN을 사용하지 않는 주된 이유입니다.

1440
01:04:26,880 --> 01:04:29,260
tanh 때문이고,

1441
01:04:29,260 --> 01:04:31,400
많은 시나리오에서 가중치

1442
01:04:31,400 --> 01:04:37,660
행렬이 활성화를 확장하거나 줄이는 특성을 가지고 있기 때문입니다.

1443
01:04:37,660 --> 01:04:42,240
그래서 네, 이것들이 RNN 아키텍처의 변화를 촉진한 주된

1444
01:04:42,240 --> 01:04:45,200
이유이며, 사람들이 RNN을 사용하지

1445
01:04:45,200 --> 01:04:46,760
않는 이유입니다.

1446
01:04:46,760 --> 01:04:48,560
이것이 주요 문제 중 하나입니다.

1447
01:04:48,560 --> 01:04:50,680
그렇다면 이를 어떻게 해결할까요?

1448
01:04:50,680 --> 01:04:54,760
사람들이 이를 해결한 방법은 LSTM을 만드는 것이었습니다.

1449
01:04:54,760 --> 01:04:57,520
고수준 아이디어는 여러 가지

1450
01:04:57,520 --> 01:04:59,590
복잡한 세부 사항이 있지만,

1451
01:04:59,590 --> 01:05:03,110
서로 다른 값을 추적하는 네 가지

1452
01:05:03,110 --> 01:05:05,790
게이트가 있다는 것입니다.

1453
01:05:05,790 --> 01:05:07,390
단일 은닉 상태 대신

1454
01:05:07,390 --> 01:05:09,330
여러 값을 갖습니다.

1455
01:05:09,330 --> 01:05:11,890
은닉 상태를 변경하는 방법과 다른

1456
01:05:11,890 --> 01:05:14,030
경로를 통해 전달할 정보를

1457
01:05:14,030 --> 01:05:15,295
미리 계산합니다.

1458
01:05:15,295 --> 01:05:17,170
정상적인 은닉 상태 경로가 있습니다.

1459
01:05:17,170 --> 01:05:19,087
정보를 전달하기 쉬운

1460
01:05:19,087 --> 01:05:20,630
다른 경로가 있습니다.

1461
01:05:20,630 --> 01:05:26,470
이것이 기본 아이디어이며, 고수준에서 게이트라고 부르거나 셀의 은닉

1462
01:05:26,470 --> 01:05:30,070
상태에 실제로 무엇을 기록하는지를 나타냅니다.

1463
01:05:30,070 --> 01:05:32,870
입력 게이트는 셀에 정보를 기록할지

1464
01:05:32,870 --> 01:05:34,530
여부를 결정합니다.

1465
01:05:34,530 --> 01:05:37,150
망각 게이트는 이전 시간 단계에서 얼마나

1466
01:05:37,150 --> 01:05:40,030
잊을지를 결정하며, 출력 게이트는 은닉 상태에

1467
01:05:40,030 --> 01:05:42,963
대해 실제로 얼마나 출력하는지를 나타냅니다.

1468
01:05:42,963 --> 01:05:44,630
이것은 정말 복잡하며, 많은

1469
01:05:44,630 --> 01:05:46,530
설계 선택이 포함되어 있습니다.

1470
01:05:46,530 --> 01:05:49,470
그들은 이것을 모두 모아 꽤 복잡한 다이어그램을

1471
01:05:49,470 --> 01:05:51,670
만들었지만, 기본 아이디어는

1472
01:05:51,670 --> 01:05:54,110
이 부분이 동일하여 가중치 곱셈을

1473
01:05:54,110 --> 01:05:55,548
수행한다는 것입니다.

1474
01:05:55,548 --> 01:05:58,090
이제 h,t 대신 네 가지

1475
01:05:58,090 --> 01:06:00,370
다른 값을 계산하고 있습니다.

1476
01:06:00,370 --> 01:06:03,290
입력 게이트와 여기에 얼마나

1477
01:06:03,290 --> 01:06:05,410
기록할지를 결정하는 게이트가

1478
01:06:05,410 --> 01:06:10,050
있으며, 다음 은닉 상태로 전달되는 출력이 있습니다.

1479
01:06:10,050 --> 01:06:12,610
여기 위쪽 섹션을 고속도로로 생각할

1480
01:06:12,610 --> 01:06:14,970
수 있으며, 목표는 활성화 함수가

1481
01:06:14,970 --> 01:06:16,910
없도록 하는 것입니다.

1482
01:06:16,910 --> 01:06:17,770
따라서 tanh가 없습니다.

1483
01:06:17,770 --> 01:06:20,130
tanh가 기울기를 소실하게

1484
01:06:20,130 --> 01:06:24,530
만든 문제를 피할 수 있으며, 우리가 적용하는 것은 이 망각

1485
01:06:24,530 --> 01:06:25,630
게이트뿐입니다.

1486
01:06:25,630 --> 01:06:28,530
각 시간 단계에서 모든 정보를

1487
01:06:28,530 --> 01:06:32,150
기본적으로 잊지 않는 한, 정보를 더

1488
01:06:32,150 --> 01:06:34,830
쉽게 전달할 수 있습니다.

1489
01:06:34,830 --> 01:06:36,510
이것이 고수준 설명입니다.

1490
01:06:36,510 --> 01:06:38,350
그리고 더 중요한 것은, 실제로

1491
01:06:38,350 --> 01:06:41,570
사람들은 이것이 매우 잘 작동한다는 것을 알게 되는 것 같습니다.

1492
01:06:41,570 --> 01:06:45,090
다시 말하지만, 여러분은 이 과정을 위해 이것을 전혀 구현하지 않을

1493
01:06:45,090 --> 01:06:47,930
것이지만, 이것은 여전히 매우 일반적으로 사용되는

1494
01:06:47,930 --> 01:06:50,150
기준선이며 일부 딥러닝 논문에서도 사용됩니다.

1495
01:06:50,150 --> 01:06:51,670
그래서 이것에 대해 아는 것이 좋습니다.

1496
01:06:51,670 --> 01:06:54,690
하지만 저는 사람들이 RNN이 가지고 있던

1497
01:06:54,690 --> 01:06:56,750
모든 문제, 즉 소실 기울기와

1498
01:06:56,750 --> 01:06:58,250
정보 포착 부족을

1499
01:06:58,250 --> 01:07:01,790
보완하기 위해 이러한 것들을 구성하려고 한다고 생각할

1500
01:07:01,790 --> 01:07:02,745
수 있습니다.

1501
01:07:02,745 --> 01:07:04,870
모든 것을 이 숨겨진 상태에 집어넣어야 합니다.

1502
01:07:04,870 --> 01:07:06,330
그래서 정말 긴 의존성이 있습니다.

1503
01:07:06,330 --> 01:07:07,050
그것들은 잃어버립니다.

1504
01:07:07,050 --> 01:07:08,670
그래서 그들은 여기

1505
01:07:08,670 --> 01:07:12,910
위쪽에서 더 긴 정보가 전달될 수 있는

1506
01:07:12,910 --> 01:07:15,230
별도의 경로를 만들었습니다.

1507
01:07:15,230 --> 01:07:19,190
그렇다면 LSTM이 소실 기울기 문제를 완전히 해결합니까?

1508
01:07:19,190 --> 01:07:20,330
확실히 도움이 됩니다.

1509
01:07:20,330 --> 01:07:22,648
그래서 이 상단 경로 다이어그램을 사용하여

1510
01:07:22,648 --> 01:07:24,190
여러 시간 단계에 걸쳐 이

1511
01:07:24,190 --> 01:07:26,950
정보를 보존하는 것이 RNN을 더 쉽게 만듭니다.

1512
01:07:26,950 --> 01:07:29,850
그래서 대조적으로, 바닐라 RNN이

1513
01:07:29,850 --> 01:07:32,190
매 시간 단계마다 숨겨진

1514
01:07:32,190 --> 01:07:35,950
상태에서 정보를 보존하는 현재 가중치 행렬을 학습하는

1515
01:07:35,950 --> 01:07:38,450
것은 훨씬 더 어렵습니다.

1516
01:07:38,450 --> 01:07:40,270
항상 같은 작업을 수행하고

1517
01:07:40,270 --> 01:07:42,630
활성화 함수 없이 정보를

1518
01:07:42,630 --> 01:07:45,190
직접 전달할 수 없기 때문입니다.

1519
01:07:45,190 --> 01:07:49,550
그래서 그것을 보장하지는 않지만, 훨씬 더 쉽게 만들고

1520
01:07:49,550 --> 01:07:54,010
장기 의존성을 학습하는 데 도움이 되며 경험적으로

1521
01:07:54,010 --> 01:07:55,430
매우 잘 작동합니다.

1522
01:07:55,430 --> 01:07:59,450
그래서 사람들은 일반적으로 RNN을 많이 훈련하지 않고,

1523
01:07:59,450 --> 01:08:01,210
이 순환 모델링 경로를

1524
01:08:01,210 --> 01:08:07,323
선택할 경우 LSTM을 더 자주 훈련합니다. 하지만 일반적으로 이들은
또한-- 제가

1525
01:08:07,323 --> 01:08:07,990
말했듯이,

1526
01:08:07,990 --> 01:08:10,570
상당히 유행에서 벗어났다고 말할 수

1527
01:08:10,570 --> 01:08:12,290
있지만, 사람들이 직면한

1528
01:08:12,290 --> 01:08:15,850
문제를 설명하기 위해 RNN을 설계하려고 시도한

1529
01:08:15,850 --> 01:08:18,450
방식을 이해하는 데 도움이 됩니다.

1530
01:08:18,450 --> 01:08:22,689
그래서 이 과정에서 이전에 배운 것과 연결할 수 있는 또

1531
01:08:22,689 --> 01:08:24,189
다른 흥미로운

1532
01:08:24,189 --> 01:08:27,609
점은, 출력을 직접 추가하고 일부 활성화 함수나

1533
01:08:27,609 --> 01:08:29,770
다른 레이어를 건너뛰는

1534
01:08:29,770 --> 01:08:31,649
아이디어가 실제로 ResNet에서

1535
01:08:31,649 --> 01:08:33,529
논의한 아이디어와

1536
01:08:33,529 --> 01:08:36,850
매우 관련이 있다는 것입니다. 여기서 스킵

1537
01:08:36,850 --> 01:08:41,550
연결이 있어 값이 복사되어 나중에 레이어 블록에서 추가됩니다.

1538
01:08:41,550 --> 01:08:43,162
그래서 ResNet에서는 여러 개가 있습니다.

1539
01:08:43,162 --> 01:08:45,370
여러 개의 합성곱 층이 함께

1540
01:08:45,370 --> 01:08:48,210
쌓여 있고, 여기서 값이 더해지는 스킵

1541
01:08:48,210 --> 01:08:49,310
연결을 추가합니다.

1542
01:08:49,310 --> 01:08:52,590
LSTM에 대해서도 비슷한 관점에서

1543
01:08:52,590 --> 01:08:56,390
생각할 수 있으며, 이는 일부 층을

1544
01:08:56,390 --> 01:08:59,729
건너뛰는 방식으로 모델의 깊이가 매우

1545
01:08:59,729 --> 01:09:02,010
크지 않도록 도와줍니다.

1546
01:09:02,010 --> 01:09:04,569
매우 긴 시간 단계의 시퀀스입니다.

1547
01:09:04,569 --> 01:09:07,212
이것은 병렬적이지만,

1548
01:09:07,212 --> 01:09:08,670
한쪽은 층의 수이고

1549
01:09:08,670 --> 01:09:12,750
다른 쪽은 시간 단계의 수입니다.

1550
01:09:12,750 --> 01:09:16,910
오늘 강의의 마지막 슬라이드는 RNN이 지난

1551
01:09:16,910 --> 01:09:21,670
1~2년 동안 다시 주목받고 있다는 점을 간단히 연결하는

1552
01:09:21,670 --> 01:09:24,410
것입니다. 재미있는 점은, 만약

1553
01:09:24,410 --> 01:09:28,247
우리가 1~2년 전에 이 과정을 가르쳤다면

1554
01:09:28,247 --> 01:09:30,830
RNN을 완전히 배제하고 싶었을

1555
01:09:30,830 --> 01:09:31,993
것입니다.

1556
01:09:31,993 --> 01:09:34,410
하지만 실제로 그들이 가진 많은 장점이 있습니다.

1557
01:09:34,410 --> 01:09:37,250
주요 장점은 무제한의 컨텍스트 길이입니다.

1558
01:09:37,250 --> 01:09:39,510
트랜스포머의 주요 문제 중 하나는 제한된 컨텍스트

1559
01:09:39,510 --> 01:09:42,109
길이를 가지고 있다는 것입니다. 사람들이 이 모델들이

1560
01:09:42,109 --> 01:09:44,609
할 수 있는 것의 경계를 정말로 밀어붙이고 있습니다.

1561
01:09:44,609 --> 01:09:47,170
이 컨텍스트 길이는 점점 더 문제가 되고 있습니다.

1562
01:09:47,170 --> 01:09:49,229
트랜스포머 공간에서 다양한 우회 방법이

1563
01:09:49,229 --> 01:09:49,843
있었습니다.

1564
01:09:49,843 --> 01:09:52,010
사람들은 로프와 같은 기술을

1565
01:09:52,010 --> 01:09:54,229
사용하여 컨텍스트 길이를

1566
01:09:54,229 --> 01:09:57,770
늘리려고 하지만, 이는 모델의 상당한 제한입니다.

1567
01:09:57,770 --> 01:10:02,230
RNN의 추론 중에는 계산이 시퀀스 길이에 따라

1568
01:10:02,230 --> 01:10:05,210
선형적으로 확장되며, 훈련 중에도

1569
01:10:05,210 --> 01:10:07,510
마찬가지입니다. 기본적으로

1570
01:10:07,510 --> 01:10:11,592
시퀀스에 더 많은 단계를 추가할수록 같은

1571
01:10:11,592 --> 01:10:14,050
작업을 반복해서 재계산해야

1572
01:10:14,050 --> 01:10:14,550
합니다.

1573
01:10:14,550 --> 01:10:17,530
트랜스포머와 같은 전체 입력

1574
01:10:17,530 --> 01:10:20,410
시퀀스를 가로지르는 작업은 없습니다.

1575
01:10:20,410 --> 01:10:22,350
이것들은 정말 큰 장점이며,

1576
01:10:22,350 --> 01:10:24,030
몇 가지 논문이 있습니다.

1577
01:10:24,030 --> 01:10:28,773
몇 가지를 언급하자면, RWKV 모델이 있습니다.

1578
01:10:28,773 --> 01:10:30,190
여기 arXiv 링크를

1579
01:10:30,190 --> 01:10:33,558
확인할 수 있으며, Mamba도 주로 선형 시간 시퀀스

1580
01:10:33,558 --> 01:10:35,850
모델링을 달성할 수 있다는 아이디어를

1581
01:10:35,850 --> 01:10:36,350
강조합니다.

1582
01:10:36,350 --> 01:10:38,310
입력 시퀀스를 확장할수록

1583
01:10:38,310 --> 01:10:41,130
계산도 트랜스포머와 달리 선형적으로

1584
01:10:41,130 --> 01:10:42,490
확장됩니다.

1585
01:10:42,490 --> 01:10:46,610
그래서 때때로 긴 컨텍스트 문제에 더 좋습니다.

1586
01:10:46,610 --> 01:10:48,835
계산 측면에서 더 잘 작동하며, 이러한

1587
01:10:48,835 --> 01:10:50,210
주요 장점이 있습니다.

1588
01:10:50,210 --> 01:10:51,810
사람들은 두 세계의 장점을 모두 얻으려고

1589
01:10:51,810 --> 01:10:53,850
노력하며, 이 분야에서 많은 연구가 진행되고 있습니다.

1590
01:10:53,850 --> 01:10:55,808
트랜스포머의 성능을 RNN의

1591
01:10:55,808 --> 01:10:58,350
확장성과 어떻게 결합할 수 있을까요?

1592
01:10:58,350 --> 01:11:01,330
오늘 수업은 여기까지입니다.

1593
01:11:01,330 --> 01:11:04,070
우리는 기본적으로 RNN으로 아키텍처를 설계할 수

1594
01:11:04,070 --> 01:11:06,370
있는 다양한 방법에 대해 이야기했습니다.

1595
01:11:06,370 --> 01:11:09,327
바닐라 RNN은 간단하지만 잘 작동하지 않습니다.

1596
01:11:09,327 --> 01:11:10,910
정보를 선택적으로

1597
01:11:10,910 --> 01:11:13,710
전달하는 방법을 도입한 더 복잡한

1598
01:11:13,710 --> 01:11:16,190
변형들이 제안되었습니다.

1599
01:11:16,190 --> 01:11:18,110
RNN에서의 그래디언트의 역흐름은

1600
01:11:18,110 --> 01:11:20,190
사용하는 활성화 함수나 가중치

1601
01:11:20,190 --> 01:11:22,607
행렬의 특성에 따라 폭발하거나 사라질 수

1602
01:11:22,607 --> 01:11:23,230
있습니다.

1603
01:11:23,230 --> 01:11:26,910
그래서 실제로 그래디언트를 계산하기 위해

1604
01:11:26,910 --> 01:11:29,630
시간에 따른 역전파가 필요합니다.

1605
01:11:29,630 --> 01:11:32,510
마지막으로, 이러한 더 나은 아키텍처는 현재

1606
01:11:32,510 --> 01:11:34,670
뜨거운 연구 주제이며, 시퀀스를 다루는

1607
01:11:34,670 --> 01:11:37,748
새로운 패러다임에 대한 일반적인 관심도 있습니다.

1608
01:11:37,748 --> 01:11:39,290
그래서 오늘은 여기까지입니다.

1609
01:11:39,290 --> 01:11:43,180
다음 시간에는 주의(attention)와 트랜스포머에 대해 이야기하겠습니다.
