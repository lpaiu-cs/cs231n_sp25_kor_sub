1
00:00:05,560 --> 00:00:07,020
안녕하세요 여러분.

2
00:00:07,020 --> 00:00:09,360
7강에 오신 것을 환영합니다.

3
00:00:09,360 --> 00:00:12,205
지난 시간에 대해 몇 가지 정리할 점도 말씀드리고 싶었습니다.

4
00:00:12,205 --> 00:00:13,580
지난 강의 때,

5
00:00:13,580 --> 00:00:17,360
여러분이 확인해 보면 좋을 두 개의 ed

6
00:00:17,360 --> 00:00:18,780
게시물이 있었습니다.

7
00:00:18,780 --> 00:00:19,760
아직 못 보셨다면,

8
00:00:19,760 --> 00:00:21,260
간단히 설명드리겠습니다.

9
00:00:21,260 --> 00:00:23,760
드롭아웃을 설명하면서 테스트 시

10
00:00:23,760 --> 00:00:26,220
확률을 어떻게 조정하는지에 대해

11
00:00:26,220 --> 00:00:28,520
약간 혼동이 있었던 것 같습니다.

12
00:00:28,520 --> 00:00:32,680
기본적으로 제가 슬라이드에서 말한 내용이 맞지 않는 부분이 있었습니다.

13
00:00:32,680 --> 00:00:36,040
드롭아웃의 각 순전파에서 p라는

14
00:00:36,040 --> 00:00:38,960
하이퍼파라미터가 있는데, 이 p는 드롭아웃하는

15
00:00:38,960 --> 00:00:41,120
뉴런의 비율이거나,

16
00:00:41,120 --> 00:00:43,080
사용하는 드롭아웃 구현

17
00:00:43,080 --> 00:00:45,320
방식에 따라 유지하는 뉴런의

18
00:00:45,320 --> 00:00:46,500
비율입니다.

19
00:00:46,500 --> 00:00:47,620
일반적으로 그렇습니다.

20
00:00:47,620 --> 00:00:50,400
그것은 드롭아웃하는 노드의 비율입니다.

21
00:00:50,400 --> 00:00:52,940
그래서 대부분의 라이브러리에서 p는 그 의미입니다.

22
00:00:52,940 --> 00:00:54,860
하지만 기본 아이디어는 테스트

23
00:00:54,860 --> 00:00:58,480
시에 기대 출력이 훈련 시와 같아야 한다는 겁니다.

24
00:00:58,480 --> 00:01:03,020
즉, 훈련 시에 활성화의 25%를 드롭했다면,

25
00:01:03,020 --> 00:01:07,020
테스트 시에는 0.75로 스케일링해서 기대 출력이

26
00:01:07,020 --> 00:01:09,068
같도록 하는 거죠.

27
00:01:09,068 --> 00:01:10,860
이 슬라이드에서 구현은

28
00:01:10,860 --> 00:01:14,020
p를 유닛을 활성 상태로 유지할 확률로

29
00:01:14,020 --> 00:01:17,840
사용해서 약간 혼동이 있었던 것 같습니다.

30
00:01:17,840 --> 00:01:20,200
그래서 약간의 불일치가 있었습니다.

31
00:01:20,200 --> 00:01:21,900
명확히 하기 위해서 말씀드립니다.

32
00:01:21,900 --> 00:01:25,020
지난 시간에 수업 중에 정규화가 어떻게 유용할 수

33
00:01:25,020 --> 00:01:26,760
있는지에 대한 질문도 있었습니다.

34
00:01:26,760 --> 00:01:28,220
그리고 잘못 초기화된

35
00:01:28,220 --> 00:01:31,280
가중치에서 발생하는 문제를 해결할 수도 있습니다.

36
00:01:31,280 --> 00:01:34,340
하지만 여기서는 2D 입력을 모델에 넣고, ReLU를

37
00:01:34,340 --> 00:01:37,380
가진 두 층 신경망을 사용하는 설정이 있습니다.

38
00:01:37,380 --> 00:01:40,160
기본적으로 이 사분면 함수를 출력하고 있죠.

39
00:01:40,160 --> 00:01:43,580
그래서 점이 오른쪽 위 사분면에 있으면, 점이 어느

40
00:01:43,580 --> 00:01:47,500
사분면에 있느냐에 따라 1, 2, 3, 4 중 하나를 출력합니다.

41
00:01:47,500 --> 00:01:51,300
그리고 좋은 초기화인 Kaiming 초기화와

42
00:01:51,300 --> 00:01:54,760
표준편차가 너무 높은 나쁜 초기화에 대해

43
00:01:54,760 --> 00:01:57,020
훈련 손실과 테스트 손실을

44
00:01:57,020 --> 00:01:59,280
각각 그래프로 그렸습니다.

45
00:01:59,280 --> 00:02:02,400
여기 파란색 그래프는 나쁜 초기화를, 초록색은

46
00:02:02,400 --> 00:02:05,200
LayerNorm을 적용한 나쁜 초기화를 나타냅니다.

47
00:02:05,200 --> 00:02:07,700
그래서 실제로 많은 문제를 해결하는 것을 볼 수 있습니다.

48
00:02:07,700 --> 00:02:09,120
하지만 최고의 성능을 내려면

49
00:02:09,120 --> 00:02:10,703
여전히 좋은 가중치 초기화가 필요하며,

50
00:02:10,703 --> 00:02:12,460
그게 바로 이후 두 개의 선입니다.

51
00:02:12,460 --> 00:02:13,860
자세히 살펴보실 수 있습니다.

52
00:02:13,860 --> 00:02:15,840
또한 LayerNorm이 도움이

53
00:02:15,840 --> 00:02:18,080
되는지는 문제에 따라 다릅니다.

54
00:02:18,080 --> 00:02:20,960
예를 들어 이 사분면 I에서는 각 점의 정확한

55
00:02:20,960 --> 00:02:23,963
2D 위치를 알 필요가 없다고 상상할 수 있습니다.

56
00:02:23,963 --> 00:02:25,380
그래서 LayerNorm이 실제로 도움이 되었습니다.

57
00:02:25,380 --> 00:02:28,100
하지만 코드에 있는 다른 함수들 중에서는

58
00:02:28,100 --> 00:02:30,037
정확한 좌표를 알아야 올바른 출력을

59
00:02:30,037 --> 00:02:32,620
얻을 수 있는 경우가 있는데, 이때는

60
00:02:32,620 --> 00:02:34,465
LayerNorm이 오히려

61
00:02:34,465 --> 00:02:35,840
성능을 저하시킵니다.

62
00:02:35,840 --> 00:02:38,060
왜냐하면 평균을 빼고 표준편차로

63
00:02:38,060 --> 00:02:39,935
나누면서 입력의 정확한 공간 위치

64
00:02:39,935 --> 00:02:42,040
정보 일부가 손실되기 때문입니다.

65
00:02:42,040 --> 00:02:44,480
여기 몇 가지 참고 사항입니다.

66
00:02:44,480 --> 00:02:47,740
기본적으로 높은 수준에서 보면 문제 해결에 도움이 되지만, 여전히

67
00:02:47,740 --> 00:02:48,873
차이가 남아 있습니다.

68
00:02:48,873 --> 00:02:51,040
그래서 단순한 정규화만으로는 이 가중치 초기화

69
00:02:51,040 --> 00:02:52,380
문제를 해결할 수 없습니다.

70
00:02:55,040 --> 00:02:57,260
그리고 말씀드렸듯이, 모델링하려는

71
00:02:57,260 --> 00:03:00,380
대상에 따라 항상 의미가 있지는 않을 수 있습니다.

72
00:03:00,380 --> 00:03:02,840
지난 시간 내용을 간단히 정리하자면,

73
00:03:02,840 --> 00:03:05,700
지금까지는 주로 이런 일반적인, 표준적인 비순환

74
00:03:05,700 --> 00:03:07,860
신경망에 대해 이야기했습니다.

75
00:03:07,860 --> 00:03:12,260
즉, 고정된 크기의 입력과 고정된 크기의 출력을 다루는 거죠.

76
00:03:12,260 --> 00:03:15,020
활성화 함수를 한 번 설정하는 고정된 설정이

77
00:03:15,020 --> 00:03:15,880
있습니다.

78
00:03:15,880 --> 00:03:17,980
이미지 채널에 대해 고정된

79
00:03:17,980 --> 00:03:21,540
평균과 표준편차를 기준으로 데이터

80
00:03:21,540 --> 00:03:22,580
전처리를 합니다.

81
00:03:22,580 --> 00:03:25,140
가중치 초기화와 정규화

82
00:03:25,140 --> 00:03:27,660
함수, 그리고

83
00:03:27,660 --> 00:03:30,520
전이 학습도 사용합니다.

84
00:03:30,520 --> 00:03:33,460
예를 들어 ImageNet 같은 대규모 인터넷

85
00:03:33,460 --> 00:03:35,560
데이터셋에서 사전 학습을 하면, 그 가중치

86
00:03:35,560 --> 00:03:37,935
값으로 초기화할 때 더 좋은 결과를 얻을

87
00:03:37,935 --> 00:03:38,740
수 있습니다.

88
00:03:38,740 --> 00:03:40,573
또 학습 동역학에 대해서도

89
00:03:40,573 --> 00:03:42,740
이야기했는데, 좋은 학습률을

90
00:03:42,740 --> 00:03:44,340
선택하고 하이퍼파라미터를

91
00:03:44,340 --> 00:03:46,560
어떻게 업데이트할지, 검증

92
00:03:46,560 --> 00:03:48,980
성능에 기반해 최적화하는 방법과

93
00:03:48,980 --> 00:03:51,860
테스트 시 증강을 통해 성능을 더 향상시키는

94
00:03:51,860 --> 00:03:53,640
방법도 다뤘습니다.

95
00:03:53,640 --> 00:03:56,500
여기서 2번과 3번 항목에 정말 좋은 도구가 있는데, 저는

96
00:03:56,500 --> 00:03:59,600
거의 모든 프로젝트에서 weights and biases라는

97
00:03:59,600 --> 00:04:00,540
도구를 사용합니다.

98
00:04:00,540 --> 00:04:01,980
여러분도 유용하게 쓰실 수 있을 겁니다.

99
00:04:01,980 --> 00:04:05,280
이 도구는 서로 다른 하이퍼파라미터로 여러 실험을

100
00:04:05,280 --> 00:04:07,760
설정하고 결과를 시각화할 수 있는

101
00:04:07,760 --> 00:04:09,060
아주 멋진 방법입니다.

102
00:04:09,060 --> 00:04:12,060
예를 들어 여기 드롭아웃 열이 있는데,

103
00:04:12,060 --> 00:04:14,200
이건 드롭아웃 값들이 모두 표시된 겁니다.

104
00:04:14,200 --> 00:04:15,700
색상 구분도 아주 잘 되어 있죠.

105
00:04:15,700 --> 00:04:18,160
일반적으로 드롭아웃 값이 낮을수록 더 높은

106
00:04:18,160 --> 00:04:20,100
정확도를 달성하는 것을 볼 수 있습니다.

107
00:04:20,100 --> 00:04:24,800
이렇게 검증 세트 성능에 따라 다양한

108
00:04:24,800 --> 00:04:26,400
하이퍼파라미터를

109
00:04:26,400 --> 00:04:28,680
시각화할 수 있습니다.

110
00:04:28,680 --> 00:04:33,000
많은 실험 결과를 바탕으로 어떤 하이퍼파라미터가

111
00:04:33,000 --> 00:04:35,445
가장 좋은지 알 수 있죠.

112
00:04:35,445 --> 00:04:36,320
저는 항상 이걸 사용합니다.

113
00:04:36,320 --> 00:04:37,260
정말 좋다고 생각해요.

114
00:04:37,260 --> 00:04:38,840
특히 컴퓨팅 자원이 충분해서 여러

115
00:04:38,840 --> 00:04:39,880
번 반복 실행해 성능을

116
00:04:39,880 --> 00:04:41,160
높일 수 있다면, 이 방법이

117
00:04:41,160 --> 00:04:42,500
시각화에 아주 유용합니다.

118
00:04:42,500 --> 00:04:43,500
잘 만들어졌다고 생각합니다.

119
00:04:43,500 --> 00:04:45,360
TensorBoard 같은 다른 도구도 있지만,

120
00:04:45,360 --> 00:04:49,640
개인적으로는 이걸 가장 좋아합니다.

121
00:04:49,640 --> 00:04:52,700
좋습니다, 오늘 강의의 나머지 부분에서는

122
00:04:52,700 --> 00:04:55,140
시퀀스 모델링에 대해 다루겠습니다.

123
00:04:55,140 --> 00:04:59,060
즉, 지금까지 다룬 고정 크기 입력과는 달리, 길이가

124
00:04:59,060 --> 00:05:00,860
가변적인 시퀀스를 입력으로

125
00:05:00,860 --> 00:05:03,100
받는 경우를 생각해보는 겁니다.

126
00:05:03,100 --> 00:05:05,380
그리고 트랜스포머 시대 이전에

127
00:05:05,380 --> 00:05:07,380
사람들이 주로 사용했던

128
00:05:07,380 --> 00:05:09,880
간단한 신경망, 주로

129
00:05:09,880 --> 00:05:13,300
RNN과 그 변형들에 대해 이야기할 겁니다.

130
00:05:13,300 --> 00:05:15,580
또 한 슬라이드에서는 RNN이

131
00:05:15,580 --> 00:05:20,620
어떻게 현대 언어 모델인 상태 공간 모델(state space

132
00:05:20,620 --> 00:05:23,060
models)과 유사하고 영감을

133
00:05:23,060 --> 00:05:24,785
주었는지도 설명할 겁니다.

134
00:05:24,785 --> 00:05:26,160
Mamba라는 모델도 들어보셨을 텐데,

135
00:05:26,160 --> 00:05:28,785
슬라이드에서 다른 모델들도 함께 다룰 겁니다만, 기본

136
00:05:28,785 --> 00:05:31,500
아이디어는 RNN의 핵심 개념이 오늘날에도 여전히

137
00:05:31,500 --> 00:05:32,760
사용되고 있다는 점입니다.

138
00:05:32,760 --> 00:05:34,437
과거에만 쓰인 게 아니고,

139
00:05:34,437 --> 00:05:36,020
트랜스포머에

140
00:05:36,020 --> 00:05:39,500
비해 여러 장점도 많다는 점을 설명할 겁니다.

141
00:05:39,500 --> 00:05:43,100
좋습니다, 이제 이 시퀀스 모델링 과제를 구체적으로

142
00:05:43,100 --> 00:05:45,780
수식화해보면, 지금까지 강의에서 다룬 것처럼

143
00:05:45,780 --> 00:05:48,700
고정 크기 입력에서 고정 크기 출력으로

144
00:05:48,700 --> 00:05:51,680
가는 일반적인 신경망을 상상할 수 있습니다.

145
00:05:51,680 --> 00:05:55,760
반대로, 하나의 입력에 대해 여러 개의 출력을 내는 시퀀스 모델링 작업도
있을 수

146
00:05:55,760 --> 00:05:56,260
있습니다.

147
00:05:56,260 --> 00:05:59,300
여기서는 여전히 고정 크기의 입력, 예를 들어 이미지가

148
00:05:59,300 --> 00:06:01,780
있지만, 출력은 가변 길이의 시퀀스를 원합니다.

149
00:06:01,780 --> 00:06:04,180
일반적인 예로 이미지 캡셔닝이 있습니다.

150
00:06:04,180 --> 00:06:06,680
이미지를 입력으로 넣고, 단어나

151
00:06:06,680 --> 00:06:08,480
문자, 혹은 언어를 모델링하거나

152
00:06:08,480 --> 00:06:11,980
인코딩하는 방식에 따라 시퀀스를 출력하는데,

153
00:06:11,980 --> 00:06:14,360
목표는 이미지에서 일어나는 일을

154
00:06:14,360 --> 00:06:17,800
설명하는 가변 길이의 캡션을 만드는 것입니다.

155
00:06:17,800 --> 00:06:21,200
또한 여러 입력에 대해 하나의 출력을 내는 시퀀스 모델링 작업도 있을 수
있습니다.

156
00:06:21,200 --> 00:06:25,400
예를 들어 입력이 비디오이고, 이 비디오가 무엇인지

157
00:06:25,400 --> 00:06:27,960
분류하는 작업을 생각할 수 있습니다.

158
00:06:27,960 --> 00:06:30,500
비디오 프레임 시퀀스를 입력으로 줍니다.

159
00:06:30,500 --> 00:06:33,800
출력은 이미지 분류와 비슷하게 하나의 클래스

160
00:06:33,800 --> 00:06:34,853
레이블입니다.

161
00:06:34,853 --> 00:06:36,520
하지만 이번에는 단일 이미지가

162
00:06:36,520 --> 00:06:38,600
아니라 여러 프레임이 입력으로 들어갑니다.

163
00:06:38,600 --> 00:06:41,200
이것이 바로 many to one의 예입니다.

164
00:06:41,200 --> 00:06:42,940
그리고 many to many도 있습니다.

165
00:06:42,940 --> 00:06:48,860
입력과 출력 시퀀스의 길이가 꼭 같을 필요는 없어서, 입력은

166
00:06:48,860 --> 00:06:51,300
가변 길이의 프레임이고

167
00:06:51,300 --> 00:06:54,617
출력은 가변 길이의 캡션일 수 있으며,

168
00:06:54,617 --> 00:06:56,200
꼭 길이가

169
00:06:56,200 --> 00:06:58,380
같을 필요는 없지만 같을

170
00:06:58,380 --> 00:06:59,760
수도 있습니다.

171
00:06:59,760 --> 00:07:02,540
즉, 각 입력에 대해 하나의 출력이 있습니다.

172
00:07:02,540 --> 00:07:04,460
RNN을 논의할 때는 주로 오른쪽

173
00:07:04,460 --> 00:07:06,400
끝의 설정에 집중할 것입니다.

174
00:07:06,400 --> 00:07:08,740
하지만 문제를 다른 설정에

175
00:07:08,740 --> 00:07:12,180
맞게 바꾸고 재구성하는 작은 변화들이 많습니다만,

176
00:07:12,180 --> 00:07:14,847
이 설정이 가장 직관적입니다.

177
00:07:14,847 --> 00:07:16,805
입력이 있을 때마다 출력이

178
00:07:16,805 --> 00:07:18,980
있고, 수업 초반에 RNN이

179
00:07:18,980 --> 00:07:22,460
어떻게 작동하는지 설명할 때 사용할 것입니다.

180
00:07:22,460 --> 00:07:24,260
여기서 대표적인 예제

181
00:07:24,260 --> 00:07:26,052
문제는 비디오 분류로,

182
00:07:26,052 --> 00:07:28,820
각 프레임을 분류하는 것입니다.

183
00:07:28,820 --> 00:07:31,060
자, 그럼 RNN이란 무엇일까요?

184
00:07:31,060 --> 00:07:36,020
기본 아이디어는 입력 시퀀스 x와 출력 시퀀스 y가

185
00:07:36,020 --> 00:07:37,340
있다는 것입니다.

186
00:07:37,340 --> 00:07:41,180
RNN이 RNN인 이유는 바로 이 순환적 특성 때문입니다.

187
00:07:41,180 --> 00:07:44,020
사람들이 종종 이 블록에 다시 피드백되는

188
00:07:44,020 --> 00:07:46,180
화살표로 다이어그램을 그리는데,

189
00:07:46,180 --> 00:07:48,490
이것이 바로 다이어그램에서 순환 레이어임을

190
00:07:48,490 --> 00:07:50,190
알 수 있는 표시입니다.

191
00:07:50,190 --> 00:07:52,530
하지만 실제 의미는 RNN이 내부 상태,

192
00:07:52,530 --> 00:07:55,510
흔히 숨겨진 상태(hidden state)라고 부르는 것을

193
00:07:55,510 --> 00:07:57,570
가지고 있고, 시퀀스를 처리하면서 이

194
00:07:57,570 --> 00:07:59,190
상태가 업데이트된다는 것입니다.

195
00:07:59,190 --> 00:08:01,910
모델에 새로운 입력이 들어올 때마다

196
00:08:01,910 --> 00:08:04,570
이를 처리하고 새로운 숨겨진 상태나 내부

197
00:08:04,570 --> 00:08:05,910
상태를 계산합니다.

198
00:08:05,910 --> 00:08:07,790
숨겨진 상태가 있고,

199
00:08:07,790 --> 00:08:11,170
이 상태는 새 입력과 이전 숨겨진 상태

200
00:08:11,170 --> 00:08:14,050
모두에 의존해서 업데이트됩니다.

201
00:08:14,050 --> 00:08:16,118
이 다이어그램은 때때로 실제로

202
00:08:16,118 --> 00:08:18,410
그래디언트가 어떻게 계산되고 연산 순서가 어떻게

203
00:08:18,410 --> 00:08:21,190
되는지 생각할 때 약간 혼란스러울 수 있습니다.

204
00:08:21,190 --> 00:08:25,850
그래서 사람들은 종종 펼쳐진(unrolled) RNN 다이어그램을 그립니다.

205
00:08:25,850 --> 00:08:28,850
여기서는 이전과 같지만, 현재

206
00:08:28,850 --> 00:08:32,370
숨겨진 상태 계산이 그 시점의

207
00:08:32,370 --> 00:08:35,370
입력과 이전 RNN 상태에

208
00:08:35,370 --> 00:08:38,669
의존한다는 것을 명확히 보여줍니다.

209
00:08:38,669 --> 00:08:41,409
즉, 각 출력을 계산하는 데 정확히

210
00:08:41,409 --> 00:08:43,890
무엇이 필요한지 더 명확히 모델링하는

211
00:08:43,890 --> 00:08:48,830
것이고, 각 RNN 단계마다 계산 그래프를 거꾸로 이동합니다.

212
00:08:48,830 --> 00:08:51,330
지금까지는 말을 사용해서 설명했는데,

213
00:08:51,330 --> 00:08:53,990
이제 수학적 식으로 공식화해 보겠습니다.

214
00:08:53,990 --> 00:08:57,670
기본 아이디어는 벡터 시퀀스 x를 처리하려고

215
00:08:57,670 --> 00:08:59,230
한다는 것입니다.

216
00:08:59,230 --> 00:09:01,430
그리고 우리는 이 재귀 공식을 매

217
00:09:01,430 --> 00:09:03,310
시간 단계마다 적용하고 있습니다.

218
00:09:03,310 --> 00:09:06,910
그래서 새로운 은닉 상태는 이전 은닉

219
00:09:06,910 --> 00:09:10,670
상태와 해당 시간 단계의 입력 벡터의

220
00:09:10,670 --> 00:09:14,350
함수이고, 보통 활성화 함수와 함께

221
00:09:14,350 --> 00:09:15,910
파라미터 W를

222
00:09:15,910 --> 00:09:17,630
가진 함수입니다. 이것은 우리가 처음

223
00:09:17,630 --> 00:09:21,110
배운 신경망 층과 매우 비슷하다고 생각할 수

224
00:09:21,110 --> 00:09:23,750
있는데, 가중치 행렬에 곱하고

225
00:09:23,750 --> 00:09:27,650
그 다음에 활성화 함수를 적용하는 방식입니다.

226
00:09:27,650 --> 00:09:29,070
여기서도 같은 원리입니다.

227
00:09:29,070 --> 00:09:31,970
유일한 차이점은 이제 이것이 재귀 공식이라는 점입니다.

228
00:09:31,970 --> 00:09:39,150
그래서 은닉 상태를 계산할 때마다 같은 W 집합과

229
00:09:39,150 --> 00:09:43,410
같은 활성화 함수를 사용합니다.

230
00:09:43,410 --> 00:09:48,450
기본적으로, 제가 말했듯이 이것은 재귀 공식입니다.

231
00:09:48,450 --> 00:09:50,370
그리고 실제 출력을 얻으려면, 이 파란

232
00:09:50,370 --> 00:09:51,710
블록을 어떻게 계산할까요?

233
00:09:51,710 --> 00:09:55,370
우리는 숨겨진 차원 상태를 출력

234
00:09:55,370 --> 00:09:57,850
차원으로 변환하는 별도의

235
00:09:57,850 --> 00:10:02,810
매개변수 집합에 의존하는 별도의 함수가

236
00:10:02,810 --> 00:10:03,550
있습니다.

237
00:10:03,550 --> 00:10:06,410
그리고 숨겨진 상태를 출력으로 변환하는 가중치

238
00:10:06,410 --> 00:10:07,350
집합도 있습니다.

239
00:10:07,350 --> 00:10:08,710
그래서 이것은 두 가지를 하나로 합니다.

240
00:10:08,710 --> 00:10:10,530
숨겨진 상태의 차원 크기에서 출력의

241
00:10:10,530 --> 00:10:12,613
차원 크기로 벡터의 차원을 변경합니다.

242
00:10:12,613 --> 00:10:15,150
숨겨진 상태 차원은 원하는 대로 설정할 수 있습니다.

243
00:10:15,150 --> 00:10:18,330
그리고 그곳에서 변환도 제공합니다.

244
00:10:18,330 --> 00:10:23,050
그래서 W는 숨겨진 상태에 곱해서 결과를 얻는 가중치 행렬입니다.

245
00:10:23,050 --> 00:10:24,910
즉, 두 가지

246
00:10:24,910 --> 00:10:25,847
역할을 합니다.

247
00:10:25,847 --> 00:10:28,430
숨겨진 상태를 출력 차원으로 변환합니다.

248
00:10:28,430 --> 00:10:31,013
숨겨진 상태와 출력은 서로 다른 차원일 수 있습니다.

249
00:10:31,013 --> 00:10:35,070
그리고 또한, 이것은 여러분이 학습하는 가중치 행렬입니다.

250
00:10:35,070 --> 00:10:36,950
그래서 이것은 차원 변환뿐만

251
00:10:36,950 --> 00:10:38,930
아니라 은닉 상태에

252
00:10:38,930 --> 00:10:40,120
변환을 적용합니다.

253
00:10:40,120 --> 00:10:41,870
즉, 은닉 상태를

254
00:10:41,870 --> 00:10:45,390
출력으로 변환하는 방법이 바로 W hy입니다.

255
00:10:45,390 --> 00:10:48,870
이전 슬라이드는 새로운 은닉 상태를 계산하는 방법이었죠.

256
00:10:48,870 --> 00:10:51,190
본질적으로 같은 아이디어로, 같은

257
00:10:51,190 --> 00:10:53,493
파라미터 집합을 사용해 재귀적으로 수행하는

258
00:10:53,493 --> 00:10:54,910
겁니다만, 은닉 상태를

259
00:10:54,910 --> 00:10:56,830
계산하는 함수와 파라미터 집합이

260
00:10:56,830 --> 00:10:57,620
하나 있고,

261
00:10:57,620 --> 00:10:59,870
출력을 계산하는 함수와 파라미터

262
00:10:59,870 --> 00:11:02,430
집합이 또 하나 있습니다.

263
00:11:02,430 --> 00:11:06,590
이는 작업 유형과 RNN 모델링 방식에 따라 다릅니다.

264
00:11:06,590 --> 00:11:09,867
네, 각 시간 단계마다 같은 가중치를 공유하지만, 여기에는 두

265
00:11:09,867 --> 00:11:11,450
가지 다른 부분이 있습니다.

266
00:11:11,450 --> 00:11:13,710
하나는 기본적으로 어떻게 계산하느냐인데, 좀

267
00:11:13,710 --> 00:11:16,210
더 구체적인 예제로 진행하면서 명확해질 겁니다.

268
00:11:16,210 --> 00:11:19,010
즉, RNN의 내부 상태인 새로운 은닉 상태를 실제로

269
00:11:19,010 --> 00:11:20,755
어떻게 계산하느냐 하는 거죠.

270
00:11:20,755 --> 00:11:22,630
그리고 그 은닉 상태를

271
00:11:22,630 --> 00:11:26,390
출력으로 어떻게 변환하느냐가 이 슬라이드 내용입니다.

272
00:11:26,390 --> 00:11:31,830
이 펼쳐진(unrolled) 다이어그램을 보면, 은닉 상태를

273
00:11:31,830 --> 00:11:35,670
어떤 값으로 초기화해야 한다는 걸 알 수

274
00:11:35,670 --> 00:11:36,730
있습니다.

275
00:11:36,730 --> 00:11:40,890
보통 이를 h,0이라고 부르며, 원하는 값으로 초기화할

276
00:11:40,890 --> 00:11:42,070
수 있습니다.

277
00:11:42,070 --> 00:11:47,170
원칙적으로는 보통 학습되는 입력 벡터이지만, 이제

278
00:11:47,170 --> 00:11:50,250
펼쳐진 RNN의 각 단계를

279
00:11:50,250 --> 00:11:53,195
구체적으로 살펴보며 순전파가

280
00:11:53,195 --> 00:11:55,570
어떻게 이루어지는지

281
00:11:55,570 --> 00:11:58,170
예를 들어 설명하겠습니다.

282
00:11:58,170 --> 00:12:00,730
이미 몇몇 질문에서 나온 점인데,

283
00:12:00,730 --> 00:12:02,890
우리는 벡터 시퀀스

284
00:12:02,890 --> 00:12:05,110
x를 처리하며, 각 시간

285
00:12:05,110 --> 00:12:07,610
단계마다 이 재귀 공식을

286
00:12:07,610 --> 00:12:08,410
적용합니다.

287
00:12:08,410 --> 00:12:12,142
그래서 은닉 상태를 계산할 때 매 시간 단계마다 같은 함수와 같은

288
00:12:12,142 --> 00:12:13,850
파라미터 집합이 사용된다는

289
00:12:13,850 --> 00:12:15,397
점을 확실히 알 수 있습니다.

290
00:12:15,397 --> 00:12:17,730
그리고 은닉 상태로부터 출력을 예측할

291
00:12:17,730 --> 00:12:19,290
때는 매 시간 단계마다

292
00:12:19,290 --> 00:12:22,850
별도의 함수와 별도의 파라미터 집합이 항상 사용됩니다.

293
00:12:22,850 --> 00:12:25,850
네, 그래서 이전 y 값들이 새로운 hidden state에 영향을 줄 수
있나요?

294
00:12:25,850 --> 00:12:27,088
어떤 수식에서는 그렇습니다.

295
00:12:27,088 --> 00:12:29,630
그리고 왜 그런 방식을 사용하는지 한 가지 예를 실제로 살펴보겠습니다.

296
00:12:29,630 --> 00:12:33,770
가장 흔히 사용하는 경우는 다음 값을 예측하려고 할 때입니다—언어

297
00:12:33,770 --> 00:12:36,055
모델링이나 자기회귀 모델링

298
00:12:36,055 --> 00:12:37,430
작업에서 이전 값들을

299
00:12:37,430 --> 00:12:40,030
바탕으로 한 값을 예측할 때,

300
00:12:40,030 --> 00:12:43,055
사람들은 이전 값을 입력으로 그냥 사용합니다.

301
00:12:43,055 --> 00:12:44,430
그래서 일반적으로 사람들이

302
00:12:44,430 --> 00:12:47,150
y가 다음 hidden state에 어떻게 영향을 미치는지

303
00:12:47,150 --> 00:12:49,430
명시적으로 수식화하는 방법이 바로 이겁니다.

304
00:12:49,430 --> 00:12:53,390
첫 번째 타임스텝에서 h와 x의 차이는 무엇인가요?

305
00:12:53,390 --> 00:12:57,190
기본적으로 서로 다른 가중치를 사용합니다.

306
00:12:57,190 --> 00:13:02,190
h,0은 모든 hidden state를 다음 상태로

307
00:13:02,190 --> 00:13:06,243
업데이트하는 데 사용되는 가중치를 사용합니다.

308
00:13:06,243 --> 00:13:08,410
가중치가 어떻게 생겼는지 정확히

309
00:13:08,410 --> 00:13:10,327
살펴보겠지만, 간단히 말해 서로

310
00:13:10,327 --> 00:13:12,290
다른 가중치를 사용한다는 겁니다.

311
00:13:12,290 --> 00:13:15,270
사람들이 Vanilla RNN이라고 할

312
00:13:15,270 --> 00:13:18,470
때, 보통 거의 정확히 이 유형의 모델을

313
00:13:18,470 --> 00:13:21,990
말하는데, 여기서 hidden state t는

314
00:13:21,990 --> 00:13:26,550
tanh 또는 쌍곡선 탄젠트 함수를 활성화 함수로 사용합니다.

315
00:13:26,550 --> 00:13:28,770
이 함수는 1과 -1 사이에 값이

316
00:13:28,770 --> 00:13:30,070
제한되어 있어서 좋습니다.

317
00:13:30,070 --> 00:13:32,930
그래서 이 연산을 반복해도 값이 이

318
00:13:32,930 --> 00:13:35,410
범위 내에 머무르게 됩니다.

319
00:13:35,410 --> 00:13:36,870
이것은 좋은 특성입니다.

320
00:13:36,870 --> 00:13:38,090
또한 0을 중심으로 하고,

321
00:13:38,090 --> 00:13:40,030
양수와 음수 값을 모두 표현할 수 있습니다.

322
00:13:40,030 --> 00:13:43,410
이것이 사람들이 tanh를 사용하는 이유입니다.

323
00:13:43,410 --> 00:13:48,430
또한 여기서 출력 함수 fy가 있을 수도 있지만, 가장 단순한 경우

324
00:13:48,430 --> 00:13:50,890
출력 yt는 hidden

325
00:13:50,890 --> 00:13:53,830
state에 행렬을 곱한 것일 수 있습니다.

326
00:13:53,830 --> 00:13:57,410
이것이 정말 가장 단순한 RNN의 공식화입니다.

327
00:13:57,410 --> 00:14:00,530
오늘 강의에서 구체적인 예제로

328
00:14:00,530 --> 00:14:05,490
다룰 것은 수동으로 순환 신경망을 만드는

329
00:14:05,490 --> 00:14:06,983
아이디어입니다.

330
00:14:06,983 --> 00:14:09,650
그래서 이걸 경사 하강법이나 다른 방법으로

331
00:14:09,650 --> 00:14:10,750
학습하지는 않을 겁니다.

332
00:14:10,750 --> 00:14:15,030
직접 손으로 어떻게 구성할 수 있는지 보여드릴 거예요.

333
00:14:15,030 --> 00:14:16,490
그리고 순전파가 어떻게

334
00:14:16,490 --> 00:14:18,010
진행되는지, 각

335
00:14:18,010 --> 00:14:20,550
가중치 행렬이 무엇을 하는지, 출력이

336
00:14:20,550 --> 00:14:23,610
어떻게 계산되는지 이해할 수 있을 겁니다.

337
00:14:23,610 --> 00:14:25,290
아주 단순한 예제여야

338
00:14:25,290 --> 00:14:26,450
하니까, 모든

339
00:14:26,450 --> 00:14:28,690
가중치를 다 살펴볼 겁니다.

340
00:14:28,690 --> 00:14:32,330
0과 1로 이루어진 시퀀스가 주어지고,

341
00:14:32,330 --> 00:14:36,730
연속해서 1이 두 번 나오면 1을 출력하는 게 목표입니다.

342
00:14:36,730 --> 00:14:39,750
즉, 반복된 1을 감지해서 그렇지 않으면

343
00:14:39,750 --> 00:14:41,250
0을 출력하는 거죠.

344
00:14:41,250 --> 00:14:45,430
입력 시퀀스가 0, 1, 0, 1로 들어오는 걸 볼 수 있습니다.

345
00:14:45,430 --> 00:14:47,050
지금까지는 반복된 1이 없습니다.

346
00:14:47,050 --> 00:14:48,590
하지만 이제 반복된 1이 나오고,

347
00:14:48,590 --> 00:14:50,840
또 다른 반복된 1도 있습니다. 여기서

348
00:14:50,840 --> 00:14:52,110
연속으로 두 개가 나오니까요.

349
00:14:52,110 --> 00:14:54,443
이런 유형의 모델을 만드는 겁니다.

350
00:14:54,443 --> 00:14:55,610
이 작업을 수행하려고 하는 거죠.

351
00:14:55,610 --> 00:14:58,430
특히 입력마다 하나의 출력을 내는

352
00:14:58,430 --> 00:15:01,910
many to many 시퀀스 모델링 작업입니다.

353
00:15:01,910 --> 00:15:04,610
지금까지는 개념적으로 이야기했는데,

354
00:15:04,610 --> 00:15:07,310
이 작업을 수행하는 RNN을

355
00:15:07,310 --> 00:15:11,265
만든다면, 숨겨진 상태에 어떤 정보가 담겨야 할까요?

356
00:15:11,265 --> 00:15:13,390
모델 내부 상태에 어떤

357
00:15:13,390 --> 00:15:17,670
정보가 저장되어야 이 작업을 할 수 있을까요?

358
00:15:17,670 --> 00:15:20,077
네, 이전 시점의 입력입니다.

359
00:15:20,077 --> 00:15:22,410
출력이 은닉 상태에만 의존한다면, 우리는

360
00:15:22,410 --> 00:15:23,690
무엇을 더 알아야 할까요?

361
00:15:23,690 --> 00:15:24,630
그리고 현재--

362
00:15:24,630 --> 00:15:26,070
네, 맞습니다.

363
00:15:26,070 --> 00:15:28,030
그래서 이것이 우리가 은닉

364
00:15:28,030 --> 00:15:29,770
상태에 담아야 할 정보입니다.

365
00:15:29,770 --> 00:15:34,190
이전 입력과 현재 x 값, 즉 0 또는 1입니다.

366
00:15:34,190 --> 00:15:36,410
이렇게 할 건데, 은닉 상태

367
00:15:36,410 --> 00:15:38,830
t를 3차원 벡터로 설정하겠습니다.

368
00:15:38,830 --> 00:15:41,247
3차원인 이유는 출력 단계

369
00:15:41,247 --> 00:15:44,190
계산할 때 유용하기 때문인데,

370
00:15:44,190 --> 00:15:47,770
1이 없어도 만들 수는 있습니다.

371
00:15:47,770 --> 00:15:49,730
오늘 강의 목적상 수학을

372
00:15:49,730 --> 00:15:51,952
쉽게 하기 위한 것입니다.

373
00:15:51,952 --> 00:15:53,910
그리고 다른 정보는 현재 값입니다.

374
00:15:53,910 --> 00:15:57,130
이 값은 0 또는 1이고, 이전 값도 0

375
00:15:57,130 --> 00:15:58,110
또는 1입니다.

376
00:15:58,110 --> 00:16:00,610
초기값은 001로 설정할

377
00:16:00,610 --> 00:16:03,970
건데, 사실상 두 개의 0을 연속으로

378
00:16:03,970 --> 00:16:06,890
본 상태라고 가정하는 겁니다.

379
00:16:06,890 --> 00:16:09,970
네, 이렇게 할 겁니다.

380
00:16:09,970 --> 00:16:11,840
이것이 우리가 은닉 상태에서

381
00:16:11,840 --> 00:16:13,590
추적하려는 변수 유형입니다.

382
00:16:13,590 --> 00:16:15,190
그리고 이렇게 h,0을 초기화할 겁니다.

383
00:16:15,190 --> 00:16:16,570
초기화 방법은 여러 가지가

384
00:16:16,570 --> 00:16:19,030
있고, 학습할 수도 있다고 말씀드렸죠.

385
00:16:19,030 --> 00:16:21,450
이것이 우리가 초기화할 값입니다.

386
00:16:21,450 --> 00:16:24,050
자, 이제 코드를 단계별로

387
00:16:24,050 --> 00:16:25,510
살펴보겠습니다.

388
00:16:25,510 --> 00:16:28,210
지금 화면에 그냥 보여드리고 있습니다.

389
00:16:28,210 --> 00:16:32,050
하지만 ReLU도 할 거고요, 아, 죄송합니다, 이 슬라이드에서 제가

390
00:16:32,050 --> 00:16:33,550
놓친 한 가지가 있는데, 활성화

391
00:16:33,550 --> 00:16:35,383
함수를 ReLU로 설정해서 수학을

392
00:16:35,383 --> 00:16:36,610
쉽게 하려는 겁니다.

393
00:16:36,610 --> 00:16:39,387
그래서 0과 값 중 큰 쪽을 취하는 max 함수입니다.

394
00:16:39,387 --> 00:16:41,470
사실상 여기서는 0과 1만 다루고

395
00:16:41,470 --> 00:16:42,090
있습니다.

396
00:16:42,090 --> 00:16:44,490
그래서 생각하기가 꽤 간단해집니다.

397
00:16:44,490 --> 00:16:44,990
네,

398
00:16:44,990 --> 00:16:48,190
tanh로도 작동하도록 구성할 수는 있을 겁니다.

399
00:16:48,190 --> 00:16:49,630
하지만 이건 제가

400
00:16:49,630 --> 00:16:52,050
실행 방법 예제로 만든 겁니다.

401
00:16:52,050 --> 00:16:55,390
그래서 수학을 정말 쉽게 하기 위해 ReLU를 사용할 겁니다.

402
00:16:55,390 --> 00:16:57,350
하지만 tanh로도 이걸 할 수

403
00:16:57,350 --> 00:16:59,430
있는 모델을 만들 수는 있겠죠.

404
00:16:59,430 --> 00:17:02,830
네, 좋습니다.

405
00:17:02,830 --> 00:17:04,569
그래서 ReLU가 있습니다.

406
00:17:04,569 --> 00:17:06,890
여기 두 개의 특정 가중치가 있습니다.

407
00:17:06,890 --> 00:17:10,390
첫 번째 가중치는 이전 은닉

408
00:17:10,390 --> 00:17:13,790
상태를 변환하는 역할을 합니다.

409
00:17:13,790 --> 00:17:16,869
이전 은닉 상태에 변환을 적용해서

410
00:17:16,869 --> 00:17:20,130
다음 상태를 계산하는 거죠.

411
00:17:20,130 --> 00:17:21,849
그리고 여기 두 번째

412
00:17:21,849 --> 00:17:25,869
가중치가 있는데, 입력 x를 은닉 상태 차원으로

413
00:17:25,869 --> 00:17:28,290
변환하고 변환을 적용합니다.

414
00:17:28,290 --> 00:17:31,030
그래서 두 번째 가중치를 설정하는 겁니다.

415
00:17:31,030 --> 00:17:32,890
그래서 현재의 hidden

416
00:17:32,890 --> 00:17:36,610
state는 이전 hidden state와 현재 시간 단계의

417
00:17:36,610 --> 00:17:37,490
함수입니다.

418
00:17:37,490 --> 00:17:41,250
그래서 시간 단계 t에서 이 hidden

419
00:17:41,250 --> 00:17:46,010
state를 계산하려고 할 때, 먼저 이 현재 값을 계산하려고

420
00:17:46,010 --> 00:17:46,670
합니다.

421
00:17:46,670 --> 00:17:49,470
그래서 여기서 x 값을 사용할 겁니다.

422
00:17:49,470 --> 00:17:55,250
가중치는 3x1 열 벡터로 1, 0, 0 값을 가지도록 설정할

423
00:17:55,250 --> 00:18:00,790
겁니다. 그래서 x가 0일 때 행렬 곱을 하면 0 벡터가

424
00:18:00,790 --> 00:18:06,292
나오고, x가 1일 때는 1, 0, 0이 나오게 됩니다.

425
00:18:06,292 --> 00:18:07,750
그리고 이것을 다른 항에

426
00:18:07,750 --> 00:18:09,667
더할 건데, 기본적으로 이것이 현재

427
00:18:09,667 --> 00:18:11,030
값을 계산하는 겁니다.

428
00:18:11,030 --> 00:18:15,110
그래서 위에는 0이거나 1이 될 겁니다.

429
00:18:15,110 --> 00:18:18,490
그리고 이것은 여기 첫 번째 연산을 기반으로 계산됩니다.

430
00:18:21,770 --> 00:18:23,970
이렇게 입력을 기반으로

431
00:18:23,970 --> 00:18:26,790
현재 값을 계산하는 거죠.

432
00:18:26,790 --> 00:18:31,390
이제 hidden state 변환을 어떻게 하는지 이야기해

433
00:18:31,390 --> 00:18:32,310
보겠습니다.

434
00:18:32,310 --> 00:18:35,550
그래서 여기 위쪽 값에는 현재 값만 사용하려고

435
00:18:35,550 --> 00:18:36,050
합니다.

436
00:18:36,050 --> 00:18:38,750
그래서 가중치 행렬에서는 맨 위 행에 0을 넣을 겁니다.

437
00:18:38,750 --> 00:18:40,230
이 말은 이전 hidden

438
00:18:40,230 --> 00:18:41,750
state와 곱할 때

439
00:18:41,750 --> 00:18:43,930
위쪽 값이 0이 된다는 뜻입니다.

440
00:18:43,930 --> 00:18:46,270
그래서 0에 오른쪽 값이

441
00:18:46,270 --> 00:18:47,490
더해지는 거죠.

442
00:18:47,490 --> 00:18:50,270
이렇게 해서 이전 hidden state에 따라 변하지

443
00:18:50,270 --> 00:18:51,870
않도록 유지할 수 있습니다.

444
00:18:51,870 --> 00:18:55,550
그리고 다음 행에는 1, 0, 0으로 설정할 겁니다.

445
00:18:55,550 --> 00:18:56,510
왜 이렇게 하냐면요?

446
00:18:56,510 --> 00:18:58,390
이전 시점의 hidden

447
00:18:58,390 --> 00:19:00,670
state가 있다고 상상해 보겠습니다.

448
00:19:00,670 --> 00:19:05,070
그리고 지금 이전 값을 이전 시점의 현재 값으로

449
00:19:05,070 --> 00:19:06,890
설정하고 싶습니다.

450
00:19:06,890 --> 00:19:08,490
그래서 1, 0, 0이 있습니다.

451
00:19:08,490 --> 00:19:10,930
이것은 h,t-1에 곱해질 겁니다.

452
00:19:10,930 --> 00:19:14,630
현재 값을 이번 시점의 이전 값으로

453
00:19:14,630 --> 00:19:16,810
설정할 겁니다.

454
00:19:16,810 --> 00:19:19,650
기본적으로 이 항은 위에는

455
00:19:19,650 --> 00:19:22,870
0이고, 두 번째 항은 이전 시점

456
00:19:22,870 --> 00:19:24,650
입력 값이 됩니다.

457
00:19:24,650 --> 00:19:27,730
그리고 마지막 부분은 1을 유지해서

458
00:19:27,730 --> 00:19:31,170
모든 계산에서 이 1을 유지합니다.

459
00:19:31,170 --> 00:19:33,290
요약하자면, 오른쪽

460
00:19:33,290 --> 00:19:36,130
항이 현재 값을

461
00:19:36,130 --> 00:19:39,230
추적하도록 0을 둔 겁니다.

462
00:19:39,230 --> 00:19:42,050
여기 1은 이전 시점의

463
00:19:42,050 --> 00:19:43,930
현재 값을

464
00:19:43,930 --> 00:19:46,810
이번 시점의 이전 값으로

465
00:19:46,810 --> 00:19:50,130
복사하기 위한 겁니다.

466
00:19:50,130 --> 00:19:52,330
즉, h,t 이전이

467
00:19:52,330 --> 00:19:57,230
h,t와 같게 하고, 해당 값을

468
00:19:57,230 --> 00:20:00,090
한 칸 아래로 이동시키는

469
00:20:00,090 --> 00:20:01,290
거죠.

470
00:20:01,290 --> 00:20:03,682
그리고 이것은 1을 복사한 겁니다.

471
00:20:03,682 --> 00:20:05,390
그럼 이제 출력은 어떻게 얻을까요?

472
00:20:05,390 --> 00:20:10,010
앞서 말한 Whh와 Wxh 같은 가중치 행렬을 통해

473
00:20:10,010 --> 00:20:15,010
값을 추적하는 방법을 이야기했습니다. 만약 hidden

474
00:20:15,010 --> 00:20:17,250
state를 출력

475
00:20:17,250 --> 00:20:20,110
차원으로 변환하는 가중치 행렬이

476
00:20:20,110 --> 00:20:23,010
있다면, 1x3 크기여야 합니다.

477
00:20:23,010 --> 00:20:26,350
즉, hidden 차원을 입력으로 받아 단일

478
00:20:26,350 --> 00:20:28,050
값을 출력하는 겁니다.

479
00:20:28,050 --> 00:20:31,870
이것은 여기 값들과 여기 값들 간의 내적(dot

480
00:20:31,870 --> 00:20:32,930
product)입니다.

481
00:20:32,930 --> 00:20:35,830
따라서 이것은 현재 값과 이전 값에서 1을

482
00:20:35,830 --> 00:20:39,750
뺀 값에 해당합니다, 왜냐하면 여기서 -1을 곱하기

483
00:20:39,750 --> 00:20:40,250
때문입니다.

484
00:20:40,250 --> 00:20:42,710
여기서 1이 유용해지고,

485
00:20:42,710 --> 00:20:46,570
현재 값이 여기서 1과 연관되어 있습니다.

486
00:20:46,570 --> 00:20:49,150
그리고 이전 값도 여기서

487
00:20:49,150 --> 00:20:51,110
1과 연관되어 있죠.

488
00:20:51,110 --> 00:20:52,450
이렇게 실제로 계산을 합니다.

489
00:20:52,450 --> 00:20:57,570
생각해보면, 이 일반적인 공식이 작동할 겁니다.

490
00:20:57,570 --> 00:21:02,070
예를 들어 여기서 보면, 현재 값과 이전 값이

491
00:21:02,070 --> 00:21:06,350
2이고 1을 빼면 ReLU 안의 왼쪽 항은

492
00:21:06,350 --> 00:21:07,690
1이 됩니다.

493
00:21:07,690 --> 00:21:09,670
1과 0 중 최대값은 1이죠.

494
00:21:09,670 --> 00:21:13,390
만약 둘 다 0이면, 1을 빼서 0이 됩니다.

495
00:21:13,390 --> 00:21:14,890
여기서는 1과 0입니다.

496
00:21:14,890 --> 00:21:16,510
그래서 여전히 0이 나오죠.

497
00:21:16,510 --> 00:21:19,770
이렇게 가중치 행렬을 구성할 수 있습니다.

498
00:21:19,770 --> 00:21:23,650
하지만 잠시 멈추고 이 계산 과정 중에 질문이

499
00:21:23,650 --> 00:21:26,450
있는지 이야기하고 싶습니다. 이

500
00:21:26,450 --> 00:21:29,865
수업에서 행렬과 벡터 곱셈을 모두 직접

501
00:21:29,865 --> 00:21:32,490
다루는 유일한 예제이기 때문입니다.

502
00:21:32,490 --> 00:21:34,290
나머지는 보통 사람들이

503
00:21:34,290 --> 00:21:37,770
레이어를 어떻게 조합하는지에 대한 고수준

504
00:21:37,770 --> 00:21:39,630
설명이 될 겁니다.

505
00:21:39,630 --> 00:21:43,610
그래서 행렬과 벡터가 어떻게 추적되고 곱해지고

506
00:21:43,610 --> 00:21:46,170
업데이트되는지에 대해 질문이 있는지

507
00:21:46,170 --> 00:21:47,790
잠시 확인하고 싶습니다.

508
00:21:47,790 --> 00:21:49,290
네, 질문은 가중치

509
00:21:49,290 --> 00:21:51,950
행렬을 어떻게 구성하느냐는 거죠?

510
00:21:51,950 --> 00:21:53,560
정말 좋은 질문이고,

511
00:21:53,560 --> 00:21:55,310
슬라이드에 넣으려고 했습니다.

512
00:21:55,310 --> 00:21:56,848
실제로 어떻게 하겠습니까?

513
00:21:56,848 --> 00:21:58,890
이 수업에서 항상 가중치 행렬을 찾는

514
00:21:58,890 --> 00:21:59,830
방법과 같습니다.

515
00:21:59,830 --> 00:22:01,150
우리는 경사 하강법을 사용할 겁니다.

516
00:22:01,150 --> 00:22:03,570
그리고 여러 시간 단계가 있을 때 경사 하강법을 어떻게

517
00:22:03,570 --> 00:22:05,410
하는지, 각 시간 단계에서 손실이

518
00:22:05,410 --> 00:22:07,550
계산될 수도 있는 상황에 대해 이야기할 겁니다.

519
00:22:07,550 --> 00:22:10,710
바로 다음에 자세히 다룰 내용입니다.

520
00:22:10,710 --> 00:22:13,850
정말 좋은 질문이고 강의와 매우 관련이 있습니다.

521
00:22:13,850 --> 00:22:16,450
이것은 단지 예제일 뿐이며,

522
00:22:16,450 --> 00:22:20,880
모든 가중치 행렬이 어떻게 곱해지는지 보여줍니다.

523
00:22:20,880 --> 00:22:22,728
기본적으로 만약 이걸

524
00:22:22,728 --> 00:22:24,520
초기화하고 다른 작업을

525
00:22:24,520 --> 00:22:26,320
위해 훈련시키려 한다면,

526
00:22:26,320 --> 00:22:29,840
이것이 전이 학습이 될 겁니다. 즉, 이

527
00:22:29,840 --> 00:22:31,880
가중치로 초기화하는 거죠.

528
00:22:31,880 --> 00:22:35,412
하지만 실제로는 잘 작동하지 않을 겁니다. 왜냐하면 숨겨진

529
00:22:35,412 --> 00:22:37,120
상태가 너무 작고, 보통은

530
00:22:37,120 --> 00:22:40,022
훨씬 큰 숨겨진 상태를 사용하기 때문입니다.

531
00:22:40,022 --> 00:22:42,480
슬라이드에 맞게 간단한 예제를

532
00:22:42,480 --> 00:22:43,640
만든 겁니다.

533
00:22:43,640 --> 00:22:46,160
네, 두 번째 행을 다시 설명하겠습니다.

534
00:22:46,160 --> 00:22:51,280
여기서 h,t-1이 열 벡터라고 상상해보세요.

535
00:22:51,280 --> 00:22:55,560
행렬 곱셈을 할 때, 왼쪽 항의 값을

536
00:22:55,560 --> 00:22:59,520
얻기 위해 두 번째 행은 값을

537
00:22:59,520 --> 00:23:02,840
가져와서 회전시키고 여기 값들과

538
00:23:02,840 --> 00:23:04,540
내적을 합니다.

539
00:23:04,540 --> 00:23:08,680
그래서 두 번째 행의 계산 결과는

540
00:23:08,680 --> 00:23:12,940
벡터의 맨 위 값과 같아집니다.

541
00:23:12,940 --> 00:23:16,200
이 단계에서 현재 값을 이전 값으로 이동시키는 방법이

542
00:23:16,200 --> 00:23:17,560
바로 이것입니다.

543
00:23:17,560 --> 00:23:20,420
그래서 이 행렬 곱셈의

544
00:23:20,420 --> 00:23:27,100
최종 결과는 두 번째 값이 t-1 시점의 현재

545
00:23:27,100 --> 00:23:30,140
값이 되도록 합니다.

546
00:23:30,140 --> 00:23:33,060
t-1과 행렬 곱을 하고, 이 연산의

547
00:23:33,060 --> 00:23:36,660
두 번째 행과 이 연산의 두 번째 행은

548
00:23:36,660 --> 00:23:40,220
모두 숨겨진 상태 크기의 벡터를 줍니다.

549
00:23:40,220 --> 00:23:42,700
그리고 이 둘을 더하는 거죠.

550
00:23:42,700 --> 00:23:45,540
네, 왼쪽은 이전 값을 전달하고,

551
00:23:45,540 --> 00:23:47,760
오른쪽은 현재 값을 처리합니다.

552
00:23:47,760 --> 00:23:51,780
그리고 이것은 RNN에서 어떻게 작동하는지에 대한 예시를

553
00:23:51,780 --> 00:23:55,060
넘어서, 현재 입력에 곱해지는 가중치

554
00:23:55,060 --> 00:23:57,620
행렬과 이전 은닉 상태에 곱해지는

555
00:23:57,620 --> 00:24:00,680
다른 가중치 행렬이 있을 때의 경우입니다.

556
00:24:00,680 --> 00:24:03,460
그래서 이 가중치 행렬들은 특정 문제보다

557
00:24:03,460 --> 00:24:06,540
더 일반적으로 추적하는 역할을 합니다.

558
00:24:06,540 --> 00:24:11,280
그럼 실제로 어떻게 그래디언트를 계산할까요?

559
00:24:11,280 --> 00:24:13,080
계산 그래프를 살펴보겠습니다.

560
00:24:13,080 --> 00:24:16,720
이전보다 좀 더 명확하게 그려보면, x1과

561
00:24:16,720 --> 00:24:19,580
x2가 들어오고, x들의

562
00:24:19,580 --> 00:24:20,940
시퀀스가 있습니다.

563
00:24:20,940 --> 00:24:23,480
각 시간 단계마다 은닉 상태를

564
00:24:23,480 --> 00:24:27,120
계산하고, 이 계산들 모두에 같은 W,

565
00:24:27,120 --> 00:24:30,380
같은 가중치 행렬을 사용합니다.

566
00:24:30,380 --> 00:24:31,880
그래서 그래디언트를 계산할

567
00:24:31,880 --> 00:24:35,440
때 이 점을 고려해야 합니다. 이제 many to many

568
00:24:35,440 --> 00:24:37,580
시나리오부터 시작해 보겠습니다.

569
00:24:37,580 --> 00:24:40,520
각 입력마다 출력이 있습니다.

570
00:24:40,520 --> 00:24:43,727
이 시나리오에서는 각 출력에 대해 손실을 계산할 수도 있는데,

571
00:24:43,727 --> 00:24:45,560
이는 해당 단계에서 출력이

572
00:24:45,560 --> 00:24:47,100
얼마나 정확한지를 나타냅니다.

573
00:24:47,100 --> 00:24:53,000
이 설정에서는 각 단계마다 손실이 있고, 이들을 모두 합산해

574
00:24:53,000 --> 00:24:55,875
전체 손실을 구할 수 있습니다.

575
00:24:55,875 --> 00:24:58,000
이것이 전체 입력 시퀀스에

576
00:24:58,000 --> 00:24:59,640
대한 손실입니다.

577
00:24:59,640 --> 00:25:07,303
역전파를 할 때, 최종 손실을 계산하면 각 시간 단계별

578
00:25:07,303 --> 00:25:08,720
손실도 계산할

579
00:25:08,720 --> 00:25:11,040
수 있습니다.

580
00:25:11,040 --> 00:25:14,280
또는 공식에 따라 다릅니다.

581
00:25:14,280 --> 00:25:16,720
시간 단계별 손실을 계산한다면, 각각을

582
00:25:16,720 --> 00:25:18,340
독립적으로 다룰 수 있습니다.

583
00:25:18,340 --> 00:25:20,460
때로는 시간 단계별 손실을

584
00:25:20,460 --> 00:25:23,180
기반으로 전체 손실을 구하기도 합니다.

585
00:25:23,180 --> 00:25:28,020
또한 각 W에 대한 최종 그래디언트를 구할 수

586
00:25:28,020 --> 00:25:29,180
있습니다.

587
00:25:29,180 --> 00:25:32,380
각 시간 단계별로 그래디언트를 계산한 후,

588
00:25:32,380 --> 00:25:34,400
모두 합산하는 방식입니다.

589
00:25:34,400 --> 00:25:36,560
이것이 실제로 어떻게 작동하는지입니다.

590
00:25:36,560 --> 00:25:39,980
각 시간 단계마다 다른 W가 있다고 상상할 수 있습니다.

591
00:25:39,980 --> 00:25:43,800
각기 다른 W에 대해 다른 그래디언트를 계산하도록

592
00:25:43,800 --> 00:25:46,300
계산 그래프가 어떻게 구성될 수

593
00:25:46,300 --> 00:25:48,800
있는지 쉽게 이해할 수 있을 겁니다.

594
00:25:48,800 --> 00:25:51,620
그래서 계산 목적상 단일 W를

595
00:25:51,620 --> 00:25:57,302
여러 개의 다른 W 집합처럼 취급하지만, 결국에는 같은 가중치

596
00:25:57,302 --> 00:25:59,260
행렬이 곱해지기 때문에

597
00:25:59,260 --> 00:26:00,635
모든 그래디언트를

598
00:26:00,635 --> 00:26:02,040
합칩니다.

599
00:26:02,040 --> 00:26:06,260
개념적으로는 각 시간 단계마다 다른 W를

600
00:26:06,260 --> 00:26:08,740
사용하는 것처럼 계산하지만,

601
00:26:08,740 --> 00:26:10,560
가중치 값이

602
00:26:10,560 --> 00:26:12,740
같으므로 각 시간

603
00:26:12,740 --> 00:26:15,240
단계에서 계산한 모든

604
00:26:15,240 --> 00:26:18,000
그래디언트를 합산하면 됩니다.

605
00:26:18,000 --> 00:26:20,000
many to one

606
00:26:20,000 --> 00:26:24,080
시나리오에서는 여기서 단일 손실만 계산합니다.

607
00:26:24,080 --> 00:26:28,107
문제 설정에 따라 최종 은닉 상태만 사용해

608
00:26:28,107 --> 00:26:30,440
값을 계산할 수도 있습니다.

609
00:26:30,440 --> 00:26:32,420
예를 들어 비디오 분류를 한다면,

610
00:26:32,420 --> 00:26:35,420
분류 과정 전체에 걸쳐 비디오에 대한 정보가

611
00:26:35,420 --> 00:26:37,503
있을 수 있으니 모든 단계의 은닉

612
00:26:37,503 --> 00:26:40,170
상태를 사용하는 것이 합리적일 수 있습니다.

613
00:26:40,170 --> 00:26:42,720
평균 풀링이나 맥스 풀링

614
00:26:42,720 --> 00:26:46,520
같은 풀링을 해서 y 값을 계산하게 됩니다.

615
00:26:46,520 --> 00:26:51,800
그리고 image classification이 아니라 image

616
00:26:51,800 --> 00:26:56,073
captioning 같은 one to many 매핑에서는 이전

617
00:26:56,073 --> 00:26:58,240
y들을 어떻게 포함할지에 대한

618
00:26:58,240 --> 00:26:59,620
질문이 있었습니다.

619
00:26:59,620 --> 00:27:04,640
그래서 f,w에 입력이 필요합니다. 입력 벡터 x를

620
00:27:04,640 --> 00:27:07,460
기대하는 가중치 행렬과 이전

621
00:27:07,460 --> 00:27:09,840
시간 단계의 은닉 상태를

622
00:27:09,840 --> 00:27:14,100
기대하는 가중치 행렬 두 개가 있기 때문입니다.

623
00:27:14,100 --> 00:27:14,600
기대하는 가중치 행렬 두 개가 있기 때문입니다.

624
00:27:14,600 --> 00:27:19,080
여기에 여러 값을 넣을 수 있다고 상상할 수 있습니다.

625
00:27:19,080 --> 00:27:24,940
0을 넣거나 이전 출력을 넣을 수도

626
00:27:24,940 --> 00:27:26,820
있습니다.

627
00:27:26,820 --> 00:27:30,480
역전파를 어떻게 하는지 대략적으로 설명했습니다.

628
00:27:30,480 --> 00:27:32,500
하지만 이 개념적 틀을

629
00:27:32,500 --> 00:27:35,283
살펴볼 때 실제로 GPU 메모리

630
00:27:35,283 --> 00:27:37,700
부족 같은 구체적인

631
00:27:37,700 --> 00:27:40,842
문제들이 발생하는데, 이는 신경망 훈련

632
00:27:40,842 --> 00:27:43,300
시 거의 모든 문제의

633
00:27:43,300 --> 00:27:46,360
원인이며, NaN 손실이 발생하는

634
00:27:46,360 --> 00:27:47,880
경우도 포함됩니다.

635
00:27:47,880 --> 00:27:53,180
각 시간 단계에서 손실을 계산하고 입력 시퀀스가

636
00:27:53,180 --> 00:27:56,668
매우 길 때, 이해하기 매우

637
00:27:56,668 --> 00:27:57,960
쉽습니다.

638
00:27:57,960 --> 00:28:00,580
각 시간 단계에서 활성화와 그래디언트를

639
00:28:00,580 --> 00:28:03,880
메모리에 저장하고 모두 합산해야 합니다.

640
00:28:03,880 --> 00:28:07,180
입력 시퀀스가 길어질수록 이 크기는 매우

641
00:28:07,180 --> 00:28:07,920
커집니다.

642
00:28:07,920 --> 00:28:11,840
그럼 이 문제를 실제로 어떻게 해결할 수 있을까요?

643
00:28:11,840 --> 00:28:13,740
이것을 backpropagation

644
00:28:13,740 --> 00:28:16,080
through time이라고

645
00:28:16,080 --> 00:28:18,620
합니다. 동일한 가중치 행렬이 여러

646
00:28:18,620 --> 00:28:21,040
시간 단계에 적용되고 각 단계의 그래디언트를

647
00:28:21,040 --> 00:28:22,520
합산하는 경우죠.

648
00:28:22,520 --> 00:28:24,593
그래서 할 수 있는 방법이 truncated back

649
00:28:24,593 --> 00:28:26,260
propagation through time입니다.

650
00:28:26,260 --> 00:28:28,640
기본적으로 고정된 시간 창을

651
00:28:28,640 --> 00:28:31,680
설정하고, 지금까지 모델이 이 창

652
00:28:31,680 --> 00:28:35,400
안에서만 학습된 것처럼 간주하는 겁니다.

653
00:28:35,400 --> 00:28:37,480
우리는 h,0에서 시작합니다.

654
00:28:37,480 --> 00:28:41,200
시간 단계 1의 입력과 이전 h 값을

655
00:28:41,200 --> 00:28:43,300
기반으로 계산합니다.

656
00:28:43,300 --> 00:28:47,258
현재 숨겨진 상태 h,1을 계산하고, 이를 사용해

657
00:28:47,258 --> 00:28:49,300
출력을 계산할 수 있습니다.

658
00:28:49,300 --> 00:28:50,640
손실(loss)을 구합니다.

659
00:28:50,640 --> 00:28:52,580
각 예제에 대해 이 과정을 반복할 수 있습니다.

660
00:28:52,580 --> 00:28:54,247
이 설정에서는 시작

661
00:28:54,247 --> 00:28:57,560
시퀀스를 훈련 중에 보는 전부인 것처럼 다루는

662
00:28:57,560 --> 00:29:00,800
것이 상대적으로 쉽다는 것을 알 수 있습니다.

663
00:29:00,800 --> 00:29:03,620
다음 블록으로 넘어가면,

664
00:29:03,620 --> 00:29:06,460
이제 h,0을 이전 단계의

665
00:29:06,460 --> 00:29:08,380
출력으로 초기화합니다.

666
00:29:08,380 --> 00:29:11,153
숨겨진 상태를 이전 단계의 출력으로

667
00:29:11,153 --> 00:29:12,820
초기화하지만, 그래디언트는 더

668
00:29:12,820 --> 00:29:14,400
이상 전달되지 않습니다.

669
00:29:14,400 --> 00:29:16,900
즉, 계산 그래프를 배치 처리하여

670
00:29:16,900 --> 00:29:19,780
한 번에 이 시간 단계 근처의

671
00:29:19,780 --> 00:29:21,600
손실만 살펴보는 겁니다.

672
00:29:21,600 --> 00:29:23,760
이것은 사용자가 설정하는 고정된 창 크기입니다.

673
00:29:23,760 --> 00:29:29,820
이렇게 하면 특히 입력 시퀀스가 매우 길 때 흔히

674
00:29:29,820 --> 00:29:32,460
발생하는 문제를 해결할

675
00:29:32,460 --> 00:29:34,260
수 있습니다.

676
00:29:34,260 --> 00:29:37,500
즉, 배치 처리하면서 전체

677
00:29:37,500 --> 00:29:42,380
입력 시퀀스에 대해 계속 반복할 수 있습니다.

678
00:29:42,380 --> 00:29:46,375
또 한 가지 질문은, 만약 출력이 마지막에

679
00:29:46,375 --> 00:29:48,500
한 번만 있다면 어떻게

680
00:29:48,500 --> 00:29:50,500
작동하느냐는 것입니다.

681
00:29:50,500 --> 00:29:54,320
각 시간 단계에서 그래디언트를 계산할

682
00:29:54,320 --> 00:29:59,540
수 있지만, 시간 단계 자체의 출력에

683
00:29:59,540 --> 00:30:02,420
의존하는 손실은 없고, 대신

684
00:30:02,420 --> 00:30:04,040
상류 그래디언트에

685
00:30:04,040 --> 00:30:06,333
의존하게 됩니다.

686
00:30:06,333 --> 00:30:09,000
도표의 가장 오른쪽을 보면,

687
00:30:09,000 --> 00:30:11,120
마지막 시간 단계 출력에

688
00:30:11,120 --> 00:30:13,800
기반한 손실을 계산합니다.

689
00:30:13,800 --> 00:30:16,640
마지막 숨겨진 상태에 대한

690
00:30:16,640 --> 00:30:19,680
그래디언트를 계산할 수 있습니다.

691
00:30:19,680 --> 00:30:23,800
그리고 Whh 행렬을 사용해 이전 숨겨진

692
00:30:23,800 --> 00:30:27,480
상태가 최종 숨겨진 상태에

693
00:30:27,480 --> 00:30:30,420
어떻게 기여했는지 이해합니다.

694
00:30:30,420 --> 00:30:34,240
이를 통해 이전 숨겨진 상태와 가중치 행렬을

695
00:30:34,240 --> 00:30:36,680
기반으로 그래디언트를 계산할

696
00:30:36,680 --> 00:30:37,700
수 있습니다.

697
00:30:37,700 --> 00:30:42,000
Whh 변환 행렬을 어떻게 바꾸면 손실이

698
00:30:42,000 --> 00:30:44,880
변하는지 알 수 있죠.

699
00:30:44,880 --> 00:30:49,778
그리고 이 과정을 Whh에 대해 반복적으로

700
00:30:49,778 --> 00:30:50,820
적용합니다.

701
00:30:50,820 --> 00:30:53,112
숨겨진 상태가 다음 숨겨진 상태를 어떻게

702
00:30:53,112 --> 00:30:56,720
바꾸고, 그것이 손실에 어떻게 기여하는지만 살펴봅니다.

703
00:30:56,720 --> 00:30:59,100
마지막 예제를 보면,

704
00:30:59,100 --> 00:31:01,080
숨겨진 상태 변경이 손실에

705
00:31:01,080 --> 00:31:04,040
어떻게 의존하는지, 그리고 이전 숨겨진

706
00:31:04,040 --> 00:31:06,020
상태가 현재 숨겨진 상태에

707
00:31:06,020 --> 00:31:10,220
어떻게 영향을 미치는지 Whh 행렬로 알 수 있습니다.

708
00:31:10,220 --> 00:31:14,100
각 시간 단계마다 다른 W를 사용하면, 더

709
00:31:14,100 --> 00:31:17,340
이상 순환 관계로 모델링하지 않는다는

710
00:31:17,340 --> 00:31:18,520
의미입니다.

711
00:31:18,520 --> 00:31:22,300
즉, 각 시간 단계마다 별도의 레이어가 있는

712
00:31:22,300 --> 00:31:24,900
것으로 생각할 수 있습니다.

713
00:31:24,900 --> 00:31:28,580
이렇게 하면 순차적으로 재귀적으로

714
00:31:28,580 --> 00:31:33,860
모델링하지 않기 때문에 성능이 떨어질 가능성이 큽니다. 입력

715
00:31:33,860 --> 00:31:35,923
시퀀스마다 별도의

716
00:31:35,923 --> 00:31:37,340
가중치를 사용하는

717
00:31:37,340 --> 00:31:38,640
신경망을

718
00:31:38,640 --> 00:31:40,720
훈련하는 것과 같기

719
00:31:40,720 --> 00:31:41,420
때문입니다.

720
00:31:41,420 --> 00:31:42,360
네, 독립적으로 합니다.

721
00:31:42,360 --> 00:31:43,818
이것은 시퀀스 모델링

722
00:31:43,818 --> 00:31:45,960
문제가 아닌, 단순히 분류하려는

723
00:31:45,960 --> 00:31:49,740
항목들의 집합이 있는 문제에 적합할 것입니다.

724
00:31:49,740 --> 00:31:52,920
시퀀스의 길이를 미리 알고 있어야 합니다.

725
00:31:52,920 --> 00:31:54,920
그래서 시퀀스가

726
00:31:54,920 --> 00:31:59,300
아니라면 작동할 수 있지만, 길이가 가변적인

727
00:31:59,300 --> 00:32:03,440
시퀀스에는 잘 작동하지 않을 것 같습니다.

728
00:32:03,440 --> 00:32:06,663
왜냐하면 각 타임스텝마다 하나의

729
00:32:06,663 --> 00:32:08,080
신경망을 훈련시키는

730
00:32:08,080 --> 00:32:10,660
것과 같아서, 그렇게

731
00:32:10,660 --> 00:32:13,800
공식화하는 방법이 아니기 때문입니다.

732
00:32:13,800 --> 00:32:15,300
그럼 chunking과는 어떻게 작동하나요?

733
00:32:15,300 --> 00:32:18,960
여기까지 이해하셨나요? 빨간 점이

734
00:32:18,960 --> 00:32:21,560
있는 바로 이 지점에서,

735
00:32:21,560 --> 00:32:24,800
최종 은닉 상태에 대한

736
00:32:24,800 --> 00:32:27,160
손실의 기울기를 계산할

737
00:32:27,160 --> 00:32:29,000
수 있습니다.

738
00:32:29,000 --> 00:32:32,240
그렇다면, 최종 바로 전 은닉 상태에

739
00:32:32,240 --> 00:32:35,460
대한 손실의 기울기도 계산할 수 있습니다.

740
00:32:35,460 --> 00:32:37,280
왜냐하면 최종 은닉 상태는

741
00:32:37,280 --> 00:32:40,160
이전 은닉 상태와 이 가중치 행렬

742
00:32:40,160 --> 00:32:42,300
W의 곱에 의존하기 때문입니다.

743
00:32:42,300 --> 00:32:45,200
그래서 이렇게 할 수 있고, 여기까지 왔다 갔다 할 수 있습니다.

744
00:32:45,200 --> 00:32:50,660
이 시점에서 우리가 저장해야 할 것은 바로 이 마지막 단계뿐입니다.

745
00:32:50,660 --> 00:32:56,040
그럼 이 아주 마지막 혹은 마지막들—아마 '마지막'이라는 단어를 너무

746
00:32:56,040 --> 00:32:57,580
많이 쓰는 것 같네요.

747
00:32:57,580 --> 00:33:01,980
하지만 이 잘린 배치 내 초기 은닉 상태에

748
00:33:01,980 --> 00:33:05,320
대한 손실의 기울기는 무엇일까요?

749
00:33:05,320 --> 00:33:07,280
그리고 역전파를 계산할 때,

750
00:33:07,280 --> 00:33:10,420
그 값을 사용해서 이전 모든 타임스텝을

751
00:33:10,420 --> 00:33:11,140
계산합니다.

752
00:33:11,140 --> 00:33:13,200
이것이 전체 과정입니다.

753
00:33:13,200 --> 00:33:17,220
은닉 상태가 어떻게 변해서 새로운 은닉 상태가

754
00:33:17,220 --> 00:33:19,820
되는지만 보고, 이 값만

755
00:33:19,820 --> 00:33:21,200
업데이트됩니다.

756
00:33:23,940 --> 00:33:28,000
네, 그리고 입력이 은닉 상태를 어떻게 변화시키는지도 봅니다.

757
00:33:28,000 --> 00:33:30,860
그래서 두 가지 값을 보는 거죠, 입력이 은닉

758
00:33:30,860 --> 00:33:33,380
상태에 미치는 영향과 입력이 다음 은닉

759
00:33:33,380 --> 00:33:36,240
상태와 이전 은닉 상태에 미치는 영향입니다.

760
00:33:36,240 --> 00:33:38,580
죄송합니다, 두 가지 값이 있습니다, 네.

761
00:33:38,580 --> 00:33:41,500
그래서 학습은 모든 배치에 대해 계속 일어납니다.

762
00:33:41,500 --> 00:33:44,300
여기 W의 각 파라미터에

763
00:33:44,300 --> 00:33:49,080
대한 손실 기울기가 있습니다.

764
00:33:49,080 --> 00:33:53,660
그리고 이전 타임스텝을 계산할

765
00:33:53,660 --> 00:33:57,440
때는 이 한 값을 유지합니다.

766
00:33:57,440 --> 00:33:59,580
여기 초기 은닉 상태를 바꾸면 손실이

767
00:33:59,580 --> 00:34:00,460
어떻게 변할까요?

768
00:34:00,460 --> 00:34:01,960
그걸 계산할 수 있고,

769
00:34:01,960 --> 00:34:04,722
이 원래 은닉 상태와 현재 타임스텝

770
00:34:04,722 --> 00:34:06,680
같은 모든 변수들이 어떻게

771
00:34:06,680 --> 00:34:08,900
영향을 미치는지 알 수 있습니다.

772
00:34:08,900 --> 00:34:11,580
하지만 실제로 다음 청크로 넘어갈 때는,

773
00:34:11,580 --> 00:34:14,679
이 은닉 상태가 다음 청크의 은닉 상태에

774
00:34:14,679 --> 00:34:17,620
어떻게 영향을 미치는지만 보면 됩니다.

775
00:34:17,620 --> 00:34:19,659
그래서 이 분할 경계를 보는 거죠.

776
00:34:19,659 --> 00:34:21,320
추적해야 할

777
00:34:21,320 --> 00:34:24,960
변수는 청크 이후에 발생하는 은닉

778
00:34:24,960 --> 00:34:27,560
상태의 기울기입니다.

779
00:34:27,560 --> 00:34:29,199
그걸 사용해서 현재 은닉 상태의

780
00:34:29,199 --> 00:34:31,157
기울기를 계산할 수 있는데,

781
00:34:31,157 --> 00:34:33,741
이 상태는 입력 x와 이전 상태에 의존합니다.

782
00:34:33,741 --> 00:34:35,699
공식화하는 방법은 여러 가지가

783
00:34:35,699 --> 00:34:38,120
있지만, 여기 모든 가중치에 업데이트를

784
00:34:38,120 --> 00:34:41,860
적용하고 메모리는 초기화한다고 상상할 수 있습니다.

785
00:34:41,860 --> 00:34:46,620
우리가 추적하는 유일한 것은 바로 이 기울기입니다.

786
00:34:46,620 --> 00:34:49,440
그래서 기울기 적용 단계를 수행해서

787
00:34:49,440 --> 00:34:51,550
학습률과 옵티마이저 등에 따라

788
00:34:51,550 --> 00:34:53,800
모든 기울기를 가중치에 적용할 수

789
00:34:53,800 --> 00:34:54,500
있습니다.

790
00:34:54,500 --> 00:34:56,739
그리고 다음 배치 계산으로 넘어갑니다.

791
00:34:56,739 --> 00:34:59,020
이것이 완벽한 계산이 아닌 이유는,

792
00:34:59,020 --> 00:35:03,380
모든 것을 한꺼번에 계산하는 것이 아니라 독립적으로 계산하기

793
00:35:03,380 --> 00:35:04,340
때문입니다.

794
00:35:04,340 --> 00:35:07,380
그래서 한 번에 한 번의 업데이트가

795
00:35:07,380 --> 00:35:10,620
아니라 세 가지 다른 업데이트가 있지만,

796
00:35:10,620 --> 00:35:14,620
여기서도 각 단계마다 그래디언트를 계산하는 겁니다.

797
00:35:14,620 --> 00:35:16,340
메모리에 한 가지를 유지하는데,

798
00:35:16,340 --> 00:35:19,670
그게 바로 배치에서 첫 번째 히든 상태가 어떻게 되는지입니다.

799
00:35:22,220 --> 00:35:24,580
여기서 히든 상태를 어떻게 업데이트해서

800
00:35:24,580 --> 00:35:26,920
손실을 결정할 수 있을까요?

801
00:35:26,920 --> 00:35:29,460
그리고 나머지는 모두 버립니다.

802
00:35:29,460 --> 00:35:31,500
그래서 가중치가 메모리에 있습니다.

803
00:35:31,500 --> 00:35:34,580
그래디언트를 적용할 수 있고, 학습률을

804
00:35:34,580 --> 00:35:36,500
곱해서 가중치에 적용합니다.

805
00:35:36,500 --> 00:35:41,080
분산 학습을 할 때도 비슷한 걸 볼 수 있습니다.

806
00:35:41,080 --> 00:35:43,740
각 GPU에서 별도로 계산된 그래디언트를

807
00:35:43,740 --> 00:35:47,140
모두 같은 가중치 집합에 적용하죠, 비록

808
00:35:47,140 --> 00:35:49,320
독립적으로 계산되었더라도요.

809
00:35:49,320 --> 00:35:52,720
분산 학습에 관한 강의가 곧 있을 겁니다.

810
00:35:52,720 --> 00:35:55,320
그래서 이 경우도 모든 걸 같은

811
00:35:55,320 --> 00:35:57,460
메모리에서 동시에 추적하지

812
00:35:57,460 --> 00:36:01,140
않고, 가중치에 하나씩 적용하는 비슷한 방식입니다.

813
00:36:01,140 --> 00:36:03,598
네, 모든 걸 메모리에 다 넣을 수 있다면 더 좋겠죠.

814
00:36:03,598 --> 00:36:06,057
네, 모든 걸 메모리에 다 넣을 수 있다면 더 좋겠죠.

815
00:36:06,057 --> 00:36:07,160
이 경우는

816
00:36:07,160 --> 00:36:09,340
본질적으로 거의 같습니다.

817
00:36:09,340 --> 00:36:11,400
하지만 이 설정에서는 정보를

818
00:36:11,400 --> 00:36:14,080
명시적으로 잃는 과정이 더 명확할 수 있습니다.

819
00:36:14,080 --> 00:36:19,600
여기서는 한 번에 일부 출력만 보고 있습니다.

820
00:36:19,600 --> 00:36:22,760
그래서 각 시간 단계마다 손실이

821
00:36:22,760 --> 00:36:26,640
있기 때문에 전체 손실 집합을 계산할 때 모두

822
00:36:26,640 --> 00:36:28,520
보지 않는다는 게

823
00:36:28,520 --> 00:36:30,020
아주 명확합니다.

824
00:36:30,020 --> 00:36:32,620
그래서 여기서는 정보가 손실되지만, 이

825
00:36:32,620 --> 00:36:34,880
경우에는 정보가 손실되지 않습니다.

826
00:36:34,880 --> 00:36:37,240
슬라이드에 전체 RNN을 다

827
00:36:37,240 --> 00:36:39,560
넣을 수 없는 또 다른 실용적인

828
00:36:39,560 --> 00:36:42,040
예는 문자 단위 언어 모델입니다.

829
00:36:42,040 --> 00:36:43,960
이게 정말 재미있는 점은

830
00:36:43,960 --> 00:36:48,480
10년 전에 꽤 효과적이라는 게 입증되었다는 겁니다.

831
00:36:48,480 --> 00:36:50,160
그리고 현재의 언어

832
00:36:50,160 --> 00:36:53,150
모델들이 단순히 RNN으로 문자를

833
00:36:53,150 --> 00:36:56,230
예측하는 이 간단한 접근법의 발전이라는 걸

834
00:36:56,230 --> 00:36:58,830
볼 수 있어서 정말 재미있습니다.

835
00:36:58,830 --> 00:37:01,330
보통 이런 모델을 할

836
00:37:01,330 --> 00:37:05,070
때는 문자를 입력으로 넣는데, 이를

837
00:37:05,070 --> 00:37:10,750
원-핫 인코딩이라고 부릅니다. 벡터에서 하나의 위치만

838
00:37:10,750 --> 00:37:12,750
1이고 나머지는 모두

839
00:37:12,750 --> 00:37:14,370
0인 방식이죠.

840
00:37:14,370 --> 00:37:16,790
여기 인덱스가 됩니다.

841
00:37:16,790 --> 00:37:18,910
이걸 인덱스로 인코딩할 수

842
00:37:18,910 --> 00:37:22,010
있고, 이걸 입력으로 사용할 수 있습니다.

843
00:37:22,010 --> 00:37:23,630
이전 히든 레이어와

844
00:37:23,630 --> 00:37:25,510
현재 입력을

845
00:37:25,510 --> 00:37:28,550
기반으로 히든 레이어를 계산합니다.

846
00:37:28,550 --> 00:37:30,510
그리고 출력 레이어도

847
00:37:30,510 --> 00:37:33,630
마찬가지로, 여기서 다음

848
00:37:33,630 --> 00:37:37,390
시간 단계로 취해지는 올바른 값에 해당하는

849
00:37:37,390 --> 00:37:39,290
출력을 봅니다.

850
00:37:39,290 --> 00:37:42,590
예를 들어 출력이 e가 되길 원한다면, 여기

851
00:37:42,590 --> 00:37:45,012
매핑해서 볼 수 있습니다. 이건

852
00:37:45,012 --> 00:37:46,970
소프트맥스 같은 거라고 생각할

853
00:37:46,970 --> 00:37:50,050
수 있고, 로짓, 즉 점수들이 있습니다.

854
00:37:50,050 --> 00:37:52,670
2.2인데 4.1보다 낮죠.

855
00:37:52,670 --> 00:37:57,290
그래서 이 시간 단계에서 출력이 그렇게 좋지 않을

856
00:37:57,290 --> 00:37:59,290
수도 있습니다, 이런 식으로

857
00:37:59,290 --> 00:38:00,550
계속됩니다.

858
00:38:00,550 --> 00:38:03,650
그래서 이걸 시간 단계별 분류 문제로 볼 수

859
00:38:03,650 --> 00:38:04,310
있습니다.

860
00:38:04,310 --> 00:38:09,210
일반적으로 이런 언어 모델들이 하는 게 바로

861
00:38:09,210 --> 00:38:14,490
소프트맥스를 기반으로 한 시간 단계별 분류입니다.

862
00:38:14,490 --> 00:38:17,410
테스트 시에는 기본적으로 문자를 한

863
00:38:17,410 --> 00:38:19,668
번에 하나씩 샘플링해서

864
00:38:19,668 --> 00:38:21,210
모델에 다시 넣어야

865
00:38:21,210 --> 00:38:25,170
합니다. 이전 시간 단계에서 생성한 걸 모델이

866
00:38:25,170 --> 00:38:29,590
보게 하면서, 단어가 생성될 때까지 반복하는 거죠.

867
00:38:29,590 --> 00:38:36,930
그래서 문자 단위로 작동하는 RNN을 만들어 이런 기본 언어 모델링 작업을

868
00:38:36,930 --> 00:38:39,110
할 수 있고, 꽤

869
00:38:39,110 --> 00:38:40,570
잘 작동합니다.

870
00:38:40,570 --> 00:38:44,570
한 가지 주목할 점은, 이 입력 층에 관해서

871
00:38:44,570 --> 00:38:47,450
보통 모델에 원-핫 임베딩을 직접

872
00:38:47,450 --> 00:38:49,110
입력하지 않고,

873
00:38:49,110 --> 00:38:52,310
대신 임베딩 층이라는 것을 사용하는데,

874
00:38:52,310 --> 00:38:57,310
이 임베딩 층은 본질적으로 거대한 행렬이라는 겁니다.

875
00:38:57,310 --> 00:39:00,290
이 행렬은 D 곱하기 D 크기인데, 여기서 D는 모델에 들어가는

876
00:39:00,290 --> 00:39:01,710
서로 다른 입력의 개수입니다.

877
00:39:01,710 --> 00:39:04,390
그리고 이걸 행렬 곱셈으로

878
00:39:04,390 --> 00:39:08,165
생각할 수 있는데, 입력 샘플에 따라

879
00:39:08,165 --> 00:39:09,790
임베딩 행렬의

880
00:39:09,790 --> 00:39:14,410
첫 번째 행이나 두 번째 행을 가져오는 거죠.

881
00:39:14,410 --> 00:39:17,710
그리고 이걸 행렬 곱셈으로 사용합니다.

882
00:39:17,710 --> 00:39:19,972
사실 이건 틀렸네요, 이 부분이 더

883
00:39:19,972 --> 00:39:21,430
높은 확률이어야 합니다.

884
00:39:23,737 --> 00:39:26,070
재밌는 건, 이 슬라이드를 몇

885
00:39:26,070 --> 00:39:29,350
년째 써왔는데 아무도 눈치채지 못했다는 겁니다.

886
00:39:29,350 --> 00:39:34,350
어쨌든 여기서 E가 목표 문자입니다.

887
00:39:34,350 --> 00:39:37,168
그래서 이 경우 모델이 실제로 틀렸기 때문에

888
00:39:37,168 --> 00:39:38,710
이 타임스텝에 대해

889
00:39:38,710 --> 00:39:40,585
크게 벌점을 주고 싶습니다.

890
00:39:40,585 --> 00:39:43,150
좋은 질문입니다.

891
00:39:43,150 --> 00:39:45,990
이 구현의 좋은 점 중 하나는 정말

892
00:39:45,990 --> 00:39:47,230
간단하다는 겁니다.

893
00:39:47,230 --> 00:39:49,710
파이썬 코드 112줄로 되어 있고, 다양한

894
00:39:49,710 --> 00:39:52,870
작업에 대해 이 모델들을 훈련할 수 있습니다.

895
00:39:52,870 --> 00:39:56,150
이건 LLM 이전 시대에 할 수 있었던 것들이죠.

896
00:39:56,150 --> 00:39:58,590
윌리엄 셰익스피어의 소네트로도 훈련할 수 있습니다.

897
00:39:58,590 --> 00:40:00,410
그리고 제가 언급했듯이, 이

898
00:40:00,410 --> 00:40:03,330
강좌의 전 강사인 Andrej Karpathy가

899
00:40:03,330 --> 00:40:07,850
2015년에 쓴 블로그 글이 있는데, 그 글에서 RNN이 텍스트 생성에서
얼마나

900
00:40:07,850 --> 00:40:09,930
비상식적으로 효과적인지 이야기했습니다.

901
00:40:09,930 --> 00:40:10,430
네?

902
00:40:10,430 --> 00:40:14,570
임베딩 층을 왜 사용하는지 다시 설명해 주시겠어요?

903
00:40:14,570 --> 00:40:16,610
네, 임베딩 레이어의 기본 아이디어는

904
00:40:16,610 --> 00:40:19,290
일반적으로 모델에 벡터를 입력하는

905
00:40:19,290 --> 00:40:20,620
것이 더 낫다는 겁니다.

906
00:40:20,620 --> 00:40:22,870
그리고 이 임베딩 레이어들도 학습할 수 있습니다.

907
00:40:22,870 --> 00:40:26,810
그래서 보통 임베딩을 학습할 때는 가중치가 넓게

908
00:40:26,810 --> 00:40:28,540
퍼져 있는 걸 선호합니다.

909
00:40:28,540 --> 00:40:30,290
임베딩 레이어를 Kaiming

910
00:40:30,290 --> 00:40:34,217
초기화 같은 방법으로 아주 작은 0에 가까운 값으로

911
00:40:34,217 --> 00:40:35,550
초기화할 수 있습니다.

912
00:40:35,550 --> 00:40:37,050
그리고 입력으로 숫자

913
00:40:37,050 --> 00:40:38,730
하나가 아니라 한 번에

914
00:40:38,730 --> 00:40:41,890
한 행씩 벡터를 보는 거죠, 숫자를 입력으로

915
00:40:41,890 --> 00:40:43,930
표현하려면 0이 많은

916
00:40:43,930 --> 00:40:47,830
1로 표현해야 하는데, 임베딩을 최적화하는 게 더 잘

917
00:40:47,830 --> 00:40:48,610
작동합니다.

918
00:40:53,415 --> 00:40:55,790
네, 파이썬 코드 112줄로 할 수

919
00:40:55,790 --> 00:40:56,950
있는데 꽤 멋집니다.

920
00:40:56,950 --> 00:40:59,070
William Shakespeare의 Sonnets로

921
00:40:59,070 --> 00:41:00,998
학습시키면 실제로 그럴듯한 텍스트를 출력할 수 있습니다.

922
00:41:00,998 --> 00:41:02,290
몇 가지 예제를 살펴보겠습니다.

923
00:41:02,290 --> 00:41:03,990
멋진 점 중 하나는 모델을 더

924
00:41:03,990 --> 00:41:05,448
많이 훈련할수록 점점 더

925
00:41:05,448 --> 00:41:06,910
일관성 있게 된다는 겁니다.

926
00:41:06,910 --> 00:41:09,190
처음에는 W에 대한 적절한 값을 배우지

927
00:41:09,190 --> 00:41:11,750
못해서 거의 의미 없는 글자들의 나열입니다.

928
00:41:11,750 --> 00:41:15,230
그리고 계속 훈련하면 3단계처럼 영어처럼 보이기

929
00:41:15,230 --> 00:41:17,790
시작하고, 적어도 몇몇 단어는

930
00:41:17,790 --> 00:41:19,085
제대로 나옵니다.

931
00:41:19,085 --> 00:41:20,710
더 훈련할수록

932
00:41:20,710 --> 00:41:22,750
실제로 잘 작동하기

933
00:41:22,750 --> 00:41:25,230
시작하는데, 이게 AI

934
00:41:25,230 --> 00:41:30,310
시대에 올 것들을 조금 예고한 셈이죠, 꽤 멋집니다.

935
00:41:30,310 --> 00:41:32,550
완전히 스타일에 대해

936
00:41:32,550 --> 00:41:36,510
배우고, 이름을 어떻게 써야 하는지, 꽤

937
00:41:36,510 --> 00:41:40,435
그럴듯해 보이는 문장을 생성하다가도 점점 덜

938
00:41:40,435 --> 00:41:41,810
말이 되게 만드는

939
00:41:41,810 --> 00:41:43,352
걸 볼 수 있는데,

940
00:41:43,352 --> 00:41:45,930
그래도 꽤 흥미롭습니다.

941
00:41:45,930 --> 00:41:50,110
코드로도 훈련할 수 있는데, 이 예에서는 Linux 소스코드로

942
00:41:50,110 --> 00:41:51,480
훈련했다고 생각합니다.

943
00:41:51,480 --> 00:41:53,230
Linux의 소스코드가 있는 거죠.

944
00:41:53,230 --> 00:41:55,188
이런 문자 단위 RNN 중 하나를

945
00:41:55,188 --> 00:41:59,332
훈련시켜서 꽤 괜찮아 보이는 C 코드를 생성하는 걸 볼 수 있습니다.

946
00:41:59,332 --> 00:42:00,790
이게 컴파일될지는 모르겠지만,

947
00:42:00,790 --> 00:42:03,410
그냥 보기에는 합리적으로 보입니다.

948
00:42:03,410 --> 00:42:07,950
이 아이디어는 지난 몇 년 동안 정말로 크게 확산되었습니다.

949
00:42:07,950 --> 00:42:12,570
즉, 여러분 대부분이 컴퓨터 과학이나 코딩 분야에서 일하거나

950
00:42:12,570 --> 00:42:14,850
이 분야 학생들이기 때문에 잘

951
00:42:14,850 --> 00:42:18,370
아시겠지만, 지금은 이런 언어 모델을 위한 다양한

952
00:42:18,370 --> 00:42:20,410
프로그래밍 도구들이

953
00:42:20,410 --> 00:42:22,770
있습니다. 이 모델들은 본질적으로

954
00:42:22,770 --> 00:42:26,850
비슷한 작업에서 훈련되었는데, 기존 코드라는 방대한 훈련

955
00:42:26,850 --> 00:42:29,513
데이터를 소비했고, 다음 문자를

956
00:42:29,513 --> 00:42:30,930
예측하는 대신 다음

957
00:42:30,930 --> 00:42:33,950
토큰, 즉 문자 그룹을 예측하려고 합니다.

958
00:42:33,950 --> 00:42:35,950
토큰을 어떻게 정의하느냐는 모델에 따라 다르며,

959
00:42:35,950 --> 00:42:37,670
이 부분에 대해 자세히 들어가면 많지만,

960
00:42:37,670 --> 00:42:39,430
큰 틀에서 보면, 이건 정말 비슷한

961
00:42:39,430 --> 00:42:41,263
개념으로, 문자 그룹을 하나씩 순차적으로

962
00:42:41,263 --> 00:42:43,113
자기회귀 방식으로 예측하는 겁니다.

963
00:42:43,113 --> 00:42:45,030
그리고 최근 몇 년간 이런 도구들이 많이

964
00:42:45,030 --> 00:42:46,850
나오면서 정말 폭발적인 성장을 보였습니다.

965
00:42:46,850 --> 00:42:47,350
네?

966
00:42:51,030 --> 00:42:52,650
모델에 입력되는 것은 무엇인가요?

967
00:42:52,650 --> 00:42:54,030
트리거 같은 건가요?

968
00:42:54,030 --> 00:42:55,530
아, 이거에 대해서요?

969
00:42:55,530 --> 00:42:56,030
네.

970
00:42:56,030 --> 00:42:58,990
입력은 그냥 무작위 문자로

971
00:42:58,990 --> 00:43:01,450
시작할 수도 있습니다.

972
00:43:01,450 --> 00:43:03,150
그렇게 할 수도 있지만,

973
00:43:03,150 --> 00:43:05,230
초기 입력이 필요합니다.

974
00:43:05,230 --> 00:43:08,550
보통 언어 모델에는 시작 토큰이

975
00:43:08,550 --> 00:43:11,353
미리 정해져 있습니다.

976
00:43:11,353 --> 00:43:13,770
이것이 시퀀스 시작 부분에 항상 보이는 것입니다.

977
00:43:13,770 --> 00:43:15,490
RNN에서도 비슷한 방식을 사용할 수 있습니다.

978
00:43:15,490 --> 00:43:17,573
이 정확한 상황에서 그들이 무엇을 했는지는 모르겠습니다.

979
00:43:17,573 --> 00:43:20,288
아마도 그냥 한 글자씩 했을 수도 있지만, 확실히 알기 어렵습니다.

980
00:43:20,288 --> 00:43:21,830
그래서 질문은, 언어 모델에서 라벨링은

981
00:43:21,830 --> 00:43:22,930
어떻게 작동하느냐는 겁니다.

982
00:43:22,930 --> 00:43:25,750
이 순수한 언어 모델들의 멋진 점은, 그저

983
00:43:25,750 --> 00:43:28,250
다음 토큰을 예측하는 것뿐이라는 겁니다.

984
00:43:28,250 --> 00:43:29,470
라벨링할 필요 없이, 그냥

985
00:43:29,470 --> 00:43:30,970
많은 텍스트를 주기만 하면 됩니다.

986
00:43:30,970 --> 00:43:32,490
그래서 이 모델들이 매우

987
00:43:32,490 --> 00:43:35,950
뛰어난 이유는, 인터넷에서 사실상 모든 가능한 텍스트를

988
00:43:35,950 --> 00:43:38,610
긁어모아 그걸로 모델을 학습시키기 때문입니다.

989
00:43:38,610 --> 00:43:40,848
그래서 이 모델들이 그렇게 좋은 거죠.

990
00:43:40,848 --> 00:43:42,890
그 이유는 단지 다음 토큰을 생성하는 것이고,

991
00:43:42,890 --> 00:43:44,182
라벨링이 필요 없기 때문입니다.

992
00:43:44,182 --> 00:43:46,610
그래서 언어 모델들이 매우 좋은 겁니다.

993
00:43:46,610 --> 00:43:49,170
그래서 질문은, 매 시간 단계마다

994
00:43:49,170 --> 00:43:53,130
최대 확률 출력을 항상 선택하면, 계속 똑같은

995
00:43:53,130 --> 00:43:56,577
것을 반복해서 생성하지 않느냐는 겁니다.

996
00:43:56,577 --> 00:43:57,910
그리고 답은 사실 그렇습니다.

997
00:43:57,910 --> 00:44:01,490
만약 최대 확률만 선택한다면, 이 예시는 그렇게

998
00:44:01,490 --> 00:44:03,790
좋은 예가 아닐 수도 있지만,

999
00:44:03,790 --> 00:44:05,770
확률이 정확하다고 가정하고 매

1000
00:44:05,770 --> 00:44:08,270
시간 단계마다 최대 확률을 선택한다면,

1001
00:44:08,270 --> 00:44:10,062
같은 입력에 대해 항상 같은 출력을

1002
00:44:10,062 --> 00:44:11,070
얻게 될 겁니다.

1003
00:44:11,070 --> 00:44:12,570
실제로 사람들이 하는 것은, 이것을

1004
00:44:12,570 --> 00:44:14,693
greedy decoding이라고 부르는데,

1005
00:44:14,693 --> 00:44:16,610
항상 최대 확률을 선택하는 것입니다.

1006
00:44:16,610 --> 00:44:18,967
실제로는 softmax가 출력하는

1007
00:44:18,967 --> 00:44:21,050
확률 분포를 기반으로

1008
00:44:21,050 --> 00:44:22,107
샘플링을 합니다.

1009
00:44:22,107 --> 00:44:23,690
그래서 최대 확률을

1010
00:44:23,690 --> 00:44:28,790
선택하는 게 아니라, 예를 들어 이 경우에는 0.84 확률을 선택하거나,
다른

1011
00:44:28,790 --> 00:44:32,710
출력 변수에 대해서는 0.13 확률을 선택하는 식입니다.

1012
00:44:32,710 --> 00:44:34,910
그리고 각 시퀀스마다 그렇게 실행하는 거죠.

1013
00:44:34,910 --> 00:44:37,243
그리고 이걸 할 수 있는 여러 가지 방법이 있습니다.

1014
00:44:37,243 --> 00:44:38,210
앞으로 탐색할 수도 있고요.

1015
00:44:38,210 --> 00:44:39,750
이걸 beam searching이라고

1016
00:44:39,750 --> 00:44:41,333
하는데, 여러 가지를 시도해 보면서

1017
00:44:41,333 --> 00:44:43,930
시퀀스 전체에서 가장 높은 확률을 가진 것을 찾는 방법입니다.

1018
00:44:43,930 --> 00:44:46,810
그래서 이 분야는 활발한 연구 영역입니다, 이 모델들에서 어떻게

1019
00:44:46,810 --> 00:44:48,490
샘플링할 것인지에 대한 연구가 많죠.

1020
00:44:48,490 --> 00:44:50,350
하지만 간단한 답은 항상 가장

1021
00:44:50,350 --> 00:44:52,430
높은 확률을 선택하지는 않는다는 겁니다.

1022
00:44:52,430 --> 00:44:55,390
네, 질문은 다대일 출력의 경우에 매번

1023
00:44:55,390 --> 00:44:57,670
출력을 내는지 아니면 여기서 뭔가를

1024
00:44:57,670 --> 00:44:59,410
보는지에 관한 거죠?

1025
00:44:59,410 --> 00:45:01,310
실제로는 계산량을 줄이기 위해

1026
00:45:01,310 --> 00:45:03,730
사용되지 않는 출력을 내고 싶지는 않을 겁니다.

1027
00:45:03,730 --> 00:45:05,880
하지만 각 타임스텝마다 출력을 내는 것도 가능하긴 합니다.

1028
00:45:05,880 --> 00:45:08,130
그리고 문제에 따라서는 그걸 보는 게

1029
00:45:08,130 --> 00:45:10,872
흥미로울 수 있습니다, 출력이 학습 과정에서 수렴하는지

1030
00:45:10,872 --> 00:45:12,330
아닌지 이해하는 데 말이죠.

1031
00:45:12,330 --> 00:45:13,163
그런 식으로요.

1032
00:45:13,163 --> 00:45:15,250
그래서 보는 게 유용할 수 있지만,

1033
00:45:15,250 --> 00:45:17,930
일반적으로는 계산량을 아끼기 위해 그렇게 하지는 않습니다.

1034
00:45:17,930 --> 00:45:19,630
하지만 실제로는 유용할 수 있습니다, 네.

1035
00:45:19,630 --> 00:45:21,370
모델이 어떻게 작동하는지 이해하는 데 도움이 될 수 있죠.

1036
00:45:21,370 --> 00:45:23,590
특정 트리거나 올바른 답을

1037
00:45:23,590 --> 00:45:27,630
예측하는 데 도움이 되는 요소가 있는지 알 수 있습니다.

1038
00:45:27,630 --> 00:45:30,730
좋은 질문들입니다.

1039
00:45:30,730 --> 00:45:34,110
계속해서 진행하겠습니다.

1040
00:45:34,110 --> 00:45:36,090
우리는 RNN이 얼마나 문자를 생성하는

1041
00:45:36,090 --> 00:45:37,533
데 좋은지 이야기했죠.

1042
00:45:37,533 --> 00:45:39,450
그리고 이걸 현대 코딩 도구들과

1043
00:45:39,450 --> 00:45:41,050
연결지어 봤는데, 정말 멋집니다.

1044
00:45:41,050 --> 00:45:42,970
RNN의 또 다른 멋진 점은

1045
00:45:42,970 --> 00:45:46,905
활성화 값을 볼 수 있다는 겁니다, 이 값들이 모델이

1046
00:45:46,905 --> 00:45:48,530
무엇을 추적하는지

1047
00:45:48,530 --> 00:45:50,697
흥미로운 정보를 줄 때가 있죠.

1048
00:45:50,697 --> 00:45:55,010
우리 작은 예제에서는 출력 활성화 값을 봤는데,

1049
00:45:55,010 --> 00:45:57,810
현재 값과 이전 값을 보여줬습니다.

1050
00:45:57,810 --> 00:45:58,310
현재 값과 이전 값을 보여줬습니다.

1051
00:45:58,310 --> 00:46:02,090
이게 바로 RNN 상태나 셀이 추적하는 내용이었죠.

1052
00:46:02,090 --> 00:46:05,722
또 하나 할 수 있는 건, 기본적으로 시퀀스를 입력으로 주는 겁니다.

1053
00:46:05,722 --> 00:46:07,430
제가 슬라이드에서 보여줄

1054
00:46:07,430 --> 00:46:11,970
모델들은 tanh 활성화를 사용합니다, 그래서 값이 1에서

1055
00:46:11,970 --> 00:46:16,090
-1 사이인데, -1은 여기서 빨간색으로 시각화되고,

1056
00:46:16,090 --> 00:46:18,450
1에 가까울수록 파란색입니다.

1057
00:46:18,450 --> 00:46:20,050
전체 스펙트럼을 볼 수 있죠.

1058
00:46:20,050 --> 00:46:21,730
그리고 각 문자가 들어올

1059
00:46:21,730 --> 00:46:23,250
때 그 타임스텝에서

1060
00:46:23,250 --> 00:46:26,500
셀의 활성화가 어떤지 볼 수 있습니다.

1061
00:46:26,500 --> 00:46:28,750
그래서 이 플롯들이 색으로 코딩된 겁니다.

1062
00:46:28,750 --> 00:46:30,470
이것은 사실 아무것도 보여주지 않는데, 무작위입니다.

1063
00:46:30,470 --> 00:46:32,070
많은 경우 해석이 어렵죠.

1064
00:46:32,070 --> 00:46:34,750
하지만 어떤 경우에는 꽤 멋진 것들을 추적할 수 있습니다.

1065
00:46:34,750 --> 00:46:37,410
예를 들어, 이건 인용부호 감지기로,

1066
00:46:37,410 --> 00:46:40,110
인용부호가 시작되면 켜지고

1067
00:46:40,110 --> 00:46:42,410
인용부호가 끝나면 꺼집니다.

1068
00:46:42,410 --> 00:46:44,610
이것은 기본적으로 RNN 추적에서 언젠가

1069
00:46:44,610 --> 00:46:46,750
끝나는 따옴표를 가져야 한다는 겁니다.

1070
00:46:46,750 --> 00:46:50,550
언제 넣을지는 모델이 알아내려고 하는데,

1071
00:46:50,550 --> 00:46:53,190
추적하고 있는 거죠.

1072
00:46:53,190 --> 00:46:57,650
또 다른 흥미로운 것은 줄 길이 추적 셀입니다.

1073
00:46:57,650 --> 00:47:02,930
처음에는 매우 높은 값으로 시작합니다.

1074
00:47:02,930 --> 00:47:05,010
그리고 모델이 새 줄 문자가 있을 거라고

1075
00:47:05,010 --> 00:47:06,510
생각하는 위치에

1076
00:47:06,510 --> 00:47:08,190
가까워질수록 매우 낮은 값이 됩니다.

1077
00:47:08,190 --> 00:47:13,370
이것도 이 다른 값을 보는 멋진 방법입니다.

1078
00:47:13,370 --> 00:47:15,430
이것들은 다시 말해, 우리가

1079
00:47:15,430 --> 00:47:18,910
보고 있는 이 모델의 한 층에 있는 단일 활성화들이고,

1080
00:47:18,910 --> 00:47:20,690
각 문자에 매핑된 겁니다.

1081
00:47:20,690 --> 00:47:23,270
그래서 해석하기가 매우 쉽습니다.

1082
00:47:23,270 --> 00:47:25,450
그들은 if 문 셀도 가지고 있습니다.

1083
00:47:25,450 --> 00:47:27,150
if 문 안에 있는 모든 것이

1084
00:47:27,150 --> 00:47:29,670
여기서 추적되고 있는데, 이것도 꽤 멋집니다.

1085
00:47:29,670 --> 00:47:33,290
심지어 따옴표나 주석을 감지하는 것도 있는데,

1086
00:47:33,290 --> 00:47:36,850
여기서 주석 끝 문자를 출력해야 하니까 알아야

1087
00:47:36,850 --> 00:47:37,670
합니다.

1088
00:47:37,670 --> 00:47:39,750
그래서 이것도 추적해야 하는 겁니다.

1089
00:47:39,750 --> 00:47:42,310
그래서 해석하기 좋은 셀도 있습니다.

1090
00:47:42,310 --> 00:47:44,710
마지막으로 코드 깊이 셀입니다.

1091
00:47:44,710 --> 00:47:47,870
코드에 중첩이 있을 때마다,

1092
00:47:47,870 --> 00:47:51,610
각 단계마다, 들여쓰기나

1093
00:47:51,610 --> 00:47:53,690
코드 계층 구조로

1094
00:47:53,690 --> 00:47:58,170
들어갈 때마다 점점 더 활성화됩니다.

1095
00:47:58,170 --> 00:47:59,750
네, 이거 꽤 멋집니다.

1096
00:47:59,750 --> 00:48:01,530
실제로 활성화를 보고 입력에

1097
00:48:01,530 --> 00:48:03,130
직접 매핑할 수 있는데,

1098
00:48:03,130 --> 00:48:06,770
복잡한 트릭이 필요 없다는 점이 꽤 놀랍습니다. RNN의

1099
00:48:06,770 --> 00:48:08,490
일부 숨겨진 상태가

1100
00:48:08,490 --> 00:48:10,953
얼마나 해석 가능한지 생각해보면 말이죠.

1101
00:48:10,953 --> 00:48:13,370
사실 우리가 수동으로 할당하던 것과

1102
00:48:13,370 --> 00:48:14,830
어느 정도 비슷합니다.

1103
00:48:14,830 --> 00:48:19,410
하지만 RNN은 내부적으로 매우 유사한 과정을 수행합니다.

1104
00:48:19,410 --> 00:48:22,050
좋습니다, 이제 RNN을 사용하고 싶을 때와

1105
00:48:22,050 --> 00:48:25,530
그 이유에 대해 몇 가지 트레이드오프를 말씀드리겠습니다.

1106
00:48:25,530 --> 00:48:29,390
좋은 점은 어떤 길이의 입력도 처리할 수 있다는 겁니다.

1107
00:48:29,390 --> 00:48:31,563
그래서 트랜스포머에 의존하는

1108
00:48:31,563 --> 00:48:33,230
최신 언어 모델들은

1109
00:48:33,230 --> 00:48:36,070
최대 컨텍스트 길이나 컨텍스트 윈도우라는

1110
00:48:36,070 --> 00:48:37,510
개념이 있습니다.

1111
00:48:37,510 --> 00:48:38,530
하지만 RNN은 이런 제한이 없습니다.

1112
00:48:38,530 --> 00:48:40,890
모델을 계속 실행할 수 있는 한, 사실상

1113
00:48:40,890 --> 00:48:43,710
무한 길이의 시퀀스를 처리할 수 있습니다.

1114
00:48:43,710 --> 00:48:46,350
즉, 컨텍스트 길이 제한이 없다는 거죠.

1115
00:48:46,350 --> 00:48:50,030
시간 단계 t의 계산은 이론적으로 숨겨진 상태에

1116
00:48:50,030 --> 00:48:52,190
저장된 경우 훨씬 이전 단계의

1117
00:48:52,190 --> 00:48:54,470
정보를 사용할 수 있습니다.

1118
00:48:54,470 --> 00:48:56,430
모델이 입력 시퀀스의 모든 동적

1119
00:48:56,430 --> 00:48:58,030
특성을 숨겨진 상태에

1120
00:48:58,030 --> 00:48:59,670
효과적으로 캡처한다면, 이론상

1121
00:48:59,670 --> 00:49:02,350
매우 오래전 값도 사용할 수 있습니다.

1122
00:49:02,350 --> 00:49:04,750
하지만 실제로는 몇 가지 문제가

1123
00:49:04,750 --> 00:49:08,990
있을 수 있는데, 이에 대해서는 자세히 다루겠습니다.

1124
00:49:08,990 --> 00:49:13,490
또한, 입력이 길어져도 모델 크기가 커지지 않습니다.

1125
00:49:13,490 --> 00:49:16,230
예를 들어, 각 입력 시간

1126
00:49:16,230 --> 00:49:20,170
단계마다 다른 레이어를 가진다면 어떻게 될까요?

1127
00:49:20,170 --> 00:49:22,230
이런 문제는 없다는 점이 좋습니다.

1128
00:49:22,230 --> 00:49:25,130
그리고 각 시간 단계마다 같은 가중치를 적용합니다.

1129
00:49:25,130 --> 00:49:27,325
기본적으로, 출력 계산을 위한 업데이트

1130
00:49:27,325 --> 00:49:29,950
규칙은 매번 동일하다는 것을 알고 있습니다.

1131
00:49:29,950 --> 00:49:32,848
그래서 여기에는 멋진 대칭성이 있습니다.

1132
00:49:32,848 --> 00:49:35,390
또한 개념적으로 문제를 생각할 때,

1133
00:49:35,390 --> 00:49:36,290
매

1134
00:49:36,290 --> 00:49:38,930
타임스텝마다 항상 같은 작업을 수행한다는

1135
00:49:38,930 --> 00:49:42,570
점에서 개념적으로도 좋고 구현에도 도움이 됩니다.

1136
00:49:42,570 --> 00:49:44,270
그럼 주요 단점은 무엇일까요?

1137
00:49:44,270 --> 00:49:48,690
다음 상태를 계산하려면 매번 이전 hidden

1138
00:49:48,690 --> 00:49:50,970
state를 계산해야 합니다.

1139
00:49:50,970 --> 00:49:55,830
그래서 필요하다면 이 과정이 느릴 수 있습니다.

1140
00:49:55,830 --> 00:49:58,930
각 hidden state는 이전 모든 상태에

1141
00:49:58,930 --> 00:50:02,410
의해 결정되고 조건화되기 때문에, 이 순환

1142
00:50:02,410 --> 00:50:05,550
계산이 실제로 많은 시간이 걸릴 수 있습니다.

1143
00:50:05,550 --> 00:50:08,690
이것은 추론 시에는 문제가 되지 않지만,

1144
00:50:08,690 --> 00:50:10,750
예를 들어

1145
00:50:10,750 --> 00:50:12,570
transformer도 매번

1146
00:50:12,570 --> 00:50:16,150
다음 토큰이나 문자를 출력해야 하는 문제가

1147
00:50:16,150 --> 00:50:17,970
있고, 훈련 시에는

1148
00:50:17,970 --> 00:50:21,890
손실을 계산하기 위해 이전 hidden

1149
00:50:21,890 --> 00:50:24,470
state를 계산해야 해서 이들을

1150
00:50:24,470 --> 00:50:27,440
한꺼번에 배치 처리하기 어렵습니다.

1151
00:50:27,440 --> 00:50:32,400
그래서 많은 데이터를 다룰 때 확장성에 도전이 될 수 있습니다.

1152
00:50:32,400 --> 00:50:34,800
그리고 실제로는 고정 크기의

1153
00:50:34,800 --> 00:50:37,400
hidden state에 모든 정보를

1154
00:50:37,400 --> 00:50:40,120
담으려 하기 때문에, 여러 타임스텝

1155
00:50:40,120 --> 00:50:42,730
전의 정보를 접근하기가 어렵습니다.

1156
00:50:42,730 --> 00:50:44,480
따라서 시퀀스가 길어질수록

1157
00:50:44,480 --> 00:50:46,270
결국 일부 정보는 손실됩니다.

1158
00:50:49,160 --> 00:50:52,560
좋습니다, 이제 RNN이 성공을 거둔 컴퓨터 비전

1159
00:50:52,560 --> 00:50:56,200
분야의 좀 더 구체적인 응용 사례에 대해 이야기하겠습니다.

1160
00:50:56,200 --> 00:50:58,103
그중 하나가 이미지 캡셔닝입니다, 우리가

1161
00:50:58,103 --> 00:50:59,020
앞서 이야기한 내용이죠.

1162
00:50:59,020 --> 00:51:02,040
기본적으로 시퀀스를 시작하는 start

1163
00:51:02,040 --> 00:51:05,000
token 또는 start character가

1164
00:51:05,000 --> 00:51:06,300
있습니다.

1165
00:51:06,300 --> 00:51:08,800
그리고 end character 또는 end token이

1166
00:51:08,800 --> 00:51:09,940
나오면 종료합니다.

1167
00:51:09,940 --> 00:51:13,040
여기서는 단어 수준 토큰인 것 같습니다.

1168
00:51:13,040 --> 00:51:16,840
그래서 이런 모델을 생각할 수 있습니다.

1169
00:51:16,840 --> 00:51:19,080
가장 기본적인 방법은

1170
00:51:19,080 --> 00:51:22,080
CNN 같은 시각 인코더가

1171
00:51:22,080 --> 00:51:24,580
이미지를 인코딩하고, 이를

1172
00:51:24,580 --> 00:51:28,780
RNN의 입력으로 사용하며 이전에 생성된

1173
00:51:28,780 --> 00:51:32,220
텍스트도 함께 사용하는 것입니다.

1174
00:51:32,220 --> 00:51:33,840
여기에는 두 단계가 있습니다.

1175
00:51:33,840 --> 00:51:36,180
좀 더 구체적으로 CNN과 RNN을

1176
00:51:36,180 --> 00:51:37,680
어떻게 결합할 수 있을까요?

1177
00:51:37,680 --> 00:51:39,680
테스트 이미지를 상상해 보세요.

1178
00:51:39,680 --> 00:51:43,780
이미지가 들어오면 모델은 위쪽 첫 레이어에서

1179
00:51:43,780 --> 00:51:45,580
시작해 아래로

1180
00:51:45,580 --> 00:51:47,020
내려갑니다.

1181
00:51:47,020 --> 00:51:49,140
이것은 ImageNet 같은

1182
00:51:49,140 --> 00:51:51,420
데이터로 학습된 것일 수 있습니다.

1183
00:51:51,420 --> 00:51:53,960
우리는 클래스 레이블은 사용하지 않고, 두

1184
00:51:53,960 --> 00:51:56,120
번째 마지막 레이어를 사용합니다.

1185
00:51:56,120 --> 00:51:58,220
이것은 전이 학습에서

1186
00:51:58,220 --> 00:52:00,180
좋은 시각 표현을

1187
00:52:00,180 --> 00:52:04,420
얻기 위해 흔히 사용하는 전략입니다.

1188
00:52:04,420 --> 00:52:06,820
그래서 두 번째 마지막 레이어를 사용합니다.

1189
00:52:06,820 --> 00:52:11,560
그리고 이제 이것을 hidden state의 입력으로 사용합니다.

1190
00:52:11,560 --> 00:52:13,660
이제 hidden

1191
00:52:13,660 --> 00:52:18,680
state는 여기 Wih 값의 함수이기도 합니다.

1192
00:52:18,680 --> 00:52:23,340
즉, 단순히 hidden state만 있는 것이

1193
00:52:23,340 --> 00:52:27,100
아니라 시각적 요소도 함께 추적하는 겁니다.

1194
00:52:27,100 --> 00:52:28,760
이 부분은 과제에 포함되지

1195
00:52:28,760 --> 00:52:31,860
않으니 자세히 다루지 않겠지만, 역사적으로 CNN과 함께

1196
00:52:31,860 --> 00:52:33,640
RNN이 어떻게 사용되었는지 감을

1197
00:52:33,640 --> 00:52:35,740
드리기 위해 설명하는 겁니다.

1198
00:52:35,740 --> 00:52:37,820
ImageNet으로 사전 학습된 CNN을

1199
00:52:37,820 --> 00:52:39,487
사용하고, 이 정보를 hidden

1200
00:52:39,487 --> 00:52:41,680
state에 포함시키는 방식입니다.

1201
00:52:41,680 --> 00:52:44,480
각 타임스텝에서 토큰을 계산하기 위해

1202
00:52:44,480 --> 00:52:46,720
greedy sampling이나 다른

1203
00:52:46,720 --> 00:52:48,220
샘플링 방식을 사용합니다.

1204
00:52:48,220 --> 00:52:50,745
end token이 나오면 종료합니다.

1205
00:52:50,745 --> 00:52:52,120
우리가 end 토큰을 샘플링할 때,

1206
00:52:52,120 --> 00:52:53,720
그때가 끝낼 시점이라는 걸 알 수 있습니다.

1207
00:52:53,720 --> 00:52:56,180
그리고 이 모델들은 당시로서는 실제로 매우 잘 작동했습니다.

1208
00:52:56,180 --> 00:52:58,860
저는 이 모델들이 많은 성공을 거뒀다고 생각합니다.

1209
00:52:58,860 --> 00:53:02,040
여기서 보시면, 모델이 입력 이미지에

1210
00:53:02,040 --> 00:53:05,400
기반해 매우 합리적인 캡션을 출력하는

1211
00:53:05,400 --> 00:53:10,480
좋은 예시들이 많지만, 동시에 많은 상황에서 모델이 어려움을

1212
00:53:10,480 --> 00:53:11,960
겪기도 했습니다.

1213
00:53:11,960 --> 00:53:16,520
이것들 중 많은 부분이 훈련 데이터에서 이미지가 흔히

1214
00:53:16,520 --> 00:53:19,300
나타나는 분포와 관련이 있습니다.

1215
00:53:19,300 --> 00:53:22,700
예를 들어, 누군가가 손을 이렇게 모아서 무언가를

1216
00:53:22,700 --> 00:53:27,040
잡고 있는 모습은 마우스를 잡는 모습과 매우 비슷합니다.

1217
00:53:27,040 --> 00:53:31,860
하지만 분명히 이건 전화기라는 걸 알 수 있는데, 왜냐하면 그들이 잡고 있는

1218
00:53:31,860 --> 00:53:34,880
물체가 평평하고, 손바닥이 아래가 아니라 위를 향하고

1219
00:53:34,880 --> 00:53:36,080
있기 때문입니다.

1220
00:53:36,080 --> 00:53:39,260
이 부분은 흥미롭게 볼 수 있습니다.

1221
00:53:39,260 --> 00:53:41,380
또한, 아마도 모델은 여성이

1222
00:53:41,380 --> 00:53:46,700
고양이를 안고 있다고 생각하는데, 사실은 그녀가 털옷을 입고 있는 거죠.

1223
00:53:46,700 --> 00:53:49,760
해변을 보고 서핑보드가 있다고 추정하기도 합니다.

1224
00:53:49,760 --> 00:53:51,520
이런 종류의 환각 현상은

1225
00:53:51,520 --> 00:53:54,260
오늘날 비전-언어 모델에서

1226
00:53:54,260 --> 00:53:58,340
여전히 매우 흔한데, 특정 장면에 흔히 존재하는

1227
00:53:58,340 --> 00:54:00,080
물체가 실제로는 그 장면에

1228
00:54:00,080 --> 00:54:03,140
없는데도 있다고 생각하는 겁니다.

1229
00:54:03,140 --> 00:54:05,340
또한, 새가 나무에 앉아 있거나 공을 던지는

1230
00:54:05,340 --> 00:54:07,960
것처럼 보이지만, 실제로는 공을 잡고 있는 경우도 있습니다.

1231
00:54:07,960 --> 00:54:11,080
이 모든 것은 본질적으로 데이터셋의 편향과

1232
00:54:11,080 --> 00:54:14,340
모델이 훈련 중에 학습한 결과로, 특정 물체나

1233
00:54:14,340 --> 00:54:16,140
행동이 실제 이미지에서는

1234
00:54:16,140 --> 00:54:17,900
그렇지 않음에도 불구하고

1235
00:54:17,900 --> 00:54:20,800
매우 가능성이 높다고 판단하는 겁니다.

1236
00:54:20,800 --> 00:54:25,160
데이터셋에서는 특정 장면과 이러한 행동이나 물체가

1237
00:54:25,160 --> 00:54:27,577
함께 자주 나타납니다.

1238
00:54:27,577 --> 00:54:29,160
그래서 모델은 이들을 연관 짓는 법을

1239
00:54:29,160 --> 00:54:30,900
배우지만, 분리하는 법은 배우지 못합니다.

1240
00:54:30,900 --> 00:54:33,828
이 장면에서는 글러브가 여기 있고 공이 다른 손이

1241
00:54:33,828 --> 00:54:36,120
아니라 글러브 쪽으로 들어가고 있기

1242
00:54:36,120 --> 00:54:38,780
때문에 던지는 게 아니라는 걸 알아야 하는데,

1243
00:54:38,780 --> 00:54:40,723
모델은 그렇게 설명하지 못합니다.

1244
00:54:40,723 --> 00:54:42,140
이 모델들을 훈련시키는 방법은

1245
00:54:42,140 --> 00:54:44,180
단순히 캡션을 출력하도록 훈련시키는 것입니다.

1246
00:54:44,180 --> 00:54:46,020
그래서 여기서는 어떤

1247
00:54:46,020 --> 00:54:47,840
설명도 하지 않으며,

1248
00:54:47,840 --> 00:54:51,680
이것이 공출현 문제를 보는 이유 중 하나입니다.

1249
00:54:51,680 --> 00:54:55,300
시각적 질문 응답(Visual Question

1250
00:54:55,300 --> 00:54:57,820
Answering)은 RNN이 자주 사용된 또

1251
00:54:57,820 --> 00:55:01,520
다른 일반적인 작업이며, 시각적 질문 응답에는 두 가지

1252
00:55:01,520 --> 00:55:03,220
일반적인 공식화가 있습니다.

1253
00:55:03,220 --> 00:55:05,880
하나는 기본적으로 캡션 생성 모델이

1254
00:55:05,880 --> 00:55:08,160
있다고 가정하고, 이 모델이

1255
00:55:08,160 --> 00:55:12,020
질문에 얼마나 잘 답할 수 있는지 보는 것입니다.

1256
00:55:12,020 --> 00:55:14,658
한 가지 방법은 질문을 주고 텍스트를

1257
00:55:14,658 --> 00:55:16,200
출력하게 한 다음,

1258
00:55:16,200 --> 00:55:19,520
각 답변 시퀀스의 확률을 보는 것입니다.

1259
00:55:19,520 --> 00:55:21,920
각 문자나 토큰에 대한 확률이 있고,

1260
00:55:21,920 --> 00:55:23,420
이들을 곱해서 전체 답변의

1261
00:55:23,420 --> 00:55:25,295
확률을 구할 수 있습니다.

1262
00:55:25,295 --> 00:55:29,860
이것이 RNN 스타일 모델을 질문 응답에 사용하는

1263
00:55:29,860 --> 00:55:31,940
한 가지 방법입니다.

1264
00:55:31,940 --> 00:55:36,460
더 일반적인 방법은 질문을 모델의 입력으로

1265
00:55:36,460 --> 00:55:40,260
주고, 여러 개의 다른 답변들도

1266
00:55:40,260 --> 00:55:42,180
별도의 입력으로

1267
00:55:42,180 --> 00:55:43,577
주는 것입니다.

1268
00:55:43,577 --> 00:55:45,660
그럼 모델은 본질적으로 질문마다 확률을

1269
00:55:45,660 --> 00:55:46,280
출력합니다.

1270
00:55:46,280 --> 00:55:50,020
이 경우 네 가지 클래스가 있는 4분류기처럼 되어, 답변

1271
00:55:50,020 --> 00:55:52,628
1, 답변 2, 답변 3, 답변 4

1272
00:55:52,628 --> 00:55:54,920
각각에 대한 확률을 출력하는 겁니다.

1273
00:55:54,920 --> 00:55:56,962
여러 가지 공식화 방법이

1274
00:55:56,962 --> 00:56:00,433
있지만, 언어를 사용해야 하고 시퀀스 모델링이

1275
00:56:00,433 --> 00:56:02,100
도움이 되는 컴퓨터

1276
00:56:02,100 --> 00:56:04,900
비전에서 매우 흔한 작업입니다.

1277
00:56:04,900 --> 00:56:06,940
또한 시각적 대화(visual dialogue)도 있습니다.

1278
00:56:06,940 --> 00:56:10,520
당시에는 이 모든 작업이 매우 별개의 작업으로 여겨졌습니다.

1279
00:56:10,520 --> 00:56:13,580
요즘은 하나의 모델이 거의 모든 작업을 수행할

1280
00:56:13,580 --> 00:56:17,117
수 있지만, 이미지에 대해 어떻게 대화할 수 있을까요?

1281
00:56:17,117 --> 00:56:19,200
지난 2년간 이런 종류

1282
00:56:19,200 --> 00:56:22,960
모델들의 능력이 폭발적으로 향상된 것을 보았습니다.

1283
00:56:22,960 --> 00:56:27,800
RNN이 자주 사용된 또 다른 유형의 모델은 시각적

1284
00:56:27,800 --> 00:56:30,140
내비게이션 작업입니다.

1285
00:56:30,140 --> 00:56:33,800
그래서 이런 이미지들이 들어오고, 2D 평면도에서

1286
00:56:33,800 --> 00:56:37,420
이동할 방향의 순서를 출력하고 싶습니다.

1287
00:56:37,420 --> 00:56:40,942
목표 지점에 어떻게 도달할 수 있을까요?

1288
00:56:40,942 --> 00:56:42,400
여러분이 알아야 할

1289
00:56:42,400 --> 00:56:48,480
또 다른 응용 분야가 있는데, 여기서 이 시퀀스 모델들이 사용되었습니다.

1290
00:56:48,480 --> 00:56:53,680
제가 전에 명확히 언급하지 않았던 점

1291
00:56:53,680 --> 00:56:56,440
하나를 말씀드리자면,

1292
00:56:56,440 --> 00:57:02,600
다층 CNN이나 다층 완전 연결층처럼, 다층

1293
00:57:02,600 --> 00:57:06,060
RNN도 있을 수 있습니다.

1294
00:57:06,060 --> 00:57:07,920
실제로 제가 보여드린

1295
00:57:07,920 --> 00:57:10,280
대부분의 RNN은 다층 RNN입니다.

1296
00:57:10,280 --> 00:57:16,280
주요 차이점은 각 층을 별도로 다룬다는 겁니다.

1297
00:57:16,280 --> 00:57:19,300
예를 들어 1층의 은닉 상태는

1298
00:57:19,300 --> 00:57:21,300
1층의 이전 시점 은닉

1299
00:57:21,300 --> 00:57:23,220
상태에 의존합니다.

1300
00:57:23,220 --> 00:57:26,320
깊이 차원, 즉 depth

1301
00:57:26,320 --> 00:57:31,473
차원에서는 각 층이 이전 시점에서 그 층의

1302
00:57:31,473 --> 00:57:33,140
은닉 상태만

1303
00:57:33,140 --> 00:57:35,280
바라본다는 뜻입니다.

1304
00:57:35,280 --> 00:57:40,300
시간 차원에서 윈도우를 보는 관점에서는, 첫 번째

1305
00:57:40,300 --> 00:57:43,060
층은 실제 입력 x를 입력으로

1306
00:57:43,060 --> 00:57:45,220
받고, 두 번째

1307
00:57:45,220 --> 00:57:49,600
층은 이전 층의 출력 y를 입력으로 받습니다.

1308
00:57:49,600 --> 00:57:51,860
이렇게 층을 쌓으면,

1309
00:57:51,860 --> 00:57:57,700
각 층은 그 층 내 이전 은닉 상태만 참고하며, 층

1310
00:57:57,700 --> 00:58:00,200
간에는 입력과 출력을

1311
00:58:00,200 --> 00:58:02,140
주고받는 격자

1312
00:58:02,140 --> 00:58:03,460
형태가 됩니다.

1313
00:58:03,460 --> 00:58:06,580
그래서 오른쪽 위 값을 계산하려면

1314
00:58:06,580 --> 00:58:10,782
이 전체 계산 그래프의 모든 은닉

1315
00:58:10,782 --> 00:58:13,240
상태 값을 미리 계산해야

1316
00:58:13,240 --> 00:58:14,380
합니다.

1317
00:58:14,380 --> 00:58:17,540
훈련을 시작하면 이 과정이 매우

1318
00:58:17,540 --> 00:58:22,720
복잡하고 비효율적이라는 걸 감을 잡을 수 있습니다.

1319
00:58:22,720 --> 00:58:27,680
좋습니다, 1990년대에 제안되어 트랜스포머

1320
00:58:27,680 --> 00:58:30,900
혁명 전까지 꽤 성공을 거둔

1321
00:58:30,900 --> 00:58:34,400
RNN의 주요 변형 중 하나인

1322
00:58:34,400 --> 00:58:37,400
LSTM에 대해 이야기하겠습니다.

1323
00:58:37,400 --> 00:58:41,500
LSTM 작동 원리를 자세히 알 필요는

1324
00:58:41,500 --> 00:58:44,360
없지만, RNN이 가진 주요

1325
00:58:44,360 --> 00:58:49,300
단점을 LSTM이 완화하려 한다는 점과, 최신 상태

1326
00:58:49,300 --> 00:58:51,680
공간 모델들도 RNN이

1327
00:58:51,680 --> 00:58:53,840
겪는 비슷한 문제를

1328
00:58:53,840 --> 00:58:57,640
해결하려 한다는 점을 이해하시길 바랍니다.

1329
00:58:57,640 --> 00:59:01,720
기본적으로 tanh가 매우 흔히 쓰이는 활성화

1330
00:59:01,720 --> 00:59:04,580
함수라는 점을 이야기했죠.

1331
00:59:04,580 --> 00:59:06,560
그리고 이전 은닉

1332
00:59:06,560 --> 00:59:09,200
상태를 새로운 은닉 상태로

1333
00:59:09,200 --> 00:59:11,920
변환하는 Whh 행렬이 있고,

1334
00:59:11,920 --> 00:59:17,180
현재 시점 입력 벡터 xt를 은닉 상태 차원으로

1335
00:59:17,180 --> 00:59:19,420
변환하는 Wxh 행렬과

1336
00:59:19,420 --> 00:59:22,860
합산된다는 점도 말씀드렸습니다.

1337
00:59:22,860 --> 00:59:28,320
이걸 이렇게 가중치가 있고 벡터를 쌓는 형태로도

1338
00:59:28,320 --> 00:59:31,300
표현할 수 있습니다.

1339
00:59:31,300 --> 00:59:33,100
그래서 간단히 두

1340
00:59:33,100 --> 00:59:37,540
W를 합쳐 하나의 큰 W로 표현하기도 합니다.

1341
00:59:37,540 --> 00:59:42,700
하지만 이 두 블록은 대각선으로 위치해 있어서,

1342
00:59:42,700 --> 00:59:45,540
예를 들어 이렇게 표현하면

1343
00:59:45,540 --> 00:59:48,280
W 안에 0이 많습니다.

1344
00:59:48,280 --> 00:59:50,200
Whh는 xt와

1345
00:59:50,200 --> 00:59:53,680
상호작용하지 않기 때문입니다.

1346
00:59:53,680 --> 00:59:55,700
이건 수학적 표기를

1347
00:59:55,700 --> 00:59:57,820
쉽게 하기 위한

1348
00:59:57,820 --> 00:59:58,760
축약형입니다.

1349
00:59:58,760 --> 01:00:00,940
여기 세 가지 변형을 모두 보실 수 있습니다.

1350
01:00:00,940 --> 01:00:02,420
이것이 아마 실제 값,

1351
01:00:02,420 --> 01:00:05,480
0이 아닌 값, 그리고 가중치 행렬이 어디에 있는지

1352
01:00:05,480 --> 01:00:07,100
가장 명확하게 보여줍니다.

1353
01:00:07,100 --> 01:00:09,680
한 가지 생각하는 방법은

1354
01:00:09,680 --> 01:00:11,240
벡터들을 이렇게 쌓고,

1355
01:00:11,240 --> 01:00:13,980
이 W를 곱한 뒤 tanh를 통과시키는 겁니다.

1356
01:00:13,980 --> 01:00:18,243
이렇게 해서 출력 h,t를 얻고, 다음 RNN에 전달합니다.

1357
01:00:18,243 --> 01:00:19,660
이것들이 쌓여 있다고 상상할 수 있습니다.

1358
01:00:19,660 --> 01:00:23,320
그리고 출력 yt를 직접 가질 수도

1359
01:00:23,320 --> 01:00:27,760
있고, 아니면 가중치 행렬과 활성화 함수가

1360
01:00:27,760 --> 01:00:31,040
있는 층을 거칠 수도 있습니다.

1361
01:00:31,040 --> 01:00:32,960
네, 질문 있나요?

1362
01:00:32,960 --> 01:00:35,680
네, 물론입니다.

1363
01:00:35,680 --> 01:00:40,320
여기 다층 RNN이 있는데, [잘 안 들림] 행렬은

1364
01:00:40,320 --> 01:00:41,320
어떻게 되나요?

1365
01:00:41,320 --> 01:00:43,640
네, 다층 RNN에서는 가중치가 층

1366
01:00:43,640 --> 01:00:44,700
내에서 공유됩니다.

1367
01:00:44,700 --> 01:00:49,600
그래서 모든 은닉 상태 업데이트에 같은

1368
01:00:49,600 --> 01:00:51,300
가중치를 사용합니다.

1369
01:00:51,300 --> 01:00:54,360
그리고 이 다이어그램에서 수직으로 쌓인

1370
01:00:54,360 --> 01:00:58,440
각 레이어는 각각 별도의 가중치 집합을 갖습니다.

1371
01:00:58,440 --> 01:01:04,420
네, 이렇게 작동하는 방식입니다.

1372
01:01:04,420 --> 01:01:06,700
그리고 우리가 이야기했던 역전파가

1373
01:01:06,700 --> 01:01:07,640
있을 때,

1374
01:01:07,640 --> 01:01:11,060
각 타임스텝마다 손실이 없다면,

1375
01:01:11,060 --> 01:01:14,380
출력 h,t의 손실만을 기준으로 손실을

1376
01:01:14,380 --> 01:01:18,060
계산해야 하는 이유입니다. 역전파를

1377
01:01:18,060 --> 01:01:20,920
할 때는 W를 곱하고,

1378
01:01:20,920 --> 01:01:23,380
tanh의 미분도 취합니다.

1379
01:01:23,380 --> 01:01:25,560
이 두 가지 모두 실제로 문제가 될 수 있습니다.

1380
01:01:25,560 --> 01:01:28,860
특히 수학적으로 보면, h,

1381
01:01:28,860 --> 01:01:32,300
t-1의 각 구성 요소를 변경하면 h,

1382
01:01:32,300 --> 01:01:36,820
t에 어떤 영향을 미치는지 그라디언트를

1383
01:01:36,820 --> 01:01:38,340
계산하는 겁니다.

1384
01:01:38,340 --> 01:01:40,880
죄송합니다, h,t-1의 각 구성 요소를 변경하면

1385
01:01:40,880 --> 01:01:43,010
h,t가 어떻게 변하는지 계산하는 거죠.

1386
01:01:43,010 --> 01:01:44,760
이 그라디언트가 바로 그것을 계산합니다.

1387
01:01:44,760 --> 01:01:47,900
tanh의 미분이 필요한 이유는 이것이 우리의 활성화 함수이기

1388
01:01:47,900 --> 01:01:48,520
때문입니다.

1389
01:01:48,520 --> 01:01:52,500
그리고 Whh는 이전 은닉 상태를 다음 상태로

1390
01:01:52,500 --> 01:01:55,357
변환하기 위해 곱하는 행렬입니다.

1391
01:01:55,357 --> 01:01:57,440
이것이 실제로 그라디언트를 계산하는 방법입니다.

1392
01:01:57,440 --> 01:01:58,920
여기서 문제가 발생할 수 있습니다.

1393
01:01:58,920 --> 01:02:04,260
각 타임스텝에서 손실을 계산하고, 여기서 총

1394
01:02:04,260 --> 01:02:06,980
손실은 각 가중치에

1395
01:02:06,980 --> 01:02:09,760
대해 합산한 값입니다.

1396
01:02:09,760 --> 01:02:11,200
즉, 총 손실은

1397
01:02:11,200 --> 01:02:13,600
이 재사용된 W

1398
01:02:13,600 --> 01:02:18,560
행렬에 대해 각 타임스텝 손실의 합입니다.

1399
01:02:18,560 --> 01:02:23,280
그래서 최종 단계 L,t에서 손실

1400
01:02:26,640 --> 01:02:32,200
L의 그라디언트를 계산하려면, 체인

1401
01:02:32,200 --> 01:02:35,560
룰을 사용해 중간 은닉

1402
01:02:35,560 --> 01:02:39,880
상태들이 W에 미치는 영향을 모두

1403
01:02:39,880 --> 01:02:42,400
계산해야 합니다.

1404
01:02:42,400 --> 01:02:44,400
여기 예시를 언급했는데, 왜 이것이

1405
01:02:44,400 --> 01:02:46,640
문제가 되는지 설명하기 위해서입니다.

1406
01:02:46,640 --> 01:02:48,540
이 개별 항들을

1407
01:02:48,540 --> 01:02:50,840
살펴보면, 현재 은닉 상태를

1408
01:02:50,840 --> 01:02:55,360
바꾸는 것이 다음 상태에 어떻게 영향을

1409
01:02:55,360 --> 01:02:58,560
미치는지에 집중할 때, 이 곱셈 항에

1410
01:02:58,560 --> 01:03:00,640
포함된 계산 대부분이

1411
01:03:00,640 --> 01:03:04,860
바로 그것입니다. 여기서 tanh의

1412
01:03:04,860 --> 01:03:09,100
도함수와 Whh가 곱해지는 것과 같은 내용이죠.

1413
01:03:09,100 --> 01:03:10,860
그럼 왜 이것이 문제일까요?

1414
01:03:10,860 --> 01:03:15,560
우선, 여기 tanh 도함수가 그래프로 그려져 있습니다.

1415
01:03:15,560 --> 01:03:19,220
최대값은 1이고, 거의 항상

1416
01:03:19,220 --> 01:03:20,920
1보다 작습니다.

1417
01:03:20,920 --> 01:03:25,540
그래서 이 항에서 기울기 소실(vanishing gradients)이 발생할
수 있습니다.

1418
01:03:25,540 --> 01:03:30,540
하지만 비선형성이 없다고 가정하거나, 이런 문제가 없는

1419
01:03:30,540 --> 01:03:33,140
다른 활성화 함수를 선택한다고

1420
01:03:33,140 --> 01:03:34,620
해도 말이죠.

1421
01:03:34,620 --> 01:03:38,860
여기서 매 시간 단계마다 곱해지는 이 가중치

1422
01:03:38,860 --> 01:03:41,980
행렬을 보면, 큰 특이값(singular

1423
01:03:41,980 --> 01:03:45,160
value)을 가질 수도

1424
01:03:45,160 --> 01:03:49,440
있습니다. 즉, 벡터가 들어올 때 최대 얼마만큼

1425
01:03:49,440 --> 01:03:52,640
늘어날 수 있는지를 나타냅니다.

1426
01:03:52,640 --> 01:03:55,980
예를 들어 단위 벡터의 경우, 특이값은 행렬에

1427
01:03:55,980 --> 01:03:58,060
의해 최대 얼마만큼 늘어날

1428
01:03:58,060 --> 01:03:59,700
수 있는지를 알려줍니다.

1429
01:03:59,700 --> 01:04:02,220
만약 매우 크면 기울기가 폭발할

1430
01:04:02,220 --> 01:04:04,720
수 있고, 매우 작으면 기울기 소실

1431
01:04:04,720 --> 01:04:06,280
문제가 발생합니다.

1432
01:04:06,280 --> 01:04:08,440
기울기 폭발(exploding gradients)이

1433
01:04:08,440 --> 01:04:11,300
발생하면, 이를 해결하는 방법이 있는데 바로 기울기를 스케일링하는 것입니다.

1434
01:04:11,300 --> 01:04:14,920
즉, 기울기를 나누거나 클리핑해서 너무 큰

1435
01:04:14,920 --> 01:04:15,460
기울기가

1436
01:04:15,460 --> 01:04:17,793
문제가 되지 않도록 하는 거죠.

1437
01:04:17,793 --> 01:04:21,240
하지만 이 아주 작은 기울기 소실

1438
01:04:21,240 --> 01:04:23,640
문제는 실제로 사람들이 긴

1439
01:04:23,640 --> 01:04:26,880
RNN을 잘 사용하지 않는 주된

1440
01:04:26,880 --> 01:04:29,260
이유입니다. tanh

1441
01:04:29,260 --> 01:04:31,400
때문이고, 많은 경우 가중치

1442
01:04:31,400 --> 01:04:37,660
행렬이 활성화를 확장하거나 축소하는 성질을 가지기 때문입니다.

1443
01:04:37,660 --> 01:04:42,240
그래서 네, 이런 점들이 RNN 구조 변화를 촉진한 주요 이유들이고,

1444
01:04:42,240 --> 01:04:45,200
사람들이 RNN을 잘 사용하지 않는

1445
01:04:45,200 --> 01:04:46,760
많은 이유 중 하나입니다.

1446
01:04:46,760 --> 01:04:48,560
이것이 주요 문제 중 하나입니다.

1447
01:04:48,560 --> 01:04:50,680
그럼 어떻게 해결할까요?

1448
01:04:50,680 --> 01:04:54,760
사람들이 한 방법은 바로 LSTM을 만든 것입니다.

1449
01:04:54,760 --> 01:04:57,520
그리고 고수준 아이디어는, 너무 자세히

1450
01:04:57,520 --> 01:04:59,590
들어가진 않겠지만, 실제로 꽤

1451
01:04:59,590 --> 01:05:03,110
복잡한데, 네 가지 다른 게이트가 각각 다른

1452
01:05:03,110 --> 01:05:05,790
값을 추적한다는 겁니다. 하나의 히든

1453
01:05:05,790 --> 01:05:07,390
상태만 있는 게 아니라

1454
01:05:07,390 --> 01:05:09,330
여러 개의 값이 있다는 거죠.

1455
01:05:09,330 --> 01:05:11,890
히든 상태를 어떻게 바꿀지 미리 계산하고,

1456
01:05:11,890 --> 01:05:14,030
또 다른 경로로 어떤 정보를

1457
01:05:14,030 --> 01:05:15,295
전달할지도 결정합니다.

1458
01:05:15,295 --> 01:05:17,170
그래서 일반적인 히든 상태 경로가 있고,

1459
01:05:17,170 --> 01:05:19,087
정보를 더 쉽게 전달할 수 있는

1460
01:05:19,087 --> 01:05:20,630
다른 경로가 있습니다.

1461
01:05:20,630 --> 01:05:26,470
이게 기본 아이디어인데, 고수준에서 보면 게이트라고 부르며, 실제로 셀의

1462
01:05:26,470 --> 01:05:30,070
히든 상태에 무엇을 쓰는지 결정하는 겁니다.

1463
01:05:30,070 --> 01:05:32,870
입력 게이트는 셀에 정보를 쓸지

1464
01:05:32,870 --> 01:05:34,530
말지를 결정합니다.

1465
01:05:34,530 --> 01:05:37,150
포겟 게이트는 이전 시점에서 얼마나

1466
01:05:37,150 --> 01:05:40,030
잊을지, 그리고 출력 게이트는 히든

1467
01:05:40,030 --> 01:05:42,963
상태로 얼마나 출력할지를 결정합니다.

1468
01:05:42,963 --> 01:05:44,630
보시다시피, 이건 정말 복잡하고

1469
01:05:44,630 --> 01:05:46,530
많은 설계 선택이 필요합니다.

1470
01:05:46,530 --> 01:05:49,470
이 모든 걸 모아서 꽤 복잡한 다이어그램으로

1471
01:05:49,470 --> 01:05:51,670
만들었는데, 기본 아이디어는

1472
01:05:51,670 --> 01:05:54,110
이 부분은 똑같이 가중치

1473
01:05:54,110 --> 01:05:55,548
곱셈을 한다는 겁니다.

1474
01:05:55,548 --> 01:05:58,090
하지만 이제는 h,t 하나만 계산하는 게

1475
01:05:58,090 --> 01:06:00,370
아니라 네 가지 다른 값을 계산합니다.

1476
01:06:00,370 --> 01:06:03,290
입력 게이트와 여기에 얼마나 쓸지

1477
01:06:03,290 --> 01:06:05,410
결정하는 게이트, 그리고

1478
01:06:05,410 --> 01:06:10,050
다음 히든 상태로 전달되는 출력을 갖고 있습니다.

1479
01:06:10,050 --> 01:06:12,610
여기 위쪽 부분은 활성화 함수가

1480
01:06:12,610 --> 01:06:14,970
전혀 없는 고속도로라고

1481
01:06:14,970 --> 01:06:16,910
생각할 수 있습니다.

1482
01:06:16,910 --> 01:06:17,770
그래서 tanh가 없습니다.

1483
01:06:17,770 --> 01:06:20,130
그래서 tanh 때문에 그래디언트가

1484
01:06:20,130 --> 01:06:24,530
사라지는 문제를 피하고, 우리가 적용하는 건 포겟

1485
01:06:24,530 --> 01:06:25,630
게이트뿐입니다.

1486
01:06:25,630 --> 01:06:28,530
즉, 매 시점마다 모든

1487
01:06:28,530 --> 01:06:32,150
정보를 잊지 않는 한, 정보를 더

1488
01:06:32,150 --> 01:06:34,830
쉽게 전달할 수 있습니다.

1489
01:06:34,830 --> 01:06:36,510
이게 고수준 설명입니다.

1490
01:06:36,510 --> 01:06:38,350
그리고 더 중요한 것은, 실제로

1491
01:06:38,350 --> 01:06:41,570
사람들이 이것이 매우 잘 작동한다고 보는 것 같습니다.

1492
01:06:41,570 --> 01:06:45,090
다시 말하지만, 이 과제에서는 전혀 구현하지 않지만,

1493
01:06:45,090 --> 01:06:47,930
이것은 여전히 많은 딥러닝 논문에서 흔히

1494
01:06:47,930 --> 01:06:50,150
사용되는 기준선이라고 생각합니다.

1495
01:06:50,150 --> 01:06:51,670
그래서 알아두는 것이 좋습니다.

1496
01:06:51,670 --> 01:06:54,690
하지만 이것을 RNN이 가진 문제들, 즉 소실되는

1497
01:06:54,690 --> 01:06:56,750
그래디언트와 정보가 제대로 포착되지

1498
01:06:56,750 --> 01:06:58,250
않는 문제를 보완하기

1499
01:06:58,250 --> 01:07:01,790
위해 사람들이 이런 구조를 만들려고 했다는 관점에서 생각할

1500
01:07:01,790 --> 01:07:02,745
수 있습니다.

1501
01:07:02,745 --> 01:07:04,870
모든 것을 이 숨겨진 상태에 다 집어넣어야 하니까요.

1502
01:07:04,870 --> 01:07:06,330
그래서 정말 긴 시계열 의존성이 있을 때,

1503
01:07:06,330 --> 01:07:07,050
그것들이 사라지는 겁니다.

1504
01:07:07,050 --> 01:07:08,670
그래서 이 더 긴

1505
01:07:08,670 --> 01:07:12,910
시계열 정보를 위쪽 경로를 통해 전달하는

1506
01:07:12,910 --> 01:07:15,230
별도의 경로를 만든 거죠.

1507
01:07:15,230 --> 01:07:19,190
그렇다면 LSTM이 소실되는 그래디언트 문제를 완전히 해결했나요?

1508
01:07:19,190 --> 01:07:20,330
확실히 도움이 됩니다.

1509
01:07:20,330 --> 01:07:22,648
그래서 이 위쪽 경로 다이어그램을

1510
01:07:22,648 --> 01:07:24,190
사용해서 RNN이 여러 시간

1511
01:07:24,190 --> 01:07:26,950
단계에 걸쳐 정보를 보존하기가 더 쉬워졌습니다.

1512
01:07:26,950 --> 01:07:29,850
반면에, Vanilla RNN은

1513
01:07:29,850 --> 01:07:32,190
매 시간 단계마다 같은 연산을

1514
01:07:32,190 --> 01:07:35,950
반복하고 활성화 함수를 거치지 않고 직접

1515
01:07:35,950 --> 01:07:38,450
정보를 전달할 수 없기 때문에,

1516
01:07:38,450 --> 01:07:40,270
숨겨진 상태에

1517
01:07:40,270 --> 01:07:42,630
정보를 보존하는 가중치 행렬을

1518
01:07:42,630 --> 01:07:45,190
학습하기가 훨씬 어렵습니다.

1519
01:07:45,190 --> 01:07:49,550
그래서 완전히 보장하지는 않지만, 훨씬 쉽게 만들고

1520
01:07:49,550 --> 01:07:54,010
장기 의존성 학습을 개선하며 경험적으로도 매우

1521
01:07:54,010 --> 01:07:55,430
잘 작동합니다.

1522
01:07:55,430 --> 01:07:59,450
그래서 일반적으로 RNN을 많이 훈련시키지 않고,

1523
01:07:59,450 --> 01:08:01,210
이 순환 모델링 방식을

1524
01:08:01,210 --> 01:08:07,323
선택한다면 LSTM을 더 자주 훈련시킵니다. 하지만 전반적으로 이

1525
01:08:07,323 --> 01:08:07,990
방법들도—제가

1526
01:08:07,990 --> 01:08:10,570
말했듯이—상당히 유행이 지난

1527
01:08:10,570 --> 01:08:12,290
편이지만, 사람들이

1528
01:08:12,290 --> 01:08:15,850
RNN의 문제를 해결하기 위해 어떻게 설계했는지

1529
01:08:15,850 --> 01:08:18,450
이해하는 데 도움이 됩니다.

1530
01:08:18,450 --> 01:08:22,689
그리고 이 강의에서 이전에 배운 내용과 연결하면

1531
01:08:22,689 --> 01:08:24,189
흥미로운 점이 하나

1532
01:08:24,189 --> 01:08:27,609
있는데, 출력값을 직접 더하고 일부 활성화

1533
01:08:27,609 --> 01:08:29,770
함수나 다른 층을 건너뛰는

1534
01:08:29,770 --> 01:08:31,649
아이디어가 ResNet에서

1535
01:08:31,649 --> 01:08:33,529
논의한 스킵 연결과

1536
01:08:33,529 --> 01:08:36,850
매우 관련이 있다는 겁니다. 여기서는

1537
01:08:36,850 --> 01:08:41,550
값이 단순히 복사되어 나중에 레이어 블록에서 더해집니다.

1538
01:08:41,550 --> 01:08:43,162
ResNet에서는 이런 스킵 연결이 여러 개 있습니다.

1539
01:08:43,162 --> 01:08:45,370
여러 개의 convolution layer를

1540
01:08:45,370 --> 01:08:48,210
쌓고, 그다음에 값이 그냥 더해지는 skip connection을

1541
01:08:48,210 --> 01:08:49,310
추가합니다.

1542
01:08:49,310 --> 01:08:52,590
이것을 LSTM에도 비슷하게 생각할

1543
01:08:52,590 --> 01:08:56,390
수 있는데, 일부 레이어를 건너뛰면서

1544
01:08:56,390 --> 01:08:59,729
모델의 깊이가 매우 깊어지는 대신

1545
01:08:59,729 --> 01:09:02,010
성능이 향상되는 거죠.

1546
01:09:02,010 --> 01:09:04,569
여기서는 매우 긴 시퀀스의 시간 단계입니다.

1547
01:09:04,569 --> 01:09:07,212
그래서 이건 병렬적이지만, 하나는

1548
01:09:07,212 --> 01:09:08,670
레이어 수이고

1549
01:09:08,670 --> 01:09:12,750
다른 하나는 시간 단계 수라서 조금 다릅니다.

1550
01:09:12,750 --> 01:09:16,910
좋습니다, 오늘 강의의 마지막 슬라이드는 최근

1551
01:09:16,910 --> 01:09:21,670
1~2년 사이에 RNN이 다시 주목받게 된 점에 대한

1552
01:09:21,670 --> 01:09:24,410
간단한 연결입니다. 재미있는

1553
01:09:24,410 --> 01:09:28,247
점은, 만약 1~2년 전에 이 강의를 했다면

1554
01:09:28,247 --> 01:09:30,830
RNN을 완전히 빼고 싶었을

1555
01:09:30,830 --> 01:09:31,993
거라는 겁니다.

1556
01:09:31,993 --> 01:09:34,410
하지만 사실 RNN이 가진 좋은 장점들이 많습니다.

1557
01:09:34,410 --> 01:09:37,250
가장 큰 장점은 무제한의 컨텍스트 길이입니다.

1558
01:09:37,250 --> 01:09:39,510
트랜스포머의 주요 문제 중 하나는 제한된

1559
01:09:39,510 --> 01:09:42,109
컨텍스트 길이인데, 사람들이 이 모델들이 할 수 있는

1560
01:09:42,109 --> 01:09:44,609
것의 한계를 계속 밀어붙이고 있기 때문입니다.

1561
01:09:44,609 --> 01:09:47,170
이 컨텍스트 길이 문제가 점점 더 중요해지고 있습니다.

1562
01:09:47,170 --> 01:09:49,229
그래서 트랜스포머 분야에서는 여러 가지 해결책이

1563
01:09:49,229 --> 01:09:49,843
나왔습니다.

1564
01:09:49,843 --> 01:09:52,010
사람들은 rope 같은 기법이나

1565
01:09:52,010 --> 01:09:54,229
다른 기술들을 사용해서 컨텍스트 길이를

1566
01:09:54,229 --> 01:09:57,770
늘리려고 시도합니다만, 이것은 모델의 꽤 큰 한계입니다.

1567
01:09:57,770 --> 01:10:02,230
또한 RNN의 추론 시 계산량은 시퀀스 길이에

1568
01:10:02,230 --> 01:10:05,210
비례해서 선형적으로 증가합니다.

1569
01:10:05,210 --> 01:10:07,510
훈련 시에도 마찬가지인데,

1570
01:10:07,510 --> 01:10:11,592
시퀀스가 길어질수록 같은 연산을

1571
01:10:11,592 --> 01:10:14,050
반복해서 다시 계산해야

1572
01:10:14,050 --> 01:10:14,550
합니다.

1573
01:10:14,550 --> 01:10:17,530
그래서 트랜스포머처럼 전체 입력

1574
01:10:17,530 --> 01:10:20,410
시퀀스를 한 번에 보는 연산은 없습니다.

1575
01:10:20,410 --> 01:10:22,350
이 점들이 큰 장점입니다. 그리고

1576
01:10:22,350 --> 01:10:24,030
몇몇 논문들이 있습니다.

1577
01:10:24,030 --> 01:10:28,773
몇 가지 예를 들자면 RWKV 모델이 있습니다.

1578
01:10:28,773 --> 01:10:30,190
여기 arXiv 링크를

1579
01:10:30,190 --> 01:10:33,558
확인할 수 있고, Mamba도 있습니다. 이들은 주로 선형 시간

1580
01:10:33,558 --> 01:10:35,850
시퀀스 모델링을 달성할 수 있다는 점을

1581
01:10:35,850 --> 01:10:36,350
강조합니다.

1582
01:10:36,350 --> 01:10:38,310
즉, 입력 시퀀스가 커질수록

1583
01:10:38,310 --> 01:10:41,130
계산량도 선형적으로 증가해서 트랜스포머의 제곱

1584
01:10:41,130 --> 01:10:42,490
증가보다 효율적입니다.

1585
01:10:42,490 --> 01:10:46,610
그래서 긴 컨텍스트 문제에 더 적합할 때가 있습니다.

1586
01:10:46,610 --> 01:10:48,835
계산 측면에서 더 효율적이고 이런

1587
01:10:48,835 --> 01:10:50,210
주요 장점들이 있습니다.

1588
01:10:50,210 --> 01:10:51,810
사람들은 두 세계의 장점을 모두 얻으려고

1589
01:10:51,810 --> 01:10:53,850
노력하고 있고, 이 분야에서 많은 연구가 진행 중입니다.

1590
01:10:53,850 --> 01:10:55,808
어떻게 하면 RNN의 확장성과

1591
01:10:55,808 --> 01:10:58,350
트랜스포머의 성능을 모두 얻을 수 있을까요?

1592
01:10:58,350 --> 01:11:01,330
좋습니다, 오늘 수업은 여기까지입니다.

1593
01:11:01,330 --> 01:11:04,070
우리는 기본적으로 RNN으로 다양한 아키텍처를

1594
01:11:04,070 --> 01:11:06,370
설계하는 방법에 대해 이야기했습니다.

1595
01:11:06,370 --> 01:11:09,327
Vanilla RNN은 단순하지만 성능이 좋지 않습니다.

1596
01:11:09,327 --> 01:11:10,910
사람들이 제안한 더

1597
01:11:10,910 --> 01:11:13,710
복잡한 변형들은 정보를 선택적으로

1598
01:11:13,710 --> 01:11:16,190
전달하는 방법을 도입했습니다.

1599
01:11:16,190 --> 01:11:18,110
RNN에서의 역방향 그래디언트

1600
01:11:18,110 --> 01:11:20,190
흐름은 사용하는 활성화 함수나 가중치

1601
01:11:20,190 --> 01:11:22,607
행렬의 특성에 따라 폭발하거나 소실될 수

1602
01:11:22,607 --> 01:11:23,230
있습니다.

1603
01:11:23,230 --> 01:11:26,910
그래서 보통 시간에 따른 역전파(back propagation through

1604
01:11:26,910 --> 01:11:29,630
time)를 통해 그래디언트를 계산해야 합니다.

1605
01:11:29,630 --> 01:11:32,510
마지막으로, 더 나은 아키텍처는 현재

1606
01:11:32,510 --> 01:11:34,670
활발한 연구 주제이며, 시퀀스를

1607
01:11:34,670 --> 01:11:37,748
다루는 새로운 패러다임도 마찬가지입니다.

1608
01:11:37,748 --> 01:11:39,290
네, 오늘은 여기까지입니다.

1609
01:11:39,290 --> 01:11:43,180
다음 시간에는 attention과 transformers에 대해
이야기하겠습니다.
