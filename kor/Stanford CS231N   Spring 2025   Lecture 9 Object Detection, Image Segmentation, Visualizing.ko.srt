1
00:00:05,270 --> 00:00:06,470
네.

2
00:00:06,470 --> 00:00:12,920
오늘은 컴퓨터 비전의 핵심 작업들, 알고리즘과

3
00:00:12,920 --> 00:00:19,110
작업들, 검출과 분할에 대해 이야기하겠습니다.

4
00:00:19,110 --> 00:00:22,250
또한 시각화와 이해에 관한 주제들도

5
00:00:22,250 --> 00:00:23,700
다룰 것입니다.

6
00:00:23,700 --> 00:00:26,275
가장 중요한 것들을 다루겠습니다.

7
00:00:28,820 --> 00:00:32,040
지난 강의에서 다룬

8
00:00:32,040 --> 00:00:37,430
내용처럼, 시퀀스 투 시퀀스 모델, RNN에서

9
00:00:37,430 --> 00:00:43,940
트랜스포머로의 전환에 대해 이야기했습니다.

10
00:00:43,940 --> 00:00:47,570
트랜스포머는 인코더, 여러

11
00:00:47,570 --> 00:00:52,520
층, 멀티헤드 셀프 어텐션,

12
00:00:52,520 --> 00:00:57,540
레이어 노름, 그리고 MLP 층으로

13
00:00:57,540 --> 00:01:01,470
정의된다는 것을 보았습니다.

14
00:01:01,470 --> 00:01:06,500
이것이 결국 우리가 인코더가 시퀀스를

15
00:01:06,500 --> 00:01:10,500
인코딩한다고 부르는 것입니다.

16
00:01:10,500 --> 00:01:16,100
그리고 만약 이미지나 언어 시퀀스를

17
00:01:16,100 --> 00:01:19,970
출력으로 디코딩해야

18
00:01:19,970 --> 00:01:27,210
한다면, 유사한 구조가 디코더에 사용되어

19
00:01:27,210 --> 00:01:32,660
인코더 토큰을 입력으로 받아

20
00:01:32,660 --> 00:01:34,970
원하는 출력을

21
00:01:34,970 --> 00:01:42,060
생성합니다. 제 커서도 보이시길 바랍니다.

22
00:01:42,060 --> 00:01:43,560
저스틴이

23
00:01:43,560 --> 00:01:48,620
시퀀스 모델링, 순환 신경망 RNN과

24
00:01:48,620 --> 00:01:55,470
그 변형들에 대해 지난주 화요일에 꽤 자세히

25
00:01:55,470 --> 00:01:59,210
이야기했고, 또 다른 접근법으로

26
00:01:59,210 --> 00:02:04,940
컨볼루션을 사용하는 것에 대해서도

27
00:02:04,940 --> 00:02:06,690
이야기했습니다.

28
00:02:06,690 --> 00:02:10,639
하지만 결국 요즘 많은 응용에서

29
00:02:10,639 --> 00:02:17,910
우리가 사용하는 것은 셀프 어텐션이라는 점을 이야기했습니다.

30
00:02:17,910 --> 00:02:24,470
이것들이 다른 두 방법보다 훨씬 잘 작동합니다.

31
00:02:24,470 --> 00:02:25,590
비용이 더 많이 듭니다.

32
00:02:25,590 --> 00:02:31,775
계산과 메모리 요구량이 증가하지만, 시퀀스

33
00:02:31,775 --> 00:02:36,650
모델링이 훨씬 좋아지고 모든

34
00:02:36,650 --> 00:02:42,110
작업에서 더 나은 결과를 가져옵니다.

35
00:02:42,110 --> 00:02:48,835
여기까지는 주로 셀프 어텐션에 대해 이야기했습니다.

36
00:02:48,835 --> 00:02:50,210
크로스

37
00:02:50,210 --> 00:02:55,050
어텐션과 관련 주제도 조금 다뤘습니다.

38
00:02:55,050 --> 00:02:59,750
그리고 현대 컴퓨터 비전 응용에서

39
00:02:59,750 --> 00:03:02,840
핵심 모델 중 하나인

40
00:03:02,840 --> 00:03:06,290
비전 트랜스포머 주제로

41
00:03:06,290 --> 00:03:07,410
넘어갔습니다.

42
00:03:07,410 --> 00:03:17,330
지난 강의 마지막 몇 분 동안 이 내용을 다뤘습니다.

43
00:03:17,330 --> 00:03:18,300
지난 강의 마지막 몇 분 동안 이 내용을 다뤘습니다.

44
00:03:18,300 --> 00:03:21,030
그리고 이 주제를 다시 살펴보고자 합니다.

45
00:03:21,030 --> 00:03:26,000
그 후에는 제가 멈추고 지금까지 이야기한 과제나

46
00:03:26,000 --> 00:03:28,340
내용에 대해 질문이나

47
00:03:28,340 --> 00:03:30,830
의견이 있으시면 듣겠습니다.

48
00:03:30,830 --> 00:03:33,320
트랜스포머를

49
00:03:33,320 --> 00:03:38,460
이용해 이미지를 처리할 때,

50
00:03:38,460 --> 00:03:48,440
이미지를 패치로 나누어 시퀀스를 만든다는 점을 이야기했습니다.

51
00:03:48,440 --> 00:03:57,590
이미지는 S 곱하기 S, 여기서는 아마 3 곱하기 3 패치로

52
00:03:57,590 --> 00:03:59,760
나누어졌습니다.

53
00:03:59,760 --> 00:04:02,630
각 패치는 토큰이라고 부르는

54
00:04:02,630 --> 00:04:05,280
것으로 표현됩니다.

55
00:04:05,280 --> 00:04:10,610
토큰은 보통 이미지를

56
00:04:10,610 --> 00:04:17,810
벡터로 재구성한 후 선형 투영한

57
00:04:17,810 --> 00:04:20,100
것입니다.

58
00:04:20,100 --> 00:04:24,510
기본적으로 이 슬라이드에서 보시는 것처럼

59
00:04:24,510 --> 00:04:27,060
D차원 벡터입니다.

60
00:04:27,060 --> 00:04:29,330
하지만 이미지를 패치로 나누었기 때문에,

61
00:04:29,330 --> 00:04:32,210
중요한 것은—우리가 여기서 무엇을 잃고

62
00:04:32,210 --> 00:04:33,540
있는가 하는 점입니다.

63
00:04:33,540 --> 00:04:36,390
기본적으로 이미지의 2D 위치

64
00:04:36,390 --> 00:04:38,550
정보를 잃고 있는 거죠.

65
00:04:38,550 --> 00:04:43,970
그래서 우리는 종종 positional embedding이라고 부르는

66
00:04:43,970 --> 00:04:46,110
것을 생성하거나 추가합니다.

67
00:04:46,110 --> 00:04:49,080
이것을 하는 방법은 여러 가지가 있습니다.

68
00:04:49,080 --> 00:04:53,790
시퀀스를 만들어서 1, 2, 3처럼 숫자를 순서대로

69
00:04:53,790 --> 00:04:55,290
넣을 수도 있고요.

70
00:04:55,290 --> 00:05:02,250
또는 x, y 좌표의 2D 버전을 만들어서 이 둘을 더하면,

71
00:05:02,250 --> 00:05:09,080
새로운 토큰이 생성되어 트랜스포머 레이어로 들어갑니다.

72
00:05:09,080 --> 00:05:12,350
이 과정은 우리가

73
00:05:12,350 --> 00:05:15,020
지난주에 이야기한

74
00:05:15,020 --> 00:05:19,640
self-attention, layer

75
00:05:19,640 --> 00:05:24,680
norm, MLP 등과 동일하게 진행됩니다.

76
00:05:24,680 --> 00:05:29,000
그리고 출력 레이어가 우리를 위한 출력 벡터를

77
00:05:29,000 --> 00:05:29,615
생성합니다.

78
00:05:29,615 --> 00:05:32,280
이것은 어떤 응용 분야에도 사용할 수 있습니다.

79
00:05:32,280 --> 00:05:35,270
컴퓨터 비전에서 주요 응용 중

80
00:05:35,270 --> 00:05:36,960
하나는 분류입니다.

81
00:05:36,960 --> 00:05:39,120
우리는 이미지 분류부터 시작했습니다.

82
00:05:39,120 --> 00:05:42,800
그래서 이미지 분류에서는

83
00:05:42,800 --> 00:05:51,530
클래스에 대표성을 가지는 출력을 어떻게든 인코딩하거나

84
00:05:51,530 --> 00:05:54,960
생성하는 것이 중요해집니다.

85
00:05:54,960 --> 00:06:00,380
그래서 보통 하는 것은, 변환기(transformer)에

86
00:06:00,380 --> 00:06:05,730
동일한 차원의 특별한 추가

87
00:06:05,730 --> 00:06:09,440
입력 토큰을 하나 더 넣는데,

88
00:06:09,440 --> 00:06:13,670
이 토큰은 학습 가능한 파라미터로서

89
00:06:13,670 --> 00:06:19,590
출력 공간에서 클래스 확률 벡터로 변환됩니다.

90
00:06:19,590 --> 00:06:22,680
즉, C차원의 클래스 확률 벡터입니다.

91
00:06:22,680 --> 00:06:26,670
이것을 보통 클래스 토큰(class token)이라고 부릅니다.

92
00:06:26,670 --> 00:06:31,790
이것이 ViTs, 즉 비전 트랜스포머를

93
00:06:31,790 --> 00:06:35,360
사용한 이미지 분류의 기본적이고

94
00:06:35,360 --> 00:06:39,830
가장 표준적인 방법 중 하나입니다.

95
00:06:39,830 --> 00:06:42,810
하지만 트랜스포머는 분류뿐만

96
00:06:42,810 --> 00:06:45,200
아니라 오늘 다룰

97
00:06:45,200 --> 00:06:49,410
여러 다른 작업에도 사용될 수 있습니다.

98
00:06:49,410 --> 00:06:52,970
지난주에도 트랜스포머의 또 다른 변형에

99
00:06:52,970 --> 00:06:54,720
대해 이야기했습니다.

100
00:06:54,720 --> 00:06:56,250
다시 말해, 같은 토큰들을 사용합니다.

101
00:06:56,250 --> 00:07:03,090
그리고 토큰들로부터 트랜스포머 레이어를 거칩니다.

102
00:07:03,090 --> 00:07:05,600
지난번에 이야기했듯이,

103
00:07:05,600 --> 00:07:09,780
여러 층의 트랜스포머가 있습니다.

104
00:07:09,780 --> 00:07:13,890
말씀드렸듯이, 위치 임베딩(positional embeddings)이
추가됩니다.

105
00:07:13,890 --> 00:07:20,130
여기서는 전체 이미지를 한꺼번에 보기 때문에, 언어처럼

106
00:07:20,130 --> 00:07:22,370
미래 정보를 사용하지

107
00:07:22,370 --> 00:07:25,160
말아야 하는 시퀀스가

108
00:07:25,160 --> 00:07:29,490
아니어서 마스킹을 할 필요가 없습니다.

109
00:07:29,490 --> 00:07:33,650
결국 트랜스포머는 각

110
00:07:33,650 --> 00:07:40,680
입력에 대해 벡터 패치를 출력합니다.

111
00:07:40,680 --> 00:07:44,210
트랜스포머를 훈련하는 또 다른

112
00:07:44,210 --> 00:07:48,150
방법은 별도의 클래스 토큰을 두는 대신,

113
00:07:48,150 --> 00:07:51,300
출력들을 풀링 레이어에

114
00:07:51,300 --> 00:07:55,340
통과시켜 C개의 클래스에 대한 확률 벡터로

115
00:07:55,340 --> 00:07:57,360
만드는 것입니다.

116
00:07:57,360 --> 00:08:00,810
그래서 저는 두 가지 버전의 트랜스포머에 대해 이야기했습니다.

117
00:08:00,810 --> 00:08:03,540
그중 하나는 클래스 토큰을 사용하는 것이었습니다.

118
00:08:03,540 --> 00:08:07,920
다른 하나는 모든 출력 토큰을 가져오는 것이었죠.

119
00:08:07,920 --> 00:08:12,110
우리는 풀링과 프로젝션을 적용해서 클래스 확률을

120
00:08:12,110 --> 00:08:14,010
나타내는 벡터로 만듭니다.

121
00:08:14,010 --> 00:08:15,960
이걸 어떻게 감독하냐고요?

122
00:08:15,960 --> 00:08:19,920
이전에도 이야기했던 것과 정확히 같습니다.

123
00:08:19,920 --> 00:08:23,870
즉, 역전파, 손실 함수 정의, 이진 크로스

124
00:08:23,870 --> 00:08:27,200
엔트로피, 소프트 로그 맥스 손실

125
00:08:27,200 --> 00:08:29,450
등을 사용하는 겁니다.

126
00:08:29,450 --> 00:08:33,230
이것이 ViT입니다.

127
00:08:33,230 --> 00:08:35,103
간단히 말해 이것이 ViT입니다.

128
00:08:37,969 --> 00:08:41,840
수년 동안 이런 유형의 아키텍처는

129
00:08:41,840 --> 00:08:46,490
다양한 응용 분야에서 거의 변하지 않았습니다.

130
00:08:46,490 --> 00:08:53,300
현재 많은 최신 아키텍처들이 여기서 소개한 구성 요소들을 매우

131
00:08:53,300 --> 00:08:56,550
유사하게 사용하고 있습니다.

132
00:08:56,550 --> 00:09:00,320
하지만 지난주 슬라이드에서

133
00:09:00,320 --> 00:09:03,110
몇 가지 최적화가

134
00:09:03,110 --> 00:09:10,170
있었는데, 잠깐 몇 분만 그 부분을 다루겠습니다.

135
00:09:10,170 --> 00:09:12,290
여러분이 이해해야 할

136
00:09:12,290 --> 00:09:18,200
것은 더 나은 성능과 트랜스포머 훈련을 좀 더 안정적으로

137
00:09:18,200 --> 00:09:21,740
만드는 다양한 조정과 최적화가

138
00:09:21,740 --> 00:09:23,870
있다는 점입니다.

139
00:09:23,870 --> 00:09:28,710
그중 하나가 바로 잔차 연결(residual connections)입니다.

140
00:09:28,710 --> 00:09:33,900
이 레이어 노름(layer norm)은 기본적으로 잔차 연결 바깥에

141
00:09:33,900 --> 00:09:38,890
있습니다. 즉, 여기서 나오는 값을 정규화한다는 뜻입니다.

142
00:09:38,890 --> 00:09:42,860
이것이 더 이상 어떤 형태의 항등 함수(identity function)를

143
00:09:42,860 --> 00:09:46,190
복제할 수 없다는 의미는 아닙니다. 그것이 바로 잔차 연결이

144
00:09:46,190 --> 00:09:47,610
정말 하고 싶었던 겁니다.

145
00:09:47,610 --> 00:09:50,030
그래서 그 해결책은 layer

146
00:09:50,030 --> 00:09:53,160
normalization을 도입하는 것입니다.

147
00:09:53,160 --> 00:09:56,210
우리는 보통 self-attention 앞에 하나,

148
00:09:56,210 --> 00:09:59,370
그리고 두 번째는 MLP 레이어 앞에 넣습니다.

149
00:09:59,370 --> 00:10:01,320
그래서 normalization이 존재합니다.

150
00:10:01,320 --> 00:10:05,030
하지만 동시에 identity function도 유지합니다.

151
00:10:05,030 --> 00:10:09,180
다른 normalization 방법들도 있습니다.

152
00:10:09,180 --> 00:10:13,070
RMSNorm, root mean square

153
00:10:13,070 --> 00:10:19,820
normalization이 있는데, 이것은 사실 매우 기본적인
normalization 유형입니다.

154
00:10:19,820 --> 00:10:21,620
각 feature마다,

155
00:10:21,620 --> 00:10:25,590
normalization에 feature의 평균값을 사용하지 않습니다.

156
00:10:25,590 --> 00:10:29,610
하지만 이것이 훈련을 좀 더 안정적으로 만듭니다.

157
00:10:29,610 --> 00:10:38,390
다시 말하지만, 이 모든 방법들은 경험적으로 더 나은 선택임이
입증되었습니다.

158
00:10:38,390 --> 00:10:40,490
왜 잘 작동하는지에 대한 몇

159
00:10:40,490 --> 00:10:42,040
가지 근거도 있습니다.

160
00:10:42,040 --> 00:10:46,065
하지만 대부분은 이 방법들이

161
00:10:46,065 --> 00:10:53,290
훈련을 더 안정적으로 만든다는 사실 때문에 채택됩니다.

162
00:10:53,290 --> 00:10:58,380
다른 방법은 단순 MLP 대신 SwiGLU

163
00:10:58,380 --> 00:11:05,190
MLP를 사용하는 것입니다. 여기서는 gated

164
00:11:05,190 --> 00:11:08,530
nonlinearity를 적용합니다.

165
00:11:08,530 --> 00:11:13,720
W1과 W2 두 개의 가중치 행렬 대신 세 번째 행렬을

166
00:11:13,720 --> 00:11:16,750
추가합니다, 1, 2, 그리고 3입니다.

167
00:11:16,750 --> 00:11:22,710
여기서 gated nonlinearity를 만듭니다.

168
00:11:22,710 --> 00:11:36,390
기본적으로 이것은 더 많은 학습 가능한 파라미터를 얻는 것뿐만 아니라, 작은

169
00:11:36,390 --> 00:11:38,700
아키텍처에서 더

170
00:11:38,700 --> 00:11:42,210
나은 비선형성을 만드는

171
00:11:42,210 --> 00:11:43,750
것입니다.

172
00:11:43,750 --> 00:11:50,250
숨겨진 레이어 크기를 8 나누기 3으로

173
00:11:50,250 --> 00:11:53,790
선택해도, 파라미터 수

174
00:11:53,790 --> 00:11:56,200
측면에서 네트워크

175
00:11:56,200 --> 00:12:04,540
크기는 같지만, 그 레이어에서 더 고차원적인 비선형성을

176
00:12:04,540 --> 00:12:05,950
학습합니다.

177
00:12:05,950 --> 00:12:09,540
마지막으로 소개할 것은 mixture of

178
00:12:09,540 --> 00:12:13,590
experts로, 요즘 매우 현대적인 아키텍처에서도 자주

179
00:12:13,590 --> 00:12:14,530
사용됩니다.

180
00:12:14,530 --> 00:12:17,770
하나의 MLP 층 세트 대신에 여러 개의

181
00:12:17,770 --> 00:12:20,560
MLP 층 세트를 가질 수 있습니다.

182
00:12:20,560 --> 00:12:22,720
각각이 전문가(expert)가 되는 거죠.

183
00:12:22,720 --> 00:12:28,930
그리고 라우터를 통해 토큰들이 그 E개의

184
00:12:28,930 --> 00:12:34,630
전문가 중 A개에게 라우팅됩니다.

185
00:12:34,630 --> 00:12:40,360
이렇게 해서 실제로 A개의 활성 전문가가 존재하게 됩니다.

186
00:12:40,360 --> 00:12:43,110
하지만 이 방법은

187
00:12:43,110 --> 00:12:48,220
파라미터 수를 늘리면서도 계산량을

188
00:12:48,220 --> 00:12:55,200
크게 증가시키지 않고 더 견고한 모델 학습에

189
00:12:55,200 --> 00:12:57,850
도움을 줍니다.

190
00:12:57,850 --> 00:13:01,240
그리고 이들은 모두 병렬로 작동하는 MLP들입니다.

191
00:13:01,240 --> 00:13:05,290
그래서 여러 전문가를 병렬로 가질 수 있습니다.

192
00:13:05,290 --> 00:13:08,430
말씀드렸듯이, 요즘 모든 LLM,

193
00:13:08,430 --> 00:13:10,260
즉 대형 언어

194
00:13:10,260 --> 00:13:15,420
모델에서 이런 방식들이 사용되고 있으며, 우리가 아는 최신

195
00:13:15,420 --> 00:13:20,860
LLM들 모두 이런 종류의 개선 기법을 적용하고 있습니다.

196
00:13:20,860 --> 00:13:23,520
이것이 제가 방금 언급한 모든

197
00:13:23,520 --> 00:13:25,740
개선 기법들의 요약입니다.

198
00:13:25,740 --> 00:13:28,805
이것은 [? bias. ?]와 비슷합니다. 아닙니다.

199
00:13:28,805 --> 00:13:32,010
이것은 완전히 학습 가능한

200
00:13:32,010 --> 00:13:37,500
파라미터로, 피드포워드 네트워크나 단순 선형 투영을

201
00:13:37,500 --> 00:13:42,420
통해 확률 벡터로 변환하는 방식입니다.

202
00:13:42,420 --> 00:13:45,570
그래서 [?]가 아닙니다. 그냥 bias입니다. ?]

203
00:13:45,570 --> 00:13:47,550
그리고 다시 한 번,

204
00:13:47,550 --> 00:13:54,370
여기에는 아주 많은 self-attention 네트워크, 레이어가 있다는
것을 기억하세요.

205
00:13:54,370 --> 00:13:56,250
그리고 그 self-attention

206
00:13:56,250 --> 00:13:59,050
레이어들은 기본적으로 정보를 융합하고,

207
00:13:59,050 --> 00:14:02,190
모든 토큰과 이 클래스 토큰 사이에 어텐션을

208
00:14:02,190 --> 00:14:02,830
생성합니다.

209
00:14:02,830 --> 00:14:05,260
그래서 여기서 감독 신호를 주면,

210
00:14:05,260 --> 00:14:06,700
손실 함수가 작용합니다.

211
00:14:06,700 --> 00:14:13,860
이것이 클래스 확률 벡터를 나타낼 겁니다.

212
00:14:13,860 --> 00:14:16,800
그래서 질문은, 만약 좋은 직관이 있다면,

213
00:14:16,800 --> 00:14:19,920
서로 다른 전문가들이 무엇을 하는가 하는 거죠?

214
00:14:19,920 --> 00:14:21,930
좋은 질문입니다.

215
00:14:21,930 --> 00:14:24,930
이들은 병렬로 훈련되고,

216
00:14:24,930 --> 00:14:27,700
초기화도 다르기 때문에,

217
00:14:27,700 --> 00:14:33,630
종종 한 가지 측면이나 관련된, 때로는 매우 밀접한

218
00:14:33,630 --> 00:14:36,220
측면을 배우려고

219
00:14:36,220 --> 00:14:43,510
합니다. 하지만 단지 더 많은 계산과 파라미터를 추가해서 네트워크가

220
00:14:43,510 --> 00:14:47,760
여러 개념을 배워야 할 때 다른

221
00:14:47,760 --> 00:14:51,160
것들을 배우게 하는 겁니다.

222
00:14:51,160 --> 00:14:54,660
예를 들어, 여러 확률 분포를 다뤄야

223
00:14:54,660 --> 00:14:57,400
한다면, 이 MLP들을 통해

224
00:14:57,400 --> 00:15:02,260
데이터의 여러 모드를 분리할 수 있는 능력이 종종 있습니다.

225
00:15:02,260 --> 00:15:06,150
그래서 질문은, 전문가 수가

226
00:15:06,150 --> 00:15:09,370
하이퍼파라미터인가 아닌가 하는 거죠?

227
00:15:09,370 --> 00:15:11,800
네, 확실히 하이퍼파라미터입니다.

228
00:15:11,800 --> 00:15:15,780
제가 아는 바로는 보통 미리 정해져 있습니다.

229
00:15:15,780 --> 00:15:18,910
굳이 너무 세밀하게 튜닝하지는 않습니다.

230
00:15:18,910 --> 00:15:21,553
하지만 네, 모두 하이퍼파라미터입니다.

231
00:15:21,553 --> 00:15:22,470
또한 학습되기도 합니다.

232
00:15:22,470 --> 00:15:23,490
네.

233
00:15:23,490 --> 00:15:25,050
그리고 학습됩니다.

234
00:15:25,050 --> 00:15:30,450
그럼 왜 layer norm을 옮기는 것이 identity

235
00:15:30,450 --> 00:15:33,160
변환 학습에 도움이 될까요?

236
00:15:33,160 --> 00:15:35,110
이 아키텍처를 보세요.

237
00:15:35,110 --> 00:15:38,470
어떤 형태로든 identity를 만들 수 있나요?

238
00:15:38,470 --> 00:15:41,680
왜냐하면 바로 그 residual 연결 뒤에서 feature

239
00:15:41,680 --> 00:15:44,490
값들이 바뀌기 때문입니다, normalization이

240
00:15:44,490 --> 00:15:45,900
있기 때문에요.

241
00:15:45,900 --> 00:15:48,990
피처에서 절대 identity를 가질 수 없습니다.

242
00:15:48,990 --> 00:15:53,880
왜냐하면 바로 그 뒤에 layer normalization이 있기
때문입니다.

243
00:15:53,880 --> 00:15:57,570
그래서 우리가 하는 것은 그것을 안으로 가져오는 겁니다.

244
00:15:57,570 --> 00:16:04,750
컴퓨터 비전에는 꽤 다양한 작업들이 있습니다.

245
00:16:04,750 --> 00:16:09,990
이것들은 수년간 컴퓨터 비전 응용에서 핵심적이고

246
00:16:09,990 --> 00:16:13,360
가장 중요한 작업들이었습니다.

247
00:16:13,360 --> 00:16:16,900
물론 요즘은 훨씬 더 어려운 작업들을 해결하고 있습니다.

248
00:16:16,900 --> 00:16:19,320
그리고 이제는 객체 탐지에 아무도 신경 쓰지

249
00:16:19,320 --> 00:16:22,510
않는데, 왜냐하면 한 줄 코드로 할 수 있기 때문입니다.

250
00:16:22,510 --> 00:16:25,470
하지만 지난 10년, 15년 동안 많은

251
00:16:25,470 --> 00:16:27,000
발전이 있었습니다.

252
00:16:27,000 --> 00:16:31,230
오늘은 그 중 일부를 정말 다루고

253
00:16:31,230 --> 00:16:32,290
싶습니다.

254
00:16:32,290 --> 00:16:35,640
그래서 만약 여러분이 직접 새로운 것을 설계해야

255
00:16:35,640 --> 00:16:40,960
한다면, 어디를 보고 어떻게 모델을 설계할지 알 수 있도록 말이죠.

256
00:16:40,960 --> 00:16:43,230
그리고 궁극적으로, 시각화와

257
00:16:43,230 --> 00:16:47,310
이해라는 주제가 있는데, 이는 많은 응용

258
00:16:47,310 --> 00:16:49,510
분야에서 매우 중요합니다.

259
00:16:49,510 --> 00:16:52,360
예를 들어, 의료 데이터를 다룰 때는

260
00:16:52,360 --> 00:16:54,330
분류 자체나 종양 탐지보다

261
00:16:54,330 --> 00:16:57,150
시각화와 이해가 더 중요할 때가

262
00:16:57,150 --> 00:16:58,895
많습니다. 예를 들어, 어디에

263
00:16:58,895 --> 00:17:02,075
있고 왜 그런지 알고 싶을 때 말이죠.

264
00:17:06,480 --> 00:17:11,670
수업을 시작할 때, 이 슬라이드는 아마도 모두에게

265
00:17:11,670 --> 00:17:18,030
매우 익숙할 겁니다. 우리는 다양한 작업에 대해 이야기했습니다.

266
00:17:18,030 --> 00:17:23,290
그리고 객체 분류, 즉 분류 작업에 대해

267
00:17:23,290 --> 00:17:24,579
이야기했죠.

268
00:17:24,579 --> 00:17:29,610
처음 몇 강의 동안 이미지를 픽셀에서

269
00:17:29,610 --> 00:17:33,540
레이블로 분류하는 분류기를 어떻게

270
00:17:33,540 --> 00:17:37,990
만드는지 꽤 많은 시간을 보냈습니다.

271
00:17:37,990 --> 00:17:45,750
하지만 비슷하게 중요한 다른 작업 중 하나는 의미론적

272
00:17:45,750 --> 00:17:48,130
분할입니다.

273
00:17:48,130 --> 00:17:50,110
의미론적

274
00:17:50,110 --> 00:17:56,520
분할에서는 이미지 내 모든 픽셀에 레이블을

275
00:17:56,520 --> 00:18:01,110
할당하는 것이 중요합니다.

276
00:18:01,110 --> 00:18:08,235
각 픽셀을 그 객체나 장면 내 어떤 것에

277
00:18:08,235 --> 00:18:12,630
대한 레이블로 바꾸는 거죠.

278
00:18:12,630 --> 00:18:15,600
즉, 이런 모델을 훈련시키면 테스트

279
00:18:15,600 --> 00:18:19,890
시 이미지 하나를 입력으로 받아 같은 크기의 레이블

280
00:18:19,890 --> 00:18:23,130
맵을 출력으로 생성하기를 원합니다.

281
00:18:23,130 --> 00:18:24,910
어떻게 할 수 있을까요?

282
00:18:24,910 --> 00:18:26,650
여러 가지 방법이 있습니다.

283
00:18:26,650 --> 00:18:31,140
예를 들어, 각 픽셀마다 그

284
00:18:31,140 --> 00:18:36,330
픽셀의 값이나 레이블이 무엇인지

285
00:18:36,330 --> 00:18:40,360
판단하는 방법이 있죠.

286
00:18:40,360 --> 00:18:43,270
가장 기본적인 형태로 보면, 여기서

287
00:18:43,270 --> 00:18:46,120
보시다시피 사실상 불가능에 가깝습니다.

288
00:18:46,120 --> 00:18:52,650
픽셀 자체만 보면 그 픽셀이 어떤 특정 객체를

289
00:18:52,650 --> 00:18:57,360
나타내는지 알기 어렵기 때문입니다.

290
00:18:57,360 --> 00:19:00,430
맥락이 없으니까요.

291
00:19:00,430 --> 00:19:04,270
그래서 맥락이 중요합니다.

292
00:19:04,270 --> 00:19:06,900
주변 영역을 봐야 합니다.

293
00:19:09,475 --> 00:19:13,920
중심 픽셀과 주변 영역을 포함한 패치를 가져와서,

294
00:19:13,920 --> 00:19:16,270
이제 우리는 출력

295
00:19:16,270 --> 00:19:19,920
레이블을 생성하는 컨볼루션 신경망이나

296
00:19:19,920 --> 00:19:23,300
다른 네트워크를 훈련할 수 있습니다.

297
00:19:23,300 --> 00:19:25,170
이것은 이번 학기 동안

298
00:19:25,170 --> 00:19:28,090
이야기한 것과 같은 아키텍처입니다.

299
00:19:28,090 --> 00:19:30,660
이미지 분류에 사용했던 어떤 네트워크든

300
00:19:30,660 --> 00:19:32,500
선택할 수 있습니다.

301
00:19:32,500 --> 00:19:35,140
왜냐하면 이제 전체 이미지를 분류하는 것이니까요.

302
00:19:35,140 --> 00:19:38,250
CNN일 수도 있고, ResNet일 수도

303
00:19:38,250 --> 00:19:40,290
있고, ViT일 수도 있습니다.

304
00:19:40,290 --> 00:19:44,880
하지만 이 방법은 매우 시간이 많이 걸립니다. 이미지

305
00:19:44,880 --> 00:19:50,200
내 모든 픽셀마다 네트워크를 한 번씩 돌려야 하니까요.

306
00:19:50,200 --> 00:19:55,050
세그멘테이션 맵을 만드는 데 시간이 엄청 걸립니다.

307
00:19:55,050 --> 00:19:58,380
다른 방법은, 모든 픽셀마다

308
00:19:58,380 --> 00:20:05,080
네트워크를 돌리는 대신, 이미지를 입력으로 받아 전체

309
00:20:05,080 --> 00:20:07,860
픽셀 맵, 즉 세그멘테이션

310
00:20:07,860 --> 00:20:13,770
맵을 한 번에 출력하는 신경망을 훈련하는

311
00:20:13,770 --> 00:20:17,380
것입니다. 단일 레이블이 아니라

312
00:20:17,380 --> 00:20:20,640
레이블 행렬을 출력하는 거죠.

313
00:20:20,640 --> 00:20:28,150
이렇게 하면 세그멘테이션 작업을 해결할 수 있습니다.

314
00:20:28,150 --> 00:20:32,070
이를 위해서는 입력 레이어가

315
00:20:32,070 --> 00:20:36,960
이미지와 같은 크기여야 하고,

316
00:20:36,960 --> 00:20:39,720
출력도 어떤 식으로든 확장된

317
00:20:39,720 --> 00:20:42,220
레이어가 필요합니다.

318
00:20:42,220 --> 00:20:47,970
완전 연결층으로 가면 안 됩니다. 왜냐하면 이제 이미지를

319
00:20:47,970 --> 00:20:50,080
생성하는 것이니까요.

320
00:20:50,080 --> 00:20:57,695
그래서 네트워크를 확장된 상태로 유지해야 합니다.

321
00:20:57,695 --> 00:21:03,180
이것이 흔히 완전 컨볼루션 신경망, 즉

322
00:21:03,180 --> 00:21:06,600
FCN이라고 부르는 이유입니다.

323
00:21:06,600 --> 00:21:10,725
완전 컨볼루션 신경망은 확실히

324
00:21:10,725 --> 00:21:13,780
좋은 아이디어입니다.

325
00:21:13,780 --> 00:21:15,130
하지만 단점이 있습니다.

326
00:21:15,130 --> 00:21:16,240
문제가 있죠.

327
00:21:16,240 --> 00:21:18,100
이 이미지들은 크기가 큽니다.

328
00:21:18,100 --> 00:21:24,210
이 네트워크들, 이 레이어들은 매우 커질 것입니다.

329
00:21:24,210 --> 00:21:26,550
그리고 최적화해야 할 파라미터가

330
00:21:26,550 --> 00:21:28,620
너무 많아질 텐데,

331
00:21:28,620 --> 00:21:31,300
특히 초기에는 강력한 GPU가

332
00:21:31,300 --> 00:21:34,950
없어서 이것이 훈련 알고리즘의 병목 현상,

333
00:21:34,950 --> 00:21:37,210
문제, 도전 과제였습니다.

334
00:21:37,210 --> 00:21:40,980
그래서 알고리즘은 원본 크기의

335
00:21:40,980 --> 00:21:46,530
이미지에서 시작해 해상도를 점점 줄여가며 컨볼루션,

336
00:21:46,530 --> 00:21:49,680
즉 공간 해상도를

337
00:21:49,680 --> 00:21:53,790
다운샘플링 연산을 통해 점점 작게 만드는

338
00:21:53,790 --> 00:21:56,650
방향으로 발전했습니다.

339
00:21:56,650 --> 00:21:59,100
그리고 중간 어딘가에서는

340
00:21:59,100 --> 00:22:02,490
해상도는 낮지만 채널 수가

341
00:22:02,490 --> 00:22:05,165
두꺼운 상태가 됩니다.

342
00:22:05,165 --> 00:22:08,370
그다음에는 출력 픽셀을 만들기

343
00:22:08,370 --> 00:22:10,740
위해 다시 이미지

344
00:22:10,740 --> 00:22:13,800
크기와 같은 크기로 올라갑니다.

345
00:22:13,800 --> 00:22:19,860
그렇게 하기 위해 우리는 다운샘플링 방법을 알고 있습니다.

346
00:22:19,860 --> 00:22:21,250
다운샘플링은 쉬웠습니다.

347
00:22:21,250 --> 00:22:22,420
우리가 이미 이야기했죠.

348
00:22:22,420 --> 00:22:26,500
풀링 연산, 스트라이드 컨볼루션,

349
00:22:26,500 --> 00:22:35,460
그리고 여기서 사용할 수 있는 여러 다른 단계나 연산에

350
00:22:35,460 --> 00:22:37,980
대해 이야기했습니다.

351
00:22:37,980 --> 00:22:41,880
하지만 업샘플링 쪽은,

352
00:22:41,880 --> 00:22:46,770
풀링이나 업풀링의 역, 스트라이드

353
00:22:46,770 --> 00:22:55,135
컨볼루션의 역이 없기 때문에 어떻게 해야 할지 잘 몰랐습니다.

354
00:22:55,135 --> 00:22:57,990
그래서 다운샘플링을

355
00:22:57,990 --> 00:23:04,750
스스로 역전시키는 새로운 연산을 발명해야 했습니다.

356
00:23:04,750 --> 00:23:09,900
하지만 업샘플링을

357
00:23:09,900 --> 00:23:12,870
정의하기 전에,

358
00:23:12,870 --> 00:23:21,160
잠깐 질문을 드리고 싶습니다.

359
00:23:21,160 --> 00:23:24,100
이 네트워크가 어떻게 훈련된다고 생각하십니까?

360
00:23:24,100 --> 00:23:27,540
지금 우리는 이미지에서 시작해 이미지로 끝나는

361
00:23:27,540 --> 00:23:29,440
네트워크를 가지고 있습니다.

362
00:23:29,440 --> 00:23:33,570
그리고 이 네트워크를 훈련하기

363
00:23:33,570 --> 00:23:37,560
위한 도구는 손실 함수입니다.

364
00:23:37,560 --> 00:23:41,400
이 네트워크를 훈련하거나 손실 함수를 정의하는 가장

365
00:23:41,400 --> 00:23:44,370
좋은 방법은 무엇이라고 생각하십니까?

366
00:23:44,370 --> 00:23:47,130
우리는 softmax loss에 대해 이야기했습니다.

367
00:23:47,130 --> 00:23:51,300
또한 약간의 회귀 손실과 SVM 손실에

368
00:23:51,300 --> 00:23:53,170
대해서도 이야기했습니다.

369
00:23:53,170 --> 00:23:58,650
하지만 softmax loss 함수를 사용한다고 가정하면, 이것을 어떻게

370
00:23:58,650 --> 00:24:02,730
정의하거나 네트워크를 어떻게 학습시킬 수 있을까요?

371
00:24:02,730 --> 00:24:04,680
목표는 무엇일까요?

372
00:24:04,680 --> 00:24:09,670
각 픽셀에 대한 평균 분류 손실이라고 말씀하셨죠.

373
00:24:09,670 --> 00:24:11,950
맞습니다.

374
00:24:11,950 --> 00:24:16,690
모든 픽셀마다 손실 함수를 더할 수 있습니다. 왜냐하면

375
00:24:16,690 --> 00:24:20,200
각 픽셀이 분류 작업을 하기 때문입니다.

376
00:24:20,200 --> 00:24:24,370
그래서 이미지의 모든 픽셀에 대해 시그마를 취하게 됩니다.

377
00:24:24,370 --> 00:24:28,720
손실 함수는 단순한 softmax입니다.

378
00:24:28,720 --> 00:24:30,970
그리고 나서 역전파를 할 수 있습니다.

379
00:24:30,970 --> 00:24:33,810
이것이 필요한 전체 손실 함수입니다.

380
00:24:33,810 --> 00:24:37,680
질문은, 우리가 학습을 위해 'ground truths'가

381
00:24:37,680 --> 00:24:39,340
필요한가 하는 것입니다.

382
00:24:39,340 --> 00:24:41,920
이것이 바로 segmentation의 ground truths입니다.

383
00:24:41,920 --> 00:24:44,280
네, 이러한 유형의 알고리즘은 완전

384
00:24:44,280 --> 00:24:46,060
감독 학습이기 때문에,

385
00:24:46,060 --> 00:24:49,290
ground truths 레이블 맵이 필요합니다.

386
00:24:49,290 --> 00:24:52,470
초기에는 이러한 알고리즘을

387
00:24:52,470 --> 00:24:57,030
학습시키기 위해 픽셀을 수동으로 라벨링하는

388
00:24:57,030 --> 00:24:59,530
작업이 많이 있었습니다.

389
00:24:59,530 --> 00:25:00,720
네.

390
00:25:00,720 --> 00:25:04,180
요즘은 도구들이 있어서 그럴 필요가 없습니다.

391
00:25:04,180 --> 00:25:07,540
하지만 초창기에는 이 알고리즘들을 학습시키기 위해서 정답

392
00:25:07,540 --> 00:25:08,990
데이터가 필요했습니다.

393
00:25:13,530 --> 00:25:17,260
간단히 업샘플링에 대해 설명드리겠습니다.

394
00:25:17,260 --> 00:25:19,660
업샘플링은 사실 그렇게 어렵지 않습니다.

395
00:25:19,660 --> 00:25:22,720
언풀링 연산을 사용할 수 있습니다.

396
00:25:22,720 --> 00:25:24,730
여러 가지 방법이 있습니다.

397
00:25:24,730 --> 00:25:26,440
그중 하나가 최근접 이웃법입니다.

398
00:25:26,440 --> 00:25:32,080
예를 들어 2x2 행렬에서 4x4로 확장하고

399
00:25:32,080 --> 00:25:34,000
싶을 때입니다.

400
00:25:34,000 --> 00:25:39,070
각각에 대해 데이터를 복사하고, 저해상도 쪽에서 가장

401
00:25:39,070 --> 00:25:42,360
가까운 이웃을 선택하면 됩니다.

402
00:25:42,360 --> 00:25:45,300
또는 bed of nails 방식은

403
00:25:45,300 --> 00:25:48,810
업샘플된 버전에서 하나만 선택하는 겁니다.

404
00:25:48,810 --> 00:25:53,850
코너에 있는 하나만 선택하는 거죠.

405
00:25:53,850 --> 00:25:57,880
데이터를 복사하고 나머지는 모두 0으로 바꾸면 됩니다.

406
00:25:57,880 --> 00:26:00,240
그리고 여러 층의

407
00:26:00,240 --> 00:26:05,250
컨볼루션을 거치면서 이 값들이 나타나기 시작합니다.

408
00:26:05,250 --> 00:26:11,550
네트워크에서 인코딩 쪽에 max

409
00:26:11,550 --> 00:26:15,120
pooling을 사용하면,

410
00:26:15,120 --> 00:26:20,220
최대값이 선택된 위치를 저장하고,

411
00:26:20,220 --> 00:26:25,500
unpooling 단계에서

412
00:26:25,500 --> 00:26:34,930
그 최대값이 정의된 위치에 데이터를 복사할 수 있습니다.

413
00:26:34,930 --> 00:26:37,980
즉, 위치를 저장하는 겁니다.

414
00:26:37,980 --> 00:26:40,350
인코딩 부분에서 저장한

415
00:26:40,350 --> 00:26:46,950
좌표를 디코딩 부분의 업샘플링 단계에서 재사용합니다.

416
00:26:46,950 --> 00:26:51,750
다른 방법은 학습된 업샘플링을 하는 것입니다.

417
00:26:51,750 --> 00:26:55,660
제가 보여드린 모든 것들은 학습할 파라미터가 없습니다.

418
00:26:55,660 --> 00:26:57,370
그냥 연산일 뿐입니다.

419
00:26:57,370 --> 00:27:01,440
하지만 학습된 업샘플링도 가능합니다.

420
00:27:01,440 --> 00:27:05,040
아주 간단하게, 컨볼루션을 다시 살펴보겠습니다.

421
00:27:05,040 --> 00:27:07,260
컨볼루션 레이어에서는

422
00:27:07,260 --> 00:27:10,860
픽셀에 컨볼루션 필터를 적용해서

423
00:27:10,860 --> 00:27:13,860
출력을 만들고, 이 과정을

424
00:27:13,860 --> 00:27:16,620
모든 픽셀에 반복했습니다.

425
00:27:16,620 --> 00:27:20,400
다운샘플링을 할 때는 스트라이드

426
00:27:20,400 --> 00:27:23,610
컨볼루션을 사용했는데,

427
00:27:23,610 --> 00:27:27,220
1씩 이동하는 대신 2씩

428
00:27:27,220 --> 00:27:31,720
이동하면서 출력을 단계별로 생성했습니다.

429
00:27:31,720 --> 00:27:34,888
이 부분이 기억나지 않으면 강의로 돌아가서 확인하세요.

430
00:27:34,888 --> 00:27:35,680
우리가 다뤘던 내용입니다.

431
00:27:35,680 --> 00:27:36,638
3번째 강의였던 것 같네요.

432
00:27:39,275 --> 00:27:44,590
그리고 같은 방식을 업샘플링 과정에도 적용할 수 있습니다.

433
00:27:44,590 --> 00:27:51,400
이 부분은 업샘플된 이미지에서 이 영역을 나타냅니다.

434
00:27:51,400 --> 00:27:53,670
그리고 여기에 가중치를

435
00:27:53,670 --> 00:27:57,550
정의해서 출력 맵으로 매핑합니다.

436
00:27:57,550 --> 00:27:59,830
다음 것도 마찬가지지만, 겹치는

437
00:27:59,830 --> 00:28:01,480
부분이 생깁니다.

438
00:28:01,480 --> 00:28:04,840
겹치는 부분은 보통 값들을 더합니다.

439
00:28:04,840 --> 00:28:08,320
예를 들어, 출력값들을 더하는 식입니다.

440
00:28:08,320 --> 00:28:09,940
예를 들어 설명해 드리겠습니다.

441
00:28:09,940 --> 00:28:14,680
간단한 1D 함수로 설명하겠습니다.

442
00:28:14,680 --> 00:28:18,810
입력이 A와 B 두 값이라면, 우리는

443
00:28:18,810 --> 00:28:24,450
이 필터가 더 높은 해상도 출력으로 매핑하도록

444
00:28:24,450 --> 00:28:26,206
학습합니다.

445
00:28:26,206 --> 00:28:30,000
그렇게 하기 위해 각 값에

446
00:28:30,000 --> 00:28:34,980
필터를 적용하고 출력을 여기에 씁니다.

447
00:28:34,980 --> 00:28:38,550
겹치는 부분은 합산하는 겁니다.

448
00:28:38,550 --> 00:28:46,306
두 위치에서 오는 값을 더하는 거죠.

449
00:28:49,380 --> 00:28:58,575
앞서 완전 합성곱 신경망에 대해 이야기했고, 그것들이 어떻게 사용되는지

450
00:28:58,575 --> 00:29:00,160
설명했습니다.

451
00:29:00,160 --> 00:29:07,500
이것들은 사실 분할(segmentation)에 가장 기본적이고

452
00:29:07,500 --> 00:29:11,670
널리 쓰이는 알고리즘들입니다.

453
00:29:11,670 --> 00:29:15,300
그리고 빠르게 널리 사용되는

454
00:29:15,300 --> 00:29:21,040
네트워크 유닛 하나를 강조하고 싶습니다.

455
00:29:21,040 --> 00:29:23,700
보시다시피 U자 형태인데, 여기서

456
00:29:23,700 --> 00:29:26,020
보여드린 것과 같은 구조입니다.

457
00:29:26,020 --> 00:29:31,900
U자 형태로 그려보겠습니다.

458
00:29:31,900 --> 00:29:35,520
이걸 강조하는 이유는,

459
00:29:35,520 --> 00:29:40,770
오늘날에도 의료 분야에서 분할 작업에 이

460
00:29:40,770 --> 00:29:44,560
유닛이나 변형된 버전이 최첨단

461
00:29:44,560 --> 00:29:53,040
결과를 내기 때문입니다, 만약 어떤 foundation model을

462
00:29:53,040 --> 00:29:56,490
사용하지 않는다면요.

463
00:29:56,490 --> 00:30:01,080
이 유닛이 하는 일은 우리가 설명한 것과 정확히 같습니다.

464
00:30:01,080 --> 00:30:05,340
시야를 넓히고 공간 정보를 일부

465
00:30:05,340 --> 00:30:08,380
잃는 다운샘플링 단계와,

466
00:30:08,380 --> 00:30:12,570
다시 이미지 해상도로 돌아가는 업샘플링

467
00:30:12,570 --> 00:30:14,560
단계가 있죠.

468
00:30:14,560 --> 00:30:16,440
U-Net의

469
00:30:16,440 --> 00:30:20,370
유일한 차이점은 분할에 쓰이기

470
00:30:20,370 --> 00:30:24,480
때문에 디코더 쪽에서

471
00:30:24,480 --> 00:30:32,230
공간 정보를 유지해야 한다는 점입니다, 다운샘플링 시

472
00:30:32,230 --> 00:30:35,500
해상도를 잃기 때문이고,

473
00:30:35,500 --> 00:30:38,770
업샘플링 시 정보가 없으면

474
00:30:38,770 --> 00:30:40,930
어려움이 있기 때문입니다.

475
00:30:40,930 --> 00:30:45,610
경계가 흐려지는 경우가 종종 있습니다.

476
00:30:45,610 --> 00:30:53,220
인코더 쪽의 피처 맵이 디코더 레이어의 입력으로 그대로

477
00:30:53,220 --> 00:30:58,570
복사되지 않도록 하기 위해서입니다.

478
00:30:58,570 --> 00:31:03,900
이렇게 하면 이미지 내의 구조 정보를 유지하면서

479
00:31:03,900 --> 00:31:06,600
훨씬 선명한 출력을

480
00:31:06,600 --> 00:31:09,070
생성할 수 있습니다.

481
00:31:09,070 --> 00:31:14,050
이것이 U-Net의 아이디어였고, 말씀드렸듯이

482
00:31:14,050 --> 00:31:17,560
실제로 꽤 자주 사용됩니다.

483
00:31:17,560 --> 00:31:21,390
오늘 이야기한 의미론적 분할의

484
00:31:21,390 --> 00:31:24,840
요약입니다—완전 합성곱 신경망에 대해

485
00:31:24,840 --> 00:31:26,580
이야기했습니다.

486
00:31:26,580 --> 00:31:32,010
여기서 다운샘플링에 사용했던 것과 같은

487
00:31:32,010 --> 00:31:34,100
필터를 사용합니다.

488
00:31:42,780 --> 00:31:45,630
시간을 절약하기 위해 이 부분에서 일부 슬라이드를

489
00:31:45,630 --> 00:31:46,690
제거했습니다.

490
00:31:46,690 --> 00:31:48,370
그 슬라이드는 뒷부분에 있습니다.

491
00:31:48,370 --> 00:31:49,510
한번 확인해 보시기 바랍니다.

492
00:31:49,510 --> 00:31:51,330
이것은 역전파입니다.

493
00:31:51,330 --> 00:31:54,130
이것은 변환된 전치 합성곱입니다.

494
00:31:54,130 --> 00:31:57,220
여기 3x3 행렬이 있습니다.

495
00:31:57,220 --> 00:32:03,040
그리고 입력 이미지나 데이터를 합성곱하는

496
00:32:03,040 --> 00:32:05,530
대신, 입력의 전치

497
00:32:05,530 --> 00:32:09,410
버전에 합성곱을 적용합니다.

498
00:32:09,410 --> 00:32:12,590
그러면 실제로 더 큰 출력을 생성합니다.

499
00:32:12,590 --> 00:32:16,690
이것이 전치 합성곱입니다.

500
00:32:16,690 --> 00:32:20,510
일반 합성곱의 역과 같은 것입니다.

501
00:32:20,510 --> 00:32:21,760
그런데 왜 전치된 건가요?

502
00:32:21,760 --> 00:32:25,850
추가 슬라이드를 한번 참고해 보시길 권합니다.

503
00:32:25,850 --> 00:32:28,640
그래서 질문은, 필터가 학습되나요?

504
00:32:28,640 --> 00:32:29,140
네.

505
00:32:29,140 --> 00:32:31,338
다른 컨볼루션 레이어와 매우 비슷합니다.

506
00:32:31,338 --> 00:32:32,630
모든 필터가 학습됩니다.

507
00:32:32,630 --> 00:32:33,100
네.

508
00:32:33,100 --> 00:32:33,600
네.

509
00:32:38,860 --> 00:32:40,120
좋습니다.

510
00:32:40,120 --> 00:32:45,920
이것이 바로 의미론적 분할(semantic segmentation)의
주제였습니다.

511
00:32:45,920 --> 00:32:51,860
그리고 우리가 이야기했듯이, 우리는 픽셀에 대한 레이블만 얻습니다.

512
00:32:51,860 --> 00:32:55,570
하지만 같은 객체가 두 개 있으면, 어느

513
00:32:55,570 --> 00:32:58,885
것이 어느 것인지 알 수 없습니다.

514
00:32:58,885 --> 00:33:07,960
왜냐하면 이것은 단지 픽셀 레이블을 생성하거나 출력하는 것이기 때문입니다.

515
00:33:07,960 --> 00:33:13,040
그리고 이것이 인스턴스 분할(instance segmentation)

516
00:33:13,040 --> 00:33:18,410
주제로 이어집니다. 여기서는 픽셀 클래스뿐만

517
00:33:18,410 --> 00:33:23,050
아니라, 이 픽셀들이 한 개의 개 인스턴스에 속한다는

518
00:33:23,050 --> 00:33:24,800
것도 알아야 합니다.

519
00:33:24,800 --> 00:33:29,442
그리고 다음 것은 사실 다른 개입니다.

520
00:33:29,442 --> 00:33:37,430
이를 위해 필요한 것은 이미지 내 여러 객체를 이해하는 것이고, 이것이 객체

521
00:33:37,430 --> 00:33:41,200
검출(object detection)

522
00:33:41,200 --> 00:33:43,430
주제로 이어집니다.

523
00:33:43,430 --> 00:33:49,030
객체 검출은 이미지 분류 다음으로,

524
00:33:49,030 --> 00:33:51,280
또는 이미지

525
00:33:51,280 --> 00:33:58,020
분류와 함께 컴퓨터 비전의 핵심 문제 중

526
00:33:58,020 --> 00:33:59,350
하나였습니다.

527
00:33:59,350 --> 00:34:03,240
수년간 객체 검출 작업만을

528
00:34:03,240 --> 00:34:09,520
위한 다양한 알고리즘이 제안되었습니다.

529
00:34:09,520 --> 00:34:11,940
몇 가지를 간략히 훑어보고

530
00:34:11,940 --> 00:34:15,040
중요한 것 몇 개를 강조할 것입니다.

531
00:34:15,040 --> 00:34:20,280
하지만 다시 말하지만, 딥러닝 문헌에도

532
00:34:20,280 --> 00:34:22,590
너무 많은 연구가

533
00:34:22,590 --> 00:34:30,239
있어서 여기서 다 다루지 못합니다, 지난 10~15년간 말이죠.

534
00:34:30,239 --> 00:34:35,170
어떻게 하면 객체 검출 문제를 해결할 수 있을까요?

535
00:34:35,170 --> 00:34:40,330
만약 단일 객체라면, 분류를

536
00:34:40,330 --> 00:34:42,810
수행하고 클래스 점수를

537
00:34:42,810 --> 00:34:46,739
생성하며, 바운딩 박스

538
00:34:46,739 --> 00:34:50,350
좌표도 얻어야 합니다.

539
00:34:50,350 --> 00:34:52,650
즉, 박스의 좌표 x, y,

540
00:34:52,650 --> 00:34:58,360
높이 h, 너비 w를 출력으로 얻고, 어떤 클래스인지도 알아야 합니다.

541
00:34:58,360 --> 00:35:01,630
이것이 바로 객체 검출의 과제입니다.

542
00:35:01,630 --> 00:35:03,190
어떻게 해결할 수 있을까요?

543
00:35:03,190 --> 00:35:04,440
매우 간단합니다.

544
00:35:04,440 --> 00:35:11,410
클래스 점수에 대해 소프트맥스 손실 함수를 정의할 수 있습니다.

545
00:35:11,410 --> 00:35:15,210
그리고 박스 좌표에 대해 L2 손실

546
00:35:15,210 --> 00:35:20,910
함수, 즉 단순 거리 측정 회귀 손실을 정의할 수

547
00:35:20,910 --> 00:35:21,790
있습니다.

548
00:35:21,790 --> 00:35:27,580
이 두 가지를 정의하면, 다중 작업 손실(multi-task loss)이
됩니다.

549
00:35:27,580 --> 00:35:30,640
두 가지 작업을 동시에 해결하는 거죠.

550
00:35:30,640 --> 00:35:34,110
이를 위해 손실 값을

551
00:35:34,110 --> 00:35:40,420
더해 복합 손실 함수를 만듭니다,

552
00:35:40,420 --> 00:35:43,920
여기서 보시는 것처럼요.

553
00:35:43,920 --> 00:35:46,480
이것이 간단한 방법입니다.

554
00:35:46,480 --> 00:35:47,760
이건 할 수 있습니다.

555
00:35:47,760 --> 00:35:54,780
만약 단일 객체가 있다면, 제가 말씀드린 이 아키텍처를 사용해서

556
00:35:54,780 --> 00:35:56,880
문제를 확실히 해결할 수

557
00:35:56,880 --> 00:35:58,060
있습니다.

558
00:35:58,060 --> 00:36:00,420
하지만 장면에 여러 객체가

559
00:36:00,420 --> 00:36:02,620
있다면 그렇게 쉽지 않습니다.

560
00:36:02,620 --> 00:36:07,270
세 개의 객체가 있다면, 12개의 출력 숫자를 생성해야 합니다.

561
00:36:07,270 --> 00:36:09,540
더 많으면 생성해야

562
00:36:09,540 --> 00:36:12,820
할 숫자가 너무 많아집니다.

563
00:36:12,820 --> 00:36:15,995
그래서 이 알고리즘은 확장성이 좋지 않습니다.

564
00:36:15,995 --> 00:36:17,970
단순히 분류를 객체

565
00:36:17,970 --> 00:36:20,560
탐지로 확장한 것인데, 괜찮긴

566
00:36:20,560 --> 00:36:23,040
하지만 확장성은 떨어집니다.

567
00:36:23,040 --> 00:36:28,270
그래서 여러 객체가 있을 때 한 가지

568
00:36:28,270 --> 00:36:35,310
해결책은 전체 이미지를 입력으로 받는 대신 바운딩

569
00:36:35,310 --> 00:36:38,970
박스를 보는 것입니다.

570
00:36:38,970 --> 00:36:44,800
각 바운딩 박스마다 하나의 레이블만 가지고, 고양이인지

571
00:36:44,800 --> 00:36:49,470
개인지 배경인지 판단할 수 있습니다.

572
00:36:49,470 --> 00:36:56,460
이렇게 각 바운딩 박스를 분류할 수 있다면, 슬라이딩

573
00:36:56,460 --> 00:36:59,140
윈도우를 할 수 있습니다.

574
00:36:59,140 --> 00:37:01,060
0,0 좌표부터

575
00:37:01,060 --> 00:37:06,300
모든 x, y, 높이, 너비 조합에 대해 바운딩 박스를

576
00:37:06,300 --> 00:37:10,740
만들어 이미지를 슬라이딩하며 객체를

577
00:37:10,740 --> 00:37:12,670
탐지할 수 있습니다.

578
00:37:12,670 --> 00:37:16,080
단계별로 최대 확률을

579
00:37:16,080 --> 00:37:18,090
가진 바운딩

580
00:37:18,090 --> 00:37:23,070
박스를 찾아낼 수 있습니다.

581
00:37:23,070 --> 00:37:26,140
하지만 여기에는 큰 문제가 있습니다.

582
00:37:26,140 --> 00:37:29,400
다시 말하지만, 사용할 수 있는 바운딩

583
00:37:29,400 --> 00:37:31,740
박스 조합이 너무 많습니다.

584
00:37:31,740 --> 00:37:35,820
그래서 이 알고리즘도 확장성이 없습니다.

585
00:37:35,820 --> 00:37:39,875
문헌에서 우리가 해온 것은,

586
00:37:39,875 --> 00:37:43,660
다시 말해 초기 연구들,

587
00:37:43,660 --> 00:37:46,230
2014년

588
00:37:46,230 --> 00:37:52,300
이전에 발표된 논문들을 보면, 객체가

589
00:37:52,300 --> 00:37:58,750
있을 확률이 높은 영역을 찾는 연구가

590
00:37:58,750 --> 00:38:00,560
많았습니다.

591
00:38:00,560 --> 00:38:02,540
그래서 region proposals입니다.

592
00:38:02,540 --> 00:38:07,190
만약 region proposals를 찾는 방법이

593
00:38:07,190 --> 00:38:10,940
있다면, 그건 사실 꽤 쉬운 문제입니다.

594
00:38:10,940 --> 00:38:14,740
앞서 설명한 것과 같은 방법을 사용할 수 있습니다.

595
00:38:14,740 --> 00:38:19,490
이미지에 대해 region proposals가 있다면,

596
00:38:19,490 --> 00:38:23,800
그 부분, 그 패치를 잘라내서 CNN, 즉

597
00:38:23,800 --> 00:38:29,620
convolutional neural network를 그 패치에

598
00:38:29,620 --> 00:38:31,820
적용하고 분류할 수 있습니다.

599
00:38:31,820 --> 00:38:33,460
그리고 심지어

600
00:38:33,460 --> 00:38:37,810
bounding box도 더 정교하게 조정할 수 있습니다.

601
00:38:37,810 --> 00:38:44,680
그래서 분류하고 bounding box를 조정해서

602
00:38:44,680 --> 00:38:48,620
객체를 검출할 수 있습니다.

603
00:38:48,620 --> 00:38:51,610
좌표를 조금 수정해야 한다면, 박스를

604
00:38:51,610 --> 00:38:53,470
분류하고 bounding

605
00:38:53,470 --> 00:38:55,840
box도 조정할 수 있습니다.

606
00:38:55,840 --> 00:39:01,450
이것이 바로 R-CNN 알고리즘입니다.

607
00:39:01,450 --> 00:39:09,010
그리고 작동은 하지만, 다시 말해, 이것은 2014년까지의

608
00:39:09,010 --> 00:39:12,860
초기 알고리즘 중 하나입니다.

609
00:39:12,860 --> 00:39:17,120
이 방법들은 매우 느린데, 왜냐하면 각 박스마다 다시

610
00:39:17,120 --> 00:39:20,920
전체 컨볼루션 신경망을 실행하기 때문입니다.

611
00:39:20,920 --> 00:39:26,500
하지만 한 가지 요점이 있는데, 각

612
00:39:26,500 --> 00:39:34,780
박스마다 컨볼루션 신경망을 실행하는 대신에, 컨볼루션

613
00:39:34,780 --> 00:39:39,500
연산은 공간 정보를 보존하기

614
00:39:39,500 --> 00:39:41,710
때문입니다.

615
00:39:41,710 --> 00:39:45,020
컨볼루션 연산은 다운샘플링이나 업샘플링을 합니다.

616
00:39:45,020 --> 00:39:50,230
그래서 우리는 항상 픽셀 공간에서 그 위치를 추적할 방법이

617
00:39:50,230 --> 00:39:51,115
있습니다.

618
00:39:51,115 --> 00:39:56,380
따라서 이 경우, 패치마다 컨볼루션

619
00:39:56,380 --> 00:40:02,920
신경망을 실행하는 대신, 전체 이미지에 대해 하나의

620
00:40:02,920 --> 00:40:06,680
큰 컨볼루션을 실행합니다.

621
00:40:06,680 --> 00:40:09,460
그러면 이제 전체 이미지에 대응하는

622
00:40:09,460 --> 00:40:11,900
특징 맵 내의 영역들이 생깁니다.

623
00:40:11,900 --> 00:40:13,990
그 영역들을 살펴보겠습니다.

624
00:40:13,990 --> 00:40:18,040
이제, 그 위에 더 작은 CNN을

625
00:40:18,040 --> 00:40:24,490
실행해서 제가 원하는 두 출력 각각에 대한 결과를

626
00:40:24,490 --> 00:40:25,370
생성합니다.

627
00:40:25,370 --> 00:40:27,580
먼저, 박스 오프셋입니다.

628
00:40:27,580 --> 00:40:30,010
경계 상자를 조금 이동시켜야 할까요?

629
00:40:30,010 --> 00:40:34,070
아니면 객체 카테고리가 무엇인지요?

630
00:40:34,070 --> 00:40:37,700
이것이 바로 fast R-CNN의 버전입니다. 이것들은

631
00:40:37,700 --> 00:40:40,600
우리가 convolutional

632
00:40:40,600 --> 00:40:44,570
neural networks를 사용해 객체와 경계 상자 등을

633
00:40:44,570 --> 00:40:46,390
감지하는 기본 알고리즘입니다.

634
00:40:46,390 --> 00:40:49,480
질문은, 제안된 영역의 수가 미리

635
00:40:49,480 --> 00:40:51,760
정해져 있느냐는 것입니다.

636
00:40:51,760 --> 00:40:53,720
간단한 대답은 그렇다는 겁니다.

637
00:40:53,720 --> 00:40:57,360
region proposal networks가 하는 일을 아주 간단히
설명하겠습니다.

638
00:41:00,550 --> 00:41:03,260
간단한 알고리즘입니다.

639
00:41:03,260 --> 00:41:07,360
하나는 제안된 영역의 경계 상자를 이미지 아래에

640
00:41:07,360 --> 00:41:08,530
놓습니다.

641
00:41:08,530 --> 00:41:12,650
다른 하나는 ConvNet의 feature maps 위에 놓습니다.

642
00:41:12,650 --> 00:41:16,460
그리고 둘 다 출력 클래스 레이블과

643
00:41:16,460 --> 00:41:21,160
감지된 객체 위치를 개선하기 위한

644
00:41:21,160 --> 00:41:22,960
오프셋을 생성합니다.

645
00:41:22,960 --> 00:41:26,620
하지만 이것은 먼저 경계 상자

646
00:41:26,620 --> 00:41:35,000
영역 제안을 해야 하고, 이미지에서 어디를 봐야 하는지 알려주는 region

647
00:41:35,000 --> 00:41:37,720
proposal

648
00:41:37,720 --> 00:41:40,970
network가 필요하다는 뜻입니다.

649
00:41:40,970 --> 00:41:46,480
region proposal networks, 즉 RPN을 만드는

650
00:41:46,480 --> 00:41:47,980
연구가 있었습니다.

651
00:41:47,980 --> 00:41:53,900
여기서는 그냥 CNN을 무작위로 시작합니다.

652
00:41:53,900 --> 00:41:59,200
이미지의 여러 위치에서 무작위로 시작해

653
00:41:59,200 --> 00:42:00,130
보죠.

654
00:42:00,130 --> 00:42:03,250
그리고 convolution

655
00:42:03,250 --> 00:42:09,370
레이어를 거치면서 객체가 있을 확률이 높은 영역을 점점 정제합니다.

656
00:42:09,370 --> 00:42:12,190
왜냐하면 객체 레이블과 위치를

657
00:42:12,190 --> 00:42:14,600
알고 있기 때문입니다.

658
00:42:14,600 --> 00:42:17,210
그래서 최적화하고 감독할 수 있습니다.

659
00:42:17,210 --> 00:42:21,500
그리고 각각의 영역은 박스 좌표도 정제합니다.

660
00:42:21,500 --> 00:42:26,920
기본적으로 region proposal

661
00:42:26,920 --> 00:42:32,770
network는 객체가 있을 확률이 높은 각

662
00:42:32,770 --> 00:42:36,010
박스를 정제하는 역할을

663
00:42:36,010 --> 00:42:42,160
합니다, 즉 출력 박스의 위치를 보정하는 거죠.

664
00:42:42,160 --> 00:42:43,340
박스 보정입니다.

665
00:42:43,340 --> 00:42:47,260
좌표나 차원에 관한 세부사항은

666
00:42:47,260 --> 00:42:50,110
너무 시간이 걸리니

667
00:42:50,110 --> 00:42:52,930
나중에 직접 확인하시길

668
00:42:52,930 --> 00:42:54,200
바랍니다.

669
00:42:54,200 --> 00:42:57,140
이 알고리즘에 너무 많은 시간을 쓰고 싶지 않아서요.

670
00:42:57,140 --> 00:43:01,160
하지만 중요한 점은, 질문으로 돌아가서,

671
00:43:01,160 --> 00:43:05,560
우리는 보통 객체가 있을 확률이

672
00:43:05,560 --> 00:43:08,380
가장 높은 상위 K개를 이

673
00:43:08,380 --> 00:43:12,320
이미지의 제안으로 사용한다는 겁니다.

674
00:43:12,320 --> 00:43:18,230
이 이미지는 단순해서 객체가 하나뿐입니다.

675
00:43:18,230 --> 00:43:21,730
그래서 대부분의 영역이 그 단일 객체를 중심으로

676
00:43:21,730 --> 00:43:23,480
형성되어 있습니다.

677
00:43:23,480 --> 00:43:25,280
하지만 일반적으로는 그렇지 않습니다.

678
00:43:25,280 --> 00:43:29,590
그래서 많은 설정에서 region proposal을

679
00:43:29,590 --> 00:43:33,640
다양한 방식으로 사용할 수 있습니다.

680
00:43:33,640 --> 00:43:37,870
더 높은 확률을 가진 다양한 객체들을 얻을 수 있죠.

681
00:43:37,870 --> 00:43:42,040
이제 R-CNN과 mask R-CNN에

682
00:43:42,040 --> 00:43:46,970
대해 조금 이야기했는데, 여러분에게는 세부사항을 꼼꼼히

683
00:43:46,970 --> 00:43:49,940
살펴보는 것이 중요합니다.

684
00:43:49,940 --> 00:43:55,640
그리고 직접 계산을 해보는 시간을 가지면

685
00:43:55,640 --> 00:43:57,410
매우 좋습니다.

686
00:43:57,410 --> 00:44:01,110
하지만 R-CNN, mask R-CNN 같은

687
00:44:01,110 --> 00:44:04,570
알고리즘은 요즘에는 계산량이 너무 많아서

688
00:44:04,570 --> 00:44:07,300
더 이상 많이 사용되지 않습니다.

689
00:44:07,300 --> 00:44:09,700
이 지점에 도달한 과정을

690
00:44:09,700 --> 00:44:11,960
이해하는 것이 중요합니다만,

691
00:44:11,960 --> 00:44:15,860
하지만 그것들은 여러 가지 이유 때문입니다.

692
00:44:15,860 --> 00:44:19,510
그 이유 중 하나는 두 개의 별도 네트워크가 필요하다는

693
00:44:19,510 --> 00:44:25,180
점입니다. 하나는 region proposal network이고, 다른
하나는 분류 및 박스

694
00:44:25,180 --> 00:44:26,480
정제 네트워크입니다.

695
00:44:26,480 --> 00:44:32,176
그래서 각 이미지마다 최소 두 번의 패스를 거쳐

696
00:44:32,176 --> 00:44:35,080
객체를 탐지하는 셈입니다.

697
00:44:35,080 --> 00:44:38,950
그래서 단일 단계 객체 탐지기인

698
00:44:38,950 --> 00:44:45,800
SSD 이후로 발전이 있었습니다. 가장 인기

699
00:44:45,800 --> 00:44:50,080
있는 것 중 하나가 YOLO입니다.

700
00:44:50,080 --> 00:44:55,090
YOLO는 아마도 컴퓨터 비전 문제를

701
00:44:55,090 --> 00:44:58,880
다룬다면 지금까지도 들어봤을 겁니다.

702
00:44:58,880 --> 00:45:04,090
비록 오늘날에는 컨볼루션이 많이 사용된 네트워크지만,

703
00:45:04,090 --> 00:45:07,450
적어도 초기 버전에서는 그렇습니다.

704
00:45:07,450 --> 00:45:10,520
많은 산업 현장에서도

705
00:45:10,520 --> 00:45:16,240
YOLO는 빠른 객체 탐지기로서 객체 탐지의

706
00:45:16,240 --> 00:45:19,220
기반으로 사용되고 있습니다.

707
00:45:19,220 --> 00:45:25,100
그리고 객체 탐지 성능도 매우 뛰어납니다.

708
00:45:25,100 --> 00:45:30,040
YOLO가 하는 일을 간단히

709
00:45:30,040 --> 00:45:31,130
설명드리겠습니다.

710
00:45:31,130 --> 00:45:34,270
기본적으로 이미지를 한 번만 한 패스로

711
00:45:34,270 --> 00:45:35,150
봅니다.

712
00:45:35,150 --> 00:45:38,750
모든 바운딩 박스를 생성합니다.

713
00:45:38,750 --> 00:45:48,640
어떻게 하냐면, 이미지를 S 곱하기 S 그리드로 나눕니다. 이

714
00:45:48,640 --> 00:45:52,430
예에서는 7 곱하기 7입니다.

715
00:45:52,430 --> 00:45:56,930
각 그리드 박스마다, 그

716
00:45:56,930 --> 00:46:02,260
위치에 객체가 있을 확률과 바운딩

717
00:46:02,260 --> 00:46:07,660
박스의 정제를 출력하는 완전

718
00:46:07,660 --> 00:46:12,770
컨볼루션 네트워크를 만듭니다.

719
00:46:12,770 --> 00:46:16,510
그래서 B개의 바운딩 박스, 즉 객체가

720
00:46:16,510 --> 00:46:20,740
있는 박스의 정제된 B개의 바운딩

721
00:46:20,740 --> 00:46:22,370
박스를 생성합니다.

722
00:46:22,370 --> 00:46:27,640
또한 객체 클래스 확률도

723
00:46:27,640 --> 00:46:28,760
생성합니다.

724
00:46:28,760 --> 00:46:33,560
이 경우 예를 들어 B가 2라면, 서로 다른

725
00:46:33,560 --> 00:46:35,860
확률을 가진 두 개의

726
00:46:35,860 --> 00:46:38,380
바운딩 박스만 생성합니다.

727
00:46:38,380 --> 00:46:43,580
이 작업을 모든 박스에 동시에 수행합니다.

728
00:46:43,580 --> 00:46:45,910
기본적으로 각 바운딩

729
00:46:45,910 --> 00:46:49,900
박스마다 출력을 생성하는

730
00:46:49,900 --> 00:46:52,790
동일한 네트워크입니다.

731
00:46:52,790 --> 00:46:59,662
그리고 객체에 대해 여러 가지 다른 옵션을

732
00:46:59,662 --> 00:47:00,950
생성합니다.

733
00:47:00,950 --> 00:47:03,490
말씀드렸듯이, 각 박스는

734
00:47:03,490 --> 00:47:05,660
확률과 연관되어 있습니다.

735
00:47:05,660 --> 00:47:07,600
이 예에서는 각

736
00:47:07,600 --> 00:47:13,640
박스의 확률이 엣지의 가중치로 표시되어 있습니다.

737
00:47:13,640 --> 00:47:17,470
이렇게 많은 바운딩 박스와 객체 확률에

738
00:47:17,470 --> 00:47:21,050
대해 임계값 처리를 할 수 있습니다.

739
00:47:21,050 --> 00:47:29,300
또한 논문에서 사용하는 알고리즘이 있습니다.

740
00:47:29,300 --> 00:47:31,750
자세한 내용은 다루지

741
00:47:31,750 --> 00:47:37,570
않겠지만, non-maximum suppression과 임계값

742
00:47:37,570 --> 00:47:41,800
처리를 포함한 알고리즘으로 가장 높은 확률을

743
00:47:41,800 --> 00:47:44,450
가진 박스를 식별합니다.

744
00:47:44,450 --> 00:47:48,460
이것이 객체 검출의

745
00:47:48,460 --> 00:47:55,550
간단한 구현 또는 사용 예입니다.

746
00:47:55,550 --> 00:47:58,930
다시 말하지만, 매우 유용한 방법입니다.

747
00:47:58,930 --> 00:48:02,360
시간이 있다면 YOLO 저장소를 살펴보시길 권합니다.

748
00:48:02,360 --> 00:48:05,800
의료, 로봇공학, 그리고 다양한

749
00:48:05,800 --> 00:48:10,730
산업 분야에서 사용되는 최신 YOLO

750
00:48:10,730 --> 00:48:13,490
버전이 매우 많습니다.

751
00:48:13,490 --> 00:48:16,900
그렇다면 두 번째 이미지는 어떻게 얻는 걸까요?

752
00:48:16,900 --> 00:48:18,870
그리고 그 직관은 무엇일까요?

753
00:48:18,870 --> 00:48:20,680
말씀드렸듯이, 각

754
00:48:20,680 --> 00:48:23,560
그리드마다 바운딩 박스를 생성합니다.

755
00:48:23,560 --> 00:48:27,310
이 경우에는 두 개를 생성했고, 다른 모든 경우에도

756
00:48:27,310 --> 00:48:28,900
두 개를 생성합니다.

757
00:48:28,900 --> 00:48:32,980
이 bee는 다시 말해 확률 벡터이고, 각

758
00:48:32,980 --> 00:48:36,670
박스는 그 안에 객체가 존재할 확률과

759
00:48:36,670 --> 00:48:37,970
연관되어 있습니다.

760
00:48:37,970 --> 00:48:41,500
그리고 모든 패치에 대해 이들을 모두 합치면

761
00:48:41,500 --> 00:48:43,390
박스가 아주 많아집니다.

762
00:48:43,390 --> 00:48:47,300
그리고 이제 각 박스는 확률과 연관되어 있습니다.

763
00:48:51,654 --> 00:48:54,765
완벽합니다. 다음으로

764
00:48:55,630 --> 00:48:56,595
넘어가겠습니다.

765
00:49:00,340 --> 00:49:05,980
최근 객체 탐지 방법 중 하나는 detection

766
00:49:05,980 --> 00:49:09,500
transformer입니다.

767
00:49:09,500 --> 00:49:15,070
이것은 순수하게 transformers에 기반하며,

768
00:49:15,070 --> 00:49:17,240
지난주에 다룬 주제입니다.

769
00:49:17,240 --> 00:49:21,100
그리고 오늘 시작한 것처럼, 동일한 종류의

770
00:49:21,100 --> 00:49:25,390
self-attention과

771
00:49:25,390 --> 00:49:30,880
cross-attention 모듈도 객체 탐지와 바운딩 박스를 생성할

772
00:49:30,880 --> 00:49:32,080
수 있습니다.

773
00:49:32,080 --> 00:49:33,920
이것이 어떻게 작동하는지요?

774
00:49:33,920 --> 00:49:36,640
이 논문은 사실 그렇게 오래되지 않았고,

775
00:49:36,640 --> 00:49:39,230
2020년, 거의 5년 전입니다.

776
00:49:39,230 --> 00:49:41,930
비록 지금은 더 이상 사용되지 않습니다.

777
00:49:41,930 --> 00:49:45,430
실제 응용에서는 아무도 사용하지 않지만,

778
00:49:45,430 --> 00:49:48,340
transformers를 객체

779
00:49:48,340 --> 00:49:51,440
탐지에 활용하는 좋은 예시입니다.

780
00:49:51,440 --> 00:49:55,600
여기서 하는 것은 기본적으로 이전에 설명한

781
00:49:55,600 --> 00:49:57,260
것과 비슷합니다.

782
00:49:57,260 --> 00:50:01,280
이미지를 패치로 나누고,

783
00:50:01,280 --> 00:50:04,480
그 패치들을 CNN에 통과시켜

784
00:50:04,480 --> 00:50:06,530
토큰을 만듭니다.

785
00:50:06,530 --> 00:50:09,490
그다음에 제가 패치에 대해 설명한 것과 같은

786
00:50:09,490 --> 00:50:12,710
방식으로 positional encoding을 추가합니다.

787
00:50:12,710 --> 00:50:17,260
그리고 그것들이 입력 토큰을 정의하는데, 이 입력 토큰들은

788
00:50:17,260 --> 00:50:22,250
transformer encoder에 들어가는 입력입니다.
transformer

789
00:50:22,250 --> 00:50:25,570
encoder는 다시 말해 여러 개의 self-attention,

790
00:50:25,570 --> 00:50:28,040
layer normalization

791
00:50:28,040 --> 00:50:31,720
또는 다른 normalization, 그리고

792
00:50:31,720 --> 00:50:38,860
MLP 레이어들로 구성되어 여러 층의 transformer encoder를
거친 후 출력 토큰을 생성합니다.

793
00:50:38,860 --> 00:50:42,950
그다음 바운딩 박스를 생성하기 위해, 이

794
00:50:42,950 --> 00:50:45,220
알고리즘의 스마트한

795
00:50:45,220 --> 00:50:49,900
부분인데, transformer decoder는

796
00:50:49,900 --> 00:50:52,640
encoder 출력 토큰을

797
00:50:52,640 --> 00:50:57,250
입력으로 받지만, 동시에 쿼리도 정의합니다. 이

798
00:50:57,250 --> 00:50:59,800
쿼리는 학습 가능한 파라미터들로,

799
00:50:59,800 --> 00:51:05,830
예를 들어 5개의 쿼리를 입력으로 넣으면 최대 5개의

800
00:51:05,830 --> 00:51:08,900
객체를, 10개나 20개 쿼리를

801
00:51:08,900 --> 00:51:14,840
넣으면 최대 20개의 객체를 이미지에서 탐지하려는 겁니다.

802
00:51:14,840 --> 00:51:17,500
그리고 다시 transformer

803
00:51:17,500 --> 00:51:24,230
decoder의 시작 부분에서 self-attention

804
00:51:24,230 --> 00:51:30,860
레이어들과 encoder 출력과의 cross-attention 조합을

805
00:51:30,860 --> 00:51:32,780
통해 진행됩니다.

806
00:51:32,780 --> 00:51:39,235
그래서 cross-attention과 self-attention

807
00:51:39,235 --> 00:51:44,600
네트워크 레이어들이 각 쿼리에 대한 출력 값을

808
00:51:44,600 --> 00:51:48,980
생성하고, 이 출력 값들은 FNN(feed

809
00:51:48,980 --> 00:51:56,670
forward network)을 거쳐 클래스 레이블과 바운딩 박스를

810
00:51:56,670 --> 00:52:01,760
생성합니다. 이전에 논의한 것과 매우 유사하며,

811
00:52:01,760 --> 00:52:05,150
경우에 따라서는 탐지할 객체가 없다고

812
00:52:05,150 --> 00:52:07,495
판단하기도 합니다.

813
00:52:07,495 --> 00:52:10,640
마지막으로 바운딩 박스와

814
00:52:10,640 --> 00:52:16,190
바운딩 박스에 연관된 클래스가 출력으로 나옵니다.

815
00:52:16,190 --> 00:52:19,428
그럼 질문은, transformer에 모든 가능한 박스를 입력하느냐

816
00:52:19,428 --> 00:52:20,220
하는 건데요,

817
00:52:20,220 --> 00:52:20,930
아닙니다.

818
00:52:20,930 --> 00:52:23,500
여기 입력은 삼각형

819
00:52:23,500 --> 00:52:30,370
파라미터 같은 쿼리들로, 실제로는 이 입력 쿼리

820
00:52:30,370 --> 00:52:34,345
대신에 객체가 존재하기를 원하는

821
00:52:34,345 --> 00:52:37,280
질문을 나타냅니다.

822
00:52:37,280 --> 00:52:40,840
그래서 입력으로 박스나 그런 게 있는 게 아닙니다.

823
00:52:40,840 --> 00:52:43,480
출력의 일부로 클래스 레이블과

824
00:52:43,480 --> 00:52:46,780
박스 좌표를 생성하는 겁니다.

825
00:52:46,780 --> 00:52:51,520
그럼 질문은, 쿼리가 우리가 찾고자 하는

826
00:52:51,520 --> 00:52:56,740
것과 이미지 내 위치를 실제로 나타내도록 형성되었느냐

827
00:52:56,740 --> 00:52:58,480
하는 건데,

828
00:52:58,480 --> 00:53:02,410
이 경우 우리가 찾는 것은 미리

829
00:53:02,410 --> 00:53:06,670
정의된 클래스 레이블로 정의되며, 이들은

830
00:53:06,670 --> 00:53:08,810
출력의 일부입니다.

831
00:53:08,810 --> 00:53:10,970
그래서 우리의 감독 신호는 클래스 레이블에 기반합니다.

832
00:53:10,970 --> 00:53:13,140
클래스 확률 벡터를 가지고 있습니다.

833
00:53:13,140 --> 00:53:15,920
다른 알고리즘에서 정의한 것과 같은 방식입니다.

834
00:53:15,920 --> 00:53:20,650
그래서 알고리즘이 어떤 종류의 클래스를 찾아야 하는지 아는

835
00:53:20,650 --> 00:53:21,650
방법입니다.

836
00:53:21,650 --> 00:53:25,390
그리고 출력에 관해서도 다시 말하지만, 이

837
00:53:25,390 --> 00:53:28,310
출력들은 감독 학습을 받습니다.

838
00:53:28,310 --> 00:53:31,090
기억하신다면, L2 노름, 즉

839
00:53:31,090 --> 00:53:35,110
실제 박스들의 L2 손실을 기반으로 합니다.

840
00:53:35,110 --> 00:53:41,500
쿼리 부분에서는 어떤 객체를 어디서 찾아야

841
00:53:41,500 --> 00:53:45,740
하는지 전혀 알려주지 않습니다.

842
00:53:45,740 --> 00:53:49,450
학습 과정 자체는 역전파를 수행합니다.

843
00:53:49,450 --> 00:53:52,310
손실이나 오류가 있으면

844
00:53:52,310 --> 00:53:55,640
출력으로부터 역전파가 이루어집니다.

845
00:53:55,640 --> 00:54:00,340
즉, 처음이나 이 부분에서 아무것도

846
00:54:00,340 --> 00:54:02,920
결정하지 않습니다.

847
00:54:02,920 --> 00:54:09,040
질문은 쿼리가 최대 아홉 개의 객체를 주는 것이냐는 것이었죠.

848
00:54:09,040 --> 00:54:14,510
네, 기본적으로 그 의미입니다.

849
00:54:14,510 --> 00:54:19,010
그리고 self-attention과

850
00:54:19,010 --> 00:54:22,600
cross-attention을 통해, FNN

851
00:54:22,600 --> 00:54:25,960
연산으로 클래스와 박스 좌표로 변환되는

852
00:54:25,960 --> 00:54:29,380
출력 토큰을 생성하려고 합니다.

853
00:54:29,380 --> 00:54:33,640
질문은 쿼리가 이미지 패치인지

854
00:54:33,640 --> 00:54:35,900
아닌지였죠?

855
00:54:35,900 --> 00:54:38,480
아니요, 이미지 패치가 아닙니다.

856
00:54:38,480 --> 00:54:44,270
그냥 학습 가능한 파라미터를 위한 쿼리일 뿐입니다.

857
00:54:44,270 --> 00:54:47,200
출력을 생성하기 위해 넣는 것입니다.

858
00:54:47,200 --> 00:54:51,850
각 입력마다 값을 출력으로 얻고,

859
00:54:51,850 --> 00:54:55,810
그 값이 클래스와 박스 좌표로 변환됩니다.

860
00:54:55,810 --> 00:54:58,130
다시 질문입니다, object queries가 무엇인가요?

861
00:54:58,130 --> 00:55:00,980
이것들은 학습 가능한 파라미터입니다.

862
00:55:00,980 --> 00:55:02,980
그래서 초기화합니다.

863
00:55:02,980 --> 00:55:06,550
네트워크가 그 값들을 최적화합니다.

864
00:55:06,550 --> 00:55:09,010
그리고 그것이 출력으로 나오는 것입니다.

865
00:55:09,010 --> 00:55:12,040
질문은, 어떤 FNN이 어떤 박스를

866
00:55:12,040 --> 00:55:14,710
잡는지에 대한 직관이 있나요?

867
00:55:14,710 --> 00:55:19,720
간단한 답은, 아니요, 네트워크가

868
00:55:19,720 --> 00:55:22,280
여러 개를

869
00:55:22,280 --> 00:55:24,940
생성하는 것을

870
00:55:24,940 --> 00:55:28,430
막는 것은 없습니다.

871
00:55:28,430 --> 00:55:32,320
하지만 기억하세요, 그곳에는 수많은

872
00:55:32,320 --> 00:55:34,930
self-attention과

873
00:55:34,930 --> 00:55:38,875
cross-attention 레이어가 있어서 서로 상호작용하며

874
00:55:38,875 --> 00:55:41,380
각 쿼리가 출력 레이어의

875
00:55:41,380 --> 00:55:43,730
하나와 매칭되도록 합니다.

876
00:55:43,730 --> 00:55:46,880
그래서 출력과 정확히 같은 것을 생성하지는 않습니다.

877
00:55:46,880 --> 00:55:52,930
그리고 우리는 그 FNN들을 감독하는 것도 제어할 수 있습니다.

878
00:55:52,930 --> 00:55:57,580
그래서 질문은, 훈련에 이미지 분할, 즉

879
00:55:57,580 --> 00:56:00,490
픽셀 단위 분할이 포함되나요?

880
00:56:00,490 --> 00:56:08,040
이 알고리즘은 픽셀 단위 분할을 필요로 하지 않습니다.

881
00:56:08,040 --> 00:56:11,930
오직 클래스 레이블과 바운딩 박스만으로 감독됩니다.

882
00:56:11,930 --> 00:56:15,410
하지만 픽셀 단위 분할이 있다면,

883
00:56:15,410 --> 00:56:17,560
그것을 바운딩 박스로

884
00:56:17,560 --> 00:56:20,980
변환해 이 알고리즘을 훈련시킬

885
00:56:20,980 --> 00:56:24,130
수는 있지만 필수는 아닙니다.

886
00:56:24,130 --> 00:56:26,860
그래서 질문은, 본 적 없는

887
00:56:26,860 --> 00:56:29,890
객체를 일반화할 수 있나요?

888
00:56:29,890 --> 00:56:32,545
여기서 본 적 없다는 것은 새로운 클래스 레이블을 의미하나요?

889
00:56:32,545 --> 00:56:34,360
네.

890
00:56:34,360 --> 00:56:39,170
이러한 완전 감독 알고리즘의 경우, 클래스 확률 벡터를 생성하기

891
00:56:39,170 --> 00:56:41,320
때문에 종종 새로운 클래스를

892
00:56:41,320 --> 00:56:43,040
추가할 방법이 없습니다.

893
00:56:43,040 --> 00:56:45,100
이전에 다른 클래스가

894
00:56:45,100 --> 00:56:52,225
있다는 것을 알지 못하면 새로운 클래스를 끝에 추가할

895
00:56:52,225 --> 00:56:53,810
방법이 없습니다.

896
00:56:53,810 --> 00:56:58,010
그래서 완전 감독 네트워크는 종종 새로운 객체를 다루지 않습니다.

897
00:56:58,010 --> 00:57:00,320
배경 객체나 객체가 없음을 가질 수 있습니다.

898
00:57:00,320 --> 00:57:03,080
보시다시피, 객체가 없다는 레이블도 있습니다.

899
00:57:03,080 --> 00:57:07,030
하지만 제로샷 학습에 사용되는 이러한 유형의

900
00:57:07,030 --> 00:57:08,620
알고리즘과 혼합 확장

901
00:57:08,620 --> 00:57:10,930
알고리즘이 많이 있습니다.

902
00:57:10,930 --> 00:57:13,990
제로샷은 훈련 데이터에 예제가 없더라도

903
00:57:13,990 --> 00:57:16,940
새로운 것을 이해하고 찾는 것을 의미합니다.

904
00:57:16,940 --> 00:57:19,640
하지만 이 주제는 넘어가겠습니다.

905
00:57:19,640 --> 00:57:25,150
장면에 쿼리로 넣은 것보다 더 많은 객체가

906
00:57:25,150 --> 00:57:27,910
있으면 어떻게 될까요?

907
00:57:27,910 --> 00:57:29,900
좋은 질문입니다.

908
00:57:29,900 --> 00:57:34,300
보통 가장 높은 신뢰도를 가진 객체, 즉 신뢰도가

909
00:57:34,300 --> 00:57:36,490
가장 높은 바운딩

910
00:57:36,490 --> 00:57:38,030
박스를 생성합니다.

911
00:57:38,030 --> 00:57:40,240
그럴 때는 더 많은 객체를

912
00:57:40,240 --> 00:57:44,220
얻기 위해 쿼리를 추가하는 것이 좋습니다.

913
00:57:47,050 --> 00:57:48,580
수업 후에 질문이 있으면

914
00:57:48,580 --> 00:57:50,840
여기서 답변해 드리겠습니다.

915
00:57:50,840 --> 00:57:54,890
하지만 다뤄야 할 다른 주제가 많아서 꼭

916
00:57:54,890 --> 00:57:57,080
다루고 넘어가고 싶습니다.

917
00:57:57,080 --> 00:58:01,250
적어도 주제에 익숙해지실 수 있도록요.

918
00:58:01,250 --> 00:58:04,750
객체 검출과 관련해, 앞서 받은

919
00:58:04,750 --> 00:58:07,250
질문으로 돌아가서, 이런 유형의

920
00:58:07,250 --> 00:58:10,240
알고리즘을 인스턴스 세분화에

921
00:58:10,240 --> 00:58:13,130
어떻게 사용할 수 있을까요?

922
00:58:13,130 --> 00:58:15,140
사실 그렇게 어렵지 않습니다.

923
00:58:15,140 --> 00:58:18,640
R-CNN 알고리즘에 대해

924
00:58:18,640 --> 00:58:24,440
이야기할 때 설명했듯이, 이미지에 CNN을 적용합니다.

925
00:58:24,440 --> 00:58:27,430
그다음 지역 제안 네트워크가 바운딩

926
00:58:27,430 --> 00:58:29,450
박스를 제공합니다.

927
00:58:29,450 --> 00:58:35,560
그리고 이 바운딩 박스는 클래스 레이블과 바운딩

928
00:58:35,560 --> 00:58:38,750
박스 보정으로 변환됩니다.

929
00:58:38,750 --> 00:58:43,970
지금까지 R-CNN 등에 대해 이야기한 내용입니다.

930
00:58:43,970 --> 00:58:46,840
이제 이것을 마스크 R-CNN으로

931
00:58:46,840 --> 00:58:49,960
바꿔서 마스크도 생성할 수 있습니다.

932
00:58:49,960 --> 00:58:54,680
기본적으로 앞서 이야기한 것과 같은 아키텍처입니다.

933
00:58:54,680 --> 00:59:01,450
출력을 하나 더 추가해 멀티태스크로 만들어 마스크

934
00:59:01,450 --> 00:59:04,700
예측을 생성하는 거죠.

935
00:59:04,700 --> 00:59:07,570
이전에는 이미지, 지역

936
00:59:07,570 --> 00:59:12,880
제안, CNN이 클래스 레이블과 박스 좌표를

937
00:59:12,880 --> 00:59:14,840
제공했습니다.

938
00:59:14,840 --> 00:59:18,910
이제는 그 객체의 픽셀 수준

939
00:59:18,910 --> 00:59:26,360
마스크를 생성하는 또 다른 합성곱 층을 추가합니다.

940
00:59:26,360 --> 00:59:28,990
그리고 그 마스크는 입력

941
00:59:28,990 --> 00:59:35,450
이미지와 같은 크기일 수 있으며, 기본적으로 층 자체에서 생성됩니다.

942
00:59:35,450 --> 00:59:37,580
완전 합성곱 신경망을 사용하면

943
00:59:37,580 --> 00:59:40,000
보통 이런 출력이 나옵니다.

944
00:59:40,000 --> 00:59:44,390
작은 박스가 있을 때마다 각 객체에 대해

945
00:59:44,390 --> 00:59:47,630
마스크를 얻을 수 있습니다.

946
00:59:47,630 --> 00:59:52,430
박스 설정이 다르면 의자, 침대,

947
00:59:52,430 --> 00:59:58,460
사람, 아기 등 이미지 내 객체별로 마스크를

948
00:59:58,460 --> 01:00:00,740
얻을 수 있습니다.

949
01:00:00,740 --> 01:00:04,700
이것이 R-CNN 알고리즘의 확장판인

950
01:00:04,700 --> 01:00:07,420
마스크 R-CNN입니다.

951
01:00:07,420 --> 01:00:14,380
그리고 mask R-CNN을 사용하면, 우리가 알고리즘을

952
01:00:14,380 --> 01:00:21,850
학습시킬 수 있는 다양한 객체들을 매우 잘 감지할

953
01:00:21,850 --> 01:00:23,780
수 있습니다.

954
01:00:23,780 --> 01:00:29,920
그리고 탐색할 수 있는 많은 API와 오픈

955
01:00:29,920 --> 01:00:34,670
소스 객체 탐지기들이 있습니다.

956
01:00:34,670 --> 01:00:37,460
여기에 몇 가지 링크와 자료가

957
01:00:37,460 --> 01:00:40,870
있지만, 이 모든 것은 기본적으로

958
01:00:40,870 --> 01:00:45,727
우리가 다루고자 했던 작업들을 요약한 것입니다.

959
01:00:45,727 --> 01:00:47,560
이 작업들을 이해하는 것이

960
01:00:47,560 --> 01:00:49,190
실제로 매우 중요합니다.

961
01:00:49,190 --> 01:00:52,400
이 작업들은 컴퓨터 비전의 핵심 작업들이었습니다.

962
01:00:52,400 --> 01:00:56,570
비록 요즘 컴퓨터 비전은 훨씬 더 발전했지만요.

963
01:00:56,570 --> 01:00:59,270
이 작업들에만 국한되지는 않습니다.

964
01:00:59,270 --> 01:01:03,430
하지만 예를 들어 산업 현장에서

965
01:01:03,430 --> 01:01:09,310
썩은 토마토와 좋은 토마토를 구분하는 품질

966
01:01:09,310 --> 01:01:12,380
관리 같은 응용이 있다면,

967
01:01:12,380 --> 01:01:14,050
컴퓨터 비전으로

968
01:01:14,050 --> 01:01:15,880
객체를 감지하고 좋은

969
01:01:15,880 --> 01:01:19,780
것과 나쁜 것으로 분류할 수 있어야 합니다.

970
01:01:19,780 --> 01:01:22,540
그래서 이러한 단계와 파이프라인을 실시간으로

971
01:01:22,540 --> 01:01:26,510
수행하는 방법을 이해하고 아는 것이 여전히 중요합니다.

972
01:01:26,510 --> 01:01:29,170
하지만 이제는 여러분 모두가 익숙한

973
01:01:29,170 --> 01:01:31,660
더 큰 규모의 모델들이 있습니다.

974
01:01:31,660 --> 01:01:36,340
이것이 제가 이야기하고자 했던 첫 번째 부분, 컴퓨터 비전 작업들을

975
01:01:36,340 --> 01:01:37,610
요약한 내용입니다.

976
01:01:37,610 --> 01:01:40,480
그리고 제가 10분 정도

977
01:01:40,480 --> 01:01:46,330
할애하고 싶은 마지막 부분은 시각화와 이해에 관한 것입니다.

978
01:01:46,330 --> 01:01:50,320
이 주제도 자체로 큰 강의가 될 수 있습니다.

979
01:01:50,320 --> 01:01:57,040
2014년, 2013년, 심지어 2020년대까지

980
01:01:57,040 --> 01:02:01,810
신경망 시각화 주제는 매우 뜨거웠고,

981
01:02:01,810 --> 01:02:06,755
네트워크가 무엇을 학습하는지

982
01:02:09,840 --> 01:02:12,560
이해하는 데 큰 도움이

983
01:02:12,560 --> 01:02:14,040
되었습니다.

984
01:02:14,040 --> 01:02:16,500
여기서는 여러분이 응용에

985
01:02:16,500 --> 01:02:20,285
사용할 수 있는 가장 중요한 몇

986
01:02:20,285 --> 01:02:22,470
가지를 요약하겠습니다.

987
01:02:22,470 --> 01:02:26,660
하지만 그 전에, 우리가 이야기했던 선형

988
01:02:26,660 --> 01:02:28,760
분류기로 돌아가 보겠습니다.

989
01:02:28,760 --> 01:02:33,240
우리는 선형 분류기에 대해 꽤 많은 시간을 썼습니다.

990
01:02:33,240 --> 01:02:37,710
선형 분류기에서는, 네트워크가 학습하는

991
01:02:37,710 --> 01:02:43,100
선형 함수를 보면 각 클래스에 대한

992
01:02:43,100 --> 01:02:46,472
템플릿을 가질 수 있다고 했죠.

993
01:02:46,472 --> 01:02:47,930
예를 들어, 이 자동차에

994
01:02:47,930 --> 01:02:52,310
대해 항상 정면을 향한 자동차를 템플릿으로 볼 수 있습니다.

995
01:02:52,310 --> 01:02:55,100
신경망에서도 같은 것을 할 수 있습니다.

996
01:02:55,100 --> 01:02:57,630
필터 중 하나를 시각화하면,

997
01:02:57,630 --> 01:03:01,740
여기서는 선형 함수의 가중치를 시각화했습니다.

998
01:03:01,740 --> 01:03:03,300
이것이 시각적 관점이었죠.

999
01:03:03,300 --> 01:03:06,140
신경망의 필터를

1000
01:03:06,140 --> 01:03:11,700
시각화하는 것도 마찬가지입니다.

1001
01:03:11,700 --> 01:03:15,080
각 필터에 대해 네트워크는

1002
01:03:15,080 --> 01:03:20,510
기본적인 형태, 방향, 또는 간단한 모양

1003
01:03:20,510 --> 01:03:25,980
같은 것을 학습하고 있습니다, 여기서

1004
01:03:25,980 --> 01:03:28,040
볼 수 있듯이요.

1005
01:03:28,040 --> 01:03:32,720
이 시각화는 채널 수가 적은 층에서만 할 수

1006
01:03:32,720 --> 01:03:33,950
있습니다.

1007
01:03:33,950 --> 01:03:36,150
예를 들어, 채널이 3개라면

1008
01:03:36,150 --> 01:03:39,330
RGB 이미지로 만들어 시각화할 수 있죠.

1009
01:03:39,330 --> 01:03:42,690
하지만 기억하시겠지만, CNN에서는 그렇지 않았습니다.

1010
01:03:42,690 --> 01:03:50,420
CNN에서는 중간 층에 꽤 많은 채널이 있어서, 우리가 볼 수

1011
01:03:50,420 --> 01:03:53,660
있는 형태로 시각화하기 쉽지

1012
01:03:53,660 --> 01:03:55,050
않습니다.

1013
01:03:55,050 --> 01:03:57,770
하지만 기본적으로 여러분이 보는 것은 이렇습니다.

1014
01:03:57,770 --> 01:04:00,570
채널 수가 적은 초기 층에서는 시각화가 가능하며,

1015
01:04:00,570 --> 01:04:03,590
네트워크가 실제로 어떤 패턴을 학습하고 있음을

1016
01:04:03,590 --> 01:04:04,800
볼 수 있습니다.

1017
01:04:04,800 --> 01:04:06,920
즉, 네트워크가 패턴을 학습하기 시작하는 거죠.

1018
01:04:06,920 --> 01:04:14,960
그리고 나중 단계에서는 더 전체적이고 큰 패턴이 나타납니다. 만약

1019
01:04:14,960 --> 01:04:21,200
우리가 guided back propagation이라는

1020
01:04:21,200 --> 01:04:26,130
것을 실행한다면, 그것들도 시각화할 수

1021
01:04:26,130 --> 01:04:30,560
있지만, 이만큼 간단하지는 않습니다.

1022
01:04:30,560 --> 01:04:34,310
신경망을 이해하고 시각화하는 몇

1023
01:04:34,310 --> 01:04:39,350
가지 방법을 강조하고 싶습니다. 이것들은 실제로

1024
01:04:39,350 --> 01:04:41,190
매우 중요합니다.

1025
01:04:41,190 --> 01:04:47,060
첫 번째는 saliency 개념입니다.

1026
01:04:47,060 --> 01:04:50,330
많은 응용에서 어떤 픽셀이 중요한지

1027
01:04:50,330 --> 01:04:53,070
아는 것이 매우 중요합니다.

1028
01:04:53,070 --> 01:04:55,200
예를 들어, 의료

1029
01:04:55,200 --> 01:04:58,890
응용에서 종양과 비종양을 분류할 때,

1030
01:04:58,890 --> 01:05:01,550
이미지의 어느 부분이 실제로

1031
01:05:01,550 --> 01:05:03,540
종양인지 보고 싶습니다.

1032
01:05:03,540 --> 01:05:05,850
자동화를 하려면, 종양이

1033
01:05:05,850 --> 01:05:09,090
있는지 없는지 아는 것보다,

1034
01:05:09,090 --> 01:05:12,500
모두가 관심 있는 것은 이미지에서 종양이 어디에 있는지입니다.

1035
01:05:12,500 --> 01:05:17,210
이를 위해 가장 간단한 방법은 네트워크, 즉

1036
01:05:17,210 --> 01:05:21,890
feedforward 신경망을 훈련시켜서

1037
01:05:21,890 --> 01:05:26,040
값이나 클래스 레이블을 생성하는 것입니다.

1038
01:05:26,040 --> 01:05:31,940
하지만 실제로, 이걸 보여주기 전에,

1039
01:05:31,940 --> 01:05:35,670
이 네트워크를 훈련시키기

1040
01:05:35,670 --> 01:05:38,450
위해 우리는

1041
01:05:38,450 --> 01:05:45,660
항상 손실이나 클래스 점수에 대해 네트워크

1042
01:05:45,660 --> 01:05:50,000
가중치의 미분을 취해서 가중치를

1043
01:05:50,000 --> 01:05:53,204
업데이트했습니다.

1044
01:05:53,204 --> 01:05:56,340
이제 필요한 것은 각

1045
01:05:56,340 --> 01:06:03,380
픽셀에 대해 픽셀 값을 변경하면 dog 점수에 얼마나 영향을

1046
01:06:03,380 --> 01:06:06,740
주는지 보는 것입니다.

1047
01:06:06,740 --> 01:06:07,830
이게 무슨 뜻일까요?

1048
01:06:07,830 --> 01:06:12,950
제가 설명한 것은 변화의 의미입니다.

1049
01:06:12,950 --> 01:06:19,080
즉, 기본적으로 gradient의 의미라는 겁니다.

1050
01:06:19,080 --> 01:06:23,000
그래서 이제 픽셀 값에 대해 점수의 gradient를

1051
01:06:23,000 --> 01:06:26,000
취하면, 네트워크 가중치가 아니라

1052
01:06:26,000 --> 01:06:30,680
픽셀 값에 대해, 그 gradient를 시각화할 수 있습니다.

1053
01:06:30,680 --> 01:06:33,650
그리고 그것을 시각화한다는 것은, 이

1054
01:06:33,650 --> 01:06:36,000
픽셀들이 중요한 픽셀이라는 뜻입니다.

1055
01:06:36,000 --> 01:06:40,020
이 이미지에서 개를 분류하기 위해 중요한 픽셀들이

1056
01:06:40,020 --> 01:06:41,700
바로 저 픽셀들입니다.

1057
01:06:41,700 --> 01:06:44,840
그래서 그 픽셀들의 값을

1058
01:06:44,840 --> 01:06:48,590
바꾸면, 개 점수도 바뀌게 됩니다.

1059
01:06:48,590 --> 01:06:52,280
다시 말하지만, 이것이 우리가 이야기한 그래디언트의

1060
01:06:52,280 --> 01:06:53,970
기본 의미와 정의입니다.

1061
01:06:53,970 --> 01:06:56,160
이것이 한 가지 방법입니다.

1062
01:06:56,160 --> 01:06:59,630
네트워크에서 학습한 다른

1063
01:06:59,630 --> 01:07:02,700
객체들에 대해 이것을

1064
01:07:02,700 --> 01:07:07,220
실행하면, 이렇게 결과가 나옵니다.

1065
01:07:07,220 --> 01:07:12,080
이것이 saliency를 이해하는 한 가지 방법입니다.

1066
01:07:12,080 --> 01:07:14,790
그리고 많은 경우에 매우 효과적입니다.

1067
01:07:14,790 --> 01:07:19,790
하지만 때로는 단순히 픽셀 값만 뒤로 추적하는

1068
01:07:19,790 --> 01:07:21,810
것이 아닙니다.

1069
01:07:21,810 --> 01:07:25,640
각 클래스별로 활성화가 어떻게 작동하는지

1070
01:07:25,640 --> 01:07:28,770
보고 싶을 때가 있습니다.

1071
01:07:28,770 --> 01:07:32,330
이것이 바로 클래스 활성화 맵, 즉 CAM

1072
01:07:32,330 --> 01:07:37,010
알고리즘, 클래스 활성화 매핑, CAM 또는 Grad-CAM에

1073
01:07:37,010 --> 01:07:40,760
관한 내용입니다. 제가 2분 후에 설명할 텐데,

1074
01:07:40,760 --> 01:07:44,900
이것들은 CNN을 이해하는 데 가장 널리 사용되는

1075
01:07:44,900 --> 01:07:47,430
알고리즘 중 하나이며, 다른

1076
01:07:47,430 --> 01:07:49,950
아키텍처에도 사용할 수 있습니다.

1077
01:07:49,950 --> 01:07:53,360
하지만 트랜스포머의 경우, 훨씬 더

1078
01:07:53,360 --> 01:07:56,900
좋은 해석 방법이 있는데, 이는 지난

1079
01:07:56,900 --> 01:07:59,090
강의에서 다뤘습니다.

1080
01:07:59,090 --> 01:08:04,100
일단 각 컨볼루션 레이어마다 보통

1081
01:08:04,100 --> 01:08:06,200
풀링을 합니다.

1082
01:08:06,200 --> 01:08:08,220
그리고 풀링은 피처 맵을 생성합니다.

1083
01:08:08,220 --> 01:08:11,400
그 피처 맵들은 점수로 변환됩니다.

1084
01:08:11,400 --> 01:08:20,134
그리고 그 점수들은 가중치 값들과 함께 사용됩니다.

1085
01:08:20,134 --> 01:08:24,950
수학적으로 확장하면, 기본적으로 클래스 점수를

1086
01:08:24,950 --> 01:08:29,430
가중 합 형태로 강조할 수 있습니다.

1087
01:08:29,430 --> 01:08:35,359
즉, 클래스 예측을 피처 맵과 이미지 공간 내 특정

1088
01:08:35,359 --> 01:08:39,500
위치까지 추적할 수 있다는 뜻입니다.

1089
01:08:39,500 --> 01:08:42,060
왜냐하면 컨볼루션

1090
01:08:42,060 --> 01:08:44,630
레이어는 항상 이미지 공간에

1091
01:08:44,630 --> 01:08:47,379
매핑되기 때문입니다.

1092
01:08:47,379 --> 01:08:52,850
우리는 모든 연산에서 공간적 일관성을 유지하기 위해 컨볼루션을

1093
01:08:52,850 --> 01:08:55,729
수행하며, 이를 통해 이미지

1094
01:08:55,729 --> 01:08:58,140
공간까지 추적할 수 있습니다.

1095
01:08:58,140 --> 01:09:02,180
어쨌든, 우리는 특징 맵을 보고 각

1096
01:09:02,180 --> 01:09:04,890
클래스 활성화가 이미지

1097
01:09:04,890 --> 01:09:06,859
내의 위치에 어떻게

1098
01:09:06,859 --> 01:09:11,040
영향을 미치는지 확인할 수 있습니다.

1099
01:09:11,040 --> 01:09:16,880
이제 특징 값 위에 학습된 가중치와 이

1100
01:09:16,880 --> 01:09:21,470
가중치의 곱셈을 하면, 클래스

1101
01:09:21,470 --> 01:09:26,160
활성화를 생성할 수 있습니다.

1102
01:09:26,160 --> 01:09:31,100
이것은 제가 이제 이미지 공간으로 다시 돌아갈 수

1103
01:09:31,100 --> 01:09:33,810
있는 방법을 가지고 있다는

1104
01:09:33,810 --> 01:09:36,080
의미입니다. 컨볼루션

1105
01:09:36,080 --> 01:09:41,060
공간에 있는 한, 이미지까지 완전히 되돌아가서

1106
01:09:41,060 --> 01:09:46,850
예를 들어 궁전, 돔, 교회, 제단, 수도원 같은 각

1107
01:09:46,850 --> 01:09:52,529
클래스에 대한 다양한 클래스 활성화 맵을 만들 수 있습니다.

1108
01:09:52,529 --> 01:09:54,270
이것들이 가중치입니다.

1109
01:09:54,270 --> 01:09:57,380
이것들은 특정 클래스

1110
01:09:57,380 --> 01:10:01,220
점수를 유도하는 컨볼루션

1111
01:10:01,220 --> 01:10:06,260
층의 픽셀 또는 영역입니다.

1112
01:10:06,260 --> 01:10:09,920
다른 이미지 내 단일 객체에 대한

1113
01:10:09,920 --> 01:10:14,000
클래스 활성화 맵도 마찬가지입니다.

1114
01:10:14,000 --> 01:10:15,780
하지만 여기에 문제가 있습니다.

1115
01:10:15,780 --> 01:10:18,050
그리고 그 문제는 우리가 이것을 마지막

1116
01:10:18,050 --> 01:10:21,090
convolution layer에만 적용할 수 있다는 점입니다, 왜냐하면

1117
01:10:21,090 --> 01:10:23,910
이것이 우리가 할 수 있는 유일한 방법이기 때문입니다.

1118
01:10:23,910 --> 01:10:26,550
우리는 마지막 convolution layer로만 갈 수 있습니다.

1119
01:10:26,550 --> 01:10:29,550
여기서 계산한 방식입니다.

1120
01:10:29,550 --> 01:10:32,390
그리고 그 문제를 해결하기 위해 Grad-CAM이라는

1121
01:10:32,390 --> 01:10:34,640
알고리즘 변형이 있습니다,

1122
01:10:34,640 --> 01:10:38,990
즉 gradient-weighted class activation

1123
01:10:38,990 --> 01:10:40,170
maps입니다.

1124
01:10:40,170 --> 01:10:43,040
기본적으로 같은 알고리즘입니다.

1125
01:10:43,040 --> 01:10:46,850
우리는 가중치를 계산할

1126
01:10:46,850 --> 01:10:52,160
때, 활성화를 만든 레이어 중

1127
01:10:52,160 --> 01:10:54,900
하나를 선택합니다.

1128
01:10:54,900 --> 01:10:59,130
클래스 수준에서, 단순히 W와 feature의

1129
01:10:59,130 --> 01:11:03,170
곱을 계산하는 대신에, 그래디언트를

1130
01:11:03,170 --> 01:11:04,760
계산합니다.

1131
01:11:04,760 --> 01:11:07,640
그래디언트를 따라 거슬러 올라가서

1132
01:11:07,640 --> 01:11:11,180
그래디언트를 기반으로 가중치를

1133
01:11:11,180 --> 01:11:15,900
만들고, 그 가중치가 대신 사용됩니다, 이는 특정

1134
01:11:15,900 --> 01:11:17,810
레이어까지의 모든

1135
01:11:17,810 --> 01:11:20,700
가중치와 그래디언트의 집합입니다.

1136
01:11:20,700 --> 01:11:24,960
그리고 그것을 가중치로 곱합니다.

1137
01:11:24,960 --> 01:11:32,850
그리고 ReLU를 사용해 양수만 통과시킵니다.

1138
01:11:32,850 --> 01:11:36,170
그리고 그것은 이미지 공간 전체에

1139
01:11:36,170 --> 01:11:38,910
걸쳐서도 나타낼 수 있습니다.

1140
01:11:38,910 --> 01:11:43,820
그래서 저는 마지막 합성곱 층에만 적용되는 CAM에

1141
01:11:43,820 --> 01:11:45,600
대해 이야기했습니다.

1142
01:11:45,600 --> 01:11:47,260
만약 원하신다면—하지만

1143
01:11:47,260 --> 01:11:51,660
대부분의 CNN 알고리즘에서는 마지막에 합성곱

1144
01:11:51,660 --> 01:11:55,990
층이 하나만 있는 경우가 없기 때문에 불가능합니다.

1145
01:11:55,990 --> 01:11:59,860
항상 완전 연결층 같은 다른 연산들이 있기 때문입니다.

1146
01:11:59,860 --> 01:12:03,870
그래서 이 클래스 활성화를 중간에

1147
01:12:03,870 --> 01:12:06,390
다른 층이 있어도 합성곱

1148
01:12:06,390 --> 01:12:10,230
층으로 전달하려면, 보통

1149
01:12:10,230 --> 01:12:17,560
그래디언트를 사용해서 맵을 그래디언트의 합으로 가중치 조정합니다.

1150
01:12:17,560 --> 01:12:20,410
그렇게 하면 실제로 시각화를 할 수 있습니다.

1151
01:12:20,410 --> 01:12:24,060
각 객체에 대해 이런 히트맵을 만듭니다.

1152
01:12:24,060 --> 01:12:26,980
이것이 CNN에 관한 내용이었습니다.

1153
01:12:26,980 --> 01:12:32,310
하지만 지난주 강의에서 트랜스포머에 대해 이야기했는데,

1154
01:12:32,310 --> 01:12:37,270
트랜스포머는 본질적으로 활성화 맵을 가지고 있습니다.

1155
01:12:37,270 --> 01:12:41,610
Justin이 보여준 언어 행렬 기억하시죠,

1156
01:12:41,610 --> 01:12:45,660
출력 단어마다 입력에 대한 어텐션

1157
01:12:45,660 --> 01:12:47,820
가중치가 있었습니다.

1158
01:12:47,820 --> 01:12:50,610
이것을 픽셀에도 똑같이 할 수 있습니다.

1159
01:12:50,610 --> 01:12:54,140
출력마다 픽셀 공간에서 이런

1160
01:12:54,140 --> 01:13:00,110
맵을 만들어 ViT의 특징을 시각화할 수

1161
01:13:00,110 --> 01:13:01,020
있습니다.

1162
01:13:01,020 --> 01:13:03,140
즉, ViT와 트랜스포머에서는 이

1163
01:13:03,140 --> 01:13:04,410
작업이 훨씬 쉽습니다.

1164
01:13:04,410 --> 01:13:09,080
이미 어텐션, 가중치를 시각화하는

1165
01:13:09,080 --> 01:13:11,730
방법이 있기 때문입니다.

1166
01:13:11,730 --> 01:13:16,880
하지만 CNN에서는 보통 Grad-CAM이나 이런

1167
01:13:16,880 --> 01:13:19,830
종류의 알고리즘을 사용합니다.

1168
01:13:19,830 --> 01:13:24,530
그렇긴 하지만, 오늘 이야기하고 싶었던

1169
01:13:24,530 --> 01:13:29,180
주제를 마무리하면서 제가 못할 줄 알았던

1170
01:13:29,180 --> 01:13:31,350
과제를 해냈습니다.

1171
01:13:31,350 --> 01:13:35,000
그리고 다음 세션에서는 비디오 이해에 관한

1172
01:13:35,000 --> 01:13:36,810
강의를 진행할 예정입니다.

1173
01:13:36,810 --> 01:13:38,740
감사합니다.
