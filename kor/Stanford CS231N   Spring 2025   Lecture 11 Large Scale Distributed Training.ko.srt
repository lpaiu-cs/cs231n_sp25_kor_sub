1
00:00:05,240 --> 00:00:07,737
CS231n 강의 11에 다시 오신 것을 환영합니다.

2
00:00:07,737 --> 00:00:10,070
오늘은 대규모 분산 학습에 대해

3
00:00:10,070 --> 00:00:10,770
이야기하겠습니다.

4
00:00:10,770 --> 00:00:12,020
이 주제는 매우 흥미로운데요,

5
00:00:12,020 --> 00:00:14,228
왜냐하면 오늘날 모든 신경망이 실제로

6
00:00:14,228 --> 00:00:15,510
이렇게 학습되기 때문입니다.

7
00:00:15,510 --> 00:00:18,420
스타트업, 산업계, 심지어 학계에서 대규모

8
00:00:18,420 --> 00:00:20,630
모델을 보면, 대규모가

9
00:00:20,630 --> 00:00:22,850
딥러닝의 새로운 표준이 되었습니다.

10
00:00:22,850 --> 00:00:24,350
이것은 우리가 이 수업을

11
00:00:24,350 --> 00:00:26,190
시작한 지난 10년

12
00:00:26,190 --> 00:00:27,800
동안 많이 변한 점입니다.

13
00:00:27,800 --> 00:00:30,619
10년 전만 해도 대부분의 모델을 한 개의

14
00:00:30,619 --> 00:00:32,119
GPU, 한 장치에서

15
00:00:32,119 --> 00:00:34,550
학습하는 것이 일반적이었고, 여러

16
00:00:34,550 --> 00:00:36,960
장치에서 학습하는 것은 드물었습니다.

17
00:00:36,960 --> 00:00:38,690
하지만 지금은 수십, 수백,

18
00:00:38,690 --> 00:00:41,960
수천, 심지어 수만 개의 장치에서 동시에 모델을

19
00:00:41,960 --> 00:00:43,820
학습하는 것이 새로운 표준입니다.

20
00:00:43,820 --> 00:00:45,320
그래서 이를 위해

21
00:00:45,320 --> 00:00:49,130
새로운 알고리즘과 새로운 사고방식을 개발해야 합니다.

22
00:00:49,130 --> 00:00:51,900
오늘 강의 내내 예시로 사용할 모델은

23
00:00:51,900 --> 00:00:55,040
Llama3-405B입니다. 이 모델이 가장

24
00:00:55,040 --> 00:00:57,560
뛰어나거나 가장 흥미로운 모델이라서가

25
00:00:57,560 --> 00:00:59,920
아니라, 실제로 훈련 방법,

26
00:00:59,920 --> 00:01:02,500
모델 아키텍처 등 구현 세부사항을 꽤

27
00:01:02,500 --> 00:01:04,700
자세히 공유하는 최신 모델에

28
00:01:04,700 --> 00:01:06,680
가까운 모델이기 때문입니다.

29
00:01:06,680 --> 00:01:08,680
지난 몇 년간 구글, OpenAI,

30
00:01:08,680 --> 00:01:10,805
Anthropic 등 여러 곳에서 정말

31
00:01:10,805 --> 00:01:13,162
놀랍고 강력한 모델들이 많이 나왔습니다.

32
00:01:13,162 --> 00:01:15,370
하지만 기본적으로 이들 모델에 대한 세부사항은

33
00:01:15,370 --> 00:01:16,880
전혀 공유하지 않습니다.

34
00:01:16,880 --> 00:01:19,750
저에게 산업계에 큰 변화를 가져온 유명한

35
00:01:19,750 --> 00:01:23,960
인용구가 있는데, 2023년 GPT4 논문에 나왔습니다.

36
00:01:23,960 --> 00:01:26,553
GPT4 모델을 공개하면서, "경쟁

37
00:01:26,553 --> 00:01:28,720
환경과 GPT4 같은 대규모

38
00:01:28,720 --> 00:01:32,150
모델의 안전성 문제를 고려해, 이 보고서(즉,

39
00:01:32,150 --> 00:01:34,370
GPT4 논문)에는 아키텍처, 모델

40
00:01:34,370 --> 00:01:36,980
크기, 하드웨어, 훈련 컴퓨팅,

41
00:01:36,980 --> 00:01:39,470
데이터셋 구성, 훈련 방법 등 어떤

42
00:01:39,470 --> 00:01:42,080
세부사항도 포함하지 않는다"고 했습니다.

43
00:01:42,080 --> 00:01:43,400
이게 바로 핵심 소식입니다.

44
00:01:43,400 --> 00:01:45,910
GPT4 이후 지난 3년간

45
00:01:45,910 --> 00:01:49,040
대규모 모델의 최신 동향은 이렇습니다.

46
00:01:49,040 --> 00:01:50,902
그들은 어떤 것도 알려주지 않습니다.

47
00:01:50,902 --> 00:01:52,610
모델에 대해 아무것도 알려주지 않을 겁니다.

48
00:01:52,610 --> 00:01:54,902
트랜스포머라고 말해주면 운이 좋은 거죠.

49
00:01:54,902 --> 00:01:56,260
그 정도는 말해줄 수도 있습니다.

50
00:01:56,260 --> 00:01:58,420
그래서 Llama3가 주목받는 이유는 최고의

51
00:01:58,420 --> 00:02:00,170
모델이라서가 아니라, 가장

52
00:02:00,170 --> 00:02:01,950
개방된 모델 중 하나이기 때문입니다.

53
00:02:01,950 --> 00:02:04,670
이 모델은 Meta가 훈련시키고 2024년

54
00:02:04,670 --> 00:02:08,340
4월에 오픈 소스로 공개한 대형 언어 모델입니다.

55
00:02:08,340 --> 00:02:10,370
OpenAI와 달리, 논문에서는

56
00:02:10,370 --> 00:02:13,440
모델 훈련에 관한 많은 세부사항을 공유합니다.

57
00:02:13,440 --> 00:02:15,080
데이터셋에 대한 내용은 많지 않지만,

58
00:02:15,080 --> 00:02:18,000
훈련에 사용된 시스템 인프라에 대해서는 많이 다룹니다.

59
00:02:18,000 --> 00:02:19,640
이것은 우리가 요즘

60
00:02:19,640 --> 00:02:23,030
대규모 LLM이 실제로 어떻게 훈련되는지

61
00:02:23,030 --> 00:02:24,800
엿볼 수 있게 해줍니다.

62
00:02:24,800 --> 00:02:27,110
참고로, Meta에서 지난달인

63
00:02:27,110 --> 00:02:30,850
2025년 4월에 새로운 Llama4 모델이 나왔습니다.

64
00:02:30,850 --> 00:02:33,350
이미 오픈 소스에 약간 더 나은 모델들이 있지만,

65
00:02:33,350 --> 00:02:35,872
Llama4에 관한 논문은 아직 없습니다.

66
00:02:35,872 --> 00:02:37,580
몇 달 후에 나올 논문을

67
00:02:37,580 --> 00:02:38,750
읽고 새로운 세대

68
00:02:38,750 --> 00:02:41,420
Llama 훈련에서 무엇을 배울 수 있을지

69
00:02:41,420 --> 00:02:42,000
기대됩니다.

70
00:02:42,000 --> 00:02:44,415
하지만 오늘 강의 내내

71
00:02:44,415 --> 00:02:46,040
예시로 Llama3-405B

72
00:02:46,040 --> 00:02:50,252
모델을 많이 사용할 예정입니다.

73
00:02:50,252 --> 00:02:51,710
좋습니다, 오늘 이야기할

74
00:02:51,710 --> 00:02:53,270
주제는 두 가지입니다.

75
00:02:53,270 --> 00:02:56,110
하나는 GPU 하드웨어에 관한 것이고, 다른 하나는

76
00:02:56,110 --> 00:02:57,860
많은 GPU에서 훈련하는 방법입니다.

77
00:02:57,860 --> 00:02:59,920
이 장치들이 실제로 어떤 하드웨어에서

78
00:02:59,920 --> 00:03:02,260
실행되는지와, 많은 GPU에서

79
00:03:02,260 --> 00:03:04,510
훈련하기 위해 필요한 알고리즘을

80
00:03:04,510 --> 00:03:06,020
모두 알려드리고자 합니다.

81
00:03:06,020 --> 00:03:08,990
먼저 GPU 하드웨어에 대해 조금 이야기하겠습니다.

82
00:03:08,990 --> 00:03:11,230
GPU는 잘 모르시는 분들을 위해 설명드리자면, Graphics

83
00:03:11,230 --> 00:03:12,860
Processing Unit 즉 그래픽 처리 장치입니다.

84
00:03:12,860 --> 00:03:14,650
원래 컴퓨터 그래픽을 위해

85
00:03:14,650 --> 00:03:17,060
개발된 특수한 보조 프로세서였죠.

86
00:03:17,060 --> 00:03:19,400
그런데 이게 매우 유용한 범용

87
00:03:19,400 --> 00:03:21,335
병렬 처리기로 발전했습니다.

88
00:03:21,335 --> 00:03:22,960
이 강의를 이 방에서

89
00:03:22,960 --> 00:03:24,850
하는 게 아주 적절한데요,

90
00:03:24,850 --> 00:03:26,690
이곳은 Huang 강당입니다.

91
00:03:26,690 --> 00:03:28,660
Jen-Hsun Huang은

92
00:03:28,660 --> 00:03:32,050
NVIDIA의 CEO이자 창립자로, 현재

93
00:03:32,050 --> 00:03:35,090
그리고 지난 수십 년간 게임과 머신러닝용

94
00:03:35,090 --> 00:03:37,790
GPU를 생산하는 가장 큰 회사입니다.

95
00:03:37,790 --> 00:03:39,952
이 장치들은 기본적으로 그래픽 용도로 시작했습니다.

96
00:03:39,952 --> 00:03:41,410
컴퓨터 그래픽을 할

97
00:03:41,410 --> 00:03:42,570
때 화면에 많은

98
00:03:42,570 --> 00:03:44,528
픽셀을 생성해야 하니까요.

99
00:03:44,528 --> 00:03:46,540
픽셀을 만들기 위해 많은 작은

100
00:03:46,540 --> 00:03:48,770
원시 기하학 조각들을 처리해야 합니다.

101
00:03:48,770 --> 00:03:51,670
그래서 컴퓨터 그래픽에서는 많은 계산을 병렬로 하는

102
00:03:51,670 --> 00:03:53,380
게 아주 자연스러운 일이죠.

103
00:03:53,380 --> 00:03:57,200
그래서 사람들은 원래 그래픽용으로 만들어진

104
00:03:57,200 --> 00:03:59,600
이 하드웨어가 훨씬 더

105
00:03:59,600 --> 00:04:01,100
일반적인 병렬

106
00:04:01,100 --> 00:04:04,970
계산에도 쓸 수 있다는 걸 금방 알아냈습니다.

107
00:04:04,970 --> 00:04:08,068
2000년대 초반에 연구자들이 그래픽

108
00:04:08,068 --> 00:04:10,610
카드를 일반 병렬 프로그래밍에

109
00:04:10,610 --> 00:04:13,910
맞게 변형하는 방법을 찾아냈습니다.

110
00:04:13,910 --> 00:04:16,040
그리고 2000년대 후반부터

111
00:04:16,040 --> 00:04:18,950
2010년대에 걸쳐 NVIDIA는 이걸

112
00:04:18,950 --> 00:04:21,082
본격적으로 발전시키고, 마케팅하며,

113
00:04:21,082 --> 00:04:23,540
범용 병렬 프로세서로 만들기

114
00:04:23,540 --> 00:04:24,375
시작했습니다.

115
00:04:24,375 --> 00:04:26,750
당시에는 정확히 어떤 용도로 쓰일지 잘

116
00:04:26,750 --> 00:04:27,930
몰랐던 것 같습니다.

117
00:04:27,930 --> 00:04:30,350
병렬 처리가 중요해질 거라는

118
00:04:30,350 --> 00:04:32,960
일반적인 생각은 있었고, 2010년대

119
00:04:32,960 --> 00:04:36,302
초 딥러닝이 부상할 때 이를 잘 활용했습니다.

120
00:04:36,302 --> 00:04:38,510
NVIDIA는 이 연구 분야의

121
00:04:38,510 --> 00:04:41,650
잠재력을 아주 일찍, 2010년대

122
00:04:41,650 --> 00:04:44,240
초반부터 인지하고, 딥러닝 학습에

123
00:04:44,240 --> 00:04:46,820
적합한 하드웨어를 만들기

124
00:04:46,820 --> 00:04:49,230
위해 많은 자원을 투입했습니다.

125
00:04:49,230 --> 00:04:52,190
그 결과 지난 10년 넘게 대규모

126
00:04:52,190 --> 00:04:54,840
딥러닝 모델을 학습하는 주된

127
00:04:54,840 --> 00:04:56,452
방법이 되었습니다.

128
00:04:56,452 --> 00:04:58,660
그것은 조금씩 변하기 시작하고 있는데,

129
00:04:58,660 --> 00:05:03,690
우리가 조금 후에 볼 수 있듯이, 그들의 칩이 사람들이 주로 사용하는
것입니다.

130
00:05:03,690 --> 00:05:06,030
그래서 저는 항상 이런 것들을 들여다보고 내부에 무엇이

131
00:05:06,030 --> 00:05:07,240
있는지 보는 것을 좋아합니다.

132
00:05:07,240 --> 00:05:09,370
이것은 현재 딥러닝

133
00:05:09,370 --> 00:05:12,930
훈련의 주력인 NVIDIA H100의

134
00:05:12,930 --> 00:05:14,035
사진입니다.

135
00:05:14,035 --> 00:05:15,910
다음 세대 제품이 방금 출시되었지만,

136
00:05:15,910 --> 00:05:17,368
아직 접근하기 어렵습니다.

137
00:05:17,368 --> 00:05:19,180
저도 아직 그것으로 훈련한 적이 없습니다.

138
00:05:19,180 --> 00:05:21,870
그래서 이것이 현재 최첨단 기술입니다.

139
00:05:21,870 --> 00:05:25,702
이 NVIDIA GPU, 가운데에 있는 H100 GPU 내부에는 이런

140
00:05:25,702 --> 00:05:26,910
컴퓨트 코어들이 있습니다.

141
00:05:26,910 --> 00:05:29,400
그리고 그 주위를 80기가바이트의 HBM 메모리,

142
00:05:29,400 --> 00:05:30,907
고대역폭 메모리가 둘러싸고 있습니다.

143
00:05:30,907 --> 00:05:32,490
메모리가 컴퓨트 코어와 분리되어 있는

144
00:05:32,490 --> 00:05:33,460
것을 볼 수 있습니다.

145
00:05:33,460 --> 00:05:34,360
데이터를 이동시켜야 합니다.

146
00:05:34,360 --> 00:05:36,790
GPU 메모리와 코어 사이에서 데이터를

147
00:05:36,790 --> 00:05:40,030
주고받기 위해 이 버스를 통해 서로 통신해야 합니다.

148
00:05:40,030 --> 00:05:43,180
이 속도는 초당 약 3테라바이트로, 매우

149
00:05:43,180 --> 00:05:45,330
많은 비트가 이동하는 셈입니다.

150
00:05:45,330 --> 00:05:48,640
GPU 코어 내부를 좀 더 자세히 들여다보면,

151
00:05:48,640 --> 00:05:51,240
컴퓨트 코어 부분 중앙에 약 50메가바이트

152
00:05:51,240 --> 00:05:52,920
크기의 L2 캐시라는

153
00:05:52,920 --> 00:05:55,190
작은 메모리가 있습니다. 이는

154
00:05:55,190 --> 00:05:58,890
80기가바이트에 달하는 HBM 메모리보다 훨씬 작습니다.

155
00:05:58,890 --> 00:06:02,032
하지만 실제 연산 요소와 매우 가까이 위치해 있습니다.

156
00:06:02,032 --> 00:06:03,740
그래서 컴퓨트 코어에서 훨씬

157
00:06:03,740 --> 00:06:05,300
빠르게 접근할 수 있습니다.

158
00:06:05,300 --> 00:06:06,950
그리고 이 장치의 진짜 핵심은

159
00:06:06,950 --> 00:06:10,940
132개의 Streaming Multiprocessors, 즉 SM입니다.

160
00:06:10,940 --> 00:06:13,760
이것들은 독립적인 병렬 코어들입니다.

161
00:06:13,760 --> 00:06:15,290
일반 CPU 코어보다 병렬

162
00:06:15,290 --> 00:06:17,498
처리를 훨씬 더 많이 할 수 있어서

163
00:06:17,498 --> 00:06:19,290
어떤 면에서는 좀 더 강력합니다.

164
00:06:19,290 --> 00:06:21,410
하지만 클럭 속도가 느린 경향이 있어서

165
00:06:21,410 --> 00:06:23,360
여러 면에서 일반 CPU 코어보다

166
00:06:23,360 --> 00:06:24,585
훨씬 약하기도 합니다.

167
00:06:24,585 --> 00:06:26,460
명령어 예측이나 분기 예측도 그렇게

168
00:06:26,460 --> 00:06:27,600
많이 할 수 없습니다.

169
00:06:27,600 --> 00:06:29,540
그래서 이 GPU 코어와 CPU

170
00:06:29,540 --> 00:06:32,450
코어를 정확히 똑같이 비교하기는 정말

171
00:06:32,450 --> 00:06:32,970
어렵습니다.

172
00:06:32,970 --> 00:06:35,600
하지만 저는 보통 이 스트리밍 멀티프로세서들을

173
00:06:35,600 --> 00:06:38,480
대략 CPU 코어와 비슷하다고 생각합니다.

174
00:06:38,480 --> 00:06:41,030
또 누군가는 집에 가서 이 화면에

175
00:06:41,030 --> 00:06:43,020
있는 작은 박스들을 다 세어볼

176
00:06:43,020 --> 00:06:45,810
텐데, 실제로는 144개가 있고 제가

177
00:06:45,810 --> 00:06:47,460
132개라고 말했을 겁니다.

178
00:06:47,460 --> 00:06:48,270
왜 그럴까요?

179
00:06:48,270 --> 00:06:51,070
GPU 하드웨어는 모두 '비닝(binning)'이라는 과정을

180
00:06:51,070 --> 00:06:53,640
사용하는데, 이걸 만드는 데에는 너무 많은 트랜지스터와

181
00:06:53,640 --> 00:06:54,940
작은 연산 요소들이 있습니다.

182
00:06:54,940 --> 00:06:57,107
아무리 많은 돈을 투자해도

183
00:06:57,107 --> 00:06:58,600
완벽하게 나오지 않습니다.

184
00:06:58,600 --> 00:07:00,683
항상 일부는 약간 문제가 생기게 마련입니다.

185
00:07:00,683 --> 00:07:03,340
그래서 제품 개발 단계에서 이를 고려합니다.

186
00:07:03,340 --> 00:07:05,530
그리고 이렇게 말하죠, 칩을 만들려고 하는데,

187
00:07:05,530 --> 00:07:08,250
이론상 완전한 칩은 144개가 있어야 하지만, 완벽한

188
00:07:08,250 --> 00:07:08,890
칩은 없습니다.

189
00:07:08,890 --> 00:07:10,973
하지만 적어도 132개는 제대로 작동하는

190
00:07:10,973 --> 00:07:13,360
칩을 합리적으로 얻을 수 있다는 걸 알고 있습니다.

191
00:07:13,360 --> 00:07:15,400
그래서 이 비닝 과정을 사용하는 겁니다.

192
00:07:15,400 --> 00:07:17,810
그래서 실제로는 132개만

193
00:07:17,810 --> 00:07:19,560
켜질

194
00:07:19,560 --> 00:07:24,825
것이라고 약속함으로써 생산하려던 칩의 훨씬 더 큰

195
00:07:24,825 --> 00:07:27,840
비율을 판매할 수 있습니다.

196
00:07:27,840 --> 00:07:30,270
그다음에는 그 스트리밍 멀티프로세서 중 하나를 더 깊이

197
00:07:30,270 --> 00:07:31,180
들여다볼 수 있습니다.

198
00:07:31,180 --> 00:07:33,600
그리고 GPU 내부에서 무슨 일이 일어나고 있는지

199
00:07:33,600 --> 00:07:35,260
더 자세히 볼 수 있습니다.

200
00:07:35,260 --> 00:07:38,070
이것은 H100 안에 있는 132개의 활성

201
00:07:38,070 --> 00:07:40,120
스트리밍 멀티프로세서 중 하나입니다.

202
00:07:40,120 --> 00:07:43,050
여기서 살펴볼 흥미로운 요소가 몇 가지 있습니다.

203
00:07:43,050 --> 00:07:46,530
먼저, 256킬로바이트의 L1 캐시와 레지스터

204
00:07:46,530 --> 00:07:47,810
파일이 있습니다.

205
00:07:47,810 --> 00:07:51,030
이것은 GPU의 메모리 계층 구조 추세를 계속 보여줍니다.

206
00:07:51,030 --> 00:07:53,583
일반적으로 딥러닝을 배우고 있다고

207
00:07:53,583 --> 00:07:54,750
생각했지만,

208
00:07:54,750 --> 00:07:56,708
사실은 컴퓨터 아키텍처를 배우고 있는 겁니다.

209
00:07:56,708 --> 00:07:57,710
죄송합니다, 놀라운 사실이죠.

210
00:07:57,710 --> 00:07:59,480
그리고 메모리 계층 구조가

211
00:07:59,480 --> 00:08:01,070
딥러닝과 모든 고성능 컴퓨팅에

212
00:08:01,070 --> 00:08:03,415
정말 중요하다는 것이 밝혀졌습니다.

213
00:08:03,415 --> 00:08:04,790
일반적인 추세는 컴퓨트

214
00:08:04,790 --> 00:08:06,860
코어에서 멀어질수록 더 큰

215
00:08:06,860 --> 00:08:08,220
메모리가 있다는 것입니다.

216
00:08:08,220 --> 00:08:10,080
컴퓨트 코어에 가까워질수록

217
00:08:10,080 --> 00:08:12,690
메모리 크기는 작지만 훨씬 더 빠릅니다.

218
00:08:12,690 --> 00:08:14,390
이 메모리 계층

219
00:08:14,390 --> 00:08:16,803
구조를 잘 이해하고,

220
00:08:16,803 --> 00:08:18,470
이 계층 간에

221
00:08:18,470 --> 00:08:20,240
데이터를 주고받는

222
00:08:20,240 --> 00:08:21,890
데 매우

223
00:08:21,890 --> 00:08:24,813
신경 쓰는 것이 중요합니다.

224
00:08:24,813 --> 00:08:26,730
성능 좋은 GPU 커널을 작성한다면,

225
00:08:26,730 --> 00:08:29,460
이 부분을 최적화하는 데 많은 시간을 투자합니다.

226
00:08:29,460 --> 00:08:31,910
이것을 보여주기 위해, H100의

227
00:08:31,910 --> 00:08:35,309
세 가지 메모리 계층을 볼 수 있습니다.

228
00:08:35,309 --> 00:08:40,650
L1 캐시는 256킬로바이트, L2 캐시는 50메가바이트, 그리고 HBM

229
00:08:40,650 --> 00:08:43,169
메모리는 80기가바이트입니다.

230
00:08:43,169 --> 00:08:44,750
이것들이 H100의

231
00:08:44,750 --> 00:08:47,680
세 가지 주요 메모리 계층입니다.

232
00:08:47,680 --> 00:08:51,430
그리고 FP32 코어가 128개 있습니다.

233
00:08:51,430 --> 00:08:53,260
이 코어들은 일반적인 부동소수점

234
00:08:53,260 --> 00:08:56,060
연산을 수행하는 작은 산술 유닛입니다.

235
00:08:56,060 --> 00:08:59,980
특히 이 128개의 FP32 코어 각각은 a, x, b가

236
00:08:59,980 --> 00:09:03,890
모두 스칼라인 ax 더하기 b를 계산할 수 있습니다.

237
00:09:03,890 --> 00:09:06,950
이 계산을 한 클럭 사이클에 수행할 수 있습니다.

238
00:09:06,950 --> 00:09:10,030
따라서 이 ax 더하기 b는 곱셈

239
00:09:10,030 --> 00:09:12,400
1번, 덧셈 1번이고, 코어가

240
00:09:12,400 --> 00:09:15,190
128개 있으니 모두 합치면,

241
00:09:15,190 --> 00:09:20,170
이 SM 하나가 장치의 클럭 사이클당 SM당 256개의 부동소수점

242
00:09:20,170 --> 00:09:22,690
연산을 수행할 수 있습니다.

243
00:09:22,690 --> 00:09:24,940
그리고 빨간색으로 표시된 부분을

244
00:09:24,940 --> 00:09:27,880
보면, 진짜 마법이 일어나는 곳인데,

245
00:09:27,880 --> 00:09:31,990
FP32 코어 외에 4개의 텐서 코어가 있습니다.

246
00:09:31,990 --> 00:09:34,910
이 이름은 약간 오해의 소지가 있다고 생각합니다.

247
00:09:34,910 --> 00:09:37,210
사실 이들은 매트릭스 코어입니다.

248
00:09:37,210 --> 00:09:39,250
이 작은 텐서 코어들은

249
00:09:39,250 --> 00:09:41,990
특별히 설계된 회로입니다.

250
00:09:41,990 --> 00:09:43,130
오직 한 가지 일만 합니다.

251
00:09:43,130 --> 00:09:44,340
바로 매트릭스 곱셈입니다.

252
00:09:44,340 --> 00:09:46,190
각 텐서 코어는 매트릭스

253
00:09:46,190 --> 00:09:48,890
곱셈의 한 조각을 수행할 수 있습니다.

254
00:09:48,890 --> 00:09:52,940
특히 H100의 텐서 코어는 입력 매트릭스 A가

255
00:09:52,940 --> 00:09:57,180
16x4, 입력 매트릭스 B가 4x8이고, 크기 16x8의

256
00:09:57,180 --> 00:10:01,440
바이어스 매트릭스가 더해지는 연산을 수행할 수 있습니다.

257
00:10:01,440 --> 00:10:03,920
즉, a, x, b가 이 고정 크기의

258
00:10:03,920 --> 00:10:06,900
작은 매트릭스 조각인 ax 더하기 b를 수행합니다.

259
00:10:06,900 --> 00:10:09,320
그리고 이 작은 매트릭스 곱셈 조각을 텐서

260
00:10:09,320 --> 00:10:12,560
코어 하나당 클럭 사이클마다 한 번 수행할 수 있습니다.

261
00:10:12,560 --> 00:10:15,390
이 숫자들을 모두 곱해보면, 이 특정

262
00:10:15,390 --> 00:10:19,850
크기의 ax 더하기 b 매트릭스 곱셈은 1,024개의

263
00:10:19,850 --> 00:10:23,450
부동소수점 연산에 해당합니다. 여기서 곱셈과

264
00:10:23,450 --> 00:10:25,130
덧셈 각각을 하나의

265
00:10:25,130 --> 00:10:27,300
부동소수점 연산으로 셉니다.

266
00:10:27,300 --> 00:10:30,360
이걸 SM 내 4개의 텐서 코어에 곱하면,

267
00:10:30,360 --> 00:10:34,310
텐서 코어를 사용하는 SM 하나가 클럭

268
00:10:34,310 --> 00:10:39,350
사이클당 SM당 4,096개의 부동소수점 연산을 수행할

269
00:10:39,350 --> 00:10:40,460
수 있습니다.

270
00:10:40,460 --> 00:10:42,660
이걸 FP32 코어가 할 수 있는

271
00:10:42,660 --> 00:10:44,590
256과 비교해야 합니다.

272
00:10:44,590 --> 00:10:46,980
텐서 코어가 모든 마법이 일어나는

273
00:10:46,980 --> 00:10:48,622
곳이라는 점을 알 수 있습니다.

274
00:10:48,622 --> 00:10:50,580
이곳이 장치의 주요 처리량이 나오는

275
00:10:50,580 --> 00:10:51,163
곳입니다.

276
00:10:51,163 --> 00:10:53,580
이 GPU에서 최대 성능을

277
00:10:53,580 --> 00:10:56,290
내고 싶다면, 이 텐서

278
00:10:56,290 --> 00:10:59,297
코어를 최대한 활용해야 합니다.

279
00:10:59,297 --> 00:11:01,380
텐서 코어의 또 다른 흥미로운 점은

280
00:11:01,380 --> 00:11:03,452
혼합 정밀도로 작동한다는 것입니다.

281
00:11:03,452 --> 00:11:05,410
일반적인 32비트 부동소수점

282
00:11:05,410 --> 00:11:08,760
대신, 텐서 코어는 보통 16비트 입력을

283
00:11:08,760 --> 00:11:11,903
사용하는 혼합 정밀도 방식을 씁니다.

284
00:11:11,903 --> 00:11:14,070
여러 가지 흥미로운 16비트

285
00:11:14,070 --> 00:11:17,320
포맷이 있는데, 오늘은 자세히 다루지 않겠습니다.

286
00:11:17,320 --> 00:11:20,100
곱셈은 낮은 정밀도인 16비트로

287
00:11:20,100 --> 00:11:22,450
수행하고, 덧셈과 누적은 더 높은

288
00:11:22,450 --> 00:11:24,820
정밀도인 32비트로 합니다.

289
00:11:24,820 --> 00:11:29,170
즉, 텐서 코어는 낮은 정밀도 16비트 입력을 받아

290
00:11:29,170 --> 00:11:31,170
중간 계산을 하고, 출력은

291
00:11:31,170 --> 00:11:34,290
더 높은 정밀도 32비트로 만듭니다.

292
00:11:34,290 --> 00:11:37,660
이 점이 중요한데, PyTorch 레이어에서

293
00:11:37,660 --> 00:11:40,760
모델을 16비트로 변환하지 않으면, 부동소수점

294
00:11:40,760 --> 00:11:42,580
코어에서 실행되어,

295
00:11:42,580 --> 00:11:45,080
예상보다 20배 느리게 실행됩니다.

296
00:11:45,080 --> 00:11:47,690
이건 사소한 것처럼 보일 수 있지만,

297
00:11:47,690 --> 00:11:50,530
PyTorch 코드에서 데이터 타입을

298
00:11:50,530 --> 00:11:53,290
잘못 다루면 매우 실감할 수 있습니다.

299
00:11:53,290 --> 00:11:55,910
그리고 참고로, GPU는 정말

300
00:11:55,910 --> 00:11:58,930
빠르고, 지난 10년에서 15년 사이에

301
00:11:58,930 --> 00:12:02,200
얼마나 빨라졌는지 정말 놀랍습니다.

302
00:12:02,200 --> 00:12:05,470
제가 박사 과정을 시작하고 딥러닝을 연구할

303
00:12:05,470 --> 00:12:08,110
때, 우리가 모두 사용하던 최첨단

304
00:12:08,110 --> 00:12:12,290
GPU는 2013년에 출시된 K40 GPU였습니다.

305
00:12:12,290 --> 00:12:16,420
이 장치는 전체 장치에서 FP32 연산을 5 테라플롭스 정도

306
00:12:16,420 --> 00:12:17,662
수행할 수 있었습니다.

307
00:12:17,662 --> 00:12:19,370
자, 이제 그래프를 설명해야겠네요.

308
00:12:19,370 --> 00:12:23,260
x축은 2013년부터 현재까지의

309
00:12:23,260 --> 00:12:24,380
시간입니다.

310
00:12:24,380 --> 00:12:26,500
y축은 각 장치의

311
00:12:26,500 --> 00:12:31,180
초당 테라플롭스 단위로 측정된 최대

312
00:12:31,180 --> 00:12:33,040
처리량입니다.

313
00:12:33,040 --> 00:12:35,870
그래프가 많이 상승하는 걸 볼 수 있습니다.

314
00:12:35,870 --> 00:12:37,900
하지만 여기서 주목할 점은 K40에서

315
00:12:37,900 --> 00:12:40,600
P100으로 넘어가는 과정에서 정말

316
00:12:40,600 --> 00:12:42,820
놀라운 일이 있었는데, 제 박사

317
00:12:42,820 --> 00:12:47,200
과정 말기인 2016년, 2017년쯤에 나온 V100이 바로 그

318
00:12:47,200 --> 00:12:49,290
주인공입니다. V100은 최초로 텐서

319
00:12:49,290 --> 00:12:50,970
코어를 도입한 장치였습니다.

320
00:12:50,970 --> 00:12:54,960
그 이후로 최신 장치들은 더 많은 텐서 코어,

321
00:12:54,960 --> 00:12:57,970
더 큰 텐서 코어, 그리고 장치

322
00:12:57,970 --> 00:13:00,310
면적의 더 많은 부분을 텐서

323
00:13:00,310 --> 00:13:02,775
코어에 할당하면서 지난

324
00:13:02,775 --> 00:13:04,900
10~15년간 처리량이 엄청나게

325
00:13:04,900 --> 00:13:06,270
증가했습니다.

326
00:13:06,270 --> 00:13:09,000
가장 최신 장치는 공식 발표된

327
00:13:09,000 --> 00:13:10,750
B200입니다.

328
00:13:10,750 --> 00:13:12,310
현재 천천히 보급되고 있습니다.

329
00:13:12,310 --> 00:13:21,600
이 장치는 이론상 FP32 연산에서 초당 약 83.3 테라플롭스, 텐서
코어에서 혼합 정밀도

330
00:13:21,600 --> 00:13:25,560
연산으로는 이론상 초당 5,000

331
00:13:25,560 --> 00:13:28,270
테라플롭스를 수행할 수 있습니다.

332
00:13:28,270 --> 00:13:31,290
한 걸음 물러서서 보면, 지난

333
00:13:31,290 --> 00:13:35,580
12년간 계산 능력이 1,000배나 증가한 시대를 살아온

334
00:13:35,580 --> 00:13:36,640
셈입니다.

335
00:13:36,640 --> 00:13:38,625
이건 단지 장치 하나당 수준의 이야기입니다.

336
00:13:38,625 --> 00:13:42,220
그래서 지난 10년간 AI가 이렇게 발전한 이유 중 하나는,

337
00:13:42,220 --> 00:13:43,660
무슨 일이 있었냐는 겁니다.

338
00:13:43,660 --> 00:13:44,840
이것이 답입니다.

339
00:13:44,840 --> 00:13:47,110
지금 우리는 계산 자원을 활용하고

340
00:13:47,110 --> 00:13:50,390
있는데, 지난 10년 동안 1,000배나 증가했습니다.

341
00:13:50,390 --> 00:13:53,375
세상에서 어떤 것이 1,000배 변하면 반드시

342
00:13:53,375 --> 00:13:55,750
주목해야 합니다. 그 변화가 우리의

343
00:13:55,750 --> 00:13:58,700
기술 능력에 큰 변화를 가져올 것이기 때문입니다.

344
00:13:58,700 --> 00:14:01,010
이 1,000배 향상이 지난

345
00:14:01,010 --> 00:14:03,610
10년간 딥러닝 발전의 주요

346
00:14:03,610 --> 00:14:05,290
원동력이라고 생각합니다.

347
00:14:05,290 --> 00:14:08,840
그래서 5,000개의 텐서 코어가 있는 것은 아닙니다.

348
00:14:08,840 --> 00:14:12,660
텐서 코어에서 5,000 테라플롭스의 연산 능력을 가진 것입니다.

349
00:14:12,660 --> 00:14:13,160
네.

350
00:14:13,160 --> 00:14:15,285
그래서 우리는 항상 텐서 코어의

351
00:14:15,285 --> 00:14:19,330
연산 능력과 FP32 코어의 연산 능력을 구분하려고 합니다.

352
00:14:19,330 --> 00:14:20,960
이건 이미 엄청난 수준입니다.

353
00:14:20,960 --> 00:14:23,650
손에 들 수 있는 장치에서 1,000배 증가했다는

354
00:14:23,650 --> 00:14:25,790
사실 자체가 이미 미친 거죠.

355
00:14:25,790 --> 00:14:29,620
저는 K40을 손에 들어본 적이 있지만, B100을 들어볼

356
00:14:29,620 --> 00:14:30,620
기회는 없었습니다.

357
00:14:30,620 --> 00:14:33,110
하지만 두 장치는 같은 물리적 객체처럼 느껴집니다.

358
00:14:33,110 --> 00:14:35,490
크기도 비슷하고, 무게도 비슷하며, 생김새도

359
00:14:35,490 --> 00:14:38,490
비슷하지만, 오늘날의 장치는 12년 전 것보다 1,

360
00:14:38,490 --> 00:14:39,640
000배 빠릅니다.

361
00:14:39,640 --> 00:14:40,830
그건 정말 미친 수준입니다.

362
00:14:40,830 --> 00:14:44,980
하지만 더 미친 것은, 우리는 한 개의 GPU만으로 학습하지 않는다는
점입니다.

363
00:14:44,980 --> 00:14:48,250
2013년에 K40이 처음 나왔을 때만 해도, 많은

364
00:14:48,250 --> 00:14:51,400
모델을 한 개의 GPU에서 학습하는 것이 흔했습니다.

365
00:14:51,400 --> 00:14:53,740
하지만 오늘날에는 한 개의 GPU만 사용하는 것이 아닙니다.

366
00:14:53,740 --> 00:14:55,780
수천, 수만, 때로는 수십만

367
00:14:55,780 --> 00:14:57,730
개의 GPU가 함께

368
00:14:57,730 --> 00:15:00,482
하나의 모델을 훈련시키고 있습니다.

369
00:15:00,482 --> 00:15:03,870
여기에 장치당 처리량이 1,000배 증가한 것을

370
00:15:03,870 --> 00:15:04,720
더해보세요.

371
00:15:04,720 --> 00:15:08,520
지난 10년간 정말 미친 일이 벌어진 겁니다.

372
00:15:08,520 --> 00:15:13,030
그래서 우리는 GPU 내부를 살펴봤습니다.

373
00:15:13,030 --> 00:15:16,360
이제 여기서 확대해서 GPU를 개별 장치가

374
00:15:16,360 --> 00:15:17,980
아닌, 여러 장치를

375
00:15:17,980 --> 00:15:19,860
연결해 만든 현대 GPU

376
00:15:19,860 --> 00:15:22,300
클러스터의 맥락에서 보겠습니다.

377
00:15:22,300 --> 00:15:26,427
우리는 이미 단일 H100 GPU를 봤습니다.

378
00:15:26,427 --> 00:15:28,260
여기서는 이것을 또 다른 수준의 메모리

379
00:15:28,260 --> 00:15:29,670
계층 구조로 생각할 수 있습니다.

380
00:15:29,670 --> 00:15:31,680
H100 내부에는 컴퓨트

381
00:15:31,680 --> 00:15:33,742
요소에 가까워질수록 세 층의

382
00:15:33,742 --> 00:15:35,450
메모리 계층이 있었습니다.

383
00:15:35,450 --> 00:15:37,520
컴퓨트 요소에서 멀어질수록

384
00:15:37,520 --> 00:15:39,020
메모리 대역폭, 즉

385
00:15:39,020 --> 00:15:41,020
시스템 내 여러 부분 간 비트를

386
00:15:41,020 --> 00:15:43,460
이동시키는 능력이 느려집니다.

387
00:15:43,460 --> 00:15:45,430
이 경향은 단일 장치

388
00:15:45,430 --> 00:15:47,650
범위를 벗어나 전체 데이터

389
00:15:47,650 --> 00:15:50,480
센터 맥락에서도 계속됩니다.

390
00:15:50,480 --> 00:15:54,250
여기서 단일 H100 GPU는 약 3테라바이트의 메모리

391
00:15:54,250 --> 00:15:55,640
대역폭을 가집니다.

392
00:15:55,640 --> 00:15:59,260
GPU 메모리, 즉 자체 HBM 메모리에서 컴퓨트 요소로

393
00:15:59,260 --> 00:16:01,640
초당 3테라바이트를 이동시킬 수 있습니다.

394
00:16:01,640 --> 00:16:03,100
비트를 이동시킬 수 있는 거죠.

395
00:16:03,100 --> 00:16:05,890
하지만 이들은 보통 GPU 서버 안에 있습니다.

396
00:16:05,890 --> 00:16:10,120
거의 모든 GPU 서버는 하나의 큰 박스에 8개의 장치를 가지고 있고,

397
00:16:10,120 --> 00:16:12,560
이 GPU들은 서로 통신할 수 있습니다.

398
00:16:12,560 --> 00:16:14,140
서버 내 어느 한

399
00:16:14,140 --> 00:16:16,540
GPU에서 다른 GPU로는 보통

400
00:16:16,540 --> 00:16:20,060
초당 약 900기가바이트 속도로 통신합니다.

401
00:16:20,060 --> 00:16:23,860
즉, 단일 장치 내부에서 GPU가

402
00:16:23,860 --> 00:16:29,050
통신하는 것보다 약 3배 낮은 메모리 통신 대역폭입니다.

403
00:16:29,050 --> 00:16:32,580
여기서 다시 Llama3를 봅니다.

404
00:16:32,580 --> 00:16:34,597
많은 주요 업체들은 훈련 클러스터에 대한

405
00:16:34,597 --> 00:16:36,430
자세한 내용을 공개하지 않지만,

406
00:16:36,430 --> 00:16:38,340
Llama3 기술 보고서는 훈련 클러스터에

407
00:16:38,340 --> 00:16:40,300
관한 많은 세부 정보를 제공했습니다.

408
00:16:40,300 --> 00:16:41,790
여기서 클러스터마다

409
00:16:41,790 --> 00:16:44,260
구체적인 사항은 조금 다를 수 있습니다.

410
00:16:44,260 --> 00:16:46,710
하지만 이 숫자들은 Llama3

411
00:16:46,710 --> 00:16:49,860
클러스터에서 모델 훈련에 사용된 데이터입니다.

412
00:16:49,860 --> 00:16:52,660
한 GPU 박스에는 GPU가 들어있고,

413
00:16:52,660 --> 00:16:55,530
이 박스 두 개를 하나의 서버 랙에 쌓습니다.

414
00:16:55,530 --> 00:16:57,280
서버 랙을 본 적

415
00:16:57,280 --> 00:17:00,120
없으면, 대략 6피트 높이로 사람

416
00:17:00,120 --> 00:17:02,680
크기 정도라고 생각하면 됩니다.

417
00:17:02,680 --> 00:17:04,740
한 서버 랙에는 두 개의 서버가

418
00:17:04,740 --> 00:17:07,410
있고, 총 16개의 GPU가 있습니다.

419
00:17:07,410 --> 00:17:11,550
그리고 여러 서버 랙을 연결해 GPU 팟을 만듭니다.

420
00:17:11,550 --> 00:17:16,230
Llama3 클러스터의 GPU 팟은 192개의 랙으로 구성되어

421
00:17:16,230 --> 00:17:19,210
총 3,072개의 GPU가 있습니다.

422
00:17:19,210 --> 00:17:21,480
이 랙들 사이에는 매우 높은

423
00:17:21,480 --> 00:17:23,589
대역폭의 커넥터가 있습니다.

424
00:17:23,589 --> 00:17:27,599
그 결과, 팟 내 어느 두 GPU도 초당 약

425
00:17:27,599 --> 00:17:30,140
50기가바이트 속도로 통신할 수

426
00:17:30,140 --> 00:17:30,870
있습니다.

427
00:17:30,870 --> 00:17:34,100
이제 보시다시피, 개별 서버 내

428
00:17:34,100 --> 00:17:37,220
통신과 팟 내 전체 GPU 간 통신

429
00:17:37,220 --> 00:17:40,490
사이에 메모리 트래픽이 20배나

430
00:17:40,490 --> 00:17:41,330
감소했습니다.

431
00:17:41,330 --> 00:17:44,100
3,072개의 GPU는 많은 컴퓨팅 파워처럼

432
00:17:44,100 --> 00:17:46,310
보이지만, 전혀 충분하지 않습니다.

433
00:17:46,310 --> 00:17:48,590
그래서 우리는 그 GPU 팟들을 모아서 전체

434
00:17:48,590 --> 00:17:50,390
GPU 클러스터를 만들 것입니다.

435
00:17:50,390 --> 00:17:52,520
이것이 바로 Meta가 Llama3

436
00:17:52,520 --> 00:17:55,650
모델을 훈련시키기 위해 만든 전체 GPU 클러스터입니다.

437
00:17:55,650 --> 00:17:58,670
이 장치는 8개의 GPU 팟을 합쳐 총

438
00:17:58,670 --> 00:18:01,790
24,576개의 GPU를 가지고 있습니다.

439
00:18:01,790 --> 00:18:04,610
이 장치들 간의 메모리 트래픽에 대한 정확한

440
00:18:04,610 --> 00:18:06,530
수치는 찾지 못했지만,

441
00:18:06,530 --> 00:18:08,870
분명 초당 50기가바이트 이하입니다.

442
00:18:08,870 --> 00:18:12,050
참고로, 이것이 세계에서 가장 큰 GPU

443
00:18:12,050 --> 00:18:13,522
클러스터는 아닙니다.

444
00:18:13,522 --> 00:18:15,230
정확한 수치를 빠르게 찾을 수 있었던

445
00:18:15,230 --> 00:18:16,270
가장 큰 클러스터입니다.

446
00:18:16,270 --> 00:18:18,770
하지만 분명 세계에는 50,000개, 100,

447
00:18:18,770 --> 00:18:21,930
000개의 GPU를 가진 클러스터도 존재합니다.

448
00:18:21,930 --> 00:18:24,555
그런 클러스터들이 존재하고, 사람들이 그 위에서 모델을 훈련시키고 있습니다.

449
00:18:24,555 --> 00:18:27,180
이 방식은 자연스럽게 확장할 수 있습니다.

450
00:18:27,180 --> 00:18:29,730
즉, 더 많은 팟을 모아서 더 큰

451
00:18:29,730 --> 00:18:31,818
클러스터를 만들거나, 슈퍼 팟들이

452
00:18:31,818 --> 00:18:33,360
다른 슈퍼 팟과

453
00:18:33,360 --> 00:18:35,220
연결되는 또 다른 계층 구조를

454
00:18:35,220 --> 00:18:38,310
만들어 한 단계 더 확장할 수 있습니다.

455
00:18:38,310 --> 00:18:40,560
그 GPU 클러스터로 얼마나 오래 훈련하나요?

456
00:18:40,560 --> 00:18:43,300
Llama3 모델에 대해서는 바로 기억나지

457
00:18:43,300 --> 00:18:45,030
않지만, 지난 10년간의

458
00:18:45,030 --> 00:18:48,090
경험으로 보면 가장 오래 훈련하는 모델들은

459
00:18:48,090 --> 00:18:49,972
보통 몇 달 단위입니다.

460
00:18:49,972 --> 00:18:51,930
이것은 기술적인 문제보다는 사람과

461
00:18:51,930 --> 00:18:53,070
관련이 더 큽니다.

462
00:18:53,070 --> 00:18:57,120
진행 상황 관리, 계획 수립, 작업자 관리 측면에서

463
00:18:57,120 --> 00:18:59,130
매우 긴 훈련 기간을

464
00:18:59,130 --> 00:19:01,543
유지하는 것은 매우 어렵습니다.

465
00:19:01,543 --> 00:19:03,960
그래서 가장 긴 훈련 기간과 최신

466
00:19:03,960 --> 00:19:06,360
모델들은 보통 몇 달 단위로 측정됩니다.

467
00:19:06,360 --> 00:19:09,010
아마도 가장 큰 모델들, 예를 들어 GPT-4.

468
00:19:09,010 --> 00:19:13,140
5나 GPT-5 같은 경우는 지금쯤 거의 1년에 가까운 훈련 기간일

469
00:19:13,140 --> 00:19:14,340
수도 있다고 생각합니다.

470
00:19:14,340 --> 00:19:16,350
하지만 정말 큰 훈련

471
00:19:16,350 --> 00:19:18,960
클러스터에서 몇 달씩 걸리는

472
00:19:18,960 --> 00:19:20,760
훈련도 꽤 흔합니다.

473
00:19:20,760 --> 00:19:23,190
질문은, 왜 서버를 포드가 아니라 랙에

474
00:19:23,190 --> 00:19:24,310
배치하느냐는 겁니다.

475
00:19:24,310 --> 00:19:25,660
어딘가에 두어야 하니까요.

476
00:19:25,660 --> 00:19:27,640
물리적인 제약이 있기 때문입니다.

477
00:19:27,640 --> 00:19:30,820
그래서 서버 랙은 수십 년 동안 데이터

478
00:19:30,820 --> 00:19:33,500
센터에서 표준 단위로 사용되어 왔습니다.

479
00:19:33,500 --> 00:19:36,970
GPU 같은 새로운 장치가 등장하면서 다른

480
00:19:36,970 --> 00:19:38,737
종류의 서버가 생겼습니다.

481
00:19:38,737 --> 00:19:40,070
물리적으로 훨씬 더 큽니다.

482
00:19:40,070 --> 00:19:41,770
전력도 훨씬 많지만, 데이터

483
00:19:41,770 --> 00:19:45,470
센터를 하루아침에 완전히 새로 설계할 수는 없습니다.

484
00:19:45,470 --> 00:19:48,700
그래서 서버 랙은 표준 하드웨어 크기와

485
00:19:48,700 --> 00:19:50,620
함께 데이터 센터가 보통

486
00:19:50,620 --> 00:19:53,380
구축되는 표준 단위로 남아 있습니다.

487
00:19:53,380 --> 00:19:56,360
클러스터 같은 게 물리적으로 얼마나 공간을

488
00:19:56,360 --> 00:19:56,860
차지하나요?

489
00:19:56,860 --> 00:19:59,800
아, 좋은 질문입니다.

490
00:19:59,800 --> 00:20:02,560
서버 랙 하나는 대략

491
00:20:02,560 --> 00:20:06,410
6~8피트, 이 정도 크기라고 생각하시면

492
00:20:06,410 --> 00:20:07,580
됩니다.

493
00:20:07,580 --> 00:20:10,600
그래서 서버 랙은 이 연단 크기쯤

494
00:20:10,600 --> 00:20:13,120
되고, 제 키 정도 높이입니다.

495
00:20:13,120 --> 00:20:15,530
그리고 포드에는 192개의 랙이 있습니다.

496
00:20:15,530 --> 00:20:17,733
그러니까 이 연단 200개 정도를 상상해 보세요.

497
00:20:17,733 --> 00:20:18,650
그럼 얼마나 클까요?

498
00:20:18,650 --> 00:20:20,320
그리고 그걸 8배 곱합니다.

499
00:20:20,320 --> 00:20:22,660
하지만 사실 이건 조금 과소평가한 거예요,

500
00:20:22,660 --> 00:20:24,820
왜냐하면 보통은 사람들이 그 사이를 걸어

501
00:20:24,820 --> 00:20:26,780
다닐 수 있도록 행으로 배치하거든요.

502
00:20:26,780 --> 00:20:28,197
그리고 클러스터에 넣어야 할

503
00:20:28,197 --> 00:20:31,600
하드웨어가 더 많아요, 단순히 컴퓨트 랙만 있는 게 아니에요.

504
00:20:31,600 --> 00:20:34,330
그래서 물리적인 GPU 서버가 있는 컴퓨트 랙 외에도

505
00:20:34,330 --> 00:20:35,980
네트워킹 하드웨어를 담은

506
00:20:35,980 --> 00:20:37,310
다른 랙들이 있을 겁니다.

507
00:20:37,310 --> 00:20:40,270
이 장치들 사이에 엄청난 양의 데이터가

508
00:20:40,270 --> 00:20:41,403
오가야 하거든요.

509
00:20:41,403 --> 00:20:42,820
그래서 네트워킹 하드웨어만 담는

510
00:20:42,820 --> 00:20:44,070
전용 랙들이 있을 겁니다.

511
00:20:44,070 --> 00:20:45,487
또 저장 장비만 담는 전용

512
00:20:45,487 --> 00:20:47,350
랙들도 있을 텐데, 훈련 데이터를

513
00:20:47,350 --> 00:20:49,060
어딘가에 저장하고 장치로

514
00:20:49,060 --> 00:20:50,030
전달해야 하니까요.

515
00:20:50,030 --> 00:20:52,860
그래서 이런 것들이 꽤 많은 공간을 차지할 수 있습니다.

516
00:20:52,860 --> 00:20:53,360
네, 맞아요.

517
00:20:53,360 --> 00:20:55,985
질문입니다, 이렇게 큰 클러스터에 가면 작은

518
00:20:55,985 --> 00:20:58,610
컴퓨트 단위들도 높은 처리량을 유지하나요?

519
00:20:58,610 --> 00:20:59,510
네, 유지합니다.

520
00:20:59,510 --> 00:21:02,740
이게 바로 이런 시스템을 설계할 때의 비밀이자

521
00:21:02,740 --> 00:21:04,340
도전 과제인데요,

522
00:21:04,340 --> 00:21:06,340
이상적으로는 빠른 통신을

523
00:21:06,340 --> 00:21:08,043
최대한 활용하고 싶지만,

524
00:21:08,043 --> 00:21:10,210
확장할 때는 느린 통신으로도

525
00:21:10,210 --> 00:21:12,850
원활하게 전환할 수 있어야 합니다.

526
00:21:12,850 --> 00:21:14,410
아, 얼마나 뜨거워지나요?

527
00:21:14,410 --> 00:21:16,165
꽤 뜨거워집니다.

528
00:21:16,165 --> 00:21:19,165
만약 여러분 중에 게이머가 있고 집에 4090

529
00:21:19,165 --> 00:21:23,540
GPU나 5080 GPU가 있다면, 단일 4090 GPU가 게임을

530
00:21:23,540 --> 00:21:25,540
할 때 방을 뜨겁게 만들어서

531
00:21:25,540 --> 00:21:27,250
창문을 열고 싶어질 겁니다.

532
00:21:27,250 --> 00:21:29,830
물리적으로 방을 더 따뜻하게 만듭니다.

533
00:21:29,830 --> 00:21:31,780
그러니까 단일 게이밍 GPU가 평균

534
00:21:31,780 --> 00:21:33,620
크기의 방 하나에 할 수 있는

535
00:21:33,620 --> 00:21:35,230
일을 상상해 보세요. 네,

536
00:21:35,230 --> 00:21:37,605
수만 개를 대형 데이터 센터에 쌓으면 냉각

537
00:21:37,605 --> 00:21:38,625
요구가 엄청나집니다.

538
00:21:41,515 --> 00:21:43,390
또 다른 흥미로운 점은 냉각이

539
00:21:43,390 --> 00:21:44,810
정말 미쳐버린다는 겁니다.

540
00:21:44,810 --> 00:21:46,360
게이밍 데스크탑은 보통 공기

541
00:21:46,360 --> 00:21:48,020
냉각을 하고, 때로는 수냉을 하죠.

542
00:21:48,020 --> 00:21:50,452
그리고 다양한 냉각 시스템을 설계할 수 있습니다.

543
00:21:50,452 --> 00:21:51,910
여기서 하드웨어를

544
00:21:51,910 --> 00:21:54,400
엄청나게 최적화하려고 할 수도 있죠.

545
00:21:54,400 --> 00:21:54,950
좋습니다.

546
00:21:54,950 --> 00:21:56,450
이게 정말 멋지다고 생각합니다.

547
00:21:56,450 --> 00:21:59,355
이 GPU들이 클라우드에 떠다니는 신화 속

548
00:21:59,355 --> 00:22:01,730
존재가 아니라는 걸 상상하는 거죠.

549
00:22:01,730 --> 00:22:04,313
이건 실제로 누군가가 만들고 방 한 켠에 쌓아둔

550
00:22:04,313 --> 00:22:05,390
물리적인 원자들입니다.

551
00:22:05,390 --> 00:22:08,290
그 모습이 정말 궁금하죠.

552
00:22:08,290 --> 00:22:12,070
그래서 큰 GPU 클러스터로 넘어가면서

553
00:22:12,070 --> 00:22:13,780
한 가지 사고방식

554
00:22:13,780 --> 00:22:16,580
전환이 있었는데, 개별

555
00:22:16,580 --> 00:22:17,930
장치나 서버보다는

556
00:22:17,930 --> 00:22:20,080
전체 데이터 센터를 하나의

557
00:22:20,080 --> 00:22:22,250
거대한 컴퓨터로 생각하는 겁니다.

558
00:22:22,250 --> 00:22:25,860
이 거대한 컴퓨터에는 24,000개의 GPU,

559
00:22:25,860 --> 00:22:29,040
GPU에 1.8테라바이트 HBM 메모리, 4억

560
00:22:29,040 --> 00:22:33,750
1,500만 FP32 코어, 1,300만 텐서 코어가 있습니다.

561
00:22:33,750 --> 00:22:37,830
이 전체 시스템은 초당 24 엑사플롭스의 연산을 수행할 수 있습니다.

562
00:22:37,830 --> 00:22:39,710
24 곱하기 10의 18제곱이죠.

563
00:22:39,710 --> 00:22:41,070
엄청난 플롭스입니다.

564
00:22:41,070 --> 00:22:42,745
엄청난 플롭스 수치지만, 5년

565
00:22:42,745 --> 00:22:44,120
후에는 이게 그렇게 많게

566
00:22:44,120 --> 00:22:47,270
느껴지지 않을 거라고 장담합니다. 그게 더 놀라운 부분이죠.

567
00:22:47,270 --> 00:22:50,480
여기서 우리의 목표는 이 24,000개의 GPU

568
00:22:50,480 --> 00:22:54,120
전체를 하나의 거대한 슈퍼컴퓨터로 생각하는 겁니다.

569
00:22:54,120 --> 00:22:56,120
그럼 질문은, 이 거대한

570
00:22:56,120 --> 00:22:58,190
슈퍼컴퓨터에서 한 신경망을 몇 달씩

571
00:22:58,190 --> 00:23:00,700
어떻게 훈련시킬 수 있을까 하는 거죠.

572
00:23:00,700 --> 00:23:02,450
그리고 엄청나게 강력한, 방대한

573
00:23:02,450 --> 00:23:05,390
데이터를 흡수할 수 있는 거대한 신경망을 훈련시키는 겁니다.

574
00:23:05,390 --> 00:23:07,610
이게 바로 우리가 딥러닝에서

575
00:23:07,610 --> 00:23:10,640
옮겨온 질문이자 패러다임입니다.

576
00:23:10,640 --> 00:23:12,955
참고로, 제가 계속 GPU, NVIDIA라고

577
00:23:12,955 --> 00:23:14,330
하는 이유는

578
00:23:14,330 --> 00:23:18,090
오늘날 가장 지배적인 훈련 아키텍처와 하드웨어이기 때문입니다.

579
00:23:18,090 --> 00:23:20,240
하지만 다른 경쟁자들도 등장했습니다.

580
00:23:20,240 --> 00:23:23,810
지금 NVIDIA 훈련 하드웨어의 가장 큰 경쟁자는

581
00:23:23,810 --> 00:23:24,900
구글이라고 생각합니다.

582
00:23:24,900 --> 00:23:26,810
구글은 Tensor Processing Units,

583
00:23:26,810 --> 00:23:28,800
즉 TPU라는 자체 하드웨어를 가지고 있습니다.

584
00:23:28,800 --> 00:23:30,890
이 TPU들은 정말 훌륭합니다.

585
00:23:30,890 --> 00:23:34,220
이미 6세대까지 발전했죠.

586
00:23:34,220 --> 00:23:37,130
이것은 오늘날 구글 클라우드에서 임대할 수

587
00:23:37,130 --> 00:23:39,020
있는 v5p TPU의 사양입니다.

588
00:23:39,020 --> 00:23:41,780
대략 우리가 방금 얘기한 H100과

589
00:23:41,780 --> 00:23:44,150
비슷한 규모와 사양입니다.

590
00:23:44,150 --> 00:23:46,400
TPU에는 GPU와는 꽤 다른 흥미로운

591
00:23:46,400 --> 00:23:48,350
설계 결정들이 있는데,

592
00:23:48,350 --> 00:23:50,308
매우 흥미롭지만 오늘은 다룰 시간이

593
00:23:50,308 --> 00:23:51,058
없습니다.

594
00:23:51,058 --> 00:23:53,100
누군가 크기가 얼마나 되는지 물었는데,

595
00:23:53,100 --> 00:23:54,500
이게 실제 사진입니다.

596
00:23:54,500 --> 00:23:57,920
GPU처럼 TPU도 팟(pod)으로

597
00:23:57,920 --> 00:24:05,030
배열되며, v5p TPU는 최대 8960개의 칩으로 팟을 구성할 수
있습니다.

598
00:24:05,030 --> 00:24:07,670
이 사진은 V2 TPU 팟으로,

599
00:24:07,670 --> 00:24:10,440
칩이 256개뿐입니다.

600
00:24:10,440 --> 00:24:13,290
그래서 이게 얼마나 큰지 감이 오시죠,

601
00:24:13,290 --> 00:24:14,220
각각의 칩이요.

602
00:24:14,220 --> 00:24:15,930
사진에 랙이 네 개 보이는데,

603
00:24:15,930 --> 00:24:19,080
이 랙들은 제 키보다 조금 더

604
00:24:19,080 --> 00:24:19,830
크고,

605
00:24:19,830 --> 00:24:22,938
네 개가 나란히 있어 256개의 TPU 칩을 담고 있습니다.

606
00:24:22,938 --> 00:24:25,230
이제 이게 최신 팟에서는 거의

607
00:24:25,230 --> 00:24:29,015
9,000개 칩까지 훨씬 커진다고 상상해 보세요.

608
00:24:29,015 --> 00:24:31,140
네, 구글의 Gemini 모델들은 거의

609
00:24:31,140 --> 00:24:32,898
확실히 TPU에서 훈련됐을 겁니다.

610
00:24:32,898 --> 00:24:34,440
물론 공개하지는

611
00:24:34,440 --> 00:24:38,520
않지만, 그렇지 않다면 정말 놀랄 겁니다.

612
00:24:38,520 --> 00:24:41,760
그리고 말씀드렸듯이 TPU는 정말 좋은 하드웨어입니다.

613
00:24:41,760 --> 00:24:43,620
대부분의 대규모 구글 모델이 이 TPU에서

614
00:24:43,620 --> 00:24:45,203
훈련된다고 추정하며, 이

615
00:24:45,203 --> 00:24:46,500
모델들은 매우 경쟁력 있습니다.

616
00:24:46,500 --> 00:24:48,812
정말 좋은 훈련 하드웨어죠.

617
00:24:48,812 --> 00:24:50,770
NVIDIA와 다른 점은 TPU는 구매할 수 없다는 겁니다.

618
00:24:50,770 --> 00:24:53,580
TPU에 접근하는 유일한 방법은 구글에서 일하거나 구글

619
00:24:53,580 --> 00:24:55,408
클라우드를 통해 임대하는 것뿐입니다.

620
00:24:55,408 --> 00:24:57,450
하지만 매우 좋은 하드웨어이고 많은

621
00:24:57,450 --> 00:24:58,950
사람들이 사용하고 있지만,

622
00:24:58,950 --> 00:25:03,600
오늘날에는 여전히 NVIDIA GPU보다 조금 덜 대중적이라고 생각합니다.

623
00:25:03,600 --> 00:25:05,310
그리고 물론 다른 회사들도 이게 매우

624
00:25:05,310 --> 00:25:07,090
중요한 기술임을 잘 알고 있습니다.

625
00:25:07,090 --> 00:25:08,160
그래서 경쟁력 있는 학습

626
00:25:08,160 --> 00:25:10,452
하드웨어를 만들려고 하는 다른 많은 회사들이 있습니다.

627
00:25:10,452 --> 00:25:12,570
하지만 제 솔직한 평가로는 지금으로서는

628
00:25:12,570 --> 00:25:16,715
아마 NVIDIA와 TPU가 두 가지 큰 축인 것 같습니다.

629
00:25:16,715 --> 00:25:18,590
이들은 사용성, 성능, 그리고 시장

630
00:25:18,590 --> 00:25:21,640
점유율 면에서 현재 다른 모든 회사들보다 훨씬 앞서

631
00:25:21,640 --> 00:25:22,160
있습니다.

632
00:25:22,160 --> 00:25:23,577
하지만 따라잡으려는 다른

633
00:25:23,577 --> 00:25:25,040
회사들도 많이 있습니다.

634
00:25:25,040 --> 00:25:27,340
두드러진 예로는 AMD가 있습니다.

635
00:25:27,340 --> 00:25:29,680
AMD는 수십 년 동안 두 번째

636
00:25:29,680 --> 00:25:31,340
주요 GPU 제조사였습니다.

637
00:25:31,340 --> 00:25:35,530
그들은 MI325X라는 학습 가속기도 가지고 있습니다.

638
00:25:35,530 --> 00:25:37,330
이론상으로는 H100과

639
00:25:37,330 --> 00:25:39,170
비교해도 꽤 좋은 성능을

640
00:25:39,170 --> 00:25:43,840
가지고 있지만, 현재 H100만큼 큰 영향력을 발휘하지는 못했습니다.

641
00:25:43,840 --> 00:25:45,820
AWS도 Trainium이라는

642
00:25:45,820 --> 00:25:48,070
자체 개발한 학습 칩이 있습니다.

643
00:25:48,070 --> 00:25:49,723
이 칩에 대해서는 잘 모릅니다.

644
00:25:49,723 --> 00:25:51,140
직접 사용해본 적도 없지만,

645
00:25:51,140 --> 00:25:53,630
Anthropic이 일부 학습에 이 칩을 사용한다는 것은 알고 있습니다.

646
00:25:53,630 --> 00:25:55,880
그들의 학습이 전적으로 Trainium인지

647
00:25:55,880 --> 00:25:58,570
아니면 GPU와 병행하는지는 알 수 없습니다.

648
00:25:58,570 --> 00:25:59,960
앞으로 더 많은 제품이 나올 것으로 기대할 수 있습니다.

649
00:25:59,960 --> 00:26:03,860
하지만 오늘날에는 NVIDIA GPU가 아마도 가장 지배적일 것입니다.

650
00:26:03,860 --> 00:26:05,427
그리고 Google TPU도 그 바로 뒤에 있습니다.

651
00:26:05,427 --> 00:26:07,510
이것도 매우 좋지만 아마 NVIDIA

652
00:26:07,510 --> 00:26:10,340
GPU만큼 널리 사용되지는 않는 것 같습니다.

653
00:26:10,340 --> 00:26:10,840
네.

654
00:26:10,840 --> 00:26:12,320
이게 기본적으로 첫 번째 부분입니다.

655
00:26:12,320 --> 00:26:13,338
GPU가 무엇인지?

656
00:26:13,338 --> 00:26:14,880
어떻게 클러스터로 구성하는지?

657
00:26:14,880 --> 00:26:17,090
우리가 구축하고 학습하는 이 기계들의 물리적인

658
00:26:17,090 --> 00:26:18,740
모습을 이해할 수 있도록 설명했습니다.

659
00:26:18,740 --> 00:26:21,110
그 다음 두 번째 질문은, 수만 개의

660
00:26:21,110 --> 00:26:25,190
GPU로 이루어진 거대한 GPU 클러스터를 실제로 활용할 수 있는 알고리즘을

661
00:26:25,190 --> 00:26:27,032
어떻게 작성할 것인가입니다.

662
00:26:27,032 --> 00:26:28,490
이것은 새로운 알고리즘,

663
00:26:28,490 --> 00:26:31,710
컴퓨팅에 대한 새로운 사고방식, 그리고 신경망을 병렬화하고

664
00:26:31,710 --> 00:26:33,800
분할하는 새로운 방법을 개발해야 한다는

665
00:26:33,800 --> 00:26:34,920
것을 의미합니다.

666
00:26:34,920 --> 00:26:36,710
기본 전략은 계산을

667
00:26:36,710 --> 00:26:38,340
분할하는 것입니다.

668
00:26:38,340 --> 00:26:40,410
이 장치들은 거대한 병렬 장치입니다.

669
00:26:40,410 --> 00:26:43,610
우리가 봤듯이 많은 GPU, 많은 CPU 코어, 많은 GPU

670
00:26:43,610 --> 00:26:45,150
코어를 가지고 있습니다.

671
00:26:45,150 --> 00:26:46,760
이들은 모두 독립적으로 작동할 수 있고,

672
00:26:46,760 --> 00:26:48,662
서로 간에 너무 많이 소통하지는 못합니다.

673
00:26:48,662 --> 00:26:50,870
컴퓨터가 실제로 하는 일을 높은 수준에서 생각해보면,

674
00:26:50,870 --> 00:26:53,190
컴퓨터는 기본적으로 두 가지 일을 합니다.

675
00:26:53,190 --> 00:26:55,610
하나는 계산으로, 입력 비트를 받아

676
00:26:55,610 --> 00:26:57,665
새로운 출력 비트를 계산하는

677
00:26:57,665 --> 00:27:00,290
것이고, 다른 하나는 통신으로, 비트를 한

678
00:27:00,290 --> 00:27:01,850
메모리 위치에서 다른

679
00:27:01,850 --> 00:27:03,900
메모리 위치로 옮기는 것입니다.

680
00:27:03,900 --> 00:27:05,450
핵심은 클러스터

681
00:27:05,450 --> 00:27:09,590
전체에 걸친 다양한 메모리 계층을 어떻게

682
00:27:09,590 --> 00:27:11,390
활용해서 통신과

683
00:27:11,390 --> 00:27:14,870
계산을 겹치게 할 것인가입니다.

684
00:27:14,870 --> 00:27:16,690
또한 계산을 분할하고 병렬화해서

685
00:27:16,690 --> 00:27:20,260
거대한 신경망을 학습하는 과정에서 수만 개의 개별 GPU,

686
00:27:20,260 --> 00:27:22,840
수백만 개의 개별 계산 요소가 모두

687
00:27:22,840 --> 00:27:25,700
유용한 작업을 병렬로 수행하도록 하는 것입니다.

688
00:27:25,700 --> 00:27:28,100
그리고 이들이 서로 작업

689
00:27:28,100 --> 00:27:30,605
결과를 소통하게 하여 이 거대한

690
00:27:30,605 --> 00:27:32,980
클러스터에서 거대한 신경망

691
00:27:32,980 --> 00:27:35,740
학습을 성공적으로 수행하는

692
00:27:35,740 --> 00:27:37,720
방법을 찾는 것입니다.

693
00:27:37,720 --> 00:27:40,330
그래서 이를 위해 제가 생각하는 한 가지

694
00:27:40,330 --> 00:27:43,185
방법은, 오늘날 대규모 신경망을 훈련할 때

695
00:27:43,185 --> 00:27:44,560
사람들이 활용하는 병렬성의

696
00:27:44,560 --> 00:27:47,350
정도가 기본적으로 다섯 가지라는 겁니다.

697
00:27:47,350 --> 00:27:49,100
이 중 많은 부분이 트랜스포머에 특화된

698
00:27:49,100 --> 00:27:51,558
내용인데, 트랜스포머가 대규모 훈련에 가장 많이

699
00:27:51,558 --> 00:27:53,000
사용되는 아키텍처이기 때문입니다.

700
00:27:53,000 --> 00:27:54,650
트랜스포머를 생각해보면,

701
00:27:54,650 --> 00:27:58,310
트랜스포머는 기본적으로 L개의 층으로 쌓여 있는 구조입니다.

702
00:27:58,310 --> 00:28:00,190
그리고 그 L개의 각

703
00:28:00,190 --> 00:28:01,870
층은 미니배치 차원

704
00:28:01,870 --> 00:28:05,613
하나를 포함한 3차원 텐서에서 연산을 수행합니다.

705
00:28:05,613 --> 00:28:07,030
우리는 미니배치 내에서 여러

706
00:28:07,030 --> 00:28:09,610
시퀀스를 가지고 있고, 시퀀스 차원이 있습니다.

707
00:28:09,610 --> 00:28:12,140
우리는 시퀀스나 토큰 집합에 대해 연산을 수행하고,

708
00:28:12,140 --> 00:28:13,620
dim 차원이 있습니다.

709
00:28:13,620 --> 00:28:17,250
각 토큰 자체가 어떤 차원을 가진 벡터인 거죠.

710
00:28:17,250 --> 00:28:21,582
그래서 우리 트랜스포머는 이 3차원 텐서에서

711
00:28:21,582 --> 00:28:23,040
연산을 수행합니다.

712
00:28:23,040 --> 00:28:25,320
그리고 층을 쌓아가며 연산을 진행합니다.

713
00:28:25,320 --> 00:28:28,130
이렇게 해서 병렬화할 수 있는 네 가지 축이 생깁니다.

714
00:28:28,130 --> 00:28:30,540
층 축을 따라 병렬화할 수 있는데, 이를

715
00:28:30,540 --> 00:28:32,150
파이프라인 병렬화라고 합니다.

716
00:28:32,150 --> 00:28:35,247
배치 차원을 따라 병렬화할 수 있는데, 이를 데이터

717
00:28:35,247 --> 00:28:36,330
병렬화라고 합니다.

718
00:28:36,330 --> 00:28:37,970
시퀀스 차원을 나눌 수 있는데,

719
00:28:37,970 --> 00:28:39,360
이를 컨텍스트 병렬화라고 합니다.

720
00:28:39,360 --> 00:28:41,330
그리고 dim 차원을 나눌 수 있는데,

721
00:28:41,330 --> 00:28:42,870
이를 텐서 병렬화라고 합니다.

722
00:28:42,870 --> 00:28:44,587
이 모든 것들이 다소 생소한 이름들이지만,

723
00:28:44,587 --> 00:28:46,170
이렇게 생각하면, 기본적으로

724
00:28:46,170 --> 00:28:47,900
트랜스포머 내부의 네

725
00:28:47,900 --> 00:28:50,240
가지 연산 축을 따라 계산을

726
00:28:50,240 --> 00:28:52,160
분할하는 다양한 방법들인 겁니다.

727
00:28:52,160 --> 00:28:54,410
그다음에 이 각각의 방법들을 더

728
00:28:54,410 --> 00:28:56,810
자세히 살펴볼 텐데요, 분산 학습의

729
00:28:56,810 --> 00:28:59,090
다양한 메커니즘에는 흥미로운 세부

730
00:28:59,090 --> 00:29:00,680
사항들이 많기 때문입니다.

731
00:29:00,680 --> 00:29:03,920
첫 번째는 Data Parallelism, 즉 DP입니다.

732
00:29:03,920 --> 00:29:06,888
기본 아이디어는 간단합니다.

733
00:29:06,888 --> 00:29:08,930
신경망을 학습할 때 항상

734
00:29:08,930 --> 00:29:12,300
미니배치 단위로 샘플을 처리한다는 점을 기억하세요.

735
00:29:12,300 --> 00:29:14,677
항상 미니배치의 요소들을 가져옵니다.

736
00:29:14,677 --> 00:29:17,010
학습 과제에 따라 미니배치 내 각

737
00:29:17,010 --> 00:29:19,050
항목에 대한 손실을 계산합니다.

738
00:29:19,050 --> 00:29:21,920
그다음에 그래디언트를 계산하는데, 이

739
00:29:21,920 --> 00:29:23,912
그래디언트는 보통 미니배치 내 개별

740
00:29:23,912 --> 00:29:26,370
요소들의 손실 그래디언트 평균입니다.

741
00:29:26,370 --> 00:29:28,650
대부분의 신경망 구조에서 손실

742
00:29:28,650 --> 00:29:30,433
계산과 그래디언트 계산은

743
00:29:30,433 --> 00:29:31,850
미니배치 내 각 요소에

744
00:29:31,850 --> 00:29:34,260
대해 독립적으로 이루어집니다.

745
00:29:34,260 --> 00:29:37,230
그래서 이것은 명백히 병렬화가 가능한 작업처럼 보입니다.

746
00:29:37,230 --> 00:29:40,100
기본 아이디어는, 만약 하나의 GPU에

747
00:29:40,100 --> 00:29:43,160
N개의 샘플 미니배치를 올릴 수

748
00:29:43,160 --> 00:29:45,530
있고 M개의 GPU를 사용할

749
00:29:45,530 --> 00:29:48,710
수 있다면, M 곱하기 N개의 샘플로

750
00:29:48,710 --> 00:29:52,220
구성된 거대한 미니배치를 학습하는 겁니다. 이

751
00:29:52,220 --> 00:29:55,160
거대한 미니배치를 각 GPU에 N개씩

752
00:29:55,160 --> 00:29:57,218
나누어 처리하는 거죠.

753
00:29:57,218 --> 00:29:58,760
수학적으로 왜 이게 말이

754
00:29:58,760 --> 00:30:01,530
되는지 생각해보면, 그래디언트가 선형이기 때문입니다.

755
00:30:01,530 --> 00:30:04,940
실제로 단일 스칼라 손실 L을 계산한다고

756
00:30:04,940 --> 00:30:07,100
하면, 이 L은 전체

757
00:30:07,100 --> 00:30:10,355
매크로 배치에 걸친 개별 손실들의

758
00:30:10,355 --> 00:30:14,510
평균입니다. 여기서 xi,j는 전체 매크로 배치 내

759
00:30:14,510 --> 00:30:15,980
모든 항목들이고,

760
00:30:15,980 --> 00:30:19,292
W는 네트워크 전체의 가중치 행렬입니다.

761
00:30:19,292 --> 00:30:20,750
보통 순전파

762
00:30:20,750 --> 00:30:22,820
마지막에 계산하는 손실은

763
00:30:22,820 --> 00:30:24,770
개별 미니배치 요소들의

764
00:30:24,770 --> 00:30:26,395
손실 평균입니다.

765
00:30:26,395 --> 00:30:27,770
그리고 가중치에 대한

766
00:30:27,770 --> 00:30:30,497
손실의 그래디언트를 구하는데, 이게 바로

767
00:30:30,497 --> 00:30:32,330
가중치 업데이트를 위해 계산해야

768
00:30:32,330 --> 00:30:33,510
하는 값입니다.

769
00:30:33,510 --> 00:30:36,020
그럼 이 값은 실제로 분할될 수 있습니다.

770
00:30:36,020 --> 00:30:37,850
기울기는 선형이기 때문에, 합을

771
00:30:37,850 --> 00:30:40,060
어떤 순서로 할지 선택할 수 있습니다.

772
00:30:40,060 --> 00:30:41,310
기울기를 먼저 할까요?

773
00:30:41,310 --> 00:30:42,602
평균을 먼저 할까요?

774
00:30:42,602 --> 00:30:44,720
특히, 여기 파란색으로 강조한

775
00:30:44,720 --> 00:30:46,820
내부 항이 있는 이 특정한

776
00:30:46,820 --> 00:30:49,340
수식으로 기울기를 배열하는 것이

777
00:30:49,340 --> 00:30:52,940
편리해집니다. 이 항은 기본적으로 N개의 요소에

778
00:30:52,940 --> 00:30:54,600
대한 역전파의 표준입니다.

779
00:30:54,600 --> 00:30:58,710
이것들은 서로 다른 GPU에서 병렬로 계산할 수 있습니다.

780
00:30:58,710 --> 00:31:00,298
그리고 외부 합이

781
00:31:00,298 --> 00:31:02,090
있는데, 우리가

782
00:31:02,090 --> 00:31:07,630
작동하는 M개의 서로 다른 장치에서 기울기의 평균을 내야 합니다.

783
00:31:07,630 --> 00:31:10,988
수학적인 관점에서 보면 이런 일이 일어나는 겁니다.

784
00:31:10,988 --> 00:31:13,280
이것이 완벽하게 수학적으로 타당하다는 것을 알 수 있습니다.

785
00:31:13,280 --> 00:31:15,490
이것은 기본적으로 단일 장치에서

786
00:31:15,490 --> 00:31:18,020
훈련하는 것과 수학적으로 정확히 같습니다.

787
00:31:18,020 --> 00:31:19,660
우리는 단지 대수학을 영리하게

788
00:31:19,660 --> 00:31:22,790
사용해서 평균과 합산을 하는 순서를 바꾼 것뿐입니다.

789
00:31:22,790 --> 00:31:24,230
하지만 이것은 근사가 아닙니다.

790
00:31:24,230 --> 00:31:25,730
이것은 단일 더 큰 GPU에서

791
00:31:25,730 --> 00:31:28,300
했을 계산과 정확히 같은 계산입니다.

792
00:31:28,300 --> 00:31:30,910
GPU 관점에서 보면,

793
00:31:30,910 --> 00:31:33,680
M개의 GPU가 있습니다.

794
00:31:33,680 --> 00:31:36,250
여기서는 M=3으로 보여주는데, 슬라이드에 적당히

795
00:31:36,250 --> 00:31:37,820
들어가는 수치라서 그렇습니다.

796
00:31:37,820 --> 00:31:40,640
하지만 실제로는 3보다 훨씬 더 많다고 생각하세요.

797
00:31:40,640 --> 00:31:42,910
그런 다음 각 GPU는

798
00:31:42,910 --> 00:31:45,010
신경망 가중치, 옵티마이저

799
00:31:45,010 --> 00:31:48,860
상태, 기울기의 별도 복사본을 유지합니다.

800
00:31:48,860 --> 00:31:50,740
그래서 각 GPU는 병렬로

801
00:31:50,740 --> 00:31:53,830
서로 다른 미니배치 데이터를 불러옵니다.

802
00:31:53,830 --> 00:31:56,230
여기서는 각 GPU가 3개의 요소로 된 미니배치를

803
00:31:56,230 --> 00:31:57,590
불러오는 것을 보여줍니다.

804
00:31:57,590 --> 00:32:00,190
중요한 점은, 서로 다른 GPU가 서로 다른

805
00:32:00,190 --> 00:32:02,290
미니배치를 불러와야 한다는 겁니다.

806
00:32:02,290 --> 00:32:04,348
저도 코드에서, 학생들도 같은

807
00:32:04,348 --> 00:32:06,140
미니배치를 모든 GPU가 불러오는

808
00:32:06,140 --> 00:32:07,860
버그를 겪은 적이 있습니다.

809
00:32:07,860 --> 00:32:08,640
그렇게 하면 도움이 안 됩니다.

810
00:32:08,640 --> 00:32:09,807
좋지 않습니다.

811
00:32:09,807 --> 00:32:11,150
그런 실수 하지 마세요.

812
00:32:11,150 --> 00:32:14,270
서로 다른 GPU가 반드시 서로 다른 미니배치를

813
00:32:14,270 --> 00:32:16,580
불러오는 것이 매우 중요합니다.

814
00:32:16,580 --> 00:32:20,000
그 다음 각 GPU는 자기 미니배치 데이터에

815
00:32:20,000 --> 00:32:23,090
대해 독립적으로 순전파를 수행해서 자기

816
00:32:23,090 --> 00:32:25,050
지역 손실을 계산합니다.

817
00:32:25,050 --> 00:32:27,840
이것들은 모두 완전히 독립적으로 작동할 수 있습니다.

818
00:32:27,840 --> 00:32:30,540
GPU 간 통신이 전혀 필요 없습니다.

819
00:32:30,540 --> 00:32:33,860
그 다음 각 네트워크는 자기 지역 손실에 대한

820
00:32:33,860 --> 00:32:36,290
모든 모델 가중치에 대한 기울기를

821
00:32:36,290 --> 00:32:38,520
계산하기 위해 역전파를 수행합니다.

822
00:32:38,520 --> 00:32:40,520
다시 말하지만, 각 모델, 즉 각

823
00:32:40,520 --> 00:32:42,440
GPU가 독립적인 모델 가중치 복사본을

824
00:32:42,440 --> 00:32:45,450
가지고 있기 때문에 완전히 독립적으로 할 수 있습니다.

825
00:32:45,450 --> 00:32:47,510
자기 순전파와 역전파를 완전히

826
00:32:47,510 --> 00:32:49,550
독립적으로 수행할 수 있습니다.

827
00:32:49,550 --> 00:32:51,587
하지만 역전파가 끝난

828
00:32:51,587 --> 00:32:52,920
후가 문제입니다.

829
00:32:52,920 --> 00:32:54,830
우리가 훈련에 참여하는 모든

830
00:32:54,830 --> 00:32:57,650
장치에서 기울기의 평균을 계산해야 한다고

831
00:32:57,650 --> 00:32:59,130
했던 것을 기억하세요.

832
00:32:59,130 --> 00:33:00,960
그래서 통신이 필요합니다.

833
00:33:00,960 --> 00:33:04,360
여기서 all reduce 연산을 수행합니다.

834
00:33:04,360 --> 00:33:09,580
각 GPU는 자신의 gradient를 다른 모든 GPU에 보내야 합니다.

835
00:33:09,580 --> 00:33:11,950
그래서 두 가지 일이 동시에 일어납니다.

836
00:33:11,950 --> 00:33:15,910
첫째, 각 GPU는 자신의 gradient를 모든 GPU에 브로드캐스트해야
합니다.

837
00:33:15,910 --> 00:33:18,900
둘째, 각 GPU는 훈련에 참여하는 모든

838
00:33:18,900 --> 00:33:22,050
GPU로부터 gradient를 수집해야 합니다.

839
00:33:22,050 --> 00:33:24,130
이것이 allreduce 연산입니다.

840
00:33:24,130 --> 00:33:28,590
이 연산은 보통 GPU 수에 따라 로그 시간

841
00:33:28,590 --> 00:33:30,490
내에 일어납니다.

842
00:33:30,490 --> 00:33:32,430
이 allreduce

843
00:33:32,430 --> 00:33:37,650
연산이 끝나면 각 GPU는 모든 장치의 gradient 평균을

844
00:33:37,650 --> 00:33:39,040
갖게 됩니다.

845
00:33:39,040 --> 00:33:41,290
이 시점에서 통신이 완료된 것입니다.

846
00:33:41,290 --> 00:33:43,590
각 GPU는 이제 모든 장치에서 all

847
00:33:43,590 --> 00:33:45,420
reduced 된 동일한 gradient

848
00:33:45,420 --> 00:33:46,630
사본을 갖고 있습니다.

849
00:33:46,630 --> 00:33:49,030
훈련 반복 시작 시, 각 GPU는

850
00:33:49,030 --> 00:33:51,780
독립적인 모델 가중치 사본을 갖고 있다고

851
00:33:51,780 --> 00:33:52,660
가정했습니다.

852
00:33:52,660 --> 00:33:55,410
이제 각 GPU는 전체 매크로 배치 데이터에

853
00:33:55,410 --> 00:33:58,980
대한 독립적이지만 동일한 gradient 사본을

854
00:33:58,980 --> 00:33:59,920
갖고 있습니다.

855
00:33:59,920 --> 00:34:03,620
이 시점에서 각 GPU는 자신의 로컬 가중치 사본에 대해 가중치

856
00:34:03,620 --> 00:34:05,370
업데이트를 할 수 있습니다.

857
00:34:05,370 --> 00:34:07,781
같은 가중치에서 시작했고 같은

858
00:34:07,781 --> 00:34:09,239
gradient를 적용했기

859
00:34:09,239 --> 00:34:11,781
때문에, 산술 연산이 결정적이라면

860
00:34:11,781 --> 00:34:15,620
로컬 가중치 업데이트 후에도 같은 가중치를 갖게 됩니다.

861
00:34:15,620 --> 00:34:19,340
그리고 덧붙여서, 이것이 정말 중요합니다.

862
00:34:19,340 --> 00:34:22,110
4단계와 5단계는 실제로 병렬로 진행될 수 있습니다.

863
00:34:22,110 --> 00:34:24,650
여기서 실제로 병렬로 일어날 수 있는 두 가지가

864
00:34:24,650 --> 00:34:25,920
있다고 말씀드렸습니다.

865
00:34:25,920 --> 00:34:28,790
하나는 각 GPU가 자체적으로 역전파를 수행해

866
00:34:28,790 --> 00:34:30,804
그래디언트를 계산하는 것이고,

867
00:34:30,804 --> 00:34:32,929
다른 하나는 GPU 간에 그래디언트를

868
00:34:32,929 --> 00:34:34,048
통신하는 것입니다.

869
00:34:34,048 --> 00:34:35,840
이 두 가지는 실제로 보통

870
00:34:35,840 --> 00:34:37,260
동시에 일어납니다.

871
00:34:37,260 --> 00:34:39,590
즉, 각 모델은 네트워크의 마지막

872
00:34:39,590 --> 00:34:42,080
층에서 역전파를 시작해 자체 로컬

873
00:34:42,080 --> 00:34:43,889
그래디언트를 계산합니다.

874
00:34:43,889 --> 00:34:45,508
그다음 모델은 두

875
00:34:45,508 --> 00:34:47,300
번째 마지막 층의 역전파

876
00:34:47,300 --> 00:34:49,380
계산으로 넘어갑니다.

877
00:34:49,380 --> 00:34:51,080
그리고 두 번째 마지막

878
00:34:51,080 --> 00:34:54,179
층의 역전파를 계산하는 동안, GPU들은

879
00:34:54,179 --> 00:34:56,600
동시에 마지막 층의 그래디언트에 대해

880
00:34:56,600 --> 00:34:59,500
all reduce 연산을 수행합니다.

881
00:34:59,500 --> 00:35:02,700
즉, 이 과정은 L+1 층의 통신과 L 층의

882
00:35:02,700 --> 00:35:05,520
역전파가 병렬로 진행된다는 뜻입니다.

883
00:35:05,520 --> 00:35:07,978
이렇게 병렬로 진행되면 네트워크 끝에

884
00:35:07,978 --> 00:35:10,062
도달해 역전파가 끝날 때쯤에는

885
00:35:10,062 --> 00:35:11,950
그래디언트가 모든 장치에서

886
00:35:11,950 --> 00:35:13,980
이미 all reduced

887
00:35:13,980 --> 00:35:15,340
되어 있을 것입니다.

888
00:35:15,340 --> 00:35:17,520
그래서 기다리지 않고 한 번에 가중치 업데이트를

889
00:35:17,520 --> 00:35:18,600
할 수 있습니다.

890
00:35:18,600 --> 00:35:21,150
이것이 매우 중요한 이유는, 말씀드렸듯이 통신이

891
00:35:21,150 --> 00:35:22,853
상대적으로 느리기 때문입니다.

892
00:35:22,853 --> 00:35:24,270
그래서 이런 시스템의

893
00:35:24,270 --> 00:35:26,940
핵심은 통신 비용을 숨기고 계산과 동시에

894
00:35:26,940 --> 00:35:29,040
수행하는 방법을 찾는 것입니다.

895
00:35:29,040 --> 00:35:31,540
질문은 4단계와 5단계 중 어느 쪽이 병목이 될 것인가 하는 점입니다.

896
00:35:31,540 --> 00:35:32,640
답은 그렇다는 겁니다.

897
00:35:32,640 --> 00:35:35,440
전적으로 장치의 속도에 달려 있습니다.

898
00:35:35,440 --> 00:35:36,610
모델의 크기가 얼마나 큰지도 중요합니다.

899
00:35:36,610 --> 00:35:37,780
미니배치 크기는 얼마나 되나요?

900
00:35:37,780 --> 00:35:40,270
장치 간 인터커넥트 속도는 얼마나 빠른가요?

901
00:35:40,270 --> 00:35:42,520
대규모 분산 학습에 도달하면,

902
00:35:42,520 --> 00:35:44,730
답은 항상 상황에 따라 다릅니다.

903
00:35:44,730 --> 00:35:47,220
그래서 자신의 상황에 맞게 벤치마크를 해야 합니다.

904
00:35:47,220 --> 00:35:49,720
왜 각기 다른 M개의 그래디언트 스텝을 각각 수행하지 않나요?

905
00:35:49,720 --> 00:35:51,210
사실 그거 정말 멋진 아이디어입니다.

906
00:35:51,210 --> 00:35:54,330
예전에 비동기 SGD라는 인기 있는

907
00:35:54,330 --> 00:35:57,767
알고리즘 세트가 있었는데, 기본적으로 그런

908
00:35:57,767 --> 00:35:59,600
방식으로 여러 모델

909
00:35:59,600 --> 00:36:01,260
복제본이 독립적으로 여러

910
00:36:01,260 --> 00:36:03,052
스텝을 수행한 후

911
00:36:03,052 --> 00:36:05,420
가끔씩 평균을 내는 방식이었죠.

912
00:36:05,420 --> 00:36:07,528
그 알고리즘들은 꽤 인기가 있었습니다.

913
00:36:07,528 --> 00:36:10,070
실제로 구글도 TPU 팟을 개발하기 전에는 이런 방식을

914
00:36:10,070 --> 00:36:10,970
사용했었습니다.

915
00:36:10,970 --> 00:36:13,610
2010년대 초반 그들의 초기 네트워크 중 일부가

916
00:36:13,610 --> 00:36:14,930
이런 방식으로 훈련되었죠.

917
00:36:14,930 --> 00:36:17,880
하지만 첫째, 이 방식은 훨씬 불안정한 경향이 있습니다.

918
00:36:17,880 --> 00:36:20,790
둘째, 디버깅과 재현이 매우 어렵습니다.

919
00:36:20,790 --> 00:36:23,640
그리고 전반적으로 성능이 조금 떨어지는 편입니다.

920
00:36:23,640 --> 00:36:26,190
그래서 더 확장 가능한 접근법처럼 느껴지긴 합니다.

921
00:36:26,190 --> 00:36:28,890
하지만 실제로는 모든 것을 동기적으로 처리할 수

922
00:36:28,890 --> 00:36:30,890
있다면, 알고리즘을 디버깅하고 이해하며

923
00:36:30,890 --> 00:36:32,790
논리적으로 접근하기가 훨씬 쉽습니다.

924
00:36:32,790 --> 00:36:34,820
기본적으로 동기식 그래디언트 업데이트가

925
00:36:34,820 --> 00:36:36,237
가능하다면, 아마도 그 방법이

926
00:36:36,237 --> 00:36:37,695
더 잘 작동할 것입니다.

927
00:36:37,695 --> 00:36:39,260
사실 개인적으로는 향후 몇 년

928
00:36:39,260 --> 00:36:42,850
내에 async SGD 방법들이 다시 부활하는 걸 보게 될지도

929
00:36:42,850 --> 00:36:44,600
모른다고 생각합니다. 왜냐하면 이

930
00:36:44,600 --> 00:36:47,060
방법들이 분산 학습에 훨씬 더 친화적이기

931
00:36:47,060 --> 00:36:47,630
때문입니다.

932
00:36:47,630 --> 00:36:50,130
이 모든 걸 조율할 수 있는 단일 컴퓨터는 없습니다.

933
00:36:50,130 --> 00:36:51,470
이 모든 것들은 각각 독립적인 장치들이고,

934
00:36:51,470 --> 00:36:52,845
각자 독립적인 작업을 수행합니다.

935
00:36:52,845 --> 00:36:56,440
신의 눈으로 전체를 바라보고 단계를 조율할 수

936
00:36:56,440 --> 00:36:58,480
있는 드라이버는 없습니다.

937
00:36:58,480 --> 00:37:00,880
모든 계산은 어딘가에서 반드시 이루어져야 합니다.

938
00:37:00,880 --> 00:37:01,700
아, 좋은 질문입니다.

939
00:37:01,700 --> 00:37:04,220
통신과 계산을 겹치게 할 때, 이걸 위해 코드를 직접

940
00:37:04,220 --> 00:37:05,140
작성해야 하나요,

941
00:37:05,140 --> 00:37:06,890
아니면 하드웨어가 자동으로 처리하나요?

942
00:37:06,890 --> 00:37:08,270
분명히 이걸 위해 코드를 작성해야 합니다.

943
00:37:08,270 --> 00:37:09,700
하드웨어가 여러분이 무엇을 하려는지

944
00:37:09,700 --> 00:37:10,837
이해할 만큼 똑똑하지 않습니다.

945
00:37:10,837 --> 00:37:13,420
하드웨어는 앞서 말했듯이, 작은 행렬 곱셈

946
00:37:13,420 --> 00:37:14,300
단위만 이해합니다.

947
00:37:14,300 --> 00:37:16,210
상당히 저수준의 작업만 이해합니다.

948
00:37:16,210 --> 00:37:19,375
통신 스케줄링을 위해 하려는 모든 작업은

949
00:37:19,375 --> 00:37:21,230
소프트웨어에서 처리해야 합니다.

950
00:37:21,230 --> 00:37:24,110
하지만 다행히도 이런 일반적인 사용 사례 대부분에 대해

951
00:37:24,110 --> 00:37:25,760
PyTorch가 이미 지원해 줍니다.

952
00:37:25,760 --> 00:37:27,580
예를 들어,

953
00:37:27,580 --> 00:37:29,890
DistributedDataParallel이라는

954
00:37:29,890 --> 00:37:33,190
PyTorch 클래스가 있어서, 여러분이 작성한 일반적인

955
00:37:33,190 --> 00:37:35,230
PyTorch 코드 위에서 이

956
00:37:35,230 --> 00:37:37,747
작업을 비교적 투명하게 처리해 줍니다.

957
00:37:37,747 --> 00:37:39,580
사실 이 점은 개별 장치와

958
00:37:39,580 --> 00:37:41,390
대조해 보면 정말 흥미로운데,

959
00:37:41,390 --> 00:37:45,160
개별 GPU를 CUDA, 즉 NVIDIA의 GPU

960
00:37:45,160 --> 00:37:47,450
프로그래밍 언어로 프로그래밍할

961
00:37:47,450 --> 00:37:49,000
때는 하드웨어가 많은

962
00:37:49,000 --> 00:37:51,890
비동기 전송을 자동으로 처리해 줍니다.

963
00:37:51,890 --> 00:37:54,075
하지만 클러스터 수준에서는 보통 그렇지 않습니다.

964
00:37:54,075 --> 00:37:55,950
그럼 보통은 소프트웨어에서 처리해야 합니다.

965
00:37:55,950 --> 00:37:56,960
그래서 실제로 개별

966
00:37:56,960 --> 00:37:59,335
장치 수준에서의 병렬 처리와 클러스터 수준에서의 병렬

967
00:37:59,335 --> 00:38:01,460
처리 사이에 약간 흥미로운 비대칭성이 있습니다.

968
00:38:01,460 --> 00:38:04,100
개별 장치에서는 하드웨어가 자동으로 많이 처리하지만,

969
00:38:04,100 --> 00:38:06,590
클러스터 수준에서는 소프트웨어로 조율해야 합니다.

970
00:38:06,590 --> 00:38:09,062
네, 보통 이런 시스템은 이기종 시스템으로,

971
00:38:09,062 --> 00:38:10,520
시스템의 다른 부분들이 서로

972
00:38:10,520 --> 00:38:12,150
다른 프로그래밍 언어로 작성됩니다.

973
00:38:12,150 --> 00:38:13,730
그래서 실제로 GPU

974
00:38:13,730 --> 00:38:15,355
내부에서 실행되는 저수준

975
00:38:15,355 --> 00:38:16,873
장치 커널들이 있습니다.

976
00:38:16,873 --> 00:38:18,540
이 커널들은 보통 CUDA로

977
00:38:18,540 --> 00:38:21,020
작성되는데, CUDA는 NVIDIA가 자사

978
00:38:21,020 --> 00:38:23,990
GPU 프로그래밍을 위해 만든 C와 비슷한 언어입니다.

979
00:38:23,990 --> 00:38:26,970
그런데 이 개별 GPU 커널들은 감싸지게 됩니다.

980
00:38:26,970 --> 00:38:29,310
그리고 Python에서 이 GPU 커널들을 호출할 수 있습니다.

981
00:38:29,310 --> 00:38:31,200
이것이 기본적으로 PyTorch가 작동하는 방식입니다.

982
00:38:31,200 --> 00:38:34,910
PyTorch는 GPU에서 다양한 흥미로운 작업을 할 수 있는 많은

983
00:38:34,910 --> 00:38:36,660
GPU 커널들의 모음이고, 그

984
00:38:36,660 --> 00:38:39,920
커널들을 감싸서 더 사용자 친화적으로 프로그래밍할 수

985
00:38:39,920 --> 00:38:42,560
있게 하는 C++와 Python 코드가 많습니다.

986
00:38:42,560 --> 00:38:45,680
이 그림에서 각 GPU는 검은색으로 표시된 자기 자신의

987
00:38:45,680 --> 00:38:48,205
그래디언트를 독립적으로 계산하고 있습니다.

988
00:38:48,205 --> 00:38:49,580
그리고 빨간색 그래디언트는

989
00:38:49,580 --> 00:38:53,520
모든 GPU에서 병렬로 allreduce를 통해 계산됩니다.

990
00:38:53,520 --> 00:38:55,200
아래층의 역전파는 이전

991
00:38:55,200 --> 00:38:57,450
층의 그래디언트에 의존합니다.

992
00:38:57,450 --> 00:39:02,160
하지만 중요한 점은 각 GPU가 자기 미니배치에 대해서만 역전파를

993
00:39:02,160 --> 00:39:03,250
수행한다는 겁니다.

994
00:39:03,250 --> 00:39:05,250
그래서 이제 각 층에서 생각해야 할

995
00:39:05,250 --> 00:39:06,540
그래디언트가 두 가지

996
00:39:06,540 --> 00:39:07,560
변형이 있습니다.

997
00:39:07,560 --> 00:39:09,990
하나는 내 미니배치의 로컬 손실에 대한

998
00:39:09,990 --> 00:39:12,240
네트워크 가중치의 그래디언트인 로컬

999
00:39:12,240 --> 00:39:13,207
그래디언트이고,

1000
00:39:13,207 --> 00:39:14,790
다른 하나는 매크로배치의

1001
00:39:14,790 --> 00:39:16,785
전체 손실에 대한

1002
00:39:16,785 --> 00:39:20,790
네트워크 가중치의 도함수인 글로벌 그래디언트입니다.

1003
00:39:20,790 --> 00:39:22,300
역전파를 계산하려면 각

1004
00:39:22,300 --> 00:39:24,030
GPU는 상류 그래디언트의 로컬

1005
00:39:24,030 --> 00:39:25,120
버전만 필요합니다.

1006
00:39:25,120 --> 00:39:27,930
하지만 글로벌 상류 그래디언트를

1007
00:39:27,930 --> 00:39:31,530
계산하려면 통신이 필요합니다.

1008
00:39:31,530 --> 00:39:33,280
이것이 데이터 병렬 처리입니다.

1009
00:39:33,280 --> 00:39:35,100
사실 여기서 약간 문제가 있는데,

1010
00:39:35,100 --> 00:39:36,600
이 방법은 GPU 계산을

1011
00:39:36,600 --> 00:39:38,107
병렬화하는 훌륭한 방법입니다.

1012
00:39:38,107 --> 00:39:39,690
그리고 이것이 사람들이 신경망

1013
00:39:39,690 --> 00:39:41,982
훈련에서 GPU 계산을 병렬화하기 시작한 첫 번째

1014
00:39:41,982 --> 00:39:42,550
방법이었습니다.

1015
00:39:42,550 --> 00:39:46,000
하지만 모델 크기에서 병목 현상에 빠르게 도달했습니다.

1016
00:39:46,000 --> 00:39:48,660
여기서 각 GPU가 모델 파라미터의 독립적인

1017
00:39:48,660 --> 00:39:50,880
복사본을 유지한다는 점을 기억하세요.

1018
00:39:50,880 --> 00:39:52,160
이게 정말 큰 모델을

1019
00:39:52,160 --> 00:39:53,490
다룰 때 병목이 됩니다.

1020
00:39:53,490 --> 00:39:56,257
특히, 신경망의 각 가중치마다 기본적으로

1021
00:39:56,257 --> 00:39:58,340
네 가지 숫자를 추적해야

1022
00:39:58,340 --> 00:40:02,240
합니다—가중치 자체, 그 가중치의 그래디언트, 그리고

1023
00:40:02,240 --> 00:40:03,453
옵티마이저 상태입니다.

1024
00:40:03,453 --> 00:40:05,120
Adam을 사용한다면, 보통

1025
00:40:05,120 --> 00:40:07,760
네트워크의 각 파라미터마다 베타1과 베타2가

1026
00:40:07,760 --> 00:40:08,560
있습니다.

1027
00:40:08,560 --> 00:40:11,060
때로는 모델 파라미터의 지수 이동

1028
00:40:11,060 --> 00:40:12,600
평균도 유지합니다.

1029
00:40:12,600 --> 00:40:15,162
그래서 보통 네트워크의 각 가중치마다

1030
00:40:15,162 --> 00:40:17,120
4~5개의 스칼라 값을

1031
00:40:17,120 --> 00:40:18,320
추적해야 합니다.

1032
00:40:18,320 --> 00:40:20,790
요즘 흔한 16비트 정밀도로

1033
00:40:20,790 --> 00:40:22,867
훈련할 때, 일부는 더 높은

1034
00:40:22,867 --> 00:40:25,200
정밀도로 유지하기도 합니다.

1035
00:40:25,200 --> 00:40:27,150
하지만 하한선으로 16비트를 기준으로 이야기해 보겠습니다.

1036
00:40:27,150 --> 00:40:29,340
그러면 각 숫자마다 2바이트가 필요합니다.

1037
00:40:29,340 --> 00:40:31,520
즉, 네트워크의 각

1038
00:40:31,520 --> 00:40:34,670
스칼라마다 4개의 숫자 × 2바이트

1039
00:40:34,670 --> 00:40:37,350
= 8바이트가 필요합니다.

1040
00:40:37,350 --> 00:40:40,910
따라서 10억 개의 모델 파라미터를 저장하려면

1041
00:40:40,910 --> 00:40:43,310
약 8기가바이트의 GPU

1042
00:40:43,310 --> 00:40:44,730
메모리가 필요합니다.

1043
00:40:44,730 --> 00:40:46,820
그리고 우리는 H100의 전체 GPU 메모리가

1044
00:40:46,820 --> 00:40:48,150
80기가바이트밖에 없다고 했습니다.

1045
00:40:48,150 --> 00:40:50,275
즉, 이 상황에서 훈련할 수 있는

1046
00:40:50,275 --> 00:40:52,090
가장 큰 모델은 약 100억 개의

1047
00:40:52,090 --> 00:40:53,690
파라미터 정도라는 뜻입니다.

1048
00:40:53,690 --> 00:40:54,830
그런데 그 정도는 충분히 크지 않습니다.

1049
00:40:54,830 --> 00:40:56,000
우리는 정말 큰 모델을 원합니다.

1050
00:40:56,000 --> 00:40:58,667
GPU 메모리 크기에 제약받아 얼마나 큰

1051
00:40:58,667 --> 00:41:01,490
모델을 훈련할 수 있는지 제한받고 싶지 않습니다.

1052
00:41:01,490 --> 00:41:03,580
그래서 이 문제를 어떻게든 해결해야 합니다.

1053
00:41:03,580 --> 00:41:06,423
그리고 이 해결책은 사실 비교적 간단합니다.

1054
00:41:06,423 --> 00:41:07,840
모델 가중치를 여러

1055
00:41:07,840 --> 00:41:09,410
GPU에 나누어야 합니다.

1056
00:41:09,410 --> 00:41:12,410
그래서 데이터 배치를 GPU에 나누는 것

1057
00:41:12,410 --> 00:41:15,560
외에도, 모델 가중치도 GPU에 나누게 됩니다.

1058
00:41:15,560 --> 00:41:17,890
이것이 Fully Sharded Data

1059
00:41:17,890 --> 00:41:20,950
Parallelism, 즉 FSDP라는 데이터 병렬성의 변형으로 이어집니다.

1060
00:41:20,950 --> 00:41:23,362
이것은 비교적 간단합니다.

1061
00:41:23,362 --> 00:41:24,820
개념적으로 우리가

1062
00:41:24,820 --> 00:41:27,610
할 일은 네트워크 내 각 모델 가중치

1063
00:41:27,610 --> 00:41:31,460
Wi를 소유하는 GPU에 할당하는 것입니다.

1064
00:41:31,460 --> 00:41:35,020
즉, 각 가중치는 우리가 훈련하는 M개의 GPU 중 하나에

1065
00:41:35,020 --> 00:41:36,040
고유하게 소유됩니다.

1066
00:41:36,040 --> 00:41:38,560
그리고 각 가중치를 소유한 GPU는

1067
00:41:38,560 --> 00:41:40,780
그 가중치에 대한 전역

1068
00:41:40,780 --> 00:41:42,350
그래디언트와 옵티마이저

1069
00:41:42,350 --> 00:41:44,802
상태를 관리하는 책임도 집니다.

1070
00:41:44,802 --> 00:41:46,760
보통은 이 작업을 레이어 단위로 나누게 됩니다.

1071
00:41:46,760 --> 00:41:48,510
개별 스칼라 단위로 관리하지는 않습니다.

1072
00:41:48,510 --> 00:41:50,873
이 W는 신경망의 한 층 전체에 대한

1073
00:41:50,873 --> 00:41:52,915
가중치 행렬로 생각하시면 됩니다.

1074
00:41:55,820 --> 00:41:58,230
그래서 오른쪽 그림이 조금 바뀝니다.

1075
00:41:58,230 --> 00:42:00,647
여기서는 GPU 두 개만 보여드리는데, 스포일러를

1076
00:42:00,647 --> 00:42:02,647
하자면 곧 훨씬 더 많은 화살표가

1077
00:42:02,647 --> 00:42:03,460
등장할 겁니다.

1078
00:42:03,460 --> 00:42:05,210
여기서는 네 층짜리 네트워크를 두 개의

1079
00:42:05,210 --> 00:42:07,560
서로 다른 GPU에 분산시키는 모습을 보여줍니다.

1080
00:42:07,560 --> 00:42:11,360
첫 두 네트워크 층의 가중치 W1과 W2를

1081
00:42:11,360 --> 00:42:12,840
할당했습니다.

1082
00:42:12,840 --> 00:42:14,690
이것들은 GPU 1이 소유하고 있습니다.

1083
00:42:14,690 --> 00:42:18,660
가중치 W3와 W4는 GPU 3 또는 GPU 2가 소유하고 있습니다.

1084
00:42:18,660 --> 00:42:21,780
즉, 각 배치가 시작될 때 네트워크 가중치가

1085
00:42:21,780 --> 00:42:25,080
이렇게 GPU들에 나누어져 있다는 뜻입니다.

1086
00:42:25,080 --> 00:42:26,820
하지만 여전히 데이터 병렬 처리입니다.

1087
00:42:26,820 --> 00:42:28,968
각 GPU가 독립적인 배치 데이터를 로드하고,

1088
00:42:28,968 --> 00:42:31,260
그 배치에 대해 완전한 순전파와 역전파를

1089
00:42:31,260 --> 00:42:33,470
수행해 로컬 그래디언트를 계산한 다음, 모든

1090
00:42:33,470 --> 00:42:35,010
그래디언트를 모아 합산하고

1091
00:42:35,010 --> 00:42:37,320
그래디언트 스텝을 밟는 기본 아이디어는 같습니다.

1092
00:42:37,320 --> 00:42:39,320
기본 알고리즘은 같지만, 모델 가중치가

1093
00:42:39,320 --> 00:42:41,130
나누어져 있어서 이제 복잡해집니다.

1094
00:42:41,130 --> 00:42:43,565
그래서 여기서는 추가 통신이 필요합니다.

1095
00:42:43,565 --> 00:42:45,190
완전 분산 데이터

1096
00:42:45,190 --> 00:42:48,230
병렬 처리를 할 때는, 순전파 시작 전에

1097
00:42:48,230 --> 00:42:51,580
첫 번째 층의 가중치를 소유한 GPU가

1098
00:42:51,580 --> 00:42:54,100
그 가중치 행렬을 훈련 중인

1099
00:42:54,100 --> 00:42:56,590
다른 모든 GPU에 브로드캐스트해야

1100
00:42:56,590 --> 00:42:57,590
합니다.

1101
00:42:57,590 --> 00:43:02,810
이 경우 GPU 1이 W1을 소유하므로, GPU 2에게 W1을
브로드캐스트합니다.

1102
00:43:02,810 --> 00:43:05,240
그래서 GPU 2도 이제 W1의 복사본을 갖게 됩니다.

1103
00:43:05,240 --> 00:43:07,575
모든 GPU가 W1 복사본을 갖게

1104
00:43:07,575 --> 00:43:09,700
되었으니, 네트워크 첫 번째 층의

1105
00:43:09,700 --> 00:43:11,450
순전파를 실행해 첫 번째 층의

1106
00:43:11,450 --> 00:43:13,120
활성화를 계산할 수 있습니다.

1107
00:43:13,120 --> 00:43:15,880
이제 순전파를 실행한 후, W1을 소유하지

1108
00:43:15,880 --> 00:43:18,580
않은 각 GPU는 메모리를 절약하기

1109
00:43:18,580 --> 00:43:21,400
위해 W1 가중치 행렬의 로컬 복사본을

1110
00:43:21,400 --> 00:43:22,430
삭제할 것입니다.

1111
00:43:22,430 --> 00:43:25,592
그래서 첫 번째 층의 순전파를 실행한 후에는

1112
00:43:25,592 --> 00:43:27,550
모델 가중치가 다시 GPU들에

1113
00:43:27,550 --> 00:43:29,150
분산된 상태로 돌아갑니다.

1114
00:43:29,150 --> 00:43:33,050
하지만 이제 모든 GPU는 네트워크 첫 번째 층을 실행한 결과인

1115
00:43:33,050 --> 00:43:35,780
활성화 값을 GPU 메모리에 가지고 있습니다.

1116
00:43:35,780 --> 00:43:37,960
그리고 이제 두 번째 층을 수행할 시간이고,

1117
00:43:37,960 --> 00:43:39,290
똑같은 작업을 합니다.

1118
00:43:39,290 --> 00:43:42,730
그래서 두 번째 층 가중치 행렬을 소유한 GPU가 훈련

1119
00:43:42,730 --> 00:43:44,622
중인 모든 GPU에 이를 브로드캐스트할

1120
00:43:44,622 --> 00:43:45,580
것입니다.

1121
00:43:45,580 --> 00:43:48,820
이제 모두가 W2의 로컬 복사본을 가지고

1122
00:43:48,820 --> 00:43:50,710
앞으로 진행할 수 있습니다.

1123
00:43:50,710 --> 00:43:52,630
참고로, 여기서도 계산과

1124
00:43:52,630 --> 00:43:55,210
통신을 교차 수행할 기회가 있어서,

1125
00:43:55,210 --> 00:43:57,130
i번째 층의 순전파를

1126
00:43:57,130 --> 00:43:59,830
계산하는 동안 다음 층의 가중치를 미리

1127
00:43:59,830 --> 00:44:01,220
가져올 수 있습니다.

1128
00:44:01,220 --> 00:44:03,430
실제로는 FSDP 실행 중 순전파

1129
00:44:03,430 --> 00:44:06,020
동안 이 작업이 병렬로 일어납니다.

1130
00:44:06,020 --> 00:44:08,775
그래서 두 번째 층을 계산하는 동안,

1131
00:44:08,775 --> 00:44:10,150
동시에 세 번째 층의

1132
00:44:10,150 --> 00:44:11,590
가중치를 가져오고 있습니다.

1133
00:44:11,590 --> 00:44:15,800
그리고 세 번째 층에 도달하면, 이제 GPU 1이 세 번째 층을 소유한다는
점을 주목하세요.

1134
00:44:15,800 --> 00:44:18,130
그래서 GPU 1이 훈련 중인 모든 GPU에

1135
00:44:18,130 --> 00:44:19,850
가중치를 브로드캐스트할 것입니다.

1136
00:44:19,850 --> 00:44:21,280
이 과정은 네트워크

1137
00:44:21,280 --> 00:44:22,363
끝까지 반복됩니다.

1138
00:44:22,363 --> 00:44:25,180
네트워크 끝에 도달하면, 모든 모델이 완전한

1139
00:44:25,180 --> 00:44:27,500
순전파를 수행했고, 각자의 로컬 배치에

1140
00:44:27,500 --> 00:44:29,893
대해 로컬 손실을 계산했으며, 역전파를

1141
00:44:29,893 --> 00:44:32,560
위해 모든 층의 활성화를 메모리에 이미 가지고

1142
00:44:32,560 --> 00:44:33,347
있습니다.

1143
00:44:33,347 --> 00:44:35,680
이제 역전파를 계산하기 위해 같은 과정을

1144
00:44:35,680 --> 00:44:36,950
역순으로 수행해야 합니다.

1145
00:44:36,950 --> 00:44:39,490
마지막 층 역전파 시작 시, 그

1146
00:44:39,490 --> 00:44:42,600
층 가중치를 소유한 사람이 모든 장치에

1147
00:44:42,600 --> 00:44:44,770
이를 브로드캐스트합니다.

1148
00:44:44,770 --> 00:44:46,720
장치들이 그 가중치를 받으면

1149
00:44:46,720 --> 00:44:49,000
역전파를 수행할 수 있습니다.

1150
00:44:49,000 --> 00:44:52,030
이 전체 과정은 역전파에서도 유사하게 진행됩니다.

1151
00:44:52,030 --> 00:44:53,970
네트워크 마지막 층에서는

1152
00:44:53,970 --> 00:44:56,580
약간의 최적화를 할 수 있는데,

1153
00:44:56,580 --> 00:45:00,510
모든 GPU가 마지막 층 가중치를 메모리에

1154
00:45:00,510 --> 00:45:01,870
유지하는 것입니다.

1155
00:45:01,870 --> 00:45:04,203
이것은 실제로 보통 하는 일입니다.

1156
00:45:04,203 --> 00:45:06,330
왜냐하면 마지막에 모든 GPU가 순전파에서

1157
00:45:06,330 --> 00:45:08,830
마지막 층 가중치 복사본을 이미 가지고 있기 때문입니다.

1158
00:45:08,830 --> 00:45:09,940
그들은 역전파 때 다시 사용할 것을 알기 때문에, 마지막 층 가중치를
삭제하지 않고 메모리에 유지합니다.

1159
00:45:09,940 --> 00:45:11,340
하지만 역전파

1160
00:45:11,340 --> 00:45:13,298
동안 기본적으로 세

1161
00:45:13,298 --> 00:45:15,000
가지 일이 일어나야

1162
00:45:15,000 --> 00:45:16,085
합니다.

1163
00:45:18,247 --> 00:45:19,830
첫째, GPU들이 네트워크 마지막 층의 역전파를 계산하면, 그

1164
00:45:19,830 --> 00:45:22,440
시점에 각 GPU는 그 마지막 층 가중치에 대한 로컬 손실의 로컬
그래디언트를 계산한 상태입니다.

1165
00:45:22,440 --> 00:45:25,080
그다음

1166
00:45:25,080 --> 00:45:27,690
이

1167
00:45:27,690 --> 00:45:30,190
그래디언트를

1168
00:45:30,190 --> 00:45:34,380
다시 통신해야

1169
00:45:34,380 --> 00:45:38,100
합니다.

1170
00:45:38,100 --> 00:45:41,810
그리고 우리는 가중치 행렬을 소유한 GPU가 그 가중치 행렬의 그래디언트
관리를 담당한다고 했습니다.

1171
00:45:41,810 --> 00:45:44,590
그래서 데이터 병렬 처리 때처럼

1172
00:45:44,590 --> 00:45:47,200
모든 GPU가 그래디언트를

1173
00:45:47,200 --> 00:45:48,290
합치는 대신,

1174
00:45:48,290 --> 00:45:50,830
마지막 층 가중치를

1175
00:45:50,830 --> 00:45:52,790
소유한 단

1176
00:45:52,790 --> 00:45:53,890
하나의

1177
00:45:56,920 --> 00:46:00,100
GPU가 모든 장치의 로컬

1178
00:46:00,100 --> 00:46:03,670
그래디언트를 모아서 합산할

1179
00:46:03,670 --> 00:46:05,030
것입니다.

1180
00:46:05,030 --> 00:46:08,020
이 경우 GPU 1이 마지막 층 로컬

1181
00:46:08,020 --> 00:46:11,770
그래디언트를 GPU 2에 보내면, GPU 2는

1182
00:46:11,770 --> 00:46:16,720
전체 매크로배치에 대한 마지막 층 가중치의 전체 그래디언트

1183
00:46:16,720 --> 00:46:18,925
dL/dW4를 가지게 됩니다.

1184
00:46:18,925 --> 00:46:20,300
휴지기 동안에는 무슨 일이 일어나나요?

1185
00:46:20,300 --> 00:46:23,748
이 모든 작업이 병렬로 일어나야 합니다.

1186
00:46:23,748 --> 00:46:25,540
그래서 역전파 동안 기본적으로 세

1187
00:46:25,540 --> 00:46:26,832
가지 일이 일어나야 합니다.

1188
00:46:26,832 --> 00:46:29,480
역전파 중에는 가중치를 전달해야 합니다.

1189
00:46:29,480 --> 00:46:33,940
그래서 해당 레이어를 소유한 GPU가 그 레이어의 가중치를

1190
00:46:33,940 --> 00:46:35,210
브로드캐스트해야 합니다.

1191
00:46:35,210 --> 00:46:38,150
두 번째로, 모든 GPU는 그 가중치를 받으면 해당

1192
00:46:38,150 --> 00:46:40,450
레이어에 대해 역전파를 계산해야 합니다.

1193
00:46:40,450 --> 00:46:44,500
그리고 세 번째로, 각 GPU가 역전파를 계산한 후에는

1194
00:46:44,500 --> 00:46:47,460
그 역전파에 대한 가중치의 그래디언트

1195
00:46:47,460 --> 00:46:49,200
결과를 가중치 소유

1196
00:46:49,200 --> 00:46:51,670
GPU에 다시 보내야 합니다.

1197
00:46:51,670 --> 00:46:55,830
그 후에 가중치 소유자가 전체 그래디언트를

1198
00:46:55,830 --> 00:46:58,140
받으면, 그 가중치 행렬에

1199
00:46:58,140 --> 00:47:00,720
대해 그래디언트 업데이트를

1200
00:47:00,720 --> 00:47:02,820
할 수 있습니다.

1201
00:47:02,820 --> 00:47:04,710
하지만 이 시점에서는 업데이트된

1202
00:47:04,710 --> 00:47:08,220
가중치 행렬을 다시 전달할 필요는 없습니다. 다음

1203
00:47:08,220 --> 00:47:10,080
순전파 때 모든 GPU에 다시

1204
00:47:10,080 --> 00:47:11,550
전달되기 때문입니다.

1205
00:47:11,550 --> 00:47:15,930
이 점이 DP 방식과는 조금 다를 수 있습니다.

1206
00:47:15,930 --> 00:47:17,790
그리고 기본적으로 이 모든 과정은

1207
00:47:17,790 --> 00:47:19,720
병렬로 진행될 수 있습니다.

1208
00:47:19,720 --> 00:47:22,270
이 과정을 네트워크의 모든 레이어에 대해 반복합니다.

1209
00:47:22,270 --> 00:47:25,150
그리고 매우 깊은 네트워크의 정상 상태에서는

1210
00:47:25,150 --> 00:47:28,245
이 세 가지 작업이 동시에 일어납니다.

1211
00:47:28,245 --> 00:47:32,050
즉, 레이어 L의 역전파를 계산하는 동안,

1212
00:47:32,050 --> 00:47:34,440
레이어 L+1의 그래디언트를

1213
00:47:34,440 --> 00:47:38,230
집계하고 가중치 업데이트를 수행하며,

1214
00:47:38,230 --> 00:47:42,085
레이어 L-1의 가중치를 미리 가져오는 작업을 합니다.

1215
00:47:42,085 --> 00:47:44,210
앞서 말했듯이 세 가지 작업이 필요합니다.

1216
00:47:44,210 --> 00:47:48,938
가중치를 받고, 역전파를 실행하고, 그래디언트를 집계하고

1217
00:47:48,938 --> 00:47:51,230
가중치를 업데이트하는 것입니다.

1218
00:47:51,230 --> 00:47:53,000
이 모든 작업은 병렬로 진행할 수 있습니다.

1219
00:47:53,000 --> 00:47:54,010
즉, 일반적으로

1220
00:47:54,010 --> 00:47:56,740
세 개의 연속된 레이어에서 이 세 가지

1221
00:47:56,740 --> 00:47:59,460
작업을 병렬로 수행하며 역전파를 진행합니다.

1222
00:48:03,880 --> 00:48:06,410
그리고 네트워크를 거꾸로 chunk

1223
00:48:06,410 --> 00:48:09,130
하면서, 만약 통신과 계산을 제대로

1224
00:48:09,130 --> 00:48:11,960
겹치게 할 수 있었다면, 역전파가

1225
00:48:11,960 --> 00:48:14,540
끝날 때쯤에는 모든 그래디언트가

1226
00:48:14,540 --> 00:48:16,790
이미 통신이 완료된 상태입니다.

1227
00:48:16,790 --> 00:48:18,280
모든 GPU가 이미 모든 가중치에

1228
00:48:18,280 --> 00:48:20,690
대해 업데이트를 마쳤고, 준비가 된 상태입니다.

1229
00:48:20,690 --> 00:48:23,410
그리고 보통 서버의 CPU 코어에서 비동기적으로

1230
00:48:23,410 --> 00:48:25,540
데이터 로더가 데이터를 불러오는 작업도

1231
00:48:25,540 --> 00:48:27,320
동시에 진행되고 있길 바랍니다.

1232
00:48:27,320 --> 00:48:29,980
그래서 CPU는 다시 앞으로 나아갈 새로운 배치 데이터를

1233
00:48:29,980 --> 00:48:31,070
준비해 둔 상태입니다.

1234
00:48:31,070 --> 00:48:33,405
이런 것들이 기본적으로 병렬화 기계라고 할 수 있습니다.

1235
00:48:33,405 --> 00:48:34,780
GPU 내부뿐만 아니라

1236
00:48:34,780 --> 00:48:37,928
GPU 간에도 해야 할 일이 많고, 이 모든

1237
00:48:37,928 --> 00:48:40,220
것을 최대한 겹치게 해야 합니다.

1238
00:48:40,220 --> 00:48:41,920
그래서 항상 GPU에 데이터를 공급하고

1239
00:48:41,920 --> 00:48:44,460
텐서 코어가 최대한 밀도 있게 작동하도록 할 수 있습니다.

1240
00:48:47,305 --> 00:48:51,550
그럼 다음 배치를 처리할 준비가 된 셈입니다.

1241
00:48:51,550 --> 00:48:52,700
이것은 아주 좋은 방법입니다.

1242
00:48:52,700 --> 00:48:54,440
이것이 Fully Sharded Data Parallelism입니다.

1243
00:48:54,440 --> 00:48:56,420
이 방법으로도 꽤 멀리 갈 수 있습니다.

1244
00:48:56,420 --> 00:48:59,322
하지만 사람들이 가끔 사용하는 조금 더 고급 데이터

1245
00:48:59,322 --> 00:49:01,030
병렬화 방식이 있는데,

1246
00:49:01,030 --> 00:49:05,650
Hybrid Sharded Data Parallelism, 즉 HSDP라고
합니다.

1247
00:49:05,650 --> 00:49:07,270
이 경우에는 GPU들을

1248
00:49:07,270 --> 00:49:09,700
개념적으로 2차원 격자로 나누는

1249
00:49:09,700 --> 00:49:11,210
것을 상상합니다.

1250
00:49:11,210 --> 00:49:14,690
이전 예제에서는 GPU가 N개 있다고 했습니다.

1251
00:49:14,690 --> 00:49:18,170
그리고 병렬화 방식은 동일했습니다.

1252
00:49:18,170 --> 00:49:20,890
이전 데이터 병렬화 방식에서는 한

1253
00:49:20,890 --> 00:49:23,090
축으로만 병렬화를 했습니다.

1254
00:49:23,090 --> 00:49:25,280
Hybrid Sharded Data

1255
00:49:25,280 --> 00:49:28,090
Parallelism에 도달하면, 이제 동시에 수행할 두

1256
00:49:28,090 --> 00:49:29,870
개의 별도 병렬 축이 생깁니다.

1257
00:49:29,870 --> 00:49:34,090
첫 번째 축은 우리가 방금 이야기한 전형적인 FSDP, Fully
Sharded Data

1258
00:49:34,090 --> 00:49:36,700
Parallelism을 한 축을 따라 수행하는 것입니다.

1259
00:49:36,700 --> 00:49:40,000
그래서 K개의 GPU 그룹들이 있을 겁니다.

1260
00:49:40,000 --> 00:49:43,587
그리고 각 K GPU 그룹은 우리가 방금 이야기한 Fully Sharded
Data

1261
00:49:43,587 --> 00:49:45,170
Parallelism을 수행합니다.

1262
00:49:45,170 --> 00:49:47,920
각 K GPU 그룹 내에서 모델 가중치는 그

1263
00:49:47,920 --> 00:49:50,010
K개의 GPU에 걸쳐 분할됩니다.

1264
00:49:50,010 --> 00:49:52,510
그리고 순전파와 역전파 동안 가중치와

1265
00:49:52,510 --> 00:49:54,040
그래디언트를 서로

1266
00:49:54,040 --> 00:49:55,250
주고받으며 교차합니다.

1267
00:49:55,250 --> 00:50:00,490
하지만 이제 M개의 K 그룹 복사본이 병렬로

1268
00:50:00,490 --> 00:50:01,700
작동합니다.

1269
00:50:01,700 --> 00:50:05,240
이 경우, 네 개의 GPU 두 그룹이 있습니다.

1270
00:50:05,240 --> 00:50:07,510
각 네 개 GPU 그룹은 네 개

1271
00:50:07,510 --> 00:50:10,820
GPU에 걸쳐 가중치가 분할되어 있습니다.

1272
00:50:10,820 --> 00:50:15,550
하지만 전체 설정이 두 번째 두 GPU 그룹에

1273
00:50:15,550 --> 00:50:19,060
두 번째로 복제되어 있습니다.

1274
00:50:19,060 --> 00:50:21,610
이렇게 하면 그룹 간에 전형적인

1275
00:50:21,610 --> 00:50:23,960
데이터 병렬 처리를 수행합니다.

1276
00:50:23,960 --> 00:50:26,680
그룹 내에서는 순전파와 역전파를 수행합니다.

1277
00:50:26,680 --> 00:50:28,690
역전파가 끝나면 각 그룹은

1278
00:50:28,690 --> 00:50:31,320
자체 로컬 그래디언트를 계산합니다.

1279
00:50:31,320 --> 00:50:33,540
그리고 역전파 후에는 각 그룹이

1280
00:50:33,540 --> 00:50:36,570
그룹 간 그래디언트를 모두 감소(all

1281
00:50:36,570 --> 00:50:40,110
reduce)시켜 두 그룹 전체의 매크로 배치 그래디언트를

1282
00:50:40,110 --> 00:50:41,290
갖게 됩니다.

1283
00:50:41,290 --> 00:50:44,310
그 후 각 그룹은 매크로 배치 전체

1284
00:50:44,310 --> 00:50:46,590
그래디언트를 받은 후 독립적으로

1285
00:50:46,590 --> 00:50:49,440
그래디언트 업데이트를 할 수 있습니다.

1286
00:50:49,440 --> 00:50:51,570
이것을 다차원 병렬성이라고 하는데,

1287
00:50:51,570 --> 00:50:53,873
이제 기본적으로 두 개의 다른 축,

1288
00:50:53,873 --> 00:50:56,040
두 가지 다른 전략을 동시에

1289
00:50:56,040 --> 00:50:58,200
사용해 계산을 병렬화하기 때문입니다.

1290
00:50:58,200 --> 00:51:01,103
이것이 유용할 수 있는 이유는 이 두

1291
00:51:01,103 --> 00:51:02,520
가지 병렬 방식에

1292
00:51:02,520 --> 00:51:04,870
필요한 통신량이 다르기 때문입니다.

1293
00:51:04,870 --> 00:51:07,398
Fully sharded

1294
00:51:07,398 --> 00:51:08,940
parallelism을 생각해보면,

1295
00:51:08,940 --> 00:51:11,250
실제로 무엇을 통신해야 할까요?

1296
00:51:11,250 --> 00:51:13,480
FSDP 동안 순전파에서는

1297
00:51:13,480 --> 00:51:16,433
가중치를 여기저기 복사했었죠.

1298
00:51:16,433 --> 00:51:17,850
순전파 동안 네트워크

1299
00:51:17,850 --> 00:51:20,058
가중치 전체 복사본 하나를

1300
00:51:20,058 --> 00:51:20,590
통신합니다.

1301
00:51:20,590 --> 00:51:22,140
역전파 동안에는 네트워크

1302
00:51:22,140 --> 00:51:24,103
가중치를 다시 통신해야 하고,

1303
00:51:24,103 --> 00:51:26,020
그래디언트도 통신해야 합니다.

1304
00:51:26,020 --> 00:51:29,960
즉, FSDP를 사용할 때 단일

1305
00:51:29,960 --> 00:51:31,700
순전파-역전파 패스

1306
00:51:31,700 --> 00:51:33,250
동안 네트워크

1307
00:51:33,250 --> 00:51:35,410
가중치를 세 번 통신해야

1308
00:51:35,410 --> 00:51:36,650
합니다.

1309
00:51:36,650 --> 00:51:40,335
하지만 일반 데이터 병렬 처리에서는 각 그룹이 독립적인 가중치

1310
00:51:40,335 --> 00:51:42,710
복사본을 유지하므로 그래디언트만 all

1311
00:51:42,710 --> 00:51:44,460
reduce 하면 됩니다.

1312
00:51:44,460 --> 00:51:47,110
즉, 여러 데이터 병렬 그룹 간에는

1313
00:51:47,110 --> 00:51:49,120
순전파-역전파 동안 네트워크

1314
00:51:49,120 --> 00:51:51,560
가중치를 한 번만 통신하면 됩니다.

1315
00:51:51,560 --> 00:51:54,520
이것은 GPU 클러스터 내 여러 계층

1316
00:51:54,520 --> 00:51:56,060
구조 개념과 연결됩니다.

1317
00:51:56,060 --> 00:51:57,890
예를 들어, 하나의 머신

1318
00:51:57,890 --> 00:52:00,460
내에 8개의 GPU가 높은 인터커넥트로

1319
00:52:00,460 --> 00:52:02,140
연결된 GPU 서버가

1320
00:52:02,140 --> 00:52:03,440
있을 수 있습니다.

1321
00:52:03,440 --> 00:52:05,470
이들은 FSDP 그룹이 될 수 있는데, FSDP

1322
00:52:05,470 --> 00:52:07,943
그룹 내에서는 더 많은 통신이 필요하기 때문입니다.

1323
00:52:07,943 --> 00:52:09,610
하지만 다른 축으로

1324
00:52:09,610 --> 00:52:11,870
여러 서버가 있을 수 있습니다.

1325
00:52:11,870 --> 00:52:14,483
즉, 한 서버는 모델 가중치 전체 복사본을 가지고,

1326
00:52:14,483 --> 00:52:17,150
다른 서버도 또 다른 전체 복사본을 가집니다.

1327
00:52:17,150 --> 00:52:18,900
서버 간 통신은 서버 내부

1328
00:52:18,900 --> 00:52:21,380
통신보다 느리다는 점을 기억하세요.

1329
00:52:21,380 --> 00:52:26,260
이것이 바로 우리가 알고 있는 네트워크 토폴로지를

1330
00:52:26,260 --> 00:52:28,370
활용해 알고리즘을

1331
00:52:28,370 --> 00:52:30,680
설계하는 첫 번째 예입니다.

1332
00:52:30,680 --> 00:52:33,335
질문은, 이런 것들은 조정하기 불가능한데,

1333
00:52:33,335 --> 00:52:35,188
여러분은 어떤 쪽을 선호하시겠습니까?

1334
00:52:35,188 --> 00:52:36,355
정말, 정말 말하기 어렵습니다.

1335
00:52:40,502 --> 00:52:43,080
그리고 기본적으로, 일단 데이터 병렬화, 즉

1336
00:52:43,080 --> 00:52:45,720
DP, FSDP, HSDP를 갖추면, 이것이

1337
00:52:45,720 --> 00:52:48,060
실제로 꽤 멀리 갈 수 있는 방법입니다.

1338
00:52:48,060 --> 00:52:51,860
예를 들어, 1000억 개의 파라미터를 가진 모델은

1339
00:52:51,860 --> 00:52:54,860
저장하는 데 800GB의 메모리가 필요합니다.

1340
00:52:54,860 --> 00:52:56,970
그리고 이걸 80개의 GPU에 나누면,

1341
00:52:56,970 --> 00:52:59,070
GPU당 10GB의 메모리만 필요합니다.

1342
00:52:59,070 --> 00:53:03,560
그래서 FSDP를 갖추면 꽤 큰 모델을 가질 수 있습니다.

1343
00:53:03,560 --> 00:53:06,320
하지만 또 다른 문제는 모델의 활성화 값들이 메모리를

1344
00:53:06,320 --> 00:53:07,590
채우기 시작한다는 점입니다.

1345
00:53:07,590 --> 00:53:11,510
Llama3-405B로 다시 돌아가 보면, 126개의 레이어,

1346
00:53:11,510 --> 00:53:15,740
모델 차원 16,000, 시퀀스 길이 4,096인 트랜스포머입니다.

1347
00:53:15,740 --> 00:53:18,050
순전파 동안 숨겨진 상태를 저장하는

1348
00:53:18,050 --> 00:53:19,910
데 얼마나 많은 GPU 메모리가

1349
00:53:19,910 --> 00:53:22,560
필요한지 상상해 보세요, 엄청날 겁니다.

1350
00:53:22,560 --> 00:53:24,577
그래서 모델과 시퀀스가 정말

1351
00:53:24,577 --> 00:53:27,160
커지면 GPU 메모리가 금방

1352
00:53:27,160 --> 00:53:27,890
부족해집니다.

1353
00:53:27,890 --> 00:53:29,650
그래서 또 다른 방법이 있는데,

1354
00:53:29,650 --> 00:53:31,387
활성화 체크포인팅이라고

1355
00:53:31,387 --> 00:53:33,220
합니다. 이건 모든 활성화를

1356
00:53:33,220 --> 00:53:34,918
메모리에 저장하지 않고,

1357
00:53:34,918 --> 00:53:37,210
역전파 때 다시 계산한다는 뜻입니다.

1358
00:53:37,210 --> 00:53:40,720
이게 어떻게 작동하는지 이해하려면, 신경망을 조금 다르게

1359
00:53:40,720 --> 00:53:42,470
생각하는 게 도움이 됩니다. 신경망의

1360
00:53:42,470 --> 00:53:44,905
각 레이어는 사실 두 가지 일을 합니다.

1361
00:53:44,905 --> 00:53:47,300
신경망의 각 레이어는 두 가지 일을 합니다.

1362
00:53:47,300 --> 00:53:49,030
다음 레이어의 활성화를 계산하는

1363
00:53:49,030 --> 00:53:50,390
순전파를 수행합니다.

1364
00:53:50,390 --> 00:53:52,098
그리고 상류 그래디언트와

1365
00:53:52,098 --> 00:53:54,940
활성화를 모두 사용해 그래디언트를 계산하는

1366
00:53:54,940 --> 00:53:56,540
역전파를 수행합니다.

1367
00:53:56,540 --> 00:54:00,170
보통 이 모든 계산과 메모리는 얼마나 필요할까요?

1368
00:54:00,170 --> 00:54:02,800
만약 이 모든 것이 일정하다고 가정하면,

1369
00:54:02,800 --> 00:54:07,195
일반적으로 순전파 시에는 1, 2, 3, 4, 네 단계가

1370
00:54:07,195 --> 00:54:07,820
걸립니다.

1371
00:54:07,820 --> 00:54:09,220
순전파 중에 그 활성화들을

1372
00:54:09,220 --> 00:54:10,400
기억하게 됩니다.

1373
00:54:10,400 --> 00:54:13,550
그다음 역전파 시에도 1, 2, 3, 4 단계가 걸립니다.

1374
00:54:13,550 --> 00:54:15,620
그래서 일반적인 순전파-역전파

1375
00:54:15,620 --> 00:54:20,367
과정에서는 N층 네트워크에 대해 O(N) 연산과 O(N) 메모리가
필요합니다.

1376
00:54:20,367 --> 00:54:22,700
하지만 방금 말했듯이, 이 방법은 메모리가 부족해질 겁니다.

1377
00:54:22,700 --> 00:54:24,500
그래서 대신 역전파 시에

1378
00:54:24,500 --> 00:54:27,000
활성화를 다시 계산한다고 상상할 수 있습니다.

1379
00:54:27,000 --> 00:54:29,220
그 모습은 대략 이런 식입니다.

1380
00:54:29,220 --> 00:54:30,750
활성화부터 시작합니다.

1381
00:54:30,750 --> 00:54:33,125
첫 번째 층을 실행하고 바로 첫 번째

1382
00:54:33,125 --> 00:54:34,742
층의 활성화를 버립니다—첫

1383
00:54:34,742 --> 00:54:36,450
번째 층의 순전파를 실행하고요.

1384
00:54:36,450 --> 00:54:38,492
그리고 바로 그 활성화를 버리면서 이

1385
00:54:38,492 --> 00:54:39,930
과정을 네 번 반복합니다.

1386
00:54:39,930 --> 00:54:41,820
이제 네트워크를 한 번 통과해서

1387
00:54:41,820 --> 00:54:43,590
마지막 층의 활성화를 얻었습니다.

1388
00:54:43,590 --> 00:54:46,290
이 시점에서 마지막 층에 대해 역전파를 계산할 수

1389
00:54:46,290 --> 00:54:48,210
있지만, 이제 좀 곤란해졌습니다.

1390
00:54:48,210 --> 00:54:50,450
다음 역전파를 계산하려면 A3의 활성화가 필요한데,

1391
00:54:50,450 --> 00:54:53,390
그것이 없기 때문입니다. 하지만 다시 계산할 수 있습니다.

1392
00:54:53,390 --> 00:54:54,947
그래서 다시 계산합니다.

1393
00:54:54,947 --> 00:54:56,280
이제 역전파를 할 수 있습니다.

1394
00:54:56,280 --> 00:54:57,630
그리고 다시 좀 더 계산합니다.

1395
00:54:57,630 --> 00:54:58,800
이제 역전파를 수행합니다.

1396
00:54:58,800 --> 00:54:59,610
이제 다시 계산합니다.

1397
00:54:59,610 --> 00:55:01,050
이제 또 다른 역전파를 수행합니다.

1398
00:55:01,050 --> 00:55:03,710
이것을 모두 더하면, N개의 레이어를 가진

1399
00:55:03,710 --> 00:55:07,160
네트워크에 대해 N 제곱의 계산량과 일정한 메모리가

1400
00:55:07,160 --> 00:55:11,030
필요합니다. 왜냐하면 합이 N, N-1, N-2, N-3,...

1401
00:55:11,030 --> 00:55:12,780
1까지이기 때문입니다.

1402
00:55:12,780 --> 00:55:14,450
이것은 이차 시간 복잡도입니다.

1403
00:55:14,450 --> 00:55:15,840
그리고 이것을 나눌 수 있습니다.

1404
00:55:15,840 --> 00:55:18,510
N 제곱의 계산량은 깊은 네트워크에 꽤 부담스럽습니다.

1405
00:55:18,510 --> 00:55:20,880
그래서 대신 모든 것을 다시 계산하지 맙시다.

1406
00:55:20,880 --> 00:55:23,560
대신 매 C 레이어마다 활성화 체크포인트를

1407
00:55:23,560 --> 00:55:24,680
찍는다고 상상해 봅시다.

1408
00:55:24,680 --> 00:55:30,290
그래서 네트워크 내에서 더 작은 블록 단위로만 다시 계산할 것입니다.

1409
00:55:30,290 --> 00:55:33,010
그렇다면 C개의 체크포인트를 찍어 네트워크

1410
00:55:33,010 --> 00:55:35,080
전반에 걸쳐 활성화를 C번

1411
00:55:35,080 --> 00:55:36,760
기억하면, 계산량은 N

1412
00:55:36,760 --> 00:55:39,010
제곱 나누기 C가 되고 메모리는

1413
00:55:39,010 --> 00:55:40,143
O(C)가 됩니다.

1414
00:55:40,143 --> 00:55:41,560
그리고 흔히 C를 루트

1415
00:55:41,560 --> 00:55:44,440
N으로 설정하는데, 이 경우 계산량은 N

1416
00:55:44,440 --> 00:55:47,915
곱하기 루트 N이 되고 메모리는 O(루트 N)이 됩니다.

1417
00:55:47,915 --> 00:55:49,540
이것이 계산량과 메모리를

1418
00:55:49,540 --> 00:55:51,580
절충하여 더 큰 모델을 훈련할

1419
00:55:51,580 --> 00:55:53,795
수 있는 꽤 일반적인 방법입니다.

1420
00:55:53,795 --> 00:55:54,295
좋습니다.

1421
00:55:54,295 --> 00:55:55,960
이제 FSDP, 활성화

1422
00:55:55,960 --> 00:56:00,540
체크포인트, HSDP가 있으면, 여기서 많은 성과를 낼 수

1423
00:56:00,540 --> 00:56:01,040
있습니다.

1424
00:56:01,040 --> 00:56:03,130
정말 큰 모델들을 훈련하기 시작할 수 있습니다.

1425
00:56:03,130 --> 00:56:05,740
그 레시피는 기본적으로 다음과 같습니다.

1426
00:56:05,740 --> 00:56:07,930
여기서부터 꽤 멀리 갈 수 있는 스케일링

1427
00:56:07,930 --> 00:56:11,570
레시피는 먼저, 데이터 병렬 처리, 그냥 순수한 데이터 병렬

1428
00:56:11,570 --> 00:56:15,430
처리를 사용하는 겁니다. 대략 128개의 GPU까지, 그리고 대략 10억

1429
00:56:15,430 --> 00:56:17,900
개 정도의 파라미터를 가진 모델까지입니다.

1430
00:56:17,900 --> 00:56:19,750
이 정도 크기의 모델에는 그냥 일반적인 데이터

1431
00:56:19,750 --> 00:56:20,810
병렬 처리를 하면 됩니다.

1432
00:56:20,810 --> 00:56:22,700
이 방법은 꽤 잘 작동하는 편입니다.

1433
00:56:22,700 --> 00:56:24,650
그리고 거의 항상 GPU당 로컬

1434
00:56:24,650 --> 00:56:27,470
배치 크기를 GPU 메모리를 최대한 활용하도록

1435
00:56:27,470 --> 00:56:28,770
설정하는 것이 좋습니다.

1436
00:56:28,770 --> 00:56:31,250
그게 거의 항상 올바른 선택입니다.

1437
00:56:31,250 --> 00:56:33,180
그리고 모델이 커지기 시작하면,

1438
00:56:33,180 --> 00:56:34,850
모델 자체가 GPU 내부에서

1439
00:56:34,850 --> 00:56:36,750
많은 메모리를 차지하게 됩니다.

1440
00:56:36,750 --> 00:56:38,970
그래서 문제가 발생하기 시작하죠.

1441
00:56:38,970 --> 00:56:42,750
이것은 GPU 메모리 용량과 인터커넥트 속도에

1442
00:56:42,750 --> 00:56:44,280
따라 다릅니다.

1443
00:56:44,280 --> 00:56:46,100
하지만 일반적으로 모델이 10억

1444
00:56:46,100 --> 00:56:47,760
개 이상의 파라미터가 되면,

1445
00:56:47,760 --> 00:56:49,160
데이터 병렬 처리에서

1446
00:56:49,160 --> 00:56:50,720
완전 분산 데이터 병렬

1447
00:56:50,720 --> 00:56:53,210
처리(FSDP)로 전환하는 것을 고려해야 합니다.

1448
00:56:53,210 --> 00:56:55,463
이 시점에서 꽤 많이 확장할 수 있지만,

1449
00:56:55,463 --> 00:56:57,380
활성화 메모리 병목 현상에

1450
00:56:57,380 --> 00:56:58,380
부딪히게 됩니다.

1451
00:56:58,380 --> 00:57:00,360
그때 활성화 체크포인팅을 켜는 겁니다.

1452
00:57:00,360 --> 00:57:02,180
활성화 체크포인팅은 속도를

1453
00:57:02,180 --> 00:57:04,160
많이 느리게 해서 불편하지만,

1454
00:57:04,160 --> 00:57:07,640
훨씬 더 큰 모델을 학습할 수 있게 해줍니다.

1455
00:57:07,640 --> 00:57:10,523
이 방법은 수백 개의 GPU까지 확장할 수 있습니다.

1456
00:57:10,523 --> 00:57:12,440
그리고 보통 클러스터

1457
00:57:12,440 --> 00:57:15,530
토폴로지에 따라 다르지만, 대략 256개에서

1458
00:57:15,530 --> 00:57:18,440
512개 GPU 정도, 수백 개

1459
00:57:18,440 --> 00:57:22,880
이상의 장치에 도달하면 FSDP가 너무 비용이 많이 듭니다.

1460
00:57:22,880 --> 00:57:25,580
그때는 HSDP로 전환해야 합니다.

1461
00:57:25,580 --> 00:57:27,860
그리고 이 방법을 사용하면 대략

1462
00:57:27,860 --> 00:57:31,140
수십억 개의 파라미터를 가진 모델을, 아마

1463
00:57:31,140 --> 00:57:34,190
천 개 정도의 GPU에서 꽤 긴 시퀀스

1464
00:57:34,190 --> 00:57:36,060
길이로 학습할 수 있습니다.

1465
00:57:36,060 --> 00:57:37,490
그래서 꽤 좋은 편입니다.

1466
00:57:37,490 --> 00:57:40,530
하지만 만약 천 개 이상의 GPU, 500억

1467
00:57:40,530 --> 00:57:42,390
개 이상의 파라미터 모델,

1468
00:57:42,390 --> 00:57:46,020
시퀀스 길이가 10,000 이상이라면, 이때는 더

1469
00:57:46,020 --> 00:57:48,020
고급 전략인 컨텍스트 병렬화,

1470
00:57:48,020 --> 00:57:49,940
파이프라인 병렬화, 텐서

1471
00:57:49,940 --> 00:57:51,707
병렬화로 전환해야 합니다.

1472
00:57:51,707 --> 00:57:53,040
그리고 큰 질문이 있습니다.

1473
00:57:53,040 --> 00:57:53,700
이게, 세상에.

1474
00:57:53,700 --> 00:57:55,110
조절해야 할 변수가 너무 많습니다.

1475
00:57:55,110 --> 00:57:56,568
어떻게 최적화해야 할까요?

1476
00:57:56,568 --> 00:57:59,120
글로벌 배치 크기, 로컬 배치 크기,

1477
00:57:59,120 --> 00:58:01,860
HSDP 차원, FSDP 차원,

1478
00:58:01,860 --> 00:58:03,500
재계산량까지 설정해야 합니다.

1479
00:58:03,500 --> 00:58:04,168
완전히 혼란스럽습니다.

1480
00:58:04,168 --> 00:58:04,710
어떻게 해야 하나요?

1481
00:58:04,710 --> 00:58:07,880
변수가 너무 많아서 어떻게 해야 할지 모르겠어요.

1482
00:58:07,880 --> 00:58:10,340
답은 Model Flops Utilization, 즉

1483
00:58:10,340 --> 00:58:12,320
MFU라는 매우 중요한 지표를 최적화하는 것입니다.

1484
00:58:12,320 --> 00:58:15,680
GPU 병렬화의 바다에서 길을 잃었을 때, Model Flops

1485
00:58:15,680 --> 00:58:17,630
Utilization이 여러분의 길잡이가 되어줍니다.

1486
00:58:17,630 --> 00:58:19,047
이 지표를 따르면 학습

1487
00:58:19,047 --> 00:58:21,740
스택을 최적화하는 방법을 알려줄 겁니다.

1488
00:58:21,740 --> 00:58:23,820
하지만 Model Flops Utilization에 대해 이야기하기 전에,

1489
00:58:23,820 --> 00:58:25,980
Hardware Flops Utilization에 대해 먼저 알아야 합니다.

1490
00:58:25,980 --> 00:58:28,580
이전에 말했듯이, 이론적으로 H100은

1491
00:58:28,580 --> 00:58:34,142
텐서 코어에서 초당 989.4 TFLOPs의 연산을 수행할 수 있습니다.

1492
00:58:34,142 --> 00:58:35,100
하지만 이건 이론적인 이야기입니다.

1493
00:58:35,100 --> 00:58:36,560
실제로 얼마나 얻을 수 있을까요?

1494
00:58:36,560 --> 00:58:38,390
질문은 실제로 얼마나 달성할

1495
00:58:38,390 --> 00:58:40,190
수 있느냐는 겁니다.

1496
00:58:40,190 --> 00:58:42,450
이것이 바로 Hardware Flops Utilization의 지표입니다.

1497
00:58:42,450 --> 00:58:44,340
장치에서 어떤 연산을 실행하고 있습니다.

1498
00:58:44,340 --> 00:58:46,070
그 이론적 최대치 중에서 실제로

1499
00:58:46,070 --> 00:58:47,750
얼마나 연산을 실현하느냐는 거죠.

1500
00:58:47,750 --> 00:58:48,980
이건 어렵지 않습니다.

1501
00:58:48,980 --> 00:58:50,605
PyTorch 코드 몇 줄만

1502
00:58:50,605 --> 00:58:52,170
작성하면 벤치마크할 수 있습니다.

1503
00:58:52,170 --> 00:58:54,710
이건 제가 어제 H100에서 실행한

1504
00:58:54,710 --> 00:58:55,820
벤치마크입니다.

1505
00:58:55,820 --> 00:58:58,410
보시면 x축이 기본적으로 무엇을 하는지 알 수 있습니다.

1506
00:58:58,410 --> 00:59:00,960
단순히 루프에서 밀집 행렬 곱셈을 수행하고,

1507
00:59:00,960 --> 00:59:03,390
행렬 곱셈이 얼마나 걸렸는지 측정합니다.

1508
00:59:03,390 --> 00:59:05,420
행렬 곱셈에 걸린 시간을 측정하는 거죠.

1509
00:59:05,420 --> 00:59:07,890
행렬 곱셈이 몇 플롭스가 필요한지 계산할 수 있습니다.

1510
00:59:07,890 --> 00:59:10,910
그리고 x축에는 512부터 32,

1511
00:59:10,910 --> 00:59:13,980
000까지 행렬 크기를 표시했습니다.

1512
00:59:13,980 --> 00:59:16,673
y축은 하드웨어 플롭스 활용도,

1513
00:59:16,673 --> 00:59:19,090
즉 이론적 최대

1514
00:59:19,090 --> 00:59:21,220
처리량 대비 실제로 실현한

1515
00:59:21,220 --> 00:59:22,700
비율입니다.

1516
00:59:22,700 --> 00:59:25,870
이 단순한 PyTorch 루프에서 8,000x8,

1517
00:59:25,870 --> 00:59:29,560
000 정도의 큰 행렬 곱셈에서는 H100에서

1518
00:59:29,560 --> 00:59:33,020
약 80%의 HFU를 얻는 걸 볼 수 있습니다.

1519
00:59:33,020 --> 00:59:34,250
그럼 꽤 괜찮은 거죠.

1520
00:59:34,250 --> 00:59:36,100
하지만 문제는 HFU가 모델이

1521
00:59:36,100 --> 00:59:39,220
해야 하는 다른 모든 작업을 고려하지 않는다는 겁니다.

1522
00:59:39,220 --> 00:59:41,570
우리는 아마 활성화 계산을 하고 있을 겁니다.

1523
00:59:41,570 --> 00:59:43,730
아마 옆에서 다른 모델도 실행하고 있을 수 있죠.

1524
00:59:43,730 --> 00:59:45,680
아마 데이터 로딩, 데이터 증강도 하고 있을 겁니다.

1525
00:59:45,680 --> 00:59:47,222
GPU가 원시 모델의 순전파

1526
00:59:47,222 --> 00:59:49,850
역전파만 하는 게 아니라 다른 많은 작업도 하고 있습니다.

1527
00:59:49,850 --> 00:59:52,330
그래서 여기서 Hardware Flops Utilization에서 Model

1528
00:59:52,330 --> 00:59:54,290
Flops Utilization으로 넘어가는 거죠.

1529
00:59:54,290 --> 00:59:56,530
Model Flops Utilization은

1530
00:59:56,530 --> 01:00:00,250
기본적으로 내 모델의 순전파 역전파에 GPU 이론적

1531
01:00:00,250 --> 01:00:03,610
TFLOPs 중 몇 퍼센트를 사용하고 있는지를 말합니다.

1532
01:00:03,610 --> 01:00:06,050
이게 항상 최적화하려는 대상입니다.

1533
01:00:06,050 --> 01:00:09,340
좀 더 구체적으로 말하면, 모델 아키텍처,

1534
01:00:09,340 --> 01:00:10,740
레이어 수와

1535
01:00:10,740 --> 01:00:13,240
크기를 기반으로 미니배치

1536
01:00:13,240 --> 01:00:15,020
데이터에 대해 전체 순전파

1537
01:00:15,020 --> 01:00:16,610
역전파를 수행하는

1538
01:00:16,610 --> 01:00:19,520
데 필요한 플롭 수를 계산합니다.

1539
01:00:19,520 --> 01:00:22,787
그다음 실행하는 장치의 이론적 최대

1540
01:00:22,787 --> 01:00:24,120
처리량을 찾아봅니다.

1541
01:00:24,120 --> 01:00:25,370
그리고 두 값을 나누는 거죠.

1542
01:00:25,370 --> 01:00:28,070
그럼 장치의 이론적 최대 처리량을 달성했을

1543
01:00:28,070 --> 01:00:31,070
때 전체 순전파 역전파가 얼마나 걸려야 하는지

1544
01:00:31,070 --> 01:00:32,160
알 수 있습니다.

1545
01:00:32,160 --> 01:00:34,760
이게 모델의 순전파 역전파를 할 수 있는

1546
01:00:34,760 --> 01:00:36,726
이론상 가장 빠른 시간입니다.

1547
01:00:36,726 --> 01:00:40,828
그다음 실제로 모델의 순전파 역전파 시간을 측정합니다.

1548
01:00:40,828 --> 01:00:42,870
훈련 루프는 이 모든 다른 작업도 수행하고 있죠.

1549
01:00:42,870 --> 01:00:44,270
데이터 로딩을 하고 있습니다.

1550
01:00:44,270 --> 01:00:45,360
증강을 하고 있습니다.

1551
01:00:45,360 --> 01:00:46,590
통신을 하고 있습니다.

1552
01:00:46,590 --> 01:00:48,900
아마도 activation checkpointing을 하고 있습니다.

1553
01:00:48,900 --> 01:00:50,760
즉, 재계산을 하면서 역전파를 수행하고 있습니다.

1554
01:00:50,760 --> 01:00:52,552
훈련 루프가 많은 작업을 수행하고 있는 거죠.

1555
01:00:52,552 --> 01:00:54,323
실제로 얼마나 걸리는지 시간을 재보세요.

1556
01:00:54,323 --> 01:00:55,740
그리고 그 두 숫자를 나누면 됩니다.

1557
01:00:55,740 --> 01:00:58,340
그럼 0과 1 사이의 숫자가 나오는데,

1558
01:00:58,340 --> 01:01:00,560
이 숫자는 이론적인 최대치

1559
01:01:00,560 --> 01:01:03,770
중 훈련 루프에서 실제로 달성하는 비율입니다.

1560
01:01:03,770 --> 01:01:06,530
이것이 바로 MFU, 즉 Model Flops Utilization입니다.

1561
01:01:06,530 --> 01:01:09,080
다시 말해, 비교적 간단한 PyTorch 코드로 벤치마크할

1562
01:01:09,080 --> 01:01:09,970
수 있습니다.

1563
01:01:09,970 --> 01:01:11,720
여기 단순한 다층

1564
01:01:11,720 --> 01:01:15,790
퍼셉트론(MLP)에서 ReLU 비선형성을 사용하고, 매우 넓은

1565
01:01:15,790 --> 01:01:19,420
MLP 층과 거대한 배치 크기로 단일 H100에서

1566
01:01:19,420 --> 01:01:22,460
순전파와 역전파를 실행하는 예가 있습니다.

1567
01:01:22,460 --> 01:01:24,555
이 경우 약 50% MFU를 얻고 있습니다.

1568
01:01:27,220 --> 01:01:28,870
일반적으로 분산 훈련의

1569
01:01:28,870 --> 01:01:30,980
설정을 조정할 때는 가능한

1570
01:01:30,980 --> 01:01:32,650
모든 설정을 조정해

1571
01:01:32,650 --> 01:01:35,202
MFU를 최대화하려고 해야 합니다.

1572
01:01:35,202 --> 01:01:37,660
왜냐하면 훈련 처리량 최적화에서

1573
01:01:37,660 --> 01:01:40,090
가장 중요한 지표이기 때문입니다.

1574
01:01:40,090 --> 01:01:44,440
요즘은 일반적으로 MFU가 30% 이상이면 꽤 좋은

1575
01:01:44,440 --> 01:01:45,440
편입니다.

1576
01:01:45,440 --> 01:01:47,117
만약 30% 이하라면, 아마도

1577
01:01:47,117 --> 01:01:48,700
어딘가에 큰 병목 현상이 있거나

1578
01:01:48,700 --> 01:01:50,030
문제가 발생한 것입니다.

1579
01:01:50,030 --> 01:01:52,790
그리고 40% 이상이면 꽤, 꽤 훌륭한 수준입니다.

1580
01:01:52,790 --> 01:01:54,412
그리고 그것이 기본적으로 최신 기술 수준입니다.

1581
01:01:54,412 --> 01:01:55,870
여기 몇몇 논문에서 가져온

1582
01:01:55,870 --> 01:01:57,160
숫자들이 있습니다.

1583
01:01:57,160 --> 01:02:00,580
특히, 우리가 이야기했던 Llama3-405B

1584
01:02:00,580 --> 01:02:02,215
논문입니다.

1585
01:02:02,215 --> 01:02:04,750
최종 훈련 단계에서, 그들은 8,

1586
01:02:04,750 --> 01:02:07,480
000개에서 16,000개 GPU를 동시에

1587
01:02:07,480 --> 01:02:11,400
사용하는 몇 가지 다른 훈련 변형을 가지고 있습니다.

1588
01:02:11,400 --> 01:02:15,250
그 결과, MFU가 대략 30대 후반에서 40대 초반 정도

1589
01:02:15,250 --> 01:02:15,970
나옵니다.

1590
01:02:15,970 --> 01:02:18,330
그것은 꽤 좋은 수치입니다.

1591
01:02:18,330 --> 01:02:19,890
H100에서는 그

1592
01:02:19,890 --> 01:02:22,140
이상으로 크게 올라가기는 어렵습니다.

1593
01:02:22,140 --> 01:02:25,200
사실, 역설적으로, 최신 장치일수록 MFU가 더

1594
01:02:25,200 --> 01:02:26,400
낮을 때도 있습니다.

1595
01:02:26,400 --> 01:02:29,350
이전 세대 장치인 H100에서는 때때로 50%

1596
01:02:29,350 --> 01:02:31,690
이상의 MFU를 얻을 수 있었습니다.

1597
01:02:31,690 --> 01:02:33,960
그 이유는 GPU가 통신

1598
01:02:33,960 --> 01:02:36,810
속도보다 더 빠르게 빨라지고 있기

1599
01:02:36,810 --> 01:02:37,780
때문입니다.

1600
01:02:37,780 --> 01:02:40,330
A100에서 H100으로 넘어가면서

1601
01:02:40,330 --> 01:02:43,170
계산 처리량은 이론적으로 약 3배 향상되었지만,

1602
01:02:43,170 --> 01:02:45,570
메모리 대역폭은 이론적으로

1603
01:02:45,570 --> 01:02:47,280
2배 향상에 그쳤습니다.

1604
01:02:47,280 --> 01:02:50,940
그래서 GPU는 매우 빠르게 빨라지고 있지만, GPU

1605
01:02:50,940 --> 01:02:52,810
간 통신을 확장하는

1606
01:02:52,810 --> 01:02:55,780
것은 더 어려워지는 격차가 커지고 있습니다.

1607
01:02:55,780 --> 01:02:58,350
그 결과, 최신 세대 장치에서는 오히려 MFU가

1608
01:02:58,350 --> 01:03:00,335
더 낮아지는 경우가 종종 있습니다.

1609
01:03:03,015 --> 01:03:05,760
이 점들에 대부분 시간을 할애한 이유는,

1610
01:03:05,760 --> 01:03:07,803
여러분이 실제로 사용할

1611
01:03:07,803 --> 01:03:09,970
가능성이 높은 부분이기 때문입니다.

1612
01:03:09,970 --> 01:03:12,400
이 방에 있는 누구도 아마 10,000 GPU

1613
01:03:12,400 --> 01:03:14,060
클러스터에 접근할 수 없을 겁니다.

1614
01:03:14,060 --> 01:03:15,980
수업 끝나고 저한테 이야기하러 오세요.

1615
01:03:15,980 --> 01:03:18,310
친구가 되고 싶습니다.

1616
01:03:18,310 --> 01:03:19,750
그래서 실제로 마주칠

1617
01:03:19,750 --> 01:03:21,730
가능성이 높은 것들은 수백 개의

1618
01:03:21,730 --> 01:03:23,620
GPU까지 사용하는 경우입니다.

1619
01:03:23,620 --> 01:03:25,700
하지만 제가 그냥 좋아하는 다른 것들도 있습니다.

1620
01:03:25,700 --> 01:03:27,980
여기 꽤 괜찮은 슬라이드들이 있지만, 이걸

1621
01:03:27,980 --> 01:03:29,710
전부 자세히 다루지 않아도

1622
01:03:29,710 --> 01:03:30,210
괜찮습니다.

1623
01:03:30,210 --> 01:03:32,320
오프라인에서 확인하실 수 있습니다.

1624
01:03:32,320 --> 01:03:34,480
그래서 컨텍스트 병렬처리는 기본적으로

1625
01:03:34,480 --> 01:03:36,080
시퀀스 차원에서 나누는 것입니다.

1626
01:03:36,080 --> 01:03:38,930
트랜스포머는 시퀀스 위에서 작동한다고 했습니다.

1627
01:03:38,930 --> 01:03:42,715
기본적으로 긴 시퀀스가 있다고 생각하세요.

1628
01:03:42,715 --> 01:03:45,925
서로 다른 GPU가 시퀀스의 다른 부분을 처리하게 하는 겁니다.

1629
01:03:45,925 --> 01:03:48,587
기억하신다면, 트랜스포머 블록에서

1630
01:03:48,587 --> 01:03:50,920
LayerNorm, FFN,

1631
01:03:50,920 --> 01:03:54,640
MLP, 그리고 잔차 연결은 시퀀스 전체에

1632
01:03:54,640 --> 01:03:56,230
독립적으로 작동하기

1633
01:03:56,230 --> 01:03:59,133
때문에 이 부분은 사실 쉽습니다.

1634
01:03:59,133 --> 01:04:00,550
그래서 시퀀스

1635
01:04:00,550 --> 01:04:03,100
차원에서 계산을 나누는 것은

1636
01:04:03,100 --> 01:04:05,050
비교적 간단합니다.

1637
01:04:05,050 --> 01:04:07,320
MLP 내부에서는 약간 복잡해지긴 합니다. 거기에

1638
01:04:07,320 --> 01:04:08,778
가중치가 있기 때문입니다.

1639
01:04:08,778 --> 01:04:10,950
그래서 데이터 병렬 처리에서 했던 것처럼

1640
01:04:10,950 --> 01:04:13,685
그래디언트에 대해 all reduce를 해야 합니다.

1641
01:04:13,685 --> 01:04:15,060
시퀀스 병렬처리에서

1642
01:04:15,060 --> 01:04:16,690
어려운 부분은 어텐션입니다.

1643
01:04:16,690 --> 01:04:18,360
어텐션에서는 시퀀스

1644
01:04:18,360 --> 01:04:20,670
내 모든 원소 쌍 사이의

1645
01:04:20,670 --> 01:04:23,310
상호작용을 계산해야 하기 때문입니다.

1646
01:04:23,310 --> 01:04:25,380
QKV 프로젝션은 시퀀스에

1647
01:04:25,380 --> 01:04:27,490
대해 쉽게 병렬화할 수 있어서

1648
01:04:27,490 --> 01:04:30,450
간단합니다만, 핵심 어텐션 매트릭스는

1649
01:04:30,450 --> 01:04:32,640
병렬화하기가 꽤 까다롭습니다.

1650
01:04:32,640 --> 01:04:34,713
사람들이 처음 개발한 버전은 링

1651
01:04:34,713 --> 01:04:36,630
어텐션이라고 불리는데, 전체

1652
01:04:36,630 --> 01:04:39,850
어텐션 매트릭스를 블록 단위로 나누고, GPU들이

1653
01:04:39,850 --> 01:04:43,470
그 블록들을 독립적으로 병렬 처리하면서 올바른

1654
01:04:43,470 --> 01:04:45,510
순서로 작업해서 모든 게 잘

1655
01:04:45,510 --> 01:04:47,133
작동하도록 하는 방식입니다.

1656
01:04:47,133 --> 01:04:48,550
거기에 많은 세부 사항이 있습니다.

1657
01:04:48,550 --> 01:04:50,760
자세한 내용은 논문을 참고하시면 됩니다.

1658
01:04:50,760 --> 01:04:53,320
두 번째 방법은 개념적으로 좀 더 쉬운

1659
01:04:53,320 --> 01:04:56,010
Ulysses 어텐션인데, 헤드 단위로

1660
01:04:56,010 --> 01:04:57,520
병렬화를 하는 겁니다.

1661
01:04:57,520 --> 01:04:59,610
트랜스포머에서는 거의 항상

1662
01:04:59,610 --> 01:05:01,380
멀티헤드 어텐션을 사용해서

1663
01:05:01,380 --> 01:05:04,500
여러 어텐션 매트릭스를 병렬로 계산한다는

1664
01:05:04,500 --> 01:05:05,800
점을 기억하세요.

1665
01:05:05,800 --> 01:05:07,530
Ulysses 어텐션에서는

1666
01:05:07,530 --> 01:05:11,110
핵심 어텐션 연산자의 계산을 병렬화합니다.

1667
01:05:11,110 --> 01:05:13,493
그 오버헤드를 병렬화하고,

1668
01:05:13,493 --> 01:05:14,910
나머지 트랜스포머의

1669
01:05:14,910 --> 01:05:18,300
모든 부분은 시퀀스 차원에서 병렬화합니다.

1670
01:05:18,300 --> 01:05:22,290
예를 들어, 시퀀스 길이가 매우 커질 때

1671
01:05:22,290 --> 01:05:24,780
이 컨텍스트 병렬화가

1672
01:05:24,780 --> 01:05:25,840
중요해집니다.

1673
01:05:25,840 --> 01:05:28,570
Llama3 사전학습 예제로 돌아가 보면,

1674
01:05:28,570 --> 01:05:30,670
실제로 모델을 두 단계로 학습합니다.

1675
01:05:30,670 --> 01:05:34,410
첫 번째 단계에서는 시퀀스 길이를 8,000으로 하고 컨텍스트 병렬화는

1676
01:05:34,410 --> 01:05:35,437
전혀 하지 않습니다.

1677
01:05:35,437 --> 01:05:37,770
두 번째 단계에서는 시퀀스

1678
01:05:37,770 --> 01:05:40,540
길이를 130,000으로 늘립니다.

1679
01:05:40,540 --> 01:05:44,260
그때 16방향 컨텍스트 병렬화를 수행합니다.

1680
01:05:44,260 --> 01:05:48,630
즉, 131,000 길이의 각 시퀀스에 대해 16개의

1681
01:05:48,630 --> 01:05:51,130
GPU가 병렬로 작업하는 겁니다.

1682
01:05:51,130 --> 01:05:54,670
이것은 배치 크기가 1/16인 것과

1683
01:05:54,670 --> 01:06:01,050
같아서, 각 배치, 각 GPU가 하나의 요소보다 적게 작업하는 셈입니다.

1684
01:06:01,050 --> 01:06:02,440
이것이 바로 컨텍스트 병렬화입니다.

1685
01:06:02,440 --> 01:06:03,460
파이프라인 병렬 처리입니다.

1686
01:06:03,460 --> 01:06:05,670
레이어 차원을 기준으로 나눌 겁니다.

1687
01:06:05,670 --> 01:06:08,160
직관적으로 생각해보면, 여러 레이어로 구성된

1688
01:06:08,160 --> 01:06:09,250
네트워크가 있죠.

1689
01:06:09,250 --> 01:06:12,370
그리고 레이어를 GPU들에 나누는 겁니다.

1690
01:06:12,370 --> 01:06:14,820
이건 사실 매우 직관적인 방법입니다.

1691
01:06:14,820 --> 01:06:17,380
문제는 순차적 의존성이 있다는 점입니다.

1692
01:06:17,380 --> 01:06:20,310
각 GPU는 이전 GPU의 활성화 값을

1693
01:06:20,310 --> 01:06:22,380
받아야만 순전파를 계속할

1694
01:06:22,380 --> 01:06:23,447
수 있습니다.

1695
01:06:23,447 --> 01:06:25,530
역전파 시에는 상위 레이어에서 온

1696
01:06:25,530 --> 01:06:27,000
그래디언트가 필요해서

1697
01:06:27,000 --> 01:06:28,420
역전파를 계산할 수 있죠.

1698
01:06:28,420 --> 01:06:31,830
이런 식으로 도표를 그릴 수 있는데, 세로축은 GPU

1699
01:06:31,830 --> 01:06:33,280
1부터 4까지입니다.

1700
01:06:33,280 --> 01:06:36,370
가로축은 시간의 흐름을 나타냅니다.

1701
01:06:36,370 --> 01:06:39,810
GPU 1이 순전파를 실행하고, 활성화 값을 GPU 2에

1702
01:06:39,810 --> 01:06:43,080
넘기고, GPU 2는 GPU 3에, GPU 3은 GPU

1703
01:06:43,080 --> 01:06:45,090
4에 넘기는 걸 볼 수 있습니다.

1704
01:06:45,090 --> 01:06:46,260
GPU 4는 운이 좋습니다.

1705
01:06:46,260 --> 01:06:48,160
순전파와 역전파를 한 번에 처리하고,

1706
01:06:48,160 --> 01:06:51,970
그래디언트를 GPU 3, GPU 2, GPU 1로 되돌려 보낼 수 있죠.

1707
01:06:51,970 --> 01:06:55,630
이 그래프를 보면, GPU들이 대부분 놀고 있어서 정말

1708
01:06:55,630 --> 01:06:57,940
안 좋은 상황임을 알 수 있습니다.

1709
01:06:57,940 --> 01:07:02,320
실제로 N개의 GPU가 있으면, 그 중 1/N 시간만 유용한

1710
01:07:02,320 --> 01:07:04,370
작업을 수행하는 셈입니다.

1711
01:07:04,370 --> 01:07:07,250
즉, 8-way 파이프라인 병렬 처리를

1712
01:07:07,250 --> 01:07:09,400
한다면, 최대 MFU가 12%

1713
01:07:09,400 --> 01:07:11,420
정도로 매우 낮다는 뜻입니다.

1714
01:07:11,420 --> 01:07:13,120
그건 정말 안 좋은 상황입니다.

1715
01:07:13,120 --> 01:07:14,747
그리고 참고로 귀여운 이름이 있습니다.

1716
01:07:14,747 --> 01:07:16,330
이것을 가끔 버블이라고

1717
01:07:16,330 --> 01:07:19,490
부르는데, GPU가 작업을 기다리는 그 구간을 의미합니다.

1718
01:07:19,490 --> 01:07:22,040
즉, 통신을 기다리면서 대기하는 상태인 거죠.

1719
01:07:22,040 --> 01:07:23,650
그래서 파이프라인 병렬 처리의 핵심은

1720
01:07:23,650 --> 01:07:24,890
이 버블을 줄이는 것입니다.

1721
01:07:24,890 --> 01:07:26,270
버블이 적을수록 좋습니다.

1722
01:07:26,270 --> 01:07:29,290
그 방법은 여러 마이크로배치를 동시에

1723
01:07:29,290 --> 01:07:30,530
실행하는 것입니다.

1724
01:07:30,530 --> 01:07:33,160
그래서 이제 한 번에 하나의 배치를

1725
01:07:33,160 --> 01:07:35,390
모든 GPU에서 순방향과 역방향으로

1726
01:07:35,390 --> 01:07:37,660
처리하는 대신, 여러 배치를

1727
01:07:37,660 --> 01:07:39,700
동시에 처리하며 이 배치들을

1728
01:07:39,700 --> 01:07:42,537
GPU 간에 병렬로 전달하는 겁니다.

1729
01:07:42,537 --> 01:07:44,620
이걸 설계할 때 시도해볼 수 있는 다양한

1730
01:07:44,620 --> 01:07:45,980
흥미로운 패턴들이 많습니다.

1731
01:07:45,980 --> 01:07:47,590
하지만 여기서는 비교적 단순한 예를

1732
01:07:47,590 --> 01:07:50,240
들어보겠습니다. 네 방향 파이프라인 병렬 처리입니다.

1733
01:07:50,240 --> 01:07:52,790
즉, 네 개의 GPU가 모두 병렬로 작업하는 거죠.

1734
01:07:52,790 --> 01:07:55,360
그리고 네 개의 데이터 배치가

1735
01:07:55,360 --> 01:07:57,250
동시에 활성화되어 있습니다.

1736
01:07:57,250 --> 01:07:58,970
이 배치들은 색깔로 구분되어 있습니다.

1737
01:07:58,970 --> 01:08:02,610
그래서 GPU 1은 파란 배치에 대해 순방향 연산을 하고, 그

1738
01:08:02,610 --> 01:08:03,990
다음 노란 배치, 초록

1739
01:08:03,990 --> 01:08:06,615
배치, 빨간 배치 순으로 순방향 연산을 합니다.

1740
01:08:06,615 --> 01:08:09,320
그리고 GPU 1이 노란 배치 순방향

1741
01:08:09,320 --> 01:08:12,170
연산을 하는 동안, 파란 배치의 활성화 값은 GPU

1742
01:08:12,170 --> 01:08:13,260
2로 전달됩니다.

1743
01:08:13,260 --> 01:08:15,810
그럼 GPU 2는 이제 파란 배치에 대해 순방향 연산을 할 수 있습니다.

1744
01:08:15,810 --> 01:08:19,140
이런 식으로 모든 작업이 연쇄적으로 병렬 처리될 수 있습니다.

1745
01:08:19,140 --> 01:08:20,540
그리고 역전파 과정에서도

1746
01:08:20,540 --> 01:08:21,600
같은 패턴이 반복됩니다.

1747
01:08:21,600 --> 01:08:25,189
우리는 이 서로 다른 마이크로배치를 여러 GPU에

1748
01:08:25,189 --> 01:08:28,160
파이프라인으로 처리하면서 교차시킬 수 있습니다.

1749
01:08:28,160 --> 01:08:32,240
그리고 네 개의 마이크로배치로 구성된 4방향 파이프라인

1750
01:08:32,240 --> 01:08:35,660
병렬 처리의 경우, 이 그래프에서 흰색이 아닌

1751
01:08:35,660 --> 01:08:38,580
부분의 비율이 최대 이론적 MFU입니다.

1752
01:08:38,580 --> 01:08:41,689
그리고 이제 57%로 증가했는데, 꽤 좋은 수치입니다.

1753
01:08:41,689 --> 01:08:43,920
그래서 파이프라인 병렬 처리에서는

1754
01:08:43,920 --> 01:08:46,260
이론적으로 마이크로배치 수가 많아질수록

1755
01:08:46,260 --> 01:08:48,260
병렬로 많은 작업을 할 수

1756
01:08:48,260 --> 01:08:50,340
있기 때문에 MFU가 좋아집니다.

1757
01:08:50,340 --> 01:08:52,288
하지만 마이크로배치가 많아질수록 모든

1758
01:08:52,288 --> 01:08:54,330
활성화 값을 메모리에 저장해야 합니다.

1759
01:08:54,330 --> 01:08:56,310
그래서 이제 활성화 체크포인팅을 해야 하고,

1760
01:08:56,310 --> 01:08:57,895
그때 '아, 이런' 하는 생각이 듭니다.

1761
01:08:57,895 --> 01:08:59,020
이걸 어떻게 조절해야 할까요?

1762
01:08:59,020 --> 01:09:01,095
파이프라인 병렬 처리를 더 늘려야 할까요?

1763
01:09:01,095 --> 01:09:02,470
마이크로배치를 줄여야 할까요?

1764
01:09:02,470 --> 01:09:04,762
더 적극적인 활성화 체크포인팅을 해야 할까요?

1765
01:09:04,762 --> 01:09:07,498
그리고 그 위에 데이터 병렬 처리도 추가해야 할까요?

1766
01:09:07,498 --> 01:09:08,040
잘 모르겠습니다.

1767
01:09:08,040 --> 01:09:09,082
어떻게 하시겠습니까?

1768
01:09:09,082 --> 01:09:09,810
MFU를 최대화하는 겁니다.

1769
01:09:09,810 --> 01:09:11,850
훈련 파이프라인의 MFU를

1770
01:09:11,850 --> 01:09:16,109
최대화하기 위해 모든 조절 가능한 요소를 조정하려고 할 겁니다.

1771
01:09:16,109 --> 01:09:18,460
마지막으로 텐서 병렬화가 있습니다.

1772
01:09:18,460 --> 01:09:22,870
이 경우에는 모델 차원에서 분할을 하게 됩니다.

1773
01:09:22,870 --> 01:09:24,390
기본적으로 우리가 할 일은

1774
01:09:24,390 --> 01:09:26,830
모델에 있는 많은 가중치 행렬들을 다루는 겁니다.

1775
01:09:26,830 --> 01:09:30,060
모든 가중치 행렬은 XW =

1776
01:09:30,060 --> 01:09:33,510
Y를 계산하는 것과 같습니다. 이것이 바로 트랜스포머 내부에서 반복적으로
수행하는

1777
01:09:33,510 --> 01:09:34,569
작업입니다.

1778
01:09:34,569 --> 01:09:39,240
이제 아이디어는 각 가중치 행렬을 GPU들에 걸쳐 분할하는 것입니다.

1779
01:09:39,240 --> 01:09:40,750
이것은 FSDP와 다릅니다.

1780
01:09:40,750 --> 01:09:42,750
왜냐하면 단일 가중치 행렬을 GPU들에

1781
01:09:42,750 --> 01:09:44,229
걸쳐 분할하기 때문입니다.

1782
01:09:44,229 --> 01:09:46,390
그리고 이제는 통신이 없습니다.

1783
01:09:46,390 --> 01:09:48,340
이제 블록 행렬 곱셈을 수행합니다.

1784
01:09:48,340 --> 01:09:52,229
각 GPU는 전체 입력에 대해 그 행렬 곱셈의 일부 조각을

1785
01:09:52,229 --> 01:09:53,200
계산합니다.

1786
01:09:53,200 --> 01:09:55,050
이 경우, 가중치 행렬을

1787
01:09:55,050 --> 01:09:57,450
W1, W2, W3, W4로 나눕니다.

1788
01:09:57,450 --> 01:10:00,120
그리고 각 GPU는 출력의 일부 조각을 계산하기

1789
01:10:00,120 --> 01:10:02,870
위해 그 행렬 곱셈의 일부 조각을 계산합니다.

1790
01:10:02,870 --> 01:10:06,180
문제는, 이렇게 순전파를 한 후에 다음

1791
01:10:06,180 --> 01:10:09,960
순전파를 위해 모든 GPU에서 활성화를 모아야

1792
01:10:09,960 --> 01:10:11,660
한다는 점입니다.

1793
01:10:11,660 --> 01:10:13,220
약간의 트릭이 있는데,

1794
01:10:13,220 --> 01:10:15,750
만약 두 개의 레이어가

1795
01:10:15,750 --> 01:10:20,510
연속되어 있다면, 두 레이어 사이에서 모으지 않아도

1796
01:10:20,510 --> 01:10:21,180
됩니다.

1797
01:10:21,180 --> 01:10:23,780
두 레이어가 있다면 조용한 곳에 앉아서 이 과정을

1798
01:10:23,780 --> 01:10:24,810
직접 따라 해보세요.

1799
01:10:24,810 --> 01:10:26,185
첫 번째 가중치 행렬은 열

1800
01:10:26,185 --> 01:10:27,450
모양의 조각으로 나누고,

1801
01:10:27,450 --> 01:10:30,120
두 번째 가중치 행렬은 행 모양의 조각으로 나눕니다.

1802
01:10:30,120 --> 01:10:32,390
이렇게 하면 블록 행렬 곱셈의

1803
01:10:32,390 --> 01:10:34,850
마법과 신비 덕분에 모든 것이

1804
01:10:34,850 --> 01:10:36,295
마법처럼 잘 작동합니다.

1805
01:10:36,295 --> 01:10:37,670
최종 출력은

1806
01:10:37,670 --> 01:10:41,840
Y와 U의 블록 행렬 곱셈의 내적 구조로 계산할

1807
01:10:41,840 --> 01:10:44,790
수 있음을 알 수 있습니다.

1808
01:10:44,790 --> 01:10:49,490
그래서 기본적으로 여러 GPU에 걸쳐 분할된 두 개의 행렬 곱셈

1809
01:10:49,490 --> 01:10:51,627
레이어를 가질 수 있습니다.

1810
01:10:51,627 --> 01:10:53,210
그리고 두 레이어마다 한

1811
01:10:53,210 --> 01:10:54,713
번씩만 통신하면 됩니다.

1812
01:10:54,713 --> 01:10:56,130
이것은 실제로 잘 맞아떨어지는데,

1813
01:10:56,130 --> 01:11:00,100
트랜스포머의 FFN에는 두 개의 레이어 MLP가 있기 때문입니다.

1814
01:11:00,100 --> 01:11:01,860
그래서 이것은 트랜스포머가 항상

1815
01:11:01,860 --> 01:11:03,900
갖고 있는 두 레이어 MLP에 아주

1816
01:11:03,900 --> 01:11:05,282
잘 맞는 멋진 트릭입니다.

1817
01:11:05,282 --> 01:11:06,990
그래서 큰 트랜스포머에서는

1818
01:11:06,990 --> 01:11:11,070
텐서 병렬화, 특히 MLP에서 이 두

1819
01:11:11,070 --> 01:11:16,050
레이어 텐서 병렬화 트릭을 사용하는 것이 꽤 일반적입니다.

1820
01:11:16,050 --> 01:11:19,020
이것이 GPU 간 계산 분할을 위한

1821
01:11:19,020 --> 01:11:20,890
모든 메커니즘입니다.

1822
01:11:20,890 --> 01:11:22,240
어떤 것이 가장 좋을까요?

1823
01:11:22,240 --> 01:11:23,890
실제 답은 모두 다입니다.

1824
01:11:23,890 --> 01:11:26,500
실제로는 ND 병렬화를 사용합니다.

1825
01:11:26,500 --> 01:11:28,950
우리는 이미 HSRP로 2차원 병렬화 예시를

1826
01:11:28,950 --> 01:11:29,860
보았습니다.

1827
01:11:29,860 --> 01:11:32,160
현재 최첨단은 4차원

1828
01:11:32,160 --> 01:11:33,940
병렬화입니다.

1829
01:11:33,940 --> 01:11:37,080
Lama로 돌아가 보면, 그들은 최대 훈련에서

1830
01:11:37,080 --> 01:11:40,240
16,000개의 GPU를 사용하고 있습니다.

1831
01:11:40,240 --> 01:11:43,110
8방향 텐서 병렬화, 16방향 컨텍스트 병렬화,

1832
01:11:43,110 --> 01:11:46,650
16방향 파이프라인 병렬화, 그리고 8방향 데이터

1833
01:11:46,650 --> 01:11:48,750
병렬화를 동시에 사용하고 있습니다.

1834
01:11:48,750 --> 01:11:53,090
그리고 주의 깊게 살펴보면, 병렬 처리의 다양한 메커니즘은 서로 다른

1835
01:11:53,090 --> 01:11:54,990
통신 요구사항을 가지고 있습니다.

1836
01:11:54,990 --> 01:11:58,850
그래서 클러스터 내에서 병렬 처리의

1837
01:11:58,850 --> 01:12:00,630
다양한 축을 어떻게

1838
01:12:00,630 --> 01:12:02,780
배치하느냐에 따라

1839
01:12:02,780 --> 01:12:07,910
클러스터 전체의 통신 속도 차이를 활용할 수 있습니다.

1840
01:12:07,910 --> 01:12:09,380
이것이 대규모 분산 학습에

1841
01:12:09,380 --> 01:12:10,890
대한 간단한 개요입니다.

1842
01:12:10,890 --> 01:12:13,730
오늘의 핵심은 개별 GPU가 기본적으로

1843
01:12:13,730 --> 01:12:16,920
일반화 가능한 병렬 컴퓨팅 머신이라는 점입니다.

1844
01:12:16,920 --> 01:12:20,485
GPU 클러스터는 수만, 어쩌면 수십만 개의 개별 GPU로

1845
01:12:20,485 --> 01:12:22,610
구성된 거대한 대규모 병렬 머신이며,

1846
01:12:22,610 --> 01:12:25,790
우리는 이를 하나의 큰 단위로 프로그래밍하려고 합니다.

1847
01:12:25,790 --> 01:12:28,040
그리고 대규모 클러스터에서 계산을

1848
01:12:28,040 --> 01:12:30,960
병렬화하는 여러 메커니즘과 메모리 절약을

1849
01:12:30,960 --> 01:12:34,230
위한 활성화 체크포인팅 트릭, 그리고 파이프라인

1850
01:12:34,230 --> 01:12:36,740
설계 시 항상 최적화하려는 핵심

1851
01:12:36,740 --> 01:12:39,115
지표인 모델 FLOPS 활용률에 대해

1852
01:12:39,115 --> 01:12:40,260
이야기했습니다.

1853
01:12:40,260 --> 01:12:41,780
다음에 수만 개의 GPU로

1854
01:12:41,780 --> 01:12:44,450
학습할 때 이 점을 꼭 기억하시기 바랍니다.

1855
01:12:44,450 --> 01:12:48,040
그리고 알려주시면 수만 개의 GPU를 빌릴 수 있게 해주세요.
