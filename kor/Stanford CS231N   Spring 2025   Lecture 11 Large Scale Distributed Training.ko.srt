1
00:00:05,240 --> 00:00:07,737
CS231n 강의 11에 다시 오신 것을 환영합니다.

2
00:00:07,737 --> 00:00:10,070
오늘은 대규모 분산 훈련에 대해 이야기할

3
00:00:10,070 --> 00:00:10,770
것입니다.

4
00:00:10,770 --> 00:00:12,020
이 주제는 매우 흥미로운데,

5
00:00:12,020 --> 00:00:14,228
이는 기본적으로 현재 모든 신경망이 실제로

6
00:00:14,228 --> 00:00:15,510
훈련되는 방식이기 때문입니다.

7
00:00:15,510 --> 00:00:18,420
스타트업, 산업, 심지어 학계에서 대규모

8
00:00:18,420 --> 00:00:20,630
모델을 보면, 현재 딥러닝에서는

9
00:00:20,630 --> 00:00:22,850
정말 대규모가 새로운 표준입니다.

10
00:00:22,850 --> 00:00:24,350
사실, 이 수업을

11
00:00:24,350 --> 00:00:26,190
시작한 지난 10년 동안

12
00:00:26,190 --> 00:00:27,800
많이 변화한 부분입니다.

13
00:00:27,800 --> 00:00:30,619
10년 전에는 모든 모델을 기본적으로 하나의

14
00:00:30,619 --> 00:00:32,119
GPU, 하나의 장치에서

15
00:00:32,119 --> 00:00:34,550
훈련하는 것이 일반적이었고, 여러

16
00:00:34,550 --> 00:00:36,960
장치에서 훈련하는 것은 드물었습니다.

17
00:00:36,960 --> 00:00:38,690
하지만 요즘은 모델을 수십,

18
00:00:38,690 --> 00:00:41,960
수백, 수천, 심지어 수만 개의 장치에서 동시에

19
00:00:41,960 --> 00:00:43,820
훈련하는 것이 새로운 표준입니다.

20
00:00:43,820 --> 00:00:45,320
그래서 이를 위해

21
00:00:45,320 --> 00:00:49,130
새로운 알고리즘과 사고 방식을 개발해야 합니다.

22
00:00:49,130 --> 00:00:51,900
오늘 강의의 예로 Llama3-405B에

23
00:00:51,900 --> 00:00:55,040
대해 많이 이야기할 것입니다. 이

24
00:00:55,040 --> 00:00:57,560
모델이 가장 좋은 모델이나 가장

25
00:00:57,560 --> 00:00:59,920
흥미로운 모델이 아니라, 훈련

26
00:00:59,920 --> 00:01:02,500
방법, 모델 아키텍처 등 많은

27
00:01:02,500 --> 00:01:04,700
구현 세부사항을 공유하는 최신

28
00:01:04,700 --> 00:01:06,680
모델에 가깝기 때문입니다.

29
00:01:06,680 --> 00:01:08,680
최근 몇 년 동안 Google,

30
00:01:08,680 --> 00:01:10,805
OpenAI, Anthropic 등에서 훈련된

31
00:01:10,805 --> 00:01:13,162
정말 놀랍고 강력한 모델들이 많이 있습니다.

32
00:01:13,162 --> 00:01:15,370
하지만 기본적으로 그들은 더 이상 모델에 대한

33
00:01:15,370 --> 00:01:16,880
세부사항을 공유하지 않습니다.

34
00:01:16,880 --> 00:01:19,750
2023년 GPT4 논문에서 산업의

35
00:01:19,750 --> 00:01:23,960
큰 변화를 나타내는 매우 유명한 인용구가 있습니다.

36
00:01:23,960 --> 00:01:26,553
GPT4 모델을 출시할 때, 그들은 "경쟁

37
00:01:26,553 --> 00:01:28,720
환경과 GPT4와 같은 대규모 모델의

38
00:01:28,720 --> 00:01:32,150
안전성 문제를 고려할 때, 이 보고서, 즉 그들이 작성한

39
00:01:32,150 --> 00:01:34,370
GPT4에 대한 논문은 모델 크기,

40
00:01:34,370 --> 00:01:36,980
하드웨어, 훈련 컴퓨트, 데이터 세트 구성,

41
00:01:36,980 --> 00:01:39,470
훈련 방법 또는 유사한 것에 대한 추가

42
00:01:39,470 --> 00:01:42,080
세부정보를 포함하지 않습니다."라고 말했습니다.

43
00:01:42,080 --> 00:01:43,400
이게 기본적으로 뉴스입니다.

44
00:01:43,400 --> 00:01:45,910
지난 3년 동안 GPT4 이후

45
00:01:45,910 --> 00:01:49,040
대규모 모델의 최신 상태였습니다.

46
00:01:49,040 --> 00:01:50,902
그들은 아무것도 알려주지 않습니다.

47
00:01:50,902 --> 00:01:52,610
모델에 대해 아무것도 말하지 않을 것입니다.

48
00:01:52,610 --> 00:01:54,902
그들이 트랜스포머라고 말해주면 행운입니다.

49
00:01:54,902 --> 00:01:56,260
그 정도는 말해줄지도 모릅니다.

50
00:01:56,260 --> 00:01:58,420
그래서 Llama3는 최고의 모델이

51
00:01:58,420 --> 00:02:00,170
아니라 가장 개방적인 모델 중

52
00:02:00,170 --> 00:02:01,950
하나라는 점에서 주목할 만합니다.

53
00:02:01,950 --> 00:02:04,670
이 모델은 Meta에 의해 훈련된 대형

54
00:02:04,670 --> 00:02:08,340
언어 모델로, 2024년 4월에 오픈 소스로 출시되었습니다.

55
00:02:08,340 --> 00:02:10,370
OpenAI와 달리, 이 논문은

56
00:02:10,370 --> 00:02:13,440
모델 훈련에 대한 많은 세부사항을 공유합니다.

57
00:02:13,440 --> 00:02:15,080
데이터 세트에 대한 정보는 많지

58
00:02:15,080 --> 00:02:18,000
않지만, 훈련에 사용된 시스템 인프라에 대한 정보는 많습니다.

59
00:02:18,000 --> 00:02:19,640
이것은 우리가-- 그리고

60
00:02:19,640 --> 00:02:23,030
이것은 현재 대규모 LLM이 실제로 어떻게 훈련되는지를

61
00:02:23,030 --> 00:02:24,800
엿볼 수 있게 해줍니다.

62
00:02:24,800 --> 00:02:27,110
그런데, 지난달 2025년 4월에

63
00:02:27,110 --> 00:02:30,850
Meta에서 새로운 Llama4 모델이 출시되었습니다.

64
00:02:30,850 --> 00:02:33,350
이미 오픈 소스에는 약간 더 나은 모델들이 있지만,

65
00:02:33,350 --> 00:02:35,872
Llama4에 대한 논문은 아직 없습니다.

66
00:02:35,872 --> 00:02:37,580
그래서 몇 달 후에 나올 때 그

67
00:02:37,580 --> 00:02:38,750
논문을 읽고 새로운

68
00:02:38,750 --> 00:02:41,420
Llama 훈련 세대에서 무엇을 배울 수 있을지 기대하고

69
00:02:41,420 --> 00:02:42,000
있습니다.

70
00:02:42,000 --> 00:02:44,415
하지만 오늘 강의의 예로

71
00:02:44,415 --> 00:02:46,040
Llama3-405B

72
00:02:46,040 --> 00:02:50,252
모델의 많은 예를 지적할 것입니다.

73
00:02:50,252 --> 00:02:51,710
좋습니다, 오늘 이야기하고

74
00:02:51,710 --> 00:02:53,270
싶은 두 가지가 있습니다.

75
00:02:53,270 --> 00:02:56,110
하나는 GPU 하드웨어에 대한 것이고, 다른 하나는

76
00:02:56,110 --> 00:02:57,860
많은 GPU에서 훈련하는 방법입니다.

77
00:02:57,860 --> 00:02:59,920
그래서 이들이 실행되는 하드웨어가

78
00:02:59,920 --> 00:03:02,260
실제로 무엇인지와 많은 GPU에서

79
00:03:02,260 --> 00:03:04,510
훈련하기 위해 필요한 알고리즘에 대한

80
00:03:04,510 --> 00:03:06,020
감각을 드리고 싶습니다.

81
00:03:06,020 --> 00:03:08,990
먼저 GPU 하드웨어에 대해 조금 이야기하겠습니다.

82
00:03:08,990 --> 00:03:11,230
GPU는 잘 모르는 분들을 위해

83
00:03:11,230 --> 00:03:12,860
그래픽 처리 장치입니다.

84
00:03:12,860 --> 00:03:14,650
이것들은 원래 컴퓨터 그래픽을

85
00:03:14,650 --> 00:03:17,060
위해 개발된 전문 코프로세서였습니다.

86
00:03:17,060 --> 00:03:19,400
그리고 이들은 매우 유용한 일반화 가능한

87
00:03:19,400 --> 00:03:21,335
병렬 프로세서로 판명되었습니다.

88
00:03:21,335 --> 00:03:22,960
이 강의를 이 방에서

89
00:03:22,960 --> 00:03:24,850
하는 것은 매우 적합합니다.

90
00:03:24,850 --> 00:03:26,690
이곳은 황 강당입니다.

91
00:03:26,690 --> 00:03:28,660
Jen-Hsun Huang은

92
00:03:28,660 --> 00:03:32,050
NVIDIA의 CEO이자 창립자로, 현재 가장

93
00:03:32,050 --> 00:03:35,090
큰 GPU 제조 회사입니다. 게임과 ML

94
00:03:35,090 --> 00:03:37,790
모두를 위해 GPU를 생산해왔습니다.

95
00:03:37,790 --> 00:03:39,952
이것들은 기본적으로 그래픽을 위해 시작되었습니다.

96
00:03:39,952 --> 00:03:41,410
컴퓨터 그래픽을 할 때

97
00:03:41,410 --> 00:03:42,570
화면에 많은 픽셀을

98
00:03:42,570 --> 00:03:44,528
생성해야 하기 때문입니다.

99
00:03:44,528 --> 00:03:46,540
그 픽셀을 생성하기 위해 많은 작은

100
00:03:46,540 --> 00:03:48,770
기본 기하학 조각을 처리해야 합니다.

101
00:03:48,770 --> 00:03:51,670
그래픽 작업을 할 때 많은 계산을 병렬로 수행하는

102
00:03:51,670 --> 00:03:53,380
것이 매우 자연스럽습니다.

103
00:03:53,380 --> 00:03:57,200
사람들은 컴퓨터 그래픽을 위해 만들어진 이

104
00:03:57,200 --> 00:03:59,600
하드웨어가 훨씬 더 일반적인

105
00:03:59,600 --> 00:04:01,100
병렬 계산에도

106
00:04:01,100 --> 00:04:04,970
사용될 수 있다는 것을 빠르게 알아냈습니다.

107
00:04:04,970 --> 00:04:08,068
2000년대 초반에 연구자들은 이 그래픽 카드를

108
00:04:08,068 --> 00:04:10,610
일반화 가능한 병렬 프로그래밍을

109
00:04:10,610 --> 00:04:13,910
수행하도록 변형할 수 있는 방법을 알아냈습니다.

110
00:04:13,910 --> 00:04:16,040
그리고 2000년대 후반과

111
00:04:16,040 --> 00:04:18,950
2010년대 초반으로 넘어가면서 NVIDIA는

112
00:04:18,950 --> 00:04:21,082
이를 발전시키고 마케팅하며

113
00:04:21,082 --> 00:04:23,540
일반화된 병렬 프로세서로 만들기 위해

114
00:04:23,540 --> 00:04:24,375
노력했습니다.

115
00:04:24,375 --> 00:04:26,750
당시 그들이 정확히 무엇에 사용될지 알지는

116
00:04:26,750 --> 00:04:27,930
못했던 것 같습니다.

117
00:04:27,930 --> 00:04:30,350
그들은 병렬 처리가 중요할 것이라는

118
00:04:30,350 --> 00:04:32,960
일반적인 생각을 가지고 있었고, 2010년대

119
00:04:32,960 --> 00:04:36,302
초반에 딥러닝이 시작될 때 이를 잘 활용했습니다.

120
00:04:36,302 --> 00:04:38,510
NVIDIA의 공로 덕분에 그들은

121
00:04:38,510 --> 00:04:41,650
이 연구 분야의 잠재력을 매우 일찍,

122
00:04:41,650 --> 00:04:44,240
2010년대 초반에 인식하고 그들의 하드웨어가

123
00:04:44,240 --> 00:04:46,820
딥러닝 훈련에 정말 유용하도록

124
00:04:46,820 --> 00:04:49,230
많은 자원을 투입하기 시작했습니다.

125
00:04:49,230 --> 00:04:52,190
그리고 이는 10년 넘게 사람들이

126
00:04:52,190 --> 00:04:54,840
대규모 딥러닝 모델을 훈련하는 주요

127
00:04:54,840 --> 00:04:56,452
방법이 되어왔습니다.

128
00:04:56,452 --> 00:04:58,660
조금씩 변화하고 있지만,

129
00:04:58,660 --> 00:05:03,690
그들의 칩은 사람들이 주로 사용하는 것입니다.

130
00:05:03,690 --> 00:05:06,030
그래서 저는 항상 이 것들을 들여다보고 그 안에 무엇이

131
00:05:06,030 --> 00:05:07,240
있는지 보는 것을 좋아합니다.

132
00:05:07,240 --> 00:05:09,370
이것은 현재 딥러닝

133
00:05:09,370 --> 00:05:12,930
훈련의 주축인 NVIDIA H100의

134
00:05:12,930 --> 00:05:14,035
사진입니다.

135
00:05:14,035 --> 00:05:15,910
다음 세대가 방금 출시되었지만,

136
00:05:15,910 --> 00:05:17,368
아직 접근할 수 없습니다.

137
00:05:17,368 --> 00:05:19,180
저는 아직 그것으로 아무것도 훈련하지 않았습니다.

138
00:05:19,180 --> 00:05:21,870
그래서 이것이 현재의 최첨단입니다.

139
00:05:21,870 --> 00:05:25,702
이 NVIDIA GPU, 즉 가운데 있는 H100 GPU 내부에는 이러한

140
00:05:25,702 --> 00:05:26,910
컴퓨트 코어가 있습니다.

141
00:05:26,910 --> 00:05:29,400
그리고 그 주위에는 80기가바이트의 HBM 메모리,

142
00:05:29,400 --> 00:05:30,907
고대역폭 메모리가 있습니다.

143
00:05:30,907 --> 00:05:32,490
메모리가 컴퓨트 코어와 분리되어 있는

144
00:05:32,490 --> 00:05:33,460
것을 볼 수 있습니다.

145
00:05:33,460 --> 00:05:34,360
그들은 이동해야 합니다.

146
00:05:34,360 --> 00:05:36,790
그들은 GPU 메모리와 코어 간에 데이터를

147
00:05:36,790 --> 00:05:40,030
주고받기 위해 이 버스를 통해 서로 통신해야 합니다.

148
00:05:40,030 --> 00:05:43,180
그리고 이는 초당 약 3테라바이트의 속도로 데이터를 이동할 수

149
00:05:43,180 --> 00:05:45,330
있습니다. 이는 많은 비트가 이동하는 것입니다.

150
00:05:45,330 --> 00:05:48,640
이제 GPU 코어 내부를 더 깊이 들여다보면, 그

151
00:05:48,640 --> 00:05:51,240
컴퓨트 코어 부분의 중앙에 약

152
00:05:51,240 --> 00:05:52,920
50메가바이트의 L2 캐시라는

153
00:05:52,920 --> 00:05:55,190
훨씬 더 작은 메모리가 있습니다.

154
00:05:55,190 --> 00:05:58,890
이는 80기가바이트의 HBM 메모리보다 훨씬 작습니다.

155
00:05:58,890 --> 00:06:02,032
하지만 이는 실제 컴퓨팅 요소와 매우 가까이 있습니다.

156
00:06:02,032 --> 00:06:03,740
그래서 컴퓨트 코어에서 훨씬 더

157
00:06:03,740 --> 00:06:05,300
빠르게 접근할 수 있습니다.

158
00:06:05,300 --> 00:06:06,950
그리고 이 장치의 진정한

159
00:06:06,950 --> 00:06:10,940
핵심은 132개의 스트리밍 멀티프로세서(SM)입니다.

160
00:06:10,940 --> 00:06:13,760
이들은 독립적인 병렬 코어입니다.

161
00:06:13,760 --> 00:06:15,290
일반 CPU 코어보다 몇 가지

162
00:06:15,290 --> 00:06:17,498
면에서 더 강력합니다. 왜냐하면 더 많은

163
00:06:17,498 --> 00:06:19,290
병렬 처리를 할 수 있기 때문입니다.

164
00:06:19,290 --> 00:06:21,410
하지만 일반 CPU 코어에 비해 여러

165
00:06:21,410 --> 00:06:23,360
면에서 훨씬 약합니다. 왜냐하면 클럭

166
00:06:23,360 --> 00:06:24,585
속도가 느리기 때문입니다.

167
00:06:24,585 --> 00:06:26,460
명령어 예측이나 분기 예측을

168
00:06:26,460 --> 00:06:27,600
많이 할 수 없습니다.

169
00:06:27,600 --> 00:06:29,540
그래서 이러한 GPU 코어와

170
00:06:29,540 --> 00:06:32,450
CPU 코어 간의 정확한 비교는 정말

171
00:06:32,450 --> 00:06:32,970
어렵습니다.

172
00:06:32,970 --> 00:06:35,600
하지만 저는 이러한 스트리밍 멀티프로세서를

173
00:06:35,600 --> 00:06:38,480
대략 CPU 코어와 유사하다고 생각합니다.

174
00:06:38,480 --> 00:06:41,030
또한 누군가는 집에 가서 이 화면의 작은

175
00:06:41,030 --> 00:06:43,020
박스들을 모두 세어볼 것이고, 실제로

176
00:06:43,020 --> 00:06:45,810
144개가 있다는 것을 알게 될 것입니다. 제가

177
00:06:45,810 --> 00:06:47,460
132개라고 말했을 때 말이죠.

178
00:06:47,460 --> 00:06:48,270
왜 그럴까요?

179
00:06:48,270 --> 00:06:51,070
모든 GPU 하드웨어는 빈닝이라는 과정을 사용합니다. 이

180
00:06:51,070 --> 00:06:53,640
과정에서 이들을 만들 때, 많은 트랜지스터와 작은

181
00:06:53,640 --> 00:06:54,940
컴퓨팅 요소가 있습니다.

182
00:06:54,940 --> 00:06:57,107
얼마나 많은 돈을 쏟아 부어도

183
00:06:57,107 --> 00:06:58,600
완벽하게 나오지 않습니다.

184
00:06:58,600 --> 00:07:00,683
항상 약간 망가진 것들이 생깁니다.

185
00:07:00,683 --> 00:07:03,340
그래서 그들은 제품 개발에서 이를 계획합니다.

186
00:07:03,340 --> 00:07:05,530
그리고 그들은 칩을 만들려고 합니다.

187
00:07:05,530 --> 00:07:08,250
이론적으로 전체 칩은 144개를 가지고 있지만, 완벽한

188
00:07:08,250 --> 00:07:08,890
칩은 없습니다.

189
00:07:08,890 --> 00:07:10,973
하지만 최소한 132개가 작동하는

190
00:07:10,973 --> 00:07:13,360
합리적인 수를 얻을 것이라는 것을 알고 있습니다.

191
00:07:13,360 --> 00:07:15,400
그래서 그들은 빈닝 과정을 사용합니다.

192
00:07:15,400 --> 00:07:17,810
그럼 그들은 실제로 생산하려고

193
00:07:17,810 --> 00:07:19,560
했던 칩의

194
00:07:19,560 --> 00:07:24,825
훨씬 더 큰 비율을 판매할 수 있습니다. 단지 132개가

195
00:07:24,825 --> 00:07:27,840
켜질 것이라고 약속함으로써요.

196
00:07:27,840 --> 00:07:30,270
그런 다음 우리는 이러한 스트리밍 멀티프로세서 중 하나의 내부를 더

197
00:07:30,270 --> 00:07:31,180
깊이 파고들 수 있습니다.

198
00:07:31,180 --> 00:07:33,600
그리고 우리는 이러한 GPU 내부에서 무슨 일이 일어나고

199
00:07:33,600 --> 00:07:35,260
있는지 더 많이 볼 수 있습니다.

200
00:07:35,260 --> 00:07:38,070
이것은 H100 내부의 132개 활성 스트리밍

201
00:07:38,070 --> 00:07:40,120
멀티프로세서 중 하나입니다.

202
00:07:40,120 --> 00:07:43,050
여기에서 살펴볼 몇 가지 흥미로운 요소가 있습니다.

203
00:07:43,050 --> 00:07:46,530
먼저, 우리는 256킬로바이트의 L1 캐시와 레지스터

204
00:07:46,530 --> 00:07:47,810
파일이 있음을 봅니다.

205
00:07:47,810 --> 00:07:51,030
그래서 이것은 GPU의 메모리 계층 구조의 경향을 계속 이어갑니다.

206
00:07:51,030 --> 00:07:53,583
우리는 일반적으로 깊은 학습을 배우고

207
00:07:53,583 --> 00:07:54,750
있다고 생각했습니까?

208
00:07:54,750 --> 00:07:56,708
사실 당신은 컴퓨터 아키텍처를 배우고 있습니다.

209
00:07:56,708 --> 00:07:57,710
죄송합니다, 놀라운 사실입니다.

210
00:07:57,710 --> 00:07:59,480
그리고 메모리 계층 구조는 깊은

211
00:07:59,480 --> 00:08:01,070
학습과 모든 종류의 고성능

212
00:08:01,070 --> 00:08:03,415
컴퓨팅에 정말 중요하다는 것이 밝혀졌습니다.

213
00:08:03,415 --> 00:08:04,790
일반적인 경향은 계산

214
00:08:04,790 --> 00:08:06,860
코어에서 멀리 떨어진 더 큰 메모리

215
00:08:06,860 --> 00:08:08,220
조각을 갖는 것입니다.

216
00:08:08,220 --> 00:08:10,080
계산 코어에 가까워질수록 더

217
00:08:10,080 --> 00:08:12,690
작은 메모리 조각이 있지만 훨씬 더 빠릅니다.

218
00:08:12,690 --> 00:08:14,390
이것은 정말 중요합니다--

219
00:08:14,390 --> 00:08:16,803
잘, 이러한 것들에서 실행되는 저수준

220
00:08:16,803 --> 00:08:18,470
알고리즘을 작성하는 경우, 이

221
00:08:18,470 --> 00:08:20,240
메모리 계층 구조를 인식하고

222
00:08:20,240 --> 00:08:21,890
서로 다른 메모리 계층

223
00:08:21,890 --> 00:08:24,813
간에 데이터를 전달하는 데 매우 부지런해야 합니다.

224
00:08:24,813 --> 00:08:26,730
그리고 성능 GPU 커널을 작성하는

225
00:08:26,730 --> 00:08:29,460
경우, 이를 최적화하는 데 많은 시간을 보냅니다.

226
00:08:29,460 --> 00:08:31,910
그래서 그에 대한 맛을 제공하기 위해,

227
00:08:31,910 --> 00:08:35,309
우리는 H100의 메모리 계층 구조의 세 가지 수준을 봅니다.

228
00:08:35,309 --> 00:08:40,650
L1 캐시가 256킬로바이트, L2 캐시가 50메가바이트, 그리고 HBM

229
00:08:40,650 --> 00:08:43,169
메모리가 80기가바이트 있습니다.

230
00:08:43,169 --> 00:08:44,750
이것이 H100의 메모리

231
00:08:44,750 --> 00:08:47,680
계층 구조의 세 가지 주요 수준입니다.

232
00:08:47,680 --> 00:08:51,430
그리고 우리는 이 FP32 코어가 128개 있습니다.

233
00:08:51,430 --> 00:08:53,260
이들은 일반화된 부동 소수점

234
00:08:53,260 --> 00:08:56,060
연산을 수행할 수 있는 작은 산술 유닛입니다.

235
00:08:56,060 --> 00:08:59,980
특히 이 128개의 FP32 코어 각각은 a, x, b가

236
00:08:59,980 --> 00:09:03,890
모두 스칼라인 경우 ax + b를 계산할 수 있습니다.

237
00:09:03,890 --> 00:09:06,950
이 계산은 한 클럭 사이클 내에 수행할 수 있습니다.

238
00:09:06,950 --> 00:09:10,030
따라서 이 모든 것을 합치면 ax + b는

239
00:09:10,030 --> 00:09:12,400
기본적으로 1 곱하기 1

240
00:09:12,400 --> 00:09:15,190
더하기이며, 이 코어가 128개 있습니다.

241
00:09:15,190 --> 00:09:20,170
이 전체 SM은 장치의 클럭 사이클당 SM당 256개의 부동

242
00:09:20,170 --> 00:09:22,690
소수점 연산을 수행할 수 있습니다.

243
00:09:22,690 --> 00:09:24,940
그리고 빨간색으로 표시된 부분에서,

244
00:09:24,940 --> 00:09:27,880
여기서 진정한 마법이 일어납니다. 이

245
00:09:27,880 --> 00:09:31,990
FP32 코어 외에도 이 네 개의 텐서 코어가 있습니다.

246
00:09:31,990 --> 00:09:34,910
이름이 약간 오해의 소지가 있다고 생각합니다.

247
00:09:34,910 --> 00:09:37,210
이들은 실제로 행렬 코어입니다.

248
00:09:37,210 --> 00:09:39,250
각 텐서 코어는

249
00:09:39,250 --> 00:09:41,990
특별히 설계된 회로입니다.

250
00:09:41,990 --> 00:09:43,130
그들은 단 하나의 작업만 수행합니다.

251
00:09:43,130 --> 00:09:44,340
행렬 곱셈을 수행합니다.

252
00:09:44,340 --> 00:09:46,190
각 텐서 코어는 행렬 곱셈의

253
00:09:46,190 --> 00:09:48,890
단일 청크를 수행할 수 있습니다.

254
00:09:48,890 --> 00:09:52,940
특히 H100의 경우 입력 행렬 A는

255
00:09:52,940 --> 00:09:57,180
16x4, 입력 행렬 B는 4x8,

256
00:09:57,180 --> 00:10:01,440
그리고 편향 행렬은 16x8 크기입니다.

257
00:10:01,440 --> 00:10:03,920
즉, ax + b를 수행하는데, 여기서 a,

258
00:10:03,920 --> 00:10:06,900
x, b는 고정 크기의 작은 행렬 청크입니다.

259
00:10:06,900 --> 00:10:09,320
각 텐서 코어는 클럭 사이클당 한

260
00:10:09,320 --> 00:10:12,560
번의 행렬 곱셈 청크를 수행할 수 있습니다.

261
00:10:12,560 --> 00:10:15,390
따라서 이 모든 숫자를 곱하면, 해당

262
00:10:15,390 --> 00:10:19,850
크기의 ax + b의 작은 행렬 곱셈은 1,024개의

263
00:10:19,850 --> 00:10:23,450
부동 소수점 연산이 되며, 여기서 각 곱셈과 각

264
00:10:23,450 --> 00:10:25,130
덧셈이 하나의 부동

265
00:10:25,130 --> 00:10:27,300
소수점 연산으로 계산됩니다.

266
00:10:27,300 --> 00:10:30,360
이것을 SM의 4개의 텐서 코어로 곱합니다.

267
00:10:30,360 --> 00:10:34,310
그리고 텐서 코어를 통해 전체 SM은 클럭

268
00:10:34,310 --> 00:10:39,350
사이클당 SM당 4,096개의 부동 소수점 연산을 수행할

269
00:10:39,350 --> 00:10:40,460
수 있습니다.

270
00:10:40,460 --> 00:10:42,660
그리고 이것은 FP32 코어에서 얻을

271
00:10:42,660 --> 00:10:44,590
수 있는 256과 비교해야 합니다.

272
00:10:44,590 --> 00:10:46,980
여기서 텐서 코어가 모든 마법이 일어나는

273
00:10:46,980 --> 00:10:48,622
곳이라는 것을 알 수 있습니다.

274
00:10:48,622 --> 00:10:50,580
이곳이 장치의 주요 처리량이 나오는

275
00:10:50,580 --> 00:10:51,163
곳입니다.

276
00:10:51,163 --> 00:10:53,580
이 GPU에서 실행되기를

277
00:10:53,580 --> 00:10:56,290
원하는 코드를 작성하는 경우,

278
00:10:56,290 --> 00:10:59,297
최대한 텐서 코어를 활용해야 합니다.

279
00:10:59,297 --> 00:11:01,380
이 텐서 코어에 대한 또 다른 흥미로운 점은

280
00:11:01,380 --> 00:11:03,452
실제로 혼합 정밀도로 작동한다는 것입니다.

281
00:11:03,452 --> 00:11:05,410
전통적인 부동 소수점

282
00:11:05,410 --> 00:11:08,760
숫자(일반적으로 32비트) 대신, 텐서 코어는 보통

283
00:11:08,760 --> 00:11:11,903
16비트의 혼합 정밀도 절차를 사용합니다.

284
00:11:11,903 --> 00:11:14,070
오늘은 다룰 수 없는 몇

285
00:11:14,070 --> 00:11:17,320
가지 흥미로운 16비트 형식이 있습니다.

286
00:11:17,320 --> 00:11:20,100
곱셈은 이 낮은 정밀도 16비트에서

287
00:11:20,100 --> 00:11:22,450
수행하고, 덧셈과 누적은 더 높은

288
00:11:22,450 --> 00:11:24,820
정밀도 32비트에서 수행합니다.

289
00:11:24,820 --> 00:11:29,170
따라서 이 텐서 코어는 낮은 정밀도 16비트 입력을 받아

290
00:11:29,170 --> 00:11:31,170
중간 계산을 수행하고, 더

291
00:11:31,170 --> 00:11:34,290
높은 정밀도 32비트로 출력을 생성합니다.

292
00:11:34,290 --> 00:11:37,660
이는 PyTorch 레이어에서 모델을 16비트로

293
00:11:37,660 --> 00:11:40,760
변환하는 것을 잊으면 부동 소수점 코어에서

294
00:11:40,760 --> 00:11:42,580
실행되기 때문에 중요합니다.

295
00:11:42,580 --> 00:11:45,080
대신 예상보다 20배 느리게 실행됩니다.

296
00:11:45,080 --> 00:11:47,690
이것은 사소한 것처럼 보이지만,

297
00:11:47,690 --> 00:11:50,530
PyTorch 코드에서 데이터 유형을

298
00:11:50,530 --> 00:11:53,290
잘못 다루면 매우 실질적으로 느껴집니다.

299
00:11:53,290 --> 00:11:55,910
그리고 GPU는 정말 빠르며,

300
00:11:55,910 --> 00:11:58,930
지난 10년 또는 15년 동안

301
00:11:58,930 --> 00:12:02,200
얼마나 더 빨라졌는지 정말 놀랍습니다.

302
00:12:02,200 --> 00:12:05,470
제가 박사 과정을 처음 시작했을

303
00:12:05,470 --> 00:12:08,110
때 사용하던 최첨단 GPU는

304
00:12:08,110 --> 00:12:12,290
2013년에 출시된 K40 GPU였습니다.

305
00:12:12,290 --> 00:12:16,420
이 장치는 전체 장치에서 5 테라플롭스의 FP32 연산을 수행할

306
00:12:16,420 --> 00:12:17,662
수 있었습니다.

307
00:12:17,662 --> 00:12:19,370
그래프를 설명해야 합니다.

308
00:12:19,370 --> 00:12:23,260
x축은 2013년부터 현재까지의 시간을

309
00:12:23,260 --> 00:12:24,380
나타냅니다.

310
00:12:24,380 --> 00:12:26,500
y축은 각 장치의

311
00:12:26,500 --> 00:12:31,180
초당 테라플롭스 기준으로 측정된 최대

312
00:12:31,180 --> 00:12:33,040
처리량입니다.

313
00:12:33,040 --> 00:12:35,870
그래프가 많이 상승하는 것을 볼 수 있습니다.

314
00:12:35,870 --> 00:12:37,900
하지만 K40에서 P100으로

315
00:12:37,900 --> 00:12:40,600
넘어가면서 V100에서 정말 놀라운

316
00:12:40,600 --> 00:12:42,820
일이 발생했습니다. V100은

317
00:12:42,820 --> 00:12:47,200
제 박사 과정이 끝날 무렵인 2016년, 2017년에

318
00:12:47,200 --> 00:12:49,290
출시되었고, 텐서 코어를 처음

319
00:12:49,290 --> 00:12:50,970
도입한 장치였습니다.

320
00:12:50,970 --> 00:12:54,960
그 이후로 더 최근의 장치들은 더 많은 텐서 코어,

321
00:12:54,960 --> 00:12:57,970
더 큰 텐서 코어, 텐서 코어에

322
00:12:57,970 --> 00:13:00,310
할당된 장치 면적이 늘어나면서

323
00:13:00,310 --> 00:13:02,775
지난 10년 또는 15년 동안

324
00:13:02,775 --> 00:13:04,900
이러한 장치의 처리량이 엄청나게

325
00:13:04,900 --> 00:13:06,270
증가했습니다.

326
00:13:06,270 --> 00:13:09,000
가장 최근의 장치는 공식 발표된

327
00:13:09,000 --> 00:13:10,750
B200입니다.

328
00:13:10,750 --> 00:13:12,310
현재 천천히 출시되고 있습니다.

329
00:13:12,310 --> 00:13:21,600
이론적으로 이 장치는 FP32 연산에서 초당 약 83.3 테라플롭스, 텐서
코어에서 혼합

330
00:13:21,600 --> 00:13:25,560
정밀도 연산으로 초당 5,000

331
00:13:25,560 --> 00:13:28,270
테라플롭스를 가지고 있습니다.

332
00:13:28,270 --> 00:13:31,290
한 걸음 물러서서 보면, 우리는 지난

333
00:13:31,290 --> 00:13:35,580
12년 동안 실제로 1,000배의 연산 증가를 경험하고

334
00:13:35,580 --> 00:13:36,640
있습니다.

335
00:13:36,640 --> 00:13:38,625
이는 장치 수준에서의 이야기입니다.

336
00:13:38,625 --> 00:13:42,220
지난 10년 동안 AI가 이렇게 좋아진

337
00:13:42,220 --> 00:13:43,660
이유는 무엇일까요?

338
00:13:43,660 --> 00:13:44,840
이것이 그 답입니다.

339
00:13:44,840 --> 00:13:47,110
이제 우리가 활용하고 있는 연산의 원천이

340
00:13:47,110 --> 00:13:50,390
있으며, 지난 10년 동안 1,000배 증가했습니다.

341
00:13:50,390 --> 00:13:53,375
세상의 어떤 것이 1,000배 변화하면,

342
00:13:53,375 --> 00:13:55,750
주목해야 합니다. 이는 우리의

343
00:13:55,750 --> 00:13:58,700
기술적 능력에 큰 변화를 가져올 것입니다.

344
00:13:58,700 --> 00:14:01,010
이 1,000배 개선이 지난

345
00:14:01,010 --> 00:14:03,610
10년 동안 딥러닝 개선의 주요

346
00:14:03,610 --> 00:14:05,290
원동력이라고 생각합니다.

347
00:14:05,290 --> 00:14:08,840
따라서 5,000개의 텐서 코어가 있는 것은 아닙니다.

348
00:14:08,840 --> 00:14:12,660
텐서 코어에서의 연산이 5,000 테라플롭스입니다.

349
00:14:12,660 --> 00:14:13,160
네.

350
00:14:13,160 --> 00:14:15,285
우리는 항상 텐서 코어에서의

351
00:14:15,285 --> 00:14:19,330
연산과 FP32 코어에서의 연산을 구분하려고 합니다.

352
00:14:19,330 --> 00:14:20,960
이것은 이미 미쳤습니다.

353
00:14:20,960 --> 00:14:23,650
손에 쥘 수 있는 장치에서 1,000배

354
00:14:23,650 --> 00:14:25,790
증가한 것은 이미 미친 일입니다.

355
00:14:25,790 --> 00:14:29,620
저는 K40을 손에 쥐어본 적이 있고, B100을 쥐어볼

356
00:14:29,620 --> 00:14:30,620
기회는 없었습니다.

357
00:14:30,620 --> 00:14:33,110
하지만 두 장치는 같은 물리적 객체처럼 느껴집니다.

358
00:14:33,110 --> 00:14:35,490
크기와 무게가 비슷하고, 생김새도 비슷하지만,

359
00:14:35,490 --> 00:14:38,490
오늘날의 장치는 12년 전의 장치보다 1,

360
00:14:38,490 --> 00:14:39,640
000배 빠릅니다.

361
00:14:39,640 --> 00:14:40,830
정말 미친 일입니다.

362
00:14:40,830 --> 00:14:44,980
하지만 더 미친 것은 우리가 하나의 GPU에서 훈련하지 않는다는 것입니다.

363
00:14:44,980 --> 00:14:48,250
K40이 2013년에 처음 출시되었을 때, 실제로 많은

364
00:14:48,250 --> 00:14:51,400
모델을 하나의 GPU에서 훈련하는 것이 일반적이었습니다.

365
00:14:51,400 --> 00:14:53,740
하지만 오늘 우리는 하나의 GPU에서만 훈련하지 않습니다.

366
00:14:53,740 --> 00:14:55,780
우리는 수천, 수만, 때로는 수십만

367
00:14:55,780 --> 00:14:57,730
개의 GPU에서 훈련하고 있으며, 이

368
00:14:57,730 --> 00:15:00,482
모든 GPU가 함께 작업하여 하나의 모델을 훈련합니다.

369
00:15:00,482 --> 00:15:03,870
그래서 이것을 장치당 처리량이 1,000배 증가한 것

370
00:15:03,870 --> 00:15:04,720
위에 쌓아보세요.

371
00:15:04,720 --> 00:15:08,520
그리고 지난 10년 동안 정말 미친 일이 일어났습니다.

372
00:15:08,520 --> 00:15:13,030
그래서 우리는 GPU 내부를 살펴보았습니다.

373
00:15:13,030 --> 00:15:16,360
이제 여기서 저는 GPU를 개별 장치가

374
00:15:16,360 --> 00:15:17,980
아닌, 우리가 구축하는

375
00:15:17,980 --> 00:15:19,860
현대 GPU 클러스터의

376
00:15:19,860 --> 00:15:22,300
맥락에서 확대하고 싶습니다.

377
00:15:22,300 --> 00:15:26,427
우리는 이미 단일 H100 GPU를 보았습니다.

378
00:15:26,427 --> 00:15:28,260
여기서 우리는 그것을 또 다른 메모리 계층의

379
00:15:28,260 --> 00:15:29,670
수준으로 생각할 수 있습니다.

380
00:15:29,670 --> 00:15:31,680
우리는 H100 내부를 살펴보았고,

381
00:15:31,680 --> 00:15:33,742
계산 요소에 가까워질수록 세

382
00:15:33,742 --> 00:15:35,450
개의 메모리 계층이 있었습니다.

383
00:15:35,450 --> 00:15:37,520
계산 요소에서 멀어질수록 대역폭,

384
00:15:37,520 --> 00:15:39,020
메모리 대역폭, 시스템의

385
00:15:39,020 --> 00:15:41,020
다양한 부분 간에 비트를 이동할

386
00:15:41,020 --> 00:15:43,460
수 있는 장치의 능력이 느려집니다.

387
00:15:43,460 --> 00:15:45,430
이 경향은 단일 장치의

388
00:15:45,430 --> 00:15:47,650
경계를 벗어나 전체 데이터

389
00:15:47,650 --> 00:15:50,480
센터의 맥락에서 상상할 때 계속됩니다.

390
00:15:50,480 --> 00:15:54,250
여기서 우리는 단일 H100 GPU가 약 3테라바이트의 메모리 대역폭을

391
00:15:54,250 --> 00:15:55,640
가진다는 것을 보았습니다.

392
00:15:55,640 --> 00:15:59,260
이는 GPU 메모리가 자신의 HBM 메모리에서 자신의 계산 요소로

393
00:15:59,260 --> 00:16:01,640
3테라바이트 per 초로 이야기하는 것입니다.

394
00:16:01,640 --> 00:16:03,100
비트를 이동할 수 있습니다.

395
00:16:03,100 --> 00:16:05,890
하지만 이러한 것들은 일반적으로 GPU 서버 내부에 있습니다.

396
00:16:05,890 --> 00:16:10,120
거의 모든 GPU 서버는 하나의 큰 박스에 8개의 장치를 가지고 있으며,

397
00:16:10,120 --> 00:16:12,560
이러한 GPU는 서로 통신할 수 있습니다.

398
00:16:12,560 --> 00:16:14,140
서버 내의 어떤 GPU에서

399
00:16:14,140 --> 00:16:16,540
다른 GPU로 약 900기가바이트

400
00:16:16,540 --> 00:16:20,060
per 초의 속도로 서로 통신하는 것이 일반적입니다.

401
00:16:20,060 --> 00:16:23,860
그래서 이는 하나의 장치 내부에서 GPU가 이야기하는

402
00:16:23,860 --> 00:16:29,050
것에 비해 메모리 통신 대역폭이 3배 낮다는 것을 알 수 있습니다.

403
00:16:29,050 --> 00:16:32,580
그리고 여기서 우리는 다시 Llama3로 돌아갑니다.

404
00:16:32,580 --> 00:16:34,597
많은 주요 업체들이 훈련 클러스터에 대한 많은

405
00:16:34,597 --> 00:16:36,430
세부 정보를 공개하지 않지만, Llama3

406
00:16:36,430 --> 00:16:38,340
기술 보고서는 실제로 그들의 훈련

407
00:16:38,340 --> 00:16:40,300
클러스터에 대한 많은 세부 정보를 제공했습니다.

408
00:16:40,300 --> 00:16:41,790
그래서 여기서 일부 세부

409
00:16:41,790 --> 00:16:44,260
사항은 클러스터마다 약간 다를 수 있습니다.

410
00:16:44,260 --> 00:16:46,710
하지만 이제 이것은 그들의 모델을

411
00:16:46,710 --> 00:16:49,860
훈련하는 데 사용된 Llama3 클러스터의 숫자입니다.

412
00:16:49,860 --> 00:16:52,660
그들은 하나의 GPU 박스에서 제공합니다.

413
00:16:52,660 --> 00:16:55,530
그들은 하나의 서버 랙에 두 개의 박스를 쌓습니다.

414
00:16:55,530 --> 00:16:57,280
서버 랙은 보지

415
00:16:57,280 --> 00:17:00,120
못했다면 약 6피트 높이로,

416
00:17:00,120 --> 00:17:02,680
사람 크기 정도입니다.

417
00:17:02,680 --> 00:17:04,740
하나의 서버 랙에는 두 개의 서버가

418
00:17:04,740 --> 00:17:07,410
들어 있으며, 총 16개의 GPU가 있습니다.

419
00:17:07,410 --> 00:17:11,550
그런 다음 우리는 많은 서버 랙을 연결하여 GPU 포드를 만듭니다.

420
00:17:11,550 --> 00:17:16,230
Llama3 클러스터는 192개의 랙으로 구성된 GPU 포드를 가지고

421
00:17:16,230 --> 00:17:19,210
있으며, 총 3,072개의 GPU가 있습니다.

422
00:17:19,210 --> 00:17:21,480
이러한 것들은 모든 랙 간에 매우 높은

423
00:17:21,480 --> 00:17:23,589
대역폭 커넥터를 가지고 있습니다.

424
00:17:23,589 --> 00:17:27,599
그 결과, 해당 포드 내의 GPU 쌍은 약 50기가바이트

425
00:17:27,599 --> 00:17:30,140
per 초의 속도로 서로 통신할 수

426
00:17:30,140 --> 00:17:30,870
있습니다.

427
00:17:30,870 --> 00:17:34,100
이제 보시다시피, 이는 개별 서버가 이야기할 수

428
00:17:34,100 --> 00:17:37,220
있는 것과 전체 랙의 GPU가 서로 이야기할 수

429
00:17:37,220 --> 00:17:40,490
있는 것 간의 메모리 트래픽에서 또 다른 20배

430
00:17:40,490 --> 00:17:41,330
감소입니다.

431
00:17:41,330 --> 00:17:44,100
그래서 3,072개의 GPU는 많은 계산처럼

432
00:17:44,100 --> 00:17:46,310
보이지만, 결코 충분하지 않습니다.

433
00:17:46,310 --> 00:17:48,590
그래서 우리는 그 GPU 포드를 함께 쌓아서

434
00:17:48,590 --> 00:17:50,390
전체 GPU 클러스터를 만들 것입니다.

435
00:17:50,390 --> 00:17:52,520
그래서 이것은 Meta가 Llama3

436
00:17:52,520 --> 00:17:55,650
모델을 훈련하기 위해 만든 전체 GPU 클러스터입니다.

437
00:17:55,650 --> 00:17:58,670
이것은 총 24,576개의 GPU를 위해

438
00:17:58,670 --> 00:18:01,790
8개의 GPU 포드를 결합한 것입니다.

439
00:18:01,790 --> 00:18:04,610
이들 간의 메모리 트래픽에 대한 정확한 숫자는

440
00:18:04,610 --> 00:18:06,530
찾을 수 없었지만, 확실히

441
00:18:06,530 --> 00:18:08,870
초당 50기가바이트보다 적습니다.

442
00:18:08,870 --> 00:18:12,050
그리고 이건 세계에서 가장 큰 GPU

443
00:18:12,050 --> 00:18:13,522
클러스터는 아닙니다.

444
00:18:13,522 --> 00:18:15,230
제가 빠르게 정확한 숫자를 찾을 수 있는

445
00:18:15,230 --> 00:18:16,270
가장 큰 클러스터입니다.

446
00:18:16,270 --> 00:18:18,770
하지만 세계에는 50,000개, 100,

447
00:18:18,770 --> 00:18:21,930
000개의 GPU 클러스터가 분명히 존재합니다.

448
00:18:21,930 --> 00:18:24,555
그들은 존재하며, 사람들은 그 위에서 모델을 훈련합니다.

449
00:18:24,555 --> 00:18:27,180
이것이 작동하는 방식은 자연스럽게 확장됩니다.

450
00:18:27,180 --> 00:18:29,730
그래서 더 큰 클러스터를 만들기 위해

451
00:18:29,730 --> 00:18:31,818
더 많은 포드를 함께 클러스터링하거나,

452
00:18:31,818 --> 00:18:33,360
다른 슈퍼 포드와

453
00:18:33,360 --> 00:18:35,220
연결되는 슈퍼 포드를

454
00:18:35,220 --> 00:18:38,310
가질 수 있는 또 다른 계층이 있을 수 있습니다.

455
00:18:38,310 --> 00:18:40,560
그 GPU 클러스터로 얼마나 훈련하나요?

456
00:18:40,560 --> 00:18:43,300
Llama3 모델에 대해서는 정확히 기억나지

457
00:18:43,300 --> 00:18:45,030
않지만, 지난 10년

458
00:18:45,030 --> 00:18:48,090
동안의 경험칙은 사람들이 훈련하는 가장 긴

459
00:18:48,090 --> 00:18:49,972
모델은 보통 몇 달 정도입니다.

460
00:18:49,972 --> 00:18:51,930
그리고 이는 기술보다는 사람과 더 관련이

461
00:18:51,930 --> 00:18:53,070
있다고 생각합니다.

462
00:18:53,070 --> 00:18:57,120
진전을 이루고, 계획을 세우고, 사람들이 작업하는

463
00:18:57,120 --> 00:18:59,130
것과 관련해서는 매우 긴 훈련을

464
00:18:59,130 --> 00:19:01,543
갖는 것이 매우 어렵습니다.

465
00:19:01,543 --> 00:19:03,960
그래서 가장 긴 훈련, 가장 최신의 모델은

466
00:19:03,960 --> 00:19:06,360
보통 몇 달로 측정된다고 생각합니다.

467
00:19:06,360 --> 00:19:09,010
GPT-4.5나 GPT-5와 같은

468
00:19:09,010 --> 00:19:13,140
가장 큰 모델이 현재 1년에 가까워지고 있다면 놀라지

469
00:19:13,140 --> 00:19:14,340
않을 것입니다.

470
00:19:14,340 --> 00:19:16,350
하지만 이러한 정말 큰 훈련

471
00:19:16,350 --> 00:19:18,960
클러스터에서 몇 달 정도의 훈련을

472
00:19:18,960 --> 00:19:20,760
보는 것은 꽤 일반적입니다.

473
00:19:20,760 --> 00:19:23,190
질문은, 왜 서버를 포드가 아니라

474
00:19:23,190 --> 00:19:24,310
랙에 조직하나요?

475
00:19:24,310 --> 00:19:25,660
어딘가에 두어야 합니다.

476
00:19:25,660 --> 00:19:27,640
이런 것들에는 물리적 제약이 있습니다.

477
00:19:27,640 --> 00:19:30,820
그래서 서버 랙은 수십 년 동안 데이터

478
00:19:30,820 --> 00:19:33,500
센터의 표준 단위였습니다.

479
00:19:33,500 --> 00:19:36,970
이 새로운 GPU 장치가 등장했을 때, 이는 다른

480
00:19:36,970 --> 00:19:38,737
종류의 서버를 제공합니다.

481
00:19:38,737 --> 00:19:40,070
물리적으로 훨씬 더 큽니다.

482
00:19:40,070 --> 00:19:41,770
전력이 훨씬 더 많지만,

483
00:19:41,770 --> 00:19:45,470
데이터 센터를 하룻밤 사이에 완전히 재설계할 수는 없습니다.

484
00:19:45,470 --> 00:19:48,700
결과적으로, 서버 랙은 표준 하드웨어 크기와

485
00:19:48,700 --> 00:19:50,620
함께 데이터 센터가 일반적으로

486
00:19:50,620 --> 00:19:53,380
구축되는 표준 단위가 되었습니다.

487
00:19:53,380 --> 00:19:56,360
클러스터나 무언가가 얼마나 많은 물리적 공간을

488
00:19:56,360 --> 00:19:56,860
차지하나요?

489
00:19:56,860 --> 00:19:59,800
아, 그건 좋은 질문입니다.

490
00:19:59,800 --> 00:20:02,560
단일 서버 랙은 대략

491
00:20:02,560 --> 00:20:06,410
6~8피트 정도의 높이로 생각해야

492
00:20:06,410 --> 00:20:07,580
합니다.

493
00:20:07,580 --> 00:20:10,600
그래서 아마도 서버 랙은 이 강단의 크기

494
00:20:10,600 --> 00:20:13,120
정도이고 제 키 정도의 높이일 것입니다.

495
00:20:13,120 --> 00:20:15,530
그런 다음 포드에는 192개의 랙이 있습니다.

496
00:20:15,530 --> 00:20:17,733
그래서 이 강단 200개를 상상해 보세요.

497
00:20:17,733 --> 00:20:18,650
그것은 얼마나 클까요?

498
00:20:18,650 --> 00:20:20,320
그것을 8로 곱하세요.

499
00:20:20,320 --> 00:20:22,660
하지만 실제로는 약간 과소 추정입니다. 왜냐하면

500
00:20:22,660 --> 00:20:24,820
일반적으로 사람들이 그 사이를 걸을 수 있도록

501
00:20:24,820 --> 00:20:26,780
이러한 것들을 행으로 정리하기 때문입니다.

502
00:20:26,780 --> 00:20:28,197
클러스터에 패킹해야 할

503
00:20:28,197 --> 00:20:31,600
하드웨어가 더 있습니다. 단순히 컴퓨트 랙만이 아닙니다.

504
00:20:31,600 --> 00:20:34,330
물리적 GPU 서버가 있는 컴퓨트 랙 외에도

505
00:20:34,330 --> 00:20:35,980
네트워킹 하드웨어를 포함하는

506
00:20:35,980 --> 00:20:37,310
다른 랙이 있을 것입니다.

507
00:20:37,310 --> 00:20:40,270
모든 장치 간에 이동해야 할 비트가

508
00:20:40,270 --> 00:20:41,403
많습니다.

509
00:20:41,403 --> 00:20:42,820
그래서 네트워킹 하드웨어만을 보관하는

510
00:20:42,820 --> 00:20:44,070
전용 랙이 있을 것입니다.

511
00:20:44,070 --> 00:20:45,487
저장 하드웨어만을 보관하는

512
00:20:45,487 --> 00:20:47,350
전용 랙도 있을 것입니다. 훈련

513
00:20:47,350 --> 00:20:49,060
데이터를 어딘가에 저장하고 장치로

514
00:20:49,060 --> 00:20:50,030
가져와야 하니까요.

515
00:20:50,030 --> 00:20:52,860
이러한 것들은 꽤 많은 공간을 차지할 수 있습니다.

516
00:20:52,860 --> 00:20:53,360
아, 맞아요.

517
00:20:53,360 --> 00:20:55,985
질문은, 이러한 큰 클러스터에 가면 작은

518
00:20:55,985 --> 00:20:58,610
컴퓨트 유닛들이 더 높은 처리량을 유지하나요?

519
00:20:58,610 --> 00:20:59,510
네, 유지합니다.

520
00:20:59,510 --> 00:21:02,740
이 시스템을 설계하는 비밀과 도전의

521
00:21:02,740 --> 00:21:04,340
일부입니다. 가능할 때

522
00:21:04,340 --> 00:21:06,340
빠른 통신을 활용하고,

523
00:21:06,340 --> 00:21:08,043
규모를 키울 때 더 큰

524
00:21:08,043 --> 00:21:10,210
유닛에서 느린 통신으로

525
00:21:10,210 --> 00:21:12,850
우아하게 되돌아가고 싶기 때문입니다.

526
00:21:12,850 --> 00:21:14,410
오, 얼마나 뜨거워지나요?

527
00:21:14,410 --> 00:21:16,165
꽤 뜨겁습니다.

528
00:21:16,165 --> 00:21:19,165
여기 계신 분 중에 게이머가 있고 집에 4090

529
00:21:19,165 --> 00:21:23,540
GPU나 5080 GPU가 있다면, 단일 4090 GPU가 게임을

530
00:21:23,540 --> 00:21:25,540
할 때 방을 데우고 창문을

531
00:21:25,540 --> 00:21:27,250
열고 싶게 만들 것입니다.

532
00:21:27,250 --> 00:21:29,830
방을 물리적으로 더 따뜻하게 만들 것입니다.

533
00:21:29,830 --> 00:21:31,780
하나의 게임 GPU가 평균 크기의

534
00:21:31,780 --> 00:21:33,620
방에 미치는 영향을 상상해

535
00:21:33,620 --> 00:21:35,230
보세요. 네, 데이터 센터에

536
00:21:35,230 --> 00:21:37,605
수만 개를 쌓으면 심각한 냉각 요구

537
00:21:37,605 --> 00:21:38,625
사항이 있습니다.

538
00:21:41,515 --> 00:21:43,390
또 다른 흥미로운 점은 냉각이 미친

539
00:21:43,390 --> 00:21:44,810
듯이 변한다는 것입니다.

540
00:21:44,810 --> 00:21:46,360
게임용 데스크탑은 일반적으로

541
00:21:46,360 --> 00:21:48,020
공랭식이고, 때때로 수랭식입니다.

542
00:21:48,020 --> 00:21:50,452
그리고 다양한 냉각 시스템을 설계할 수 있습니다.

543
00:21:50,452 --> 00:21:51,910
여기에서 모든 것을

544
00:21:51,910 --> 00:21:54,400
최적화하기 위해 하드웨어에 미쳐도 됩니다.

545
00:21:54,400 --> 00:21:54,950
좋습니다.

546
00:21:54,950 --> 00:21:56,450
이것들은 정말 멋진 것이라고 생각합니다.

547
00:21:56,450 --> 00:21:59,355
이 GPU들이 단순히 구름 속에 떠다니는 신화적인

548
00:21:59,355 --> 00:22:01,730
존재가 아니라는 것을 상상하는 것입니다.

549
00:22:01,730 --> 00:22:04,313
이들은 누군가가 만들고 방 어딘가에 쌓아놓은 실제

550
00:22:04,313 --> 00:22:05,390
물리적 원자입니다.

551
00:22:05,390 --> 00:22:08,290
그들이 어떻게 생겼는지 상상하는 것이 정말 흥미롭습니다.

552
00:22:08,290 --> 00:22:12,070
그래서 기본적으로 이러한 큰 GPU 클러스터로 이동할

553
00:22:12,070 --> 00:22:13,780
때의 사고 방식 변화는

554
00:22:13,780 --> 00:22:16,580
개별 장치나 개별 서버에 대해 생각하는

555
00:22:16,580 --> 00:22:17,930
것이 아닙니다.

556
00:22:17,930 --> 00:22:20,080
저는 전체 데이터 센터를 하나의

557
00:22:20,080 --> 00:22:22,250
큰 컴퓨터로 생각하려고 했습니다.

558
00:22:22,250 --> 00:22:25,860
이 큰 컴퓨터는 24,000개의 GPU, GPU에 1.

559
00:22:25,860 --> 00:22:29,040
8TB의 HBM 메모리, 4억 1,500만

560
00:22:29,040 --> 00:22:33,750
개의 FP32 코어, 1,300만 개의 텐서 코어를 가지고 있습니다.

561
00:22:33,750 --> 00:22:37,830
이 전체 시스템은 초당 24 엑사플롭의 계산을 수행할 수 있습니다.

562
00:22:37,830 --> 00:22:39,710
24 곱하기 10의 18제곱입니다.

563
00:22:39,710 --> 00:22:41,070
정말 많은 플롭입니다.

564
00:22:41,070 --> 00:22:42,745
많은 연산이 필요하지만, 오늘로부터

565
00:22:42,745 --> 00:22:44,120
5년 후에는 그렇게 많은

566
00:22:44,120 --> 00:22:47,270
연산이 필요하지 않을 것이라고 보장합니다. 그게 더 미친 부분입니다.

567
00:22:47,270 --> 00:22:50,480
우리의 목표는 이 24,000개의 GPU

568
00:22:50,480 --> 00:22:54,120
블록을 하나의 거대한 슈퍼컴퓨터로 생각하는 것입니다.

569
00:22:54,120 --> 00:22:56,120
그리고 질문은, 이 하나의 거대한

570
00:22:56,120 --> 00:22:58,190
슈퍼컴퓨터에서 어떻게 한 신경망을 몇

571
00:22:58,190 --> 00:23:00,700
달 동안 훈련할 수 있을까 하는 것입니다.

572
00:23:00,700 --> 00:23:02,450
정말 강력하고 많은 데이터를

573
00:23:02,450 --> 00:23:05,390
흡수할 수 있는 거대한 신경망을 훈련시키는 것입니다.

574
00:23:05,390 --> 00:23:07,610
그게 기본적으로 우리가

575
00:23:07,610 --> 00:23:10,640
딥러닝에서 이동한 질문과 패러다임입니다.

576
00:23:10,640 --> 00:23:12,955
그런데 계속 GPU라고 말하고,

577
00:23:12,955 --> 00:23:14,330
NVIDIA라고 말하는 이유는

578
00:23:14,330 --> 00:23:18,090
오늘날 가장 지배적인 훈련 아키텍처와 하드웨어이기 때문입니다.

579
00:23:18,090 --> 00:23:20,240
하지만 다른 것들도 생겨났습니다.

580
00:23:20,240 --> 00:23:23,810
현재 NVIDIA 훈련 하드웨어의 가장 큰 경쟁자는 Google이라고

581
00:23:23,810 --> 00:23:24,900
생각합니다.

582
00:23:24,900 --> 00:23:26,810
Google은 텐서 처리 장치(TPU)라는

583
00:23:26,810 --> 00:23:28,800
자체 하드웨어를 가지고 있습니다.

584
00:23:28,800 --> 00:23:30,890
이것들은 정말 좋습니다.

585
00:23:30,890 --> 00:23:34,220
이미 여섯 세대를 거쳤습니다.

586
00:23:34,220 --> 00:23:37,130
이것은 오늘 Google Cloud에서 임대할 수

587
00:23:37,130 --> 00:23:39,020
있는 v5p TPU의 통계입니다.

588
00:23:39,020 --> 00:23:41,780
그리고 H100과 비슷한 사양으로

589
00:23:41,780 --> 00:23:44,150
대략 같은 규모입니다.

590
00:23:44,150 --> 00:23:46,400
TPU에는 GPU와는 상당히 다른 흥미로운 설계

591
00:23:46,400 --> 00:23:48,350
결정이 있으며, 저는 그것이 매력적이라고

592
00:23:48,350 --> 00:23:50,308
생각하지만 오늘은 그에 대해 이야기할

593
00:23:50,308 --> 00:23:51,058
시간이 없습니다.

594
00:23:51,058 --> 00:23:53,100
누군가가 이 것들이 얼마나 큰지 물어보았습니다.

595
00:23:53,100 --> 00:23:54,500
이것은 실제 사진입니다.

596
00:23:54,500 --> 00:23:57,920
GPU처럼, 이 TPU들은 포드로

597
00:23:57,920 --> 00:24:05,030
배열되어 있으며, v5p TPU는 최대 8960개의 칩으로 배열될 수
있습니다.

598
00:24:05,030 --> 00:24:07,670
이것은 256개의 칩만 있는

599
00:24:07,670 --> 00:24:10,440
V2 TPU 포드의 사진입니다.

600
00:24:10,440 --> 00:24:13,290
그래서 이것들이 얼마나 큰지 감을 잡을

601
00:24:13,290 --> 00:24:14,220
수 있습니다.

602
00:24:14,220 --> 00:24:15,930
여기 네 개의 랙이 있습니다.

603
00:24:15,930 --> 00:24:19,080
그 랙은 제가 말했듯이, 제 키보다 약간

604
00:24:19,080 --> 00:24:19,830
더 큽니다.

605
00:24:19,830 --> 00:24:22,938
256개의 TPU 칩을 위해 나란히 배치된 네 개가 있습니다.

606
00:24:22,938 --> 00:24:25,230
그리고 이제 이 것이 최근 포드에서

607
00:24:25,230 --> 00:24:29,015
거의 9,000개의 칩으로 훨씬 더 커질 것이라고 상상해 보십시오.

608
00:24:29,015 --> 00:24:31,140
네, 그래서 Google의 Gemini 모델은

609
00:24:31,140 --> 00:24:32,898
거의 확실히 TPU에서 훈련되었습니다.

610
00:24:32,898 --> 00:24:34,440
물론 그들은 말하지

611
00:24:34,440 --> 00:24:38,520
않지만, 그렇지 않다면 정말 놀랄 것입니다.

612
00:24:38,520 --> 00:24:41,760
그리고 제가 말했듯이, TPU는 실제로 매우 좋습니다.

613
00:24:41,760 --> 00:24:43,620
대부분의 대규모 Google 모델이 이들에서

614
00:24:43,620 --> 00:24:45,203
훈련되고 있다고 생각하며, 그것들은

615
00:24:45,203 --> 00:24:46,500
매우 경쟁력 있는 모델입니다.

616
00:24:46,500 --> 00:24:48,812
그래서 이것은 정말 좋은 훈련 하드웨어입니다.

617
00:24:48,812 --> 00:24:50,770
NVIDIA와의 차이점은 구매할 수 없다는 것입니다.

618
00:24:50,770 --> 00:24:53,580
TPU에 접근할 수 있는 유일한 방법은 Google에서 일하거나

619
00:24:53,580 --> 00:24:55,408
Google Cloud에서 임대하는 것입니다.

620
00:24:55,408 --> 00:24:57,450
하지만 이것은 매우 좋은 하드웨어이며,

621
00:24:57,450 --> 00:24:58,950
많은 사람들이 이를

622
00:24:58,950 --> 00:25:03,600
사용하고 있지만, 오늘날 NVIDIA GPU보다 약간 덜 인기 있는 것
같습니다.

623
00:25:03,600 --> 00:25:05,310
물론 다른 회사들도 이것이 매우

624
00:25:05,310 --> 00:25:07,090
중요한 일이라는 것을 알고 있습니다.

625
00:25:07,090 --> 00:25:08,160
경쟁력 있는 훈련

626
00:25:08,160 --> 00:25:10,452
하드웨어를 구축하려는 다른 많은 회사들이 있습니다.

627
00:25:10,452 --> 00:25:12,570
하지만 제 솔직한 평가로는

628
00:25:12,570 --> 00:25:16,715
현재 NVIDIA와 TPU가 두 개의 주요 업체일 것입니다.

629
00:25:16,715 --> 00:25:18,590
현재 사용성, 성능,

630
00:25:18,590 --> 00:25:21,640
시장 점유율 측면에서 모두를 앞서

631
00:25:21,640 --> 00:25:22,160
있습니다.

632
00:25:22,160 --> 00:25:23,577
하지만 따라잡으려는 많은

633
00:25:23,577 --> 00:25:25,040
다른 회사들이 있습니다.

634
00:25:25,040 --> 00:25:27,340
주목할 만한 두 개는 AMD입니다.

635
00:25:27,340 --> 00:25:29,680
AMD는 수십 년 동안 두 번째 주요

636
00:25:29,680 --> 00:25:31,340
GPU 제조업체였습니다.

637
00:25:31,340 --> 00:25:35,530
그들은 MI325X라는 훈련 가속기도 가지고 있습니다.

638
00:25:35,530 --> 00:25:37,330
서류상으로는 H100과

639
00:25:37,330 --> 00:25:39,170
비교할 만한 좋은 성능을

640
00:25:39,170 --> 00:25:43,840
가지고 있지만, 현재 H100과 같은 영향을 미치지 못하고 있습니다.

641
00:25:43,840 --> 00:25:45,820
AWS도 Trainium이라는

642
00:25:45,820 --> 00:25:48,070
자체 훈련 칩을 개발했습니다.

643
00:25:48,070 --> 00:25:49,723
이것에 대해서는 잘 모르겠습니다.

644
00:25:49,723 --> 00:25:51,140
저는 직접 사용해본 적이 없지만,

645
00:25:51,140 --> 00:25:53,630
Anthropic이 그들의 훈련에 사용하고 있다는 것을 알고 있습니다.

646
00:25:53,630 --> 00:25:55,880
그들의 훈련이 전적으로

647
00:25:55,880 --> 00:25:58,570
Trainium인지 GPU인지 잘 모르겠습니다.

648
00:25:58,570 --> 00:25:59,960
그래서 더 많은 것을 기대해야 합니다.

649
00:25:59,960 --> 00:26:03,860
하지만 오늘날 NVIDIA GPU가 아마도 가장 지배적일 것입니다.

650
00:26:03,860 --> 00:26:05,427
그리고 Google TPU도 그 뒤에 있습니다.

651
00:26:05,427 --> 00:26:07,510
그들도 정말 좋지만 아마도 NVIDIA의

652
00:26:07,510 --> 00:26:10,340
GPU만큼 널리 사용되지는 않을 것입니다.

653
00:26:10,340 --> 00:26:10,840
좋습니다.

654
00:26:10,840 --> 00:26:12,320
그래서 기본적으로 첫 번째 부분입니다.

655
00:26:12,320 --> 00:26:13,338
GPU란 무엇인가요?

656
00:26:13,338 --> 00:26:14,880
우리는 그것들을 클러스터로 어떻게 배열하나요?

657
00:26:14,880 --> 00:26:17,090
우리가 구축하고 훈련하는 이러한 기계의 물리적 특성을

658
00:26:17,090 --> 00:26:18,740
감각적으로 이해할 수 있도록 합니다.

659
00:26:18,740 --> 00:26:21,110
두 번째 질문은, 수만 개의 GPU로

660
00:26:21,110 --> 00:26:25,190
구성된 이 거대한 GPU 클러스터를 활용할 수 있는 알고리즘을 실제로

661
00:26:25,190 --> 00:26:27,032
어떻게 작성할 것인가입니다.

662
00:26:27,032 --> 00:26:28,490
이는 새로운 알고리즘,

663
00:26:28,490 --> 00:26:31,710
컴퓨팅에 대한 새로운 사고 방식, 신경망을

664
00:26:31,710 --> 00:26:33,800
병렬화하고 나누는 새로운 방법을

665
00:26:33,800 --> 00:26:34,920
개발해야 합니다.

666
00:26:34,920 --> 00:26:36,710
여기서 기본 전략은 계산을

667
00:26:36,710 --> 00:26:38,340
나누는 것입니다.

668
00:26:38,340 --> 00:26:40,410
이것들은 거대한 병렬 장치입니다.

669
00:26:40,410 --> 00:26:43,610
많은 GPU, 많은 CPU 코어, 많은 GPU

670
00:26:43,610 --> 00:26:45,150
코어를 가지고 있습니다.

671
00:26:45,150 --> 00:26:46,760
모두 독립적으로 작동할 수 있으며,

672
00:26:46,760 --> 00:26:48,662
서로 많이 소통할 수는 없습니다.

673
00:26:48,662 --> 00:26:50,870
컴퓨터가 실제로 하는 일을 생각해보면, 고수준에서

674
00:26:50,870 --> 00:26:53,190
컴퓨터는 기본적으로 두 가지 일을 합니다.

675
00:26:53,190 --> 00:26:55,610
입력 비트를 받아 새로운

676
00:26:55,610 --> 00:26:57,665
출력 비트를 계산하는 계산과,

677
00:26:57,665 --> 00:27:00,290
비트를 가져와 한 메모리에서

678
00:27:00,290 --> 00:27:01,850
다른 메모리로

679
00:27:01,850 --> 00:27:03,900
이동하는 통신입니다.

680
00:27:03,900 --> 00:27:05,450
그리고 전체 요점은,

681
00:27:05,450 --> 00:27:09,590
전체 클러스터에서 메모리 계층의 여러 규모를

682
00:27:09,590 --> 00:27:11,390
어떻게 활용하여

683
00:27:11,390 --> 00:27:14,870
통신과 계산을 겹치게 할 것인가입니다.

684
00:27:14,870 --> 00:27:16,690
또한 계산을 나누고

685
00:27:16,690 --> 00:27:20,260
병렬화하여 거대한 신경망을 훈련하는 과정에서

686
00:27:20,260 --> 00:27:22,840
수만 개의 개별 GPU에

687
00:27:22,840 --> 00:27:25,700
유용한 작업을 제공하는 것입니다.

688
00:27:25,700 --> 00:27:28,100
모든 수백만 개의 개별 계산 요소에

689
00:27:28,100 --> 00:27:30,605
대해 병렬로 수행할 유용한 작업이 있어야

690
00:27:30,605 --> 00:27:32,980
하며, 그런 다음 이 거대한 클러스터에서

691
00:27:32,980 --> 00:27:35,740
거대한 신경망을 훈련하는 방식으로 서로

692
00:27:35,740 --> 00:27:37,720
작업을 소통하게 해야 합니다.

693
00:27:37,720 --> 00:27:40,330
그래서 이를 위해 제가 생각하는 한 가지

694
00:27:40,330 --> 00:27:43,185
방법은 사람들이 오늘날 대규모 신경망을 훈련할

695
00:27:43,185 --> 00:27:44,560
때 활용하는 기본적으로

696
00:27:44,560 --> 00:27:47,350
다섯 가지의 병렬성 수준이 있다는 것입니다.

697
00:27:47,350 --> 00:27:49,100
이것의 많은 부분은 트랜스포머에 특화되어

698
00:27:49,100 --> 00:27:51,558
있습니다. 왜냐하면 그것들이 대규모 훈련에 사용되는

699
00:27:51,558 --> 00:27:53,000
지배적인 아키텍처이기 때문입니다.

700
00:27:53,000 --> 00:27:54,650
트랜스포머를 생각해보면,

701
00:27:54,650 --> 00:27:58,310
트랜스포머는 기본적으로 L개의 레이어로 구성된 스택입니다.

702
00:27:58,310 --> 00:28:00,190
각 L 레이어는

703
00:28:00,190 --> 00:28:01,870
미니배치 차원이 하나인

704
00:28:01,870 --> 00:28:05,613
크기의 3차원 텐서에서 작동합니다.

705
00:28:05,613 --> 00:28:07,030
우리는 미니배치에서 작동하는

706
00:28:07,030 --> 00:28:09,610
여러 시퀀스를 가지고 있으며, 시퀀스 차원이 있습니다.

707
00:28:09,610 --> 00:28:12,140
우리는 시퀀스 또는 토큰 집합에서 작동하며,

708
00:28:12,140 --> 00:28:13,620
dim 차원이 있습니다.

709
00:28:13,620 --> 00:28:17,250
따라서 각 토큰 자체는 어떤 차원을 가진 벡터입니다.

710
00:28:17,250 --> 00:28:21,582
우리의 트랜스포머는 이러한 3차원 텐서에서

711
00:28:21,582 --> 00:28:23,040
작동합니다.

712
00:28:23,040 --> 00:28:25,320
그리고 그들은 레이어 스택을 통해 작동합니다.

713
00:28:25,320 --> 00:28:28,130
그래서 우리는 병렬화할 수 있는 네 가지 축을 갖게 됩니다.

714
00:28:28,130 --> 00:28:30,540
우리는 레이어 축에서 병렬화할 수 있으며,

715
00:28:30,540 --> 00:28:32,150
이는 파이프라인 병렬성입니다.

716
00:28:32,150 --> 00:28:35,247
우리는 배치 차원에서 병렬화할 수 있으며, 이는

717
00:28:35,247 --> 00:28:36,330
데이터 병렬성입니다.

718
00:28:36,330 --> 00:28:37,970
우리는 시퀀스 차원에서 나눌 수 있으며,

719
00:28:37,970 --> 00:28:39,360
이는 컨텍스트 병렬성이라고 불립니다.

720
00:28:39,360 --> 00:28:41,330
그리고 우리는 그 dim 차원에서 나눌 수 있으며,

721
00:28:41,330 --> 00:28:42,870
이는 텐서 병렬성이라고 불립니다.

722
00:28:42,870 --> 00:28:44,587
그래서 이 모든 것들은 재미있는 이름을 가지고 있습니다.

723
00:28:44,587 --> 00:28:46,170
하지만 이렇게 생각해보면,

724
00:28:46,170 --> 00:28:47,900
기본적으로 이들은 모두 트랜스포머

725
00:28:47,900 --> 00:28:50,240
내부의 이 네 가지 계산 축을 따라

726
00:28:50,240 --> 00:28:52,160
계산을 나누는 다양한 방법입니다.

727
00:28:52,160 --> 00:28:54,410
그리고 우리는 이러한 각각을 더 자세히

728
00:28:54,410 --> 00:28:56,810
살펴볼 것이며, 분산 훈련의 이러한

729
00:28:56,810 --> 00:28:59,090
다양한 메커니즘에 많은 흥미로운

730
00:28:59,090 --> 00:29:00,680
뉘앙스가 있기 때문입니다.

731
00:29:00,680 --> 00:29:03,920
그래서 첫 번째는 데이터 병렬성, 또는 DP입니다.

732
00:29:03,920 --> 00:29:06,888
여기서 기본 아이디어는 간단합니다.

733
00:29:06,888 --> 00:29:08,930
신경망을 훈련할 때 우리는 항상

734
00:29:08,930 --> 00:29:12,300
샘플의 미니배치에서 작동하고 있다는 것을 기억하세요.

735
00:29:12,300 --> 00:29:14,677
우리는 항상 요소의 미니배치를 가져옵니다.

736
00:29:14,677 --> 00:29:17,010
우리는 훈련 작업에 따라 미니배치의 각

737
00:29:17,010 --> 00:29:19,050
항목에 대해 손실을 계산합니다.

738
00:29:19,050 --> 00:29:21,920
그런 다음 우리는 그래디언트를 계산하는데,

739
00:29:21,920 --> 00:29:23,912
그래디언트는 일반적으로 미니배치의 개별

740
00:29:23,912 --> 00:29:26,370
요소에 대한 손실의 그래디언트 평균입니다.

741
00:29:26,370 --> 00:29:28,650
대부분의 신경망 아키텍처에서 손실을

742
00:29:28,650 --> 00:29:30,433
계산하고 그래디언트를

743
00:29:30,433 --> 00:29:31,850
계산하는 과정은 미니배치의

744
00:29:31,850 --> 00:29:34,260
각 요소에 대해 독립적입니다.

745
00:29:34,260 --> 00:29:37,230
그래서 이것은 명백히 병렬화할 수 있는 것처럼 보입니다.

746
00:29:37,230 --> 00:29:40,100
기본 아이디어는 N개의 예제를 단일

747
00:29:40,100 --> 00:29:43,160
GPU에 맞출 수 있고 M개의 GPU에

748
00:29:43,160 --> 00:29:45,530
접근할 수 있다면, 우리는 M배의

749
00:29:45,530 --> 00:29:48,710
N개의 예제를 가진 거대한 미니배치로 모델을

750
00:29:48,710 --> 00:29:52,220
훈련할 것이라는 것입니다. 이 거대한

751
00:29:52,220 --> 00:29:55,160
미니배치를 각 GPU에 N개의 샘플로 구성된

752
00:29:55,160 --> 00:29:57,218
작은 미니배치로 나눕니다.

753
00:29:57,218 --> 00:29:58,760
그리고 이것이 수학적으로 왜 의미가

754
00:29:58,760 --> 00:30:01,530
있는지 생각해보면, 그래디언트는 선형이기 때문입니다.

755
00:30:01,530 --> 00:30:04,940
실제로, 단일 스칼라 손실 L을

756
00:30:04,940 --> 00:30:07,100
계산하고 있다면, 이는

757
00:30:07,100 --> 00:30:10,355
각 개별 손실의 평균이 될 것입니다.

758
00:30:10,355 --> 00:30:14,510
이 xi,j는 전체 매크로 배치의

759
00:30:14,510 --> 00:30:15,980
모든 항목입니다.

760
00:30:15,980 --> 00:30:19,292
그리고 W는 전체 네트워크의 가중치 행렬입니다.

761
00:30:19,292 --> 00:30:20,750
그런 다음 일반적으로

762
00:30:20,750 --> 00:30:22,820
순전파의 마지막에서 계산하는

763
00:30:22,820 --> 00:30:24,770
손실은 각 개별 미니배치

764
00:30:24,770 --> 00:30:26,395
요소의 손실 평균입니다.

765
00:30:26,395 --> 00:30:27,770
그리고 네트워크의 가중치에

766
00:30:27,770 --> 00:30:30,497
대한 손실의 그래디언트를 취하면, 이는 가중치

767
00:30:30,497 --> 00:30:32,330
업데이트를 수행하기 위해

768
00:30:32,330 --> 00:30:33,510
계산해야 하는 것입니다.

769
00:30:33,510 --> 00:30:36,020
그런 다음 그것은 실제로 나누어질 것입니다.

770
00:30:36,020 --> 00:30:37,850
그래디언트가 선형이기 때문에, 우리는

771
00:30:37,850 --> 00:30:40,060
합을 어떤 순서로 할지 선택할 수 있습니다.

772
00:30:40,060 --> 00:30:41,310
그래디언트를 계산할까요?

773
00:30:41,310 --> 00:30:42,602
평균을 계산할까요?

774
00:30:42,602 --> 00:30:44,720
그래서 특히, 우리는 N개의

775
00:30:44,720 --> 00:30:46,820
요소에 대한 역전파를 위한

776
00:30:46,820 --> 00:30:49,340
일반적인 내부 항이 파란색으로

777
00:30:49,340 --> 00:30:52,940
강조된 이 특정 공식화에서 그래디언트를 배열하는

778
00:30:52,940 --> 00:30:54,600
것이 편리해집니다.

779
00:30:54,600 --> 00:30:58,710
이것들은 서로 다른 GPU에서 병렬로 계산될 수 있습니다.

780
00:30:58,710 --> 00:31:00,298
그리고 우리는 운영

781
00:31:00,298 --> 00:31:02,090
중인 M개의 서로

782
00:31:02,090 --> 00:31:07,630
다른 장치에서 그래디언트의 평균을 취해야 하는 외부 합이 있습니다.

783
00:31:07,630 --> 00:31:10,988
수학적 관점에서 이것이 일어나고 있습니다.

784
00:31:10,988 --> 00:31:13,280
그리고 우리는 이것이 수학적으로 완벽하게 타당하다는 것을 봅니다.

785
00:31:13,280 --> 00:31:15,490
이것은 기본적으로 단일 장치에서

786
00:31:15,490 --> 00:31:18,020
훈련하는 것과 수학적으로 정확히 동일합니다.

787
00:31:18,020 --> 00:31:19,660
우리는 대수적으로 영리하게

788
00:31:19,660 --> 00:31:22,790
작업하고 평균과 합의 순서를 변경했습니다.

789
00:31:22,790 --> 00:31:24,230
하지만 이것은 근사가 아닙니다.

790
00:31:24,230 --> 00:31:25,730
이것은 우리가 단일 대형

791
00:31:25,730 --> 00:31:28,300
GPU에서 수행했을 것과 정확히 동일한 계산입니다.

792
00:31:28,300 --> 00:31:30,910
GPU 관점에서 이것은 M개의

793
00:31:30,910 --> 00:31:33,680
GPU가 있다는 것을 의미합니다.

794
00:31:33,680 --> 00:31:36,250
여기서는 슬라이드에 적절하게 맞는

795
00:31:36,250 --> 00:31:37,820
3을 보여주고 있습니다.

796
00:31:37,820 --> 00:31:40,640
하지만 실제로는 3보다 훨씬 더 큽니다.

797
00:31:40,640 --> 00:31:42,910
그런 다음 각 GPU는 실제로

798
00:31:42,910 --> 00:31:45,010
최적화기 상태와 그래디언트의

799
00:31:45,010 --> 00:31:48,860
신경망 가중치의 별도 복사본을 유지합니다.

800
00:31:48,860 --> 00:31:50,740
그래서 우리가 할 것은 각

801
00:31:50,740 --> 00:31:53,830
GPU가 병렬로 서로 다른 미니배치를 로드하는 것입니다.

802
00:31:53,830 --> 00:31:56,230
여기서는 각 GPU가 세 개의 요소로 구성된 미니배치를

803
00:31:56,230 --> 00:31:57,590
로드하는 모습을 보여주고 있습니다.

804
00:31:57,590 --> 00:32:00,190
그리고 중요한 것은 서로 다른 GPU가 서로 다른

805
00:32:00,190 --> 00:32:02,290
미니배치를 로드해야 한다는 것입니다.

806
00:32:02,290 --> 00:32:04,348
내 코드와 학생 코드에서 버그가 발생했는데,

807
00:32:04,348 --> 00:32:06,140
그들이 실제로 모든 GPU에서 같은

808
00:32:06,140 --> 00:32:07,860
미니배치를 로드하는 실수를 했습니다.

809
00:32:07,860 --> 00:32:08,640
그것은 도움이 되지 않을 것입니다.

810
00:32:08,640 --> 00:32:09,807
좋지 않을 것입니다.

811
00:32:09,807 --> 00:32:11,150
그 실수를 하지 마세요.

812
00:32:11,150 --> 00:32:14,270
서로 다른 GPU가 실제로 서로 다른 미니배치를

813
00:32:14,270 --> 00:32:16,580
로드하는 것이 매우 중요합니다.

814
00:32:16,580 --> 00:32:20,000
그런 다음 각 GPU는 자신의 미니배치 데이터에

815
00:32:20,000 --> 00:32:23,090
대해 독립적으로 순전파를 수행하여 자신의

816
00:32:23,090 --> 00:32:25,050
로컬 손실을 계산합니다.

817
00:32:25,050 --> 00:32:27,840
이 모든 것은 완전히 독립적으로 작동할 수 있습니다.

818
00:32:27,840 --> 00:32:30,540
GPU 간의 통신이 필요하지 않습니다.

819
00:32:30,540 --> 00:32:33,860
그런 다음 각 네트워크는 자신의 로컬 손실에 대한 모델의

820
00:32:33,860 --> 00:32:36,290
모든 가중치에 대한 그래디언트를 계산하기

821
00:32:36,290 --> 00:32:38,520
위해 자신의 역전파를 수행합니다.

822
00:32:38,520 --> 00:32:40,520
그리고 다시, 이것은 각 모델이 기억해야

823
00:32:40,520 --> 00:32:42,440
할 독립적인 모델 가중치 복사본을

824
00:32:42,440 --> 00:32:45,450
가지고 있기 때문에 완전히 독립적으로 발생할 수 있습니다.

825
00:32:45,450 --> 00:32:47,510
각 GPU는 완전히 독립적으로 자신의

826
00:32:47,510 --> 00:32:49,550
순전파 및 역전파를 수행할 수 있습니다.

827
00:32:49,550 --> 00:32:51,587
하지만 이제 역전파가 완료된 후,

828
00:32:51,587 --> 00:32:52,920
여기서 문제가 발생합니다.

829
00:32:52,920 --> 00:32:54,830
우리는 훈련에 참여하는 모든

830
00:32:54,830 --> 00:32:57,650
장치에서 이러한 그래디언트의 평균을 계산해야

831
00:32:57,650 --> 00:32:59,130
한다고 말했습니다.

832
00:32:59,130 --> 00:33:00,960
그러면 우리는 통신이 필요합니다.

833
00:33:00,960 --> 00:33:04,360
여기서 우리는 올 리듀스 작업을 수행합니다.

834
00:33:04,360 --> 00:33:09,580
모든 GPU는 자신의 그래디언트를 다른 모든 GPU에 전송해야 합니다.

835
00:33:09,580 --> 00:33:11,950
그래서 두 가지 일이 동시에 발생합니다.

836
00:33:11,950 --> 00:33:15,910
하나는 각 GPU가 모든 GPU에 그래디언트를 브로드캐스트해야 합니다.

837
00:33:15,910 --> 00:33:18,900
그리고 두 번째는 각 GPU가 훈련에 참여하는

838
00:33:18,900 --> 00:33:22,050
모든 GPU로부터 그래디언트를 수집해야 합니다.

839
00:33:22,050 --> 00:33:24,130
이것이 올 리듀스 작업입니다.

840
00:33:24,130 --> 00:33:28,590
이는 일반적으로 GPU의 수에 따라 로그 시간

841
00:33:28,590 --> 00:33:30,490
내에 발생합니다.

842
00:33:30,490 --> 00:33:32,430
하지만 이 올 리듀스

843
00:33:32,430 --> 00:33:37,650
작업이 끝나면 각 GPU는 이제 모든 장치의 그래디언트 평균을

844
00:33:37,650 --> 00:33:39,040
갖게 됩니다.

845
00:33:39,040 --> 00:33:41,290
이 시점에서 통신이 이루어졌습니다.

846
00:33:41,290 --> 00:33:43,590
각 GPU는 이제 모든 장치에서 올

847
00:33:43,590 --> 00:33:45,420
리듀스된 그래디언트의 동일한

848
00:33:45,420 --> 00:33:46,630
복사본을 갖고 있습니다.

849
00:33:46,630 --> 00:33:49,030
이제 훈련 반복의 시작에서 우리는 각

850
00:33:49,030 --> 00:33:51,780
GPU가 모델 가중치의 독립적인 복사본을 가지고 있다고

851
00:33:51,780 --> 00:33:52,660
가정했습니다.

852
00:33:52,660 --> 00:33:55,410
이제 이 시점에서 각 GPU는 전체 매크로

853
00:33:55,410 --> 00:33:58,980
배치 데이터에 걸쳐 독립적이지만 동일한 그래디언트 복사본을

854
00:33:58,980 --> 00:33:59,920
가지고 있습니다.

855
00:33:59,920 --> 00:34:03,620
이제 이 시점에서 각 GPU는 자신의 로컬 가중치 복사본에 대해 가중치

856
00:34:03,620 --> 00:34:05,370
업데이트를 수행할 수 있습니다.

857
00:34:05,370 --> 00:34:07,781
그들은 동일한 가중치로 시작했고 동일한

858
00:34:07,781 --> 00:34:09,239
그래디언트를 적용했기

859
00:34:09,239 --> 00:34:11,781
때문에, 산술이 결정론적이라고 가정하면

860
00:34:11,781 --> 00:34:15,620
로컬 가중치 업데이트 후 동일한 가중치를 가질 것입니다.

861
00:34:15,620 --> 00:34:19,340
그리고 참고로, 이것은 정말 중요합니다.

862
00:34:19,340 --> 00:34:22,110
4단계와 5단계는 실제로 병렬로 발생할 수 있습니다.

863
00:34:22,110 --> 00:34:24,650
우리는 실제로 병렬로 발생할 수 있는 두 가지가

864
00:34:24,650 --> 00:34:25,920
있다고 말했습니다.

865
00:34:25,920 --> 00:34:28,790
하나는 각 GPU가 그래디언트를 계산하기 위해

866
00:34:28,790 --> 00:34:30,804
자신의 역전파를 계산하는 것이고,

867
00:34:30,804 --> 00:34:32,929
다른 하나는 GPU 간의

868
00:34:32,929 --> 00:34:34,048
그래디언트 통신입니다.

869
00:34:34,048 --> 00:34:35,840
이 두 가지는 실제로 일반적으로

870
00:34:35,840 --> 00:34:37,260
동시에 발생합니다.

871
00:34:37,260 --> 00:34:39,590
즉, 각 모델은 네트워크의 마지막 레이어에

872
00:34:39,590 --> 00:34:42,080
대해 역전파를 수행하고 자신의 로컬

873
00:34:42,080 --> 00:34:43,889
그래디언트를 계산하기 시작합니다.

874
00:34:43,889 --> 00:34:45,508
이제 모델은 모델의 두

875
00:34:45,508 --> 00:34:47,300
번째 마지막 레이어에 대한

876
00:34:47,300 --> 00:34:49,380
역전파 계산으로 이동합니다.

877
00:34:49,380 --> 00:34:51,080
그리고 계산 요소들이 두

878
00:34:51,080 --> 00:34:54,179
번째 마지막 레이어에 대한 역전파를 계산하는

879
00:34:54,179 --> 00:34:56,600
동안, GPU는 동시에 마지막

880
00:34:56,600 --> 00:34:59,500
레이어의 그래디언트를 올 리듀스하고 있습니다.

881
00:34:59,500 --> 00:35:02,700
즉, 이러한 것들은 레이어 L+1에 대한 통신과

882
00:35:02,700 --> 00:35:05,520
레이어 L에 대한 역전파를 병렬로 진행합니다.

883
00:35:05,520 --> 00:35:07,978
그리고 이들은 병렬로 진행될 수 있어,

884
00:35:07,978 --> 00:35:10,062
네트워크의 끝에 도달하고

885
00:35:10,062 --> 00:35:11,950
역전파가 완료될 때쯤에는 그래디언트가

886
00:35:11,950 --> 00:35:13,980
이미 모든 장치에서 올

887
00:35:13,980 --> 00:35:15,340
리듀스되었을 것입니다.

888
00:35:15,340 --> 00:35:17,520
그리고 우리는 기다리지 않고 한 번에 가중치

889
00:35:17,520 --> 00:35:18,600
업데이트를 할 수 있습니다.

890
00:35:18,600 --> 00:35:21,150
이것은 정말 중요합니다. 왜냐하면 우리가 말했듯이

891
00:35:21,150 --> 00:35:22,853
통신은 상대적으로 느리기 때문입니다.

892
00:35:22,853 --> 00:35:24,270
따라서 이러한 것들의

893
00:35:24,270 --> 00:35:26,940
전체 요점은 통신 비용을 숨기고 계산과 동시에

894
00:35:26,940 --> 00:35:29,040
수행할 방법을 찾는 것입니다.

895
00:35:29,040 --> 00:35:31,540
질문은 4단계 또는 5단계가 병목 현상이 될 것인가입니다.

896
00:35:31,540 --> 00:35:32,640
답은 예입니다.

897
00:35:32,640 --> 00:35:35,440
이는 전적으로 장치의 속도에 달려 있습니다.

898
00:35:35,440 --> 00:35:36,610
모델의 크기는 얼마나 됩니까?

899
00:35:36,610 --> 00:35:37,780
미니배치 크기는 얼마나 되나요?

900
00:35:37,780 --> 00:35:40,270
장치 간의 인터커넥트 속도는 얼마나 빠른가요?

901
00:35:40,270 --> 00:35:42,520
이 대규모 분산 훈련에 도달하면,

902
00:35:42,520 --> 00:35:44,730
항상 상황에 따라 다르다고 대답합니다.

903
00:35:44,730 --> 00:35:47,220
그리고 당신의 상황에 맞게 벤치마크를 해야 합니다.

904
00:35:47,220 --> 00:35:49,720
그렇다면 각 모델에서 M개의 다른 그래디언트 스텝을 수행하는 것은 어떨까요?

905
00:35:49,720 --> 00:35:51,210
사실 정말 멋진 아이디어입니다.

906
00:35:51,210 --> 00:35:54,330
사실 사람들이 예전에 사용했던 비동기 SGD라는

907
00:35:54,330 --> 00:35:57,767
인기 있는 알고리즘 세트가 있었는데, 그들은

908
00:35:57,767 --> 00:35:59,600
기본적으로 그렇게 하고,

909
00:35:59,600 --> 00:36:01,260
여러 개의 독립적인

910
00:36:01,260 --> 00:36:03,052
모델 스텝을 수행한 다음,

911
00:36:03,052 --> 00:36:05,420
가끔 평균을 내려고 했습니다.

912
00:36:05,420 --> 00:36:07,528
그리고 그것들은 인기가 있었습니다.

913
00:36:07,528 --> 00:36:10,070
사실 구글은 TPU 팟을 개발하기 전에 이 방법을

914
00:36:10,070 --> 00:36:10,970
사용했습니다.

915
00:36:10,970 --> 00:36:13,610
그들의 초기 네트워크 중 일부는 2010년대 초반에

916
00:36:13,610 --> 00:36:14,930
이 방식으로 훈련되었습니다.

917
00:36:14,930 --> 00:36:17,880
하지만 하나, 이는 훨씬 더 불안정한 경향이 있습니다.

918
00:36:17,880 --> 00:36:20,790
그리고 두 번째, 디버깅과 재현이 매우 어렵습니다.

919
00:36:20,790 --> 00:36:23,640
그리고 그저 조금 더 잘 작동하는 경향이 있습니다.

920
00:36:23,640 --> 00:36:26,190
그래서 더 확장 가능한 접근 방식처럼 느껴집니다.

921
00:36:26,190 --> 00:36:28,890
하지만 실제로 모든 것을 동기적으로 수행할 수

922
00:36:28,890 --> 00:36:30,890
있다면, 알고리즘은 디버깅하기 쉽고,

923
00:36:30,890 --> 00:36:32,790
이해하기 쉽고, 추론하기 쉽습니다.

924
00:36:32,790 --> 00:36:34,820
기본적으로 동기 그래디언트 업데이트로

925
00:36:34,820 --> 00:36:36,237
해결할 수 있다면, 아마도

926
00:36:36,237 --> 00:36:37,695
더 잘 작동할 것입니다.

927
00:36:37,695 --> 00:36:39,260
하지만 사실, 개인적으로는

928
00:36:39,260 --> 00:36:42,850
향후 몇 년 내에 비동기 SGD 방법의 부활을 보더라도 놀라지 않을

929
00:36:42,850 --> 00:36:44,600
것 같습니다. 왜냐하면 그것들이

930
00:36:44,600 --> 00:36:47,060
분산 훈련에 훨씬 더 친숙하다고 생각하기

931
00:36:47,060 --> 00:36:47,630
때문입니다.

932
00:36:47,630 --> 00:36:50,130
모든 것을 조정할 수 있는 컴퓨터는 없습니다.

933
00:36:50,130 --> 00:36:51,470
모든 장치는 독립적인 장치로

934
00:36:51,470 --> 00:36:52,845
각자의 독립적인 작업을 수행합니다.

935
00:36:52,845 --> 00:36:56,440
신의 눈으로 모든 단계를 조정할 수 있는

936
00:36:56,440 --> 00:36:58,480
드라이버는 없습니다.

937
00:36:58,480 --> 00:37:00,880
모든 계산은 어딘가에서 이루어져야 합니다.

938
00:37:00,880 --> 00:37:01,700
아, 좋은 질문입니다.

939
00:37:01,700 --> 00:37:04,220
통신과 계산이 겹칠 때, 이를 위해 코드를 작성해야

940
00:37:04,220 --> 00:37:05,140
하나요, 아니면

941
00:37:05,140 --> 00:37:06,890
하드웨어가 자동으로 처리하나요?

942
00:37:06,890 --> 00:37:08,270
확실히 이를 위해 코드를 작성해야 합니다.

943
00:37:08,270 --> 00:37:09,700
하드웨어는 당신이 하고자 하는 일을

944
00:37:09,700 --> 00:37:10,837
이해할 만큼 똑똑하지 않습니다.

945
00:37:10,837 --> 00:37:13,420
하드웨어는 우리가 말했듯이, 이러한 작은 행렬 곱셈

946
00:37:13,420 --> 00:37:14,300
조각을 이해합니다.

947
00:37:14,300 --> 00:37:16,210
상당히 저수준의 작업을 이해합니다.

948
00:37:16,210 --> 00:37:19,375
그 통신을 스케줄링하기 위해 하고자 하는 모든 것은

949
00:37:19,375 --> 00:37:21,230
소프트웨어에서 처리해야 합니다.

950
00:37:21,230 --> 00:37:24,110
하지만 다행히도 이러한 일반적인 사용 사례에 대해

951
00:37:24,110 --> 00:37:25,760
PyTorch가 이를 제공해 줍니다.

952
00:37:25,760 --> 00:37:27,580
예를 들어, 이 경우에는

953
00:37:27,580 --> 00:37:29,890
DistributedDataParallel이라는

954
00:37:29,890 --> 00:37:33,190
PyTorch 클래스가 있어 이를 자동으로 처리하고, 당신이

955
00:37:33,190 --> 00:37:35,230
작성한 간단한 PyTorch 코드

956
00:37:35,230 --> 00:37:37,747
위에서 상대적으로 투명하게 작동하게 합니다.

957
00:37:37,747 --> 00:37:39,580
사실 이는 개별 장치와

958
00:37:39,580 --> 00:37:41,390
대조하는 것이 정말 흥미로운데,

959
00:37:41,390 --> 00:37:45,160
CUDA에서 개별 GPU를 프로그래밍할 경우,

960
00:37:45,160 --> 00:37:47,450
NVIDIA의 GPU 프로그래밍 언어인

961
00:37:47,450 --> 00:37:49,000
CUDA에서는 하드웨어가

962
00:37:49,000 --> 00:37:51,890
많은 비동기 전송을 자동으로 처리합니다.

963
00:37:51,890 --> 00:37:54,075
하지만 클러스터 수준에서는 일반적으로 그렇지 않습니다.

964
00:37:54,075 --> 00:37:55,950
그렇다면 일반적으로 소프트웨어에서 이를 수행해야 합니다.

965
00:37:55,950 --> 00:37:56,960
개별 장치

966
00:37:56,960 --> 00:37:59,335
수준에서의 병렬성과 클러스터

967
00:37:59,335 --> 00:38:01,460
수준에서 소프트웨어로

968
00:38:01,460 --> 00:38:04,100
조정해야 하는 점 사이에

969
00:38:04,100 --> 00:38:06,590
흥미로운 비대칭이 있습니다.

970
00:38:06,590 --> 00:38:09,062
네, 일반적으로 이러한 시스템은 서로 다른 프로그래밍

971
00:38:09,062 --> 00:38:10,520
언어로 작성된 다양한

972
00:38:10,520 --> 00:38:12,150
부분이 있는 이질적인 시스템입니다.

973
00:38:12,150 --> 00:38:13,730
따라서 실제로 GPU 내부에서

974
00:38:13,730 --> 00:38:15,355
실행되는 코드인 저수준

975
00:38:15,355 --> 00:38:16,873
장치 커널이 있을 것입니다.

976
00:38:16,873 --> 00:38:18,540
이 커널은 일반적으로

977
00:38:18,540 --> 00:38:21,020
CUDA로 작성되며, NVIDIA의

978
00:38:21,020 --> 00:38:23,990
GPU 프로그래밍을 위한 C 유사 언어입니다.

979
00:38:23,990 --> 00:38:26,970
하지만 이러한 개별 GPU 커널은 포장됩니다.

980
00:38:26,970 --> 00:38:29,310
그리고 Python에서 이러한 GPU 커널을 호출할 수 있습니다.

981
00:38:29,310 --> 00:38:31,200
이것이 기본적으로 PyTorch의 작동 방식입니다.

982
00:38:31,200 --> 00:38:34,910
PyTorch는 GPU에서 많은 흥미로운 작업을 수행할 수 있는 많은

983
00:38:34,910 --> 00:38:36,660
GPU 커널의 모음과, 이러한

984
00:38:36,660 --> 00:38:39,920
GPU 커널을 감싸고 프로그래밍을 더 사용자 친화적으로 만드는

985
00:38:39,920 --> 00:38:42,560
많은 C++ 및 Python 코드로 구성됩니다.

986
00:38:42,560 --> 00:38:45,680
이 그림에서 각 GPU는 검은색으로 자신의

987
00:38:45,680 --> 00:38:48,205
그래디언트를 독립적으로 계산하고 있습니다.

988
00:38:48,205 --> 00:38:49,580
그리고 빨간색 그래디언트는

989
00:38:49,580 --> 00:38:53,520
모든 GPU에서 병렬로 allreduce를 통해 계산됩니다.

990
00:38:53,520 --> 00:38:55,200
아, 하위 계층의 역전파는

991
00:38:55,200 --> 00:38:57,450
이전 계층의 그래디언트에 의존합니다.

992
00:38:57,450 --> 00:39:02,160
하지만 중요한 것은 각 GPU가 자신의 미니배치에서만 로컬로 역전파를
수행하고

993
00:39:02,160 --> 00:39:03,250
있다는 것입니다.

994
00:39:03,250 --> 00:39:05,250
따라서 이제 각 계층에서 생각해야

995
00:39:05,250 --> 00:39:06,540
할 그래디언트의 두 가지

996
00:39:06,540 --> 00:39:07,560
변형이 있습니다.

997
00:39:07,560 --> 00:39:09,990
로컬 그래디언트는 내 미니배치의 로컬

998
00:39:09,990 --> 00:39:12,240
손실에 대한 네트워크 가중치의

999
00:39:12,240 --> 00:39:13,207
그래디언트입니다.

1000
00:39:13,207 --> 00:39:14,790
그리고 글로벌

1001
00:39:14,790 --> 00:39:16,785
그래디언트는 네트워크 가중치에

1002
00:39:16,785 --> 00:39:20,790
대한 매크로 배치의 총 손실의 도함수입니다.

1003
00:39:20,790 --> 00:39:22,300
역전파를 계산하기 위해 각

1004
00:39:22,300 --> 00:39:24,030
GPU는 자신의 업스트림 그래디언트의

1005
00:39:24,030 --> 00:39:25,120
로컬 버전만 필요합니다.

1006
00:39:25,120 --> 00:39:27,930
하지만 업스트림 그래디언트의 글로벌

1007
00:39:27,930 --> 00:39:31,530
버전을 계산하려면 통신이 필요합니다.

1008
00:39:31,530 --> 00:39:33,280
이것이 데이터 병렬성입니다.

1009
00:39:33,280 --> 00:39:35,100
그리고 여기에는 약간의 문제가

1010
00:39:35,100 --> 00:39:36,600
있는데, 이는 GPU 계산을

1011
00:39:36,600 --> 00:39:38,107
병렬화하는 훌륭한 방법입니다.

1012
00:39:38,107 --> 00:39:39,690
그리고 이것은 사람들이 신경망

1013
00:39:39,690 --> 00:39:41,982
훈련에서 GPU 계산을 병렬화하기 시작한 첫 번째

1014
00:39:41,982 --> 00:39:42,550
방법이었습니다.

1015
00:39:42,550 --> 00:39:46,000
하지만 우리는 모델 크기에서 병목 현상에 빠르게 직면했습니다.

1016
00:39:46,000 --> 00:39:48,660
따라서 여기서 각 GPU는 모델 매개변수의

1017
00:39:48,660 --> 00:39:50,880
독립적인 복사본을 유지하고 있으며, 이는

1018
00:39:50,880 --> 00:39:52,160
정말 큰 모델을 원할

1019
00:39:52,160 --> 00:39:53,490
때 병목 현상이 됩니다.

1020
00:39:53,490 --> 00:39:56,257
특히 이제 신경망의 각 가중치는 기본적으로

1021
00:39:56,257 --> 00:39:58,340
네 개의 숫자를 추적해야 합니다.

1022
00:39:58,340 --> 00:40:02,240
- 가중치 자체, 해당 가중치의 그래디언트, 그리고

1023
00:40:02,240 --> 00:40:03,453
옵티마이저 상태입니다.

1024
00:40:03,453 --> 00:40:05,120
따라서 Adam을 사용하는 경우,

1025
00:40:05,120 --> 00:40:07,760
이는 일반적으로 네트워크의 각 매개변수에 대해 베타 1과

1026
00:40:07,760 --> 00:40:08,560
베타 2입니다.

1027
00:40:08,560 --> 00:40:11,060
때때로 모델 매개변수의 지수 이동

1028
00:40:11,060 --> 00:40:12,600
평균도 있을 수 있습니다.

1029
00:40:12,600 --> 00:40:15,162
따라서 일반적으로 네트워크의 각 가중치에 대해

1030
00:40:15,162 --> 00:40:17,120
추적해야 할 스칼라가 네 개에서

1031
00:40:17,120 --> 00:40:18,320
다섯 개가 필요합니다.

1032
00:40:18,320 --> 00:40:20,790
그리고 요즘 일반적인 16비트 정밀도로

1033
00:40:20,790 --> 00:40:22,867
훈련하는 경우, 일부는

1034
00:40:22,867 --> 00:40:25,200
때때로 더 높은 정밀도로 유지합니다.

1035
00:40:25,200 --> 00:40:27,150
하지만 16비트를 하한으로 이야기해 봅시다.

1036
00:40:27,150 --> 00:40:29,340
그렇다면 각 숫자에 대해 두 바이트가 필요합니다.

1037
00:40:29,340 --> 00:40:31,520
즉, 네 개의 숫자,

1038
00:40:31,520 --> 00:40:34,670
네트워크의 각 스칼라에 대해 2바이트가

1039
00:40:34,670 --> 00:40:37,350
필요하므로 8바이트가 필요합니다.

1040
00:40:37,350 --> 00:40:40,910
이는 10억 개의 모델 매개변수가 모든 것을 저장하는 데

1041
00:40:40,910 --> 00:40:43,310
약 8기가바이트의 GPU 메모리가

1042
00:40:43,310 --> 00:40:44,730
필요하다는 것을 의미합니다.

1043
00:40:44,730 --> 00:40:46,820
그리고 우리는 전체 GPU가 H100에 대해 80기가바이트의

1044
00:40:46,820 --> 00:40:48,150
메모리만 가지고 있다고 말했습니다.

1045
00:40:48,150 --> 00:40:50,275
즉, 이 시나리오에서 훈련할 수 있는

1046
00:40:50,275 --> 00:40:52,090
가장 큰 모델은 약 100억

1047
00:40:52,090 --> 00:40:53,690
개의 매개변수와 같은 것입니다.

1048
00:40:53,690 --> 00:40:54,830
그것은 충분히 크지 않습니다.

1049
00:40:54,830 --> 00:40:56,000
우리는 정말 큰 모델을 원합니다.

1050
00:40:56,000 --> 00:40:58,667
우리는 훈련할 수 있는 모델의 크기를 결정하는 GPU

1051
00:40:58,667 --> 00:41:01,490
메모리 크기의 압박에 의해 제한받고 싶지 않습니다.

1052
00:41:01,490 --> 00:41:03,580
그래서 우리는 이 문제를 어떻게든 해결해야 합니다.

1053
00:41:03,580 --> 00:41:06,423
이 문제의 해결책은 사실 상대적으로 쉽습니다.

1054
00:41:06,423 --> 00:41:07,840
모델 가중치를 서로 다른

1055
00:41:07,840 --> 00:41:09,410
GPU에 분산시켜야 합니다.

1056
00:41:09,410 --> 00:41:12,410
데이터 배치를 GPU에 분산시키는 것 외에도,

1057
00:41:12,410 --> 00:41:15,560
모델 가중치도 GPU에 분산시킬 것입니다.

1058
00:41:15,560 --> 00:41:17,890
이것은 완전 분산 데이터 병렬 처리(Fully Sharded

1059
00:41:17,890 --> 00:41:20,950
Data Parallelism, FSDP)라는 데이터 병렬 처리의 변형으로
이어집니다.

1060
00:41:20,950 --> 00:41:23,362
이것은 상대적으로 간단합니다.

1061
00:41:23,362 --> 00:41:24,820
개념적으로 우리가 할

1062
00:41:24,820 --> 00:41:27,610
것은 네트워크의 각 모델 가중치,

1063
00:41:27,610 --> 00:41:31,460
각 가중치 Wi를 소유할 GPU에 할당하는 것입니다.

1064
00:41:31,460 --> 00:41:35,020
따라서 각 가중치는 우리가 훈련하는 M개의 GPU 중에서 고유한 GPU에

1065
00:41:35,020 --> 00:41:36,040
의해 소유됩니다.

1066
00:41:36,040 --> 00:41:38,560
각 가중치를 소유하는 GPU는

1067
00:41:38,560 --> 00:41:40,780
해당 가중치의 전역

1068
00:41:40,780 --> 00:41:42,350
그래디언트와 옵티마이저

1069
00:41:42,350 --> 00:41:44,802
상태를 관리할 책임이 있습니다.

1070
00:41:44,802 --> 00:41:46,760
일반적으로 이것은 레이어별로 분할합니다.

1071
00:41:46,760 --> 00:41:48,510
개별 스칼라를 관리하는 것이 아닙니다.

1072
00:41:48,510 --> 00:41:50,873
이 W는 신경망의 전체 레이어에 대한

1073
00:41:50,873 --> 00:41:52,915
가중치 행렬처럼 생각해야 합니다.

1074
00:41:55,820 --> 00:41:58,230
그래서 이제 오른쪽 그림이 조금 바뀝니다.

1075
00:41:58,230 --> 00:42:00,647
여기서는 두 개의 GPU만 보여주고 있습니다.

1076
00:42:00,647 --> 00:42:02,647
스포일러, 곧 더 많은 화살표가 여기서

1077
00:42:02,647 --> 00:42:03,460
날아다닐 것입니다.

1078
00:42:03,460 --> 00:42:05,210
여기서는 두 개의 서로 다른 GPU에

1079
00:42:05,210 --> 00:42:07,560
분산된 네 개의 레이어 네트워크를 보여주고 있습니다.

1080
00:42:07,560 --> 00:42:11,360
첫 번째 두 개의 네트워크 레이어 W1과 W2에 대한

1081
00:42:11,360 --> 00:42:12,840
가중치를 할당했습니다.

1082
00:42:12,840 --> 00:42:14,690
이들은 GPU 1이 소유하고 있습니다.

1083
00:42:14,690 --> 00:42:18,660
가중치 W3와 W4는 GPU 3 또는 GPU 2가 소유하고 있습니다.

1084
00:42:18,660 --> 00:42:21,780
즉, 각 배치의 시작 시 네트워크

1085
00:42:21,780 --> 00:42:25,080
가중치는 이렇게 GPU에 분산됩니다.

1086
00:42:25,080 --> 00:42:26,820
하지만 여전히 데이터 병렬성입니다.

1087
00:42:26,820 --> 00:42:28,968
각 GPU는 자신의 독립적인 요소 배치를 로드하고,

1088
00:42:28,968 --> 00:42:31,260
그 배치에 대해 전체 전방 및 후방 패스를

1089
00:42:31,260 --> 00:42:33,470
수행하여 자신의 로컬 그래디언트를 계산한 다음,

1090
00:42:33,470 --> 00:42:35,010
모든 그래디언트를 축소하고 그래디언트

1091
00:42:35,010 --> 00:42:37,320
단계를 수행하는 기본 아이디어는 동일합니다.

1092
00:42:37,320 --> 00:42:39,320
기본 알고리즘은 동일하지만, 모델 가중치가

1093
00:42:39,320 --> 00:42:41,130
분할되어 있기 때문에 이제 복잡해집니다.

1094
00:42:41,130 --> 00:42:43,565
그래서 여기서는 추가 통신을 도입해야 합니다.

1095
00:42:43,565 --> 00:42:45,190
완전히 분산된 데이터

1096
00:42:45,190 --> 00:42:48,230
병렬성을 수행할 때, 이제 전방 패스를

1097
00:42:48,230 --> 00:42:51,580
시작하기 전에 첫 번째 레이어의 가중치를 소유한

1098
00:42:51,580 --> 00:42:54,100
사람이 그 가중치 행렬을 훈련

1099
00:42:54,100 --> 00:42:56,590
중인 다른 모든 GPU에 브로드캐스트해야

1100
00:42:56,590 --> 00:42:57,590
합니다.

1101
00:42:57,590 --> 00:43:02,810
이 경우 GPU 1이 W1을 소유하므로, 이를 GPU 2에
브로드캐스트합니다.

1102
00:43:02,810 --> 00:43:05,240
그래서 GPU 2는 이제 W1의 복사본을 가집니다.

1103
00:43:05,240 --> 00:43:07,575
이제 모든 GPU가 W1의 복사본을 가지므로,

1104
00:43:07,575 --> 00:43:09,700
네트워크의 첫 번째 레이어를 통해 전방

1105
00:43:09,700 --> 00:43:11,450
패스를 실행하고 첫 번째 레이어의

1106
00:43:11,450 --> 00:43:13,120
활성화를 계산할 수 있습니다.

1107
00:43:13,120 --> 00:43:15,880
이제 순전파를 실행한 후, W1을

1108
00:43:15,880 --> 00:43:18,580
소유하지 않은 각 GPU는 메모리를 절약하기

1109
00:43:18,580 --> 00:43:21,400
위해 W1 가중치 행렬의 로컬 복사본을

1110
00:43:21,400 --> 00:43:22,430
삭제합니다.

1111
00:43:22,430 --> 00:43:25,592
따라서 첫 번째 레이어에 대한 순전파를 실행한

1112
00:43:25,592 --> 00:43:27,550
후, 모델 가중치가 GPU에

1113
00:43:27,550 --> 00:43:29,150
분산된 상태로 돌아옵니다.

1114
00:43:29,150 --> 00:43:33,050
하지만 이제 모든 GPU는 네트워크의 첫 번째 레이어를 실행한

1115
00:43:33,050 --> 00:43:35,780
결과로 GPU 메모리에 활성화가 있습니다.

1116
00:43:35,780 --> 00:43:37,960
이제 두 번째 레이어를 수행할 시간이며,

1117
00:43:37,960 --> 00:43:39,290
동일한 작업을 수행합니다.

1118
00:43:39,290 --> 00:43:42,730
따라서 레이어 2의 가중치 행렬을 소유한 GPU는 이를

1119
00:43:42,730 --> 00:43:44,622
훈련 중인 모든 GPU에

1120
00:43:44,622 --> 00:43:45,580
브로드캐스트합니다.

1121
00:43:45,580 --> 00:43:48,820
이제 모든 GPU는 W2의 로컬 복사본을 가지고 있으며,

1122
00:43:48,820 --> 00:43:50,710
앞으로 진행할 수 있습니다.

1123
00:43:50,710 --> 00:43:52,630
참고로, 여기서 계산과 통신을

1124
00:43:52,630 --> 00:43:55,210
교차하여 수행할 기회도 있으므로,

1125
00:43:55,210 --> 00:43:57,130
레이어 i의 순전파를 계산하는

1126
00:43:57,130 --> 00:43:59,830
동안 다음 레이어의 가중치를 미리

1127
00:43:59,830 --> 00:44:01,220
가져올 수 있습니다.

1128
00:44:01,220 --> 00:44:03,430
실제로, 이는 FSDP 실행의

1129
00:44:03,430 --> 00:44:06,020
순전파 동안 병렬로 발생합니다.

1130
00:44:06,020 --> 00:44:08,775
따라서 레이어 2를 계산할 것입니다.

1131
00:44:08,775 --> 00:44:10,150
동시에 레이어 3의

1132
00:44:10,150 --> 00:44:11,590
가중치를 가져오고 있습니다.

1133
00:44:11,590 --> 00:44:15,800
레이어 3에 도달하면, 이제 GPU 1이 레이어 3을 소유하고 있음을
주목하십시오.

1134
00:44:15,800 --> 00:44:18,130
따라서 GPU 1은 훈련 중인 모든 GPU에

1135
00:44:18,130 --> 00:44:19,850
가중치를 브로드캐스트합니다.

1136
00:44:19,850 --> 00:44:21,280
이 과정은 네트워크의 끝에 도달할

1137
00:44:21,280 --> 00:44:22,363
때까지 반복됩니다.

1138
00:44:22,363 --> 00:44:25,180
네트워크의 끝에서 모든 모델은 이제 각 모델이

1139
00:44:25,180 --> 00:44:27,500
전체 순전파를 수행하고, 자신의 로컬 배치에

1140
00:44:27,500 --> 00:44:29,893
대한 로컬 손실을 계산했으며, 모든

1141
00:44:29,893 --> 00:44:32,560
레이어의 활성화를 메모리에 이미 가지고 있음을

1142
00:44:32,560 --> 00:44:33,347
알게 됩니다.

1143
00:44:33,347 --> 00:44:35,680
이제 역방향 전파를 계산하기 위해 동일한 작업을

1144
00:44:35,680 --> 00:44:36,950
역으로 수행해야 합니다.

1145
00:44:36,950 --> 00:44:39,490
마지막 레이어의 역방향 전파 시작 시,

1146
00:44:39,490 --> 00:44:42,600
마지막 레이어 가중치를 소유한 사람은 이를

1147
00:44:42,600 --> 00:44:44,770
모든 장치에 브로드캐스트합니다.

1148
00:44:44,770 --> 00:44:46,720
장치가 그 가중치를 받으면,

1149
00:44:46,720 --> 00:44:49,000
역방향 전파를 수행할 수 있습니다.

1150
00:44:49,000 --> 00:44:52,030
이 전체 과정은 역방향 전파에서도 유사한 절차를 수행합니다.

1151
00:44:52,030 --> 00:44:53,970
이제 네트워크의 마지막 레이어에서

1152
00:44:53,970 --> 00:44:56,580
최적화할 수 있는 작은 부분이 있는데,

1153
00:44:56,580 --> 00:45:00,510
모든 GPU가 마지막 레이어의 가중치를 메모리에 유지하도록

1154
00:45:00,510 --> 00:45:01,870
하는 것입니다.

1155
00:45:01,870 --> 00:45:04,203
이는 일반적으로 실제로 수행하는 작업입니다.

1156
00:45:04,203 --> 00:45:06,330
왜냐하면 마지막에 모든 GPU가 이미 순전파에서

1157
00:45:06,330 --> 00:45:08,830
마지막 레이어 가중치의 복사본을 가지고 있기 때문입니다.

1158
00:45:08,830 --> 00:45:09,940
그들은 단순히 메모리에 유지할 것입니다.

1159
00:45:09,940 --> 00:45:11,340
그들이 어차피 역방향 전파를

1160
00:45:11,340 --> 00:45:13,298
위해 재사용할 것이라는 것을 알기 때문에,

1161
00:45:13,298 --> 00:45:15,000
우리는 마지막 레이어의 가중치를

1162
00:45:15,000 --> 00:45:16,085
삭제하지 않을 것입니다.

1163
00:45:18,247 --> 00:45:19,830
하지만 이제 역방향 전파 동안

1164
00:45:19,830 --> 00:45:22,440
발생해야 할 기본적으로 세 가지 일이 있습니다.

1165
00:45:22,440 --> 00:45:25,080
하나는 GPU가 네트워크의 마지막

1166
00:45:25,080 --> 00:45:27,690
레이어에 대한 역방향 전파를 계산한

1167
00:45:27,690 --> 00:45:30,190
후, 이제 가중치의 복사본을 가지고

1168
00:45:30,190 --> 00:45:34,380
있으므로, 각 GPU는 마지막 레이어 가중치에 대한

1169
00:45:34,380 --> 00:45:38,100
로컬 손실에 대한 로컬 그래디언트를 계산했습니다.

1170
00:45:38,100 --> 00:45:41,810
그런 다음 우리는 그 그래디언트를 다시 통신해야 합니다.

1171
00:45:41,810 --> 00:45:44,590
그리고 우리는 가중치 행렬을 소유한 GPU가

1172
00:45:44,590 --> 00:45:47,200
해당 가중치 행렬의 그래디언트를 관리할 책임이

1173
00:45:47,200 --> 00:45:48,290
있다고 말했습니다.

1174
00:45:48,290 --> 00:45:50,830
따라서 이제 데이터 병렬 처리

1175
00:45:50,830 --> 00:45:52,790
사례에서 했던 것처럼 모든

1176
00:45:52,790 --> 00:45:53,890
그래디언트를 줄이는

1177
00:45:56,920 --> 00:46:00,100
대신, 마지막 레이어 가중치를 소유한 하나의

1178
00:46:00,100 --> 00:46:03,670
GPU만이 모든 장치의 로컬 그래디언트를 모으고

1179
00:46:03,670 --> 00:46:05,030
합산할 것입니다.

1180
00:46:05,030 --> 00:46:08,020
이 경우, GPU 1은 마지막 레이어의

1181
00:46:08,020 --> 00:46:11,770
로컬 그래디언트를 GPU 2로 보내고, GPU

1182
00:46:11,770 --> 00:46:16,720
2는 마지막 레이어 가중치에 대한 전체 매크로 배치의 그래디언트

1183
00:46:16,720 --> 00:46:18,925
dL/dW 4를 가지게 됩니다.

1184
00:46:18,925 --> 00:46:20,300
다운타임 동안 무슨 일이 발생합니까?

1185
00:46:20,300 --> 00:46:23,748
모든 작업이 병렬로 발생해야 합니다.

1186
00:46:23,748 --> 00:46:25,540
따라서 역방향 전파 동안 기본적으로 세

1187
00:46:25,540 --> 00:46:26,832
가지 일이 발생해야 합니다.

1188
00:46:26,832 --> 00:46:29,480
역전파 동안 우리는 가중치를 전달해야 합니다.

1189
00:46:29,480 --> 00:46:33,940
따라서 해당 레이어를 소유한 GPU는 그 레이어의 가중치를

1190
00:46:33,940 --> 00:46:35,210
방송해야 합니다.

1191
00:46:35,210 --> 00:46:38,150
둘째, 모든 GPU는 그 가중치를 받으면 해당

1192
00:46:38,150 --> 00:46:40,450
레이어에 대한 역전파를 계산해야 합니다.

1193
00:46:40,450 --> 00:46:44,500
셋째, 각 GPU가 역전파를 계산한 후, 그

1194
00:46:44,500 --> 00:46:47,460
결과를 해당 가중치에 대한

1195
00:46:47,460 --> 00:46:49,200
그래디언트로 소유

1196
00:46:49,200 --> 00:46:51,670
GPU에 다시 보내야 합니다.

1197
00:46:51,670 --> 00:46:55,830
그 후, 가중치의 소유자가 전체 그래디언트를 가지게

1198
00:46:55,830 --> 00:46:58,140
되면, 가중치 행렬의 소유자만이

1199
00:46:58,140 --> 00:47:00,720
그 행렬에 대한 그래디언트

1200
00:47:00,720 --> 00:47:02,820
업데이트를 할 수 있습니다.

1201
00:47:02,820 --> 00:47:04,710
하지만 이 시점에서는 업데이트된

1202
00:47:04,710 --> 00:47:08,220
가중치 행렬을 전달할 필요가 없다고 생각합니다. 다음

1203
00:47:08,220 --> 00:47:10,080
전방 패스에서 모든 GPU에

1204
00:47:10,080 --> 00:47:11,550
다시 전달될 것입니다.

1205
00:47:11,550 --> 00:47:15,930
그래서 이는 DP 사례와는 조금 다를 수 있습니다.

1206
00:47:15,930 --> 00:47:17,790
그리고 기본적으로 이러한 모든 일들은

1207
00:47:17,790 --> 00:47:19,720
실제로 병렬로 발생할 수 있습니다.

1208
00:47:19,720 --> 00:47:22,270
따라서 우리는 네트워크의 모든 레이어에 대해 이 과정을 반복할 것입니다.

1209
00:47:22,270 --> 00:47:25,150
그리고 기본적으로 매우 깊은 네트워크의 정상

1210
00:47:25,150 --> 00:47:28,245
상태에서 이 세 가지 일이 동시에 발생할 것입니다.

1211
00:47:28,245 --> 00:47:32,050
우리가 레이어 L에 대한 역전파를 계산하는 동안,

1212
00:47:32,050 --> 00:47:34,440
우리는 그래디언트를 집계하고 레이어

1213
00:47:34,440 --> 00:47:38,230
L+1에 대한 가중치 업데이트를 수행할 것입니다.

1214
00:47:38,230 --> 00:47:42,085
그리고 우리는 레이어 L-1에 대한 가중치를 미리 가져올 것입니다.

1215
00:47:42,085 --> 00:47:44,210
그래서 세 가지 일이 발생해야 한다고 말했습니다.

1216
00:47:44,210 --> 00:47:48,938
우리는 가중치를 가져오고, 역전파를 실행하고, 그래디언트를 집계하고,

1217
00:47:48,938 --> 00:47:51,230
가중치를 업데이트해야 합니다.

1218
00:47:51,230 --> 00:47:53,000
이 모든 일들은 병렬로 발생할 수 있습니다.

1219
00:47:53,000 --> 00:47:54,010
따라서 일반적으로

1220
00:47:54,010 --> 00:47:56,740
우리는 세 개의 연속적인 레이어에서 작업하고, 역전파

1221
00:47:56,740 --> 00:47:59,460
과정 동안 이 세 가지 일을 병렬로 수행할 것입니다.

1222
00:48:03,880 --> 00:48:06,410
그리고 네트워크를 거슬러

1223
00:48:06,410 --> 00:48:09,130
올라가면서, 모든 통신과 계산을 적절히

1224
00:48:09,130 --> 00:48:11,960
겹치게 할 수 있다면, 역전파가

1225
00:48:11,960 --> 00:48:14,540
끝날 때쯤 모든 그래디언트가

1226
00:48:14,540 --> 00:48:16,790
이미 전달되었을 것입니다.

1227
00:48:16,790 --> 00:48:18,280
모든 GPU는 이미 모든 가중치에

1228
00:48:18,280 --> 00:48:20,690
대한 업데이트를 완료했으며, 우리는 준비가 되어 있습니다.

1229
00:48:20,690 --> 00:48:23,410
또한 데이터를 로드하는 데이터 로더가 비동기적으로

1230
00:48:23,410 --> 00:48:25,540
작동하고 있기를 바랍니다. 보통 서버의

1231
00:48:25,540 --> 00:48:27,320
CPU 코어에서 작동합니다.

1232
00:48:27,320 --> 00:48:29,980
그래서 CPU는 다시 전방으로 나아갈 수 있는 새로운 데이터

1233
00:48:29,980 --> 00:48:31,070
배치로 준비가 되어 있습니다.

1234
00:48:31,070 --> 00:48:33,405
이러한 것들은 기본적으로 병렬화 기계입니다.

1235
00:48:33,405 --> 00:48:34,780
GPU 내와 GPU

1236
00:48:34,780 --> 00:48:37,928
간에 발생해야 할 많은 작업이 있으며, 우리는 가능한

1237
00:48:37,928 --> 00:48:40,220
한 모든 것을 겹치게 해야 합니다.

1238
00:48:40,220 --> 00:48:41,920
그래서 우리는 항상 GPU에 데이터를

1239
00:48:41,920 --> 00:48:44,460
공급하고 텐서 코어에서 최대한 밀집하게 실행할 수 있습니다.

1240
00:48:47,305 --> 00:48:51,550
그래서 우리는 그 후 다음 배치를 수행할 준비가 되어 있습니다.

1241
00:48:51,550 --> 00:48:52,700
그래서 이것은 훌륭합니다.

1242
00:48:52,700 --> 00:48:54,440
이것은 완전 분산 데이터 병렬 처리입니다.

1243
00:48:54,440 --> 00:48:56,420
이것은 당신을 멀리 데려갈 수 있습니다.

1244
00:48:56,420 --> 00:48:59,322
하지만 사람들이 때때로 사용하는 조금 더 세련된

1245
00:48:59,322 --> 00:49:01,030
데이터 병렬 처리 변형이

1246
00:49:01,030 --> 00:49:05,650
있습니다. 그것은 하이브리드 분산 데이터 병렬 처리(HSDP)입니다.

1247
00:49:05,650 --> 00:49:07,270
이 경우, 우리는 개념적으로

1248
00:49:07,270 --> 00:49:09,700
GPU를 2차원 그리드로 나누는

1249
00:49:09,700 --> 00:49:11,210
것을 상상할 것입니다.

1250
00:49:11,210 --> 00:49:14,690
이전 예제에서는 N개의 GPU가 있다고 말했습니다.

1251
00:49:14,690 --> 00:49:18,170
그리고 우리의 계산을 병렬화하는 방법은 동일했습니다.

1252
00:49:18,170 --> 00:49:20,890
이전 데이터 병렬 처리 변형에서는

1253
00:49:20,890 --> 00:49:23,090
하나의 병렬화 축이 있었습니다.

1254
00:49:23,090 --> 00:49:25,280
하이브리드 샤드 데이터 병렬 처리에

1255
00:49:25,280 --> 00:49:28,090
도달하면, 이제 동시에 수행할 두 개의

1256
00:49:28,090 --> 00:49:29,870
개별 병렬 축이 생깁니다.

1257
00:49:29,870 --> 00:49:34,090
첫 번째 축은 우리가 방금 이야기한 대로 전형적인 FSDP, 완전

1258
00:49:34,090 --> 00:49:36,700
샤드 데이터 병렬 처리를 수행하는 것입니다.

1259
00:49:36,700 --> 00:49:40,000
그래서 K개의 GPU 그룹이 있을 것입니다.

1260
00:49:40,000 --> 00:49:43,587
각 K GPU 그룹은 우리가 방금 이야기한 완전 샤드 데이터

1261
00:49:43,587 --> 00:49:45,170
병렬 처리를 수행할 것입니다.

1262
00:49:45,170 --> 00:49:47,920
각 K GPU 그룹 내에서 모델 가중치는

1263
00:49:47,920 --> 00:49:50,010
해당 K GPU에 걸쳐 분할됩니다.

1264
00:49:50,010 --> 00:49:52,510
그들은 서로 간에 가중치와 그래디언트를

1265
00:49:52,510 --> 00:49:54,040
주고받으며 전방 및 후방

1266
00:49:54,040 --> 00:49:55,250
패스를 수행합니다.

1267
00:49:55,250 --> 00:50:00,490
하지만 이제 M개의 K 그룹이 병렬로 작동하게

1268
00:50:00,490 --> 00:50:01,700
됩니다.

1269
00:50:01,700 --> 00:50:05,240
이 경우, 우리는 4개의 GPU 그룹이 두 개 있습니다.

1270
00:50:05,240 --> 00:50:07,510
각 4개의 GPU 그룹은 가중치가 4개의

1271
00:50:07,510 --> 00:50:10,820
GPU에 걸쳐 분할되어 있는 것을 볼 수 있습니다.

1272
00:50:10,820 --> 00:50:15,550
하지만 두 번째 2개의 GPU 그룹에 대해 전체

1273
00:50:15,550 --> 00:50:19,060
설정이 두 번째로 복제되었습니다.

1274
00:50:19,060 --> 00:50:21,610
그리고 이렇게 하면 이제 그룹 간에

1275
00:50:21,610 --> 00:50:23,960
전형적인 데이터 병렬 처리를 수행합니다.

1276
00:50:23,960 --> 00:50:26,680
그래서 그룹 내에서는 전방/후방을 수행할 것입니다.

1277
00:50:26,680 --> 00:50:28,690
후방 패스가 끝나면 각 그룹은

1278
00:50:28,690 --> 00:50:31,320
자신의 로컬 그래디언트를 계산하게 됩니다.

1279
00:50:31,320 --> 00:50:33,540
그리고 후방 패스가 끝난 후, 각

1280
00:50:33,540 --> 00:50:36,570
그룹은 그룹 간에 그래디언트를 모두 줄여야 하므로

1281
00:50:36,570 --> 00:50:40,110
이제 두 그룹 간에 전체 매크로 매크로 배치 그래디언트를

1282
00:50:40,110 --> 00:50:41,290
갖게 됩니다.

1283
00:50:41,290 --> 00:50:44,310
그런 다음 각 그룹은 매크로 매크로 배치에 대한

1284
00:50:44,310 --> 00:50:46,590
전체 그래디언트를 받은 후 독립적으로

1285
00:50:46,590 --> 00:50:49,440
그래디언트 업데이트를 수행할 수 있습니다.

1286
00:50:49,440 --> 00:50:51,570
이것은 다차원 병렬 처리라고 불리며,

1287
00:50:51,570 --> 00:50:53,873
이제 기본적으로 두 개의 다른 축,

1288
00:50:53,873 --> 00:50:56,040
두 개의 다른 전략을 사용하여

1289
00:50:56,040 --> 00:50:58,200
동시에 계산을 병렬화하고 있습니다.

1290
00:50:58,200 --> 00:51:01,103
이것이 유용할 수 있는 이유는 이러한 두

1291
00:51:01,103 --> 00:51:02,520
가지 종류의 병렬 처리에

1292
00:51:02,520 --> 00:51:04,870
필요한 통신량이 다르기 때문입니다.

1293
00:51:04,870 --> 00:51:07,398
완전 샤드 병렬 처리에 대해 생각해보면, 우리는 실제로

1294
00:51:07,398 --> 00:51:08,940
-- 완전 샤드 데이터 병렬

1295
00:51:08,940 --> 00:51:11,250
처리 중에 무엇을 통신해야 하는지 알아야 합니다.

1296
00:51:11,250 --> 00:51:13,480
FSDP 동안, 전방 패스에서는

1297
00:51:13,480 --> 00:51:16,433
가중치를 복사하고 있었습니다.

1298
00:51:16,433 --> 00:51:17,850
전방 패스 동안, 우리는

1299
00:51:17,850 --> 00:51:20,058
네트워크 가중치의 전체 복사본을 통신하게

1300
00:51:20,058 --> 00:51:20,590
됩니다.

1301
00:51:20,590 --> 00:51:22,140
그런 다음 후방 패스 동안,

1302
00:51:22,140 --> 00:51:24,103
우리는 네트워크 가중치를 다시 통신해야

1303
00:51:24,103 --> 00:51:26,020
하며, 그래디언트도 통신해야 합니다.

1304
00:51:26,020 --> 00:51:29,960
기본적으로 단일 전방 후방 패스 동안 완전 샤드 데이터 병렬

1305
00:51:29,960 --> 00:51:31,700
처리를 사용할 때, FSDP

1306
00:51:31,700 --> 00:51:33,250
그룹에 참여하는 모든

1307
00:51:33,250 --> 00:51:35,410
것 간에 네트워크 가중치를 세 번

1308
00:51:35,410 --> 00:51:36,650
통신해야 합니다.

1309
00:51:36,650 --> 00:51:40,335
하지만 각 그룹이 가중치의 독립적인 복사본을 유지하는 일반

1310
00:51:40,335 --> 00:51:42,710
데이터 병렬 처리를 수행할 때는

1311
00:51:42,710 --> 00:51:44,460
그래디언트만 모두 줄이면 됩니다.

1312
00:51:44,460 --> 00:51:47,110
즉, 여러 데이터 병렬 처리 그룹 간에는

1313
00:51:47,110 --> 00:51:49,120
전방 후방 패스 동안 네트워크

1314
00:51:49,120 --> 00:51:51,560
가중치를 한 번만 통신하면 됩니다.

1315
00:51:51,560 --> 00:51:54,520
이것은 GPU 클러스터 내에서 여러 수준의 계층 구조라는

1316
00:51:54,520 --> 00:51:56,060
아이디어와 관련이 있습니다.

1317
00:51:56,060 --> 00:51:57,890
예를 들어, 단일 머신 내에서

1318
00:51:57,890 --> 00:52:00,460
더 높은 상호 연결을 가진 8개의

1319
00:52:00,460 --> 00:52:02,140
GPU가 있는 GPU 서버를

1320
00:52:02,140 --> 00:52:03,440
가질 수 있습니다.

1321
00:52:03,440 --> 00:52:05,470
이들은 FSDP 그룹일 수 있습니다. 왜냐하면

1322
00:52:05,470 --> 00:52:07,943
FSDP 그룹 내에서 더 많은 통신이 필요하기 때문입니다.

1323
00:52:07,943 --> 00:52:09,610
하지만 그런 다음 이 다른

1324
00:52:09,610 --> 00:52:11,870
축에 있는 여러 서버를 가질 수 있습니다.

1325
00:52:11,870 --> 00:52:14,483
그래서 하나의 서버에는 모델 가중치의 전체 복사본이

1326
00:52:14,483 --> 00:52:17,150
있고, 다른 서버에는 또 다른 전체 복사본이 있습니다.

1327
00:52:17,150 --> 00:52:18,900
서버 간의 통신은 서버 내의

1328
00:52:18,900 --> 00:52:21,380
통신보다 느릴 것이라는 점을 기억하세요.

1329
00:52:21,380 --> 00:52:26,260
그래서 이것은 우리가 알고 있는 장치가 연결된 네트워크 토폴로지를

1330
00:52:26,260 --> 00:52:28,370
활용하기 위해 알고리즘을

1331
00:52:28,370 --> 00:52:30,680
설계하는 첫 번째 예입니다.

1332
00:52:30,680 --> 00:52:33,335
질문은, 당신은 -- 이러한 것들은

1333
00:52:33,335 --> 00:52:35,188
조정할 수 없다는 것입니다.

1334
00:52:35,188 --> 00:52:36,355
말하기 매우, 매우 어렵습니다.

1335
00:52:40,502 --> 00:52:43,080
기본적으로 데이터 병렬성이 있으면, DP,

1336
00:52:43,080 --> 00:52:45,720
FSDP, HSDP가 있으면, 이는

1337
00:52:45,720 --> 00:52:48,060
실제로 많은 도움이 되는 레시피입니다.

1338
00:52:48,060 --> 00:52:51,860
예를 들어, 1000억 개의 매개변수를 가진 모델은

1339
00:52:51,860 --> 00:52:54,860
800GB의 메모리를 필요로 합니다.

1340
00:52:54,860 --> 00:52:56,970
그리고 이를 80개의 GPU에 나누면,

1341
00:52:56,970 --> 00:52:59,070
GPU당 10GB의 메모리만 필요합니다.

1342
00:52:59,070 --> 00:53:03,560
FSDP가 있으면 꽤 큰 모델을 가질 수 있습니다.

1343
00:53:03,560 --> 00:53:06,320
하지만 모델의 활성화 자체가 이제 메모리를 채우기 시작하는

1344
00:53:06,320 --> 00:53:07,590
또 다른 문제가 있습니다.

1345
00:53:07,590 --> 00:53:11,510
Llama3-405B로 돌아가면, 이는 126개의 레이어,

1346
00:53:11,510 --> 00:53:15,740
모델 차원 16,000, 시퀀스 길이 4,096인 변환기입니다.

1347
00:53:15,740 --> 00:53:18,050
순전파 동안 숨겨진 상태를 저장하는 데

1348
00:53:18,050 --> 00:53:19,910
GPU 메모리가 얼마나

1349
00:53:19,910 --> 00:53:22,560
필요한지 상상해 보세요. 그건 상당할 것입니다.

1350
00:53:22,560 --> 00:53:24,577
모델과 시퀀스가 정말 커지면

1351
00:53:24,577 --> 00:53:27,160
GPU의 메모리가 금방 부족해질

1352
00:53:27,160 --> 00:53:27,890
것입니다.

1353
00:53:27,890 --> 00:53:29,650
그래서 활성화 체크포인팅이라는

1354
00:53:29,650 --> 00:53:31,387
또 다른 트릭이 있습니다. 이는

1355
00:53:31,387 --> 00:53:33,220
실제로 모든 활성화를 메모리에

1356
00:53:33,220 --> 00:53:34,918
저장하지 않겠다는 의미입니다.

1357
00:53:34,918 --> 00:53:37,210
우리는 역전파 동안 이를 재계산할 것입니다.

1358
00:53:37,210 --> 00:53:40,720
이것이 어떻게 작동하는지 보려면, 신경망을 다른 방식으로 생각하는

1359
00:53:40,720 --> 00:53:42,470
것이 유용합니다. 신경망에는

1360
00:53:42,470 --> 00:53:44,905
실제로 두 개의 서로 다른 레이어가 있습니다.

1361
00:53:44,905 --> 00:53:47,300
신경망의 각 레이어는 두 가지 작업을 수행합니다.

1362
00:53:47,300 --> 00:53:49,030
다음 레이어의 활성화를 계산하는

1363
00:53:49,030 --> 00:53:50,390
순전파를 수행합니다.

1364
00:53:50,390 --> 00:53:52,098
그런 다음 상류 기울기와

1365
00:53:52,098 --> 00:53:54,940
활성화를 모두 사용하는 기울기를 계산하는

1366
00:53:54,940 --> 00:53:56,540
역전파를 수행합니다.

1367
00:53:56,540 --> 00:54:00,170
그렇다면 일반적으로 이 모든 작업에 얼마나 많은 계산과 메모리가 필요할까요?

1368
00:54:00,170 --> 00:54:02,800
모든 것이 일정하다고 가정하면, 일반적으로

1369
00:54:02,800 --> 00:54:07,195
순전파와 역전파는 순전파 동안 1, 2, 3, 4단계가

1370
00:54:07,195 --> 00:54:07,820
걸립니다.

1371
00:54:07,820 --> 00:54:09,220
순전파 동안 이러한 활성화를

1372
00:54:09,220 --> 00:54:10,400
기억할 것입니다.

1373
00:54:10,400 --> 00:54:13,550
그런 다음 역전파 동안 1, 2, 3, 4단계가 걸립니다.

1374
00:54:13,550 --> 00:54:15,620
따라서 일반적인 순전파와 역전파에서는

1375
00:54:15,620 --> 00:54:20,367
N 레이어 네트워크에 대해 O(N) 계산과 O(N) 메모리가 필요합니다.

1376
00:54:20,367 --> 00:54:22,700
하지만 방금 말했듯이, 이는 메모리가 부족해질 것입니다.

1377
00:54:22,700 --> 00:54:24,500
따라서 대신 역전파 동안

1378
00:54:24,500 --> 00:54:27,000
활성화를 재계산하는 것을 상상할 수 있습니다.

1379
00:54:27,000 --> 00:54:29,220
그 모습은 이와 같습니다.

1380
00:54:29,220 --> 00:54:30,750
활성화부터 시작하겠습니다.

1381
00:54:30,750 --> 00:54:33,125
첫 번째 레이어를 실행한 다음

1382
00:54:33,125 --> 00:54:34,742
즉시 첫 번째 레이어의

1383
00:54:34,742 --> 00:54:36,450
활성화를 버립니다.

1384
00:54:36,450 --> 00:54:38,492
첫 번째 레이어의 순전파를 실행한 다음 즉시 해당

1385
00:54:38,492 --> 00:54:39,930
활성화를 버리고 이렇게 네 번 반복합니다.

1386
00:54:39,930 --> 00:54:41,820
이제 네트워크를 한 번 통과하여 마지막

1387
00:54:41,820 --> 00:54:43,590
레이어의 활성화를 얻었습니다.

1388
00:54:43,590 --> 00:54:46,290
이 시점에서 마지막 레이어에 대한 역전파를 계산할 수

1389
00:54:46,290 --> 00:54:48,210
있지만, 이제 우리는 운이 좋지 않습니다.

1390
00:54:48,210 --> 00:54:50,450
다음 역전파를 계산하기 위해 A3의 활성화가

1391
00:54:50,450 --> 00:54:53,390
없습니다. 하지만 우리는 이를 재계산할 수 있습니다.

1392
00:54:53,390 --> 00:54:54,947
그래서 우리는 이를 재계산합니다.

1393
00:54:54,947 --> 00:54:56,280
이제 역전파를 수행할 수 있습니다.

1394
00:54:56,280 --> 00:54:57,630
이제 좀 더 재계산합니다.

1395
00:54:57,630 --> 00:54:58,800
이제 역전파를 수행하세요.

1396
00:54:58,800 --> 00:54:59,610
이제 다시 계산하세요.

1397
00:54:59,610 --> 00:55:01,050
이제 또 다른 역전파를 수행하세요.

1398
00:55:01,050 --> 00:55:03,710
따라서 모든 것을 합치면, N층의 네트워크에

1399
00:55:03,710 --> 00:55:07,160
대해 N 제곱의 계산과 일정한 메모리가 필요합니다.

1400
00:55:07,160 --> 00:55:11,030
왜냐하면 N, N-1, N-2, N-3, 등등을 더하면

1401
00:55:11,030 --> 00:55:12,780
1까지 가기 때문입니다.

1402
00:55:12,780 --> 00:55:14,450
이것은 제곱 시간입니다.

1403
00:55:14,450 --> 00:55:15,840
그리고 이것을 나눌 수 있습니다.

1404
00:55:15,840 --> 00:55:18,510
N 제곱 계산은 깊은 네트워크에 대해 꽤 나쁩니다.

1405
00:55:18,510 --> 00:55:20,880
따라서 대신 모든 것을 다시 계산하지 않도록 합시다.

1406
00:55:20,880 --> 00:55:23,560
대신 C층마다 활성화 체크포인트를 찍는

1407
00:55:23,560 --> 00:55:24,680
것을 상상해 봅시다.

1408
00:55:24,680 --> 00:55:30,290
그래서 우리는 네트워크의 더 작은 블록 내에서만 다시 계산할 것입니다.

1409
00:55:30,290 --> 00:55:33,010
그런 경우, 네트워크의 과정에서 C번

1410
00:55:33,010 --> 00:55:35,080
활성화를 기억하는 C개의

1411
00:55:35,080 --> 00:55:36,760
체크포인트를 찍으면, N 제곱을

1412
00:55:36,760 --> 00:55:39,010
C로 나눈 계산과 O의 C

1413
00:55:39,010 --> 00:55:40,143
메모리가 필요합니다.

1414
00:55:40,143 --> 00:55:41,560
그리고 C를 √N으로

1415
00:55:41,560 --> 00:55:44,440
설정하는 것이 꽤 일반적인 방법이며, 이

1416
00:55:44,440 --> 00:55:47,915
경우 N√N 계산과 O의 √N 메모리가 됩니다.

1417
00:55:47,915 --> 00:55:49,540
따라서 이것은 더 큰 모델을

1418
00:55:49,540 --> 00:55:51,580
훈련하기 위해 계산과 메모리의 균형을

1419
00:55:51,580 --> 00:55:53,795
맞출 수 있는 꽤 일반적인 방법입니다.

1420
00:55:53,795 --> 00:55:54,295
좋아요.

1421
00:55:54,295 --> 00:55:55,960
이제 FSDP, 활성화

1422
00:55:55,960 --> 00:56:00,540
체크포인팅, HSDP가 있으면 많은 일을 할 수

1423
00:56:00,540 --> 00:56:01,040
있습니다.

1424
00:56:01,040 --> 00:56:03,130
정말 큰 모델을 훈련하기 시작할 수 있습니다.

1425
00:56:03,130 --> 00:56:05,740
그에 대한 레시피는 기본적으로 다음과 같습니다.

1426
00:56:05,740 --> 00:56:07,930
여기서 꽤 멀리 나아갈 수 있는

1427
00:56:07,930 --> 00:56:11,570
스케일링 레시피는 먼저 데이터 병렬성을 사용하고, 그냥 원시

1428
00:56:11,570 --> 00:56:15,430
데이터 병렬성을 사용하여 대략 128개의 GPU와 대략 10억

1429
00:56:15,430 --> 00:56:17,900
개의 매개변수를 가진 모델까지입니다.

1430
00:56:17,900 --> 00:56:19,750
이 크기의 모델에 대해서는 일반 데이터

1431
00:56:19,750 --> 00:56:20,810
병렬성을 사용할 수 있습니다.

1432
00:56:20,810 --> 00:56:22,700
상당히 잘 작동하는 경향이 있습니다.

1433
00:56:22,700 --> 00:56:24,650
그리고 거의 항상 GPU 메모리를

1434
00:56:24,650 --> 00:56:27,470
최대화하기 위해 각 GPU의 로컬 배치 크기를

1435
00:56:27,470 --> 00:56:28,770
설정하는 것이 좋습니다.

1436
00:56:28,770 --> 00:56:31,250
거의 항상 올바른 선택입니다.

1437
00:56:31,250 --> 00:56:33,180
그리고 모델이 커지기 시작하면,

1438
00:56:33,180 --> 00:56:34,850
모델 자체가 GPU 내부에서

1439
00:56:34,850 --> 00:56:36,750
많은 메모리를 차지하게 됩니다.

1440
00:56:36,750 --> 00:56:38,970
그래서 문제가 발생하기 시작합니다.

1441
00:56:38,970 --> 00:56:42,750
따라서 GPU의 메모리 용량과 인터커넥트 속도에

1442
00:56:42,750 --> 00:56:44,280
따라 다릅니다.

1443
00:56:44,280 --> 00:56:46,100
하지만 일반적으로 모델이 10억

1444
00:56:46,100 --> 00:56:47,760
개의 매개변수를 초과하기

1445
00:56:47,760 --> 00:56:49,160
시작하면, 데이터

1446
00:56:49,160 --> 00:56:50,720
병렬성에서 완전 분산 데이터

1447
00:56:50,720 --> 00:56:53,210
병렬성으로 전환하는 것을 고려해야 합니다.

1448
00:56:53,210 --> 00:56:55,463
그리고 이 시점에서 꽤 많이 확장할 수 있지만,

1449
00:56:55,463 --> 00:56:57,380
활성화에 대한 메모리 병목 현상에

1450
00:56:57,380 --> 00:56:58,380
직면하게 됩니다.

1451
00:56:58,380 --> 00:57:00,360
그때 활성화 체크포인팅을 켭니다.

1452
00:57:00,360 --> 00:57:02,180
활성화 체크포인팅은 모든 것을

1453
00:57:02,180 --> 00:57:04,160
훨씬 느리게 만들기 때문에 좋지

1454
00:57:04,160 --> 00:57:07,640
않지만, 훨씬 더 큰 모델을 훈련할 수 있게 해줍니다.

1455
00:57:07,640 --> 00:57:10,523
그리고 이것은 수백 개의 GPU까지 확장됩니다.

1456
00:57:10,523 --> 00:57:12,440
그리고 클러스터 토폴로지에

1457
00:57:12,440 --> 00:57:15,530
따라 다르지만, 보통 256개의 GPU

1458
00:57:15,530 --> 00:57:18,440
또는 512개의 GPU 정도에서

1459
00:57:18,440 --> 00:57:22,880
여러 백 개의 장치에 도달하면 FSDP가 너무 비쌉니다.

1460
00:57:22,880 --> 00:57:25,580
그때 HSDP로 전환하기 시작해야 합니다.

1461
00:57:25,580 --> 00:57:27,860
그리고 이것은 대략 수십억 개의

1462
00:57:27,860 --> 00:57:31,140
매개변수를 가진 모델에 도달할 수 있게 해주며,

1463
00:57:31,140 --> 00:57:34,190
아마도 천 개의 GPU에서 꽤 긴 시퀀스

1464
00:57:34,190 --> 00:57:36,060
길이로 훈련할 수 있습니다.

1465
00:57:36,060 --> 00:57:37,490
그래서 꽤 좋습니다.

1466
00:57:37,490 --> 00:57:40,530
하지만 만약 천 개 이상의 GPU와 500억 개

1467
00:57:40,530 --> 00:57:42,390
이상의 매개변수를 가진 모델,

1468
00:57:42,390 --> 00:57:46,020
10,000 이상 시퀀스 길이가 필요하다면, 이때는 더 고급

1469
00:57:46,020 --> 00:57:48,020
전략인 컨텍스트 병렬 처리,

1470
00:57:48,020 --> 00:57:49,940
파이프라인 병렬 처리 또는 텐서

1471
00:57:49,940 --> 00:57:51,707
병렬 처리로 전환해야 합니다.

1472
00:57:51,707 --> 00:57:53,040
그리고 큰 질문이 있습니다.

1473
00:57:53,040 --> 00:57:53,700
오, 세상에.

1474
00:57:53,700 --> 00:57:55,110
여기 조정해야 할 것이 많습니다.

1475
00:57:55,110 --> 00:57:56,568
어떻게 최적화해야 할까요?

1476
00:57:56,568 --> 00:57:59,120
전역 배치 크기, 로컬 배치 크기,

1477
00:57:59,120 --> 00:58:01,860
HSDP 차원, FSDP 차원, 얼마나

1478
00:58:01,860 --> 00:58:03,500
재계산할지를 설정해야 합니다.

1479
00:58:03,500 --> 00:58:04,168
여기서 길을 잃었습니다.

1480
00:58:04,168 --> 00:58:04,710
어떻게 해야 하나요?

1481
00:58:04,710 --> 00:58:07,880
조정해야 할 것이 너무 많아서 무엇을 해야 할지 모르겠습니다.

1482
00:58:07,880 --> 00:58:10,340
답은 모델 플롭스 활용도(MFU)라는 매우

1483
00:58:10,340 --> 00:58:12,320
중요한 지표를 최적화하는 것입니다.

1484
00:58:12,320 --> 00:58:15,680
GPU 병렬 처리의 바다에서 길을 잃을 때마다, 모델 플롭스

1485
00:58:15,680 --> 00:58:17,630
활용도가 당신의 길잡이입니다.

1486
00:58:17,630 --> 00:58:19,047
이것을 따르세요, 그러면 훈련

1487
00:58:19,047 --> 00:58:21,740
스택을 최적화하기 위해 무엇을 해야 할지 알려줄 것입니다.

1488
00:58:21,740 --> 00:58:23,820
하지만 모델 플롭스 활용도에 도달하기 전에

1489
00:58:23,820 --> 00:58:25,980
하드웨어 플롭스 활용도에 대해 이야기해야 합니다.

1490
00:58:25,980 --> 00:58:28,580
이론적으로 H100은 텐서 코어에서

1491
00:58:28,580 --> 00:58:34,142
초당 989.4 TFLOP의 연산을 수행할 수 있다고 했습니다.

1492
00:58:34,142 --> 00:58:35,100
하지만 이것은 이론적입니다.

1493
00:58:35,100 --> 00:58:36,560
실제로 얼마나 얻을 수 있을까요?

1494
00:58:36,560 --> 00:58:38,390
질문은 실제로 얼마나

1495
00:58:38,390 --> 00:58:40,190
달성할 수 있는가입니다.

1496
00:58:40,190 --> 00:58:42,450
그리고 그것이 하드웨어 플롭스 활용도의 지표입니다.

1497
00:58:42,450 --> 00:58:44,340
장치에서 일부 연산을 수행하고 있습니다.

1498
00:58:44,340 --> 00:58:46,070
이론적 최대치에서 실제로 얼마나

1499
00:58:46,070 --> 00:58:47,750
많은 연산을 실현하고 있습니까?

1500
00:58:47,750 --> 00:58:48,980
이것은 어렵지 않습니다.

1501
00:58:48,980 --> 00:58:50,605
몇 줄의 PyTorch 코드를

1502
00:58:50,605 --> 00:58:52,170
작성하고 이를 벤치마크할 수 있습니다.

1503
00:58:52,170 --> 00:58:54,710
이것은 제가 어제 H100에서 실행한

1504
00:58:54,710 --> 00:58:55,820
벤치마크입니다.

1505
00:58:55,820 --> 00:58:58,410
그리고 이것이 하는 일은 기본적으로 x축입니다.

1506
00:58:58,410 --> 00:59:00,960
루프에서 밀집 행렬 곱셈을 수행하고, 그 다음

1507
00:59:00,960 --> 00:59:03,390
행렬 곱셈이 얼마나 걸렸는지 측정합니다.

1508
00:59:03,390 --> 00:59:05,420
행렬 곱셈이 얼마나 걸렸는지 알 수 있습니다.

1509
00:59:05,420 --> 00:59:07,890
행렬 곱셈이 얼마나 많은 플롭스를 소모하는지 계산할 수 있습니다.

1510
00:59:07,890 --> 00:59:10,910
그런 다음 x축에서 행렬의 크기를 512에서

1511
00:59:10,910 --> 00:59:13,980
32,000까지 플로팅하고 있습니다.

1512
00:59:13,980 --> 00:59:16,673
y축은 하드웨어 플롭스 활용도로, 기본적으로

1513
00:59:16,673 --> 00:59:19,090
이러한 행렬 곱셈에서 우리가

1514
00:59:19,090 --> 00:59:21,220
실제로 실현한 장치의 이론적 최대

1515
00:59:21,220 --> 00:59:22,700
처리량의 비율입니다.

1516
00:59:22,700 --> 00:59:25,870
그리고 이 꽤 간단한 PyTorch 루프에서,

1517
00:59:25,870 --> 00:59:29,560
우리는 8,000 x 8,000의 대형 행렬 곱셈에

1518
00:59:29,560 --> 00:59:33,020
도달하면 H100에서 약 80% HFU를 얻고 있습니다.

1519
00:59:33,020 --> 00:59:34,250
그래서 꽤 괜찮습니다.

1520
00:59:34,250 --> 00:59:36,100
하지만 문제는 HFU가 모델이

1521
00:59:36,100 --> 00:59:39,220
해야 할 다른 모든 것을 고려하지 않는다는 것입니다.

1522
00:59:39,220 --> 00:59:41,570
우리는 아마도 활성화 계산을 하고 있습니다.

1523
00:59:41,570 --> 00:59:43,730
우리는 아마도 다른 모델을 옆에서 실행하고 있습니다.

1524
00:59:43,730 --> 00:59:45,680
우리는 아마도 데이터 로딩, 데이터 증강을 하고 있습니다.

1525
00:59:45,680 --> 00:59:47,222
GPU가 원시 모델에서 순전파와

1526
00:59:47,222 --> 00:59:49,850
역전파 외에 수행하는 다른 작업이 많습니다.

1527
00:59:49,850 --> 00:59:52,330
그래서 우리는 하드웨어 플롭스 활용도에서

1528
00:59:52,330 --> 00:59:54,290
모델 플롭스 활용도로 이동합니다.

1529
00:59:54,290 --> 00:59:56,530
모델 플롭스 활용도는 기본적으로

1530
00:59:56,530 --> 01:00:00,250
GPU의 이론적 TFLOP 중 몇 퍼센트가 내 모델의

1531
01:00:00,250 --> 01:00:03,610
순전파와 역전파에 사용되고 있는지를 말합니다.

1532
01:00:03,610 --> 01:00:06,050
그리고 이것이 항상 최적화하고 싶은 것입니다.

1533
01:00:06,050 --> 01:00:09,340
그러면 이를 더 구체적으로 만들기 위해,

1534
01:00:09,340 --> 01:00:10,740
모델 아키텍처,

1535
01:00:10,740 --> 01:00:13,240
레이어 수, 레이어 크기를

1536
01:00:13,240 --> 01:00:15,020
기반으로 전체 순전파

1537
01:00:15,020 --> 01:00:16,610
및 역전파를 수행하는

1538
01:00:16,610 --> 01:00:19,520
데 필요한 플롭 수를 계산합니다.

1539
01:00:19,520 --> 01:00:22,787
그런 다음 실행 중인 장치의 이론적 최대 처리량을

1540
01:00:22,787 --> 01:00:24,120
어디선가 찾아봅니다.

1541
01:00:24,120 --> 01:00:25,370
그리고 그 두 수를 나눕니다.

1542
01:00:25,370 --> 01:00:28,070
그것은 이론적 최대 처리량을 달성할 경우

1543
01:00:28,070 --> 01:00:31,070
전체 순전파 및 역전파가 얼마나 걸려야

1544
01:00:31,070 --> 01:00:32,160
하는지를 알려줍니다.

1545
01:00:32,160 --> 01:00:34,760
그것이 모델에서 순전파 및 역전파를 수행할

1546
01:00:34,760 --> 01:00:36,726
수 있는 이론적 최속입니다.

1547
01:00:36,726 --> 01:00:40,828
그런 다음 실제로 모델의 순전파 및 역전파를 시간 측정합니다.

1548
01:00:40,828 --> 01:00:42,870
훈련 루프는 이 모든 다른 작업을 수행하고 있습니다.

1549
01:00:42,870 --> 01:00:44,270
데이터 로딩을 하고 있습니다.

1550
01:00:44,270 --> 01:00:45,360
증강을 하고 있습니다.

1551
01:00:45,360 --> 01:00:46,590
통신을 하고 있습니다.

1552
01:00:46,590 --> 01:00:48,900
아마도 활성화 체크포인팅을 하고 있습니다.

1553
01:00:48,900 --> 01:00:50,760
그래서 재계산을 하고 역전파를 하고 있습니다.

1554
01:00:50,760 --> 01:00:52,552
훈련 루프는 많은 작업을 수행하고 있습니다.

1555
01:00:52,552 --> 01:00:54,323
실제로 얼마나 걸리는지 시간을 측정합니다.

1556
01:00:54,323 --> 01:00:55,740
그리고 그 두 숫자를 나눕니다.

1557
01:00:55,740 --> 01:00:58,340
그것은 0과 1 사이의 숫자를 제공합니다.

1558
01:00:58,340 --> 01:01:00,560
즉, 훈련 루프에서 이론적 최대의

1559
01:01:00,560 --> 01:01:03,770
몇 퍼센트를 실제로 달성하고 있는지를 나타냅니다.

1560
01:01:03,770 --> 01:01:06,530
그것이 MFU, 즉 모델 플롭스 활용도입니다.

1561
01:01:06,530 --> 01:01:09,080
그리고 다시, 우리는 이것을 상대적으로 간단한 PyTorch 코드로

1562
01:01:09,080 --> 01:01:09,970
벤치마킹할 수 있습니다.

1563
01:01:09,970 --> 01:01:11,720
여기 ReLU 비선형성을

1564
01:01:11,720 --> 01:01:15,790
가진 짧은 다층 퍼셉트론에서 순전파 및 역전파를 실행하는 예가

1565
01:01:15,790 --> 01:01:19,420
있습니다. 매우 크고 넓은 MLP 레이어와 거대한

1566
01:01:19,420 --> 01:01:22,460
배치 크기를 가진 단일 H100에서 실행됩니다.

1567
01:01:22,460 --> 01:01:24,555
이것은 약 50% MFU를 달성하고 있습니다.

1568
01:01:27,220 --> 01:01:28,870
그리고 일반적으로 분산 훈련을

1569
01:01:28,870 --> 01:01:30,980
위한 조정 노브를 조정할 때는 항상

1570
01:01:30,980 --> 01:01:32,650
MFU를 극대화하기 위해 조정할 수

1571
01:01:32,650 --> 01:01:35,202
있는 모든 노브를 조정하려고 합니다. 왜냐하면

1572
01:01:35,202 --> 01:01:37,660
훈련 처리량을 최적화할 때 우리가 일반적으로

1573
01:01:37,660 --> 01:01:40,090
중요하게 여기는 유일한 지표이기 때문입니다.

1574
01:01:40,090 --> 01:01:44,440
그리고 일반적으로 요즘 MFU가 30% 이상이면 꽤

1575
01:01:44,440 --> 01:01:45,440
괜찮습니다.

1576
01:01:45,440 --> 01:01:47,117
30% 이하라면 아마도 어딘가에

1577
01:01:47,117 --> 01:01:48,700
거대한 병목 현상이 있고 무언가

1578
01:01:48,700 --> 01:01:50,030
잘못되고 있는 것입니다.

1579
01:01:50,030 --> 01:01:52,790
40% 이상은 꽤, 꽤 훌륭합니다.

1580
01:01:52,790 --> 01:01:54,412
그리고 그것은 기본적으로 최첨단입니다.

1581
01:01:54,412 --> 01:01:55,870
그리고 여기서 몇몇 논문에서 가져올

1582
01:01:55,870 --> 01:01:57,160
수 있는 숫자들이 있습니다.

1583
01:01:57,160 --> 01:02:00,580
특히, 우리가 이야기했던 Llama3-405B

1584
01:02:00,580 --> 01:02:02,215
논문입니다.

1585
01:02:02,215 --> 01:02:04,750
그들의 최종 훈련 단계에서는 8,

1586
01:02:04,750 --> 01:02:07,480
000에서 16,000개의 GPU를

1587
01:02:07,480 --> 01:02:11,400
동시에 사용하는 여러 가지 훈련 단계 변형이 있습니다.

1588
01:02:11,400 --> 01:02:15,250
그 결과, 대략 30대 후반에서 40대 초반의 MFU를 얻고

1589
01:02:15,250 --> 01:02:15,970
있습니다.

1590
01:02:15,970 --> 01:02:18,330
그것은 꽤 좋습니다.

1591
01:02:18,330 --> 01:02:19,890
H100에서는 그보다 훨씬

1592
01:02:19,890 --> 01:02:22,140
더 높은 수치를 얻기 어렵습니다.

1593
01:02:22,140 --> 01:02:25,200
실제로, 역설적으로, 최신 장치에서는 때때로 MFU가 더

1594
01:02:25,200 --> 01:02:26,400
나빠질 수 있습니다.

1595
01:02:26,400 --> 01:02:29,350
이전 세대 장치인 H100에서는 때때로 50%

1596
01:02:29,350 --> 01:02:31,690
이상의 MFU를 얻을 수 있었습니다.

1597
01:02:31,690 --> 01:02:33,960
그 이유는 GPU가 더 빨라지고

1598
01:02:33,960 --> 01:02:36,810
있지만, 통신 속도가 그만큼 빨라지지

1599
01:02:36,810 --> 01:02:37,780
않기 때문입니다.

1600
01:02:37,780 --> 01:02:40,330
A100에서 H100으로 이동할 때,

1601
01:02:40,330 --> 01:02:43,170
이론적인 처리량에서 대략 3배의 개선을

1602
01:02:43,170 --> 01:02:45,570
얻었지만, 이론적인 메모리 대역폭에서는

1603
01:02:45,570 --> 01:02:47,280
2배의 개선만 있었습니다.

1604
01:02:47,280 --> 01:02:50,940
GPU가 매우 빠르게 빨라지고 있지만, GPU

1605
01:02:50,940 --> 01:02:52,810
간의 통신을 확장하는 것은

1606
01:02:52,810 --> 01:02:55,780
더 어려워지는 격차가 커지고 있습니다.

1607
01:02:55,780 --> 01:02:58,350
그 결과, 최신 세대 장치에서는 실제로 MFAs가

1608
01:02:58,350 --> 01:03:00,335
더 나빠지는 경향이 있습니다.

1609
01:03:03,015 --> 01:03:05,760
나는 의도적으로 그 점들에 대부분의 시간을 할애하고

1610
01:03:05,760 --> 01:03:07,803
싶었습니다. 왜냐하면 여러분이 실제로

1611
01:03:07,803 --> 01:03:09,970
사용할 가능성이 높은 것들이기 때문입니다.

1612
01:03:09,970 --> 01:03:12,400
이 방에 있는 누구도 10,000 GPU 클러스터에

1613
01:03:12,400 --> 01:03:14,060
접근할 가능성은 낮다고 생각합니다.

1614
01:03:14,060 --> 01:03:15,980
만약 그렇다면, 수업 후에 저에게 이야기해 주세요.

1615
01:03:15,980 --> 01:03:18,310
친구가 되고 싶습니다.

1616
01:03:18,310 --> 01:03:19,750
그래서 여러분이 실제로

1617
01:03:19,750 --> 01:03:21,730
마주칠 가능성이 있는 것들은

1618
01:03:21,730 --> 01:03:23,620
수백 개의 GPU까지입니다.

1619
01:03:23,620 --> 01:03:25,700
하지만 제가 좋아하는 다른 것들도 있습니다.

1620
01:03:25,700 --> 01:03:27,980
여기 꽤 괜찮은 슬라이드가 있지만, 전체

1621
01:03:27,980 --> 01:03:29,710
세부 사항을 다루지 않아도

1622
01:03:29,710 --> 01:03:30,210
괜찮습니다.

1623
01:03:30,210 --> 01:03:32,320
오프라인에서 확인할 수 있습니다.

1624
01:03:32,320 --> 01:03:34,480
우리는 맥락 병렬성이 기본적으로 시퀀스

1625
01:03:34,480 --> 01:03:36,080
차원에서 나누는 것이라고 말했습니다.

1626
01:03:36,080 --> 01:03:38,930
우리는 변환기가 시퀀스에서 작동한다고 말했습니다.

1627
01:03:38,930 --> 01:03:42,715
기본적으로 아이디어는 긴 시퀀스가 있다는 것입니다.

1628
01:03:42,715 --> 01:03:45,925
서로 다른 GPU가 시퀀스의 서로 다른 부분을 처리하게 합니다.

1629
01:03:45,925 --> 01:03:48,587
변환기 블록을 기억하신다면,

1630
01:03:48,587 --> 01:03:50,920
LayerNorm, FFN, MLP

1631
01:03:50,920 --> 01:03:54,640
및 잔여 연결은 시퀀스 전반에 걸쳐 독립적으로

1632
01:03:54,640 --> 01:03:56,230
작동하므로 변환기의

1633
01:03:56,230 --> 01:03:59,133
큰 부분에 대해서는 실제로 쉽습니다.

1634
01:03:59,133 --> 01:04:00,550
따라서 시퀀스

1635
01:04:00,550 --> 01:04:03,100
차원에서 그 계산을 청크로 나누는

1636
01:04:03,100 --> 01:04:05,050
것은 상대적으로 간단합니다.

1637
01:04:05,050 --> 01:04:07,320
MLP 내부에서는 약간 복잡해지긴 하지만, 그

1638
01:04:07,320 --> 01:04:08,778
안에 가중치가 있기 때문입니다.

1639
01:04:08,778 --> 01:04:10,950
따라서 데이터 병렬성 사례에서 했던

1640
01:04:10,950 --> 01:04:13,685
것처럼 그래디언트를 모두 줄여야 합니다.

1641
01:04:13,685 --> 01:04:15,060
주의는 시퀀스 병렬성에서

1642
01:04:15,060 --> 01:04:16,690
복잡해지는 부분입니다.

1643
01:04:16,690 --> 01:04:18,360
왜냐하면 주의를 기억하신다면,

1644
01:04:18,360 --> 01:04:20,670
시퀀스 내의 모든 쌍의 요소

1645
01:04:20,670 --> 01:04:23,310
간의 상호작용을 계산해야 하기 때문입니다.

1646
01:04:23,310 --> 01:04:25,380
QKV 프로젝션은 쉽습니다.

1647
01:04:25,380 --> 01:04:27,490
왜냐하면 시퀀스에 대해 자명하게 병렬화할

1648
01:04:27,490 --> 01:04:30,450
수 있지만, 실제로 병렬화하기는 꽤 까다로운

1649
01:04:30,450 --> 01:04:32,640
핵심 어텐션 매트릭스가 있습니다.

1650
01:04:32,640 --> 01:04:34,713
사람들이 개발한 첫 번째

1651
01:04:34,713 --> 01:04:36,630
버전은 링 어텐션이라고 불리며,

1652
01:04:36,630 --> 01:04:39,850
기본적으로 전체 어텐션 매트릭스를 블록으로

1653
01:04:39,850 --> 01:04:43,470
나누고, 그런 다음 GPU가 올바른 순서로 이

1654
01:04:43,470 --> 01:04:45,510
블록에서 독립적으로 병렬

1655
01:04:45,510 --> 01:04:47,133
작업을 하도록 합니다.

1656
01:04:47,133 --> 01:04:48,550
그 안에는 많은 세부 사항이 있습니다.

1657
01:04:48,550 --> 01:04:50,760
더 많은 세부 사항은 논문을 확인해 보세요.

1658
01:04:50,760 --> 01:04:53,320
두 번째는 개념적으로 조금 더 쉬운

1659
01:04:53,320 --> 01:04:56,010
Ulysses 어텐션이라고 하며, 여기서는 헤드에

1660
01:04:56,010 --> 01:04:57,520
대해 병렬성을 수행합니다.

1661
01:04:57,520 --> 01:04:59,610
변환기에서 거의 항상 여러 어텐션

1662
01:04:59,610 --> 01:05:01,380
매트릭스에 대해 병렬로

1663
01:05:01,380 --> 01:05:04,500
어텐션을 계산하는 멀티 헤드 어텐션을 수행하고 있다는

1664
01:05:04,500 --> 01:05:05,800
점을 기억하세요.

1665
01:05:05,800 --> 01:05:07,530
Ulysses 어텐션에서는

1666
01:05:07,530 --> 01:05:11,110
그 핵심 어텐션 연산자의 계산을 병렬화할 것입니다.

1667
01:05:11,110 --> 01:05:13,493
그 오버헤드를 병렬화하고,

1668
01:05:13,493 --> 01:05:14,910
변환기의 다른 모든

1669
01:05:14,910 --> 01:05:18,300
부분은 시퀀스 차원에 대해 병렬화됩니다.

1670
01:05:18,300 --> 01:05:22,290
예를 들어, 이 컨텍스트 병렬성은 시퀀스 길이를

1671
01:05:22,290 --> 01:05:24,780
상당히 크게 확장할 때

1672
01:05:24,780 --> 01:05:25,840
중요해집니다.

1673
01:05:25,840 --> 01:05:28,570
Llama3 사전 훈련의 예로 돌아가면,

1674
01:05:28,570 --> 01:05:30,670
실제로 모델을 두 단계로 훈련합니다.

1675
01:05:30,670 --> 01:05:34,410
첫 번째 단계에서는 컨텍스트 병렬성 없이 시퀀스 길이를 8,

1676
01:05:34,410 --> 01:05:35,437
000으로 설정합니다.

1677
01:05:35,437 --> 01:05:37,770
그런 다음 시퀀스 길이를 130,000으로

1678
01:05:37,770 --> 01:05:40,540
늘리는 두 번째 훈련 단계를 가집니다.

1679
01:05:40,540 --> 01:05:44,260
그 시점에서 16배 컨텍스트 병렬성을 수행합니다.

1680
01:05:44,260 --> 01:05:48,630
즉, 131,000 길이의 각 시퀀스는 16개의 GPU가 하나의 시퀀스에서

1681
01:05:48,630 --> 01:05:51,130
병렬로 작동하고 있다는 의미입니다.

1682
01:05:51,130 --> 01:05:54,670
이는 배치 크기가 1/16이라는 것과 같으며,

1683
01:05:54,670 --> 01:06:01,050
이제 각 배치, 각 GPU는 하나의 요소보다 적은 작업을 하고 있습니다.

1684
01:06:01,050 --> 01:06:02,440
이것이 컨텍스트 병렬성입니다.

1685
01:06:02,440 --> 01:06:03,460
파이프라인 병렬성.

1686
01:06:03,460 --> 01:06:05,670
우리는 레이어 차원에서 나눌 것입니다.

1687
01:06:05,670 --> 01:06:08,160
직관적으로 하고 싶은 것은 여러 레이어가 있는 네트워크를

1688
01:06:08,160 --> 01:06:09,250
가지는 것입니다.

1689
01:06:09,250 --> 01:06:12,370
그리고 우리는 GPU에 레이어를 나누기만 하면 됩니다.

1690
01:06:12,370 --> 01:06:14,820
사실, 이것은 매우 직관적인 일입니다.

1691
01:06:14,820 --> 01:06:17,380
문제는 순차적 의존성이 있다는 것입니다.

1692
01:06:17,380 --> 01:06:20,310
각 GPU는 포워드 패스를 계속 실행하기

1693
01:06:20,310 --> 01:06:22,380
위해 이전 GPU의 활성화를

1694
01:06:22,380 --> 01:06:23,447
필요로 합니다.

1695
01:06:23,447 --> 01:06:25,530
역방향 패스 동안, 역방향 패스를

1696
01:06:25,530 --> 01:06:27,000
계산하기 위해 상류 레이어의

1697
01:06:27,000 --> 01:06:28,420
그래디언트가 필요합니다.

1698
01:06:28,420 --> 01:06:31,830
그래서 이렇게 다이어그램을 그릴 수 있습니다. 수직 축은

1699
01:06:31,830 --> 01:06:33,280
GPU 1에서 4까지입니다.

1700
01:06:33,280 --> 01:06:36,370
수평 축은 시간 경과에 따른 상황입니다.

1701
01:06:36,370 --> 01:06:39,810
GPU 1이 포워드를 실행한 다음 활성화를 GPU 2에 전달하고, GPU

1702
01:06:39,810 --> 01:06:43,080
2가 GPU 3에 활성화를 전달하고, GPU 3이 GPU 4에

1703
01:06:43,080 --> 01:06:45,090
활성화를 전달하는 것을 볼 수 있습니다.

1704
01:06:45,090 --> 01:06:46,260
GPU 4는 운이 좋습니다.

1705
01:06:46,260 --> 01:06:48,160
한 번에 포워드와 백워드를 모두 수행한

1706
01:06:48,160 --> 01:06:51,970
다음 그래디언트를 GPU 3, GPU 2, GPU 1로 다시 전달할 수
있습니다.

1707
01:06:51,970 --> 01:06:55,630
그래프에서 보듯이, 이는 정말로 나쁜 상황입니다. 왜냐하면 GPU가

1708
01:06:55,630 --> 01:06:57,940
대부분 유휴 상태로 있기 때문입니다.

1709
01:06:57,940 --> 01:07:02,320
실제로 N개의 GPU가 있다면, 그들로부터 유용한 작업을 얻는

1710
01:07:02,320 --> 01:07:04,370
것은 1/N의 시간만 가능합니다.

1711
01:07:04,370 --> 01:07:07,250
즉, 8배 파이프라인 병렬성이 있다면, 그

1712
01:07:07,250 --> 01:07:09,400
시점에서 최대 가능한 MFU는

1713
01:07:09,400 --> 01:07:11,420
약 12%로, 이는 끔찍합니다.

1714
01:07:11,420 --> 01:07:13,120
그래서 그건 정말 나쁘다.

1715
01:07:13,120 --> 01:07:14,747
그리고 참고로, 귀여운 이름이 있다.

1716
01:07:14,747 --> 01:07:16,330
이것들은 가끔 버블이라고

1717
01:07:16,330 --> 01:07:19,490
불리며, GPU가 작업을 기다리는 그 덩어리와 같다.

1718
01:07:19,490 --> 01:07:22,040
그리고 그들은 통신을 기다리고 있다.

1719
01:07:22,040 --> 01:07:23,650
파이프라인 병렬 처리의 요점은

1720
01:07:23,650 --> 01:07:24,890
버블을 줄이는 것이다.

1721
01:07:24,890 --> 01:07:26,270
버블을 줄이는 것이 좋다.

1722
01:07:26,270 --> 01:07:29,290
우리가 이를 수행하는 방법은 여러 개의 마이크로 배치를

1723
01:07:29,290 --> 01:07:30,530
동시에 실행하는 것이다.

1724
01:07:30,530 --> 01:07:33,160
이제 모든 GPU를 통해 데이터를 한

1725
01:07:33,160 --> 01:07:35,390
배치씩 순방향과 역방향으로

1726
01:07:35,390 --> 01:07:37,660
실행하는 대신, 여러 개의 데이터

1727
01:07:37,660 --> 01:07:39,700
배치를 동시에 사용하고 이들을

1728
01:07:39,700 --> 01:07:42,537
GPU 간에 병렬로 이동시킬 것이다.

1729
01:07:42,537 --> 01:07:44,620
이것을 위해 시도할 수 있는 다양한

1730
01:07:44,620 --> 01:07:45,980
흥미로운 패턴이 있다.

1731
01:07:45,980 --> 01:07:47,590
하지만 여기에는 상대적으로 간단한

1732
01:07:47,590 --> 01:07:50,240
패턴이 있다. 네 방향 파이프라인 병렬 처리이다.

1733
01:07:50,240 --> 01:07:52,790
네 개의 GPU가 모두 병렬로 작업하고 있다.

1734
01:07:52,790 --> 01:07:55,360
그리고 동시에 활성화된 네 개의

1735
01:07:55,360 --> 01:07:57,250
데이터 배치가 있다.

1736
01:07:57,250 --> 01:07:58,970
이 배치는 색상으로 구분된다.

1737
01:07:58,970 --> 01:08:02,610
그래서 GPU 1은 파란 배치에서 순방향으로 실행하고, 그 다음

1738
01:08:02,610 --> 01:08:03,990
노란 배치, 그 다음 초록

1739
01:08:03,990 --> 01:08:06,615
배치, 그 다음 빨간 배치에서 순방향으로 실행하며,

1740
01:08:06,615 --> 01:08:09,320
GPU 1이 노란 배치에서 순방향으로 진행하는

1741
01:08:09,320 --> 01:08:12,170
동안, 우리는 파란 배치의 활성화를 GPU 1 또는

1742
01:08:12,170 --> 01:08:13,260
GPU 2로 전달했다.

1743
01:08:13,260 --> 01:08:15,810
그리고 GPU 2는 이제 파란 배치에서 순방향으로 실행할 수 있다.

1744
01:08:15,810 --> 01:08:19,140
이 모든 것들은 연속적으로 진행되고 병렬로 발생할 수 있다.

1745
01:08:19,140 --> 01:08:20,540
그리고 같은 패턴이 역방향

1746
01:08:20,540 --> 01:08:21,600
패스 동안 반복된다.

1747
01:08:21,600 --> 01:08:25,189
우리는 이 서로 다른 마이크로 배치를 서로 다른 GPU를

1748
01:08:25,189 --> 01:08:28,160
통해 파이프라인으로 처리하면서 교차할 수 있다.

1749
01:08:28,160 --> 01:08:32,240
이 경우 네 방향 파이프라인 병렬 처리와 네 개의

1750
01:08:32,240 --> 01:08:35,660
마이크로 배치로, 최대 이론적 MFU는 이

1751
01:08:35,660 --> 01:08:38,580
그래프의 흰색이 아닌 부분의 비율이다.

1752
01:08:38,580 --> 01:08:41,689
그리고 그것은 이제 57%로 증가하며, 꽤 좋은 수치이다.

1753
01:08:41,689 --> 01:08:43,920
그래서 파이프라인 병렬 처리로 이론적으로,

1754
01:08:43,920 --> 01:08:46,260
많은 마이크로 배치로 가면 MFU가

1755
01:08:46,260 --> 01:08:48,260
좋을 것이다. 왜냐하면 많은 작업을

1756
01:08:48,260 --> 01:08:50,340
병렬로 수행할 수 있기 때문이다.

1757
01:08:50,340 --> 01:08:52,288
하지만 마이크로 배치가 많아질수록,

1758
01:08:52,288 --> 01:08:54,330
모든 활성화를 메모리에 저장해야 한다.

1759
01:08:54,330 --> 01:08:56,310
그래서 이제 활성화 체크포인팅을 수행해야 하고, 그러면

1760
01:08:56,310 --> 01:08:57,895
'아, 큰일이다.'라고 생각하게 된다.

1761
01:08:57,895 --> 01:08:59,020
이것들을 어떻게 조정하지?

1762
01:08:59,020 --> 01:09:01,095
더 많은 파이프라인 병렬 처리를 해야 할까?

1763
01:09:01,095 --> 01:09:02,470
마이크로 배치를 줄여야 할까?

1764
01:09:02,470 --> 01:09:04,762
더 공격적인 활성화 체크포인팅을 해야 할까?

1765
01:09:04,762 --> 01:09:07,498
그리고 그 위에 데이터 병렬 처리를 추가해야 할까?

1766
01:09:07,498 --> 01:09:08,040
모르겠다.

1767
01:09:08,040 --> 01:09:09,082
어떻게 할 건가?

1768
01:09:09,082 --> 01:09:09,810
MFU를 극대화하라.

1769
01:09:09,810 --> 01:09:11,850
훈련 파이프라인의 MFU를

1770
01:09:11,850 --> 01:09:16,109
극대화하기 위해 모든 조정을 시도할 것이다.

1771
01:09:16,109 --> 01:09:18,460
마지막은 텐서 병렬성입니다.

1772
01:09:18,460 --> 01:09:22,870
이 모델 차원에서 나누게 됩니다.

1773
01:09:22,870 --> 01:09:24,390
기본적으로 모델에

1774
01:09:24,390 --> 01:09:26,830
많은 가중치 행렬이 있습니다.

1775
01:09:26,830 --> 01:09:30,060
모든 가중치 행렬은 XW가

1776
01:09:30,060 --> 01:09:33,510
Y와 같다는 계산을 합니다. 이것이 우리의 변환기 내부에서 반복적으로

1777
01:09:33,510 --> 01:09:34,569
수행되는 것입니다.

1778
01:09:34,569 --> 01:09:39,240
이제 각 가중치 행렬을 GPU에 걸쳐 나눌 것입니다.

1779
01:09:39,240 --> 01:09:40,750
이것은 FSDP와 다릅니다.

1780
01:09:40,750 --> 01:09:42,750
왜냐하면 실제로 단일 가중치 행렬을

1781
01:09:42,750 --> 01:09:44,229
GPU에 걸쳐 나누기 때문입니다.

1782
01:09:44,229 --> 01:09:46,390
이제 통신이 없습니다.

1783
01:09:46,390 --> 01:09:48,340
이제 블록 행렬 곱셈을 수행합니다.

1784
01:09:48,340 --> 01:09:52,229
각 GPU는 전체 입력에 대한 행렬 곱셈의 슬라이스를

1785
01:09:52,229 --> 01:09:53,200
계산합니다.

1786
01:09:53,200 --> 01:09:55,050
이 경우, 가중치 행렬을

1787
01:09:55,050 --> 01:09:57,450
W1, W2, W3, W4로 나눕니다.

1788
01:09:57,450 --> 01:10:00,120
그런 다음 각 GPU는 해당 행렬의 슬라이스를

1789
01:10:00,120 --> 01:10:02,870
계산하여 출력의 슬라이스를 계산합니다.

1790
01:10:02,870 --> 01:10:06,180
문제는, 이제 순전파를 수행한 후 모든

1791
01:10:06,180 --> 01:10:09,960
GPU에서 활성화를 모아 다음 순전파를 수행해야

1792
01:10:09,960 --> 01:10:11,660
한다는 것입니다.

1793
01:10:11,660 --> 01:10:13,220
약간의 요령이 있습니다.

1794
01:10:13,220 --> 01:10:15,750
두 개의 이러한 레이어가

1795
01:10:15,750 --> 01:10:20,510
연속으로 있을 경우, 두 레이어 사이에 모으지 않고도 진행할 수

1796
01:10:20,510 --> 01:10:21,180
있습니다.

1797
01:10:21,180 --> 01:10:23,780
두 개의 레이어가 있다면, 조용한 곳에 앉아 이 작업을

1798
01:10:23,780 --> 01:10:24,810
진행할 수 있습니다.

1799
01:10:24,810 --> 01:10:26,185
첫 번째 가중치 행렬을 열

1800
01:10:26,185 --> 01:10:27,450
모양의 청크로 나눕니다.

1801
01:10:27,450 --> 01:10:30,120
그런 다음 두 번째 가중치 행렬을 행 모양의 청크로 나눕니다.

1802
01:10:30,120 --> 01:10:32,390
이 모든 것을 수행하면 블록 행렬

1803
01:10:32,390 --> 01:10:34,850
곱셈의 마법과 신비 덕분에 모든 것이

1804
01:10:34,850 --> 01:10:36,295
마법처럼 작동합니다.

1805
01:10:36,295 --> 01:10:37,670
최종 출력을

1806
01:10:37,670 --> 01:10:41,840
Y와 U의 블록 행렬 곱셈의 내적

1807
01:10:41,840 --> 01:10:44,790
구조로 계산할 수 있습니다.

1808
01:10:44,790 --> 01:10:49,490
따라서 기본적으로 여러 GPU에 걸쳐 나누어진 두 개의 행렬 곱셈

1809
01:10:49,490 --> 01:10:51,627
레이어를 가질 수 있습니다.

1810
01:10:51,627 --> 01:10:53,210
그리고 그들은 매 두 레이어의

1811
01:10:53,210 --> 01:10:54,713
끝에서만 통신해야 합니다.

1812
01:10:54,713 --> 01:10:56,130
이것은 변환기가 FFN에서

1813
01:10:56,130 --> 01:11:00,100
두 개의 레이어 MLP를 가지고 있다는 것을 기억하면 잘 작동합니다.

1814
01:11:00,100 --> 01:11:01,860
그래서 이것은 변환기가 항상 가지고

1815
01:11:01,860 --> 01:11:03,900
있는 두 개의 레이어 MLP에 잘

1816
01:11:03,900 --> 01:11:05,282
맞는 정말 좋은 요령입니다.

1817
01:11:05,282 --> 01:11:06,990
그래서 큰 변환기에서

1818
01:11:06,990 --> 01:11:11,070
텐서 병렬성을 사용하는 것은 꽤 일반적이며,

1819
01:11:11,070 --> 01:11:16,050
변환기의 MLP에서 이 두 레이어 텐서 병렬성 요령을 사용합니다.

1820
01:11:16,050 --> 01:11:19,020
이것이 GPU에 걸쳐 계산을 나누기

1821
01:11:19,020 --> 01:11:20,890
위한 모든 메커니즘입니다.

1822
01:11:20,890 --> 01:11:22,240
어떤 것이 가장 좋습니까?

1823
01:11:22,240 --> 01:11:23,890
실제 답은 모두입니다.

1824
01:11:23,890 --> 01:11:26,500
실제로 우리는 ND 병렬성을 사용할 것입니다.

1825
01:11:26,500 --> 01:11:28,950
이미 HSRP로 2차원 병렬성의 예를

1826
01:11:28,950 --> 01:11:29,860
보았습니다.

1827
01:11:29,860 --> 01:11:32,160
실제로 현재 최첨단은 4차원

1828
01:11:32,160 --> 01:11:33,940
병렬성과 같습니다.

1829
01:11:33,940 --> 01:11:37,080
Lama로 돌아가면, 그들은 16,000개의 GPU로

1830
01:11:37,080 --> 01:11:40,240
가장 큰 훈련을 하고 있다는 것을 알 수 있습니다.

1831
01:11:40,240 --> 01:11:43,110
그들은 8방향 텐서 병렬성, 16방향 컨텍스트

1832
01:11:43,110 --> 01:11:46,650
병렬성, 16방향 파이프라인 병렬성, 8방향 데이터

1833
01:11:46,650 --> 01:11:48,750
병렬성을 동시에 사용하고 있습니다.

1834
01:11:48,750 --> 01:11:53,090
그리고 조심하면, 서로 다른 병렬 처리 메커니즘은 서로 다른 통신

1835
01:11:53,090 --> 01:11:54,990
요구 사항을 가지고 있습니다.

1836
01:11:54,990 --> 01:11:58,850
그래서 클러스터의 서로 다른 병렬 처리

1837
01:11:58,850 --> 01:12:00,630
축을 어떻게 배열하는지

1838
01:12:00,630 --> 01:12:02,780
조심하면, 전체

1839
01:12:02,780 --> 01:12:07,910
클러스터에서 통신 속도의 변화를 활용할 수 있습니다.

1840
01:12:07,910 --> 01:12:09,380
이것이 대규모 분산 훈련에

1841
01:12:09,380 --> 01:12:10,890
대한 간단한 개요입니다.

1842
01:12:10,890 --> 01:12:13,730
오늘의 핵심은 개별 GPU가 기본적으로

1843
01:12:13,730 --> 01:12:16,920
일반화 가능한 병렬 컴퓨팅 머신이라는 것입니다.

1844
01:12:16,920 --> 01:12:20,485
GPU 클러스터는 수만 개, 어쩌면 수십만 개의 개별 GPU로

1845
01:12:20,485 --> 01:12:22,610
구성된 거대한 대규모 병렬 머신이며,

1846
01:12:22,610 --> 01:12:25,790
우리는 이를 하나의 큰 단위로 프로그래밍하고자 합니다.

1847
01:12:25,790 --> 01:12:28,040
그리고 우리는 큰 클러스터에서 계산을

1848
01:12:28,040 --> 01:12:30,960
병렬화하는 여러 가지 메커니즘과 메모리를 절약하기

1849
01:12:30,960 --> 01:12:34,230
위한 트릭인 활성화 체크포인팅, 그리고 이러한

1850
01:12:34,230 --> 01:12:36,740
파이프라인을 설계할 때 항상 최적화하려고 하는

1851
01:12:36,740 --> 01:12:39,115
유일한 지표인 모델 플롭스 활용도에

1852
01:12:39,115 --> 01:12:40,260
대해 이야기했습니다.

1853
01:12:40,260 --> 01:12:41,780
다음에 수만 개의 GPU로

1854
01:12:41,780 --> 01:12:44,450
훈련할 때 이 점을 기억하시길 바랍니다.

1855
01:12:44,450 --> 01:12:48,040
그리고 제가 당신의 수만 개의 GPU를 빌릴 수 있도록 알려주세요.
