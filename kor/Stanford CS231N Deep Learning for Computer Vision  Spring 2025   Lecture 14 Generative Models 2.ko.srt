1
00:00:05,480 --> 00:00:08,392
그래서 지난 시간에 우리는 생성 모델에 대해 이야기했습니다.

2
00:00:08,392 --> 00:00:10,600
우리는 생성 모델과 판별 모델에 대한 논의로

3
00:00:10,600 --> 00:00:13,308
시작했으며, 이 두 가지는 기본적으로 확률 모델의 서로

4
00:00:13,308 --> 00:00:15,822
다른 종류라는 점을 기억하세요. 하지만 우리가

5
00:00:15,822 --> 00:00:17,780
예측하려는 것, 조건화하는 것, 그리고

6
00:00:17,780 --> 00:00:19,600
정말로 중요한 것은 우리가 정규화하는

7
00:00:19,600 --> 00:00:20,733
것에 따라 달라집니다.

8
00:00:20,733 --> 00:00:22,400
우리는 데이터 x에 조건화된

9
00:00:22,400 --> 00:00:24,920
레이블 y를 예측하려고 하는 판별

10
00:00:24,920 --> 00:00:27,280
모델과, 데이터 x에 대한 확률 분포를

11
00:00:27,280 --> 00:00:30,040
배우려고 하는 생성 모델, 그리고 사용자

12
00:00:30,040 --> 00:00:32,040
입력 y 또는 레이블 y에

13
00:00:32,040 --> 00:00:37,080
조건화된 데이터 x를 모델링하려고 하는 조건부 생성 모델에 대해
이야기했습니다.

14
00:00:37,080 --> 00:00:39,200
이들은 정규화하려는

15
00:00:39,200 --> 00:00:42,320
것에서 차이가 나며, 확률

16
00:00:42,320 --> 00:00:46,777
분포는 정규화 효과를 도입하여 서로 다른

17
00:00:46,777 --> 00:00:49,360
종류의 것들이 확률

18
00:00:49,360 --> 00:00:51,840
질량을 놓고 경쟁해야

19
00:00:51,840 --> 00:00:54,080
하는 제약을 만듭니다.

20
00:00:54,080 --> 00:00:56,480
지난 시간에는 생성 모델의 다양한 범주에

21
00:00:56,480 --> 00:00:58,480
대한 분류 체계를 살펴보았는데,

22
00:00:58,480 --> 00:01:00,740
생성 모델링 분야는 사람들이

23
00:01:00,740 --> 00:01:02,340
오랫동안 연구해온 분야로,

24
00:01:02,340 --> 00:01:03,820
이러한 문제의 변형을

25
00:01:03,820 --> 00:01:05,237
해결하기 위한 다양한

26
00:01:05,237 --> 00:01:07,240
방법의 범주가 많이 개발되었습니다.

27
00:01:07,240 --> 00:01:09,980
우리는 생성 모델의 계보를 살펴보았고,

28
00:01:09,980 --> 00:01:14,020
지난 시간에는 모델이 어떤 양 p(x)를

29
00:01:14,020 --> 00:01:18,620
출력하는 명시적 밀도 모델에 대해 이야기했습니다.

30
00:01:18,620 --> 00:01:22,140
이는 처리 가능한 밀도 모델의 경우 정확한

31
00:01:22,140 --> 00:01:24,780
예측 p(x)일 수도 있고, 근사

32
00:01:24,780 --> 00:01:27,060
밀도 모델의 경우 p(x)의

33
00:01:27,060 --> 00:01:29,440
근사값일 수도 있습니다.

34
00:01:29,440 --> 00:01:31,680
처리 가능한 밀도의 경우,

35
00:01:31,680 --> 00:01:34,600
우리는 모델의 한 범주로서 자기 회귀

36
00:01:34,600 --> 00:01:36,072
모델을 보았고, 근사

37
00:01:36,072 --> 00:01:37,780
밀도를 제공하는 예로

38
00:01:37,780 --> 00:01:40,180
변분 오토인코더를 보았습니다.

39
00:01:40,180 --> 00:01:42,420
자기 회귀 모델에서는 우리가

40
00:01:42,420 --> 00:01:45,540
이미지 또는 우리가 작업하는 데이터의

41
00:01:45,540 --> 00:01:48,260
종류를 순서로 나누는 작업을 했습니다.

42
00:01:48,260 --> 00:01:50,040
이미지 데이터의 경우, 우리는

43
00:01:50,040 --> 00:01:52,980
일반적으로 이를 픽셀 값 또는 서브픽셀

44
00:01:52,980 --> 00:01:54,540
값의 시퀀스로 처리합니다.

45
00:01:54,540 --> 00:01:56,440
우리는 보통 이를 이산적으로 처리하고 싶어합니다.

46
00:01:56,440 --> 00:02:00,000
픽셀 값을 0에서 255까지의 값을 가질 수 있는

47
00:02:00,000 --> 00:02:01,780
8비트 정수로 취급합니다.

48
00:02:01,780 --> 00:02:05,620
이것을 긴 정수 시퀀스로 나열한 다음, 일반적으로

49
00:02:05,620 --> 00:02:08,680
RNN 또는 변환기를 사용하여

50
00:02:08,680 --> 00:02:12,120
이산 자기 회귀 시퀀스 모델로 모델링합니다.

51
00:02:12,120 --> 00:02:14,940
또한 변분 오토인코더를 보았는데,

52
00:02:14,940 --> 00:02:17,580
이는 또 다른 명시적 밀도

53
00:02:17,580 --> 00:02:19,400
모델이지만, 이제는 정확한

54
00:02:19,400 --> 00:02:21,180
밀도가 아니라 밀도의

55
00:02:21,180 --> 00:02:24,800
근사값, 특히 밀도의 하한을 계산합니다.

56
00:02:24,800 --> 00:02:27,480
이를 위해 우리는 데이터를 입력하고

57
00:02:27,480 --> 00:02:29,720
잠재 코드 z에 대한 분포를

58
00:02:29,720 --> 00:02:33,880
출력하는 인코더 네트워크와 잠재 코드 z를 입력하고

59
00:02:33,880 --> 00:02:36,560
예측된 데이터 조각 x를 출력하는 디코더

60
00:02:36,560 --> 00:02:39,360
네트워크를 공동으로 훈련했습니다.

61
00:02:39,360 --> 00:02:41,160
우리는 이 두 네트워크,

62
00:02:41,160 --> 00:02:46,040
인코더와 디코더를 공동으로 훈련하여 우리의 가능도 함수에

63
00:02:46,040 --> 00:02:48,120
대한 이 변분 하한을

64
00:02:48,120 --> 00:02:49,960
최대화할 수 있었습니다.

65
00:02:49,960 --> 00:02:52,160
최대 가능도는 모든

66
00:02:52,160 --> 00:02:54,480
생성 모델링의 핵심 통찰 중

67
00:02:54,480 --> 00:02:57,700
하나로, 종종 생성 모델 훈련을

68
00:02:57,700 --> 00:03:00,820
위한 목표 함수는 우리가

69
00:03:00,820 --> 00:03:04,982
관찰하는 데이터의 가능도를 최대화하는 것입니다.

70
00:03:04,982 --> 00:03:06,940
오늘은 생성 모델에 대한

71
00:03:06,940 --> 00:03:09,780
논의를 계속하고 이 계보의 다른 절반인

72
00:03:09,780 --> 00:03:12,380
암묵적 밀도 모델을 탐구할 것입니다.

73
00:03:12,380 --> 00:03:14,820
암묵적 밀도 모델에서는 더 이상

74
00:03:14,820 --> 00:03:18,100
실제 밀도 값 p(x)에 접근할 수 없지만,

75
00:03:18,100 --> 00:03:21,300
이러한 모델은 확률 분포를 암묵적으로

76
00:03:21,300 --> 00:03:22,160
모델링합니다.

77
00:03:22,160 --> 00:03:24,220
우리는 어떤 데이터 x에 대해서도 밀도

78
00:03:24,220 --> 00:03:27,180
값 p(x)를 계산할 수 없지만, 이러한 모델이

79
00:03:27,180 --> 00:03:29,757
학습한 기본 분포에서 샘플을 추출할 수 있습니다.

80
00:03:29,757 --> 00:03:32,340
따라서 실제 밀도 값을 출력할 수

81
00:03:32,340 --> 00:03:35,940
없더라도 학습된 분포에서 샘플을 추출할 수 있습니다.

82
00:03:35,940 --> 00:03:38,100
우리가 탐구할 첫 번째 모델은

83
00:03:38,100 --> 00:03:42,820
생성적 적대 신경망, 즉 일반적으로 GAN이라고 불리는 것입니다.

84
00:03:42,820 --> 00:03:45,860
GAN을 지금까지 본 변분 오토인코더 및 자기

85
00:03:45,860 --> 00:03:48,220
회귀 모델과 대조하는 것이 유용합니다.

86
00:03:48,220 --> 00:03:50,500
앞서 말했듯이, 자기 회귀

87
00:03:50,500 --> 00:03:52,700
모델은 가능도 기반 방법입니다.

88
00:03:52,700 --> 00:03:54,980
그들의 훈련 목표는 최대 가능도입니다.

89
00:03:54,980 --> 00:03:56,960
따라서 데이터 조각 x에

90
00:03:56,960 --> 00:04:00,060
대한 매개변수화된 함수를 작성하고, 관찰한

91
00:04:00,060 --> 00:04:02,360
데이터에 대해 이를 최대화하여

92
00:04:02,360 --> 00:04:04,820
최대 가능도를 달성합니다.

93
00:04:04,820 --> 00:04:07,840
변분 오토인코더도 p(x)에

94
00:04:07,840 --> 00:04:11,920
대한 근사값을 작성하고 그 근사값을 최대화하는

95
00:04:11,920 --> 00:04:14,720
유사한 아이디어를 따릅니다.

96
00:04:14,720 --> 00:04:16,560
이제 생성적 적대 신경망은 약간

97
00:04:16,560 --> 00:04:18,268
다른 방식으로 작동합니다.

98
00:04:18,268 --> 00:04:20,880
그들은 우리가 방금 말한 것처럼

99
00:04:20,880 --> 00:04:23,280
p(x)를 직접 모델링하는 것을

100
00:04:23,280 --> 00:04:26,100
포기하지만, p(x)를 명시적으로 모델링하지

101
00:04:26,100 --> 00:04:27,600
않더라도 모델이

102
00:04:27,600 --> 00:04:32,520
적합하는 기본 분포에서 샘플을 추출할 수 있는 방법을 제공합니다.

103
00:04:32,520 --> 00:04:34,960
여기서 설정은 우리가 어떤 실제

104
00:04:34,960 --> 00:04:38,240
데이터 분포 pdata에서 추출된 유한한

105
00:04:38,240 --> 00:04:41,120
데이터 샘플 Xi를 가지고

106
00:04:41,120 --> 00:04:42,240
시작하는 것입니다.

107
00:04:42,240 --> 00:04:45,800
우리의 목표는 pdata에서 샘플을 추출할 수 있는

108
00:04:45,800 --> 00:04:48,600
것입니다. pdata는 우주의 실제 분포와

109
00:04:48,600 --> 00:04:49,580
같은 것입니다.

110
00:04:49,580 --> 00:04:52,400
이것은 우주가 데이터 샘플을 제공하는 데

111
00:04:52,400 --> 00:04:53,640
사용하는 분포입니다.

112
00:04:53,640 --> 00:04:55,920
그리고 이는 아마도 매우 복잡한 분포일 것입니다.

113
00:04:55,920 --> 00:04:56,880
물리학이 관련되어 있습니다.

114
00:04:56,880 --> 00:04:58,080
역사와 관련되어 있습니다.

115
00:04:58,080 --> 00:05:00,803
사회정치적 제약이 있을 수 있습니다.

116
00:05:00,803 --> 00:05:02,220
우주에서 발생하는 모든

117
00:05:02,220 --> 00:05:03,595
일에 복잡성이 많이

118
00:05:03,595 --> 00:05:06,660
얽혀 있어 우리가 보는 데이터가 생겨납니다.

119
00:05:06,660 --> 00:05:10,300
우리는 어떤 근사 모델을 맞추고 싶어하며, 이는

120
00:05:10,300 --> 00:05:12,740
가능한 한 진짜 데이터 분포와

121
00:05:12,740 --> 00:05:14,380
일치하도록 하여 우리가

122
00:05:14,380 --> 00:05:16,820
관찰한 원본 데이터 샘플처럼 보이는

123
00:05:16,820 --> 00:05:20,005
새로운 샘플을 추출할 수 있게 합니다.

124
00:05:20,005 --> 00:05:21,380
이를 위해 잠재

125
00:05:21,380 --> 00:05:24,385
변수 z를 도입할 것입니다.

126
00:05:24,385 --> 00:05:26,260
이는 변분 오토인코더에서

127
00:05:26,260 --> 00:05:28,880
본 잠재 변수 z와 유사하게

128
00:05:28,880 --> 00:05:31,900
보이며, 이 잠재 변수 z는 우리가 작성하고

129
00:05:31,900 --> 00:05:34,740
제어할 수 있는 알려진 사전 분포

130
00:05:34,740 --> 00:05:37,517
p(z)에 따라 분포될 것입니다.

131
00:05:37,517 --> 00:05:39,100
보통 이는 단위 가우시안

132
00:05:39,100 --> 00:05:41,893
또는 균일 분포가 되지만, 일반적으로

133
00:05:41,893 --> 00:05:44,060
단위 가우시안, 즉 우리가 샘플링하는

134
00:05:44,060 --> 00:05:45,518
방법과 분석적

135
00:05:45,518 --> 00:05:47,820
특성을 아는 매우 간단한 것입니다.

136
00:05:47,820 --> 00:05:49,460
이제 설정은 우리가

137
00:05:49,460 --> 00:05:53,320
네트워크가 모델링할 데이터 생성 프로세스를 상상하는

138
00:05:53,320 --> 00:05:54,200
것입니다.

139
00:05:54,200 --> 00:05:58,520
여기서 우리는 알려진 분포 p(z)에 따라 z를

140
00:05:58,520 --> 00:06:01,840
샘플링하여 샘플 z를 얻고, 그 샘플 z를

141
00:06:01,840 --> 00:06:05,920
생성기 네트워크 g(z)를 통해 전달할 것입니다.

142
00:06:05,920 --> 00:06:09,480
그런 다음 x는 어떤 생성기 분포 P_G에서의

143
00:06:09,480 --> 00:06:11,280
샘플이 될 것입니다.

144
00:06:11,280 --> 00:06:13,880
생성기 네트워크의 매개변수나 아키텍처

145
00:06:13,880 --> 00:06:16,580
또는 훈련을 변경함에 따라, 이는

146
00:06:16,580 --> 00:06:18,920
이 P_G 분포에서 샘플링하는

147
00:06:18,920 --> 00:06:21,600
다양한 종류의 분포를 유도할 것입니다.

148
00:06:21,600 --> 00:06:23,400
GAN 훈련의 전체 목표는

149
00:06:23,400 --> 00:06:26,480
생성기 네트워크에 의해 유도된 이 P_G 분포를

150
00:06:26,480 --> 00:06:27,820
강제로 맞추는 것입니다.

151
00:06:27,820 --> 00:06:30,520
우리는 이 P_G 분포가 진짜 P 데이터 분포와

152
00:06:30,520 --> 00:06:32,540
가능한 한 가깝게 일치하기를 원합니다.

153
00:06:32,540 --> 00:06:35,840
만약 일치한다면, 우리는 z를 샘플링하고

154
00:06:35,840 --> 00:06:38,160
이를 생성기를 통과시켜

155
00:06:38,160 --> 00:06:40,200
P 데이터와 매우 유사한

156
00:06:40,200 --> 00:06:43,127
샘플 데이터를 얻을 수 있습니다.

157
00:06:43,127 --> 00:06:45,460
이 그림은 다음과 같습니다.

158
00:06:45,460 --> 00:06:48,040
우리는 p(z)에서 z를 샘플링하여 구체적인

159
00:06:48,040 --> 00:06:51,220
잠재 z를 얻고, 이를 생성기 G를 통해 전달하여

160
00:06:51,220 --> 00:06:53,260
생성된 이미지를 얻을 것입니다.

161
00:06:53,260 --> 00:06:55,100
생성기 네트워크는

162
00:06:55,100 --> 00:06:58,660
기본적으로 알려진 분포 z에서 데이터 분포의

163
00:06:58,660 --> 00:07:01,620
샘플로 변환하도록 훈련됩니다.

164
00:07:01,620 --> 00:07:04,720
하지만 이제 질문은 이러한 출력을 어떻게 강제로 맞출 수 있는가입니다.

165
00:07:04,720 --> 00:07:07,620
어떻게 유도된 생성기 분포 P_G를 강제로 맞출 수 있는가?

166
00:07:07,620 --> 00:07:11,180
어떻게 데이터 분포 p_data와 일치하도록 강제할 수 있는가?

167
00:07:11,180 --> 00:07:13,540
생성적 적대 신경망의 요령은 이

168
00:07:13,540 --> 00:07:16,100
작업을 수행하기 위해 또 다른 신경망을

169
00:07:16,100 --> 00:07:17,580
도입하는 것입니다.

170
00:07:17,580 --> 00:07:21,802
이전의 생성 모델인 VAE와 자기 회귀 모델에서는

171
00:07:21,802 --> 00:07:23,260
데이터 분포와

172
00:07:23,260 --> 00:07:24,802
맞추기 위해 최소화할

173
00:07:24,802 --> 00:07:27,580
수 있는 어떤 목적 함수를

174
00:07:27,580 --> 00:07:29,120
작성하려고 했습니다.

175
00:07:29,120 --> 00:07:30,980
여기서는 그 제어를 포기하고

176
00:07:30,980 --> 00:07:33,180
기본적으로 다른 신경망에게 그

177
00:07:33,180 --> 00:07:35,260
작업을 해결하도록 요청할 것입니다.

178
00:07:35,260 --> 00:07:38,100
특히, 우리는 판별기 D라는 또

179
00:07:38,100 --> 00:07:40,740
다른 신경망을 훈련할 것입니다. 이 판별기는 이미지를 입력받아,

180
00:07:40,740 --> 00:07:44,860
때로는 실제 이미지, 때로는 가짜 이미지를

181
00:07:44,860 --> 00:07:46,800
입력받아야 합니다.

182
00:07:46,800 --> 00:07:49,840
그리고 그 이미지를 가짜인지 진짜인지

183
00:07:49,840 --> 00:07:51,280
분류할 것입니다.

184
00:07:51,280 --> 00:07:55,303
그리고 이 두 네트워크가 싸울 것이라는 아이디어입니다.

185
00:07:55,303 --> 00:07:56,720
우리는 생성기를

186
00:07:56,720 --> 00:07:58,095
훈련시켜 판별기를 속이도록

187
00:07:58,095 --> 00:07:59,845
하고, 판별기를 분류

188
00:07:59,845 --> 00:08:02,480
모델로 훈련시켜 실제 데이터와 가짜

189
00:08:02,480 --> 00:08:05,300
데이터를 올바르게 구별하도록 할 것입니다.

190
00:08:05,300 --> 00:08:08,520
직관적으로 이 두 네트워크가 싸우면

191
00:08:08,520 --> 00:08:10,580
판별기가 더 나아질 것입니다.

192
00:08:10,580 --> 00:08:11,580
정말 잘하게 될 것입니다.

193
00:08:11,580 --> 00:08:13,163
판별기는 실제 데이터와 가짜

194
00:08:13,163 --> 00:08:16,318
데이터의 특징을 결정하는 데 매우 능숙해질 것입니다.

195
00:08:16,318 --> 00:08:18,360
그리고 일단 판별기가 정말

196
00:08:18,360 --> 00:08:20,480
잘하게 되면, 생성된 샘플이

197
00:08:20,480 --> 00:08:23,220
진짜로 분류된다고 생각하게 만들기 위해

198
00:08:23,220 --> 00:08:25,480
생성기는 진짜 데이터처럼 보이는

199
00:08:25,480 --> 00:08:28,257
샘플을 점점 더 가까이 생성해야 합니다.

200
00:08:28,257 --> 00:08:30,840
그래서 이것이 생성적 적대

201
00:08:30,840 --> 00:08:32,159
신경망의 직관입니다.

202
00:08:32,159 --> 00:08:34,497
질문은 생성기 네트워크가 판별기로부터

203
00:08:34,497 --> 00:08:36,080
올바르게 분류하고 있는지에 대한

204
00:08:36,080 --> 00:08:37,440
피드백을 받는가입니다.

205
00:08:37,440 --> 00:08:39,880
네, 그리고 이것은 이 전체 과정이 작동하는 데 중요합니다.

206
00:08:39,880 --> 00:08:42,870
그리고 그 피드백의 유형은 그래디언트입니다.

207
00:08:42,870 --> 00:08:45,080
이 전체 시스템, 즉 생성기와

208
00:08:45,080 --> 00:08:47,140
판별기의 조합은 단지

209
00:08:47,140 --> 00:08:48,240
신경망입니다.

210
00:08:48,240 --> 00:08:50,157
우리는 그것을 통해 그래디언트를 계산하는 방법을

211
00:08:50,157 --> 00:08:52,440
알고 있으며, 그것들은 생성된 이미지를 통해 소통합니다.

212
00:08:52,440 --> 00:08:54,898
그래서 우리는 판별기에서 생성기로 생성된

213
00:08:54,898 --> 00:08:57,480
이미지를 통해 역전파를 할 것입니다.

214
00:08:57,480 --> 00:08:59,355
그래서 이것이 생성기가

215
00:08:59,355 --> 00:09:01,420
판별기로부터 배우는 방법입니다.

216
00:09:01,420 --> 00:09:03,380
그리고 더 구체적으로 우리는

217
00:09:03,380 --> 00:09:05,860
이 직관을 구체화하기 위해 실제

218
00:09:05,860 --> 00:09:08,580
방정식, 실제 수학을 작성해야 합니다.

219
00:09:08,580 --> 00:09:10,660
특히, 우리는 이 미니맥스

220
00:09:10,660 --> 00:09:13,100
게임으로 생성기 G와 판별기

221
00:09:13,100 --> 00:09:15,820
D를 공동으로 훈련할 것입니다.

222
00:09:15,820 --> 00:09:17,880
이 방정식은 아마도 조금 위협적으로

223
00:09:17,880 --> 00:09:20,780
보일 수 있으므로 각 항목을 하나씩 살펴보겠습니다.

224
00:09:20,780 --> 00:09:22,500
여기서 우리는 색상을

225
00:09:22,500 --> 00:09:25,200
코드화하여 생성기는 파란색, 판별기는

226
00:09:25,200 --> 00:09:27,260
빨간색으로 표시할 것입니다.

227
00:09:27,260 --> 00:09:29,980
그리고 판별기는 데이터 조각 x를 입력으로

228
00:09:29,980 --> 00:09:32,260
받아 그 데이터가 진짜일 확률을

229
00:09:32,260 --> 00:09:34,340
출력하는 함수가 될 것입니다.

230
00:09:34,340 --> 00:09:36,397
특히, D(x) = 0은

231
00:09:36,397 --> 00:09:38,980
판별기가 데이터 조각 x를 가짜로

232
00:09:38,980 --> 00:09:40,340
분류했음을 의미합니다.

233
00:09:40,340 --> 00:09:43,300
D(x) = 1은 판별기가 데이터 조각을 진짜로

234
00:09:43,300 --> 00:09:44,835
분류했음을 의미합니다.

235
00:09:44,835 --> 00:09:46,460
물론, 그것들은 극단적인 경우입니다.

236
00:09:46,460 --> 00:09:49,000
실제로 판별기는 이 두 결정

237
00:09:49,000 --> 00:09:53,960
사이의 부드러운 버전을 제공하는 확률을 출력할 것입니다.

238
00:09:53,960 --> 00:09:56,357
이제 생성기 G를 고정한다고 상상해 보십시오.

239
00:09:56,357 --> 00:09:58,440
그리고 판별기의 관점에서 이 문제를

240
00:09:58,440 --> 00:09:59,560
상상해 보십시오.

241
00:09:59,560 --> 00:10:01,720
그래서 판별기의 관점에서 여기에는 두

242
00:10:01,720 --> 00:10:03,040
개의 항이 있습니다.

243
00:10:03,040 --> 00:10:06,880
하나는, 이 첫 번째 항은 판별기가 진짜 데이터에 대해

244
00:10:06,880 --> 00:10:09,140
D(x) = 1을 원한다고 말합니다.

245
00:10:09,140 --> 00:10:11,920
D(x) = 1은 판별기가 그것이 진짜라고 말하는

246
00:10:11,920 --> 00:10:13,280
것을 기억하십시오.

247
00:10:13,280 --> 00:10:15,880
그리고 이 기대값은 우리가 진짜

248
00:10:15,880 --> 00:10:19,440
pdata 분포에서 데이터 샘플 x를 뽑을 것이라고

249
00:10:19,440 --> 00:10:20,300
말합니다.

250
00:10:20,300 --> 00:10:22,500
우리는 그것들을 판별기를 통해 통과시키고,

251
00:10:22,500 --> 00:10:25,240
확률로 작업할 때 거의 항상 로그 공간에서 작업하기

252
00:10:25,240 --> 00:10:26,700
때문에 로그를 취할 것입니다.

253
00:10:26,700 --> 00:10:29,140
그리고 로그는 단조 함수라는 것을 기억하십시오.

254
00:10:29,140 --> 00:10:34,000
그래서 log(x)를 최대화하는 것은 x를 최대화하는 것과 같습니다.

255
00:10:34,000 --> 00:10:37,720
따라서 이 경우, 이는 우리가 진짜 데이터에 대해 log(D(x))를

256
00:10:37,720 --> 00:10:40,920
최대화하고 싶다는 것을 의미하며, 이는 진짜

257
00:10:40,920 --> 00:10:44,180
데이터에 대해 D(x) = 1이라고 말하는 것과 같습니다.

258
00:10:44,180 --> 00:10:46,100
이제 반대편에서는 우리가

259
00:10:46,100 --> 00:10:49,540
알려진 사전 p(z)에 따라 잠재

260
00:10:49,540 --> 00:10:50,740
변수 z를

261
00:10:50,740 --> 00:10:52,980
샘플링하여 기대값을 취할

262
00:10:52,980 --> 00:10:54,180
것이라고 말합니다.

263
00:10:54,180 --> 00:10:55,900
우리는 그 z들을 가져와서

264
00:10:55,900 --> 00:10:58,660
생성기를 통과시켜 생성된 데이터 샘플을

265
00:10:58,660 --> 00:11:01,100
얻고, 그 생성된 데이터 샘플을 판별기를

266
00:11:01,100 --> 00:11:02,740
통해 통과시킬 것입니다.

267
00:11:02,740 --> 00:11:04,860
이제 판별기는 이것들을 가짜 샘플로

268
00:11:04,860 --> 00:11:06,267
분류하고 싶어합니다.

269
00:11:06,267 --> 00:11:08,100
그래서 판별기는 이것들을

270
00:11:08,100 --> 00:11:10,180
가짜로 분류하고 싶어하므로 왼쪽

271
00:11:10,180 --> 00:11:12,380
표현을 어떻게든 반전시켜야 합니다.

272
00:11:12,380 --> 00:11:15,880
그래서 여기서 우리는 D의 x가 0일 때 가짜가 되기를 원합니다.

273
00:11:15,880 --> 00:11:19,820
그래서 이를 표현하는 한 가지 방법은 1에서 D의 G의 z를 뺀

274
00:11:19,820 --> 00:11:21,330
로그를 최대화하는 것입니다.

275
00:11:21,330 --> 00:11:23,580
오른쪽의 이 항은 판별기가 가짜 데이터에 대해

276
00:11:23,580 --> 00:11:25,778
D의 x가 0이기를 원한다는 것을 나타냅니다.

277
00:11:25,778 --> 00:11:27,820
왼쪽의 항은 판별기가 실제 데이터에 대해

278
00:11:27,820 --> 00:11:30,192
D의 x가 1이기를 원한다는 것을 나타냅니다.

279
00:11:30,192 --> 00:11:32,400
좋습니다, 그래서 판별기가 하려는 것이 바로 이것입니다.

280
00:11:32,400 --> 00:11:34,420
판별기는 생성된 샘플과

281
00:11:34,420 --> 00:11:38,740
데이터 세트의 실제 샘플 간의 분류 작업을 올바르게 수행하려고

282
00:11:38,740 --> 00:11:40,360
하고, 이를 실제

283
00:11:40,360 --> 00:11:43,560
또는 가짜로 올바르게 분류하려고 합니다.

284
00:11:43,560 --> 00:11:47,280
이제 생성기의 관점에서 이것을 살펴보겠습니다.

285
00:11:47,280 --> 00:11:48,915
판별기를 고정하고

286
00:11:48,915 --> 00:11:51,040
생성기의 관점에서만 이 설정을

287
00:11:51,040 --> 00:11:53,640
살펴보는 것을 상상해 보세요.

288
00:11:53,640 --> 00:11:55,360
이 경우 첫 번째 항은

289
00:11:55,360 --> 00:11:57,220
판별기와 전혀 관련이 없는데,

290
00:11:57,220 --> 00:12:00,040
첫 번째 항은 판별기가 실제 데이터 샘플을

291
00:12:00,040 --> 00:12:02,760
올바르게 분류하는 것에 관한 것이었습니다.

292
00:12:02,760 --> 00:12:06,320
따라서 생성기는 오른쪽의 이 항만 신경 쓰면 됩니다.

293
00:12:06,320 --> 00:12:08,360
직관적으로, 우리는 생성기가 판별기를

294
00:12:08,360 --> 00:12:10,040
속여서 자신의 샘플이 실제라고

295
00:12:10,040 --> 00:12:11,560
생각하게 만들기를 원합니다.

296
00:12:11,560 --> 00:12:14,200
즉, 생성기는 가짜 데이터에 대해

297
00:12:14,200 --> 00:12:16,600
D의 x가 1이기를 원합니다.

298
00:12:16,600 --> 00:12:17,880
그래서 항은 동일합니다.

299
00:12:17,880 --> 00:12:20,560
우리는 p의 z에 따라 샘플 z를 뽑고,

300
00:12:20,560 --> 00:12:22,945
생성기를 통해 생성된 샘플을

301
00:12:22,945 --> 00:12:24,320
얻고, 판별기를 통해

302
00:12:24,320 --> 00:12:26,920
그 샘플에 대한 판별기의 예측 확률을

303
00:12:26,920 --> 00:12:27,630
얻습니다.

304
00:12:30,180 --> 00:12:33,200
이제 생성기가 D의 x가 1이기를 원한다는 것을 기억하세요.

305
00:12:33,200 --> 00:12:36,840
그래서 판별기가 원했던 것처럼 이것을

306
00:12:36,840 --> 00:12:39,360
최대화하는 대신, 우리는 생성기의

307
00:12:39,360 --> 00:12:42,980
관점에서 이것을 최소화하려고 합니다.

308
00:12:42,980 --> 00:12:45,500
그것이 바로 이 미니맥스 게임을 제공합니다.

309
00:12:45,500 --> 00:12:47,860
특히, 우리는 모든 수학을

310
00:12:47,860 --> 00:12:50,980
추상화하여 G와 D의 함수로서 어떤 스칼라

311
00:12:50,980 --> 00:12:53,540
함수 v로 작성할 수 있습니다. 그런 다음 판별기는 v를

312
00:12:53,540 --> 00:12:56,540
최대화하려고 하고, 생성기는 v를

313
00:12:56,540 --> 00:12:59,060
최소화하려고 하며, 그들은 그런

314
00:12:59,060 --> 00:13:01,660
방식으로 서로 싸울 것입니다.

315
00:13:01,660 --> 00:13:04,780
그리고 이를 최적화하기 위해 우리는 기본적으로

316
00:13:04,780 --> 00:13:07,620
교대로 최소화하고 최대화하려고 하면서 생성기와

317
00:13:07,620 --> 00:13:09,500
판별기의 매개변수에 대해

318
00:13:09,500 --> 00:13:11,740
경량 하강 루프를 실행할 것입니다.

319
00:13:11,740 --> 00:13:14,460
그래서 무한 루프를 돌면서, D를

320
00:13:14,460 --> 00:13:18,700
업데이트하고 v의 D에 대한 도함수에 따라 경량 하강 단계를

321
00:13:18,700 --> 00:13:21,800
취한 다음, 그 경량을 더합니다.

322
00:13:21,800 --> 00:13:24,400
왜냐하면 판별기는 이것을 최대화하려고

323
00:13:24,400 --> 00:13:29,180
하므로, 우리는 이 항을 최대화하려고 경량 상승을 해야 합니다.

324
00:13:29,180 --> 00:13:32,137
그리고 판별기 가중치를 업데이트한 후,

325
00:13:32,137 --> 00:13:34,220
생성기 가중치에 대해 V의

326
00:13:34,220 --> 00:13:38,700
생성기 가중치 G에 대한 도함수를 취하여 업데이트를 수행하고,

327
00:13:38,700 --> 00:13:42,200
이제 V에 대해 경량 하강 단계를 취합니다.

328
00:13:42,200 --> 00:13:46,320
왜냐하면 생성기는 그 목표를 최소화하려고 하니까요.

329
00:13:46,320 --> 00:13:47,900
그래서 기본적으로 우리의 훈련입니다.

330
00:13:47,900 --> 00:13:49,483
이것이 바로 우리가 생성적 적대

331
00:13:49,483 --> 00:13:50,840
신경망을 훈련하는 방식입니다.

332
00:13:50,840 --> 00:13:53,120
우리는 이 미니맥스 게임의 값인

333
00:13:53,120 --> 00:13:54,755
V를 가지고 있습니다.

334
00:13:54,755 --> 00:13:56,880
그리고 우리는 그 목표

335
00:13:56,880 --> 00:14:03,040
V에 대해 교대로 경량 상승과 경량 하강을 수행하여 생성기와 판별기를

336
00:14:03,040 --> 00:14:05,498
번갈아 업데이트할 것입니다.

337
00:14:05,498 --> 00:14:07,040
생성적 적대 신경망을

338
00:14:07,040 --> 00:14:09,840
훈련할 때 정말 중요한 점은 이

339
00:14:09,840 --> 00:14:12,640
V가 손실 함수가 아니라는 것입니다.

340
00:14:12,640 --> 00:14:16,240
즉, V의 절대값은 생성기와 판별기가 이

341
00:14:16,240 --> 00:14:18,840
문제를 얼마나 잘 해결하고 있는지

342
00:14:18,840 --> 00:14:22,080
또는 우리가 정말로 신경 쓰는 것은 유도된

343
00:14:22,080 --> 00:14:23,840
PG 분포가 데이터

344
00:14:23,840 --> 00:14:27,160
분포와 얼마나 잘 일치하는지를 알려주지

345
00:14:27,160 --> 00:14:28,020
않습니다.

346
00:14:28,020 --> 00:14:29,520
V의 값만 보고는 그에

347
00:14:29,520 --> 00:14:31,840
대해 아무것도 알 수 없습니다.

348
00:14:31,840 --> 00:14:34,840
왜냐하면 V의 값은 판별기가 얼마나 좋은지에

349
00:14:34,840 --> 00:14:36,560
따라 달라지기 때문입니다.

350
00:14:36,560 --> 00:14:38,195
판별기가 정말 나쁘면

351
00:14:38,195 --> 00:14:39,820
생성기가 이를 속이고

352
00:14:39,820 --> 00:14:42,200
좋은 숫자를 얻기가 정말 쉽고,

353
00:14:42,200 --> 00:14:43,920
판별기가 정말 좋으면 생성기도

354
00:14:43,920 --> 00:14:45,640
정말 잘해야 합니다.

355
00:14:45,640 --> 00:14:49,140
따라서 D와 G의 서로 다른 설정이 V에 대해 정확히

356
00:14:49,140 --> 00:14:51,380
동일한 값을 초래할 수 있습니다.

357
00:14:51,380 --> 00:14:54,100
즉, 생성적 적대 신경망은 종종 훈련하기

358
00:14:54,100 --> 00:14:57,220
매우 어렵고 훈련이 잘 되고 있는지 알기조차

359
00:14:57,220 --> 00:14:58,985
어렵다는 것을 의미합니다.

360
00:14:58,985 --> 00:15:01,360
신경망을 훈련할 때는 일반적으로 손실이 있습니다.

361
00:15:01,360 --> 00:15:02,860
손실을 네트워크의 매개변수에

362
00:15:02,860 --> 00:15:04,278
대해 최소화하려고 하고,

363
00:15:04,278 --> 00:15:05,820
훈련 과정에서 손실이

364
00:15:05,820 --> 00:15:07,380
줄어드는 것을 보고 싶지만, 생성적

365
00:15:07,380 --> 00:15:09,680
적대 신경망에서는 그런 것이 없습니다.

366
00:15:09,680 --> 00:15:10,720
생성기 손실이 있습니다.

367
00:15:10,720 --> 00:15:12,053
판별기 손실이 있습니다.

368
00:15:12,053 --> 00:15:14,660
이들을 플롯할 수는 있지만, 일반적으로 그들은

369
00:15:14,660 --> 00:15:15,813
의미가 없습니다.

370
00:15:15,813 --> 00:15:17,480
그래서 생성적 적대 신경망은

371
00:15:17,480 --> 00:15:19,540
훈련하기가 정말 어렵습니다.

372
00:15:19,540 --> 00:15:23,640
하나는 이 목표가 근본적으로 불안정하다는 것입니다.

373
00:15:23,640 --> 00:15:26,340
네트워크의 서로 다른 매개변수 집합에

374
00:15:26,340 --> 00:15:29,408
대해 같은 양을 동시에 최대화하고

375
00:15:29,408 --> 00:15:31,700
최소화하려고 하므로 본질적으로 어려운

376
00:15:31,700 --> 00:15:33,080
최적화 문제입니다.

377
00:15:33,080 --> 00:15:35,420
더 나쁜 것은, 좋은 솔루션을

378
00:15:35,420 --> 00:15:38,080
향해 진행하고 있는지 확인할 수 있는

379
00:15:38,080 --> 00:15:39,560
값이 없다는 것입니다.

380
00:15:39,560 --> 00:15:41,400
그래서 생성적 적대 신경망은

381
00:15:41,400 --> 00:15:44,000
꽤 효과적이지만, 훈련하기가 정말

382
00:15:44,000 --> 00:15:48,200
어렵고 조정하기가 정말 어렵고 진전을 이루기가 정말 어렵습니다.

383
00:15:48,200 --> 00:15:52,340
그래서 그것이 생성적 적대 신경망에 대한 주요 요점입니다.

384
00:15:52,340 --> 00:15:55,120
GAN을 훈련할 때 유용하게

385
00:15:55,120 --> 00:15:57,800
생각할 수 있는 작은

386
00:15:57,800 --> 00:16:00,560
요령이 하나 있습니다.

387
00:16:00,560 --> 00:16:03,040
훈련의 시작 부분에서 생성기가 무작위로

388
00:16:03,040 --> 00:16:05,300
초기화되고 판별기도 무작위로

389
00:16:05,300 --> 00:16:07,180
초기화된다고 상상해 보세요.

390
00:16:07,180 --> 00:16:08,200
무슨 일이 일어날까요?

391
00:16:08,200 --> 00:16:10,680
훈련의 시작 부분에서 생성기는 완전히

392
00:16:10,680 --> 00:16:12,658
무작위 노이즈를 생성합니다.

393
00:16:12,658 --> 00:16:14,200
그 완전히 무작위 노이즈는

394
00:16:14,200 --> 00:16:16,303
실제 이미지와 매우 다르게 보일 것입니다.

395
00:16:16,303 --> 00:16:18,720
훈련의 시작 부분에서 생성기가 형편없을

396
00:16:18,720 --> 00:16:21,720
때, 판별기는 매우 쉬운 문제를 가지고 있습니다.

397
00:16:21,720 --> 00:16:23,920
따라서 일반적으로 몇 번의 반복

398
00:16:23,920 --> 00:16:25,720
내에 판별기는 실제

399
00:16:25,720 --> 00:16:28,240
이미지와 무작위 생성기가 제공하는 완전히

400
00:16:28,240 --> 00:16:32,320
쓰레기 같은 가짜 이미지를 즉시 구별할 수 있습니다.

401
00:16:32,320 --> 00:16:35,603
즉, 훈련의 시작 부분에서

402
00:16:35,603 --> 00:16:37,020
판별기는

403
00:16:37,020 --> 00:16:41,180
실제와 가짜를 빠르게 구분하는 법을

404
00:16:41,180 --> 00:16:43,060
배우게 됩니다.

405
00:16:43,060 --> 00:16:47,340
그렇다면 D of G of z는 무엇인지 플롯하는 것이 흥미롭습니다.

406
00:16:47,340 --> 00:16:52,740
죄송합니다, D of G of z의 함수로서 이 항은 무엇인지요?

407
00:16:52,740 --> 00:16:55,220
이는 기본적으로 생성기의

408
00:16:55,220 --> 00:16:57,400
관점에서 손실 함수입니다.

409
00:16:57,400 --> 00:17:00,060
따라서 생성기의 관점에서

410
00:17:00,060 --> 00:17:02,420
우리는 훈련의 시작

411
00:17:02,420 --> 00:17:04,220
부분에 있습니다.

412
00:17:04,220 --> 00:17:06,500
판별기는 훈련의 시작 부분에서

413
00:17:06,500 --> 00:17:09,609
생성된 샘플을 가짜로 잘 분류하고

414
00:17:09,609 --> 00:17:11,859
있으며, 이는 판별기의

415
00:17:11,859 --> 00:17:13,506
관점에서 손실 함수가

416
00:17:13,506 --> 00:17:15,339
대략 이런 모양이라는

417
00:17:15,339 --> 00:17:16,817
것을 의미합니다.

418
00:17:16,817 --> 00:17:18,900
그리고 주목할 점은, 이 손실

419
00:17:18,900 --> 00:17:21,819
함수는 생성기가 매개변수를 최적화하려고 하는

420
00:17:21,819 --> 00:17:24,720
지점에서 평평하거나 매우 평평하다는 것입니다.

421
00:17:24,720 --> 00:17:27,660
즉, 실제로 이 단순한 목표를 사용하여

422
00:17:27,660 --> 00:17:29,905
GAN을 훈련할 때, 생성기는

423
00:17:29,905 --> 00:17:31,780
훈련의 시작 부분에서

424
00:17:31,780 --> 00:17:33,800
배우기가 정말 어렵습니다.

425
00:17:33,800 --> 00:17:36,340
좋은 질문은 데이터 세트를 어떻게 구성하느냐입니다.

426
00:17:36,340 --> 00:17:40,080
유니콘이 존재하지 않으면 유니콘의 사진을 어떻게 생성하나요?

427
00:17:40,080 --> 00:17:43,737
이 pdata는 당신이 선택한 pdata입니다.

428
00:17:43,737 --> 00:17:46,320
훈련 세트에서 조합한 데이터 세트가 무엇이든,

429
00:17:46,320 --> 00:17:48,480
그것이 당신이 모델링하려고 하는 pdata를

430
00:17:48,480 --> 00:17:49,740
선택하는 것입니다.

431
00:17:49,740 --> 00:17:51,960
일반적으로, 당신이 전에 본 적이

432
00:17:51,960 --> 00:17:53,760
없는 것처럼 보이는 샘플을

433
00:17:53,760 --> 00:17:56,460
생성하고 싶다면, 운이 좋지 않습니다.

434
00:17:56,460 --> 00:17:57,680
그것은 일어나지 않을 것입니다.

435
00:17:57,680 --> 00:18:00,200
따라서 일반적으로 샘플을 그리려면

436
00:18:00,200 --> 00:18:02,320
훈련 데이터 세트에 그런 것과

437
00:18:02,320 --> 00:18:04,080
비슷한 것이 있어야 합니다.

438
00:18:04,080 --> 00:18:07,360
모든 생성 모델과 모든 신경망은 실제로

439
00:18:07,360 --> 00:18:09,380
조금 일반화합니다.

440
00:18:09,380 --> 00:18:11,800
그래서 희망은 아마도 당신이 산타

441
00:18:11,800 --> 00:18:14,800
모자를 쓴 유니콘의 포토리얼리스틱 이미지를 본

442
00:18:14,800 --> 00:18:17,920
적이 없지만, 말의 포토리얼리스틱 이미지를 보았고,

443
00:18:17,920 --> 00:18:20,280
산타 모자의 포토리얼리스틱

444
00:18:20,280 --> 00:18:22,300
이미지를 보았으며, 유니콘의 그림을

445
00:18:22,300 --> 00:18:24,600
보았고, 말의 그림을 보았기 때문에,

446
00:18:24,600 --> 00:18:26,240
당신이 생성하고자 하는

447
00:18:26,240 --> 00:18:28,400
속성의 정확한 조합을 본 적이

448
00:18:28,400 --> 00:18:30,200
없더라도, 모델이 일반화하여

449
00:18:30,200 --> 00:18:32,180
새로운 것을 제공할 수 있을

450
00:18:32,180 --> 00:18:34,640
만큼 충분한 것을 보았다는 것입니다.

451
00:18:34,640 --> 00:18:36,377
그리고 그것이 항상 여기서의 희망입니다.

452
00:18:36,377 --> 00:18:38,960
이것이 판별자의 관점에서 어떻게 보이나요?

453
00:18:38,960 --> 00:18:41,180
그래서 만약 제가 산타 모자를 쓴

454
00:18:41,180 --> 00:18:43,740
포토리얼리스틱 유니콘 이미지를 생성했다면,

455
00:18:43,740 --> 00:18:46,118
모든 질감이 정말 완벽하고, 조명이

456
00:18:46,118 --> 00:18:48,160
완벽하고, 그림자가 완벽하고,

457
00:18:48,160 --> 00:18:49,580
나뭇잎이 완벽하므로, 그

458
00:18:49,580 --> 00:18:53,020
샘플 자체에서 이것이 명백히 잘못되었다고 말할 수 있는

459
00:18:53,020 --> 00:18:54,553
실제 증거는 없습니다.

460
00:18:54,553 --> 00:18:56,720
아마도 판별자가 정말 똑똑하다면, 판별자는

461
00:18:56,720 --> 00:18:58,220
유니콘이 실제로 존재하지 않는다는

462
00:18:58,220 --> 00:18:59,975
것을 알 수 있을 것이고, 그들의

463
00:18:59,975 --> 00:19:02,100
완벽한 포토리얼리스틱 이미지를 갖는 것은 일어날

464
00:19:02,100 --> 00:19:05,460
가능성이 낮다는 것을 알 수 있을 것입니다. 하지만 이는 해결하기

465
00:19:05,460 --> 00:19:06,740
어려운 의미론적 문제입니다.

466
00:19:06,740 --> 00:19:08,660
그래서 실제로 판별자는 그렇게

467
00:19:08,660 --> 00:19:10,660
똑똑하지 않은 경향이 있습니다.

468
00:19:10,660 --> 00:19:11,660
네, 좋은 질문입니다.

469
00:19:11,660 --> 00:19:13,758
아이디어는 두 곡선을 살펴보자는 것입니다.

470
00:19:13,758 --> 00:19:15,300
하나는 판별자가 얼마나 좋은지를 나타내는

471
00:19:15,300 --> 00:19:16,560
곡선을 살펴보자는 것입니다.

472
00:19:16,560 --> 00:19:18,102
하나는 생성자가 얼마나 좋은지를 나타내는

473
00:19:18,102 --> 00:19:19,280
곡선을 살펴보자는 것입니다.

474
00:19:19,280 --> 00:19:20,280
자유롭게 그려보세요.

475
00:19:20,280 --> 00:19:22,140
그들은 정말 쓸모없어 보이는 경향이 있습니다.

476
00:19:22,140 --> 00:19:24,045
이 문제를 해결하고 GAN

477
00:19:24,045 --> 00:19:26,420
목표를 어떻게 조정할지를 알아내려는

478
00:19:26,420 --> 00:19:28,800
연구 논문이 수백 편 있을 것입니다.

479
00:19:28,800 --> 00:19:30,000
로그를 사용하지 않으려면 어떻게 해야 할까요?

480
00:19:30,000 --> 00:19:33,000
Wasserstein 무언가를 어떻게 사용해야 할까요?

481
00:19:33,000 --> 00:19:35,120
이 곡선을 더 해석 가능하게 만들기

482
00:19:35,120 --> 00:19:37,000
위해 온갖 미친 것을 넣어보세요.

483
00:19:37,000 --> 00:19:40,200
수백 편의 논문이 작성되었고, 수천 명의 사람들이 5년 동안

484
00:19:40,200 --> 00:19:42,040
시간을 보냈지만, 좋은 해결책을

485
00:19:42,040 --> 00:19:43,760
찾은 사람은 없다고 생각합니다.

486
00:19:43,760 --> 00:19:47,120
그래서 여전히 수백, 수천 편의 논문이 GAN을

487
00:19:47,120 --> 00:19:49,920
훈련시키고 있지만, 많은 사람들이

488
00:19:49,920 --> 00:19:51,520
여전히 매우 해석 가능하지

489
00:19:51,520 --> 00:19:53,687
않은 이 바닐라 공식을

490
00:19:53,687 --> 00:19:55,280
사용하는 경향이 있습니다.

491
00:19:55,280 --> 00:19:58,000
질문은 훈련 초기에 판별자에게 무슨 일이 일어나는지가

492
00:19:58,000 --> 00:19:59,540
정말 중요하다는 것입니다.

493
00:19:59,540 --> 00:20:00,960
그리고 대답은 아닙니다.

494
00:20:00,960 --> 00:20:02,520
왜냐하면 이것은 우리가 이전에

495
00:20:02,520 --> 00:20:04,520
본 어떤 분류 문제와도 다르기 때문입니다.

496
00:20:04,520 --> 00:20:06,360
이것은 비정상적인 분포입니다.

497
00:20:06,360 --> 00:20:08,720
ImageNet이나 CIFAR와 같은 데이터

498
00:20:08,720 --> 00:20:10,840
세트에서 이미지 분류기를 훈련할 때, 데이터 세트는

499
00:20:10,840 --> 00:20:12,640
고정되어 있고 모델은 그 정적 데이터

500
00:20:12,640 --> 00:20:14,240
세트를 잘 분류하려고 합니다.

501
00:20:14,240 --> 00:20:16,200
하지만 GAN 훈련의 경우, 모델이

502
00:20:16,200 --> 00:20:18,560
맞추려고 하는 데이터 세트는 훈련

503
00:20:18,560 --> 00:20:20,960
과정에서 변화합니다. 훈련 초기에 생성된

504
00:20:20,960 --> 00:20:23,520
이미지가 정말 나쁘게 보일 수 있지만,

505
00:20:23,520 --> 00:20:25,522
문제를 해결하기 쉽습니다. 그러나

506
00:20:25,522 --> 00:20:26,980
생성기가 더 좋아집니다.

507
00:20:26,980 --> 00:20:29,100
이제 판별자가 구별하려고

508
00:20:29,100 --> 00:20:31,300
하는 데이터 세트가 훈련

509
00:20:31,300 --> 00:20:32,580
과정에서 변화합니다.

510
00:20:32,580 --> 00:20:34,580
그래서 이것은 비정상적인

511
00:20:34,580 --> 00:20:38,100
문제이며 매우 복잡한 학습 동역학을 가지고 있습니다.

512
00:20:38,100 --> 00:20:40,140
네, 좋은 질문입니다.

513
00:20:40,140 --> 00:20:41,640
이것들이 지역 최소값에 갇히나요?

514
00:20:41,640 --> 00:20:43,598
지역 최소값에서 이들을 끌어내는 방법이 있나요?

515
00:20:43,598 --> 00:20:45,060
잠시 훈련하고 끌어내는 방법이 있나요?

516
00:20:45,060 --> 00:20:47,720
다시 말하지만, 수백 편, 수천 편의 논문, 많은

517
00:20:47,720 --> 00:20:50,980
휴리스틱이 있지만, 아무것도 정말로 효과가 있지 않았습니다.

518
00:20:50,980 --> 00:20:53,722
맞습니다. 그래서 이 과정을 끝까지 훈련해야 합니다.

519
00:20:53,722 --> 00:20:55,180
그래서 판별자에서 나오는

520
00:20:55,180 --> 00:20:58,540
기울기는 항상 생성기로 전파됩니다. 특히 오른쪽의

521
00:20:58,540 --> 00:21:00,000
이 항을 통해서입니다.

522
00:21:00,000 --> 00:21:02,220
그래서 생성기의 매개변수에 기울기를

523
00:21:02,220 --> 00:21:04,100
얻는 유일한 방법은 실제로

524
00:21:04,100 --> 00:21:05,400
판별자를 통해서입니다.

525
00:21:05,400 --> 00:21:07,680
즉, 여기에서 어떤 정규화기가 없다면,

526
00:21:07,680 --> 00:21:09,237
생성기가 무엇을 해야 하는지

527
00:21:09,237 --> 00:21:11,820
알려주는 보조 항이 없고, 오직 판별기를 통해 전달되는

528
00:21:11,820 --> 00:21:13,112
그래디언트만 있습니다.

529
00:21:13,112 --> 00:21:15,500
이것이 다시 불안정한 학습 문제의

530
00:21:15,500 --> 00:21:16,860
일부로 이어집니다.

531
00:21:16,860 --> 00:21:18,860
맞습니다, 그래서 그 pdata

532
00:21:18,860 --> 00:21:21,660
분포는 훈련 과정에서 고정될 것입니다.

533
00:21:21,660 --> 00:21:22,500
좋습니다.

534
00:21:22,500 --> 00:21:24,933
우리는 생성기가 훈련 과정에서 낮은 그래디언트를

535
00:21:24,933 --> 00:21:26,600
받는다는 문제를 언급했습니다.

536
00:21:26,600 --> 00:21:30,560
여기에는 D의 G의 z의 로그 1 빼기 값을 최대화하려고

537
00:21:30,560 --> 00:21:33,460
하기보다는 D의 G의 z의 로그의

538
00:21:33,460 --> 00:21:36,667
음수를 최소화하는 작은 해킹이 있습니다.

539
00:21:36,667 --> 00:21:39,000
오프라인에서 이것들이 대략적으로 동등하다는 것을

540
00:21:39,000 --> 00:21:41,910
확신할 수 있지만, TL;DR은 생성기가 훈련 시작 시 더

541
00:21:41,910 --> 00:21:44,160
나은 그래디언트를 얻기 위한 더 나은 곡선을

542
00:21:44,160 --> 00:21:45,015
제공한다는 것입니다.

543
00:21:45,015 --> 00:21:46,140
그래서 이것은 정말 중요합니다.

544
00:21:46,140 --> 00:21:47,515
GAN을 처음부터

545
00:21:47,515 --> 00:21:50,100
훈련할 때 이 로그 목표를 사용할 때,

546
00:21:50,100 --> 00:21:53,880
생성기를 위한 수정된 손실을 사용하는 이 트릭은

547
00:21:53,880 --> 00:21:55,500
실제로 매우 중요합니다.

548
00:21:55,500 --> 00:21:56,875
즉, 생성기를 위해

549
00:21:56,875 --> 00:22:00,080
계산하는 하나의 V와 판별기를 위해 계산하는

550
00:22:00,080 --> 00:22:02,240
다른 V가 실제로 존재하며,

551
00:22:02,240 --> 00:22:04,880
이 두 개는 정확히 동일하지 않습니다.

552
00:22:04,880 --> 00:22:06,480
좋습니다, 왜 이것이 좋은 목표일 수

553
00:22:06,480 --> 00:22:08,360
있는지에 대한 또 다른 질문이 있습니다.

554
00:22:08,360 --> 00:22:11,600
이 증명을 단계별로 설명하는 슬라이드가 있었지만,

555
00:22:11,600 --> 00:22:14,340
오늘은 그럴 시간이 없을 것 같습니다.

556
00:22:14,340 --> 00:22:16,720
그래서 TL;DR을 드리고, 확인할

557
00:22:16,720 --> 00:22:21,320
수 있는 오프라인 자료를 참조할 수 있도록 하겠습니다. TL;DR은

558
00:22:21,320 --> 00:22:24,940
이 목표가 좋은 이유는 최적의 판별기를 작성할 수 있기

559
00:22:24,940 --> 00:22:25,720
때문입니다.

560
00:22:25,720 --> 00:22:28,980
이것은 D에 대한 내부 최대화와

561
00:22:28,980 --> 00:22:30,860
G에 대한 외부

562
00:22:30,860 --> 00:22:34,320
최소화가 있는 중첩 최적화 문제입니다. 조금의 수학을 하면, 실제로

563
00:22:34,320 --> 00:22:38,620
이 내부 최대화 문제를 해결하고

564
00:22:38,620 --> 00:22:43,323
최적의 판별기가 무엇인지 쓸 수 있습니다.

565
00:22:43,323 --> 00:22:44,740
이것은 특정

566
00:22:44,740 --> 00:22:46,448
생성기 G에 대한

567
00:22:46,448 --> 00:22:49,447
최적의 판별기여야 합니다. 그냥 이걸 적어두면 됩니다.

568
00:22:49,447 --> 00:22:51,280
물론 적을 수는 있지만,

569
00:22:51,280 --> 00:22:54,383
pdata에 의존하기 때문에 계산할 수는 없습니다.

570
00:22:54,383 --> 00:22:56,300
pdata 밀도에 접근할 수 있다면

571
00:22:56,300 --> 00:22:58,600
계산할 수 있지만, 절대 계산할 수 없습니다.

572
00:22:58,600 --> 00:23:00,940
그래서 이걸 슬라이드나 종이에

573
00:23:00,940 --> 00:23:04,220
방정식으로 적을 수 있지만, 계산할 수는 없습니다.

574
00:23:04,220 --> 00:23:07,520
이 내부 목표를 가지고, 최적의 판별기를

575
00:23:07,520 --> 00:23:10,620
작성하여 이 내부 목표를 극대화하면,

576
00:23:10,620 --> 00:23:12,300
외부 목표는 PG의

577
00:23:12,300 --> 00:23:14,660
x가 pdata와 같을 때만

578
00:23:14,660 --> 00:23:17,900
최소화된다는 것을 보여줄 수 있습니다.

579
00:23:17,900 --> 00:23:21,500
이론적으로 판별기와 생성기의 최적

580
00:23:21,500 --> 00:23:23,420
상태는 PG가

581
00:23:23,420 --> 00:23:27,640
pdata와 같을 때 유일하게 발생합니다.

582
00:23:27,640 --> 00:23:29,880
그래서 기분이 좋지만, 이 이론적

583
00:23:29,880 --> 00:23:32,360
결과에는 많은 주의사항이 있습니다.

584
00:23:32,360 --> 00:23:35,600
하나는 G와 D 모두 무한한 잠재 용량을

585
00:23:35,600 --> 00:23:39,240
가정하고, 생성기와 판별기가 원칙적으로 어떤 함수도

586
00:23:39,240 --> 00:23:41,120
표현할 수 있다고

587
00:23:41,120 --> 00:23:43,370
가정하지만, 고정된 크기와 용량의

588
00:23:43,370 --> 00:23:46,120
신경망이기 때문에 그럴 수 없습니다.

589
00:23:46,120 --> 00:23:48,240
이것은 우리가 이 솔루션에 수렴할 수

590
00:23:48,240 --> 00:23:51,060
있을지에 대한 아무런 정보를 제공하지 않습니다.

591
00:23:51,060 --> 00:23:53,000
이 목표 경관에

592
00:23:53,000 --> 00:23:55,840
최적점이 있지만, 유한한 데이터

593
00:23:55,840 --> 00:23:57,240
샘플로 이 경량

594
00:23:57,240 --> 00:24:00,740
하강법이나 경량 상승법을 통해 도달할

595
00:24:00,740 --> 00:24:04,400
수 있을지에 대한 정보는 전혀 없습니다.

596
00:24:04,400 --> 00:24:08,040
이론적으로 GAN에 대한 어떤 정당성이

597
00:24:08,040 --> 00:24:10,580
있지만, 실제로는 그렇게

598
00:24:10,580 --> 00:24:12,560
잘 작동하지 않거나

599
00:24:12,560 --> 00:24:16,040
강력한 보장을 제공하지 않습니다.

600
00:24:16,040 --> 00:24:19,440
실제로 이 GAN에서 생성기 G와

601
00:24:19,440 --> 00:24:21,320
판별기 D는 모두

602
00:24:21,320 --> 00:24:23,900
신경망으로 매개변수화됩니다.

603
00:24:23,900 --> 00:24:25,940
이전에는 CNN이었습니다.

604
00:24:25,940 --> 00:24:27,860
GAN은 VIT가 인기를 끌기

605
00:24:27,860 --> 00:24:29,660
전에 인기가 떨어졌지만, VIT와도

606
00:24:29,660 --> 00:24:31,900
잘 작동할 것이라고 확신합니다.

607
00:24:31,900 --> 00:24:34,380
비트리비얼한 결과를 처음으로 제공한 GAN은

608
00:24:34,380 --> 00:24:37,100
DC-GAN이라고 불리며, 5층 ConvNet

609
00:24:37,100 --> 00:24:39,420
아키텍처를 가지고 있어 당시 꽤

610
00:24:39,420 --> 00:24:41,060
흥미로운 샘플을 제공했습니다.

611
00:24:41,060 --> 00:24:44,180
DC-GAN을 언급한 이유는 첫 번째 저자 Alec Radford

612
00:24:44,180 --> 00:24:46,820
et al이 대부분의 사람들이 DC-GAN을 작업하면서

613
00:24:46,820 --> 00:24:48,960
경력의 하이라이트가 되었지만, Alec

614
00:24:48,960 --> 00:24:51,293
Radford에게는 그게 충분하지 않았기

615
00:24:51,293 --> 00:24:55,060
때문입니다. 그가 DC-GAN 직후 작업한 다음 프로젝트를 아는 사람이
있나요?

616
00:24:55,060 --> 00:24:55,860
GPT.

617
00:24:55,860 --> 00:24:57,020
GPT.

618
00:24:57,020 --> 00:25:00,360
그래서 알렉 라드포드는 DC-GAN이 그의 경력에서 다소 저조한
시기였습니다.

619
00:25:00,360 --> 00:25:04,140
그는 GPT-1과 GPT-2를 개발했으며, OpenAI에서 다른

620
00:25:04,140 --> 00:25:05,880
놀라운 작업도 수행했습니다.

621
00:25:05,880 --> 00:25:08,540
그래서 이미지 생성 모델링에 참여했던 사람들이

622
00:25:08,540 --> 00:25:10,840
실제로 이산 텍스트 데이터의 생성

623
00:25:10,840 --> 00:25:12,540
모델링으로 넘어갔다는 정말 멋진

624
00:25:12,540 --> 00:25:14,582
연결이 있다고 생각합니다. 그리고

625
00:25:14,582 --> 00:25:16,940
그곳에서 정말 중요한 작업을 했습니다.

626
00:25:16,940 --> 00:25:19,500
제가 강조할 다른 GAN 논문 중 하나는

627
00:25:19,500 --> 00:25:21,480
StyleGAN이라고 불립니다.

628
00:25:21,480 --> 00:25:23,780
이 논문의 세부 사항을 자세히 설명하지는

629
00:25:23,780 --> 00:25:25,880
않겠지만, GAN의 모범 사례를 알고

630
00:25:25,880 --> 00:25:28,900
싶다면 읽어볼 만한 좋은 논문이라고 말씀드리고 싶습니다.

631
00:25:28,900 --> 00:25:31,240
그들은 훨씬 더 복잡한 아키텍처를

632
00:25:31,240 --> 00:25:34,400
사용하지만, 실제로 꽤 좋은 결과를 얻습니다.

633
00:25:34,400 --> 00:25:36,120
GAN의 정말 좋은 점 중 하나는

634
00:25:36,120 --> 00:25:38,320
실제로 잠재 공간에서 부드러운 것을 학습하는

635
00:25:38,320 --> 00:25:39,500
경향이 있다는 것입니다.

636
00:25:39,500 --> 00:25:43,080
제가 의미하는 바는 Z0와 Z1이라는 두 개의 잠재

637
00:25:43,080 --> 00:25:45,740
벡터가 있을 때, 그 사이를 보간한다는

638
00:25:45,740 --> 00:25:49,260
것입니다. 즉, 가우시안에서 Z0 샘플을 뽑고,

639
00:25:49,260 --> 00:25:51,180
가우시안에서 Z1 샘플을 뽑습니다.

640
00:25:51,180 --> 00:25:54,540
그런 다음 Z0와 Z1 사이에 어떤 곡선을 보간합니다.

641
00:25:54,540 --> 00:25:56,040
그 곡선을 따라 모든

642
00:25:56,040 --> 00:26:00,067
점에 대해 생성기를 사용하여 샘플을 생성할 것입니다.

643
00:26:00,067 --> 00:26:02,400
그렇게 하면 이 잠재 공간을 통해 부드러운

644
00:26:02,400 --> 00:26:04,275
보간을 얻는 경향이 있으며, 이는

645
00:26:04,275 --> 00:26:05,560
GAN의 정말 멋진 점입니다.

646
00:26:05,560 --> 00:26:08,480
여기 StyleGAN3 논문에서의 잠재

647
00:26:08,480 --> 00:26:10,600
공간 보간 예시가 있습니다.

648
00:26:10,600 --> 00:26:14,000
이것들은 모두 잠재 z를 부드럽게

649
00:26:14,000 --> 00:26:16,932
변화시키고 생성기를

650
00:26:16,932 --> 00:26:18,640
통해 전달하여 생성된

651
00:26:18,640 --> 00:26:20,280
샘플입니다.

652
00:26:20,280 --> 00:26:22,900
이 동물들이 서로 부드럽게 변형되는 것을

653
00:26:22,900 --> 00:26:23,953
볼 수 있습니다.

654
00:26:23,953 --> 00:26:25,620
즉, 모델이 어떤 유용한

655
00:26:25,620 --> 00:26:27,700
구조를 발견하고 이를 잠재

656
00:26:27,700 --> 00:26:29,940
공간에 담아냈다는 의미입니다.

657
00:26:29,940 --> 00:26:32,260
그래서 꽤 멋진 일입니다.

658
00:26:32,260 --> 00:26:35,420
예전에는 생성적 적대 신경망에 대해 더

659
00:26:35,420 --> 00:26:36,780
많이 이야기했습니다.

660
00:26:36,780 --> 00:26:39,020
장점은 기본적으로 매우 간단한 수식으로

661
00:26:39,020 --> 00:26:40,320
구성되어 있다는 것입니다.

662
00:26:40,320 --> 00:26:43,400
그리고 스타일GAN3에서 보았듯이, 제대로

663
00:26:43,400 --> 00:26:45,900
조정하면 매우 멋진 결과, 아름다운

664
00:26:45,900 --> 00:26:49,580
이미지, 고해상도, 훌륭한 결과를 얻을 수 있습니다.

665
00:26:49,580 --> 00:26:52,220
하지만 단점은 우리가 이야기했던 것처럼 훈련이 상당히

666
00:26:52,220 --> 00:26:53,583
불안정하다는 것입니다.

667
00:26:53,583 --> 00:26:55,000
볼 수 있는 손실 곡선이 없습니다.

668
00:26:55,000 --> 00:26:56,480
훈련이 매우 불안정합니다.

669
00:26:56,480 --> 00:26:58,820
그들은 작은 변화에도 폭발하는

670
00:26:58,820 --> 00:27:02,102
경향이 있어 모드 붕괴라는 현상이 발생합니다.

671
00:27:02,102 --> 00:27:03,560
갑자기 NaN이 발생할 수 있습니다.

672
00:27:03,560 --> 00:27:05,200
inf가 발생할 수 있습니다.

673
00:27:05,200 --> 00:27:07,200
판별기가 미쳐버리기 시작합니다.

674
00:27:07,200 --> 00:27:09,500
생성기가 항상 완전히 무작위 쓰레기를 생성하기

675
00:27:09,500 --> 00:27:10,320
시작합니다.

676
00:27:10,320 --> 00:27:13,340
이 문제를 진단할 손실 곡선이 없습니다.

677
00:27:13,340 --> 00:27:14,500
그들은 다소 엉망입니다.

678
00:27:14,500 --> 00:27:16,380
그래서 GAN이 정말 멋진

679
00:27:16,380 --> 00:27:19,480
결과를 줄 수 있지만, 매우 조심스럽게

680
00:27:19,480 --> 00:27:22,460
조정하고 정규화, 샘플링 등 모든 것을

681
00:27:22,460 --> 00:27:25,100
매우 조심스럽게 제어해야 실제로는 정말

682
00:27:25,100 --> 00:27:27,700
큰 모델이나 큰 데이터로 확장하기가

683
00:27:27,700 --> 00:27:29,120
상당히 어렵습니다.

684
00:27:29,120 --> 00:27:31,680
그래서 GAN은 2016년부터

685
00:27:31,680 --> 00:27:36,940
2020년, 2021년 정도까지 기본적으로 생성 모델의 주요

686
00:27:36,940 --> 00:27:38,160
카테고리였습니다.

687
00:27:38,160 --> 00:27:40,800
그 5년 동안 수천 개의 논문이 나왔고,

688
00:27:40,800 --> 00:27:43,200
사람들은 다양한 GAN 공식, 다양한

689
00:27:43,200 --> 00:27:45,480
손실 함수, 다양한 수학적 형식을

690
00:27:45,480 --> 00:27:47,618
사용하려고 했으며, 상상할

691
00:27:47,618 --> 00:27:50,160
수 있는 모든 종류의 생성 모델링 작업에

692
00:27:50,160 --> 00:27:51,820
GAN을 적용했습니다.

693
00:27:51,820 --> 00:27:54,600
그래서 이것은 약 5~6년 동안 기본적으로

694
00:27:54,600 --> 00:27:57,080
사용된 생성 모델링 프레임워크였습니다.

695
00:27:57,080 --> 00:27:59,460
질문은 우리가 이것을 기대해야 할까요?

696
00:27:59,460 --> 00:28:02,040
부드러운 잠재 변수가 나타나기를 기대해야 하지 않나요?

697
00:28:02,040 --> 00:28:04,800
저는 그렇지 않다고 생각합니다. GAN에서 발생할

698
00:28:04,800 --> 00:28:08,240
수 있는 한 가지는 생성기가 고정된 수의 데이터 샘플을

699
00:28:08,240 --> 00:28:10,260
단순히 암기할 수 있다는 것입니다.

700
00:28:10,260 --> 00:28:12,920
그렇다면 생성기가 잠재 변수 z를 무시하고 훈련 데이터

701
00:28:12,920 --> 00:28:15,160
세트에서 10개의 샘플을 어떻게든 암기한다면

702
00:28:15,160 --> 00:28:16,017
어떻게 될까요?

703
00:28:16,017 --> 00:28:17,600
그런 다음 어떤 z를 주더라도

704
00:28:17,600 --> 00:28:19,380
항상 훈련 데이터 세트의 10개 샘플 중

705
00:28:19,380 --> 00:28:20,900
하나를 제공하고, 다른 것은

706
00:28:20,900 --> 00:28:22,040
절대 제공하지 않습니다.

707
00:28:22,040 --> 00:28:24,550
그 경우에는 생성기가 항상 실제 샘플 중

708
00:28:24,550 --> 00:28:26,300
하나와 비트 단위로 동일한

709
00:28:26,300 --> 00:28:28,460
것을 제공하기 때문에 판별기를

710
00:28:28,460 --> 00:28:29,440
속일 수 있습니다.

711
00:28:29,440 --> 00:28:30,940
그 경우 생성기는

712
00:28:30,940 --> 00:28:34,020
몇 개의 유한 샘플 근처에 Dirac

713
00:28:34,020 --> 00:28:36,260
델타 밀도를 쌓고,

714
00:28:36,260 --> 00:28:39,420
다른 곳에는 확률 질량을 두지 않을

715
00:28:39,420 --> 00:28:40,400
것입니다.

716
00:28:40,400 --> 00:28:43,620
그래서 이것은 실제로 생성기에 대한 정당한 해결책입니다.

717
00:28:43,620 --> 00:28:46,177
그리고 그것은 부드러운 잠재 변수를 전혀 제공하지 않을 것입니다.

718
00:28:46,177 --> 00:28:48,260
그래서 이것은 이러한 것들이 당신이 원하는

719
00:28:48,260 --> 00:28:50,260
것이 아닌 직관적이지 않은 해결책으로

720
00:28:50,260 --> 00:28:51,860
붕괴될 수 있는 한 가지 예입니다.

721
00:28:51,860 --> 00:28:52,820
오, 좋은 질문입니다.

722
00:28:52,820 --> 00:28:54,980
훈련 데이터 세트와 잠재 변수

723
00:28:54,980 --> 00:28:56,623
간의 관계는 무엇인가요?

724
00:28:56,623 --> 00:28:58,540
그래서 이것은 GAN에 대한

725
00:28:58,540 --> 00:29:00,120
매우 근본적인 질문입니다.

726
00:29:00,120 --> 00:29:02,000
하나의 방향으로 매핑할 수 있습니다.

727
00:29:02,000 --> 00:29:03,500
생성기는 잠재 공간에서

728
00:29:03,500 --> 00:29:07,040
데이터 공간으로의 매핑을 제공하며, z에서 x로 매핑하지만,

729
00:29:07,040 --> 00:29:08,740
GAN에서는 일반적으로

730
00:29:08,740 --> 00:29:11,260
x에서 z로 다시 매핑할 방법이 없습니다.

731
00:29:11,260 --> 00:29:14,720
그리고 이것은 GAN과 VAE 간의 매우 다른 점입니다.

732
00:29:14,720 --> 00:29:17,740
VAE는 x에서 z로의 명시적 매핑을 학습하지만,

733
00:29:17,740 --> 00:29:19,600
GAN에는 그런 것이 없습니다.

734
00:29:19,600 --> 00:29:21,200
역으로 계산하려고 시도할

735
00:29:21,200 --> 00:29:24,560
수 있습니다. 수치적으로 기울기 하강법을 통해 역을

736
00:29:24,560 --> 00:29:25,660
계산할 수 있습니다.

737
00:29:25,660 --> 00:29:28,080
그런 논문들이 있지만, x와 z

738
00:29:28,080 --> 00:29:31,440
간의 명시적으로 강제된 관계는 없습니다.

739
00:29:31,440 --> 00:29:33,400
대신 판별기를 생각할 때,

740
00:29:33,400 --> 00:29:35,837
생성기에서 나오는 모든 출력의

741
00:29:35,837 --> 00:29:37,920
분포와 모든 데이터 샘플의

742
00:29:37,920 --> 00:29:40,040
분포 간의 분포 정렬을

743
00:29:40,040 --> 00:29:42,082
강제하려고 한다고 생각할

744
00:29:42,082 --> 00:29:43,120
수 있습니다.

745
00:29:43,120 --> 00:29:44,683
물론 GAN에 관해서는 당신이

746
00:29:44,683 --> 00:29:46,100
생각하는 모든 것에 대해

747
00:29:46,100 --> 00:29:48,060
적어도 12개의 논문이 있을 것입니다.

748
00:29:48,060 --> 00:29:50,240
그래서 양방향 매핑을 학습하려고 하는

749
00:29:50,240 --> 00:29:52,700
GAN 변형에 대한 논문도 많이 있지만,

750
00:29:52,700 --> 00:29:54,840
그런 것들은 실제로 잘 되지 않았습니다.

751
00:29:54,840 --> 00:29:55,740
오, 좋은 질문입니다.

752
00:29:55,740 --> 00:29:56,573
우리가 얻은 것은 무엇인가요?

753
00:29:56,573 --> 00:29:59,300
VAE로 갔을 때 우리는 잠재 벡터를 얻었지만

754
00:29:59,300 --> 00:30:00,420
밀도를 포기했습니다.

755
00:30:00,420 --> 00:30:03,000
그리고 이제 GAN에서는 우리가 제어할 수 있는 잠재

756
00:30:03,000 --> 00:30:04,140
벡터를 포기한 것 같습니다.

757
00:30:04,140 --> 00:30:06,400
당신이 얻은 것은 훨씬 더 나은 샘플입니다.

758
00:30:06,400 --> 00:30:09,280
VAE에 관해서는 VAE가 매우 좋은 샘플을

759
00:30:09,280 --> 00:30:11,080
제공하지 않는 경향이 있습니다.

760
00:30:11,080 --> 00:30:14,280
VAE는 항상 흐릿한 특성을 가지고 있습니다.

761
00:30:14,280 --> 00:30:16,220
그들은 결코 좋게 보이지 않습니다.

762
00:30:16,220 --> 00:30:18,840
VAE는 혼자서는 매우 깨끗하고 선명한

763
00:30:18,840 --> 00:30:21,243
샘플을 제공하지 않지만, GAN과

764
00:30:21,243 --> 00:30:22,660
함께 사용하면

765
00:30:22,660 --> 00:30:28,220
예제에서 보았듯이 매우 선명하고 깨끗하며 좋은 샘플을 얻을 수 있지만, 이

766
00:30:28,220 --> 00:30:30,260
시스템을 조정하려고 하면서 당신의

767
00:30:30,260 --> 00:30:31,867
정신을 잃게 됩니다.

768
00:30:31,867 --> 00:30:34,200
네, 추론할 때는 판별기를 버리고

769
00:30:34,200 --> 00:30:35,325
생성기만 사용합니다.

770
00:30:35,325 --> 00:30:38,942
그래서 추론할 때는 사전에서 샘플 z를 뽑아 생성기를

771
00:30:38,942 --> 00:30:41,400
통과시켜 데이터에서 샘플을 얻습니다.

772
00:30:41,400 --> 00:30:44,100
그래서 추론할 때 매우 효율적입니다.

773
00:30:44,100 --> 00:30:44,600
좋습니다.

774
00:30:44,600 --> 00:30:46,340
GAN은 약 5~6년

775
00:30:46,340 --> 00:30:49,750
동안 생성 모델링의 주요 범주였다고

776
00:30:49,750 --> 00:30:50,780
언급했습니다.

777
00:30:50,780 --> 00:30:52,220
그들을 대체한 것은 무엇인가요?

778
00:30:52,220 --> 00:30:55,500
그들을 대체한 것은 확산 모델이라는 매우

779
00:30:55,500 --> 00:30:57,460
다른 범주의 모델입니다.

780
00:30:57,460 --> 00:31:00,300
여기서 몇 가지 주의사항을 말씀드려야 합니다.

781
00:31:00,300 --> 00:31:02,740
확산 모델 문헌은 미쳤습니다.

782
00:31:02,740 --> 00:31:05,500
이 논문들을 읽으면, 무슨 일이 일어나고 있는지

783
00:31:05,500 --> 00:31:07,600
말하기 전에 5페이지의 수학을 거칩니다.

784
00:31:07,600 --> 00:31:11,380
확산 모델로 이어지는 세 가지 다른 수학적 형식이

785
00:31:11,380 --> 00:31:13,240
있으며, 이들은 수학적으로

786
00:31:13,240 --> 00:31:14,660
모두 매우 다릅니다.

787
00:31:14,660 --> 00:31:16,785
논문 간에 매우 다른 표기법,

788
00:31:16,785 --> 00:31:18,960
매우 다른 용어, 매우 다른 수학적

789
00:31:18,960 --> 00:31:20,100
형식이 있습니다.

790
00:31:20,100 --> 00:31:23,080
그래서 이 분야는 미친 분야입니다.

791
00:31:23,080 --> 00:31:24,720
그래서 저는 모든 적절한

792
00:31:24,720 --> 00:31:28,225
수학적 형식을 가진 확산 모델의 다양한 변형을 완전히 다루지

793
00:31:28,225 --> 00:31:30,600
않을 것이라는 큰 주의사항을 말씀드려야

794
00:31:30,600 --> 00:31:31,420
합니다.

795
00:31:31,420 --> 00:31:32,960
대신, 제가 하려고 하는

796
00:31:32,960 --> 00:31:35,860
것은 확산 모델에 대한 직관적인 개요와 오늘날

797
00:31:35,860 --> 00:31:38,040
가장 일반적인 형태의 확산 모델인

798
00:31:38,040 --> 00:31:40,540
정류 흐름 모델에 대한 직관적인

799
00:31:40,540 --> 00:31:42,480
기하학적 이해를 제공하는 것입니다.

800
00:31:42,480 --> 00:31:45,212
확산 모델에 대해 많은 강의를 할 수

801
00:31:45,212 --> 00:31:47,920
있고 모든 흥미로운 수학적 뉘앙스를

802
00:31:47,920 --> 00:31:51,280
다룰 수 있지만, 불행히도 한 강의의 2/3 시간

803
00:31:51,280 --> 00:31:53,320
안에 그럴 시간은 없습니다.

804
00:31:53,320 --> 00:31:56,200
그 주의사항을 제쳐두고, 확산

805
00:31:56,200 --> 00:31:59,000
모델의 직관은 사실 쉽습니다.

806
00:31:59,000 --> 00:32:03,320
모든 생성 모델에서 우리는 샘플을 뽑고 싶습니다.

807
00:32:03,320 --> 00:32:08,080
GAN처럼 우리는 노이즈 분포 z에서 데이터 분포 px로

808
00:32:08,080 --> 00:32:11,722
샘플을 변환하고 싶지만, 확산 모델에서

809
00:32:11,722 --> 00:32:14,180
이를 수행하는 방식은 완전히

810
00:32:14,180 --> 00:32:15,380
다릅니다.

811
00:32:15,380 --> 00:32:17,300
GAN은 생성기를 통해 z를

812
00:32:17,300 --> 00:32:20,122
x로 직접 매핑하는 결정론적 매핑을 학습합니다.

813
00:32:20,122 --> 00:32:21,580
확산 모델에서는 더

814
00:32:21,580 --> 00:32:24,060
암시적이고 간접적인 방법을 사용할 것입니다.

815
00:32:24,060 --> 00:32:26,700
우리가 할 첫 번째 제약 조건은

816
00:32:26,700 --> 00:32:30,420
z, 즉 노이즈 분포가 항상 우리의 데이터와

817
00:32:30,420 --> 00:32:33,640
동일한 형태를 가져야 한다는 것입니다.

818
00:32:33,640 --> 00:32:36,560
그래서 이미지가 H x W x 3이라면,

819
00:32:36,560 --> 00:32:38,060
노이즈 분포도 항상 H

820
00:32:38,060 --> 00:32:39,840
x W x 3이어야 합니다.

821
00:32:39,840 --> 00:32:42,220
그들은 정확히 같은 형태여야 합니다.

822
00:32:42,220 --> 00:32:45,940
이제 우리는 점점 더 많은 노이즈로 손상된

823
00:32:45,940 --> 00:32:49,393
데이터의 다양한 버전을 고려할 것입니다.

824
00:32:49,393 --> 00:32:51,060
여기서, 고양이

825
00:32:51,060 --> 00:32:54,700
사진인 데이터 샘플이 있다면, t는 0에서

826
00:32:54,700 --> 00:32:58,420
1까지 범위의 노이즈 수준이 될 것입니다.

827
00:32:58,420 --> 00:33:00,920
t가 0일 때는 노이즈가 없다는 의미입니다.

828
00:33:00,920 --> 00:33:03,260
즉, 완전히 깨끗한 데이터 샘플입니다.

829
00:33:03,260 --> 00:33:06,000
t가 0.3일 때는 약간의 노이즈가 있습니다.

830
00:33:06,000 --> 00:33:08,560
우리는 우리의 노이즈

831
00:33:08,560 --> 00:33:11,240
z를 데이터 x에 섞습니다.

832
00:33:11,240 --> 00:33:14,210
t가 1이 되면 완전한 노이즈를 얻습니다.

833
00:33:14,210 --> 00:33:15,960
그리고 이것들은 우리의 노이즈

834
00:33:15,960 --> 00:33:17,660
분포에서 직접 샘플링한 것입니다.

835
00:33:17,660 --> 00:33:21,040
어떤 식으로든 이 t 매개변수는 우리의 데이터

836
00:33:21,040 --> 00:33:24,422
분포와 노이즈 분포 사이를 부드럽게 보간할 것입니다.

837
00:33:24,422 --> 00:33:25,880
그리고 이것은 우리가

838
00:33:25,880 --> 00:33:27,720
할 수 있는 것이며, 노이즈

839
00:33:27,720 --> 00:33:30,760
분포는 거의 항상 가우시안일 것입니다. 우리가

840
00:33:30,760 --> 00:33:33,707
이해하고 샘플링할 수 있는 간단한 것입니다.

841
00:33:33,707 --> 00:33:36,040
이제 우리가 할 일은 신경망을 훈련시켜 약간의

842
00:33:36,040 --> 00:33:38,440
점진적인 노이즈 제거를 수행하는 것입니다.

843
00:33:38,440 --> 00:33:43,208
신경망은 중간 정도의 노이즈로 손상된 데이터

844
00:33:43,208 --> 00:33:45,000
조각인 샘플을

845
00:33:45,000 --> 00:33:46,662
받을 것입니다.

846
00:33:46,662 --> 00:33:48,120
그리고 이제 신경망은 그것을

847
00:33:48,120 --> 00:33:50,340
약간 정리하려고 훈련될 것이며, 약간의

848
00:33:50,340 --> 00:33:52,360
노이즈를 제거하려고 할 것입니다.

849
00:33:52,360 --> 00:33:53,960
여기서 훈련 목표는

850
00:33:53,960 --> 00:33:58,777
노이즈가 있는 어떤 양의 이미지에 대한 신경망 입력이며,

851
00:33:58,777 --> 00:34:00,360
노이즈의 일부를

852
00:34:00,360 --> 00:34:02,245
제거하려고 합니다.

853
00:34:02,245 --> 00:34:04,120
그런 다음 추론 시간에 우리가

854
00:34:04,120 --> 00:34:06,480
할 일은 노이즈 분포 pz에서

855
00:34:06,480 --> 00:34:10,380
직접 노이즈 샘플을 뽑고, 그 샘플에서 하나씩 노이즈를

856
00:34:10,380 --> 00:34:12,340
제거하기 위해 신경망을

857
00:34:12,340 --> 00:34:15,500
반복적으로 적용하는 절차를 수행하는 것입니다.

858
00:34:15,500 --> 00:34:17,639
처음으로 이 작업을 수행할 때,

859
00:34:17,639 --> 00:34:20,522
우리는 완전한 노이즈 샘플을 뽑을 것입니다.

860
00:34:20,522 --> 00:34:21,980
그리고 신경망의 첫 번째

861
00:34:21,980 --> 00:34:23,397
적용에서, 네트워크는

862
00:34:23,397 --> 00:34:26,225
완전한 노이즈에서 노이즈를 제거하려고 할 것입니다.

863
00:34:26,225 --> 00:34:28,100
그래서 기본적으로 그 노이즈에서

864
00:34:28,100 --> 00:34:31,620
데이터 구조의 아주 작은 힌트를 환각하도록 강요받게 될 것입니다.

865
00:34:31,620 --> 00:34:34,617
그리고 약간 덜 노이즈가 있는 예제로 넘어가면,

866
00:34:34,617 --> 00:34:36,659
우리는 그것을 신경망에 다시

867
00:34:36,659 --> 00:34:39,380
전달하고, 이제 약간 노이즈가 제거된, 약간

868
00:34:39,380 --> 00:34:43,630
생성된 이미지에서 다시 약간의 노이즈를 제거해 달라고 요청할 것입니다.

869
00:34:43,630 --> 00:34:45,380
그렇게 되면 조금 덜 노이즈가

870
00:34:45,380 --> 00:34:47,199
생기고, 조금 덜 노이즈가 생기고,

871
00:34:47,199 --> 00:34:49,320
조금 덜 노이즈가 생길 것입니다.

872
00:34:49,320 --> 00:34:52,080
결국, 모든 것을 올바르게 설정하면,

873
00:34:52,080 --> 00:34:53,739
우리는 완전한 노이즈 샘플을

874
00:34:53,739 --> 00:34:56,020
뽑고, 그 완전한 무작위 노이즈

875
00:34:56,020 --> 00:34:57,700
샘플 z에서 노이즈를

876
00:34:57,700 --> 00:35:01,440
제거해 달라고 네트워크에 요청하는 상황에 도달하고

877
00:35:01,440 --> 00:35:03,900
싶습니다. 결국 모든 노이즈를 제거하고

878
00:35:03,900 --> 00:35:07,160
시스템에서 생성된 샘플을 얻을 수 있습니다.

879
00:35:07,160 --> 00:35:09,040
그래서 이것은 이상한 설정입니다.

880
00:35:09,040 --> 00:35:11,960
이상한 것이지만, 그것이 확산 모델의

881
00:35:11,960 --> 00:35:13,280
직관입니다.

882
00:35:13,280 --> 00:35:15,940
단계 수는 고정된 하이퍼파라미터인가요?

883
00:35:15,940 --> 00:35:16,620
상황에 따라 다릅니다.

884
00:35:16,620 --> 00:35:18,840
그래서 이 슬라이드에서는 모든

885
00:35:18,840 --> 00:35:22,600
것에 대해 의도적으로 매우 모호하게 설명해야 했습니다.

886
00:35:22,600 --> 00:35:23,700
노이즈란 무엇인가요?

887
00:35:23,700 --> 00:35:26,500
노이즈에 대해 데이터가 손상된다는 것은 무엇을 의미하나요?

888
00:35:26,500 --> 00:35:28,900
노이즈를 조금 제거한다는 것은 무엇을 의미하나요?

889
00:35:28,900 --> 00:35:32,263
추론 시 반복적으로 적용한다는 것은 무엇을 의미하나요?

890
00:35:32,263 --> 00:35:34,680
왜냐하면, 제가 말했듯이, 확산의 다양한 형식이 많고,

891
00:35:34,680 --> 00:35:36,840
이러한 것들이 서로 다른 상황에서 정확히 무엇을

892
00:35:36,840 --> 00:35:39,220
의미하는지에 대한 다양한 변형이 많기 때문입니다.

893
00:35:39,220 --> 00:35:41,480
그래서 이 슬라이드는 확산에 대한 상당히 높은 수준의 개요를

894
00:35:41,480 --> 00:35:42,568
제공하기 위한 것입니다.

895
00:35:42,568 --> 00:35:44,360
그리고 확산 모델의 다양한 구체적인

896
00:35:44,360 --> 00:35:46,920
구현은 이러한 용어들이 구체적으로 무엇을

897
00:35:46,920 --> 00:35:49,840
의미하는지에 대해 서로 다른 구체적인 선택을 할 것입니다.

898
00:35:49,840 --> 00:35:53,940
그래서 이 높은 수준의 확산 그림이 이해가 되나요?

899
00:35:53,940 --> 00:35:55,980
좋습니다, 그럼 이제 이것을 더 구체적으로 만들어 보겠습니다.

900
00:35:55,980 --> 00:35:59,400
이제 일반 확산 모델에서 정류 흐름

901
00:35:59,400 --> 00:36:02,080
모델이라는 특정 범주의 확산

902
00:36:02,080 --> 00:36:03,940
모델로 넘어가겠습니다.

903
00:36:03,940 --> 00:36:06,860
일부 사람들은 저와 논쟁할 수 있으며 정류 흐름이 확산이 아니라고

904
00:36:06,860 --> 00:36:07,793
말할 수 있습니다.

905
00:36:07,793 --> 00:36:09,960
일부 사람들은 그것들이 다른 것이라고 말할 수도 있습니다.

906
00:36:09,960 --> 00:36:11,060
나는 정말 신경 쓰지 않아.

907
00:36:11,060 --> 00:36:13,800
내게는 정정된 흐름이 일종의 확산 모델이다.

908
00:36:13,800 --> 00:36:15,220
나와 싸워.

909
00:36:15,220 --> 00:36:18,260
정정된 흐름의 직관은 기본적으로 이렇다.

910
00:36:18,260 --> 00:36:19,720
우리는 같은 것을 가지고 있다.

911
00:36:19,720 --> 00:36:20,640
우리는 우리의 pnoise를 가지고 있다.

912
00:36:20,640 --> 00:36:21,560
우리는 우리의 pdata를 가지고 있다.

913
00:36:21,560 --> 00:36:23,310
우리는 이것을 기하학적으로 그릴 것이며,

914
00:36:23,310 --> 00:36:25,640
이는 직관을 얻는 좋은 방법이라고 생각한다.

915
00:36:25,640 --> 00:36:28,140
특히 2차원에서 기하학적으로, 왜냐하면 슬라이드에

916
00:36:28,140 --> 00:36:29,960
맞는 것은 그것뿐이지만,

917
00:36:29,960 --> 00:36:32,377
물론 이것들은 매우 높은 차원의 이미지와 가우시안이

918
00:36:32,377 --> 00:36:34,300
될 것이며, 이는 쉽게 잘못된

919
00:36:34,300 --> 00:36:37,140
길로 이끌 수 있다. 왜냐하면 2차원과 3차원에서

920
00:36:37,140 --> 00:36:39,560
성립하는 직관이 많은 차원으로 가면 완전히

921
00:36:39,560 --> 00:36:41,300
무너진다는 것을 알기 때문이다.

922
00:36:41,300 --> 00:36:42,160
정말 슬프다.

923
00:36:42,160 --> 00:36:44,960
우리가 이렇게 낮은 차원의 우주에 살고 있다는 것이

924
00:36:44,960 --> 00:36:47,380
슬프다. 왜냐하면 이 우주에서 구축한 우리의

925
00:36:47,380 --> 00:36:50,200
직관이 100차원 공간이나 1,000차원 공간으로는

926
00:36:50,200 --> 00:36:51,680
잘 전이되지 않기 때문이다.

927
00:36:51,680 --> 00:36:54,040
그러니 항상 인지하라, 하지만 어쩔 수 없다.

928
00:36:54,040 --> 00:36:56,180
우리는 우리가 가진 우주에 갇혀 있다.

929
00:36:56,180 --> 00:36:58,060
정정된 흐름의 설정은 우리가

930
00:36:58,060 --> 00:37:01,840
pnoise 분포와 pdata 분포를 가지고 있다는 것이다.

931
00:37:01,840 --> 00:37:03,320
pnoise는 우리가 이해하고

932
00:37:03,320 --> 00:37:06,020
샘플링할 수 있으며, 적분을 계산할 수 있는 간단한 것이다.

933
00:37:06,020 --> 00:37:08,108
매우 친숙한 분포이다. pdata는 다시 말해

934
00:37:08,108 --> 00:37:08,900
미친 것이다.

935
00:37:08,900 --> 00:37:12,240
그것이 우주가 우리에게 이미지를 제공하는 방식이다.

936
00:37:12,240 --> 00:37:14,640
이제 매 훈련 반복마다

937
00:37:14,640 --> 00:37:18,800
우리는 사전 분포에서 z를 샘플링하고 데이터

938
00:37:18,800 --> 00:37:21,600
분포에서 x를 샘플링할 것이다.

939
00:37:21,600 --> 00:37:23,440
우리는 pz가 우리가 제어할 수 있는

940
00:37:23,440 --> 00:37:26,100
간단한 것이기 때문에 분석적으로 샘플을 그릴 수 있다.

941
00:37:26,100 --> 00:37:28,320
데이터 분포에서 샘플을 그리는 것은

942
00:37:28,320 --> 00:37:31,600
유한한 훈련 세트에서 예제를 선택하는 것을 의미한다.

943
00:37:31,600 --> 00:37:34,640
이제 t를 0에서 1까지 균일하게

944
00:37:34,640 --> 00:37:36,340
선택할 것이다.

945
00:37:36,340 --> 00:37:39,940
t는 우리의 노이즈 수준이며, t가 0이면 노이즈가 없고, t가

946
00:37:39,940 --> 00:37:42,280
1이면 모든 노이즈가 있다는 것을 기억하라.

947
00:37:42,280 --> 00:37:44,840
이제 우리는 데이터 샘플 x에서

948
00:37:44,840 --> 00:37:48,720
노이즈 샘플 z로 직접 가리키는 선을

949
00:37:48,720 --> 00:37:49,600
그릴 것이다.

950
00:37:49,600 --> 00:37:52,840
그리고 이 선, x에서 z로 가리키는 이

951
00:37:52,840 --> 00:37:55,160
벡터를 우리는 v라고 부를 것이다. 이것은 흐름 필드의

952
00:37:55,160 --> 00:37:57,280
속도가 될 것이다.

953
00:37:57,280 --> 00:38:01,860
그리고 우리는 xt를 이 선을 따라 있는 점으로 설정할 것이며,

954
00:38:01,860 --> 00:38:04,780
이는 x와 z 사이의 선형 보간이다.

955
00:38:04,780 --> 00:38:06,860
이제 우리는 노이즈 샘플

956
00:38:06,860 --> 00:38:09,260
z, 데이터 샘플 x, 그 사이의

957
00:38:09,260 --> 00:38:12,580
속도 벡터 v를 가지고 있으며, 데이터의

958
00:38:12,580 --> 00:38:15,140
노이즈 버전 xt를 선택했다.

959
00:38:15,140 --> 00:38:19,458
이전 슬라이드에서 '노이즈가 있는 데이터 얻기'라고 했을 때, 이것이

960
00:38:19,458 --> 00:38:22,000
정정된 흐름 모델의 경우 의미하는 바이다.

961
00:38:22,000 --> 00:38:24,180
데이터 샘플과 노이즈

962
00:38:24,180 --> 00:38:26,380
샘플 간의 선형 보간이다.

963
00:38:26,380 --> 00:38:28,880
이제 훈련 목표는 매우 간단하다.

964
00:38:28,880 --> 00:38:31,400
이제 우리는 학습 가능한 매개변수 theta가

965
00:38:31,400 --> 00:38:33,540
있는 신경망 f theta를 훈련할 것이다.

966
00:38:33,540 --> 00:38:36,860
그 신경망은 노이즈 샘플 xt와 노이즈 레벨 t를

967
00:38:36,860 --> 00:38:38,580
입력으로 받을 것입니다.

968
00:38:38,580 --> 00:38:41,580
그리고 그것은 초록 벡터 v를 예측하려고 할 것입니다.

969
00:38:41,580 --> 00:38:42,480
그게 전부입니다.

970
00:38:42,480 --> 00:38:44,900
이것이 정정된 흐름에서 우리가 해야 할 전부입니다.

971
00:38:44,900 --> 00:38:47,180
이 코드의 구조는 매우 간단합니다.

972
00:38:47,180 --> 00:38:50,340
여기 논문을 읽을 때 얼마나 많은 불명확성이

973
00:38:50,340 --> 00:38:51,640
있는지 놀랄 것입니다.

974
00:38:51,640 --> 00:38:53,478
그리고 결국 이 매우 간단한 코드로 귀결됩니다.

975
00:38:53,478 --> 00:38:55,020
많은 발표에서 이것이 더

976
00:38:55,020 --> 00:38:57,580
명확하게 설명되지 않는 것이 저를 미치게 합니다.

977
00:38:57,580 --> 00:38:59,640
그래서 정정된 흐름의 훈련

978
00:38:59,640 --> 00:39:01,200
루프는 매우 간단합니다.

979
00:39:01,200 --> 00:39:03,460
매 반복마다 데이터 세트를 반복합니다.

980
00:39:03,460 --> 00:39:08,720
z를 얻는데, 이는 x와 동일한 형태의 단위 가우시안입니다.

981
00:39:08,720 --> 00:39:11,980
0에서 1까지 균일한 노이즈 레벨 t를 선택합니다.

982
00:39:11,980 --> 00:39:14,600
x와 z 사이의 선형 보간인 xt를

983
00:39:14,600 --> 00:39:15,720
계산합니다.

984
00:39:15,720 --> 00:39:18,040
xt와 t를 모델에 제공하고,

985
00:39:18,040 --> 00:39:20,040
손실은 이 실제

986
00:39:20,040 --> 00:39:24,200
값 v와 모델 예측 간의 평균 제곱 오차입니다.

987
00:39:24,200 --> 00:39:24,900
그게 전부입니다.

988
00:39:24,900 --> 00:39:27,720
이것이 정정된 흐름 모델의 훈련 목표입니다.

989
00:39:27,720 --> 00:39:29,320
이것을 GAN과 대조해 보세요.

990
00:39:29,320 --> 00:39:31,440
정정된 흐름 모델이나 실제로 어떤 종류의

991
00:39:31,440 --> 00:39:33,200
확산 모델을 훈련할 때, 훈련

992
00:39:33,200 --> 00:39:35,100
중에 볼 수 있는 손실이 있습니다.

993
00:39:35,100 --> 00:39:37,460
손실이 줄어들면 모델이 일반적으로 더 좋습니다.

994
00:39:37,460 --> 00:39:40,360
반년의 GAN 광기를 겪은 우리에게, 확산

995
00:39:40,360 --> 00:39:42,840
모델을 처음 훈련할 때 볼 수 있는

996
00:39:42,840 --> 00:39:45,660
손실이 있다는 것은, 오 마이 갓,

997
00:39:45,660 --> 00:39:47,023
이건 놀라운 일입니다.

998
00:39:47,023 --> 00:39:49,440
우리는 GAN 플롯을 보며 얼마나 많은 시간을 보냈고,

999
00:39:49,440 --> 00:39:50,420
그들은 이렇게 보입니다.

1000
00:39:50,420 --> 00:39:51,300
그리고 당신은 전혀 알지 못합니다.

1001
00:39:51,300 --> 00:39:52,880
모델이 잘 작동하는지 여부를 알기

1002
00:39:52,880 --> 00:39:54,140
위해 차를 읽는 것과 같습니다.

1003
00:39:54,140 --> 00:39:55,640
확산 모델을 훈련하면,

1004
00:39:55,640 --> 00:39:57,720
아름답고 부드러운 지수 손실 곡선을

1005
00:39:57,720 --> 00:40:01,980
얻고, 그것이 당신을 매우 행복하게 만듭니다. 그래서 좋습니다.

1006
00:40:01,980 --> 00:40:03,700
그래서 이것이 확산

1007
00:40:03,700 --> 00:40:05,420
모델을 위한 훈련입니다.

1008
00:40:05,420 --> 00:40:09,417
이제 추론에서 우리는 무엇을 해야 할까요? GAN은 추론에서

1009
00:40:09,417 --> 00:40:10,500
다소 쉽습니다.

1010
00:40:10,500 --> 00:40:12,880
GAN은 z를 가져와서 생성기를 통과시키기만 하면 됩니다.

1011
00:40:12,880 --> 00:40:13,977
데이터 샘플을 얻습니다.

1012
00:40:13,977 --> 00:40:16,060
매우 간단하지만, 이제 확산

1013
00:40:16,060 --> 00:40:19,300
모델이나 이 경우 정정된 흐름 모델에서는 모델

1014
00:40:19,300 --> 00:40:21,180
출력 자체가 쓸모가 없습니다.

1015
00:40:21,180 --> 00:40:22,240
우리는 xt를 얻습니다.

1016
00:40:22,240 --> 00:40:24,160
우리는 v를 얻습니다. 이것으로 무엇을 할 것인가요?

1017
00:40:24,160 --> 00:40:25,220
그렇게 명확하지 않다.

1018
00:40:25,220 --> 00:40:26,900
따라서 추론 시

1019
00:40:26,900 --> 00:40:28,483
확산 모델은

1020
00:40:28,483 --> 00:40:31,700
GAN에 비해 조금 더 복잡해진다.

1021
00:40:31,700 --> 00:40:34,780
추론 시 먼저 고정된

1022
00:40:34,780 --> 00:40:36,980
상수인 단계 수

1023
00:40:36,980 --> 00:40:38,532
t를 선택한다.

1024
00:40:38,532 --> 00:40:40,240
정류 흐름 모델의 경우,

1025
00:40:40,240 --> 00:40:43,120
t가 50이면 시작하기에 좋은 숫자이다.

1026
00:40:43,120 --> 00:40:45,940
때때로 t를 30으로 줄일 수 있으며, 그 경우도 괜찮다.

1027
00:40:45,940 --> 00:40:48,860
그런 다음 노이즈 분포에서 직접

1028
00:40:48,860 --> 00:40:50,320
x를 샘플링한다.

1029
00:40:50,320 --> 00:40:52,862
이것은 알려진 노이즈 분포에서

1030
00:40:52,862 --> 00:40:54,500
샘플링된 순수 노이즈이다.

1031
00:40:54,500 --> 00:40:56,880
그런 다음 t가 1부터

1032
00:40:56,880 --> 00:40:59,663
시작하여 루프를 진행한다.

1033
00:40:59,663 --> 00:41:01,580
t가 0까지 거슬러 올라간다.

1034
00:41:01,580 --> 00:41:02,735
이것이 노이즈 수준이다.

1035
00:41:02,735 --> 00:41:04,360
이 간단한 버전에서는

1036
00:41:04,360 --> 00:41:07,920
완전한 노이즈 1에서 노이즈 0으로

1037
00:41:07,920 --> 00:41:11,025
완벽하게 깨끗하게 선형적으로 진행한다.

1038
00:41:11,025 --> 00:41:12,400
첫 번째 반복에서

1039
00:41:12,400 --> 00:41:16,400
처음에 완전한 노이즈였던 xt를 현재 노이즈 수준과

1040
00:41:16,400 --> 00:41:18,880
함께 네트워크에 전달하고

1041
00:41:18,880 --> 00:41:21,280
네트워크의 예측 vt를 얻는다.

1042
00:41:21,280 --> 00:41:23,180
정류 흐름의 경우 이 vt가

1043
00:41:23,180 --> 00:41:24,520
무엇인지 기억하라.

1044
00:41:24,520 --> 00:41:28,840
이 v는 데이터 샘플에서 노이즈 샘플로

1045
00:41:28,840 --> 00:41:30,820
향해야 한다.

1046
00:41:30,820 --> 00:41:32,480
따라서 정류 흐름의 경우

1047
00:41:32,480 --> 00:41:34,893
무엇을 해야 할지 기하학적으로 명확하다.

1048
00:41:34,893 --> 00:41:36,560
예측된 v 벡터를 따라

1049
00:41:36,560 --> 00:41:40,440
작은 단계를 밟아야 한다. 문제는 이 정류 흐름 모델에서

1050
00:41:40,440 --> 00:41:42,560
v가 깨끗한 샘플로 향하지

1051
00:41:42,560 --> 00:41:43,810
않는다는 것이다.

1052
00:41:43,810 --> 00:41:45,268
그것은 시작하는 데 도움을 줄 뿐이다.

1053
00:41:45,268 --> 00:41:46,840
깨끗한 샘플로 향하는

1054
00:41:46,840 --> 00:41:48,458
궤적을 설정할 것이다.

1055
00:41:48,458 --> 00:41:51,000
따라서 흐름 모델에서 예측된 v를

1056
00:41:51,000 --> 00:41:54,832
따라 작은 단계를 밟아 새로운 x2를 얻는다. 이제 모델에

1057
00:41:54,832 --> 00:41:56,540
의해 약간의 노이즈가

1058
00:41:56,540 --> 00:41:58,180
제거된 데이터 버전이다.

1059
00:41:58,180 --> 00:41:59,480
이제 이것을 반복한다.

1060
00:41:59,480 --> 00:42:01,805
x2를 얻은 후 모델에 다시

1061
00:42:01,805 --> 00:42:03,180
전달하고 모델에서

1062
00:42:03,180 --> 00:42:06,260
또 다른 예측 v 벡터를 얻는다.

1063
00:42:06,260 --> 00:42:10,740
v는 깨끗한 샘플에서 노이즈 샘플로 향해야 한다는

1064
00:42:10,740 --> 00:42:12,240
것을 기억하라.

1065
00:42:12,240 --> 00:42:14,040
따라서 다시 한 번 기울기 단계를

1066
00:42:14,040 --> 00:42:15,780
밟고 이 예측된 v를 따라 작은

1067
00:42:15,780 --> 00:42:17,820
단계를 밟아 또 다른 x1/3을 얻는다.

1068
00:42:17,820 --> 00:42:19,820
이 과정을 다시 반복한다.

1069
00:42:19,820 --> 00:42:22,460
모델을 다시 평가하여 또 다른 예측

1070
00:42:22,460 --> 00:42:25,940
v를 얻고, 이 경우 노이즈가 없는 상태로

1071
00:42:25,940 --> 00:42:29,900
그 벡터의 끝까지 나아가 우리의 예측 x0을 얻는다.

1072
00:42:29,900 --> 00:42:32,460
그것이 우리의 확산 모델에서의 샘플이다.

1073
00:42:32,460 --> 00:42:34,940
따라서 여기서 보이는 추론

1074
00:42:34,940 --> 00:42:38,100
절차는 GAN에 비해 조금 더

1075
00:42:38,100 --> 00:42:41,360
복잡해졌지만, 우리가 얻은 것은 안정성이다.

1076
00:42:41,360 --> 00:42:42,860
훈련 중에 그것을

1077
00:42:42,860 --> 00:42:45,960
회복했으며, 훨씬 더 나은 샘플을 제공하고 대규모

1078
00:42:45,960 --> 00:42:48,060
데이터 세트와 대규모 모델에

1079
00:42:48,060 --> 00:42:49,822
잘 확장되는 경향이 있다.

1080
00:42:49,822 --> 00:42:51,280
여기 코드는 정말 간단하다.

1081
00:42:51,280 --> 00:42:54,340
따라서 무작위 샘플을 가져와 완전히

1082
00:42:54,340 --> 00:42:57,200
무작위로 만든 다음, t가 1에서

1083
00:42:57,200 --> 00:42:59,520
0으로 거슬러 올라간다.

1084
00:42:59,520 --> 00:43:01,558
모든 노이즈 수준에서

1085
00:43:01,558 --> 00:43:03,600
현재 샘플과 T를 고려하여

1086
00:43:03,600 --> 00:43:06,560
모델에서 예측된 v를 얻는다. 그런 다음 모델의 예측된 v에 대해 기울기
하강

1087
00:43:06,560 --> 00:43:09,680
단계처럼 보이는 것을 취하고 샘플을 업데이트하며

1088
00:43:09,680 --> 00:43:11,840
이 전체 과정을 루프에서 반복한다.

1089
00:43:11,840 --> 00:43:14,240
그러니까 이 확산 모델들이 그렇게 무섭지 않다는

1090
00:43:14,240 --> 00:43:15,640
것을 알 수 있습니다.

1091
00:43:15,640 --> 00:43:17,880
실제로 한 슬라이드에 몇 줄만으로

1092
00:43:17,880 --> 00:43:19,920
정제된 흐름 모델의 훈련

1093
00:43:19,920 --> 00:43:23,540
및 샘플링의 완전한 구현을 적합시킬 수 있습니다.

1094
00:43:23,540 --> 00:43:26,360
저는 이것이 매우 좋다고 생각합니다.

1095
00:43:26,360 --> 00:43:28,780
좋아요, 그래서 질문을 받을 수 있습니다. 이건 꽤 좋습니다.

1096
00:43:28,780 --> 00:43:31,093
우리가 완전한 결과에 도달할 수 있어서 매우 기쁩니다. 그리고

1097
00:43:31,093 --> 00:43:32,260
이건 실제로 작동할 것입니다.

1098
00:43:32,260 --> 00:43:34,600
이 코드를 가져가서 적절한

1099
00:43:34,600 --> 00:43:37,560
모델 아키텍처에 연결하면 실제로 작동할

1100
00:43:37,560 --> 00:43:38,560
것입니다.

1101
00:43:38,560 --> 00:43:40,185
이것은 많은 경우에 합리적인

1102
00:43:40,185 --> 00:43:41,720
것으로 변환될 것입니다.

1103
00:43:41,720 --> 00:43:44,200
당신은 제가 지난 며칠 동안 이 슬라이드를

1104
00:43:44,200 --> 00:43:46,120
검토하면서 많이 생각해온 생성 모델링의

1105
00:43:46,120 --> 00:43:47,580
핵심 문제를 다루고 있습니다.

1106
00:43:47,580 --> 00:43:49,320
생성 모델링의 핵심 문제는

1107
00:43:49,320 --> 00:43:51,740
어떻게든 샘플링할 수 있는 사전

1108
00:43:51,740 --> 00:43:53,520
분포 z가 있다는 것입니다.

1109
00:43:53,520 --> 00:43:55,103
당신은 생성하고자 하는

1110
00:43:55,103 --> 00:43:56,680
데이터 분포 x가 있습니다.

1111
00:43:56,680 --> 00:43:58,700
그리고 생성 모델링의 핵심

1112
00:43:58,700 --> 00:44:01,600
문제는 z와 x를 연결하고 다양한 범주를 생성

1113
00:44:01,600 --> 00:44:03,600
모델링에 맞게 다르게 처리하는

1114
00:44:03,600 --> 00:44:05,340
방법을 찾는 것입니다.

1115
00:44:05,340 --> 00:44:09,100
VAE에서는 모델이 z를 예측하고 x를 예측한 다음, 그

1116
00:44:09,100 --> 00:44:11,460
z가 내가 샘플링할 수 있는 무언가가

1117
00:44:11,460 --> 00:44:13,380
되도록 강제하려고 합니다.

1118
00:44:13,380 --> 00:44:15,420
하지만 그건 잘 작동하지 않습니다.

1119
00:44:15,420 --> 00:44:18,598
GAN에서는 그 관계를 감독하지 않습니다.

1120
00:44:18,598 --> 00:44:20,140
생성기는 이 분포

1121
00:44:20,140 --> 00:44:23,107
일치 목표를 통해 z에서 x로의 자체

1122
00:44:23,107 --> 00:44:24,940
매핑을 피드포워드 방식으로

1123
00:44:24,940 --> 00:44:26,740
파악하고 있습니다.

1124
00:44:26,740 --> 00:44:30,860
확산에서는 이러한 곡선을 통합해야 하는

1125
00:44:30,860 --> 00:44:33,700
방식으로 파악하고 있습니다.

1126
00:44:33,700 --> 00:44:37,580
실제로 이러한 목표가 왜 합리적인 방식으로

1127
00:44:37,580 --> 00:44:40,060
확률 분포를 일치시키는지에 대한

1128
00:44:40,060 --> 00:44:41,900
여러 가지 수학적

1129
00:44:41,900 --> 00:44:43,220
형식이 있습니다.

1130
00:44:43,220 --> 00:44:45,420
하지만 다시 말해, 핵심

1131
00:44:45,420 --> 00:44:48,180
문제는 사전에서 샘플 z와 데이터에서

1132
00:44:48,180 --> 00:44:51,280
샘플 x를 미리 쌍으로 만들 방법이 없다는

1133
00:44:51,280 --> 00:44:52,340
것입니다.

1134
00:44:52,340 --> 00:44:53,840
그 쌍을 만드는 방법과

1135
00:44:53,840 --> 00:44:57,487
사전에서 샘플링하는 방법을 알았다면 끝났을 것입니다.

1136
00:44:57,487 --> 00:44:59,320
어떤 의미에서 이러한

1137
00:44:59,320 --> 00:45:01,040
다양한 생성 모델링

1138
00:45:01,040 --> 00:45:02,720
형태는 z에서 x로의

1139
00:45:02,720 --> 00:45:06,960
연관성을 배우고 훈련 시간에 그 연관성이 없더라도 z에서

1140
00:45:06,960 --> 00:45:08,680
샘플링할 수 있는

1141
00:45:08,680 --> 00:45:11,210
방법을 찾는 다양한 방법입니다.

1142
00:45:11,210 --> 00:45:12,960
이것에 대한 다양한

1143
00:45:12,960 --> 00:45:15,500
해석이 있으며, 매우

1144
00:45:15,500 --> 00:45:19,040
복잡해질 수 있으므로 피하려고 했습니다.

1145
00:45:19,040 --> 00:45:23,000
하지만 우리는 지난 강의에서 무조건 생성 모델링은 무의미하다고

1146
00:45:23,000 --> 00:45:26,320
말했으므로 우리가 거의 항상 관심을 가지는

1147
00:45:26,320 --> 00:45:28,260
것은 조건부 생성 모델링입니다.

1148
00:45:28,260 --> 00:45:30,700
그리고 이것은 정제된 흐름에서 쉽게 수용할 수 있습니다.

1149
00:45:30,700 --> 00:45:32,820
조건부 정제 흐름을 수행하기 위해

1150
00:45:32,820 --> 00:45:35,520
우리는 데이터 분포의 서로 다른 하위 부분이 있다고

1151
00:45:35,520 --> 00:45:36,320
상상합니다.

1152
00:45:36,320 --> 00:45:37,780
여기서 저는 범주형이라고 말하고 있습니다.

1153
00:45:37,780 --> 00:45:40,320
우리 데이터가 실제로 사각형과 삼각형일 수 있습니다.

1154
00:45:40,320 --> 00:45:43,520
그리고 우리는 전체 데이터 분포 pdata와

1155
00:45:43,520 --> 00:45:47,400
y가 사각형일 때의 두 하위 분포 pdata x와

1156
00:45:47,400 --> 00:45:51,583
y가 삼각형일 때의 pdata x를 가지고 있습니다.

1157
00:45:51,583 --> 00:45:53,500
그래서 이것은 조건부 생성

1158
00:45:53,500 --> 00:45:56,380
모델링을 생각할 때 마음에 두어야 할 그림입니다.

1159
00:45:56,380 --> 00:45:57,820
그런 다음 정제된 흐름의 경우

1160
00:45:57,820 --> 00:45:59,520
이것은 매우 쉽게 수용할 수 있습니다.

1161
00:45:59,520 --> 00:46:02,740
따라서 이제 데이터 세트는 쌍 x와 y를 가지고

1162
00:46:02,740 --> 00:46:05,380
있으며 모델은 이제 y를 추가 보조 입력으로

1163
00:46:05,380 --> 00:46:06,300
사용합니다.

1164
00:46:06,300 --> 00:46:09,860
그리고 샘플링 중에도 같은 방식입니다.

1165
00:46:09,860 --> 00:46:11,460
예측된 V를 얻습니다.

1166
00:46:11,460 --> 00:46:14,840
모델은 이 추가 y를 입력으로 사용하고 그것을 사용합니다.

1167
00:46:14,840 --> 00:46:16,500
그래서 이 모든 것이 통과합니다.

1168
00:46:16,500 --> 00:46:18,700
차이점은 이제 y가 사용자가

1169
00:46:18,700 --> 00:46:20,100
제어할 수 있는 조건

1170
00:46:20,100 --> 00:46:21,553
신호라는 것입니다.

1171
00:46:21,553 --> 00:46:22,720
아마도 이것은 텍스트 프롬프트일 것입니다.

1172
00:46:22,720 --> 00:46:23,928
아마도 이것은 입력 이미지일 것입니다.

1173
00:46:23,928 --> 00:46:25,823
아마도 이것은 추론 시

1174
00:46:25,823 --> 00:46:27,740
기대하는 사용자 입력의 일종으로,

1175
00:46:27,740 --> 00:46:29,823
실제로 이러한 모델을 제어

1176
00:46:29,823 --> 00:46:31,740
가능하고 유용하게 만듭니다.

1177
00:46:31,740 --> 00:46:34,326
하지만 또 다른 정말 흥미로운 질문이 있습니다.

1178
00:46:34,326 --> 00:46:37,140
모델이 조건 신호에 얼마나 주의를

1179
00:46:37,140 --> 00:46:39,580
기울이는지를 조정할 수 있는 노브가

1180
00:46:39,580 --> 00:46:40,240
있나요?

1181
00:46:40,240 --> 00:46:42,368
이것들을 단순하게 훈련하면,

1182
00:46:42,368 --> 00:46:44,660
많은 경우 원하는 만큼 조건 신호를

1183
00:46:44,660 --> 00:46:46,900
따르지 않는 경우가 많습니다.

1184
00:46:46,900 --> 00:46:49,320
그래서 분류기 없는 가이던스 또는

1185
00:46:49,320 --> 00:46:54,040
CFG라는 트릭이 있습니다. 이는 우리의 확산 훈련 루프를

1186
00:46:54,040 --> 00:46:55,280
조금 변경합니다.

1187
00:46:55,280 --> 00:46:56,480
우리가 할 것은

1188
00:46:56,480 --> 00:46:58,647
여전히 xt와 y를 입력으로

1189
00:46:58,647 --> 00:47:03,900
하는 조건부 확산 모델을 훈련시키는 것입니다. 하지만 매 훈련 반복마다

1190
00:47:03,900 --> 00:47:05,360
동전을 던질 것입니다.

1191
00:47:05,360 --> 00:47:08,040
그 동전이 앞면이면 조건 정보를 삭제할

1192
00:47:08,040 --> 00:47:08,760
것입니다.

1193
00:47:08,760 --> 00:47:10,760
그래서 우리는 그것을 어떤 0 값이나

1194
00:47:10,760 --> 00:47:13,680
null 값으로 설정하여, 기본적으로 50%의

1195
00:47:13,680 --> 00:47:15,502
시간 동안 조건 정보를 파괴합니다.

1196
00:47:15,502 --> 00:47:17,960
그것은 하이퍼파라미터가 될 수 있지만, 50은 대부분의 사람들이

1197
00:47:17,960 --> 00:47:19,335
실제로 사용하는 꽤 좋은 값입니다.

1198
00:47:19,335 --> 00:47:20,585
그래서 우리는 동전을 던질 것입니다.

1199
00:47:20,585 --> 00:47:23,047
동전이 앞면이면 조건 정보를 삭제합니다.

1200
00:47:23,047 --> 00:47:24,880
즉, 모델이 개념적으로 이제 두

1201
00:47:24,880 --> 00:47:27,610
가지 다른 종류의 속도 벡터를 학습하도록 강요받는

1202
00:47:27,610 --> 00:47:28,110
것입니다.

1203
00:47:32,640 --> 00:47:36,040
그래서 모델이 두 가지 다른 종류의 속도 벡터를

1204
00:47:36,040 --> 00:47:37,420
학습하도록 강요받습니다.

1205
00:47:37,420 --> 00:47:42,240
y에 대해 이 null 값을 전달하는 경우,

1206
00:47:42,240 --> 00:47:44,180
이는 기본적으로

1207
00:47:44,180 --> 00:47:46,880
무조건 생성 모델이

1208
00:47:46,880 --> 00:47:47,500
됩니다.

1209
00:47:47,500 --> 00:47:51,660
이제 예측된 속도 벡터 v는 전체 데이터

1210
00:47:51,660 --> 00:47:55,780
분포 pdata의 중심을 향해야 하지만,

1211
00:47:55,780 --> 00:47:58,380
파괴되지 않고 null이

1212
00:47:58,380 --> 00:48:01,620
아닌 실제 조건 입력 y를

1213
00:48:01,620 --> 00:48:04,580
전달하면, 우리는 전체 데이터

1214
00:48:04,580 --> 00:48:07,973
분포가 아니라 조건 신호에 따라

1215
00:48:07,973 --> 00:48:10,140
조건화된 데이터 분포를

1216
00:48:10,140 --> 00:48:11,890
향하는 조건 속도

1217
00:48:11,890 --> 00:48:13,460
벡터를 얻습니다.

1218
00:48:13,460 --> 00:48:15,620
그리고 그 멍청한 트릭은

1219
00:48:15,620 --> 00:48:18,340
이 두 벡터의 선형

1220
00:48:18,340 --> 00:48:24,260
조합을 취하여 조건 속도 벡터 쪽으로 더 밀어주는 것입니다.

1221
00:48:24,260 --> 00:48:26,860
특히 스칼라 하이퍼파라미터 w를

1222
00:48:26,860 --> 00:48:29,100
두고 1 더하기 vy에서 w 곱하기

1223
00:48:29,100 --> 00:48:32,362
v null을 뺀 선형 조합을 취합니다.

1224
00:48:32,362 --> 00:48:33,820
그래서 이제

1225
00:48:33,820 --> 00:48:38,580
데이터 분포보다 조건 분포 쪽으로 더 향하는

1226
00:48:38,580 --> 00:48:40,380
벡터가 됩니다.

1227
00:48:40,380 --> 00:48:44,498
그리고 샘플링 중에 이제

1228
00:48:44,498 --> 00:48:46,040
이 v

1229
00:48:46,040 --> 00:48:51,360
CFG 벡터에 따라 스텝을 진행할

1230
00:48:51,360 --> 00:48:53,560
것입니다.

1231
00:48:53,560 --> 00:48:57,000
그리고 w를 1로 설정하면-- 여기서

1232
00:48:57,000 --> 00:48:59,800
w를 0으로 설정하면 정확히

1233
00:48:59,800 --> 00:49:01,740
조건부 1을 복원합니다.

1234
00:49:01,740 --> 00:49:03,592
w가 높을수록 조건 신호를

1235
00:49:03,592 --> 00:49:05,550
더 강조하게 됩니다.

1236
00:49:08,120 --> 00:49:10,020
그리고 이것은 구현하기 꽤 쉽습니다.

1237
00:49:10,020 --> 00:49:13,560
그래서 추론 코드는 크게 변경되지 않지만, 이제

1238
00:49:13,560 --> 00:49:15,960
매 반복마다 모델을 두 번

1239
00:49:15,960 --> 00:49:18,120
평가하여 vy와 v0를 얻습니다.

1240
00:49:18,120 --> 00:49:20,320
그리고 이 선형 조합을 취한 다음

1241
00:49:20,320 --> 00:49:22,280
그에 따라 스텝을 진행합니다.

1242
00:49:22,280 --> 00:49:25,792
이것은 어리석은 이유로 분류기 없음이라고 불립니다.

1243
00:49:25,792 --> 00:49:28,000
이전에 분류기 가이던스라는 논문이 있었는데, 그에

1244
00:49:28,000 --> 00:49:29,340
대해 이야기하고 싶지 않습니다.

1245
00:49:29,340 --> 00:49:31,040
그리고 그들은 분류기를 제거했습니다.

1246
00:49:31,040 --> 00:49:33,040
그 두 논문 사이에는 단 9개월밖에 없었고,

1247
00:49:33,040 --> 00:49:35,700
두 번째 논문 이후로는 4년이 지났지만, 우리는 여전히

1248
00:49:35,700 --> 00:49:38,075
분류기 없는 가이던스라는 이름에 얽매여 있습니다.

1249
00:49:38,075 --> 00:49:39,555
그래서 어쩔 수 없습니다.

1250
00:49:39,555 --> 00:49:41,680
그래서 실제로 고품질 출력을 얻기

1251
00:49:41,680 --> 00:49:44,440
위해서는 매우 중요하고, 그것이 CFG입니다.

1252
00:49:44,440 --> 00:49:45,440
정말 중요합니다.

1253
00:49:45,440 --> 00:49:47,540
확산 모델에서 어디에나 사용됩니다.

1254
00:49:47,540 --> 00:49:49,420
하지만 샘플링 비용이 두 배로

1255
00:49:49,420 --> 00:49:51,628
늘어나는데, 이제 매 반복마다 모델을 두

1256
00:49:51,628 --> 00:49:53,692
번 호출해야 하므로 문제가 됩니다.

1257
00:49:53,692 --> 00:49:55,400
최적 예측에 대한 것이 있습니다.

1258
00:49:55,400 --> 00:49:56,358
그건 건너뛰겠습니다.

1259
00:49:56,358 --> 00:49:58,100
그렇게 흥미롭지 않습니다.

1260
00:49:58,100 --> 00:50:01,880
흥미롭긴 하지만 시간이 걱정됩니다.

1261
00:50:01,880 --> 00:50:04,060
하지만 우리가 가끔 해야

1262
00:50:04,060 --> 00:50:08,200
하는 일 중 하나는 이 t 분포를 조정하는 것입니다.

1263
00:50:08,200 --> 00:50:10,660
특히 원시 정류 흐름

1264
00:50:10,660 --> 00:50:13,060
모델에서 균일 분포에 따라

1265
00:50:13,060 --> 00:50:15,540
t를 샘플링하고 있었습니다.

1266
00:50:15,540 --> 00:50:18,020
그것에 대한 것은 모든 잡음

1267
00:50:18,020 --> 00:50:21,320
수준에 균일한 강조를 두게 됩니다.

1268
00:50:21,320 --> 00:50:23,920
직관적으로, 완전한 잡음

1269
00:50:23,920 --> 00:50:26,740
상태에서는 문제가 매우 쉽습니다.

1270
00:50:26,740 --> 00:50:29,400
완전한 잡음 상태에서는 문제가 매우 쉽습니다.

1271
00:50:29,400 --> 00:50:31,697
그런 다음 모델의 최적 예측은

1272
00:50:31,697 --> 00:50:33,780
기본적으로 데이터 분포의 평균을

1273
00:50:33,780 --> 00:50:34,860
가리키는 것입니다.

1274
00:50:34,860 --> 00:50:37,760
유사하게, 잡음이 0일 때 최적

1275
00:50:37,760 --> 00:50:39,420
예측은 실제로 잡음

1276
00:50:39,420 --> 00:50:42,300
분포의 평균을 가리키는 것입니다.

1277
00:50:42,300 --> 00:50:44,680
따라서 실제로 완전한 잡음과 완전한

1278
00:50:44,680 --> 00:50:48,240
데이터 및 잡음이 없는 경우의 모델의 최적 예측은 실제로

1279
00:50:48,240 --> 00:50:50,180
상대적으로 쉬운 문제입니다.

1280
00:50:50,180 --> 00:50:53,080
그냥 두 분포의 평균을 학습해야 합니다.

1281
00:50:53,080 --> 00:50:55,180
하지만 중간에 있을 때는

1282
00:50:55,180 --> 00:50:57,890
실제로 매우 어렵습니다. 왜냐하면

1283
00:50:57,890 --> 00:51:00,140
중간에 있을 때 xt를 샘플링하면

1284
00:51:00,140 --> 00:51:02,280
그 같은 xt를 생성할 수

1285
00:51:02,280 --> 00:51:06,420
있는 여러 쌍 x와 z가 있을 수 있기 때문입니다.

1286
00:51:06,420 --> 00:51:07,920
그런 다음 네트워크는

1287
00:51:07,920 --> 00:51:09,600
기본적으로 이 기대값 문제를

1288
00:51:09,600 --> 00:51:11,960
해결해야 하며, 이 지점 xt에서

1289
00:51:11,960 --> 00:51:15,920
교차할 수 있는 모든 가능한 x와 z를 통합하는 최적의 예측

1290
00:51:15,920 --> 00:51:17,823
방향이 무엇인지 알아내야 합니다.

1291
00:51:17,823 --> 00:51:20,240
그래서 중간의 그런 점들은 직관적으로

1292
00:51:20,240 --> 00:51:24,320
네트워크가 해결하기 훨씬 더 어렵지만, 0에서 1t까지

1293
00:51:24,320 --> 00:51:28,280
균일하게 샘플링하면 모든 잡음 수준에 동일한 중요성을 부여하게

1294
00:51:28,280 --> 00:51:30,740
되어 이 직관과 잘 맞지 않습니다.

1295
00:51:30,740 --> 00:51:32,320
그래서 실제로는 종종 서로

1296
00:51:32,320 --> 00:51:34,040
다른 잡음 스케줄에서 샘플링합니다.

1297
00:51:34,040 --> 00:51:35,680
그리고 매우 인기 있는 것 중

1298
00:51:35,680 --> 00:51:38,320
하나는 로그잇-정규 샘플링이라고 불리는 것으로, 기본적으로

1299
00:51:38,320 --> 00:51:40,660
가우시안처럼 보이며, 0과 1에 상대적으로

1300
00:51:40,660 --> 00:51:43,683
적은 가중치를 두고 중간에 훨씬 더 많은 가중치를 둡니다.

1301
00:51:43,683 --> 00:51:45,100
때때로 볼 수 있는 또

1302
00:51:45,100 --> 00:51:47,100
다른 것은 비대칭적인 이른바 이동 잡음

1303
00:51:47,100 --> 00:51:49,260
스케줄로, 한 방향이나 다른 방향으로

1304
00:51:49,260 --> 00:51:50,500
더 많이 이동합니다.

1305
00:51:50,500 --> 00:51:53,118
그리고 그것들은 고해상도 데이터로 확장할 때 중요합니다.

1306
00:51:53,118 --> 00:51:55,660
직관적으로, 매우 고해상도 이미지를 가지면

1307
00:51:55,660 --> 00:51:57,220
이웃 픽셀 간에 매우

1308
00:51:57,220 --> 00:51:59,138
강한 상관관계가 있을 수 있습니다.

1309
00:51:59,138 --> 00:52:00,680
저해상도 이미지를 가지면

1310
00:52:00,680 --> 00:52:02,660
이웃 픽셀 간의 상관관계가 작아지는

1311
00:52:02,660 --> 00:52:03,820
경향이 있습니다.

1312
00:52:03,820 --> 00:52:05,980
따라서 데이터에서 상관관계가

1313
00:52:05,980 --> 00:52:07,620
얼마나 강한지에 따라,

1314
00:52:07,620 --> 00:52:09,500
정보를 적절하게 파괴하기 위해

1315
00:52:09,500 --> 00:52:12,300
필요한 잡음 수준이 다를 수 있습니다.

1316
00:52:12,300 --> 00:52:16,490
그래서 이러한 것들은 다른 해상도로 단순하게 확장되지 않습니다.

1317
00:52:16,490 --> 00:52:18,740
그리고 이것이 확산 모델의

1318
00:52:18,740 --> 00:52:21,600
큰 문제입니다. 아름다운 공식이지만

1319
00:52:21,600 --> 00:52:23,940
고해상도 데이터에서 단순하게

1320
00:52:23,940 --> 00:52:26,180
작동하게 하기가 어렵습니다.

1321
00:52:26,180 --> 00:52:28,100
그래서 실제로는-- 나는 확산

1322
00:52:28,100 --> 00:52:30,620
모델이 생성 모델링의 가장 인기 있는

1323
00:52:30,620 --> 00:52:31,800
형태라고 말했습니다.

1324
00:52:31,800 --> 00:52:34,092
그건 약간의 거짓말이었습니다. 실제로

1325
00:52:34,092 --> 00:52:36,140
가장 인기 있는 것은 이른바

1326
00:52:36,140 --> 00:52:39,840
잠재 확산 모델로, 실제로 어디에서나 사용되는 변형입니다.

1327
00:52:39,840 --> 00:52:42,080
그래서 여기서는 다단계 절차가 될 것입니다.

1328
00:52:42,080 --> 00:52:45,280
우리가 할 일은 먼저 인코더 네트워크와 디코더 네트워크를

1329
00:52:45,280 --> 00:52:46,580
훈련하는 것입니다.

1330
00:52:46,580 --> 00:52:49,000
인코더 네트워크는 우리의 이미지를 어떤

1331
00:52:49,000 --> 00:52:52,740
잠재 공간으로 매핑할 것이며, 나는 그것을 보라색으로 칠했습니다.

1332
00:52:52,740 --> 00:52:55,160
이 잠재 공간은 이상적으로 이미지를 D

1333
00:52:55,160 --> 00:52:58,480
배율로 공간적으로 다운샘플링하고, 세 개의 채널에서

1334
00:52:58,480 --> 00:53:00,175
C 채널로 변환할 것입니다.

1335
00:53:00,175 --> 00:53:01,800
가장 일반적인 설정은 8x8

1336
00:53:01,800 --> 00:53:05,540
공간 다운샘플링을 얻고 16채널로 증가시키는 것입니다.

1337
00:53:05,540 --> 00:53:08,680
이것이 가장 일반적인 인코더-디코더 중 일부입니다.

1338
00:53:08,680 --> 00:53:11,460
이 인코더-디코더는 주로 주의(attention)가

1339
00:53:11,460 --> 00:53:15,440
있는 CNN이지만, 최근의 몇몇 논문에서는 VIT를 탐구했습니다.

1340
00:53:15,440 --> 00:53:18,240
그런 다음 우리는 원시 픽셀 공간이

1341
00:53:18,240 --> 00:53:20,180
아닌 이 인코더-디코더

1342
00:53:20,180 --> 00:53:22,560
모델이 발견한 잠재 공간에서 확산

1343
00:53:22,560 --> 00:53:24,420
모델을 훈련할 것입니다.

1344
00:53:24,420 --> 00:53:27,720
따라서 확산 모델을 훈련하는 절차는 이미지를

1345
00:53:27,720 --> 00:53:29,857
샘플링하고, 첫 번째 단계에서 학습한

1346
00:53:29,857 --> 00:53:32,440
인코더를 통해 통과시켜 잠재 공간을

1347
00:53:32,440 --> 00:53:35,680
얻고, 그 잠재 공간에 노이즈를 추가한 후,

1348
00:53:35,680 --> 00:53:40,200
노이즈 잠재 공간을 디노이즈하도록 확산 모델을 훈련하는 것입니다.

1349
00:53:40,200 --> 00:53:42,580
아주 중요하게도, 인코더를 동결하여

1350
00:53:42,580 --> 00:53:46,480
그래디언트를 인코더로 다시 전파하지 않도록 합니다.

1351
00:53:46,480 --> 00:53:48,580
우리는 단지 이 잠재 공간을 추출하는

1352
00:53:48,580 --> 00:53:50,413
데 사용하고, 인코더가 학습한

1353
00:53:50,413 --> 00:53:52,780
잠재 공간에서 직접 확산 모델을 훈련합니다.

1354
00:53:52,780 --> 00:53:55,240
그런 다음 추론 시간에 모든 훈련이 끝나면,

1355
00:53:55,240 --> 00:53:57,180
무작위 잠재 공간을 샘플링하고, 확산

1356
00:53:57,180 --> 00:53:59,430
모델을 통해 여러 번 통과시켜 모든 노이즈를

1357
00:53:59,430 --> 00:54:01,080
제거하여 깨끗한 샘플을 얻지만,

1358
00:54:01,080 --> 00:54:04,260
그 깨끗한 샘플은 이제 잠재 공간에서의 깨끗한 샘플입니다.

1359
00:54:04,260 --> 00:54:05,820
그래서 우리는 그 깨끗한 잠재

1360
00:54:05,820 --> 00:54:09,060
공간을 깨끗한 이미지로 변환하기 위해 디코더를 실행해야 합니다.

1361
00:54:09,060 --> 00:54:11,060
이것이 사실 요즘 대부분의

1362
00:54:11,060 --> 00:54:13,860
확산 모델의 가장 일반적인 형태입니다.

1363
00:54:13,860 --> 00:54:16,300
그래서 당신은 아마도 '좋아, 우리는 이 확산 모델, 이

1364
00:54:16,300 --> 00:54:18,560
인코더-디코더를 만들었습니다.'라고 물어볼 것입니다.

1365
00:54:18,560 --> 00:54:20,160
인코더-디코더를 어떻게 훈련합니까?

1366
00:54:20,160 --> 00:54:22,420
아이디어가 있습니까?

1367
00:54:22,420 --> 00:54:25,020
우리가 인코더-디코더를 본 적이 있습니까?

1368
00:54:25,020 --> 00:54:26,837
변분 오토인코더는 어떻습니까?

1369
00:54:26,837 --> 00:54:29,420
실제로 이러한 잠재 확산 모델을 훈련할

1370
00:54:29,420 --> 00:54:31,700
때, 이 인코더-디코더는 변분

1371
00:54:31,700 --> 00:54:34,623
오토인코더인 경향이 있지만, 변분 오토인코더에는

1372
00:54:34,623 --> 00:54:37,040
흐릿한 출력을 제공하는 큰 문제가

1373
00:54:37,040 --> 00:54:38,582
있다고 방금 말했습니다.

1374
00:54:38,582 --> 00:54:40,800
그리고 이 인코더-디코더가 흐릿한

1375
00:54:40,800 --> 00:54:43,880
출력을 제공한다면, 디코더에서 얻는 재구성의

1376
00:54:43,880 --> 00:54:45,400
품질이 다운스트림 확산

1377
00:54:45,400 --> 00:54:47,520
모델에서 얻는 생성의 품질을 병목

1378
00:54:47,520 --> 00:54:49,020
현상으로 만들 것입니다.

1379
00:54:49,020 --> 00:54:51,600
따라서 인코더-디코더가 흐릿하고 못생긴 재구성을

1380
00:54:51,600 --> 00:54:54,220
제공한다면, 그것은 통하지 않을 것입니다.

1381
00:54:54,220 --> 00:54:56,360
그것은 좋은 깨끗한 샘플을 얻지 못하게 할 것입니다.

1382
00:54:56,360 --> 00:54:58,440
그래서 VAE의 샘플 품질을

1383
00:54:58,440 --> 00:55:01,600
개선할 아이디어가 있는 사람 있습니까?

1384
00:55:01,600 --> 00:55:04,840
하지만 디코더 이후에 특히, 우리는

1385
00:55:04,840 --> 00:55:07,640
그것을 GAN으로 만들 수 있습니다.

1386
00:55:07,640 --> 00:55:10,200
그래서 우리가 하는 일은 이미지를

1387
00:55:10,200 --> 00:55:12,960
잠재 공간으로 인코딩하는 인코더, 잠재

1388
00:55:12,960 --> 00:55:15,800
공간에서 이미지를 다시 생성하는 디코더,

1389
00:55:15,800 --> 00:55:17,440
가짜 이미지를 진짜 이미지와

1390
00:55:17,440 --> 00:55:20,560
구별하려고 하는 판별자, 그리고 잠재

1391
00:55:20,560 --> 00:55:23,360
공간에서 이러한 것들을 생성하는 확산 모델을

1392
00:55:23,360 --> 00:55:24,820
훈련하는 것입니다.

1393
00:55:24,820 --> 00:55:27,400
그래서 이것이 현대 파이프라인을

1394
00:55:27,400 --> 00:55:30,455
이해하기 위해 생성 모델의 다양한 형식을

1395
00:55:30,455 --> 00:55:32,580
모두 살펴봐야 하는 이유입니다.

1396
00:55:32,580 --> 00:55:35,020
기본적으로 생성 모델링의 최첨단은

1397
00:55:35,020 --> 00:55:36,260
VAE입니까?

1398
00:55:36,260 --> 00:55:36,920
GAN입니까?

1399
00:55:36,920 --> 00:55:37,880
확산입니까?

1400
00:55:37,880 --> 00:55:39,180
모두입니다.

1401
00:55:39,180 --> 00:55:40,120
모두입니다.

1402
00:55:40,120 --> 00:55:41,980
현대 생성 모델링 파이프라인은

1403
00:55:41,980 --> 00:55:45,680
VAE와 GAN, 확산 모델을 훈련하는 것을 포함합니다.

1404
00:55:45,680 --> 00:55:46,280
죄송합니다.

1405
00:55:46,280 --> 00:55:47,820
엉망입니다.

1406
00:55:47,820 --> 00:55:51,060
그렇다면 여러분은 신경망이 실제로 어떻게 생겼는지

1407
00:55:51,060 --> 00:55:52,820
궁금해할 수 있습니다.

1408
00:55:52,820 --> 00:55:55,180
다행히도 지난 몇 년간 여기에

1409
00:55:55,180 --> 00:55:57,480
약간의 이성이 있습니다.

1410
00:55:57,480 --> 00:55:59,940
상대적으로 간단한 변환기들이 이러한

1411
00:55:59,940 --> 00:56:02,040
확산 모델에 적용될 수 있으며,

1412
00:56:02,040 --> 00:56:03,500
정말 잘 작동합니다.

1413
00:56:03,500 --> 00:56:06,760
이것들은 일반적으로 확산 변환기 또는 DiT라고

1414
00:56:06,760 --> 00:56:09,320
불리지만, 기본적으로 표준

1415
00:56:09,320 --> 00:56:11,820
확산 모델이나 특별한 요소가 없는

1416
00:56:11,820 --> 00:56:13,620
표준 변환기 블록입니다.

1417
00:56:13,620 --> 00:56:15,180
몇 가지 질문이 있습니다.

1418
00:56:15,180 --> 00:56:17,740
건축적 측면에서 해결해야 할 주요 질문은

1419
00:56:17,740 --> 00:56:20,460
조건 정보를 어떻게 주입할 것인가입니다.

1420
00:56:20,460 --> 00:56:22,060
특히, 확산 모델은 이제 세

1421
00:56:22,060 --> 00:56:24,038
가지를 입력으로 받아야 합니다.

1422
00:56:24,038 --> 00:56:25,580
잡음이 있는 이미지와 타임스탬프

1423
00:56:25,580 --> 00:56:26,982
t를 입력으로 받아야 합니다.

1424
00:56:26,982 --> 00:56:28,940
또한 텍스트와 같은

1425
00:56:28,940 --> 00:56:31,920
조건 신호를 입력으로 받아야 합니다.

1426
00:56:31,920 --> 00:56:33,880
그리고 조건 신호를 변환기

1427
00:56:33,880 --> 00:56:36,520
블록에 주입하는 몇 가지 다른 메커니즘이

1428
00:56:36,520 --> 00:56:37,400
있습니다.

1429
00:56:37,400 --> 00:56:40,600
첫 번째는 확산 블록의 중간 활성화를

1430
00:56:40,600 --> 00:56:42,360
조절하는 데 사용될

1431
00:56:42,360 --> 00:56:45,385
스케일과 시프트를 예측하는 것입니다.

1432
00:56:45,385 --> 00:56:47,760
이것이 일반적으로 확산 모델에 타임스탬프

1433
00:56:47,760 --> 00:56:49,760
정보를 주입하는 방법입니다.

1434
00:56:49,760 --> 00:56:51,950
또 다른 방법은 변환기가 단순히 시퀀스

1435
00:56:51,950 --> 00:56:54,200
모델이기 때문에 모든 것을 시퀀스에 넣을

1436
00:56:54,200 --> 00:56:55,360
수 있다는 것입니다.

1437
00:56:55,360 --> 00:56:57,220
타임스탬프를 시퀀스에 넣을 수 있습니다.

1438
00:56:57,220 --> 00:56:58,700
텍스트를 시퀀스에 넣을 수 있습니다.

1439
00:56:58,700 --> 00:57:00,658
원하는 것을 시퀀스에 넣고

1440
00:57:00,658 --> 00:57:03,440
변환기가 그 데이터 시퀀스를 모두 모델링하게 할

1441
00:57:03,440 --> 00:57:04,363
수 있습니다.

1442
00:57:04,363 --> 00:57:06,280
이것은 교차 주의 또는 공동 주의를 통해

1443
00:57:06,280 --> 00:57:09,420
수행할 수 있으며, 다양한 모델이 두 가지 모두를 수행합니다.

1444
00:57:09,420 --> 00:57:11,760
일반적으로 현대의 확산 DiT에서는 이

1445
00:57:11,760 --> 00:57:14,520
스케일 시프트 메커니즘을 통해 타임스탬프를

1446
00:57:14,520 --> 00:57:15,300
주입합니다.

1447
00:57:15,300 --> 00:57:17,840
텍스트나 다른 조건 신호는 일반적으로

1448
00:57:17,840 --> 00:57:20,440
교차 주의 또는 때때로 공동 주의를

1449
00:57:20,440 --> 00:57:23,135
통해 시퀀스 연결을 통해 주입합니다.

1450
00:57:23,135 --> 00:57:24,760
그렇다면 이것을 다양한 문제에

1451
00:57:24,760 --> 00:57:26,160
어떻게 적용할 수 있을까요?

1452
00:57:26,160 --> 00:57:28,080
사람들이 매우 중요하게 생각하는 작업

1453
00:57:28,080 --> 00:57:30,820
중 하나는 텍스트에서 이미지 생성하는 작업입니다.

1454
00:57:30,820 --> 00:57:32,802
여기서 우리는 텍스트 프롬프트를 입력할 것입니다.

1455
00:57:32,802 --> 00:57:34,260
이것은 제가 어제 쓴 것입니다.

1456
00:57:34,260 --> 00:57:36,180
에펠탑 앞에서 호랑이와 악수하는

1457
00:57:36,180 --> 00:57:38,210
원숭이의 전문 다큐멘터리 사진.

1458
00:57:38,210 --> 00:57:40,210
원숭이는 바나나로 만든 모자를 쓰고 있습니다.

1459
00:57:40,210 --> 00:57:42,300
호랑이는 두 발로 서서 정장을 입고 있습니다.

1460
00:57:42,300 --> 00:57:43,878
그리고 이것은 실제 샘플입니다.

1461
00:57:43,878 --> 00:57:45,420
이런 것들이 이제 작동한다니 미쳤습니다.

1462
00:57:45,420 --> 00:57:47,838
하지만 여러분 모두 이런 것들을 본 적이 있을 것입니다.

1463
00:57:47,838 --> 00:57:49,380
이것이 작동하는 방식은 텍스트

1464
00:57:49,380 --> 00:57:51,620
프롬프트를 가져와서 일반적으로 사전 훈련된 텍스트

1465
00:57:51,620 --> 00:57:53,260
인코더를 통해 전달하는 것입니다.

1466
00:57:53,260 --> 00:57:54,422
사실, 제가 거짓말을 했습니다.

1467
00:57:54,422 --> 00:57:56,380
실제로 훈련해야 할 모델이 더 많습니다.

1468
00:57:56,380 --> 00:57:58,140
인코더와 디코더, VAE,

1469
00:57:58,140 --> 00:57:59,915
판별기 외에도 이들이

1470
00:57:59,915 --> 00:58:01,540
작동하도록 비밀리에 언어

1471
00:58:01,540 --> 00:58:02,980
모델을 훈련해야 합니다.

1472
00:58:02,980 --> 00:58:05,272
그래서 일반적으로 사전 훈련된 텍스트 인코더,

1473
00:58:05,272 --> 00:58:07,020
보통 T5 클립 같은 것을

1474
00:58:07,020 --> 00:58:08,960
사용하여 텍스트 임베딩을 제공합니다.

1475
00:58:08,960 --> 00:58:10,920
그리고 일반적으로 텍스트 인코더는 동결됩니다.

1476
00:58:10,920 --> 00:58:13,740
그런 다음 텍스트 임베딩과 함께 노이즈가

1477
00:58:13,740 --> 00:58:16,703
있는 잠재 변수를 확산 변환기에 전달하여 깨끗한

1478
00:58:16,703 --> 00:58:18,620
잠재 변수를 출력하는 확산

1479
00:58:18,620 --> 00:58:19,880
시간 단계를 얻습니다.

1480
00:58:19,880 --> 00:58:22,120
이 과정은 반복적으로 진행됩니다.

1481
00:58:22,120 --> 00:58:24,100
그리고 그것은 VAE 디코더를

1482
00:58:24,100 --> 00:58:26,140
통해 최종 이미지를 제공합니다.

1483
00:58:26,140 --> 00:58:28,640
구체적인 숫자를 제시하자면, 현재

1484
00:58:28,640 --> 00:58:31,440
꽤 강력한 오픈 소스 모델은 FLUX1

1485
00:58:31,440 --> 00:58:32,860
dev라고 불립니다.

1486
00:58:32,860 --> 00:58:34,500
그들은 T5와 클립 인코더를 사용합니다.

1487
00:58:34,500 --> 00:58:36,540
그들의 인코더는 8배 다운샘플링을 사용합니다.

1488
00:58:36,540 --> 00:58:38,800
그들은 120억 개의 매개변수를 가진

1489
00:58:38,800 --> 00:58:41,000
변환기 모델을 훈련하고, 이

1490
00:58:41,000 --> 00:58:44,780
변환기는 VAE 위에 추가적인 다운샘플링 레이어를 가지고 있어

1491
00:58:44,780 --> 00:58:45,840
다소 복잡합니다.

1492
00:58:45,840 --> 00:58:50,320
결과적으로 1,024개의 이미지 토큰의 시퀀스 길이를 갖게 됩니다.

1493
00:58:50,320 --> 00:58:53,177
사람들이 매우 중요하게 생각하는 또 다른 작업은 텍스트에서 비디오로의
변환입니다.

1494
00:58:53,177 --> 00:58:54,760
그래서 우리는 텍스트 프롬프트를

1495
00:58:54,760 --> 00:58:57,740
입력하고 그 텍스트 프롬프트에 따라 비디오의 픽셀을 출력할 수 있습니다.

1496
00:58:57,740 --> 00:59:00,578
파이프라인은 기본적으로 동일하게 보입니다.

1497
00:59:00,578 --> 00:59:03,120
그래서 사전 훈련된 텍스트 인코더를 통해 텍스트를 입력하고

1498
00:59:03,120 --> 00:59:04,980
노이즈가 있는 잠재 변수를 얻습니다.

1499
00:59:04,980 --> 00:59:07,400
중요하게도, 유일한 차이점은 이제 잠재 변수에 시간을

1500
00:59:07,400 --> 00:59:09,660
수용하기 위한 추가 차원이 있다는 것입니다.

1501
00:59:09,660 --> 00:59:12,040
따라서 두 개의 공간 차원 hw 외에도

1502
00:59:12,040 --> 00:59:14,040
잠재 변수에 시간 차원이 추가되어 깨끗한

1503
00:59:14,040 --> 00:59:15,582
잠재 변수를 제공합니다.

1504
00:59:15,582 --> 00:59:17,520
그리고 디코더는 일반적으로 이제

1505
00:59:17,520 --> 00:59:19,860
공간-시간 오토인코더가 될 것입니다.

1506
00:59:19,860 --> 00:59:22,545
그래서 공간적으로와 시간적으로 다운샘플링을 수행합니다.

1507
00:59:22,545 --> 00:59:23,920
그런 다음 잠재

1508
00:59:23,920 --> 00:59:26,220
변수를 픽셀로 업샘플링하여

1509
00:59:26,220 --> 00:59:27,700
비디오를 생성합니다.

1510
00:59:27,700 --> 00:59:31,740
그리고 이것은 작년에 발표된 메타의 GOOVIGEN

1511
00:59:31,740 --> 00:59:33,680
논문에서 생성된 비디오입니다.

1512
00:59:36,780 --> 00:59:39,340
그리고 이것은 이 모델에 대한 특정 숫자를

1513
00:59:39,340 --> 00:59:40,260
제시하는 것입니다.

1514
00:59:40,260 --> 00:59:42,960
비디오 생성 모델의

1515
00:59:42,960 --> 00:59:45,460
핵심 포인트는 시퀀스

1516
00:59:45,460 --> 00:59:47,620
길이 때문에

1517
00:59:47,620 --> 00:59:51,400
훈련 비용이 매우 비싸진다는

1518
00:59:51,400 --> 00:59:52,960
것입니다.

1519
00:59:52,960 --> 00:59:56,900
고해상도, 고프레임 비디오를

1520
00:59:56,900 --> 00:59:59,020
생성하려면 많은 토큰이

1521
00:59:59,020 --> 01:00:02,100
필요하기 때문입니다.

1522
01:00:02,100 --> 01:00:03,960
우리는 꽤 최첨단 텍스트-이미지

1523
01:00:03,960 --> 01:00:05,460
확산 모델을 사용하여

1524
01:00:05,460 --> 01:00:07,293
변환기가 1,024개의 이미지 토큰

1525
01:00:07,293 --> 01:00:09,300
시퀀스에서 작동한다고 말했습니다.

1526
01:00:09,300 --> 01:00:13,273
이 텍스트-비디오 확산 모델은 전체 아키텍처가 꽤 유사해

1527
01:00:13,273 --> 01:00:15,940
보이지만, 가장 큰 차이는 시퀀스 길이에

1528
01:00:15,940 --> 01:00:16,562
있습니다.

1529
01:00:16,562 --> 01:00:18,020
이제 그들은 이 고해상도

1530
01:00:18,020 --> 01:00:20,270
비디오를 생성하기 위해 76,

1531
01:00:20,270 --> 01:00:23,020
000개의 비디오 토큰을 처리해야 합니다.

1532
01:00:23,020 --> 01:00:25,720
그래서 이러한 비디오 확산 모델에서 비용이 발생하는

1533
01:00:25,720 --> 01:00:28,780
것은 실제로 이러한 매우 긴 시퀀스를 처리하는 것입니다.

1534
01:00:28,780 --> 01:00:33,843
그래서 기본적으로 지난 1년은 비디오 확산

1535
01:00:33,843 --> 01:00:35,760
모델의 시대였던

1536
01:00:35,760 --> 01:00:37,280
것 같습니다.

1537
01:00:37,280 --> 01:00:40,280
지난 1년 동안 거의 매주 새로운 흥미로운 비디오

1538
01:00:40,280 --> 01:00:42,360
확산 모델이 출시된 것 같습니다.

1539
01:00:42,360 --> 01:00:44,800
이들은 오픈 소스 모델과 기술 보고서가 있는

1540
01:00:44,800 --> 01:00:47,362
모델, 그리고 아무것도 알려주지 않지만 신용

1541
01:00:47,362 --> 01:00:48,820
카드 번호를 받아 샘플을

1542
01:00:48,820 --> 01:00:49,880
생성할 수 있게

1543
01:00:49,880 --> 01:00:51,960
해주는 순수 산업 모델의 혼합이었습니다.

1544
01:00:51,960 --> 01:00:55,622
그래서 저는 이 모든 내용을 하나하나 다 설명하지는 않을

1545
01:00:55,622 --> 01:00:57,080
것이지만, 지난 18개월

1546
01:00:57,080 --> 01:01:01,680
동안 이 주제가 정말 뜨거운 이슈였다는 것을 말씀드리고 싶었습니다.

1547
01:01:01,680 --> 01:01:04,160
특히 2024년 3월에 OpenAI에서

1548
01:01:04,160 --> 01:01:06,760
발표한 Sora라는 정말 영향력 있는 블로그

1549
01:01:06,760 --> 01:01:10,537
포스트가 있었는데, 이는 비디오에 대한 첫 번째 확산

1550
01:01:10,537 --> 01:01:13,120
모델은 아니지만 정말 좋은 결과를 낸 첫

1551
01:01:13,120 --> 01:01:14,240
번째 모델이었습니다.

1552
01:01:14,240 --> 01:01:17,960
그들은 현대의 확산 변환기와 정류 흐름을

1553
01:01:17,960 --> 01:01:19,082
채택했습니다.

1554
01:01:19,082 --> 01:01:20,540
사실, Sora에서 정류 흐름을

1555
01:01:20,540 --> 01:01:21,748
사용했는지는 잘 모르겠습니다.

1556
01:01:21,748 --> 01:01:24,180
그들이 그렇게 말했다고는 생각하지 않지만, 그들은

1557
01:01:24,180 --> 01:01:26,660
이러한 확산 변환기를 실제로 확장하고 잘 작동하게

1558
01:01:26,660 --> 01:01:28,320
만든 최초의 팀 중 하나였습니다.

1559
01:01:28,320 --> 01:01:31,020
그리고 그것이 비디오 확산 모델에서 4분의 1

1560
01:01:31,020 --> 01:01:32,320
마일 순간이었습니다.

1561
01:01:32,320 --> 01:01:34,653
그 후 다른 대기업들이 이를 주목하고 신속하게

1562
01:01:34,653 --> 01:01:36,420
Sora를 복제하려고 했습니다.

1563
01:01:36,420 --> 01:01:39,140
그래서 저는 지난 1년 반 동안 거의 매주

1564
01:01:39,140 --> 01:01:41,260
새로운 최첨단 비디오 확산 모델이

1565
01:01:41,260 --> 01:01:43,660
등장한 것 같은 느낌이 들었습니다.

1566
01:01:43,660 --> 01:01:47,300
오늘도 예외는 아니며, 오늘 아침 11시에

1567
01:01:47,300 --> 01:01:52,600
구글이 Veo 3를 발표했는데, 이는 현재 존재하는 가장

1568
01:01:52,600 --> 01:01:55,100
뛰어난 비디오 생성 모델일

1569
01:01:55,100 --> 01:01:56,580
가능성이 높습니다.

1570
01:01:56,580 --> 01:01:58,740
저는 여기 오는 길에 차 안에서

1571
01:01:58,740 --> 01:02:02,080
블로그 포스트를 읽었는데, 정말 멋진 것 같습니다.

1572
01:02:02,080 --> 01:02:04,780
여기 V3의 몇 가지 샘플이 있습니다.

1573
01:02:04,780 --> 01:02:07,460
이것들은 구글의 새로운 모델에서 텍스트

1574
01:02:07,460 --> 01:02:10,760
프롬프트로 생성된 비디오입니다. 정말 미친 것 같습니다.

1575
01:02:10,760 --> 01:02:13,660
또한 이 모델은 소리도 함께 모델링하므로

1576
01:02:13,660 --> 01:02:17,540
비디오 프레임과 함께 오디오를 출력할 수 있습니다.

1577
01:02:17,540 --> 01:02:19,340
이것은 또 다른 생성된 비디오로,

1578
01:02:19,340 --> 01:02:21,600
텍스트로 원하는 상황을 설명할 수 있습니다.

1579
01:02:21,600 --> 01:02:25,200
여기로 날아가고 정말 미친 것처럼 보입니다.

1580
01:02:25,200 --> 01:02:30,200
좋아요, 그래서 새로운 것을 포함하는 것이 재미있다고 생각했습니다.

1581
01:02:30,200 --> 01:02:32,920
좋아요, 확산의 큰 문제 중 하나는

1582
01:02:32,920 --> 01:02:35,460
샘플링이 정말 느리다는 것입니다.

1583
01:02:35,460 --> 01:02:37,660
우리는 샘플링이 반복적인 절차라고 말했습니다.

1584
01:02:37,660 --> 01:02:39,140
이 모델들은 정말 클 수 있습니다.

1585
01:02:39,140 --> 01:02:40,600
이들은 수십억 개의 매개변수를

1586
01:02:40,600 --> 01:02:42,960
가진 모델일 수 있으며, 수만 이상의

1587
01:02:42,960 --> 01:02:45,360
시퀀스 길이에서 작동할 수 있습니다.

1588
01:02:45,360 --> 01:02:47,720
그래서 이러한 것들은 추론 시

1589
01:02:47,720 --> 01:02:49,300
정말 느려집니다. 정류

1590
01:02:49,300 --> 01:02:54,160
흐름을 사용하더라도 추론 시 모델의 수십 번의 반복이 필요합니다.

1591
01:02:54,160 --> 01:02:56,640
해결책은 증류라는 알고리즘 범주로, 이에

1592
01:02:56,640 --> 01:02:59,057
대해 자세히 설명할 시간이 없습니다.

1593
01:02:59,057 --> 01:03:01,098
여기 몇 가지 참고 문헌을 남기고 싶었고, 이러한

1594
01:03:01,098 --> 01:03:03,460
기술 세트가 존재한다는 것을 알고 계셨으면 합니다.

1595
01:03:03,460 --> 01:03:05,520
증류 알고리즘은 기본적으로

1596
01:03:05,520 --> 01:03:07,880
일반적으로 추론 시 30,

1597
01:03:07,880 --> 01:03:10,960
50, 100번의 반복이 필요한 확산 모델을

1598
01:03:10,960 --> 01:03:14,360
가져와서, 추론 시 훨씬 적은 단계로

1599
01:03:14,360 --> 01:03:17,000
좋은 샘플을 얻을 수 있도록 모델을

1600
01:03:17,000 --> 01:03:18,560
수정하는 방법입니다.

1601
01:03:18,560 --> 01:03:20,500
이들은 샘플 품질을 희생하는 경향이 있습니다.

1602
01:03:20,500 --> 01:03:22,260
그래서 증류 방법의 전체 요점은

1603
01:03:22,260 --> 01:03:24,580
가능한 한 샘플 품질을 유지하면서도 추론

1604
01:03:24,580 --> 01:03:26,300
시 더 적은 샘플을 얻을 수

1605
01:03:26,300 --> 01:03:27,532
있도록 하는 것입니다.

1606
01:03:27,532 --> 01:03:29,740
일부 증류 방법은 단일 단계

1607
01:03:29,740 --> 01:03:32,320
샘플링까지 내려갈 수 있게 해주는데,

1608
01:03:32,320 --> 01:03:35,860
이는 정말 멋진 일이지만 그렇게 할 경우

1609
01:03:35,860 --> 01:03:39,220
생성 품질이 상당히 떨어지는 경향이 있습니다.

1610
01:03:39,220 --> 01:03:40,840
이 내용은 다루지 않겠지만, 증류에

1611
01:03:40,840 --> 01:03:42,420
관한 다양한 논문에 대한 참고

1612
01:03:42,420 --> 01:03:44,100
문헌을 여기에 남겼습니다. 관심이

1613
01:03:44,100 --> 01:03:45,340
있으시면 확인해 보세요.

1614
01:03:45,340 --> 01:03:47,880
그리고 이것은 정말 활발하고 발전하는 연구 분야입니다.

1615
01:03:47,880 --> 01:03:49,338
이 참고 문헌들을 보면,

1616
01:03:49,338 --> 01:03:52,180
이들은 2024년과 2025년의 자료입니다.

1617
01:03:52,180 --> 01:03:54,380
그래서 현재 사람들이 작업하고

1618
01:03:54,380 --> 01:03:56,680
있는 것은 더 나은 증류를 어떻게

1619
01:03:56,680 --> 01:03:59,180
얻을 것인지, 추론 시 확산 모델을

1620
01:03:59,180 --> 01:04:01,580
더 효율적으로 만드는 방법입니다.

1621
01:04:01,580 --> 01:04:02,700
또 다른 사항입니다.

1622
01:04:02,700 --> 01:04:06,860
그래서 저는 확산이 당신이 빠져들 수 있는 수학의

1623
01:04:06,860 --> 01:04:08,318
블랙홀이라고 언급했습니다.

1624
01:04:08,318 --> 01:04:09,860
우리는 그 수학을 깊이

1625
01:04:09,860 --> 01:04:12,880
파고들지 않고 직관적으로 정류 흐름 모델을

1626
01:04:12,880 --> 01:04:15,060
통해 문제에 대한 기하학적 직관을

1627
01:04:15,060 --> 01:04:18,140
제공함으로써 의도적으로 그 부분을 피했습니다.

1628
01:04:18,140 --> 01:04:20,600
그래서 이러한 형식이 무엇인지

1629
01:04:20,600 --> 01:04:22,600
간략하게 설명하고 싶었지만,

1630
01:04:22,600 --> 01:04:24,760
자세히 다룰 수는 없습니다.

1631
01:04:24,760 --> 01:04:27,383
여기 정류 흐름 목표를 다시 설명합니다.

1632
01:04:27,383 --> 01:04:28,800
훈련 중에 우리는 데이터

1633
01:04:28,800 --> 01:04:31,280
분포와 노이즈 분포에 따라 x와

1634
01:04:31,280 --> 01:04:33,588
z를 샘플링할 것이라고 말했습니다.

1635
01:04:33,588 --> 01:04:35,880
우리는 선택한 분포 pt에 따라 t를

1636
01:04:35,880 --> 01:04:38,840
샘플링할 것입니다. 이는 균일, 로짓-정규 또는

1637
01:04:38,840 --> 01:04:40,388
이동된 형태일 수 있습니다.

1638
01:04:40,388 --> 01:04:42,680
그런 다음 xt를 x와 z 사이의 선형

1639
01:04:42,680 --> 01:04:43,800
보간으로 설정합니다.

1640
01:04:43,800 --> 01:04:46,720
이 슬라이드에서는 이를 약간

1641
01:04:46,720 --> 01:04:48,340
다르게 작성했습니다.

1642
01:04:48,340 --> 01:04:51,280
이제 우리는 네트워크가 예측하기를 원하는 실제 속도

1643
01:04:51,280 --> 01:04:54,160
vgt를 z에서 x를 뺀 값으로 적고 있습니다.

1644
01:04:54,160 --> 01:04:56,320
우리는 노이즈가 있는 xt와

1645
01:04:56,320 --> 01:04:58,680
t를 네트워크에 통과시켜 예측된

1646
01:04:58,680 --> 01:05:01,400
v를 계산한 다음, vgt와 네트워크의

1647
01:05:01,400 --> 01:05:04,490
예측된 v 간의 L2 손실을 최소화합니다.

1648
01:05:06,928 --> 01:05:09,220
제가 다양한 형식이 많다고

1649
01:05:09,220 --> 01:05:11,840
말했을 때, 이러한 것들이 일반적인

1650
01:05:11,840 --> 01:05:15,660
설정에서 다양한 기능적 하이퍼파라미터를 바라보는

1651
01:05:15,660 --> 01:05:16,880
것처럼 보입니다.

1652
01:05:16,880 --> 01:05:20,900
더 일반화된 확산의 경우, 보통 pt 분포가

1653
01:05:20,900 --> 01:05:23,883
무엇인지 변형할 수 있습니다.

1654
01:05:23,883 --> 01:05:25,800
보통 노이즈 분포는 변형하지 않습니다.

1655
01:05:25,800 --> 01:05:29,380
이는 거의 항상 가우시안이며, 적어도 연속 모델의 경우 그렇습니다.

1656
01:05:29,380 --> 01:05:32,660
그러나 노이즈가 있는 xt를 계산하는 방법은 변형합니다.

1657
01:05:32,660 --> 01:05:35,420
일반적으로 이는 x와 z의

1658
01:05:35,420 --> 01:05:38,420
선형 조합이 될 것입니다.

1659
01:05:38,420 --> 01:05:41,340
선형 조합의 가중치는 일반적으로 t의 함수가

1660
01:05:41,340 --> 01:05:44,220
될 것이지만, 그 함수가 정확히 무엇인지는

1661
01:05:44,220 --> 01:05:46,340
확산 공식에 따라 다릅니다.

1662
01:05:46,340 --> 01:05:49,620
또한 변하는 것은 모델에게 예측하도록 요청하는 실제

1663
01:05:49,620 --> 01:05:51,002
목표가 무엇인지입니다.

1664
01:05:51,002 --> 01:05:53,460
항상 데이터 샘플 x와 잠재

1665
01:05:53,460 --> 01:05:56,740
변수 z의 선형 조합이 될 것입니다.

1666
01:05:56,740 --> 01:05:58,960
다시 말해, 선형 조합의 가중치는

1667
01:05:58,960 --> 01:06:02,200
일부 공식에서 t의 함수일 수 있습니다.

1668
01:06:02,200 --> 01:06:03,805
기본적으로 우리는 모델에게

1669
01:06:03,805 --> 01:06:05,180
노이즈가 있는 xt와

1670
01:06:05,180 --> 01:06:08,240
t를 제공하고 예측된 y를 얻도록 요청할 것입니다.

1671
01:06:08,240 --> 01:06:10,640
그리고 항상 두 값 간의 L2 손실을 계산합니다.

1672
01:06:10,640 --> 01:06:12,885
항상 그런 것은 아니지만 보통 그렇습니다.

1673
01:06:12,885 --> 01:06:14,260
그리고 변하는 것은 기본적으로

1674
01:06:14,260 --> 01:06:16,080
이러한 다양한 기능적 형태가 무엇인지입니다.

1675
01:06:16,080 --> 01:06:17,120
이 네 가지

1676
01:06:17,120 --> 01:06:19,160
다른 자리에 배치하는 이러한

1677
01:06:19,160 --> 01:06:21,120
다양한 함수는 무엇인가요?

1678
01:06:21,120 --> 01:06:24,160
정류 흐름의 경우 이는 상당히 간단합니다.

1679
01:06:24,160 --> 01:06:26,140
이들은 모두 매우 간단한 형태를 취합니다.

1680
01:06:26,140 --> 01:06:30,200
ct와 dt는 실제로 상수입니다.

1681
01:06:30,200 --> 01:06:35,480
여기에는 분산 보존이라는 또 다른 형태가 있으며, 여기서 이 두 가지를

1682
01:06:35,480 --> 01:06:38,640
하나의 스칼라 하이퍼파라미터인 sigma of

1683
01:06:38,640 --> 01:06:39,960
t로 축소합니다.

1684
01:06:39,960 --> 01:06:42,160
이제 이러한 선형 조합이 특정

1685
01:06:42,160 --> 01:06:43,360
방식으로 이루어집니다.

1686
01:06:43,360 --> 01:06:46,200
x와 z가 독립적이고 단위 분산을 가지면

1687
01:06:46,200 --> 01:06:48,400
출력도 단위 분산을 가지도록 보장되기

1688
01:06:48,400 --> 01:06:50,030
때문에 이를 선택합니다.

1689
01:06:50,030 --> 01:06:52,280
따라서 이 두 기능적 하이퍼파라미터를

1690
01:06:52,280 --> 01:06:54,040
하나의 노이즈 스케줄로

1691
01:06:54,040 --> 01:06:56,873
축소하고 여전히 이를 선택해야 합니다.

1692
01:06:56,873 --> 01:06:58,540
분산 보존과 결합하여 분산

1693
01:06:58,540 --> 01:07:00,080
폭발이라는 또 다른 것이 있으며,

1694
01:07:00,080 --> 01:07:03,240
여기서 at를 1로 설정하고 bt를 다시 sigma

1695
01:07:03,240 --> 01:07:04,380
of t로 설정합니다.

1696
01:07:04,380 --> 01:07:06,640
그런 다음 그걸 어떻게든 선택해야 합니다.

1697
01:07:06,640 --> 01:07:10,785
사람들이 선택할 수 있는 다양한 목표가 많습니다.

1698
01:07:10,785 --> 01:07:12,160
때때로 네트워크에 깨끗한

1699
01:07:12,160 --> 01:07:13,510
데이터를 예측하도록 요청합니다.

1700
01:07:13,510 --> 01:07:15,260
때때로 모델에 추가된 노이즈를

1701
01:07:15,260 --> 01:07:16,325
예측하도록 요청합니다.

1702
01:07:16,325 --> 01:07:17,700
때때로 네트워크에 두

1703
01:07:17,700 --> 01:07:20,100
가지의 선형 조합을 예측하도록 요청합니다.

1704
01:07:20,100 --> 01:07:22,200
정류된 흐름의 경우,

1705
01:07:22,200 --> 01:07:24,380
데이터에서 노이즈로 향하는

1706
01:07:24,380 --> 01:07:26,540
속도 벡터를 예측하는

1707
01:07:26,540 --> 01:07:30,060
것이지만, 확산의 다양한 형태에서는 모두

1708
01:07:30,060 --> 01:07:31,620
달라질 수 있습니다.

1709
01:07:31,620 --> 01:07:34,580
그렇다면 하이퍼파라미터 선택이 충분히 나쁘다고

1710
01:07:34,580 --> 01:07:35,830
생각할 수 있습니다.

1711
01:07:35,830 --> 01:07:37,663
이제 우리는 t의 함수인 하이퍼파라미터를

1712
01:07:37,663 --> 01:07:38,940
선택해야 합니다.

1713
01:07:38,940 --> 01:07:40,007
이건 미친 짓입니다.

1714
01:07:40,007 --> 01:07:41,840
이것들을 직관적으로 설정할 수는

1715
01:07:41,840 --> 01:07:43,937
없으므로 어떤 수학에 의해 안내받아야 합니다.

1716
01:07:43,937 --> 01:07:46,020
확산 모델을 훈련할 때 사람들이 생각하는

1717
01:07:46,020 --> 01:07:48,597
기본적인 세 가지 수학적 형식이 있습니다.

1718
01:07:48,597 --> 01:07:51,180
다시 말하지만, 우리는 실제로는 다루지 않을

1719
01:07:51,180 --> 01:07:51,680
것입니다.

1720
01:07:51,680 --> 01:07:54,340
그 존재를 아는 것만으로 충분합니다.

1721
01:07:54,340 --> 01:07:57,460
첫 번째는 확산이 잠재 변수 모델이라는 것입니다.

1722
01:07:57,460 --> 01:07:59,680
우리는 깨끗한 데이터 샘플 x0를

1723
01:07:59,680 --> 01:08:01,460
가지고 있지만, 각 깨끗한

1724
01:08:01,460 --> 01:08:03,180
데이터 샘플에 대해 해당 깨끗한

1725
01:08:03,180 --> 01:08:06,540
샘플에 해당하는 손상되거나 노이즈가 있는 샘플의

1726
01:08:06,540 --> 01:08:07,677
시퀀스가 존재합니다.

1727
01:08:07,677 --> 01:08:08,760
우리는 그것들을 관찰할 수 없습니다.

1728
01:08:08,760 --> 01:08:10,140
우리가 무엇인지 모르지만, 어떻게든

1729
01:08:10,140 --> 01:08:11,160
그것들을 알아내야 합니다.

1730
01:08:11,160 --> 01:08:12,577
그래서 그것이 잠재 변수 모델입니다.

1731
01:08:12,577 --> 01:08:15,140
이는 변분 오토인코더와 매우 유사하게 보입니다.

1732
01:08:15,140 --> 01:08:17,700
변분 오토인코더에서는 z와 x가 있었습니다.

1733
01:08:17,700 --> 01:08:18,779
우리는 z를 관찰하지 않았습니다.

1734
01:08:18,779 --> 01:08:21,040
우리는 이 것을 어떻게든 훈련하고 싶었습니다.

1735
01:08:21,040 --> 01:08:25,359
그런데 변분 오토인코더에서 했던 것과 매우 유사한 수학적 트릭을

1736
01:08:25,359 --> 01:08:27,100
사용할 수 있고, 데이터의

1737
01:08:27,100 --> 01:08:28,840
가능성에 대한 변분 하한을

1738
01:08:28,840 --> 01:08:30,260
최대화할 수 있습니다.

1739
01:08:30,260 --> 01:08:32,359
그것이 확산의 잠재

1740
01:08:32,359 --> 01:08:35,441
변수 모델 해석을 낳습니다.

1741
01:08:35,441 --> 01:08:37,399
확산에 대한 완전히 다른 해석은 점수

1742
01:08:37,399 --> 01:08:39,760
함수(score function)를 모델링한다는 것입니다.

1743
01:08:39,760 --> 01:08:45,040
x의 분포 pdata가 주어지면, 분포의

1744
01:08:45,040 --> 01:08:47,479
점수 함수는

1745
01:08:47,479 --> 01:08:49,920
pdata의 로그에 대한

1746
01:08:49,920 --> 01:08:52,040
x의 미분입니다.

1747
01:08:52,040 --> 01:08:54,180
직관적으로 분포가 주어지면,

1748
01:08:54,180 --> 01:08:55,920
점수 함수는 높은

1749
01:08:55,920 --> 01:09:00,319
확률 밀도가 있는 영역을 가리키는 벡터 필드입니다.

1750
01:09:00,319 --> 01:09:02,785
데이터 공간의 어떤 점에 대해서도

1751
01:09:02,785 --> 01:09:04,160
점수 함수는 높은

1752
01:09:04,160 --> 01:09:08,240
데이터 밀도가 있는 영역을 가리키는 벡터가 될 것입니다.

1753
01:09:08,240 --> 01:09:10,439
이제 확산에 대한 또 다른 해석은

1754
01:09:10,439 --> 01:09:12,939
확산이 데이터 분포의 점수 함수를

1755
01:09:12,939 --> 01:09:14,660
학습하고, 사실 데이터 분포의

1756
01:09:14,660 --> 01:09:16,740
다양한 노이즈 수준에 해당하는

1757
01:09:16,740 --> 01:09:19,725
점수 함수 집합을 학습하고 있다는 것입니다.

1758
01:09:19,725 --> 01:09:21,100
그래서 확산에 대한 또

1759
01:09:21,100 --> 01:09:22,725
다른 해석은 진짜 데이터

1760
01:09:22,725 --> 01:09:25,140
분포를 증가하는 양의 알려진 노이즈로

1761
01:09:25,140 --> 01:09:27,180
손상시키는 노이즈 분포의 가족에

1762
01:09:27,180 --> 01:09:29,700
해당하는 점수 함수의 가족을 학습하려고

1763
01:09:29,700 --> 01:09:31,080
한다는 것입니다.

1764
01:09:31,080 --> 01:09:33,340
그리고 이것은 최종적으로 매우

1765
01:09:33,340 --> 01:09:36,837
유사한 알고리즘을 낳는 완전히 다른 수학적 형식입니다.

1766
01:09:36,837 --> 01:09:39,420
그리고 최근에 등장한 세 번째 해석은

1767
01:09:39,420 --> 01:09:41,620
확산이 확률적 미분 방정식을

1768
01:09:41,620 --> 01:09:43,755
푸는 것이라는 개념입니다.

1769
01:09:43,755 --> 01:09:46,380
이것은 제가 완전히 이해하지 못하는

1770
01:09:46,380 --> 01:09:49,020
부분이니 너무 많은 질문은 하지

1771
01:09:49,020 --> 01:09:52,020
마세요. 하지만 아이디어는 노이즈 분포에서

1772
01:09:52,020 --> 01:09:55,260
데이터 분포의 샘플로 샘플을 운반하는

1773
01:09:55,260 --> 01:09:58,060
미소한 방법을 기록하는 미분 방정식을

1774
01:09:58,060 --> 01:10:00,040
작성하고 싶다는 것입니다.

1775
01:10:00,040 --> 01:10:02,220
그리고 추론을 하면, 신경망은

1776
01:10:02,220 --> 01:10:05,720
기본적으로 우리가 쓸 수 있는 어떤 수치적 적분기,

1777
01:10:05,720 --> 01:10:08,620
이 확률적 미분 방정식에 대한 수치적 적분기를

1778
01:10:08,620 --> 01:10:09,940
배우고 있습니다.

1779
01:10:09,940 --> 01:10:12,683
이것은 확률적 미분 방정식의 관점에서

1780
01:10:12,683 --> 01:10:14,600
생각하는 새로운 방식을 열어주기

1781
01:10:14,600 --> 01:10:19,680
때문에, 우리는 추론 시 이러한 것들에서 샘플링할 수 있는 두 가지 전혀

1782
01:10:19,680 --> 01:10:22,380
다른 방법 범주에 접근할 수 있습니다.

1783
01:10:22,380 --> 01:10:26,880
이 관점에서 보면, 우리가 정류 흐름에서 본 단순한 경량 하강법

1784
01:10:26,880 --> 01:10:28,720
접근 방식은 기본적으로

1785
01:10:28,720 --> 01:10:31,320
확률적 미분 방정식 위에서 전진

1786
01:10:31,320 --> 01:10:33,682
오일러 유형의 적분기에 해당합니다.

1787
01:10:33,682 --> 01:10:35,140
따라서 이러한 해석 하에, 이

1788
01:10:35,140 --> 01:10:37,765
점수 함수에 따라 나아가는 데 더 나은 작업을 수행하기

1789
01:10:37,765 --> 01:10:40,440
위해 다양한 복잡한 적분기를 사용할 수 있다고 상상할

1790
01:10:40,440 --> 01:10:41,240
수 있습니다.

1791
01:10:41,240 --> 01:10:43,820
다시 말해, 이것들은 깊은 물입니다.

1792
01:10:43,820 --> 01:10:46,400
이 모든 것에 대해 자세히 설명하는

1793
01:10:46,400 --> 01:10:47,560
논문들이 있습니다.

1794
01:10:47,560 --> 01:10:49,040
제가 정말 좋아하는 블로그

1795
01:10:49,040 --> 01:10:52,203
글은 Sander Dielman의 확산에 대한 관점에 관한 글로,

1796
01:10:52,203 --> 01:10:54,120
그는 확산 모델에 대해 생각하거나

1797
01:10:54,120 --> 01:10:56,812
보는 다양한 방법에 대한 여덟 가지 관점을 제시했습니다.

1798
01:10:56,812 --> 01:10:58,020
그래서 이것은 훌륭한 글입니다.

1799
01:10:58,020 --> 01:10:59,140
강력히 추천합니다.

1800
01:10:59,140 --> 01:11:00,280
그가 쓴 확산 모델에

1801
01:11:00,280 --> 01:11:02,020
관한 모든 것을 강력히 추천합니다.

1802
01:11:02,020 --> 01:11:03,312
그의 블로그 게시물은 모두 놀랍습니다.

1803
01:11:05,800 --> 01:11:07,700
자기 회귀 모델이 실제로 돌아옵니다.

1804
01:11:07,700 --> 01:11:10,780
우리는 동일한 방식으로 인코더-디코더를 사용하고 거기에

1805
01:11:10,780 --> 01:11:12,940
자기 회귀 모델을 추가할 수 있습니다.

1806
01:11:12,940 --> 01:11:16,560
그래서 마지막에 이것을 살짝 추가하자면, 확산 모델

1807
01:11:16,560 --> 01:11:19,620
외에 생성 모델링을 위한 또 다른 현대적 방법은

1808
01:11:19,620 --> 01:11:22,260
이산 변분 오토인코더로 계산된 이산

1809
01:11:22,260 --> 01:11:24,060
잠재 변수에 대해 자기

1810
01:11:24,060 --> 01:11:26,060
회귀 모델을 훈련하는 것입니다.

1811
01:11:26,060 --> 01:11:30,860
그래서 우리가 생성 모델을 위해 GAN, VAE, 자기 회귀

1812
01:11:30,860 --> 01:11:33,220
모델, 확산 모델을 사용한 이유는

1813
01:11:33,220 --> 01:11:36,140
이들이 모두 현대 기계 학습

1814
01:11:36,140 --> 01:11:38,512
파이프라인에서 사용되기 때문입니다.

1815
01:11:38,512 --> 01:11:40,220
그래서 오늘의 요약은 이렇습니다.

1816
01:11:40,220 --> 01:11:42,660
오늘 우리는 두 가지 다른 범주의 생성 모델에 대한

1817
01:11:42,660 --> 01:11:43,720
빠른 투어를 했습니다.

1818
01:11:43,720 --> 01:11:46,137
우리는 생성적 적대 신경망과 확산 모델에 대해

1819
01:11:46,137 --> 01:11:46,880
이야기했습니다.

1820
01:11:46,880 --> 01:11:49,900
그리고 우리는 잠재 확산 모델에서 이들의 현대적 전체

1821
01:11:49,900 --> 01:11:52,100
파이프라인을 보았으며, 이는 이 생성 모델링

1822
01:11:52,100 --> 01:11:53,960
섹션을 마무리하는 좋은 방법입니다.

1823
01:11:53,960 --> 01:11:56,252
왜냐하면 우리가 본 모든 생성 모델이 기본적으로

1824
01:11:56,252 --> 01:11:59,500
돌아와서 이러한 큰 현대 파이프라인을 형성하기 때문입니다.

1825
01:11:59,500 --> 01:12:03,690
감사합니다. 다음 시간에는 비전과 언어에 대해 이야기하겠습니다.
