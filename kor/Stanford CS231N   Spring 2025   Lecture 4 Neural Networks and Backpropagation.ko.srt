1
00:00:05,270 --> 00:00:07,100
이 슬라이드에서 보시다시피,

2
00:00:07,100 --> 00:00:14,310
오늘은 neural networks와 backpropagation에 대해
이야기할 것입니다.

3
00:00:14,310 --> 00:00:17,000
사실 이 과정은—초창기에

4
00:00:17,000 --> 00:00:19,770
제가 공부할 때는

5
00:00:19,770 --> 00:00:26,180
신경망이 자신의 실수로부터 배우게 해주는 마법

6
00:00:26,180 --> 00:00:30,590
같은 과정이라고 자주 불렀는데, 인간과

7
00:00:30,590 --> 00:00:35,910
비슷하지만 더 체계적이고 약간 더 많은

8
00:00:35,910 --> 00:00:39,720
수학을 사용하는 과정입니다.

9
00:00:39,720 --> 00:00:43,380
그럼 본격적으로 주제로 들어가 보겠습니다.

10
00:00:43,380 --> 00:00:46,440
분명 흥미로울 거라고 확신합니다.

11
00:00:46,440 --> 00:00:49,460
그리고 이것이 이번 학기 나머지 내용을 위한

12
00:00:49,460 --> 00:00:51,180
기초를 다지는 것입니다.

13
00:00:51,180 --> 00:00:54,650
앞으로 다룰 모든 알고리즘은

14
00:00:54,650 --> 00:00:59,600
backpropagation의 형태를 사용하고 있습니다.

15
00:00:59,600 --> 00:01:06,850
그래서 이 강의와 주제를 이해하는 것이 매우

16
00:01:06,850 --> 00:01:08,270
중요합니다.

17
00:01:08,270 --> 00:01:12,470
좋습니다, 전통을 이어가며

18
00:01:12,470 --> 00:01:19,180
지금까지 이야기한 내용을 복습해 보겠습니다.

19
00:01:19,180 --> 00:01:27,310
지난 시간에 이야기한 내용을 기억하실 겁니다.

20
00:01:27,310 --> 00:01:34,360
목적 함수, 즉 여기서 말하는 loss function을 어떻게

21
00:01:34,360 --> 00:01:36,050
구성하는지 말했죠.

22
00:01:36,050 --> 00:01:40,040
그리고 정규화에 대해서도 이야기했습니다.

23
00:01:40,040 --> 00:01:45,520
이를 위해 x, y 쌍과 scoring

24
00:01:45,520 --> 00:01:53,180
function을 정의하는 방식으로 모든 것을 공식화했습니다.

25
00:01:53,180 --> 00:01:57,260
여기서는 보시다시피 선형 scoring

26
00:01:57,260 --> 00:02:02,610
function을 사용했고, 궁극적으로 이 loss

27
00:02:02,610 --> 00:02:04,900
function을 정의했습니다.

28
00:02:04,900 --> 00:02:07,110
오른쪽 그래프는

29
00:02:07,110 --> 00:02:14,610
우리가 그렸던 것으로, 학습의 전체 과정을

30
00:02:14,610 --> 00:02:16,240
보여줍니다.

31
00:02:16,240 --> 00:02:20,610
지난 강의나 그 이전에도 질문이 있었습니다.

32
00:02:20,610 --> 00:02:26,160
왜 softmax 함수만 사용하는지에

33
00:02:26,160 --> 00:02:29,980
대한 질문이었죠.

34
00:02:29,980 --> 00:02:34,230
softmax가 우리가 가진 유일한 loss function이 아니며
사용하는

35
00:02:34,230 --> 00:02:36,490
것도 아니라는 점을 다시 말씀드리고 싶습니다.

36
00:02:36,490 --> 00:02:42,070
딥러닝에서, 특히 분류 작업에 가장 널리 쓰이는 함수

37
00:02:42,070 --> 00:02:44,320
중 하나일 뿐입니다.

38
00:02:44,320 --> 00:02:47,490
하지만 다른 작업을 위해 사용하는

39
00:02:47,490 --> 00:02:50,590
다양한 옵션들이 많습니다.

40
00:02:50,590 --> 00:02:53,290
분류 작업에서도, 웹사이트에

41
00:02:53,290 --> 00:02:55,440
공유한 슬라이드를

42
00:02:55,440 --> 00:02:59,640
보셨다면, 두 번째 강의의 읽기 과제에

43
00:02:59,640 --> 00:03:05,630
포함된 hinge loss, 예전에는 SVM loss라고

44
00:03:05,630 --> 00:03:07,800
불렸던 것도 있습니다.

45
00:03:07,800 --> 00:03:11,690
슬라이드에는 hinge

46
00:03:11,690 --> 00:03:16,770
loss와 관련된 예제들이 있었습니다.

47
00:03:16,770 --> 00:03:20,960
이 함수도 특히 신경망 초창기에 널리

48
00:03:20,960 --> 00:03:26,700
쓰였던 loss function 중 하나입니다.

49
00:03:26,700 --> 00:03:31,460
간단히 설명드리자면, 이 함수는

50
00:03:31,460 --> 00:03:38,900
softmax와 달리 점수를 확률로

51
00:03:38,900 --> 00:03:41,250
변환하지 않습니다.

52
00:03:41,250 --> 00:03:42,980
즉, 점수를 확률로 바꾸는 것이

53
00:03:42,980 --> 00:03:44,490
유일한 방법은 아닙니다.

54
00:03:44,490 --> 00:03:48,720
다른 형태도 사용할 수 있습니다.

55
00:03:48,720 --> 00:03:53,090
이 함수는—여기 강조해

56
00:03:53,090 --> 00:03:54,820
보겠습니다—정답

57
00:03:54,820 --> 00:04:01,360
항목의 점수 Syi가 다른 모든

58
00:04:01,360 --> 00:04:06,430
항목의 점수 Sj보다 높도록

59
00:04:06,430 --> 00:04:08,900
유도합니다.

60
00:04:08,900 --> 00:04:16,329
조건을 보시면, 조건이 참이면 0이

61
00:04:16,329 --> 00:04:18,519
됩니다.

62
00:04:18,519 --> 00:04:22,990
그렇지 않으면, 제가 말했듯이,

63
00:04:22,990 --> 00:04:28,720
정답 항목 점수가 다른 항목 점수보다 최소한

64
00:04:28,720 --> 00:04:32,170
margin만큼 더 높아야

65
00:04:32,170 --> 00:04:34,220
한다고 장려합니다.

66
00:04:34,220 --> 00:04:35,890
여기서 보이는 1이라는

67
00:04:35,890 --> 00:04:38,510
숫자가 margin입니다.

68
00:04:38,510 --> 00:04:41,690
조건이 위반되면, loss가

69
00:04:41,690 --> 00:04:47,000
margin에서 비례해서 증가합니다.

70
00:04:47,000 --> 00:04:48,520
이것이

71
00:04:48,520 --> 00:04:56,440
함수의 시각화입니다.

72
00:04:56,440 --> 00:05:02,130
이 함수는 관련 없는 항목이 너무 높게 점수받는 경우를

73
00:05:02,130 --> 00:05:05,920
벌점으로 처리하여 올바른 점수를 촉진합니다.

74
00:05:05,920 --> 00:05:10,620
다시 한번, 예제와 더 나은 이해를 위해 두

75
00:05:10,620 --> 00:05:15,630
번째 강의의 읽기 과제를 참고하시기 바랍니다.

76
00:05:15,630 --> 00:05:20,580
다음으로, 우리는 일반적인 최적화, 즉

77
00:05:20,580 --> 00:05:28,750
신경망의 최적 파라미터 W를 찾는 방법에 대해 이야기했습니다.

78
00:05:28,750 --> 00:05:32,160
그 과정에서, 이 손실

79
00:05:32,160 --> 00:05:39,390
함수의 지형이 이 이미지처럼 큰 계곡과 같다는

80
00:05:39,390 --> 00:05:42,190
점을 잠깐 언급했죠.

81
00:05:42,190 --> 00:05:46,470
그 계곡의 모든 지점은 서로 다른 가중치

82
00:05:46,470 --> 00:05:48,990
파라미터 집합입니다.

83
00:05:48,990 --> 00:05:52,850
우리는 그 손실 지형을 최소화하는

84
00:05:52,850 --> 00:05:56,210
파라미터 집합 W를 찾고자 했습니다.

85
00:05:56,210 --> 00:05:58,790
핵심은 손실 함수

86
00:05:58,790 --> 00:06:03,080
L을 W에 대해 미분한 그래디언트를

87
00:06:03,080 --> 00:06:07,340
구하고, 그 그래디언트를

88
00:06:07,340 --> 00:06:12,480
단계별 최적화에 사용하는 것이었습니다.

89
00:06:12,480 --> 00:06:16,590
이것이 바로 그래디언트 하강법 알고리즘을 만들어 냈습니다.

90
00:06:16,590 --> 00:06:22,020
그래서 가중치가 기본적으로 업데이트됩니다.

91
00:06:22,020 --> 00:06:25,730
제가 지금 멀리서 가리키는

92
00:06:25,730 --> 00:06:32,210
부분을 보기 어렵지만, 대략 짐작할 수 있습니다.

93
00:06:32,210 --> 00:06:38,030
어쨌든, 손실 지형을 최소값

94
00:06:38,030 --> 00:06:42,980
쪽으로 내려가기 위해,

95
00:06:42,980 --> 00:06:47,120
스텝 크기를 정의하고

96
00:06:47,120 --> 00:06:54,280
보통 그래디언트의 음수 방향으로 한

97
00:06:54,280 --> 00:06:58,100
스텝을 이동합니다.

98
00:06:58,100 --> 00:07:01,910
이것이 그래디언트 하강법 알고리즘입니다.

99
00:07:01,910 --> 00:07:04,030
최적화를 위해, 우리는

100
00:07:04,030 --> 00:07:09,610
수치적 그래디언트와 해석적 그래디언트 두 가지

101
00:07:09,610 --> 00:07:15,290
접근법에 대해 이야기했으며, 각각 장단점이 있습니다.

102
00:07:15,290 --> 00:07:21,820
실제로는 해석적 그래디언트를

103
00:07:21,820 --> 00:07:24,680
도출하는데,

104
00:07:24,680 --> 00:07:28,930
수학적 구현이 어렵거나

105
00:07:28,930 --> 00:07:31,750
복잡할 때는 수치적

106
00:07:31,750 --> 00:07:37,030
그래디언트로 구현을 검증합니다.

107
00:07:37,030 --> 00:07:39,970
또 다른 도전 과제는 전체

108
00:07:39,970 --> 00:07:46,270
데이터셋에 대해 손실 함수와 그 그래디언트를 적용하는

109
00:07:46,270 --> 00:07:47,740
것이었습니다.

110
00:07:47,740 --> 00:07:51,480
대규모 데이터셋의 경우, 전체 데이터셋에

111
00:07:51,480 --> 00:07:58,400
대해 손실 함수와 미분을 계산하는 것은 매우 비용이 많이

112
00:07:58,400 --> 00:07:58,900
듭니다.

113
00:07:58,900 --> 00:08:02,520
그래서 우리는 미니 배치라는

114
00:08:02,520 --> 00:08:12,030
개념에 대해 이야기했는데, 데이터셋에서 샘플링한 여러 예제들, 보통 32,

115
00:08:12,030 --> 00:08:19,120
64, 128, 또는 256개 정도를 사용하는 것입니다.

116
00:08:19,120 --> 00:08:27,240
그리고 이렇게 부분적으로 샘플링한 데이터를 이용해 그래디언트를

117
00:08:27,240 --> 00:08:33,990
계산하고, 최소값을 향해 한 걸음씩 나아가는 거죠.

118
00:08:33,990 --> 00:08:39,340
SGD, 즉 확률적 경사 하강법 외에도,

119
00:08:39,340 --> 00:08:43,350
Momentum, RMSProp,

120
00:08:43,350 --> 00:08:51,570
Adam 옵티마이저 같은 SGD의 최적화 기법들에 대해 이야기했습니다.

121
00:08:51,570 --> 00:08:56,270
이 부분에 대해 구체적인 질문이 있다면,

122
00:08:56,270 --> 00:08:59,790
세 번째 강의를 참고하시길

123
00:08:59,790 --> 00:09:06,490
권합니다. 거기에 많은 세부 내용이 있습니다.

124
00:09:06,490 --> 00:09:09,230
그리고 우리가 이야기한 또

125
00:09:09,230 --> 00:09:12,380
다른 중요한 점은 학습률과 학습률

126
00:09:12,380 --> 00:09:14,700
스케줄링의 중요성입니다.

127
00:09:14,700 --> 00:09:16,910
몇몇

128
00:09:16,910 --> 00:09:21,450
옵티마이저에서는 보통 학습률을 크게

129
00:09:21,450 --> 00:09:27,800
시작해서 점차 감소시키거나, 어떤

130
00:09:27,800 --> 00:09:33,250
비율로 줄이는 방식을 사용합니다.

131
00:09:33,250 --> 00:09:38,850
이것은 많은 옵티마이저에서 일반적으로 필요한 과정입니다.

132
00:09:38,850 --> 00:09:43,500
하지만 최근의 Adam과 그 변형들 같은 옵티마이저에서는,

133
00:09:43,500 --> 00:09:47,940
학습률 감소를 수동으로 또는 명시적으로 할 필요가 없는데,

134
00:09:47,940 --> 00:09:53,280
그 이유는 옵티마이저 자체에 이미 그 기능이 내장되어 있기 때문입니다.

135
00:09:53,280 --> 00:10:01,410
이제 신경망 주제로 넘어가서, 실제로 신경망을

136
00:10:01,410 --> 00:10:06,330
어떻게 구성하고 더 흥미롭고 어려운

137
00:10:06,330 --> 00:10:13,770
문제들을 해결할 수 있는지 살펴보겠습니다.

138
00:10:13,770 --> 00:10:21,870
지금까지는 W에 x를 곱하는 선형 함수에 대해

139
00:10:21,870 --> 00:10:24,400
이야기했습니다.

140
00:10:24,400 --> 00:10:30,270
이것이 가장 기본적인 신경망으로, 단일

141
00:10:30,270 --> 00:10:33,910
레이어로 정의할 수 있습니다.

142
00:10:33,910 --> 00:10:36,150
앞으로 레이어들에 대해 이야기할 겁니다.

143
00:10:36,150 --> 00:10:42,410
여기서 주목해야 할 점은 D와 C라는

144
00:10:42,410 --> 00:10:46,130
차원인데, D는

145
00:10:46,130 --> 00:10:50,030
입력 데이터 x의 차원,

146
00:10:50,030 --> 00:10:54,360
즉 특징의 수입니다.

147
00:10:54,360 --> 00:10:57,590
그리고 C는 클래스 수, 즉

148
00:10:57,590 --> 00:11:02,300
출력 노드나 뉴런의 수, 필요한 출력의

149
00:11:02,300 --> 00:11:04,730
개수를 의미합니다.

150
00:11:04,730 --> 00:11:12,390
두 번째 레이어 신경망을 만들기 위해서는 W2라고

151
00:11:12,390 --> 00:11:19,320
하는 새로운 가중치 집합을 정의할 수 있습니다.

152
00:11:19,320 --> 00:11:24,410
그리고 이 가중치들을 이전 레이어인 W1에

153
00:11:24,410 --> 00:11:28,880
x를 곱한 결과에 적용하는 겁니다.

154
00:11:28,880 --> 00:11:31,910
여기서 차원에 주목해 주세요.

155
00:11:31,910 --> 00:11:35,360
출력은 C개이고 입력

156
00:11:35,360 --> 00:11:37,380
특징은 D개입니다.

157
00:11:37,380 --> 00:11:42,220
그리고 H를 정의하는데,

158
00:11:42,220 --> 00:11:49,210
이것이 뉴런 수, 즉 은닉층 노드

159
00:11:49,210 --> 00:11:52,720
수를 결정합니다.

160
00:11:52,720 --> 00:11:53,810
이것이 첫 번째 포인트입니다.

161
00:11:53,810 --> 00:11:56,620
두 번째 포인트는 우리가

162
00:11:56,620 --> 00:12:00,770
다시 살펴볼 max 함수입니다.

163
00:12:00,770 --> 00:12:04,540
이 함수가 무엇인지, 어떤 의미인지 설명하겠습니다.

164
00:12:04,540 --> 00:12:07,630
max 연산은 W1과

165
00:12:07,630 --> 00:12:10,960
W2가 수행하는 선형

166
00:12:10,960 --> 00:12:15,980
변환 사이에 비선형성을 만듭니다.

167
00:12:15,980 --> 00:12:20,830
이것은 사실 매우 중요한 과정의

168
00:12:20,830 --> 00:12:22,850
일부입니다.

169
00:12:22,850 --> 00:12:25,160
비선형성에 대해 조금

170
00:12:25,160 --> 00:12:30,430
이야기하고, 잊기 전에 마지막 부분도 살펴보겠습니다.

171
00:12:30,430 --> 00:12:34,400
실제로는 W와 x만 포함하는 것이 맞습니다.

172
00:12:34,400 --> 00:12:39,010
첫 번째와 두 번째 강의에서 이 부분을 다뤘죠.

173
00:12:39,010 --> 00:12:45,850
완전한 프레임워크를 위해 편향(bias)도 포함합니다.

174
00:12:45,850 --> 00:12:47,560
실제로 편향도 있지만,

175
00:12:47,560 --> 00:12:51,160
단순화를 위해 여기서는 쓰지 않았습니다.

176
00:12:51,160 --> 00:12:56,410
어쨌든 max 연산이 비선형성을 만들어냅니다.

177
00:12:56,410 --> 00:12:59,460
이것이 매우 중요한

178
00:12:59,460 --> 00:13:06,820
이유는, 지난 강의에서 선형 분류기에 대해 이야기하면서,

179
00:13:06,820 --> 00:13:11,460
단일 선으로는 분리할 수

180
00:13:11,460 --> 00:13:15,390
없는 문제들이 많다고 했기

181
00:13:15,390 --> 00:13:16,480
때문입니다.

182
00:13:16,480 --> 00:13:20,370
이 예시는 신경망에서 선형 함수만으로

183
00:13:20,370 --> 00:13:23,770
문제를 해결하려면 원래

184
00:13:23,770 --> 00:13:27,510
공간에서 새로운 공간으로 비선형

185
00:13:27,510 --> 00:13:31,780
변환이 필요하다는 것을 보여줍니다.

186
00:13:31,780 --> 00:13:35,550
그리고 새로운 공간에서는 선으로 분리가

187
00:13:35,550 --> 00:13:37,080
가능해집니다.

188
00:13:37,080 --> 00:13:44,960
즉, 입력과 두 번째 공간 사이에 비선형 변환이 일어나며,

189
00:13:44,960 --> 00:13:47,790
여기서는 x와

190
00:13:47,790 --> 00:13:52,760
y를 극좌표 r과 θ로 매핑하는

191
00:13:52,760 --> 00:13:53,580
것입니다.

192
00:13:53,580 --> 00:13:57,000
하지만 다시 말하지만, 이것은 단지 한 가지 예일 뿐입니다.

193
00:13:57,000 --> 00:14:01,130
다른 예들도 정말 많습니다.

194
00:14:01,130 --> 00:14:07,790
이 예를 가지고, 다시 돌아가서—앗, 두

195
00:14:07,790 --> 00:14:14,360
층 신경망의 정의로 돌아가 보겠습니다.

196
00:14:14,360 --> 00:14:17,630
문헌이나 이 수업 외부에서 보셨겠지만,

197
00:14:17,630 --> 00:14:20,660
가중치와 입력, 층만을

198
00:14:20,660 --> 00:14:26,570
사용하는 이런 종류의 네트워크는 곱셈 외에 다른 연산이 없으며,

199
00:14:26,570 --> 00:14:30,110
보통 완전 연결 네트워크(fully

200
00:14:30,110 --> 00:14:33,830
connected networks)나 다층

201
00:14:33,830 --> 00:14:36,880
퍼셉트론(MLP)이라고 불립니다.

202
00:14:36,880 --> 00:14:40,250
이게 한 가지입니다.

203
00:14:40,250 --> 00:14:42,610
그리고 실제로 더 많은

204
00:14:42,610 --> 00:14:49,030
층을 쌓아서 더 크고 더 좋은 네트워크를 만들 수 있습니다.

205
00:14:49,030 --> 00:14:52,540
이 경우에도, 중간의

206
00:14:52,540 --> 00:14:54,970
은닉층과 그

207
00:14:54,970 --> 00:15:00,220
차원 수가 서로 맞아떨어지는지

208
00:15:00,220 --> 00:15:04,150
주의 깊게 보십시오.

209
00:15:04,150 --> 00:15:11,470
다시 신경망이 하는 일을 시각적으로 표현한 것으로

210
00:15:11,470 --> 00:15:14,003
돌아가 보겠습니다.

211
00:15:16,540 --> 00:15:21,460
선형 표현에 대해 이야기할 때,

212
00:15:21,460 --> 00:15:28,600
네트워크가 가중치를 통해 어떤 템플릿을 학습하는

213
00:15:28,600 --> 00:15:31,780
경우가 많다고 했습니다.

214
00:15:31,780 --> 00:15:34,500
지난주에 우리가 이런 템플릿들이 학습된다고

215
00:15:34,500 --> 00:15:37,120
이야기했던 것을 기억하실 겁니다.

216
00:15:37,120 --> 00:15:40,050
다시 말하지만, 템플릿이라고 하지만, 완전한 템플릿은 아닙니다.

217
00:15:40,050 --> 00:15:44,920
즉, 이미지의 대표적인 형태들이긴 한데,

218
00:15:44,920 --> 00:15:49,780
어떤 데이터로 학습되었느냐에 따라 달라집니다.

219
00:15:49,780 --> 00:15:54,990
지난주에 논의한 이 템플릿들은

220
00:15:54,990 --> 00:15:59,820
입력 뉴런 위에 W를 적용해서

221
00:15:59,820 --> 00:16:05,200
10개의 출력으로 생성되었습니다.

222
00:16:05,200 --> 00:16:11,830
이제 층이 여러 개가 생겼으니, 더 많은

223
00:16:11,830 --> 00:16:15,370
템플릿을 만들 수 있습니다.

224
00:16:15,370 --> 00:16:18,150
중간에 있는 층이 실제로

225
00:16:18,150 --> 00:16:22,950
10개가 아닌 100개의 템플릿을 만들 수 있게

226
00:16:22,950 --> 00:16:24,250
된 겁니다.

227
00:16:24,250 --> 00:16:26,350
물론 여전히 10개도 존재합니다.

228
00:16:26,350 --> 00:16:31,340
그리고 이것을 다시 한 번, 아주 높은 수준에서—아주 높은 수준의

229
00:16:31,340 --> 00:16:35,340
이해 관점에서, 이것이 무슨 의미인지 말씀드리는 겁니다.

230
00:16:35,340 --> 00:16:38,910
중간에 100개의 뉴런이 있을 때,

231
00:16:38,910 --> 00:16:40,700
네트워크에 전체 객체가

232
00:16:40,700 --> 00:16:44,580
아니라 객체의 일부에 대한 템플릿을 만들

233
00:16:44,580 --> 00:16:47,160
수 있는 힘을 주는 것입니다.

234
00:16:47,160 --> 00:16:49,200
예를 들어, 여기

235
00:16:49,200 --> 00:16:56,460
보이는 클래스들, 새, 고양이, 사슴, 개, 개구리, 말 모두 눈이 있죠.

236
00:16:56,460 --> 00:16:59,390
그래서 10개, 100개의 템플릿 중

237
00:16:59,390 --> 00:17:03,230
하나는 아마도 모든 클래스에 공통으로 쓰일 수 있는

238
00:17:03,230 --> 00:17:05,310
객체의 일부일 수 있습니다.

239
00:17:05,310 --> 00:17:08,150
그래서 높은 수준에서 보면,

240
00:17:08,150 --> 00:17:12,240
이것들이 템플릿을 형성할 수 있다는 겁니다.

241
00:17:12,240 --> 00:17:15,859
그리고 시각화 주제와 신경망에서

242
00:17:15,859 --> 00:17:19,640
배운 내용으로 돌아오면, 지금 제가

243
00:17:19,640 --> 00:17:22,550
말하는 것에 대해 더 자세히 알

244
00:17:22,550 --> 00:17:24,500
수 있을 겁니다.

245
00:17:24,500 --> 00:17:29,430
자, 다시 max 함수로 돌아가겠습니다.

246
00:17:29,430 --> 00:17:32,230
우리는 max 함수, 여기서 만들어지는

247
00:17:32,230 --> 00:17:34,040
비선형성에 대해 이야기했습니다.

248
00:17:34,040 --> 00:17:37,780
신경망 용어로는 이것을

249
00:17:37,780 --> 00:17:41,270
활성화 함수라고 부릅니다.

250
00:17:41,270 --> 00:17:45,920
그리고 이것은 모델을 만들고 신경망을 구축하는

251
00:17:45,920 --> 00:17:49,840
데 매우, 매우 중요한 역할, 핵심적인

252
00:17:49,840 --> 00:17:51,740
역할을 합니다.

253
00:17:51,740 --> 00:17:54,260
슬라이드에 있는 이 질문에 답해 봅시다.

254
00:17:54,260 --> 00:17:59,350
만약 이 활성화 함수들 중 하나, 예를 들어 max 함수를

255
00:17:59,350 --> 00:18:03,170
빼고 신경망을 만들려고 하면 어떻게 될까요?

256
00:18:03,170 --> 00:18:06,350
max를 제거하면 우리의 함수는 이렇게 됩니다.

257
00:18:06,350 --> 00:18:10,400
즉, W2 곱하기 W1 곱하기 x가 되는 거죠.

258
00:18:10,400 --> 00:18:12,220
여기서 무슨 일이 일어날까요?

259
00:18:12,220 --> 00:18:13,160
네, 정확합니다.

260
00:18:13,160 --> 00:18:17,740
그래서 예상하셨듯이, 그리고 정확하게

261
00:18:17,740 --> 00:18:23,710
말씀하셨듯이, W2와 W1의 곱셈은 사실 다른 행렬 W3로

262
00:18:23,710 --> 00:18:27,050
쉽게 대체할 수 있습니다.

263
00:18:27,050 --> 00:18:30,070
그러면 함수는 단순한 선형 함수가 됩니다.

264
00:18:30,070 --> 00:18:32,350
모든 것을 하나로 합칠 수 있다는 거죠.

265
00:18:32,350 --> 00:18:34,950
그래서 비선형 문제를

266
00:18:34,950 --> 00:18:40,710
해결할 수 있는 힘을 주기 위해서는 중간에 어떤

267
00:18:40,710 --> 00:18:44,410
형태의 비선형성이 필요합니다.

268
00:18:44,410 --> 00:18:49,420
방금 말씀드린 함수가 ReLU입니다.

269
00:18:49,420 --> 00:18:53,710
ReLU는 rectified linear unit, 즉 정류 선형
유닛입니다.

270
00:18:53,710 --> 00:18:56,700
신경망에서 매우 인기

271
00:18:56,700 --> 00:18:59,920
있는 활성화 함수입니다.

272
00:18:59,920 --> 00:19:02,790
다른 많은 변형들이 다양한

273
00:19:02,790 --> 00:19:07,630
아키텍처, 심지어 최신 아키텍처에서도 테스트되었지만,

274
00:19:07,630 --> 00:19:11,220
ReLU의 문제 중 하나는,

275
00:19:11,220 --> 00:19:13,380
양수가 아니면

276
00:19:13,380 --> 00:19:16,470
값을 0으로 만들어서 죽은

277
00:19:16,470 --> 00:19:20,700
뉴런(dead neurons)을 만들기도

278
00:19:20,700 --> 00:19:22,240
한다는 점입니다.

279
00:19:22,240 --> 00:19:27,540
죽은 뉴런을 피하기 위해 leaky ReLU

280
00:19:27,540 --> 00:19:32,840
같은 모델링이나 ELU, exponential

281
00:19:32,840 --> 00:19:38,960
linear unit 같은 다른 옵션들이 있습니다.

282
00:19:38,960 --> 00:19:41,180
ELU는 0을 중심으로 한

283
00:19:41,180 --> 00:19:44,250
함수가 더 좋아서 조금 더 나은 편입니다.

284
00:19:44,250 --> 00:19:52,010
그리고 더 새로운 변형들도 있는데, GeLU, Gaussian linear

285
00:19:52,010 --> 00:19:55,250
unit 같은 것들이 있습니다.

286
00:19:55,250 --> 00:19:59,100
저는 ELU와 GeLU 두 가지 변형을 들어봤는데,

287
00:19:59,100 --> 00:20:02,130
둘 다 사용할 수 있습니다.

288
00:20:02,130 --> 00:20:05,600
이들은 종종 트랜스포머 같은 신경망 아키텍처에서

289
00:20:05,600 --> 00:20:07,110
더 자주 사용됩니다.

290
00:20:07,110 --> 00:20:14,840
또한 SiLU, 즉 sigmoid linear unit도 있습니다.

291
00:20:14,840 --> 00:20:20,720
이것은 일부 최신 CNNR

292
00:20:20,720 --> 00:20:25,180
아키텍처에서 사용되며,

293
00:20:25,180 --> 00:20:29,440
구글도 이걸 일부 모델 변형과

294
00:20:29,440 --> 00:20:33,910
EfficientNet에서 사용했습니다.

295
00:20:33,910 --> 00:20:40,030
이 외에도 sigmoid나 Tanh 같은

296
00:20:40,030 --> 00:20:48,020
함수들이 있는데, 이들은 종종 활성화 함수로 사용됩니다.

297
00:20:48,020 --> 00:20:52,750
하지만 이 함수들은 값을 좁은 범위로

298
00:20:52,750 --> 00:20:57,020
압축하기 때문에 몇 가지 문제가 있습니다.

299
00:20:57,020 --> 00:21:01,820
이로 인해 기울기 소실(vanishing gradients) 현상이
발생하기도 합니다.

300
00:21:01,820 --> 00:21:05,530
그래서 신경망 중간층에서는 sigmoid나

301
00:21:05,530 --> 00:21:08,420
Tanh를 잘 사용하지 않습니다.

302
00:21:08,420 --> 00:21:13,390
대신 출력 값을 이진화하는

303
00:21:13,390 --> 00:21:15,970
등 후반부 층에서

304
00:21:15,970 --> 00:21:19,370
주로 사용됩니다.

305
00:21:19,370 --> 00:21:22,850
말씀드렸듯이, ReLU가 보통 좋은 기본 선택입니다.

306
00:21:22,850 --> 00:21:26,860
많은 아키텍처에서 매우 널리 사용됩니다.

307
00:21:26,860 --> 00:21:30,120
우리가 이야기한 함수의

308
00:21:30,120 --> 00:21:33,390
변형도 매우 다양합니다.

309
00:21:33,390 --> 00:21:36,810
지금까지 이야기한 내용을 요약하고 질문에

310
00:21:36,810 --> 00:21:38,650
답변드리겠습니다.

311
00:21:38,650 --> 00:21:45,280
우리는 여러 층을 더하는 것에 대해 이야기했죠.

312
00:21:45,280 --> 00:21:49,080
하지만 활성화 함수는 주로 층 안에서

313
00:21:49,080 --> 00:21:54,040
작동하는 함수라는 점을 강조하고 싶습니다.

314
00:21:54,040 --> 00:21:58,170
그리고 이전 층과 다음 층 사이의

315
00:21:58,170 --> 00:22:01,170
가중치 매핑을

316
00:22:01,170 --> 00:22:04,030
정의하는 W도 있습니다.

317
00:22:04,030 --> 00:22:07,350
다시 말해, 아주 간단한

318
00:22:07,350 --> 00:22:12,400
구현의 완전 연결 신경망입니다.

319
00:22:12,400 --> 00:22:17,310
우리가 필요한 것은 활성화 함수를 정의하는

320
00:22:17,310 --> 00:22:17,830
것입니다.

321
00:22:17,830 --> 00:22:20,530
예를 들어, 이 예제에서는

322
00:22:20,530 --> 00:22:26,180
sigmoid 함수를 활성화 함수로 정의했고,

323
00:22:26,180 --> 00:22:30,180
이를 사용해 쉽게 계산할 수 있습니다.

324
00:22:30,180 --> 00:22:35,190
첫 번째와 두 번째 은닉층의

325
00:22:35,190 --> 00:22:42,800
값들은 W1과 x의 곱에 편향을 더한 뒤

326
00:22:42,800 --> 00:22:46,910
활성화 함수를 적용해

327
00:22:46,910 --> 00:22:48,060
계산합니다.

328
00:22:48,060 --> 00:22:50,510
H2도 마찬가지고,

329
00:22:50,510 --> 00:22:57,770
출력은 W3와 마지막 은닉층

330
00:22:57,770 --> 00:23:01,830
값의 내적을 통해 간단히

331
00:23:01,830 --> 00:23:04,290
만듭니다.

332
00:23:04,290 --> 00:23:07,140
여기서 잠시 멈추고 질문이 있으면 답변하겠습니다.

333
00:23:07,140 --> 00:23:10,280
그리고 계속 진행하겠습니다.

334
00:23:10,280 --> 00:23:11,370
좋은 질문입니다.

335
00:23:11,370 --> 00:23:13,640
새로운 문제에 어떤

336
00:23:13,640 --> 00:23:16,520
활성화 함수를 사용할지 어떻게

337
00:23:16,520 --> 00:23:18,920
선택하느냐는 질문이군요.

338
00:23:18,920 --> 00:23:21,160
간단한 답은 대부분

339
00:23:21,160 --> 00:23:26,140
경험적(empirical)이라는 겁니다.

340
00:23:26,140 --> 00:23:29,140
하지만 보통은 특정

341
00:23:29,140 --> 00:23:32,290
아키텍처에 맞게 표준

342
00:23:32,290 --> 00:23:35,890
활성화 함수부터 시작합니다.

343
00:23:35,890 --> 00:23:40,150
예를 들어 CNN이나 트랜스포머

344
00:23:40,150 --> 00:23:45,160
같은 아키텍처에서 자주 쓰이는 활성화

345
00:23:45,160 --> 00:23:47,600
함수들이 있죠.

346
00:23:47,600 --> 00:23:53,330
그래서 이전에 검증된 함수들을 주로 사용합니다.

347
00:23:53,330 --> 00:23:55,430
하지만 대부분은 경험에 의존합니다.

348
00:23:55,430 --> 00:23:59,090
새로운 문제에 맞는 네트워크를 설계할

349
00:23:59,090 --> 00:24:02,690
때, 활성화 함수 선택도 다른 하이퍼파라미터처럼

350
00:24:02,690 --> 00:24:05,830
중요한 결정 사항입니다.

351
00:24:05,830 --> 00:24:10,060
여기서 질문은, 이 모든 활성화 함수들이

352
00:24:10,060 --> 00:24:13,570
공통으로 가진 속성은 무엇이며,

353
00:24:13,570 --> 00:24:17,090
실제로 무엇을 하는가입니다.

354
00:24:17,090 --> 00:24:19,570
몇 가지 예를 들어 설명하겠습니다.

355
00:24:19,570 --> 00:24:23,580
그리고 이 활성화 함수들이 하는 일의

356
00:24:23,580 --> 00:24:26,640
세부사항도 살펴보겠습니다.

357
00:24:26,640 --> 00:24:34,770
기본적으로 가장 중요하고 공통된 특징은 비선형성(nonlinearity)을

358
00:24:34,770 --> 00:24:37,260
만드는 것입니다.

359
00:24:37,260 --> 00:24:40,420
활성화 함수로 선형 함수를 사용하지 않는 이유가 바로 여기에 있습니다.

360
00:24:40,420 --> 00:24:43,110
그래서 어떤 비선형성을

361
00:24:43,110 --> 00:24:46,330
만드는 것이 매우 중요합니다.

362
00:24:46,330 --> 00:24:48,820
그리고 왜 이렇게 많은 변형들이 있을까요?

363
00:24:48,820 --> 00:24:51,300
소실되는 그래디언트 문제에

364
00:24:51,300 --> 00:24:53,320
대해 조금 말씀드렸습니다.

365
00:24:53,320 --> 00:24:58,050
함수의 미분 가능성에 대해서도 조금

366
00:24:58,050 --> 00:24:59,440
말씀드렸죠.

367
00:24:59,440 --> 00:25:02,040
신경망에서 사용하기 때문에 함수는 미분

368
00:25:02,040 --> 00:25:03,300
가능해야 합니다.

369
00:25:03,300 --> 00:25:10,980
때로는 적절한 0 중심 값과 부드러운 함수가

370
00:25:10,980 --> 00:25:14,040
수렴 속도를 훨씬

371
00:25:14,040 --> 00:25:17,100
빠르게 만듭니다.

372
00:25:17,100 --> 00:25:19,650
그래서 여러 가지 다양한 요인들이 있습니다.

373
00:25:19,650 --> 00:25:23,310
제가 말씀드리고 이야기한 주요

374
00:25:23,310 --> 00:25:25,880
요인들이 바로 이런 함수들을

375
00:25:25,880 --> 00:25:30,650
정의하거나 설계하는 데 중요한 역할을 합니다.

376
00:25:30,650 --> 00:25:32,150
함수의 세부사항을

377
00:25:32,150 --> 00:25:36,710
설명할 때 좀 더 이야기하겠습니다.

378
00:25:36,710 --> 00:25:42,180
모든 층에서 종종 같은 활성화 함수를 사용합니다.

379
00:25:42,180 --> 00:25:45,500
하지만 말씀드렸듯이, 때로는 후반

380
00:25:45,500 --> 00:25:49,610
층이나 출력 층에서 시그모이드 함수나

381
00:25:49,610 --> 00:25:55,290
탄젠트 함수를 사용하기도 하지만, 일반적으로는 그렇습니다.

382
00:25:55,290 --> 00:26:04,340
질문은, 네트워크 전체에서 모든 뉴런에 같은 함수를

383
00:26:04,340 --> 00:26:09,020
사용하는가 하는 것이었죠?

384
00:26:09,020 --> 00:26:16,240
네, 우리가 이야기하던 신경망

385
00:26:16,240 --> 00:26:23,450
모델 구현에 대해

386
00:26:23,450 --> 00:26:25,580
계속하겠습니다.

387
00:26:25,580 --> 00:26:28,840
아주 간단한 방법이 있습니다.

388
00:26:28,840 --> 00:26:31,040
즉, 파이썬으로

389
00:26:31,040 --> 00:26:32,950
2층 신경망을 만드는

390
00:26:32,950 --> 00:26:36,740
코드는 20줄도 안 됩니다.

391
00:26:36,740 --> 00:26:40,990
매우 간단하게, 앞서 말한 차원에 맞춰 네트워크를

392
00:26:40,990 --> 00:26:42,280
정의하면 됩니다.

393
00:26:42,280 --> 00:26:45,640
N은 샘플의 개수입니다.

394
00:26:45,640 --> 00:26:48,490
D_in은 입력의 차원 수입니다.

395
00:26:48,490 --> 00:26:51,110
그리고 D_out은 출력의 차원 수입니다.

396
00:26:51,110 --> 00:26:56,570
그리고 h는 은닉층의 뉴런 수입니다.

397
00:26:56,570 --> 00:27:00,700
그리고 우리는 x와 y를 생성하고 W를

398
00:27:00,700 --> 00:27:05,630
무작위로 초기화하는 것에 대해 이야기했습니다.

399
00:27:05,630 --> 00:27:09,550
그다음에는 순전파가 있는데,

400
00:27:09,550 --> 00:27:16,000
이는 W를 입력에 층별로 적용하여

401
00:27:16,000 --> 00:27:24,870
최종적으로 출력, 즉 예측값을 만들고 마지막으로

402
00:27:24,870 --> 00:27:29,610
손실 함수를 계산해 손실 값을

403
00:27:29,610 --> 00:27:33,360
출력하는 과정입니다.

404
00:27:33,360 --> 00:27:39,240
순전파 후에는 최적화 과정이 필요합니다.

405
00:27:39,240 --> 00:27:43,260
분석적 기울기를 계산하고,

406
00:27:43,260 --> 00:27:48,420
그 기울기를 사용해 경사

407
00:27:48,420 --> 00:27:53,920
하강법을 실행하여 W1과 W2를 최적값

408
00:27:53,920 --> 00:27:57,090
쪽으로 한 걸음씩

409
00:27:57,090 --> 00:27:59,890
조정하는 방법입니다.

410
00:27:59,890 --> 00:28:04,590
하지만 여기서 분석적 기울기를 계산하는

411
00:28:04,590 --> 00:28:08,910
부분이 가장 중요한데, 아직

412
00:28:08,910 --> 00:28:11,790
많이 다루지 않았습니다.

413
00:28:11,790 --> 00:28:13,571
그래서 이 강의의

414
00:28:13,571 --> 00:28:15,410
거의 나머지 부분은 이

415
00:28:15,410 --> 00:28:21,600
과정을 작동시키고 다양한 환경에서 확장하는 방법에 관한 것입니다.

416
00:28:21,600 --> 00:28:29,490
이렇게 신경망을 훈련시키고 구축한 후, 은닉층의

417
00:28:29,490 --> 00:28:34,890
노드 수에 따라 두 클래스 간의

418
00:28:34,890 --> 00:28:36,920
분리 패턴이

419
00:28:36,920 --> 00:28:41,510
달라지는 것을 볼 수

420
00:28:41,510 --> 00:28:42,600
있습니다.

421
00:28:42,600 --> 00:28:45,920
더 많은 뉴런은 보통 더

422
00:28:45,920 --> 00:28:51,470
복잡한 함수를 학습할 수 있는 용량이 크고,

423
00:28:51,470 --> 00:28:57,903
노드나 점들의 분리가 더 잘 된다는 의미입니다.

424
00:29:00,440 --> 00:29:05,100
이걸 보면, 이것은 이와 매우 비슷합니다.

425
00:29:05,100 --> 00:29:07,820
여기서 보여주는 패턴은 두 번째

426
00:29:07,820 --> 00:29:11,400
강의에서 k 최근접 이웃에 대해 이야기할 때

427
00:29:11,400 --> 00:29:13,140
보여준 것과 비슷합니다.

428
00:29:13,140 --> 00:29:20,790
k가 1인 경우, 즉 가장 가까운 이웃 하나만 사용하는 경우가 더

429
00:29:20,790 --> 00:29:25,600
많은 뉴런을 사용하는 것과 매우 유사했습니다.

430
00:29:25,600 --> 00:29:28,650
그래서 여기서도 같은 논리가

431
00:29:28,650 --> 00:29:33,510
적용되는데, 네트워크에 너무 많은 용량을 주면 과적합

432
00:29:33,510 --> 00:29:36,600
문제가 발생할 수 있습니다.

433
00:29:36,600 --> 00:29:40,570
즉, 보지 못한 데이터에 대해 일반화하지 못할 수 있다는 겁니다.

434
00:29:40,570 --> 00:29:45,940
하지만 이 문제에 대해서도 다양한 해결책이 있습니다.

435
00:29:45,940 --> 00:29:49,810
그리고 여기서 강조하고 싶은 경험

436
00:29:49,810 --> 00:29:51,990
법칙 하나는 신경망의

437
00:29:51,990 --> 00:29:56,800
크기를 정규화 항으로 사용하지 말라는 겁니다.

438
00:29:56,800 --> 00:29:59,970
우리는 신경망 크기를 하이퍼파라미터로

439
00:29:59,970 --> 00:30:02,500
자주 조정하지 않습니다.

440
00:30:02,500 --> 00:30:06,720
물론 신경망 크기와 관련 하이퍼파라미터의

441
00:30:06,720 --> 00:30:11,040
다양한 값을 실험해 보긴 합니다.

442
00:30:11,040 --> 00:30:17,900
하지만 보통은 필요한 것보다 조금 더 큰 네트워크를

443
00:30:17,900 --> 00:30:19,350
선택합니다.

444
00:30:19,350 --> 00:30:22,400
그리고 나서

445
00:30:22,400 --> 00:30:25,310
정규화와 이 정규화

446
00:30:25,310 --> 00:30:33,710
하이퍼파라미터를 사용해서 여러 설정을

447
00:30:33,710 --> 00:30:35,250
확인하죠.

448
00:30:35,250 --> 00:30:39,080
그래서 보통 조정하는 것은 정규화와

449
00:30:39,080 --> 00:30:41,030
정규화 하이퍼파라미터이지,

450
00:30:41,030 --> 00:30:45,380
네트워크 크기 자체는 아닙니다.

451
00:30:45,380 --> 00:30:53,310
좋습니다, 이것이 신경망의 개념을 간단히 정리한 것입니다.

452
00:30:53,310 --> 00:30:58,160
하지만 신경망이 생물학과 어떻게 연관될

453
00:30:58,160 --> 00:31:02,700
수 있는지, 생물학적 영감에 대해

454
00:31:02,700 --> 00:31:04,980
들어본 적이 있죠.

455
00:31:04,980 --> 00:31:07,990
그래서 그 부분에 대해 조금 이야기하겠지만, 질문이 있습니다.

456
00:31:07,990 --> 00:31:13,930
기본적으로 질문은, 왜 여기서 람다 값을 증가시키면 모델이 더

457
00:31:13,930 --> 00:31:16,550
언더피팅되는가 하는 거죠?

458
00:31:16,550 --> 00:31:22,340
네, 그 질문에 빠르게 답하자면, 람다

459
00:31:22,340 --> 00:31:24,940
값은 정규화 항이

460
00:31:24,940 --> 00:31:27,250
전체 손실에 얼마나

461
00:31:27,250 --> 00:31:30,382
기여할지를 조절합니다.

462
00:31:30,382 --> 00:31:35,870
정규화 항의 기여도가 클수록—그리고 정규화 항은

463
00:31:35,870 --> 00:31:39,140
W에 대해 정의된 것이었죠—

464
00:31:39,140 --> 00:31:41,360
즉, W를 제약하는 겁니다.

465
00:31:41,360 --> 00:31:45,530
W 값에 자유도가 줄어들게 되고,

466
00:31:45,530 --> 00:31:54,440
자유도가 줄어든다는 것은 좀 더 일반적인 경계가 만들어진다는 뜻이지, 세밀한

467
00:31:54,440 --> 00:31:58,570
값이나 경계의 세부적인 부분을

468
00:31:58,570 --> 00:32:01,670
반영하지 않는다는 겁니다.

469
00:32:01,670 --> 00:32:06,860
그래서 모델을 너무 제한하면, 정규화 항이

470
00:32:06,860 --> 00:32:11,680
있어도 그런 값들, 그런 결정 경계가

471
00:32:11,680 --> 00:32:13,810
나오게 됩니다.

472
00:32:13,810 --> 00:32:17,320
네, 올바른 정규화 항은 항상 과적합을

473
00:32:17,320 --> 00:32:19,540
막습니다--과적합을 방지하는 거죠.

474
00:32:19,540 --> 00:32:21,830
다시 말해, 손실과 올바른

475
00:32:21,830 --> 00:32:27,470
출력을 예측하는 것 사이에서 타협, 균형을 만드는 겁니다.

476
00:32:27,470 --> 00:32:30,530
손실의 첫 번째 부분은 올바른 출력을 예측하는 것입니다.

477
00:32:30,530 --> 00:32:33,620
두 번째 부분은 가중치 값만 조절하며,

478
00:32:33,620 --> 00:32:35,780
출력에는 신경 쓰지 않습니다.

479
00:32:35,780 --> 00:32:37,870
이 부분에 너무 무게를

480
00:32:37,870 --> 00:32:40,750
두면 좋은 분류기를 얻기 어렵습니다.

481
00:32:40,750 --> 00:32:44,000
그래서 균형을 맞추는 것이 중요하며, 정규화 항은 항상 좋습니다.

482
00:32:44,000 --> 00:32:46,870
하지만 너무 많이 사용하면 아무것도 좋지 않습니다.

483
00:32:53,440 --> 00:32:55,450
왜 크기 대신

484
00:32:55,450 --> 00:32:57,790
정규화를 선택하는지 다시

485
00:32:57,790 --> 00:32:59,980
설명해 주시겠어요?

486
00:32:59,980 --> 00:33:03,000
여러 가지 이유가 있습니다.

487
00:33:03,000 --> 00:33:05,010
그중 하나는 네트워크의 크기입니다.

488
00:33:05,010 --> 00:33:06,300
네트워크를 구축할 때,

489
00:33:06,300 --> 00:33:08,060
때로는 결과를

490
00:33:08,060 --> 00:33:12,470
얻기 위해 며칠 동안 실행해야 하는

491
00:33:12,470 --> 00:33:14,840
네트워크를 만듭니다.

492
00:33:14,840 --> 00:33:20,990
그래서 보통 네트워크의 파라미터 수를 점점

493
00:33:20,990 --> 00:33:24,710
늘리다가 과적합이 나타나는

494
00:33:24,710 --> 00:33:29,390
수준까지 키우는 경우가 많습니다.

495
00:33:29,390 --> 00:33:33,200
그때가 네트워크가 데이터의 패턴을

496
00:33:33,200 --> 00:33:35,360
실제로 이해하고 데이터를

497
00:33:35,360 --> 00:33:39,080
암기할 수 있게 된 시점입니다.

498
00:33:39,080 --> 00:33:43,070
그리고 그때 정규화를 통해 과적합을

499
00:33:43,070 --> 00:33:45,750
최소화하려고 합니다.

500
00:33:45,750 --> 00:33:48,840
그래서 정규화가 중요한 역할을 합니다.

501
00:33:48,840 --> 00:33:54,080
네트워크의 파라미터 수나 복잡도를 너무

502
00:33:54,080 --> 00:33:56,900
높이면 문제가 생기게

503
00:33:56,900 --> 00:33:58,530
됩니다.

504
00:33:58,530 --> 00:34:00,600
우리는 그런 경우는 거의 하지 않습니다.

505
00:34:00,600 --> 00:34:05,020
새로운 문제에 대해서는 보통 작은 네트워크로

506
00:34:05,020 --> 00:34:09,250
시작해서 정규화 기법으로 조절하며

507
00:34:09,250 --> 00:34:11,530
점차 키워 나갑니다.

508
00:34:11,530 --> 00:34:14,710
주어진 문제에 몇 개의 뉴런이

509
00:34:14,710 --> 00:34:19,190
필요한지는 어떻게 알 수 있을까요?

510
00:34:19,190 --> 00:34:25,929
그건 경험적 연구와 비슷한 유형의 다른 연구들을 참고하는 것에

511
00:34:25,929 --> 00:34:29,590
기반합니다. 모든 문제에 맞는

512
00:34:29,590 --> 00:34:31,880
정해진 공식은 없습니다.

513
00:34:31,880 --> 00:34:36,530
비슷한 데이터로 훈련된 다른

514
00:34:36,530 --> 00:34:40,600
네트워크들을 참고해서 그 범위에서

515
00:34:40,600 --> 00:34:42,500
시작해야 합니다.

516
00:34:42,500 --> 00:34:46,960
그리고 나서 직접 여러 실험을 하면서 네트워크의

517
00:34:46,960 --> 00:34:49,480
복잡도를 조절하고

518
00:34:49,480 --> 00:34:50,870
균형을 맞춥니다.

519
00:34:50,870 --> 00:34:58,300
그래서 항상 탐색과 실험에 많이 의존하게 됩니다.

520
00:34:58,300 --> 00:35:02,370
질문하신 것은 어떤 활성화 함수를 쓰고,

521
00:35:02,370 --> 00:35:07,090
몇 개의 층을 쓸지에 대해 이론적이고

522
00:35:07,090 --> 00:35:10,120
기초적인 연구가 있느냐는 거죠?

523
00:35:10,120 --> 00:35:13,710
이와 관련된 많은 연구와

524
00:35:13,710 --> 00:35:16,860
논문들이 있고, 네트워크의

525
00:35:16,860 --> 00:35:22,980
메타파라미터나 하이퍼파라미터를 최적화하는 방법도

526
00:35:22,980 --> 00:35:24,700
있습니다.

527
00:35:24,700 --> 00:35:27,690
하지만 우리는 자세히 다루지

528
00:35:27,690 --> 00:35:29,940
않을 겁니다. 왜냐하면

529
00:35:29,940 --> 00:35:33,720
이것들은 데이터셋과 문제에 매우 의존적이기

530
00:35:33,720 --> 00:35:34,840
때문입니다.

531
00:35:34,840 --> 00:35:39,570
그래서 가장 좋은 답변은, 네, 그런 연구들이

532
00:35:39,570 --> 00:35:41,710
존재한다는 겁니다.

533
00:35:41,710 --> 00:35:45,750
하지만 각각의 연구는 여러분의 응용이나

534
00:35:45,750 --> 00:35:50,130
문제에 반드시 맞는 가정을 하고 있지는 않을

535
00:35:50,130 --> 00:35:51,600
수 있습니다.

536
00:35:51,600 --> 00:35:58,380
그래서 생물학적 영감을 받은 방법들도 있습니다.

537
00:35:58,380 --> 00:36:01,350
다시 말하지만, 이런 영감들은 매우 느슨한 수준입니다.

538
00:36:01,350 --> 00:36:03,500
여기 앉아 계시거나

539
00:36:03,500 --> 00:36:10,280
온라인으로 보고 계신 신경과학자가 있다면, 제가 드리는 모든

540
00:36:10,280 --> 00:36:17,640
예시나 이야기들을 절대 절대 진리로 받아들이지 마시기 바랍니다.

541
00:36:17,640 --> 00:36:22,010
하지만 일반적으로 뉴런에서 일어나는

542
00:36:22,010 --> 00:36:26,970
일은-- 이것은 뉴런의 시각화입니다--

543
00:36:26,970 --> 00:36:32,450
세포체가 있고, 이 세포체는 종종 수상돌기를

544
00:36:32,450 --> 00:36:39,440
통해 전달된 신호들을 모으는 역할을 합니다, 세포체 자체,

545
00:36:39,440 --> 00:36:40,980
세포체입니다.

546
00:36:40,980 --> 00:36:44,270
그리고 축색돌기를 이용해 그

547
00:36:44,270 --> 00:36:47,760
신호들은 다른 뉴런으로 전달됩니다.

548
00:36:47,760 --> 00:36:50,360
이것은 우리가 신경망에서 하는

549
00:36:50,360 --> 00:36:52,410
일과 매우 유사합니다.

550
00:36:52,410 --> 00:36:56,290
우리는 종종 이전 층에서

551
00:36:56,290 --> 00:37:04,610
온 모든 신호, 즉 이전의 활성화들을 포착하는 함수를

552
00:37:04,610 --> 00:37:07,430
가지고 있습니다.

553
00:37:07,430 --> 00:37:12,040
그리고 세포체에서는 그 함수가

554
00:37:12,040 --> 00:37:20,080
입력에 작용하여 활성화를 출력하고 다음 층,

555
00:37:20,080 --> 00:37:24,050
다음 뉴런으로 전달합니다.

556
00:37:24,050 --> 00:37:27,130
그래서 기본적으로 여기서 활성화

557
00:37:27,130 --> 00:37:32,170
함수가 필요한 이유는 신호를 생성해서 값을

558
00:37:32,170 --> 00:37:36,130
증가시키거나 감소시키기 위해서입니다.

559
00:37:36,130 --> 00:37:42,010
이 점에서, 생물학적 뉴런과 우리가 구축하는

560
00:37:42,010 --> 00:37:45,310
신경망 사이에는 많은 차이가

561
00:37:45,310 --> 00:37:50,470
있고, 실제로 생물학적 뉴런이 훨씬 더

562
00:37:50,470 --> 00:37:52,670
복잡할 수 있습니다.

563
00:37:52,670 --> 00:37:58,560
하지만 일반적으로 공통된 개념들이 있습니다.

564
00:37:58,560 --> 00:38:00,450
우리가 구축하는 신경망은

565
00:38:00,450 --> 00:38:03,520
종종 규칙적인 패턴으로 조직되어 있습니다.

566
00:38:03,520 --> 00:38:05,850
이런 패턴들은

567
00:38:05,850 --> 00:38:09,090
신경망을 구현할 때 계산

568
00:38:09,090 --> 00:38:12,490
효율을 높이기 위해서입니다.

569
00:38:12,490 --> 00:38:14,760
복잡한 신경망을

570
00:38:14,760 --> 00:38:18,340
만들고 최적화하려는 연구도

571
00:38:18,340 --> 00:38:22,470
있었지만, 결과적으로는

572
00:38:22,470 --> 00:38:27,510
우리가 이 수업에서 다룰 일반적인

573
00:38:27,510 --> 00:38:30,210
신경망과 거의 비슷한

574
00:38:30,210 --> 00:38:33,213
성능을 보입니다.

575
00:38:36,150 --> 00:38:42,480
뇌에 관한 비유를 할 때는 해석에 주의하라는

576
00:38:42,480 --> 00:38:48,310
점을 아무리 강조해도 부족합니다.

577
00:38:48,310 --> 00:38:49,860
차이가 너무 많기 때문입니다.

578
00:38:49,860 --> 00:38:54,710
여기서 멈추겠고, 신경과학 측면에

579
00:38:54,710 --> 00:38:56,780
관심 있는 분이

580
00:38:56,780 --> 00:39:01,170
계시면 기꺼이 토론하겠습니다.

581
00:39:01,170 --> 00:39:08,660
모든 것을 연결해 보면, 우리는 점수 함수가 있었습니다.

582
00:39:08,660 --> 00:39:12,410
이 점수 함수는 입력을 W의

583
00:39:12,410 --> 00:39:19,230
가중치 벡터나 가중치 행렬을 통해 점수로 변환합니다.

584
00:39:19,230 --> 00:39:27,200
그리고 네트워크의 손실 함수로 자주 사용하는 것은 그 점수들을 힌지 손실,

585
00:39:27,200 --> 00:39:31,070
소프트맥스, 또는 다른 변형을

586
00:39:31,070 --> 00:39:33,570
통해 사용하는 것입니다.

587
00:39:33,570 --> 00:39:39,470
그리고 그에 더해, 우리는 정규화 항을 정의했는데,

588
00:39:39,470 --> 00:39:44,360
이는 결국 데이터 손실과 정규화

589
00:39:44,360 --> 00:39:47,610
항을 합한 총 손실을 줍니다.

590
00:39:47,610 --> 00:39:51,610
그리고 W1과 W2를

591
00:39:51,610 --> 00:39:56,210
최적화하려면 L을

592
00:39:56,210 --> 00:40:00,070
W1과 W2에 대해

593
00:40:00,070 --> 00:40:03,430
편미분할 수 있어야

594
00:40:03,430 --> 00:40:11,000
한다는 사실에 대해 이야기했습니다.

595
00:40:11,000 --> 00:40:13,930
우리가 알아야 할

596
00:40:13,930 --> 00:40:18,830
세부 사항이 정말 많습니다.

597
00:40:18,830 --> 00:40:24,370
먼저, 이 함수들을 만들고 미분을 구해서

598
00:40:24,370 --> 00:40:27,890
적는 작업은 종종 지루합니다.

599
00:40:27,890 --> 00:40:30,250
행렬 계산이 많고 실제로

600
00:40:30,250 --> 00:40:33,520
신경망을 구현하기 전에 종이에

601
00:40:33,520 --> 00:40:36,230
많은 작업이 필요합니다.

602
00:40:36,230 --> 00:40:38,020
또 다른 문제는,

603
00:40:38,020 --> 00:40:42,370
만약 논문에서 한 것과 다르게 손실 함수를

604
00:40:42,370 --> 00:40:46,180
약간 바꾸고 싶다면 모든 계산을

605
00:40:46,180 --> 00:40:48,910
다시 해야 한다는 점입니다.

606
00:40:48,910 --> 00:40:55,840
그럴 경우, 다시 전체 작업을 해야 합니다.

607
00:40:55,840 --> 00:41:02,520
마지막으로, 손실 함수가 복잡하면 이 작업은 다루기

608
00:41:02,520 --> 00:41:06,310
어렵고 때로는 불가능해집니다.

609
00:41:06,310 --> 00:41:12,270
복잡한 함수일수록 더 어려워집니다.

610
00:41:12,270 --> 00:41:15,390
하지만 더 나은 방법이 있는데,

611
00:41:15,390 --> 00:41:18,970
이것은 우리 구현에서 자주 사용됩니다.

612
00:41:18,970 --> 00:41:22,200
오늘은 모두가 같은 이해를

613
00:41:22,200 --> 00:41:25,650
할 수 있도록 몇 가지

614
00:41:25,650 --> 00:41:28,680
예를 들어 설명하겠습니다.

615
00:41:28,680 --> 00:41:35,670
그것이 바로 계산 그래프와 역전파의 개념입니다.

616
00:41:35,670 --> 00:41:40,650
계산 그래프는 신경망의 모든

617
00:41:40,650 --> 00:41:43,470
연산을 모아서

618
00:41:43,470 --> 00:41:48,150
단계별로 구성하고, 입력과 필요한

619
00:41:48,150 --> 00:41:52,050
모든 매개변수에서

620
00:41:52,050 --> 00:41:55,410
시작해 최종 출력인

621
00:41:55,410 --> 00:41:59,080
손실을 얻는 것입니다.

622
00:41:59,080 --> 00:42:02,200
이 경우, 손실 함수는 소프트맥스

623
00:42:02,200 --> 00:42:04,140
함수나 힌지 손실

624
00:42:04,140 --> 00:42:05,740
함수일 수 있습니다.

625
00:42:05,740 --> 00:42:07,870
무엇이든 간에,

626
00:42:07,870 --> 00:42:14,430
손실 함수는 정규화 항인 함수 RW에 더해집니다.

627
00:42:14,430 --> 00:42:18,370
그리고 R은 W를 입력으로 받습니다.

628
00:42:18,370 --> 00:42:23,490
그래서 이 두 개를 더하면 손실을 계산하거나 만듭니다.

629
00:42:23,490 --> 00:42:28,780
손실을 계산하기 전에, 우리는 종종

630
00:42:28,780 --> 00:42:34,630
x와 W를 합산하여 점수를 만들어야 합니다.

631
00:42:34,630 --> 00:42:37,620
이것은 곱셈 함수입니다.

632
00:42:37,620 --> 00:42:41,040
이것은 실제로 매우 유용한데, 우리가 만드는

633
00:42:41,040 --> 00:42:43,500
대부분의 신경망이 그래픽 카드

634
00:42:43,500 --> 00:42:45,760
표현을 가지고 있기 때문입니다.

635
00:42:45,760 --> 00:42:48,310
이 모든 복잡한 함수들은

636
00:42:48,310 --> 00:42:52,430
동일한 프레임워크로 표현할 수 있습니다.

637
00:42:52,430 --> 00:42:58,090
그리고 이를 사용해 입력 이미지나 입력 데이터부터 계산

638
00:42:58,090 --> 00:43:01,160
그래프를 구축할 수 있습니다.

639
00:43:01,160 --> 00:43:03,920
네트워크 전체에 걸쳐 많은 가중치들이 있습니다.

640
00:43:03,920 --> 00:43:06,103
마지막으로 손실 함수가 있습니다.

641
00:43:09,730 --> 00:43:13,360
다시 말하지만, 이것은 신경

642
00:43:13,360 --> 00:43:18,130
튜링 머신 같은 복잡한 신경망에

643
00:43:18,130 --> 00:43:19,220
유용합니다.

644
00:43:19,220 --> 00:43:23,960
이것은 실제로 시계열 및 순차 데이터에 사용됩니다.

645
00:43:23,960 --> 00:43:27,230
그래서 이 기계가 많이 펼쳐집니다.

646
00:43:27,230 --> 00:43:30,980
만약 모든 작업을 수작업으로

647
00:43:30,980 --> 00:43:39,382
해야 한다면, 이는 불가능하고 실행 불가능할 것입니다.

648
00:43:39,382 --> 00:43:44,620
그래서 계산 그래프를 구축할 때,

649
00:43:44,620 --> 00:43:49,830
그 해결책이 역전파입니다.

650
00:43:49,830 --> 00:43:53,320
그리고 아주 간단한 예제로 시작하고자 합니다.

651
00:43:53,320 --> 00:43:59,460
그래서 우리는 x, y, z의 함수 f를 시작합니다. 이

652
00:43:59,460 --> 00:44:03,220
함수는 x 더하기 y 곱하기 z입니다.

653
00:44:03,220 --> 00:44:09,180
이 함수의 계산 그래프를 그리면,

654
00:44:09,180 --> 00:44:15,210
x와 y 사이의 덧셈 연산이

655
00:44:15,210 --> 00:44:17,440
있습니다.

656
00:44:17,440 --> 00:44:20,970
그리고 그 덧셈

657
00:44:20,970 --> 00:44:29,100
결과에 z를 곱하는 곱셈 연산이 있습니다.

658
00:44:29,100 --> 00:44:35,560
입력값으로 x가 -2, y가 5,

659
00:44:35,560 --> 00:44:39,300
z가 -4일 때, 실제로

660
00:44:39,300 --> 00:44:41,550
모든 계산을

661
00:44:41,550 --> 00:44:47,310
수행해서 신경망에서 순전파를

662
00:44:47,310 --> 00:44:50,350
할 수 있습니다.

663
00:44:50,350 --> 00:44:56,670
첫 번째 단계는 x와 y를 더하는 것으로, 결과는 3입니다.

664
00:44:56,670 --> 00:44:59,970
단계별로 이해하기 쉽도록

665
00:44:59,970 --> 00:45:02,950
이름을 붙이겠습니다.

666
00:45:02,950 --> 00:45:06,850
그래서 q는 x 더하기 y입니다.

667
00:45:06,850 --> 00:45:12,150
q를 x와 y에 대해 편미분하려면,

668
00:45:12,150 --> 00:45:14,550
q와 x,

669
00:45:14,550 --> 00:45:20,730
y 사이의 식이 있으니 매우

670
00:45:20,730 --> 00:45:22,330
간단합니다.

671
00:45:22,330 --> 00:45:24,220
공식은 나와 있습니다.

672
00:45:24,220 --> 00:45:30,270
도함수, 즉 q를 x로 편미분한 값이 1이고 q를 y로

673
00:45:30,270 --> 00:45:33,160
편미분한 값도 1입니다.

674
00:45:33,160 --> 00:45:35,830
그래서 이것은 간단한 설정입니다.

675
00:45:35,830 --> 00:45:37,480
존재한다는 것을 알고 있습니다.

676
00:45:37,480 --> 00:45:40,000
그러니 그냥 머릿속에 기억해 두세요.

677
00:45:40,000 --> 00:45:46,820
그 다음 두 번째 연산은 f가 q에 z를 곱한 것입니다.

678
00:45:46,820 --> 00:45:48,980
다시 말해, 이

679
00:45:48,980 --> 00:45:54,490
함수가 있으니 편미분을 쓰는 것이 매우 쉽습니다.

680
00:45:54,490 --> 00:45:56,710
f를 q로 편미분한 값은 z입니다.

681
00:45:56,710 --> 00:46:01,490
그리고 f를 z로 미분하면 q입니다.

682
00:46:01,490 --> 00:46:04,305
즉, z와 q가 서로 바뀌는 거죠.

683
00:46:07,900 --> 00:46:09,640
여기 나오는 것들은 모두 선형대수에서

684
00:46:09,640 --> 00:46:11,600
다들 알고 계시리라 기대합니다.

685
00:46:11,600 --> 00:46:14,830
만약 모른다면 꼭 복습해

686
00:46:14,830 --> 00:46:18,490
보시길 권합니다. 왜냐하면 이

687
00:46:18,490 --> 00:46:23,860
내용들은 이번 학기 내내 매우 중요한 대수

688
00:46:23,860 --> 00:46:26,930
개념들이기 때문입니다.

689
00:46:26,930 --> 00:46:30,860
우리가 이 설정에서 원하고 필요한

690
00:46:30,860 --> 00:46:34,070
것은, 그리고 역전파 예제를

691
00:46:34,070 --> 00:46:37,930
완성하기 위해서는, f를 x, y,

692
00:46:37,930 --> 00:46:41,753
z로 편미분한 값들이 필요합니다.

693
00:46:44,400 --> 00:46:47,730
어떻게 시작하고 역전파가 이를 구현하는지

694
00:46:47,730 --> 00:46:50,580
설명하자면, 네트워크의 앞부분, 즉

695
00:46:50,580 --> 00:46:52,480
끝부분에서 시작합니다.

696
00:46:52,480 --> 00:46:58,290
그리고 거기서부터 역방향으로, 모든 그래디언트를

697
00:46:58,290 --> 00:47:00,520
역전파해 나갑니다.

698
00:47:00,520 --> 00:47:04,920
이 과정은 기본적으로 재귀적으로 계속

699
00:47:04,920 --> 00:47:07,200
실행되는 과정입니다.

700
00:47:07,200 --> 00:47:15,450
f를 f로 미분하면 무엇일까요?

701
00:47:15,450 --> 00:47:17,650
자기 자신에 대한 미분이니까요.

702
00:47:17,650 --> 00:47:23,100
항상 마지막 부분, 즉 손실 함수(loss

703
00:47:23,100 --> 00:47:28,270
function)를 자기 자신으로 미분하면 항상 1입니다.

704
00:47:28,270 --> 00:47:36,070
역전파를 하려면 가장 먼저 고려할 것은 z입니다.

705
00:47:36,070 --> 00:47:39,720
여기서 z가 보이시죠.

706
00:47:39,720 --> 00:47:44,510
이 경우, f를 z로 미분한 값을

707
00:47:44,510 --> 00:47:47,540
계산하면 이미 알고 있습니다.

708
00:47:47,540 --> 00:47:50,730
f를 z로 미분한 값은 q와 같습니다.

709
00:47:50,730 --> 00:48:00,020
따라서 q의 값이 이 부분의 그래디언트로

710
00:48:00,020 --> 00:48:01,400
전달됩니다.

711
00:48:01,400 --> 00:48:03,880
다음은 q입니다.

712
00:48:03,880 --> 00:48:08,700
Q는 다음 변수로, f와 직접 연결되어 있습니다.

713
00:48:08,700 --> 00:48:12,980
이것도 계산하기 쉽습니다. 왜냐하면 f를 q로 미분한 값을

714
00:48:12,980 --> 00:48:14,310
알고 있기 때문입니다.

715
00:48:14,310 --> 00:48:18,620
이미 계산한 결과, 그 값은 z와 같습니다.

716
00:48:18,620 --> 00:48:23,870
z가 무엇이든, 여기서 미분값은 -4입니다.

717
00:48:23,870 --> 00:48:29,570
다음은 q 바로 앞에 있는 y입니다.

718
00:48:29,570 --> 00:48:33,650
y와 f는 직접 연결되어 있지

719
00:48:33,650 --> 00:48:37,170
않지만, f를 y로

720
00:48:37,170 --> 00:48:39,220
미분해야 합니다.

721
00:48:39,220 --> 00:48:44,640
이때 체인 룰을 사용합니다.

722
00:48:44,640 --> 00:48:51,840
중간 변수에 대한 미분을 나누어

723
00:48:51,840 --> 00:48:53,950
계산하는 거죠.

724
00:48:53,950 --> 00:49:01,510
따라서 ∂f/∂y = ∂f/∂q × ∂q/∂y입니다.

725
00:49:01,510 --> 00:49:09,580
이것이 이 경우 체인 룰을 쓰는 방법입니다.

726
00:49:09,580 --> 00:49:13,890
이제 두 가지 중요한 용어를 소개하겠습니다. 로컬

727
00:49:13,890 --> 00:49:16,660
그래디언트와 업스트림 그래디언트입니다.

728
00:49:16,660 --> 00:49:18,930
업스트림

729
00:49:18,930 --> 00:49:27,930
그래디언트는 네트워크 끝에서 현재 노드로 오는 그래디언트입니다.

730
00:49:27,930 --> 00:49:29,140
그래디언트는 네트워크 끝에서 현재 노드로 오는 그래디언트입니다.

731
00:49:29,140 --> 00:49:35,270
로컬 그래디언트는 노드의 입력에

732
00:49:35,270 --> 00:49:39,380
대한 출력의 그래디언트,

733
00:49:39,380 --> 00:49:45,110
즉 노드 자체의 그래디언트입니다.

734
00:49:45,110 --> 00:49:46,200
즉 노드 자체의 그래디언트입니다.

735
00:49:46,200 --> 00:49:48,240
즉, 로컬 그래디언트입니다.

736
00:49:48,240 --> 00:49:53,420
이 정의는 어렵지 않습니다. f를 q로 미분한 값은 이미

737
00:49:53,420 --> 00:49:55,010
알고 있습니다.

738
00:49:55,010 --> 00:49:59,310
q를 y로 미분한 값도 이미 알고 있습니다.

739
00:49:59,310 --> 00:50:06,900
따라서 1 곱하기 z이고, 값은 -4가 됩니다.

740
00:50:06,900 --> 00:50:13,860
같은 이야기이고, 다른 변수 x에 대해서도 마찬가지입니다.

741
00:50:13,860 --> 00:50:18,500
여기서도 로컬 그래디언트와 업스트림

742
00:50:18,500 --> 00:50:22,830
그래디언트는 체인 룰로 표현할 수 있습니다.

743
00:50:22,830 --> 00:50:29,810
결과는 역시 -4이고, x나

744
00:50:29,810 --> 00:50:36,400
y에 대한 그래디언트가 모두

745
00:50:36,400 --> 00:50:38,390
1이었으므로,

746
00:50:38,390 --> 00:50:41,050
두 경우 모두 같은 값을 갖습니다.

747
00:50:41,050 --> 00:50:45,380
이 계산 구조와 계산 그래프 덕분에 우리가

748
00:50:45,380 --> 00:50:50,020
하려는 작업을 모듈화하기가 매우 쉬워집니다.

749
00:50:50,020 --> 00:50:54,790
신경망의 모든 노드마다, 입력으로

750
00:50:54,790 --> 00:50:58,670
x와 y 또는 다른 변수를

751
00:50:58,670 --> 00:51:06,070
받고, 출력으로 z를 낼 때, 필요한 것은 첫째, 로컬

752
00:51:06,070 --> 00:51:08,410
그래디언트입니다.

753
00:51:08,410 --> 00:51:11,180
우리는 항상 함수 f를 알고 있습니다.

754
00:51:11,180 --> 00:51:13,520
f는 x와 y의 함수입니다.

755
00:51:13,520 --> 00:51:19,520
따라서 출력에 대한 각 입력의 그래디언트는 모든 노드에서

756
00:51:19,520 --> 00:51:22,730
쉽게 계산할 수 있습니다.

757
00:51:22,730 --> 00:51:27,220
그리고 역전파를 위해 필요한 것은

758
00:51:27,220 --> 00:51:30,280
업스트림 그래디언트입니다.

759
00:51:30,280 --> 00:51:33,450
역전파 과정은 이 업스트림

760
00:51:33,450 --> 00:51:37,210
그래디언트를 단계별로 얻을 수 있게 해줍니다.

761
00:51:37,210 --> 00:51:39,750
즉, 현재 노드에 있을 때,

762
00:51:39,750 --> 00:51:41,880
미래 노드에서 계산된

763
00:51:41,880 --> 00:51:45,700
업스트림 그래디언트를 이미 가지고 있습니다.

764
00:51:45,700 --> 00:51:51,630
이것이 우리가 필요한 것입니다.

765
00:51:51,630 --> 00:51:54,330
그 후에는 업스트림

766
00:51:54,330 --> 00:51:58,650
그래디언트에 로컬 그래디언트를 곱해서, 이제

767
00:51:58,650 --> 00:52:03,520
다운스트림 그래디언트라고 부르는 값을 만듭니다.

768
00:52:03,520 --> 00:52:05,250
다운스트림 그래디언트는

769
00:52:05,250 --> 00:52:08,530
이전 층의 업스트림 그래디언트가 됩니다.

770
00:52:08,530 --> 00:52:11,650
그래서 x에 대해 계산하는 방법도

771
00:52:11,650 --> 00:52:15,660
똑같고, y에 대해서도 마찬가지입니다.

772
00:52:15,660 --> 00:52:19,710
이 전체 과정은 모든 것을 완전히 국소적으로

773
00:52:19,710 --> 00:52:23,250
계산하고, 한 단계씩 거꾸로 돌아가

774
00:52:23,250 --> 00:52:26,340
이전 노드에 전달하여 그들이

775
00:52:26,340 --> 00:52:28,460
계속 과정을 진행할 수

776
00:52:28,460 --> 00:52:30,840
있게 하는 힘을 줍니다.

777
00:52:30,840 --> 00:52:36,770
다시 말해, 이것은 모든 신경망과 여러 층의

778
00:52:36,770 --> 00:52:41,570
정보를 포함하는 많은 최적화

779
00:52:41,570 --> 00:52:47,000
과정에서 가장 기본적인 연산 중 하나입니다.

780
00:52:47,000 --> 00:52:50,040
질문을 제대로 이해했다면, 직관적으로 그라디언트가

781
00:52:50,040 --> 00:52:51,740
무엇을 하는지 어떻게 이해할

782
00:52:51,740 --> 00:52:53,070
수 있느냐는 거죠?

783
00:52:53,070 --> 00:52:58,010
한 걸음 물러서서 우리가 왜 여기까지 왔는지부터

784
00:52:58,010 --> 00:53:00,270
살펴보겠습니다.

785
00:53:00,270 --> 00:53:03,650
우리가 필요했던 것은 W1,

786
00:53:03,650 --> 00:53:06,050
W2 그리고 일반적인

787
00:53:06,050 --> 00:53:09,690
W들에 대한 손실 함수의 그라디언트를

788
00:53:09,690 --> 00:53:15,920
계산하는 것이었고, 그라디언트의 반대 방향으로 한 걸음

789
00:53:15,920 --> 00:53:17,960
나아가 최적의 값,

790
00:53:17,960 --> 00:53:21,750
최적 손실을 찾기 위해서였습니다.

791
00:53:21,750 --> 00:53:26,980
그래서 이를 위해서는 손실 L에 대한 모든 변수의

792
00:53:26,980 --> 00:53:28,850
그라디언트가 필요합니다.

793
00:53:28,850 --> 00:53:33,220
우리가 하는 것은 네트워크 내 모든 변수에 대한

794
00:53:33,220 --> 00:53:35,720
손실 L의 그라디언트를 네트워크의

795
00:53:35,720 --> 00:53:39,380
모든 값으로 되돌려 보내는 것이고, 전체

796
00:53:39,380 --> 00:53:42,010
네트워크 함수를 일일이 작성하지는

797
00:53:42,010 --> 00:53:43,190
않습니다.

798
00:53:43,190 --> 00:53:46,698
만약 네트워크가 100층이라면, 100층

799
00:53:46,698 --> 00:53:49,240
각각의 함수를 따로

800
00:53:49,240 --> 00:53:51,020
작성하지는 않을 겁니다.

801
00:53:51,020 --> 00:53:53,590
이것이 바로 우리가

802
00:53:53,590 --> 00:53:57,130
최적화 과정에 필요한 각 가중치

803
00:53:57,130 --> 00:54:00,190
값을 얻기 위해 단계별로

804
00:54:00,190 --> 00:54:02,380
역전파하는 방법입니다.

805
00:54:02,380 --> 00:54:11,320
좋습니다, 또 다른 예를 들어보죠. 이 함수는 가중치와

806
00:54:11,320 --> 00:54:15,860
x의 함수로 좀 더 복잡합니다.

807
00:54:15,860 --> 00:54:20,440
그리고 1 더하기 e의 x와 w의 선형 결합에

808
00:54:20,440 --> 00:54:23,850
대한 지수 함수의 역수입니다.

809
00:54:23,850 --> 00:54:30,460
곱셈, 덧셈, 부정, 지수 함수가

810
00:54:30,460 --> 00:54:36,180
여러 개 있고, 결국 계산된 함수의

811
00:54:36,180 --> 00:54:40,510
역수인 1이 있습니다.

812
00:54:40,510 --> 00:54:47,070
이 모든 것을 가지고, W0, x0, W1, x1, W2에

813
00:54:47,070 --> 00:54:52,840
대한 구체적인 값이 주어진 이 예제를 봅시다.

814
00:54:52,840 --> 00:54:56,980
주어진 값들로 순전파를 수행해 이

815
00:54:56,980 --> 00:55:02,050
과정에서 모든 값을 계산할 수 있습니다.

816
00:55:02,050 --> 00:55:05,940
그리고 다시 말씀드리자면,

817
00:55:05,940 --> 00:55:08,220
지수 함수

818
00:55:08,220 --> 00:55:13,840
e의 x에 대한 도함수가 무엇인지,

819
00:55:13,840 --> 00:55:16,710
상수 곱셈에 대해서도

820
00:55:16,710 --> 00:55:19,570
알고 있습니다.

821
00:55:19,570 --> 00:55:21,990
항상 도함수는 상수

822
00:55:21,990 --> 00:55:27,500
값 자체인 1/x의 도함수인 -1/x²입니다.

823
00:55:27,500 --> 00:55:30,726
이것들은 다시 말해 우리가 대수학에서 아는 내용입니다.

824
00:55:30,726 --> 00:55:34,130
그리고 상수 덧셈이라면 도함수는

825
00:55:34,130 --> 00:55:36,900
항상 1과 같습니다.

826
00:55:36,900 --> 00:55:41,330
그래서 말씀드렸듯이, 네트워크

827
00:55:41,330 --> 00:55:45,320
맨 끝에서 L에 대한 L의

828
00:55:45,320 --> 00:55:48,960
도함수는 항상 1입니다.

829
00:55:48,960 --> 00:55:54,050
그래서 여기서부터 1/x 함수의 도함수 규칙을

830
00:55:54,050 --> 00:55:56,850
사용하기 시작합니다.

831
00:55:56,850 --> 00:55:59,630
이제 계산할 수 있습니다—위쪽으로,

832
00:55:59,630 --> 00:56:02,330
끝에서는 항상 1이라고 했죠.

833
00:56:02,330 --> 00:56:07,470
국소 기울기는 -1/x²일 수 있습니다.

834
00:56:07,470 --> 00:56:09,170
x의 값은 무엇일까요?

835
00:56:09,170 --> 00:56:10,440
입력값이 무엇이든 상관없습니다.

836
00:56:10,440 --> 00:56:16,650
그래서 이 계산 결과는 -0.53입니다.

837
00:56:16,650 --> 00:56:20,370
-0.53은 하류 기울기로, 다음 단계의

838
00:56:20,370 --> 00:56:22,960
상류 기울기를 정의합니다.

839
00:56:22,960 --> 00:56:25,570
그리고 다음 단계에서

840
00:56:25,570 --> 00:56:28,160
함수는 단순한 상수 덧셈으로,

841
00:56:28,160 --> 00:56:32,780
국소 기울기가 1임을 알고 있습니다.

842
00:56:32,780 --> 00:56:39,010
그래서 1에 상류 기울기를 곱하면 같은 값이 다시 전달됩니다.

843
00:56:39,010 --> 00:56:43,820
다음 단계는 exp 함수입니다.

844
00:56:43,820 --> 00:56:48,230
그것에 대해, 상류 기울기는 이미

845
00:56:48,230 --> 00:56:49,750
알고 있습니다.

846
00:56:49,750 --> 00:56:54,770
국소 기울기는 e의 x 제곱입니다.

847
00:56:54,770 --> 00:56:55,390
x가 무엇인가요?

848
00:56:55,390 --> 00:56:59,390
이 단계의 입력에서 1을 뺀 값입니다.

849
00:56:59,390 --> 00:57:03,860
그래서 이것을 계산하면 -0.2가 나옵니다.

850
00:57:03,860 --> 00:57:08,180
그리고 이것은 다음 단계로 되돌아갑니다.

851
00:57:08,180 --> 00:57:11,950
여기서는 다시 상수와 곱셈을

852
00:57:11,950 --> 00:57:16,030
하는데, 이 상수가 국소

853
00:57:16,030 --> 00:57:21,220
기울기를 정의하며, 새로운 기울기,

854
00:57:21,220 --> 00:57:26,290
즉 하류 기울기를 정의합니다.

855
00:57:26,290 --> 00:57:30,510
그리고 다시 돌아가서,

856
00:57:30,510 --> 00:57:37,350
여기서는 덧셈 함수가 있는데, 서로

857
00:57:37,350 --> 00:57:42,610
다른 두 입력 값을 받습니다.

858
00:57:42,610 --> 00:57:45,660
그리고 만약 상류 기울기를 계산하고 싶다면, 0.

859
00:57:45,660 --> 00:57:46,320
2입니다.

860
00:57:46,320 --> 00:57:47,490
이미 가지고 있죠.

861
00:57:47,490 --> 00:57:54,720
하류와 국소 기울기는 덧셈이기 때문에

862
00:57:54,720 --> 00:57:57,240
1과 같습니다.

863
00:57:57,240 --> 00:58:00,180
x와 y의 합에

864
00:58:00,180 --> 00:58:04,660
대한 도함수는 항상 1입니다.

865
00:58:04,660 --> 00:58:06,960
그래서 두 입력 모두 동일합니다.

866
00:58:06,960 --> 00:58:07,920
그

867
00:58:07,920 --> 00:58:11,070
다음 곱셈 연산과 상류

868
00:58:11,070 --> 00:58:14,470
기울기가 있습니다.

869
00:58:14,470 --> 00:58:15,930
다시 값들이 있죠.

870
00:58:15,930 --> 00:58:21,500
곱셈에 대한 국소 기울기는, 예를

871
00:58:21,500 --> 00:58:24,870
들어 a 곱하기 x라면,

872
00:58:24,870 --> 00:58:28,880
x에 대한 도함수는 항상

873
00:58:28,880 --> 00:58:31,850
다른 변수입니다.

874
00:58:31,850 --> 00:58:37,880
여기 첫 번째는 x 값인

875
00:58:37,880 --> 00:58:39,240
-1이고,

876
00:58:39,240 --> 00:58:42,720
두 번째는 w 값인 2입니다.

877
00:58:42,720 --> 00:58:46,140
즉, 다른 변수의 값이 무엇이든 상관없습니다.

878
00:58:46,140 --> 00:58:50,000
이렇게 해서 모든 것을 계산할 수 있고

879
00:58:50,000 --> 00:58:54,060
W1과 x1에 대한 것도 계산할 수 있습니다.

880
00:58:54,060 --> 00:58:56,820
다시 말해, 이 모든

881
00:58:56,820 --> 00:59:01,970
계산을 통해 네트워크에서 최적점으로

882
00:59:01,970 --> 00:59:08,810
나아가기 위해 W를 얼마나 변경해야 하는지 알

883
00:59:08,810 --> 00:59:10,790
수 있습니다.

884
00:59:10,790 --> 00:59:13,350
이것이 또 다른 예시였습니다.

885
00:59:13,350 --> 00:59:19,490
계산 그래프를 그리는 방법은 매우 다양합니다.

886
00:59:19,490 --> 00:59:21,590
제가 설명한 것이 유일한 방법은 아닙니다.

887
00:59:21,590 --> 00:59:24,400
사실 모든 함수를 하나로 묶어

888
00:59:24,400 --> 00:59:28,510
시그모이드로 정의할 수도 있는데, 기본적으로 이것은

889
00:59:28,510 --> 00:59:31,310
선형 함수의 시그모이드입니다.

890
00:59:31,310 --> 00:59:33,770
선형 함수는 여기 있을 수 있고,

891
00:59:33,770 --> 00:59:38,150
모든 연산은 시그모이드로 정의될 수 있습니다.

892
00:59:38,150 --> 00:59:41,080
사실 시그모이드는 흥미롭고

893
00:59:41,080 --> 00:59:46,330
매우 유용한데, 국소 기울기가 시그모이드 자체에

894
00:59:46,330 --> 00:59:48,530
의존하기 때문입니다.

895
00:59:48,530 --> 00:59:53,750
시그모이드의 국소 기울기, 즉 변수 x에 대한 기울기는

896
00:59:53,750 --> 00:59:56,600
계산 후 정리하면 1에서

897
00:59:56,600 --> 01:00:01,900
시그모이드 값을 뺀 것에 시그모이드 값을 곱한 것입니다.

898
01:00:01,900 --> 01:00:10,030
그래서 매우 유용하고 간단한 함수입니다.

899
01:00:10,030 --> 01:00:10,910
그래서 매우 유용하고 간단한 함수입니다.

900
01:00:10,910 --> 01:00:18,040
하류 기울기를 계산하기 위해, 상류

901
01:00:18,040 --> 01:00:21,100
기울기는 1이었고,

902
01:00:21,100 --> 01:00:24,270
국소 기울기를

903
01:00:24,270 --> 01:00:28,240
계산하면, 입력값 1을 대입한

904
01:00:28,240 --> 01:00:34,050
이 함수가 1에 곱해져 0.2가

905
01:00:34,050 --> 01:00:35,290
됩니다.

906
01:00:35,290 --> 01:00:37,170
이는 이전에

907
01:00:37,170 --> 01:00:42,150
따로 계산한 값과 정확히 같습니다.

908
01:00:42,150 --> 01:00:47,490
요약하자면, 데이터에는 몇 가지 패턴이

909
01:00:47,490 --> 01:00:55,980
있고, 노드에 대해 자주 볼 수 있는 패턴들이 있어 실제로

910
01:00:55,980 --> 01:00:59,970
암기할 수 있다는 것입니다.

911
01:00:59,970 --> 01:01:02,590
덧셈 게이트에는 덧셈 게이트가 있습니다.

912
01:01:02,590 --> 01:01:05,850
항상 그래디언트 분배기 역할을 합니다.

913
01:01:05,850 --> 01:01:10,770
제가 설명한 덧셈의 특성 때문에,

914
01:01:10,770 --> 01:01:18,710
그래디언트는 입력값과 동일하게 유지됩니다.

915
01:01:18,710 --> 01:01:23,850
곱셈 게이트는 스왑 함수입니다.

916
01:01:23,850 --> 01:01:27,890
다시 말씀드리지만, xy의 x에 대한 그래디언트는 y이고,

917
01:01:27,890 --> 01:01:30,090
y에 대한 그래디언트는 x입니다.

918
01:01:30,090 --> 01:01:31,710
그래서 스왑인 거죠.

919
01:01:31,710 --> 01:01:37,040
그리고 복사 게이트가 있습니다.

920
01:01:37,040 --> 01:01:41,000
복사 게이트에서 일어나는 연산은

921
01:01:41,000 --> 01:01:47,390
네트워크, 즉 노드나 게이트에 들어오는 값을 단순히

922
01:01:47,390 --> 01:01:49,980
더하는 것입니다.

923
01:01:49,980 --> 01:01:53,600
마지막으로, max 게이트가 있는데, 이는

924
01:01:53,600 --> 01:01:55,910
ReLU 함수와 매우

925
01:01:55,910 --> 01:01:58,010
비슷해서 자주 사용합니다.

926
01:01:58,010 --> 01:02:02,030
max 게이트는 입력값 중 최대값을 취하기

927
01:02:02,030 --> 01:02:06,390
때문에 그래디언트도 그에 따라 결정됩니다.

928
01:02:06,390 --> 01:02:11,080
그래서 최대값이었던 쪽으로

929
01:02:11,080 --> 01:02:15,880
그래디언트를 전달하면 됩니다.

930
01:02:15,880 --> 01:02:21,520
이렇게 하면 신경망의 순전파를 구현하고 모든 단계를

931
01:02:21,520 --> 01:02:25,880
계산하는 것이 매우 간단해집니다.

932
01:02:25,880 --> 01:02:27,610
그리고 역전파에서는

933
01:02:27,610 --> 01:02:29,480
그래디언트를 계산하기 시작합니다.

934
01:02:29,480 --> 01:02:32,110
단계별로 설명했듯이,

935
01:02:32,110 --> 01:02:38,120
손실 함수의 자기 자신에 대한 그래디언트는 항상 1입니다.

936
01:02:38,120 --> 01:02:43,190
그리고 네트워크의 끝에서부터 위로 올라갑니다.

937
01:02:43,190 --> 01:02:45,410
여기서 보시다시피, 위로 올라가고 있습니다.

938
01:02:45,410 --> 01:02:49,360
이것이 시그모이드 함수가 그래디언트를 계산하고 위로

939
01:02:49,360 --> 01:02:50,900
올라가는 과정입니다.

940
01:02:50,900 --> 01:02:52,760
저것이 add 게이트였습니다.

941
01:02:52,760 --> 01:02:54,500
우리는 또 다른 add 게이트가 있었습니다.

942
01:02:54,500 --> 01:02:58,190
그리고 두 개의 multiply 게이트가

943
01:02:58,190 --> 01:03:03,710
있었는데, 이것들은 기본적으로 아주 간단하게 구현을 제공합니다.

944
01:03:03,710 --> 01:03:07,390
이런 형태의 공식화로

945
01:03:07,390 --> 01:03:12,960
우리는 신경망의 모든 함수를

946
01:03:12,960 --> 01:03:20,400
모듈화하고, 신경망에서 필요한 각 함수에 대해

947
01:03:20,400 --> 01:03:23,280
순전파와 역전파

948
01:03:23,280 --> 01:03:25,350
API를 만들

949
01:03:25,350 --> 01:03:27,640
수 있습니다.

950
01:03:27,640 --> 01:03:31,710
이 경우, 곱셈 게이트인데,

951
01:03:31,710 --> 01:03:33,810
곱셈의 경우

952
01:03:33,810 --> 01:03:39,190
역전파에서 사용할 입력값에 접근해야 합니다.

953
01:03:39,190 --> 01:03:42,750
우리는 종종 그것들을 저장하고 기억한

954
01:03:42,750 --> 01:03:45,480
다음, 순전파 값을 계산하고

955
01:03:45,480 --> 01:03:48,600
역전파에서 그래디언트를 계산합니다.

956
01:03:48,600 --> 01:03:53,850
즉, 함수들을 작성하고 순전파와 역전파를 모두

957
01:03:53,850 --> 01:03:56,350
포함시킬 수 있다는 뜻입니다.

958
01:03:56,350 --> 01:04:00,580
이것이 현재 PyTorch 연산자들이 어떻게 생겼는지 보여줍니다.

959
01:04:00,580 --> 01:04:02,260
예를 들어 시그모이드 레이어를

960
01:04:02,260 --> 01:04:04,120
보면, 순전파만 있습니다.

961
01:04:04,120 --> 01:04:08,070
이 함수 안에서는 구현되어 있지 않지만,

962
01:04:08,070 --> 01:04:11,630
실제로는 C++ 코드, C 코드 어딘가에

963
01:04:11,630 --> 01:04:14,520
PyTorch에서 구현되어 있습니다.

964
01:04:14,520 --> 01:04:16,940
그리고 시그모이드의 역전파도

965
01:04:16,940 --> 01:04:18,950
우리가 방금 이야기한

966
01:04:18,950 --> 01:04:20,995
같은 함수를 계산합니다.

967
01:04:27,500 --> 01:04:31,140
지금까지 말한

968
01:04:31,140 --> 01:04:33,530
것들은—사실 스칼라

969
01:04:33,530 --> 01:04:40,260
값을 사용한 예제 대부분을 다뤘습니다.

970
01:04:40,260 --> 01:04:43,560
모든 예제가 스칼라 값이었지만,

971
01:04:43,560 --> 01:04:48,410
이 모든 연산은 실제로 벡터나 행렬

972
01:04:48,410 --> 01:04:52,500
형태로도 구현할 수 있다는 걸

973
01:04:52,500 --> 01:04:58,700
알고 있습니다, 여기서 그 부분을 확장하는 겁니다.

974
01:04:58,700 --> 01:05:03,080
우리는 지금까지 스칼라에서 스칼라로 가는 설정에 대해

975
01:05:03,080 --> 01:05:07,090
이야기했습니다. 즉, 입력 x와 출력 y가 모두 스칼라인

976
01:05:07,090 --> 01:05:08,240
경우입니다.

977
01:05:08,240 --> 01:05:13,070
따라서 미분값도 스칼라가 됩니다.

978
01:05:13,070 --> 01:05:17,830
즉, x를 조금 바꾸면 y의 값이 얼마나

979
01:05:17,830 --> 01:05:20,890
변하는지를 나타냅니다.

980
01:05:20,890 --> 01:05:24,070
이제 벡터화되어 x가

981
01:05:24,070 --> 01:05:28,360
n개의 원소를 가진 벡터이고 y가

982
01:05:28,360 --> 01:05:33,770
스칼라인 경우, 벡터에서 스칼라로 가는

983
01:05:33,770 --> 01:05:37,060
미분에서는 미분값이 벡터가

984
01:05:37,060 --> 01:05:38,270
됩니다.

985
01:05:38,270 --> 01:05:42,760
그 벡터의 각 원소는 x의 각 원소를

986
01:05:42,760 --> 01:05:48,440
조금 바꿨을 때 y가 얼마나 변하는지를

987
01:05:48,440 --> 01:05:51,740
의미합니다. y는 하나의

988
01:05:51,740 --> 01:05:53,770
값이니까 전체

989
01:05:53,770 --> 01:05:55,160
변화량이죠.

990
01:05:55,160 --> 01:05:59,380
그리고 x와 y가 모두 임의 크기의

991
01:05:59,380 --> 01:06:05,430
벡터인 벡터 대 벡터 프레임워크도 있습니다.

992
01:06:05,430 --> 01:06:09,300
그 경우 미분값은 행렬, 즉

993
01:06:09,300 --> 01:06:14,130
Jacobian이라고 부르는 행렬이 됩니다.

994
01:06:14,130 --> 01:06:20,250
x의 각 원소가 조금 변할 때 y의

995
01:06:20,250 --> 01:06:23,520
각 원소가 얼마나

996
01:06:23,520 --> 01:06:27,720
변하는지를 이 미분값이

997
01:06:27,720 --> 01:06:29,830
알려줍니다.

998
01:06:29,830 --> 01:06:36,365
여기 스크립트를 보면 완전히 같지는

999
01:06:36,365 --> 01:06:37,240
않습니다.

1000
01:06:37,240 --> 01:06:38,770
서로 다를 수도 있습니다.

1001
01:06:38,770 --> 01:06:42,360
이 Jacobian 행렬의 각 원소는

1002
01:06:42,360 --> 01:06:44,950
명확한 의미를 가집니다.

1003
01:06:44,950 --> 01:06:51,750
벡터로 역전파를 할 때, 예를 들어

1004
01:06:51,750 --> 01:06:57,190
x, y, z가 각각 dx,

1005
01:06:57,190 --> 01:07:04,790
dy, dz 크기의 벡터라고 합시다.

1006
01:07:04,790 --> 01:07:11,090
손실 함수 L은 항상 스칼라입니다.

1007
01:07:11,090 --> 01:07:13,040
우리가 최소화하려는

1008
01:07:13,040 --> 01:07:16,650
단일 값이니까요.

1009
01:07:16,650 --> 01:07:21,080
하지만 상류(upstream)

1010
01:07:21,080 --> 01:07:29,360
그래디언트를 계산하면 z와 같은 크기의 벡터 dz가 나옵니다.

1011
01:07:29,360 --> 01:07:34,940
하류(downstream) 그래디언트도 마찬가지입니다.

1012
01:07:34,940 --> 01:07:38,700
하류 그래디언트에 대해 이야기하기

1013
01:07:38,700 --> 01:07:41,000
전에, 국소 그래디언트(local

1014
01:07:41,000 --> 01:07:43,700
gradients)에

1015
01:07:43,700 --> 01:07:52,730
대해 잠깐 말씀드리겠습니다. 여기서 z의 x와 y에 대한 그래디언트가
있습니다.

1016
01:07:52,730 --> 01:07:55,160
이 경우가 바로

1017
01:07:55,160 --> 01:07:58,910
Jacobian 행렬이 되는 부분입니다.

1018
01:07:58,910 --> 01:08:02,440
이제 그래디언트가 행렬이 되죠.

1019
01:08:02,440 --> 01:08:06,240
입력 크기와 출력 크기를

1020
01:08:06,240 --> 01:08:10,260
곱한 두 개의 Jacobian

1021
01:08:10,260 --> 01:08:14,220
행렬이 정의됩니다.

1022
01:08:14,220 --> 01:08:21,540
그리고 이것이 상류 그래디언트와 국소 그래디언트의

1023
01:08:21,540 --> 01:08:27,189
곱으로 하류 그래디언트를 만듭니다.

1024
01:08:27,189 --> 01:08:33,220
그 결과 하류 그래디언트는 입력 x와 같은 크기가 됩니다.

1025
01:08:33,220 --> 01:08:37,080
입력이 벡터였으니,

1026
01:08:37,080 --> 01:08:40,020
그래디언트도

1027
01:08:40,020 --> 01:08:45,300
같은 크기의 벡터가 됩니다.

1028
01:08:45,300 --> 01:08:48,660
변수의 그래디언트는 항상 원래 변수와

1029
01:08:48,660 --> 01:08:51,990
같은 차원을 가진다는 점을 방금

1030
01:08:51,990 --> 01:08:57,180
말씀드렸고, 이 슬라이드에서도 확인할 수 있습니다.

1031
01:08:57,180 --> 01:09:07,830
벡터로 역전파하는 예시는 이렇습니다.

1032
01:09:07,830 --> 01:09:11,550
함수 하나를 예로 들어보죠. 0과 x 중 큰 값을 취하는 함수입니다.

1033
01:09:11,550 --> 01:09:14,430
이게 바로 ReLU 함수입니다.

1034
01:09:14,430 --> 01:09:18,660
입력 원소별로 0과 비교해서 큰 값을 선택하는 원소별

1035
01:09:18,660 --> 01:09:19,760
함수입니다.

1036
01:09:19,760 --> 01:09:21,090
0 이상이면

1037
01:09:21,090 --> 01:09:24,590
그대로 통과시키고, 그렇지 않으면

1038
01:09:24,590 --> 01:09:26,729
0으로 대체합니다.

1039
01:09:26,729 --> 01:09:29,819
상류 그래디언트가 있다고 가정하고,

1040
01:09:29,819 --> 01:09:34,010
이제 Jacobian 행렬을 만들어야 합니다.

1041
01:09:34,010 --> 01:09:36,479
이 Jacobian

1042
01:09:36,479 --> 01:09:40,140
행렬은 원소별 연산이기

1043
01:09:40,140 --> 01:09:45,890
때문에 다른 입력에 의존하지 않고, 각

1044
01:09:45,890 --> 01:09:48,990
원소 값에만 의존합니다.

1045
01:09:48,990 --> 01:09:52,700
매우 희소한 행렬로, 주대각선에만

1046
01:09:52,700 --> 01:09:55,050
값이 있습니다.

1047
01:09:55,050 --> 01:09:59,900
그 값들은 0 또는 1인데,

1048
01:09:59,900 --> 01:10:04,400
max가 선택되었는지,

1049
01:10:04,400 --> 01:10:07,940
아니면 0으로

1050
01:10:07,940 --> 01:10:13,930
대체되었는지에 따라 달라집니다.

1051
01:10:13,930 --> 01:10:16,900
이 행렬에 상류 그래디언트를 곱하면

1052
01:10:16,900 --> 01:10:19,700
하류 그래디언트를 얻습니다.

1053
01:10:19,700 --> 01:10:25,700
이것이 계산이 이루어지는 방식입니다.

1054
01:10:25,700 --> 01:10:31,100
앞서 말했듯이, 여기서 Jacobian은 희소합니다. 왜냐하면 이

1055
01:10:31,100 --> 01:10:34,180
경우 연산이 원소별 연산이기 때문입니다.

1056
01:10:34,180 --> 01:10:37,390
그리고 실제로 역전파 과정에서

1057
01:10:37,390 --> 01:10:40,870
그 거대한 희소 Jacobian

1058
01:10:40,870 --> 01:10:44,590
행렬을 계산하는 대신, 이 max

1059
01:10:44,590 --> 01:10:48,100
함수의 기울기를 규칙 기반으로

1060
01:10:48,100 --> 01:10:50,960
계산하는 방법을 사용합니다.

1061
01:10:50,960 --> 01:10:53,800
그래서 우리는 그 행렬을

1062
01:10:53,800 --> 01:10:56,910
저장하거나 계산하지 않습니다. 함수가

1063
01:10:56,910 --> 01:11:01,020
어떻게 작동하는지 알고 있기 때문입니다.

1064
01:11:01,020 --> 01:11:03,930
그리고 이것은 행렬이나

1065
01:11:03,930 --> 01:11:07,870
텐서에도 확장할 수 있습니다.

1066
01:11:07,870 --> 01:11:09,840
입력이 벡터가

1067
01:11:09,840 --> 01:11:13,840
아니라 고차원 데이터인 경우입니다.

1068
01:11:13,840 --> 01:11:18,750
그런 경우에도 변수에 대한

1069
01:11:18,750 --> 01:11:21,660
기울기는 해당

1070
01:11:21,660 --> 01:11:26,340
변수와 같은 크기를 갖습니다.

1071
01:11:26,340 --> 01:11:34,260
상류와 하류 행렬 및 도함수 계산은 앞서

1072
01:11:34,260 --> 01:11:39,810
벡터에 대해 논의하고 보여준

1073
01:11:39,810 --> 01:11:44,350
방식과 동일하게 수행됩니다.

1074
01:11:44,350 --> 01:11:49,890
그리고 국소 기울기의 경우, 출력과 입력이

1075
01:11:49,890 --> 01:11:53,930
모두 행렬이기 때문에 매우 큰

1076
01:11:53,930 --> 01:11:56,840
Jacobian 행렬이

1077
01:11:56,840 --> 01:11:58,320
됩니다.

1078
01:11:58,320 --> 01:12:00,080
국소 기울기는

1079
01:12:00,080 --> 01:12:06,170
입력 크기와 출력 크기의 곱과 같은

1080
01:12:06,170 --> 01:12:08,250
크기를 갖습니다.

1081
01:12:08,250 --> 01:12:11,880
즉, 그 자체로 매우 큰 행렬이 되는 거죠.

1082
01:12:11,880 --> 01:12:13,500
예를 들어 보겠습니다.

1083
01:12:13,500 --> 01:12:17,810
입력 x와 w가 행렬 곱셈을

1084
01:12:17,810 --> 01:12:24,980
하는 노드의 입력이고, 출력으로 y를 생성한다고

1085
01:12:24,980 --> 01:12:28,050
할 때, L을 y에

1086
01:12:28,050 --> 01:12:32,930
대해 미분하면 이러한 Jacobian

1087
01:12:32,930 --> 01:12:36,050
행렬이 나옵니다.

1088
01:12:36,050 --> 01:12:43,250
만약 미니 배치 크기가 64이고 행렬의 차원이 4,

1089
01:12:43,250 --> 01:12:47,645
096이라면, 이 거대한

1090
01:12:47,645 --> 01:12:52,930
Jacobian 행렬 하나가 단일 행렬

1091
01:12:52,930 --> 01:13:00,100
곱셈에 대해 256기가바이트가 넘는다는 뜻입니다.

1092
01:13:00,100 --> 01:13:07,720
이를 단순화하기 위해, 우리는 값들이 서로

1093
01:13:07,720 --> 01:13:12,160
어떻게 영향을 미치는지

1094
01:13:12,160 --> 01:13:14,240
살펴봅니다.

1095
01:13:14,240 --> 01:13:17,740
예를 들어, x의 한 원소가

1096
01:13:17,740 --> 01:13:24,770
영향을 받으면 y의 어떤 부분이 영향을 받는지 보는 거죠.

1097
01:13:24,770 --> 01:13:30,700
그래서 거기서 xn과 d, 이 특정 값은

1098
01:13:30,700 --> 01:13:35,720
보통 출력의 한 행에만 영향을 미칩니다.

1099
01:13:35,720 --> 01:13:41,560
그리고 이것은 기본적으로 각 노드를 계산하는 데

1100
01:13:41,560 --> 01:13:43,400
도움을 줍니다.

1101
01:13:43,400 --> 01:13:46,340
우리는 거대한 Jacobian을 만들 필요가 없습니다.

1102
01:13:46,340 --> 01:13:50,160
사실 행렬 곱셈에 대해 역전파

1103
01:13:50,160 --> 01:13:53,610
함수를 더 효율적으로

1104
01:13:53,610 --> 01:13:56,170
작성할 수 있습니다.

1105
01:13:56,170 --> 01:13:59,530
거의 다 했습니다.

1106
01:13:59,530 --> 01:14:01,320
이 질문에 답해 보겠습니다.

1107
01:14:01,320 --> 01:14:08,170
xn, d가 y와 m의 값에 얼마나 영향을 미치는가?

1108
01:14:08,170 --> 01:14:15,480
이것이 yn, m이고, xn, d로부터 영향을 받는 값입니다.

1109
01:14:15,480 --> 01:14:17,400
얼마나 영향을 받는가?

1110
01:14:17,400 --> 01:14:19,680
즉, 특정 값

1111
01:14:19,680 --> 01:14:22,680
xn, d에 대한

1112
01:14:22,680 --> 01:14:30,720
기울기로 무엇을 놓아야 하는가 하는 뜻입니다.

1113
01:14:30,720 --> 01:14:34,150
다시 말씀드리면, 이것은 곱셈 연산입니다.

1114
01:14:37,410 --> 01:14:41,010
곱셈 게이트에서는 교환이 있어야 합니다.

1115
01:14:41,010 --> 01:14:47,820
그래서 이 질문에 대한 답이 W의

1116
01:14:47,820 --> 01:14:49,500
값인가요?

1117
01:14:49,500 --> 01:14:51,800
곱셈 게이트가 교환 곱셈기였던

1118
01:14:51,800 --> 01:14:53,520
것을 기억하세요.

1119
01:14:53,520 --> 01:14:55,950
그래서 여기서 교환이 일어납니다.

1120
01:14:55,950 --> 01:15:07,730
따라서 y의 한 요소에 영향을 미치는 x의 값은 x 행렬의

1121
01:15:07,730 --> 01:15:13,040
d번째 행과 y 행렬의

1122
01:15:13,040 --> 01:15:17,510
m번째 열에 정의된 W에

1123
01:15:17,510 --> 01:15:19,800
의존합니다.

1124
01:15:19,800 --> 01:15:22,470
그래서 값들을 서로 바꾸는 겁니다.

1125
01:15:22,470 --> 01:15:23,910
같은 스왑입니다.

1126
01:15:23,910 --> 01:15:27,650
하지만 여기서는 거대한 행렬들을

1127
01:15:27,650 --> 01:15:31,950
보고 어떤 특정 원소를 찾아야 합니다.

1128
01:15:31,950 --> 01:15:34,580
그걸 바탕으로 전체를

1129
01:15:34,580 --> 01:15:40,940
행렬 곱셈과 행렬 연산으로 대체할 수

1130
01:15:40,940 --> 01:15:41,760
있습니다.

1131
01:15:41,760 --> 01:15:43,700
L의 x에 대한

1132
01:15:43,700 --> 01:15:47,690
그래디언트는 이 간단한 행렬 연산으로 정의됩니다.

1133
01:15:47,690 --> 01:15:51,130
그리고 L의 W에 대한 그래디언트는 이

1134
01:15:51,130 --> 01:15:54,080
매우 간단한 곱셈으로 정의됩니다.

1135
01:15:54,080 --> 01:15:58,940
다시 말해, 여기서 x에 대해서는 전체 w를 포함합니다.

1136
01:15:58,940 --> 01:16:05,420
w에 대해서는 전체 x를 포함하고 곱셈을 수행합니다.

1137
01:16:05,420 --> 01:16:09,220
이 공식들은 더 크고 복잡한

1138
01:16:09,220 --> 01:16:14,860
연산을 구현하고 역전파에 적용하기

1139
01:16:14,860 --> 01:16:17,470
쉽게 만듭니다.

1140
01:16:17,470 --> 01:16:20,020
좋습니다, 마쳤습니다.

1141
01:16:20,020 --> 01:16:22,690
오늘은 완전 연결 신경망에

1142
01:16:22,690 --> 01:16:24,860
대해 이야기했습니다.

1143
01:16:24,860 --> 01:16:30,320
역전파에 필요한 모든 단계, 순전파와 역전파

1144
01:16:30,320 --> 01:16:32,390
과정을 살펴봤습니다.

1145
01:16:32,390 --> 01:16:36,160
다음 시간에는 합성곱 신경망

1146
01:16:36,160 --> 01:16:39,820
주제로 들어가겠습니다.

1147
01:16:39,820 --> 01:16:41,820
감사합니다.
