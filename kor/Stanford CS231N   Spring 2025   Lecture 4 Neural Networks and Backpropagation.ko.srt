1
00:00:05,270 --> 00:00:07,100
이 슬라이드에서

2
00:00:07,100 --> 00:00:14,310
보시다시피, 오늘 우리는 신경망과 역전파에 대해 이야기할 것입니다.

3
00:00:14,310 --> 00:00:17,000
사실 이는 초기 시절에

4
00:00:17,000 --> 00:00:19,770
제가 공부하던 과정으로,

5
00:00:19,770 --> 00:00:26,180
신경망이 자신의 실수로부터 배우게 해주는 마법

6
00:00:26,180 --> 00:00:30,590
같은 과정이라고 자주 언급했습니다.

7
00:00:30,590 --> 00:00:35,910
인간과 비슷하지만 더 조직적인 방식으로,

8
00:00:35,910 --> 00:00:39,720
그리고 약간의 수학을 사용합니다.

9
00:00:39,720 --> 00:00:43,380
그럼 주제로 들어가 보겠습니다.

10
00:00:43,380 --> 00:00:46,440
이것이 흥미로울 것이라고 확신합니다.

11
00:00:46,440 --> 00:00:49,460
그리고 이것은 이번 분기의 나머지 부분을 위한

12
00:00:49,460 --> 00:00:51,180
기초를 다지는 것입니다.

13
00:00:51,180 --> 00:00:54,650
앞으로 논의할 모든 알고리즘은

14
00:00:54,650 --> 00:00:59,600
언급하지 않더라도 역전파의 형태를 사용하고 있습니다.

15
00:00:59,600 --> 00:01:06,850
그래서 이 강의와 주제를 이해하는 것이 매우

16
00:01:06,850 --> 00:01:08,270
중요합니다.

17
00:01:08,270 --> 00:01:12,470
좋습니다, 전통을 이어가며

18
00:01:12,470 --> 00:01:19,180
지금까지 이야기한 내용을 정리해 보겠습니다.

19
00:01:19,180 --> 00:01:27,310
이제 지난 시간에 이야기한 내용을 기억하고 계실 것이라고 확신합니다.

20
00:01:27,310 --> 00:01:34,360
우리는 목표 함수 또는 손실 함수, 우리가 여기서 부르는 것을 어떻게 형성할
수 있는지에

21
00:01:34,360 --> 00:01:36,050
대해 이야기했습니다.

22
00:01:36,050 --> 00:01:40,040
그리고 정규화에 대해서도 이야기했습니다.

23
00:01:40,040 --> 00:01:45,520
그것을 위해 우리는 모든 것을 x,

24
00:01:45,520 --> 00:01:53,180
y를 통해 공식화하고, 쌍과 점수 함수를 정의했습니다.

25
00:01:53,180 --> 00:01:57,260
이 경우 우리는 선형 점수 함수를

26
00:01:57,260 --> 00:02:02,610
사용하고 있으며, 궁극적으로 이 손실 함수를

27
00:02:02,610 --> 00:02:04,900
정의하고 있습니다.

28
00:02:04,900 --> 00:02:07,110
오른쪽에 보이는

29
00:02:07,110 --> 00:02:14,610
이 그래프는 우리가 그린 것으로, 학습의 전체 과정을

30
00:02:14,610 --> 00:02:16,240
보여줍니다.

31
00:02:16,240 --> 00:02:20,610
몇 가지 질문이 있었습니다.

32
00:02:20,610 --> 00:02:26,160
지난 강의와 그 이전에도 왜 우리가 소프트맥스 함수만

33
00:02:26,160 --> 00:02:29,980
사용하는지에 대한 질문이 있었습니다.

34
00:02:29,980 --> 00:02:34,230
이것이 우리가 가지고 있는 유일한 손실 함수가 아니며, 우리가

35
00:02:34,230 --> 00:02:36,490
사용하는 것임을 다시 강조하고 싶습니다.

36
00:02:36,490 --> 00:02:42,070
이는 딥러닝과 특히 분류 작업을 위한 구축에서 가장 널리

37
00:02:42,070 --> 00:02:44,320
사용되는 것 중 하나입니다.

38
00:02:44,320 --> 00:02:47,490
하지만 다른 작업을 위해 사용하는

39
00:02:47,490 --> 00:02:50,590
많은 다른 옵션이 있습니다.

40
00:02:50,590 --> 00:02:53,290
분류 작업의 경우, 우리가

41
00:02:53,290 --> 00:02:55,440
웹사이트에 공유한

42
00:02:55,440 --> 00:02:59,640
슬라이드를 보셨다면, 강의 2의 읽기

43
00:02:59,640 --> 00:03:05,630
과제에서 힌지 손실 또는 이전에 SVM 손실이라고 불리던

44
00:03:05,630 --> 00:03:07,800
것을 포함했습니다.

45
00:03:07,800 --> 00:03:11,690
슬라이드에는 힌지 손실 주제에

46
00:03:11,690 --> 00:03:16,770
대한 예시와 모든 것이 포함되어 있었습니다.

47
00:03:16,770 --> 00:03:20,960
이는 특히 신경망의 초기 시절에

48
00:03:20,960 --> 00:03:26,700
널리 사용된 손실 함수 중 하나입니다.

49
00:03:26,700 --> 00:03:31,460
그것이 무엇인지에 대한 높은 수준의 이해를

50
00:03:31,460 --> 00:03:38,900
제공하기 위해, 이는 소프트맥스와 달리 점수를 확률로 변환하지

51
00:03:38,900 --> 00:03:41,250
않는 손실 함수입니다.

52
00:03:41,250 --> 00:03:42,980
따라서 점수를 확률로 변환하는

53
00:03:42,980 --> 00:03:44,490
것이 유일한 옵션은 아닙니다.

54
00:03:44,490 --> 00:03:48,720
다른 형태를 사용할 수 있습니다.

55
00:03:48,720 --> 00:03:53,090
이 함수는 Syi로

56
00:03:53,090 --> 00:03:54,820
정의된 올바른

57
00:03:54,820 --> 00:04:01,360
항목의 점수가 모든 다른

58
00:04:01,360 --> 00:04:06,430
항목 Sj의 점수보다 높도록

59
00:04:06,430 --> 00:04:08,900
장려합니다.

60
00:04:08,900 --> 00:04:16,329
여기 조건을 보실 수 있으며, 조건이 참일 경우 0의

61
00:04:16,329 --> 00:04:18,519
값을 생성합니다.

62
00:04:18,519 --> 00:04:22,990
그렇지 않으면, 제가 말한 대로,

63
00:04:22,990 --> 00:04:28,720
올바른 항목의 점수가 모든 다른 항목의 점수보다

64
00:04:28,720 --> 00:04:32,170
최소한의 여유를 두고 높아지도록

65
00:04:32,170 --> 00:04:34,220
장려합니다.

66
00:04:34,220 --> 00:04:35,890
여기 보이는 숫자

67
00:04:35,890 --> 00:04:38,510
1은 생성된 여유입니다.

68
00:04:38,510 --> 00:04:41,690
그리고 조건이 위반되면

69
00:04:41,690 --> 00:04:47,000
손실은 여유에서 비례적으로 증가합니다.

70
00:04:47,000 --> 00:04:48,520
그리고

71
00:04:48,520 --> 00:04:56,440
이것은 함수의 시각화입니다.

72
00:04:56,440 --> 00:05:02,130
따라서 이는 관련 없는 항목이 너무 높은 점수를 받을 경우

73
00:05:02,130 --> 00:05:05,920
벌점을 부여하여 올바른 점수를 촉진합니다.

74
00:05:05,920 --> 00:05:10,620
다시 말해, 예제와 더 나은 이해를

75
00:05:10,620 --> 00:05:15,630
위해 강의 2의 읽기 과제를 참조하세요.

76
00:05:15,630 --> 00:05:20,580
다음으로, 우리는 일반 최적화에 대해

77
00:05:20,580 --> 00:05:28,750
이야기했으며, 신경망의 최적 매개변수 W를 찾는 방법에 대해 논의했습니다.

78
00:05:28,750 --> 00:05:32,160
그 과정에서 우리는 이

79
00:05:32,160 --> 00:05:39,390
이미지에 나타난 것처럼 손실 경관이 큰 계곡과 같다는 것에 대해

80
00:05:39,390 --> 00:05:42,190
조금 이야기했습니다.

81
00:05:42,190 --> 00:05:46,470
그 계곡의 모든 점은 서로 다른 가중치

82
00:05:46,470 --> 00:05:48,990
매개변수 집합입니다.

83
00:05:48,990 --> 00:05:52,850
우리는 그 손실 경관을 최소화하는

84
00:05:52,850 --> 00:05:56,210
매개변수 집합 W를 찾고자 했습니다.

85
00:05:56,210 --> 00:05:58,790
우리는 손실 함수

86
00:05:58,790 --> 00:06:03,080
L의 기울기를 W에 대해 취하고,

87
00:06:03,080 --> 00:06:07,340
이를 단계별로 최적화에 사용하는 것이

88
00:06:07,340 --> 00:06:12,480
핵심이라는 사실에 대해 이야기했습니다.

89
00:06:12,480 --> 00:06:16,590
이로 인해 우리는 경량 하강 알고리즘을 얻었습니다.

90
00:06:16,590 --> 00:06:22,020
따라서 가중치는 기본적으로 업데이트됩니다.

91
00:06:22,020 --> 00:06:25,730
이 거리에서 내가 가리키고 있는

92
00:06:25,730 --> 00:06:32,210
것을 보는 것은 매우 어렵지만, 추측할 수 있습니다.

93
00:06:32,210 --> 00:06:38,030
어쨌든 손실 경관을 최소값 쪽으로

94
00:06:38,030 --> 00:06:42,980
내려가기 위해 단계 크기가

95
00:06:42,980 --> 00:06:47,120
정의되며, 우리는 종종

96
00:06:47,120 --> 00:06:54,280
기울기의 음의 방향으로 단계 크기에 따라

97
00:06:54,280 --> 00:06:58,100
한 걸음을 내딛습니다.

98
00:06:58,100 --> 00:07:01,910
그래서 이것이 경량 하강 알고리즘이었습니다.

99
00:07:01,910 --> 00:07:04,030
최적화를 위해 우리는

100
00:07:04,030 --> 00:07:09,610
수치적 기울기와 해석적 기울기의 두 가지 접근 방식에

101
00:07:09,610 --> 00:07:15,290
대해 이야기했으며, 두 가지 모두 장단점이 있습니다.

102
00:07:15,290 --> 00:07:21,820
우리는 실제로 해석적 기울기를 도출하는 것에

103
00:07:21,820 --> 00:07:24,680
대해 논의했습니다.

104
00:07:24,680 --> 00:07:28,930
그리고 구현과 수학이

105
00:07:28,930 --> 00:07:31,750
어렵다면, 우리는

106
00:07:31,750 --> 00:07:37,030
수치적 기울기로 구현을 확인합니다.

107
00:07:37,030 --> 00:07:39,970
우리가 이야기한 또 다른 도전

108
00:07:39,970 --> 00:07:46,270
과제 중 하나는 전체 데이터 세트에서 손실 함수와 그 기울기를 통합하는

109
00:07:46,270 --> 00:07:47,740
것이었습니다.

110
00:07:47,740 --> 00:07:51,480
따라서 대규모 데이터 세트가 있는 경우,

111
00:07:51,480 --> 00:07:58,400
전체 데이터 세트에서 손실 함수와 도함수를 실행하는 것은 매우 비용이 많이

112
00:07:58,400 --> 00:07:58,900
듭니다.

113
00:07:58,900 --> 00:08:02,520
그래서 우리는 데이터 세트에서

114
00:08:02,520 --> 00:08:12,030
샘플링된 여러 예제, 보통 32, 64, 128 또는 256을

115
00:08:12,030 --> 00:08:19,120
사용하는 미니 배치의 아이디어에 대해 이야기했습니다.

116
00:08:19,120 --> 00:08:27,240
그리고 그 서브샘플링된 데이터는 그래디언트를 식별하고

117
00:08:27,240 --> 00:08:33,990
최소값으로 나아가는 단계를 취하는 데 사용됩니다.

118
00:08:33,990 --> 00:08:39,340
SGD와 확률적 경량 하강법을 넘어, 우리는

119
00:08:39,340 --> 00:08:43,350
모멘텀, RMSProp 및 Adam

120
00:08:43,350 --> 00:08:51,570
최적화기를 사용한 SGD의 몇 가지 최적화에 대해 이야기했습니다.

121
00:08:51,570 --> 00:08:56,270
그리고 이와 관련된 많은 세부

122
00:08:56,270 --> 00:08:59,790
사항이 있으며, 특정 질문이

123
00:08:59,790 --> 00:09:06,490
있다면 세 번째 강의를 참조하시기 바랍니다.

124
00:09:06,490 --> 00:09:09,230
그리고 우리가 이야기한 다른

125
00:09:09,230 --> 00:09:12,380
것 중 하나는 학습률의 중요성과

126
00:09:12,380 --> 00:09:14,700
학습률 스케줄링이었습니다.

127
00:09:14,700 --> 00:09:16,910
그리고 일부

128
00:09:16,910 --> 00:09:21,450
최적화기에서는 보통 더 큰 학습률

129
00:09:21,450 --> 00:09:27,800
값으로 시작한 다음 학습률을 감소시키거나

130
00:09:27,800 --> 00:09:33,250
값을 줄이는 다양한 방법을 사용합니다.

131
00:09:33,250 --> 00:09:38,850
이것은 많은 최적화기에서 일반적으로 필요합니다.

132
00:09:38,850 --> 00:09:43,500
하지만 최근의 Adam 및 그 변형에서는

133
00:09:43,500 --> 00:09:47,940
수동으로 또는 명시적으로 감소시킬 필요가

134
00:09:47,940 --> 00:09:53,280
없으며, 이는 최적화기 자체에 인코딩되어 있습니다.

135
00:09:53,280 --> 00:10:01,410
그래서 이를 바탕으로 신경망 주제로 넘어가서 실제로

136
00:10:01,410 --> 00:10:06,330
신경망을 구축하고 더 흥미롭고

137
00:10:06,330 --> 00:10:13,770
어려운 문제를 해결하는 방법을 살펴보겠습니다.

138
00:10:13,770 --> 00:10:21,870
지금까지 우리는 W와 x의 곱인 이 선형 함수에 대해

139
00:10:21,870 --> 00:10:24,400
이야기했습니다.

140
00:10:24,400 --> 00:10:30,270
그리고 이것은 단지 하나의 레이어로 정의될 수

141
00:10:30,270 --> 00:10:33,910
있는 가장 기본적인 신경망입니다.

142
00:10:33,910 --> 00:10:36,150
우리는 레이어에 대해 이야기할 것입니다.

143
00:10:36,150 --> 00:10:42,410
여기서 주목해야 할 것은 입력

144
00:10:42,410 --> 00:10:46,130
데이터의 차원 D와

145
00:10:46,130 --> 00:10:50,030
C로, 입력 x의 차원

146
00:10:50,030 --> 00:10:54,360
또는 특성의 수입니다.

147
00:10:54,360 --> 00:10:57,590
C는 클래스의 수,

148
00:10:57,590 --> 00:11:02,300
기본적으로 출력의 수, 노드 또는

149
00:11:02,300 --> 00:11:04,730
뉴런의 수입니다.

150
00:11:04,730 --> 00:11:12,390
두 번째 레이어에서 신경망을 만들기 위해, 우리는 여기서

151
00:11:12,390 --> 00:11:19,320
W2라고 하는 새로운 가중치 집합을 정의할 수 있습니다.

152
00:11:19,320 --> 00:11:24,410
그리고 우리는 그것을 W1과 x의

153
00:11:24,410 --> 00:11:28,880
곱인 이전 레이어에 적용합니다.

154
00:11:28,880 --> 00:11:31,910
다시 말하지만, 여기서 차원에 주목하세요.

155
00:11:31,910 --> 00:11:35,360
우리는 C개의 출력과 D개의 입력

156
00:11:35,360 --> 00:11:37,380
특성을 가지고 있습니다.

157
00:11:37,380 --> 00:11:42,220
하지만 H도 정의하고, 이는

158
00:11:42,220 --> 00:11:49,210
뉴런의 수, 은닉층 노드 또는 뉴런의

159
00:11:49,210 --> 00:11:52,720
수를 정의합니다.

160
00:11:52,720 --> 00:11:53,810
그것이 한 가지 포인트입니다.

161
00:11:53,810 --> 00:11:56,620
두 번째 포인트는 우리가

162
00:11:56,620 --> 00:12:00,770
다시 돌아올 max 함수입니다.

163
00:12:00,770 --> 00:12:04,540
우리가 그것이 무엇인지, 그리고 그것이 의미하는 바를 설명하겠습니다.

164
00:12:04,540 --> 00:12:07,630
여기서 max 연산이 하는

165
00:12:07,630 --> 00:12:10,960
것은 W1과 W2에 의해 수행된

166
00:12:10,960 --> 00:12:15,980
선형 변환 사이에 비선형성을 생성하는 것입니다.

167
00:12:15,980 --> 00:12:20,830
그리고 이것은 실제로 과정에서 매우, 매우

168
00:12:20,830 --> 00:12:22,850
중요한 부분입니다.

169
00:12:22,850 --> 00:12:25,160
비선형성에 대해 조금

170
00:12:25,160 --> 00:12:30,430
이야기하겠지만, 잊기 전에 이 마지막 부분도 살펴보겠습니다.

171
00:12:30,430 --> 00:12:34,400
실제로, 우리는 첫 번째와 두 번째 강의에서

172
00:12:34,400 --> 00:12:39,010
이야기한 것처럼 W와 x만 포함하고 있습니다.

173
00:12:39,010 --> 00:12:45,850
우리는 완전한 프레임워크를 갖기 위해 바이어스도 포함합니다.

174
00:12:45,850 --> 00:12:47,560
그래서 실제로 우리는

175
00:12:47,560 --> 00:12:51,160
바이어스도 있지만, 단순함을 위해 여기에는 쓰지 않습니다.

176
00:12:51,160 --> 00:12:56,410
어쨌든, max 연산이 비선형성을 생성하고 있습니다.

177
00:12:56,410 --> 00:12:59,460
그리고 이것은 실제로 매우

178
00:12:59,460 --> 00:13:06,820
중요합니다. 왜냐하면 지난 몇 강의에서 선형 분류기에 대해 이야기했을

179
00:13:06,820 --> 00:13:11,460
때, 우리는 샘플을 단일 선으로 분리할

180
00:13:11,460 --> 00:13:15,390
수 없는 다양한 문제가 있다고

181
00:13:15,390 --> 00:13:16,480
언급했습니다.

182
00:13:16,480 --> 00:13:20,370
이것은 신경망과 선형 함수로 이

183
00:13:20,370 --> 00:13:23,770
문제를 해결하기 위해 원래

184
00:13:23,770 --> 00:13:27,510
공간에서 새로운 공간으로 비선형

185
00:13:27,510 --> 00:13:31,780
변환이 필요하다는 예 중 하나였습니다.

186
00:13:31,780 --> 00:13:35,550
그리고 이제 새로운 공간에서, 선을 사용하여 분리할 수

187
00:13:35,550 --> 00:13:37,080
있음을 알 수 있습니다.

188
00:13:37,080 --> 00:13:44,960
따라서 이 경우, 입력과 두 번째 공간 간의 비선형 변환이

189
00:13:44,960 --> 00:13:47,790
있으며, 이는 x와

190
00:13:47,790 --> 00:13:52,760
y를 극좌표 r과 theta로 매핑합니다.

191
00:13:52,760 --> 00:13:53,580
y를 극좌표 r과 theta로 매핑합니다.

192
00:13:53,580 --> 00:13:57,000
하지만 다시 말하지만, 이것은 단지 하나의 예일 뿐입니다.

193
00:13:57,000 --> 00:14:01,130
다른 많은 예가 있습니다.

194
00:14:01,130 --> 00:14:07,790
이 예를 가지고, 다시 돌아가서-- 오ops,

195
00:14:07,790 --> 00:14:14,360
다시 두 층 신경망의 정의로 돌아갑니다.

196
00:14:14,360 --> 00:14:17,630
아마도 여러분이 문헌과 이 수업

197
00:14:17,630 --> 00:14:20,660
외부에서 보았듯이, 가중치와

198
00:14:20,660 --> 00:14:26,570
입력, 층에만 의존하는 이러한 유형의 네트워크는 곱셈

199
00:14:26,570 --> 00:14:30,110
외에 다른 연산이 없기 때문에 종종

200
00:14:30,110 --> 00:14:33,830
완전 연결 네트워크 또는 다층

201
00:14:33,830 --> 00:14:36,880
퍼셉트론(MLP)이라고 불립니다.

202
00:14:36,880 --> 00:14:40,250
그래서 그것이 한 가지입니다.

203
00:14:40,250 --> 00:14:42,610
그리고 우리는 실제로 더

204
00:14:42,610 --> 00:14:49,030
많은 층을 쌓아 더 나은, 더 큰 네트워크를 만들 수 있습니다.

205
00:14:49,030 --> 00:14:52,540
그리고 이 경우, 다시

206
00:14:52,540 --> 00:14:54,970
차원과 중간에

207
00:14:54,970 --> 00:15:00,220
있는 은닉층에 주목하고, 차원들이

208
00:15:00,220 --> 00:15:04,150
서로 일치하는지 확인하세요.

209
00:15:04,150 --> 00:15:11,470
신경망이 무엇을 하는지에 대한 이 시각적

210
00:15:11,470 --> 00:15:14,003
표현으로 돌아갑니다.

211
00:15:16,540 --> 00:15:21,460
우리는 선형 표현을 가질 때 이것에 대해

212
00:15:21,460 --> 00:15:28,600
이야기했으며, 종종 네트워크가 가중치를 통해 어떤 템플릿을

213
00:15:28,600 --> 00:15:31,780
학습하는 일이 발생합니다.

214
00:15:31,780 --> 00:15:34,500
지난 주에 우리가 학습되고 있는 이러한

215
00:15:34,500 --> 00:15:37,120
템플릿에 대해 이야기했던 것을 기억하십니까?

216
00:15:37,120 --> 00:15:40,050
그래서 다시 말하지만, 템플릿이라고 말하고 있습니다. 그렇지만 그것들은
아닙니다.

217
00:15:40,050 --> 00:15:44,920
즉, 그것들은 이미지의 대표자이지만, 어떤

218
00:15:44,920 --> 00:15:49,780
데이터에 대해 훈련되었는지에 따라 다릅니다.

219
00:15:49,780 --> 00:15:54,990
그래서 지난 주에 논의한 이러한 템플릿은

220
00:15:54,990 --> 00:15:59,820
입력 뉴런 위에 W를 적용하여 생성된

221
00:15:59,820 --> 00:16:05,200
10개의 출력에 의해 생성되었습니다.

222
00:16:05,200 --> 00:16:11,830
그래서 이제 여러 층이 있고, 더 많은 층이 있으므로, 이제 실제로

223
00:16:11,830 --> 00:16:15,370
더 많은 템플릿을 생성할 수 있습니다.

224
00:16:15,370 --> 00:16:18,150
이제 우리는 중간에 100개의 템플릿을

225
00:16:18,150 --> 00:16:22,950
생성할 수 있는 층이 있으며, 이는 선형 분류기에 대해 단지

226
00:16:22,950 --> 00:16:24,250
10개에 해당합니다.

227
00:16:24,250 --> 00:16:26,350
비록 여전히 그 10개도 가지고 있지만요.

228
00:16:26,350 --> 00:16:31,340
그리고 이것은 다시 매우 높은 수준에서, 매우 높은 수준의 이해

229
00:16:31,340 --> 00:16:35,340
관점에서, 제가 이것이 의미하는 바를 말씀드리고 있습니다.

230
00:16:35,340 --> 00:16:38,910
중간에 100개의 뉴런이 있을 때, 우리는

231
00:16:38,910 --> 00:16:40,700
네트워크에 전체

232
00:16:40,700 --> 00:16:44,580
객체가 아닌 객체의 일부를 위한 템플릿을 생성할

233
00:16:44,580 --> 00:16:47,160
수 있는 힘을 주고 있습니다.

234
00:16:47,160 --> 00:16:49,200
예를 들어, 여기 보이는

235
00:16:49,200 --> 00:16:56,460
클래스는 새, 고양이, 사슴, 개, 개구리, 말이 있으며, 이들은 모두 눈을
가지고 있습니다.

236
00:16:56,460 --> 00:16:59,390
그래서 그 10개의 템플릿 중 하나,

237
00:16:59,390 --> 00:17:03,230
100개의 템플릿은 아마도 모든 클래스 간에 공유될 수 있는

238
00:17:03,230 --> 00:17:05,310
객체의 일부일 수 있습니다.

239
00:17:05,310 --> 00:17:08,150
따라서 높은 수준의 관점에서

240
00:17:08,150 --> 00:17:12,240
보면, 이것들이 템플릿을 형성할 수 있습니다.

241
00:17:12,240 --> 00:17:15,859
그리고 시각화 주제와 신경망에서 배운

242
00:17:15,859 --> 00:17:19,640
것에 대해 돌아오면, 지금 제가 이야기하고

243
00:17:19,640 --> 00:17:22,550
있는 것에 대한 더 많은 세부사항을

244
00:17:22,550 --> 00:17:24,500
밝혀낼 것입니다.

245
00:17:24,500 --> 00:17:29,430
그래서 함수 max로 돌아갑니다.

246
00:17:29,430 --> 00:17:32,230
우리는 여기서 생성된 비선형성인 max

247
00:17:32,230 --> 00:17:34,040
함수에 대해 이야기했습니다.

248
00:17:34,040 --> 00:17:37,780
신경망 용어로 우리는 이를

249
00:17:37,780 --> 00:17:41,270
활성화 함수라고 부릅니다.

250
00:17:41,270 --> 00:17:45,920
그리고 이것은 모델을 구축하고 신경망을

251
00:17:45,920 --> 00:17:49,840
구축하는 데 매우 중요한 역할을

252
00:17:49,840 --> 00:17:51,740
하고 있습니다.

253
00:17:51,740 --> 00:17:54,260
슬라이드에 있는 이 질문에 답해봅시다.

254
00:17:54,260 --> 00:17:59,350
이 활성화 함수 중 하나, 예를 들어 max 함수를 제거하고

255
00:17:59,350 --> 00:18:03,170
신경망을 구축하려고 하면 어떻게 될까요?

256
00:18:03,170 --> 00:18:06,350
max를 제거하면 우리의 함수는 이렇게 될 것입니다.

257
00:18:06,350 --> 00:18:10,400
그래서 W2에 W1과 x를 곱한 것입니다.

258
00:18:10,400 --> 00:18:12,220
여기서 무슨 일이 일어날까요?

259
00:18:12,220 --> 00:18:13,160
네, 정확히 그렇습니다.

260
00:18:13,160 --> 00:18:17,740
당신이 추측할 수 있듯이, W2와

261
00:18:17,740 --> 00:18:23,710
W1의 곱셈은 실제로 다른 행렬 W3로

262
00:18:23,710 --> 00:18:27,050
쉽게 대체될 수 있습니다.

263
00:18:27,050 --> 00:18:30,070
그럼 당신의 함수는 단순한 선형 함수가 됩니다.

264
00:18:30,070 --> 00:18:32,350
그래서 모든 것이 함께 묶일 수 있습니다.

265
00:18:32,350 --> 00:18:34,950
따라서 비선형 문제를

266
00:18:34,950 --> 00:18:40,710
해결할 수 있는 힘을 주기 위해 중간에

267
00:18:40,710 --> 00:18:44,410
어떤 비선형성이 필요합니다.

268
00:18:44,410 --> 00:18:49,420
우리가 방금 이야기한 함수는 ReLU입니다.

269
00:18:49,420 --> 00:18:53,710
이는 정류된 선형 유닛입니다.

270
00:18:53,710 --> 00:18:56,700
신경망에서 사용되는 매우

271
00:18:56,700 --> 00:18:59,920
인기 있는 활성화 함수입니다.

272
00:18:59,920 --> 00:19:02,790
많은 다른 아키텍처와 더

273
00:19:02,790 --> 00:19:07,630
현대적인 아키텍처에서 테스트된 많은

274
00:19:07,630 --> 00:19:11,220
다른 변형이 있지만, ReLU의

275
00:19:11,220 --> 00:19:13,380
문제 중 하나는

276
00:19:13,380 --> 00:19:16,470
때때로 모든 것을 0으로

277
00:19:16,470 --> 00:19:20,700
만들기 때문에 죽은 뉴런을 생성한다는

278
00:19:20,700 --> 00:19:22,240
것입니다.

279
00:19:22,240 --> 00:19:27,540
따라서 죽은 뉴런을 피하기 위해, 이 유형의

280
00:19:27,540 --> 00:19:32,840
모델링을 사용하는 leaky ReLU

281
00:19:32,840 --> 00:19:38,960
또는 지수 선형 유닛인 ELU가 다른 옵션입니다.

282
00:19:38,960 --> 00:19:41,180
ELU는 더 나은 제로 중심

283
00:19:41,180 --> 00:19:44,250
함수를 가지고 있기 때문에 조금 더 좋습니다.

284
00:19:44,250 --> 00:19:52,010
그리고 GeLU, 가우시안 또는 선형 유닛과 같은 몇 가지

285
00:19:52,010 --> 00:19:55,250
새로운 변형이 있습니다.

286
00:19:55,250 --> 00:19:59,100
또는 ELU와 GeLU 두 가지 변형을

287
00:19:59,100 --> 00:20:02,130
모두 들어본 적이 있습니다.

288
00:20:02,130 --> 00:20:05,600
이들은 종종 변환기에서 신경 아키텍처에서 더

289
00:20:05,600 --> 00:20:07,110
자주 사용됩니다.

290
00:20:07,110 --> 00:20:14,840
그리고 SiLU 또는 스위치도 있습니다.

291
00:20:14,840 --> 00:20:20,720
이는 현대 CNNR 아키텍처 중 일부에서

292
00:20:20,720 --> 00:20:25,180
사용되는 시그모이드 선형 유닛입니다.

293
00:20:25,180 --> 00:20:29,440
구글은 그들의 모델의 일부 변형과

294
00:20:29,440 --> 00:20:33,910
EfficientNet에서 이를 사용하고 있었습니다.

295
00:20:33,910 --> 00:20:40,030
이 외에도 시그모이드와 탄하(Tanh)와

296
00:20:40,030 --> 00:20:48,020
같은 함수들이 종종 활성화 함수로 사용됩니다.

297
00:20:48,020 --> 00:20:52,750
하지만 이들은 값이 좁은 범위로 압축되기

298
00:20:52,750 --> 00:20:57,020
때문에 몇 가지 문제가 있습니다.

299
00:20:57,020 --> 00:21:01,820
이로 인해 기울기 소실이 발생하기도 합니다.

300
00:21:01,820 --> 00:21:05,530
그래서 우리는 종종 신경망의 중간에서

301
00:21:05,530 --> 00:21:08,420
시그모이드나 탄하를 사용하지 않습니다.

302
00:21:08,420 --> 00:21:13,390
이들은 종종 후반 층에서 사용되며, 예를 들어

303
00:21:13,390 --> 00:21:15,970
출력을 이진화하는

304
00:21:15,970 --> 00:21:19,370
등의 작업을 원할 때 사용됩니다.

305
00:21:19,370 --> 00:21:22,850
그래서 제가 말했듯이, ReLU는 종종 좋은 기본 선택입니다.

306
00:21:22,850 --> 00:21:26,860
많은 아키텍처에서 매우 많이 사용됩니다.

307
00:21:26,860 --> 00:21:30,120
우리가 이야기한 동일한

308
00:21:30,120 --> 00:21:33,390
함수의 변형이 매우 많습니다.

309
00:21:33,390 --> 00:21:36,810
우리가 이야기한 내용을 요약하고 몇 가지

310
00:21:36,810 --> 00:21:38,650
질문에 답하고 싶습니다.

311
00:21:38,650 --> 00:21:45,280
우리는 다양한 추가 층에 대해 이야기했습니다.

312
00:21:45,280 --> 00:21:49,080
하지만 활성화 함수는 종종 층에서

313
00:21:49,080 --> 00:21:54,040
작동하는 함수라는 점을 강조하고 싶습니다.

314
00:21:54,040 --> 00:21:58,170
그리고 우리는 이전 층과 다음

315
00:21:58,170 --> 00:22:01,170
층 간의 가중치를

316
00:22:01,170 --> 00:22:04,030
정의하는 W도 있습니다.

317
00:22:04,030 --> 00:22:07,350
다시 말해, 이것들은 매우

318
00:22:07,350 --> 00:22:12,400
간단한 구현을 가진 완전 연결 신경망입니다.

319
00:22:12,400 --> 00:22:17,310
우리가 필요한 것은 활성화 함수를 정의할 수 있는

320
00:22:17,310 --> 00:22:17,830
것입니다.

321
00:22:17,830 --> 00:22:20,530
이 예제를 보면,

322
00:22:20,530 --> 00:22:26,180
활성화 함수로 정의된 시그모이드 함수를

323
00:22:26,180 --> 00:22:30,180
쉽게 사용할 수 있습니다.

324
00:22:30,180 --> 00:22:35,190
숨겨진 값, 숨겨진 뉴런의 첫 번째 및

325
00:22:35,190 --> 00:22:42,800
두 번째 층은 W1을 x에 적용한 다음 바이어스를

326
00:22:42,800 --> 00:22:46,910
추가하고 활성화 함수를 적용하여

327
00:22:46,910 --> 00:22:48,060
계산됩니다.

328
00:22:48,060 --> 00:22:50,510
H2에 대해서도

329
00:22:50,510 --> 00:22:57,770
동일하게 적용되며, 출력은 W3와 숨겨진 값의 마지막

330
00:22:57,770 --> 00:23:01,830
층 간의 내적이 되어 출력

331
00:23:01,830 --> 00:23:04,290
층을 생성합니다.

332
00:23:04,290 --> 00:23:07,140
질문이 있다면 여기서 멈추고 답변하겠습니다.

333
00:23:07,140 --> 00:23:10,280
그리고 나서 계속 진행하고 싶습니다.

334
00:23:10,280 --> 00:23:11,370
정말 좋은 질문입니다.

335
00:23:11,370 --> 00:23:13,640
질문은 새로운 문제에 대해

336
00:23:13,640 --> 00:23:16,520
이러한 활성화 함수 중 어떤

337
00:23:16,520 --> 00:23:18,920
것을 선택할 것인가입니다.

338
00:23:18,920 --> 00:23:21,160
질문에 대한 간단한

339
00:23:21,160 --> 00:23:26,140
대답은 대부분의 경우 경험적이라는 것입니다.

340
00:23:26,140 --> 00:23:29,140
하지만 우리는 종종 값을

341
00:23:29,140 --> 00:23:32,290
시작하거나 특정 아키텍처에 사용되는

342
00:23:32,290 --> 00:23:35,890
표준 활성화 함수를 사용합니다.

343
00:23:35,890 --> 00:23:40,150
앞서 언급했듯이, CNN이나 변환기

344
00:23:40,150 --> 00:23:45,160
및 다양한 아키텍처에서 자주 사용되는 활성화

345
00:23:45,160 --> 00:23:47,600
함수가 있습니다.

346
00:23:47,600 --> 00:23:53,330
그래서 우리는 종종 이전에 테스트된 것들을 선택합니다.

347
00:23:53,330 --> 00:23:55,430
하지만 네, 대부분 경험적입니다.

348
00:23:55,430 --> 00:23:59,090
새로운 문제를 위한 새로운 네트워크를 설계하는

349
00:23:59,090 --> 00:24:02,690
경우, 이는 다른 하이퍼파라미터와 매우

350
00:24:02,690 --> 00:24:05,830
유사하게 선택해야 할 사항 중 하나입니다.

351
00:24:05,830 --> 00:24:10,060
그래서 여기서 질문은 이러한 모든 활성화

352
00:24:10,060 --> 00:24:13,570
함수 간의 공통 속성이 무엇이며,

353
00:24:13,570 --> 00:24:17,090
그것이 실제로 무엇을 하는가입니다.

354
00:24:17,090 --> 00:24:19,570
몇 가지 예를 드리겠습니다.

355
00:24:19,570 --> 00:24:23,580
그리고 이러한 활성화 함수가 무엇을 하는지에

356
00:24:23,580 --> 00:24:26,640
대한 세부 사항으로 들어가겠습니다.

357
00:24:26,640 --> 00:24:34,770
기본적으로 여기서 가장 중요하고 공통적인 특징은 비선형성을

358
00:24:34,770 --> 00:24:37,260
생성하는 것입니다.

359
00:24:37,260 --> 00:24:40,420
우리는 활성화로 선형 함수를 사용하지 않습니다.

360
00:24:40,420 --> 00:24:43,110
비선형성을 만드는

361
00:24:43,110 --> 00:24:46,330
것은 매우 중요합니다.

362
00:24:46,330 --> 00:24:48,820
왜 이렇게 많은 변형이 있을까요?

363
00:24:48,820 --> 00:24:51,300
소실 기울기에 대한 문제를

364
00:24:51,300 --> 00:24:53,320
조금 말씀드렸습니다.

365
00:24:53,320 --> 00:24:58,050
함수의 미분 가능성에 대해 조금

366
00:24:58,050 --> 00:24:59,440
말씀드렸습니다.

367
00:24:59,440 --> 00:25:02,040
신경망에서 사용하기 때문에 미분

368
00:25:02,040 --> 00:25:03,300
가능해야 합니다.

369
00:25:03,300 --> 00:25:10,980
적절한 제로 중심 값과 부드러운 함수가 있으면 수렴하는

370
00:25:10,980 --> 00:25:14,040
네트워크를 더 빠르게

371
00:25:14,040 --> 00:25:17,100
얻을 수 있습니다.

372
00:25:17,100 --> 00:25:19,650
다양한 요인이 많습니다.

373
00:25:19,650 --> 00:25:23,310
이것들은 제가 말씀드리고 이야기한

374
00:25:23,310 --> 00:25:25,880
주요 요소들로, 이러한 함수를

375
00:25:25,880 --> 00:25:30,650
정의하거나 설계하는 데 중요한 역할을 합니다.

376
00:25:30,650 --> 00:25:32,150
함수의 세부

377
00:25:32,150 --> 00:25:36,710
사항에 대해 이야기할 때 좀 더 말씀드리겠습니다.

378
00:25:36,710 --> 00:25:42,180
모든 층에서 우리는 종종 동일한 활성화 함수를 사용합니다.

379
00:25:42,180 --> 00:25:45,500
하지만 제가 말했듯이, 때때로

380
00:25:45,500 --> 00:25:49,610
후속 층이나 출력 층에서는 시그모이드

381
00:25:49,610 --> 00:25:55,290
함수와/또는 탄젠트 함수를 사용하지만, 일반적으로 그렇습니다.

382
00:25:55,290 --> 00:26:04,340
질문은, 네트워크 전반에 걸쳐 모든 뉴런에 대해 동일한

383
00:26:04,340 --> 00:26:09,020
함수를 사용하는지였습니다.

384
00:26:09,020 --> 00:26:16,240
좋습니다, 우리가 이야기하고 있던

385
00:26:16,240 --> 00:26:23,450
신경망 모델의 구현으로 계속

386
00:26:23,450 --> 00:26:25,580
진행하겠습니다.

387
00:26:25,580 --> 00:26:28,840
매우 간단한 방법이 있습니다.

388
00:26:28,840 --> 00:26:31,040
파이썬에서 두 개의 층을

389
00:26:31,040 --> 00:26:32,950
가진 신경망을

390
00:26:32,950 --> 00:26:36,740
구축하는 것은 20줄도 안 되는 코드입니다.

391
00:26:36,740 --> 00:26:40,990
매우 간단하게, 제가 이야기한 차원으로 네트워크를

392
00:26:40,990 --> 00:26:42,280
정의합니다.

393
00:26:42,280 --> 00:26:45,640
N은 샘플의 수입니다.

394
00:26:45,640 --> 00:26:48,490
D_in은 입력의 차원입니다.

395
00:26:48,490 --> 00:26:51,110
D_out은 출력의 차원입니다.

396
00:26:51,110 --> 00:26:56,570
h는 은닉층의 뉴런 수입니다.

397
00:26:56,570 --> 00:27:00,700
우리는 x와 y를 생성하고 W를

398
00:27:00,700 --> 00:27:05,630
무작위로 초기화하는 것에 대해 이야기했습니다.

399
00:27:05,630 --> 00:27:09,550
그 다음에는 순전파가 있습니다.

400
00:27:09,550 --> 00:27:16,000
이는 입력에 W를 적용하고, 층별로

401
00:27:16,000 --> 00:27:24,870
진행하여 최종적으로 출력, 예측 값을 생성하고, 마지막으로 손실

402
00:27:24,870 --> 00:27:29,610
함수를 계산하고 그 손실 값을

403
00:27:29,610 --> 00:27:33,360
출력하는 것을 의미합니다.

404
00:27:33,360 --> 00:27:39,240
순전파 후에는 최적화 과정이 필요합니다. 이는 분석적

405
00:27:39,240 --> 00:27:43,260
기울기를 계산하고 생성된 기울기를

406
00:27:43,260 --> 00:27:48,420
사용하여 W1과 W2를 최적화하기 위해

407
00:27:48,420 --> 00:27:53,920
경량 하강법을 실행하는 방법입니다. 기본적으로

408
00:27:53,920 --> 00:27:57,090
네트워크의 최적 값으로 한

409
00:27:57,090 --> 00:27:59,890
걸음 나아가는 것입니다.

410
00:27:59,890 --> 00:28:04,590
하지만 이 부분, 즉 분석적 기울기를

411
00:28:04,590 --> 00:28:08,910
계산하는 것은 우리가 많이 다루지 않은

412
00:28:08,910 --> 00:28:11,790
가장 중요한 부분입니다.

413
00:28:11,790 --> 00:28:13,571
그래서 이 강의의

414
00:28:13,571 --> 00:28:15,410
거의 나머지 부분은

415
00:28:15,410 --> 00:28:21,600
이를 작동시키고 다양한 설정에서 확장하는 것에 관한 것입니다.

416
00:28:21,600 --> 00:28:29,490
이러한 신경망을 훈련하고 구축한 후, 은닉층에서

417
00:28:29,490 --> 00:28:34,890
사용하는 노드 수에 따라 두

418
00:28:34,890 --> 00:28:36,920
클래스 간의

419
00:28:36,920 --> 00:28:41,510
분리 패턴을 식별할 수

420
00:28:41,510 --> 00:28:42,600
있습니다.

421
00:28:42,600 --> 00:28:45,920
더 많은 뉴런은 종종 더

422
00:28:45,920 --> 00:28:51,470
복잡한 함수를 학습하고 노드, 포인트의 더

423
00:28:51,470 --> 00:28:57,903
나은 분리를 위한 더 많은 용량을 의미합니다.

424
00:29:00,440 --> 00:29:05,100
이것을 보면, 이것은 매우 유사합니다.

425
00:29:05,100 --> 00:29:07,820
여기 보여주는 패턴은 두 번째

426
00:29:07,820 --> 00:29:11,400
강의에서 k 최근접 이웃에 대해 이야기할 때

427
00:29:11,400 --> 00:29:13,140
보여준 것과 유사합니다.

428
00:29:13,140 --> 00:29:20,790
k가 1일 때, 즉 최근접 이웃 프레임워크를 사용할 때, 이는 더

429
00:29:20,790 --> 00:29:25,600
많은 뉴런을 사용하는 것과 매우 유사했습니다.

430
00:29:25,600 --> 00:29:28,650
따라서 네트워크에

431
00:29:28,650 --> 00:29:33,510
많은 용량을 부여하면 과적합 문제가

432
00:29:33,510 --> 00:29:36,600
발생할 수 있습니다.

433
00:29:36,600 --> 00:29:40,570
보지 못한 데이터에 일반화할 수 없게 됩니다.

434
00:29:40,570 --> 00:29:45,940
하지만 이와 관련된 다양한 해결책이 있습니다.

435
00:29:45,940 --> 00:29:49,810
그리고 제가 강조하고 싶은 한

436
00:29:49,810 --> 00:29:51,990
가지는 신경망의

437
00:29:51,990 --> 00:29:56,800
크기를 정규화기로 사용하지 않는 것입니다.

438
00:29:56,800 --> 00:29:59,970
우리는 이 네트워크 크기를 미세 조정하기 위한

439
00:29:59,970 --> 00:30:02,500
하이퍼파라미터로 자주 사용하지 않습니다.

440
00:30:02,500 --> 00:30:06,720
다만, 우리는 네트워크 크기와 관련된

441
00:30:06,720 --> 00:30:11,040
하이퍼파라미터의 다양한 값으로 실험합니다.

442
00:30:11,040 --> 00:30:17,900
하지만 우리가 자주 하는 것은 필요한 것보다 약간 더 큰 네트워크를

443
00:30:17,900 --> 00:30:19,350
사용하는 것입니다.

444
00:30:19,350 --> 00:30:22,400
그런 다음 정규화와

445
00:30:22,400 --> 00:30:25,310
이 정규화기, 특히

446
00:30:25,310 --> 00:30:33,710
이 정규화 하이퍼파라미터를 사용하여 다양한 설정을

447
00:30:33,710 --> 00:30:35,250
확인합니다.

448
00:30:35,250 --> 00:30:39,080
그래서 우리가 자주 조정하는 것은 정규화와

449
00:30:39,080 --> 00:30:41,030
정규화 하이퍼파라미터이지,

450
00:30:41,030 --> 00:30:45,380
반드시 네트워크 크기 자체는 아닙니다.

451
00:30:45,380 --> 00:30:53,310
좋습니다, 이것이 신경망의 개념을 간단히 설명한 것입니다.

452
00:30:53,310 --> 00:30:58,160
하지만 우리는 신경망과 그것이 생물학적

453
00:30:58,160 --> 00:31:02,700
영감을 받을 수 있는 방법에

454
00:31:02,700 --> 00:31:04,980
대해 들어왔습니다.

455
00:31:04,980 --> 00:31:07,990
그래서 이에 대해 조금 이야기하겠지만 질문이 있습니다.

456
00:31:07,990 --> 00:31:13,930
기본적으로 당신의 질문은, 여기서 람다 값을 증가시킬 때 모델이

457
00:31:13,930 --> 00:31:16,550
왜 더 과소적합되는가입니다.

458
00:31:16,550 --> 00:31:22,340
네, 그 질문에 빠르게 답하자면, 람다 값은

459
00:31:22,340 --> 00:31:24,940
정규화기가 전체

460
00:31:24,940 --> 00:31:27,250
손실에 얼마나 기여해야

461
00:31:27,250 --> 00:31:30,382
하는지를 조절합니다.

462
00:31:30,382 --> 00:31:35,870
정규화기에 대한 기여가 클수록-- 그리고 그 정규화기는 W에

463
00:31:35,870 --> 00:31:39,140
대해 정의되었다는 것을 기억하세요.

464
00:31:39,140 --> 00:31:41,360
그래서 W를 제약하고 있습니다.

465
00:31:41,360 --> 00:31:45,530
W의 값에 대한 자유를 줄이고 있습니다.

466
00:31:45,530 --> 00:31:54,440
자유가 적다는 것은 조금 더 일반적인 경계를 의미하며, 반드시 세부적인

467
00:31:54,440 --> 00:31:58,570
값이나 경계의 세부적인 부분을

468
00:31:58,570 --> 00:32:01,670
제공하지는 않습니다.

469
00:32:01,670 --> 00:32:06,860
그래서 모델을 너무 많이 제약하면,

470
00:32:06,860 --> 00:32:11,680
정규화기가 있더라도 그런 값이나 결정 경계를

471
00:32:11,680 --> 00:32:13,810
얻게 됩니다.

472
00:32:13,810 --> 00:32:17,320
네, 올바른 정규화기는 항상

473
00:32:17,320 --> 00:32:19,540
과적합을 방지합니다.

474
00:32:19,540 --> 00:32:21,830
다시 말해, 당신은 손실과

475
00:32:21,830 --> 00:32:27,470
올바른 출력을 예측하는 것 사이의 균형을 만들어야 합니다.

476
00:32:27,470 --> 00:32:30,530
손실의 첫 번째 부분은 올바른 출력을 예측하는 것입니다.

477
00:32:30,530 --> 00:32:33,620
두 번째 부분은 가중치의 값만 다루고, 더

478
00:32:33,620 --> 00:32:35,780
이상 출력에 신경 쓰지 않습니다.

479
00:32:35,780 --> 00:32:37,870
이것을 과도하게 가중치를 두면,

480
00:32:37,870 --> 00:32:40,750
좋은 분류기를 얻지 못할 것입니다.

481
00:32:40,750 --> 00:32:44,000
그래서 균형을 만드는 것이 좋습니다. 정규화기는 항상 좋습니다.

482
00:32:44,000 --> 00:32:46,870
하지만 너무 많이 사용하면 아무것도 좋지 않습니다.

483
00:32:53,440 --> 00:32:55,450
왜 정규화를 선택해야

484
00:32:55,450 --> 00:32:57,790
하는지 다시 설명해

485
00:32:57,790 --> 00:32:59,980
주실 수 있나요?

486
00:32:59,980 --> 00:33:03,000
여러 가지 이유가 있습니다.

487
00:33:03,000 --> 00:33:05,010
그 중 하나는 네트워크의 크기입니다.

488
00:33:05,010 --> 00:33:06,300
당신은 네트워크를 구축하고 있습니다.

489
00:33:06,300 --> 00:33:08,060
때때로 결과를

490
00:33:08,060 --> 00:33:12,470
얻기 위해 며칠 동안 실행해야 하는 네트워크를

491
00:33:12,470 --> 00:33:14,840
구축해야 합니다.

492
00:33:14,840 --> 00:33:20,990
그래서 우리가 자주 하는 것은 네트워크의

493
00:33:20,990 --> 00:33:24,710
매개변수 수를 증가시키는 것입니다,

494
00:33:24,710 --> 00:33:29,390
과적합의 수준이 보일 때까지.

495
00:33:29,390 --> 00:33:33,200
그때 우리는 네트워크가 실제로 데이터의 패턴을

496
00:33:33,200 --> 00:33:35,360
이해하고 있으며 이제

497
00:33:35,360 --> 00:33:39,080
데이터를 기억할 수 있다는 것을 알게 됩니다.

498
00:33:39,080 --> 00:33:43,070
그때 우리는 네트워크를 정규화하여

499
00:33:43,070 --> 00:33:45,750
과적합을 최소화하려고 합니다.

500
00:33:45,750 --> 00:33:48,840
따라서 정규화는 중요한 요소로 작용합니다.

501
00:33:48,840 --> 00:33:54,080
매개변수의 수나 네트워크의 복잡성이 너무

502
00:33:54,080 --> 00:33:56,900
높아지면 문제가 발생할

503
00:33:56,900 --> 00:33:58,530
것입니다.

504
00:33:58,530 --> 00:34:00,600
우리는 그렇게 하는 경우가 거의 없습니다.

505
00:34:00,600 --> 00:34:05,020
우리는 종종 새로운 문제에 대해 작은

506
00:34:05,020 --> 00:34:09,250
네트워크로 시작하고, 이후 정규화를

507
00:34:09,250 --> 00:34:11,530
통해 이를 수정합니다.

508
00:34:11,530 --> 00:34:14,710
주어진 문제에 대해 해결하는 데

509
00:34:14,710 --> 00:34:19,190
필요한 뉴런의 수는 어떻게 알 수 있을까요?

510
00:34:19,190 --> 00:34:25,929
이는 경험적 연구 작업과 유사한 유형의 다른 사례를

511
00:34:25,929 --> 00:34:29,590
기반으로 하며, 모든 경우에 적용되는

512
00:34:29,590 --> 00:34:31,880
정답은 없습니다.

513
00:34:31,880 --> 00:34:36,530
유사한 데이터로 훈련된 다른 네트워크를

514
00:34:36,530 --> 00:34:40,600
살펴보고 그 범위에서

515
00:34:40,600 --> 00:34:42,500
시작해야 합니다.

516
00:34:42,500 --> 00:34:46,960
그리고 종종, 네트워크의 복잡성을

517
00:34:46,960 --> 00:34:49,480
조정하기 위해 여러 실험을

518
00:34:49,480 --> 00:34:50,870
수행합니다.

519
00:34:50,870 --> 00:34:58,300
그래서 이는 탐색에 매우 의존합니다.

520
00:34:58,300 --> 00:35:02,370
따라서 귀하의 질문은 어떤 이론적이고 기초적인 작업이

521
00:35:02,370 --> 00:35:07,090
수행되어 어떤 활성화 함수를 사용하고, 몇 개의 레이어를

522
00:35:07,090 --> 00:35:10,120
사용할지 결정하는지에 대한 것입니다.

523
00:35:10,120 --> 00:35:13,710
이와 관련된 많은 연구와

524
00:35:13,710 --> 00:35:16,860
논문이 있으며, 이러한

525
00:35:16,860 --> 00:35:22,980
메타 또는 하이퍼파라미터를 최적화하는 방법도

526
00:35:22,980 --> 00:35:24,700
있습니다.

527
00:35:24,700 --> 00:35:27,690
우리는 이들에 대해 자세히 다루지

528
00:35:27,690 --> 00:35:29,940
않을 것입니다. 다시 말해, 그

529
00:35:29,940 --> 00:35:33,720
대부분은 데이터 세트와 해결하려는 문제에

530
00:35:33,720 --> 00:35:34,840
매우 의존합니다.

531
00:35:34,840 --> 00:35:39,570
따라서 귀하의 질문에 대한 최선의 답변은, 예, 그런

532
00:35:39,570 --> 00:35:41,710
작업이 존재한다는 것입니다.

533
00:35:41,710 --> 00:35:45,750
하지만 다시 말해, 각 작업은 귀하의 응용

534
00:35:45,750 --> 00:35:50,130
프로그램이나 문제에 반드시 맞지 않을 수 있는

535
00:35:50,130 --> 00:35:51,600
가정을 합니다.

536
00:35:51,600 --> 00:35:58,380
그래서 발생하는 것은 몇 가지 생물학적 영감이 있습니다.

537
00:35:58,380 --> 00:36:01,350
다시 말해, 이러한 영감은 매우 느슨합니다.

538
00:36:01,350 --> 00:36:03,500
여기 앉아 있거나

539
00:36:03,500 --> 00:36:10,280
온라인으로 보고 있는 신경과학자가 있다면, 제가

540
00:36:10,280 --> 00:36:17,640
드리는 모든 예시를 절대적인 진리로 받아들이지 마세요.

541
00:36:17,640 --> 00:36:22,010
하지만 일반적으로 뉴런에서

542
00:36:22,010 --> 00:36:26,970
발생하는 일은 이렇습니다. 이것은 뉴런의

543
00:36:26,970 --> 00:36:32,450
시각화입니다. 뉴런은 종종 수상돌기를

544
00:36:32,450 --> 00:36:39,440
통해 전달된 자극을 집합하는 세포체를 가지고

545
00:36:39,440 --> 00:36:40,980
있습니다.

546
00:36:40,980 --> 00:36:44,270
그리고 축삭을 사용하여, 그

547
00:36:44,270 --> 00:36:47,760
자극은 다른 뉴런으로 전달됩니다.

548
00:36:47,760 --> 00:36:50,360
이는 우리가 신경망에서 하는

549
00:36:50,360 --> 00:36:52,410
것과 매우 유사합니다.

550
00:36:52,410 --> 00:36:56,290
우리는 종종 신호를 포착하는

551
00:36:56,290 --> 00:37:04,610
함수를 가지고 있으며, 이전 레이어의 모든 자극과

552
00:37:04,610 --> 00:37:07,430
활성화를 포함합니다.

553
00:37:07,430 --> 00:37:12,040
그리고 세포체에서 그 함수는

554
00:37:12,040 --> 00:37:20,080
입력에 대해 작동하여 활성화를 출력하고 다음 레이어,

555
00:37:20,080 --> 00:37:24,050
다음 뉴런으로 전달합니다.

556
00:37:24,050 --> 00:37:27,130
기본적으로, 우리는 여기서

557
00:37:27,130 --> 00:37:32,170
값을 증가시키거나 감소시키기 위해 어떤

558
00:37:32,170 --> 00:37:36,130
형태의 활성화 함수가 필요합니다.

559
00:37:36,130 --> 00:37:42,010
따라서 다시 말해, 생물학적 뉴런과 우리가 구축하는

560
00:37:42,010 --> 00:37:45,310
신경망 간에는 많은 차이가

561
00:37:45,310 --> 00:37:50,470
있으며, 생물학적 뉴런은 실제로 훨씬 더

562
00:37:50,470 --> 00:37:52,670
복잡할 수 있습니다.

563
00:37:52,670 --> 00:37:58,560
하지만 일반적으로 공통 개념이 있습니다.

564
00:37:58,560 --> 00:38:00,450
우리가 구축하는 신경망은

565
00:38:00,450 --> 00:38:03,520
종종 규칙적인 패턴으로 구성됩니다.

566
00:38:03,520 --> 00:38:05,850
그러한 패턴은 신경망을

567
00:38:05,850 --> 00:38:09,090
구현할 때 더 나은 계산

568
00:38:09,090 --> 00:38:12,490
효율성을 원하기 때문입니다.

569
00:38:12,490 --> 00:38:14,760
복잡한 신경망을

570
00:38:14,760 --> 00:38:18,340
만들고 최적화하려는 연구가

571
00:38:18,340 --> 00:38:22,470
있었지만, 결과 측면에서

572
00:38:22,470 --> 00:38:27,510
우리는 종종 구축하는 일반적인 함수,

573
00:38:27,510 --> 00:38:30,210
일반적인 신경망과

574
00:38:30,210 --> 00:38:33,213
거의 비슷합니다.

575
00:38:36,150 --> 00:38:42,480
귀하의 두뇌 비유와 그것이 어떻게 해석될 수

576
00:38:42,480 --> 00:38:48,310
있는지에 대해 조심하라고 경고할 수 없습니다.

577
00:38:48,310 --> 00:38:49,860
그래서 많은 차이가 있습니다.

578
00:38:49,860 --> 00:38:54,710
여기서 멈추고 신경과학 측면에 관심이

579
00:38:54,710 --> 00:38:56,780
있는 분들과

580
00:38:56,780 --> 00:39:01,170
논의할 수 있으면 좋겠습니다.

581
00:39:01,170 --> 00:39:08,660
모든 것을 연결하면, 우리는 점수 함수를 가지고 있었습니다.

582
00:39:08,660 --> 00:39:12,410
이 점수 함수는 입력을 W의

583
00:39:12,410 --> 00:39:19,230
가중치 벡터 또는 가중치 행렬을 통해 점수로 변환합니다.

584
00:39:19,230 --> 00:39:27,200
네트워크의 손실 함수로 자주 사용하는 것은 이러한 점수를 힌지 손실,

585
00:39:27,200 --> 00:39:31,070
소프트맥스 또는 다른 변형을

586
00:39:31,070 --> 00:39:33,570
통해 사용하는 것입니다.

587
00:39:33,570 --> 00:39:39,470
또한, 우리는 정규화기를 정의했으며, 이는

588
00:39:39,470 --> 00:39:44,360
궁극적으로 데이터 손실과 정규화기를

589
00:39:44,360 --> 00:39:47,610
합한 총 손실을 제공합니다.

590
00:39:47,610 --> 00:39:51,610
W1과 W2를

591
00:39:51,610 --> 00:39:56,210
최적화하기 위해 필요한

592
00:39:56,210 --> 00:40:00,070
것은 L에 대한

593
00:40:00,070 --> 00:40:03,430
W1과 W2의

594
00:40:03,430 --> 00:40:11,000
부분 미분을 취할 수 있는 것입니다.

595
00:40:11,000 --> 00:40:13,930
우리가 인식해야 할

596
00:40:13,930 --> 00:40:18,830
다양한 세부 사항이 많습니다.

597
00:40:18,830 --> 00:40:24,370
먼저 이러한 함수를 만들고 미분을 취하고 이를

598
00:40:24,370 --> 00:40:27,890
기록하는 것은 종종 번거롭습니다.

599
00:40:27,890 --> 00:40:30,250
행렬 계산이 많고

600
00:40:30,250 --> 00:40:33,520
실제로 신경망을 구현하기 전에

601
00:40:33,520 --> 00:40:36,230
많은 작업이 필요합니다.

602
00:40:36,230 --> 00:40:38,020
다른 문제는

603
00:40:38,020 --> 00:40:42,370
논문에서 했던 것과 약간 다른 손실로

604
00:40:42,370 --> 00:40:46,180
변경하고 싶을 경우 모든 계산을 다시

605
00:40:46,180 --> 00:40:48,910
해야 한다는 것입니다.

606
00:40:48,910 --> 00:40:55,840
그 경우, 다시 전체 작업을 수행해야 합니다.

607
00:40:55,840 --> 00:41:02,520
결국 손실 함수가 복잡하면 이는 다루기 힘들고

608
00:41:02,520 --> 00:41:06,310
때로는 불가능해집니다.

609
00:41:06,310 --> 00:41:12,270
복잡한 함수와 함께라면 더욱 어려워질 것입니다.

610
00:41:12,270 --> 00:41:15,390
하지만 더 나은 아이디어가 있습니다.

611
00:41:15,390 --> 00:41:18,970
이는 우리의 구현에서 자주 사용됩니다.

612
00:41:18,970 --> 00:41:22,200
오늘 몇 가지 예를 들어 모든

613
00:41:22,200 --> 00:41:25,650
사람이 같은 페이지에 있고 이

614
00:41:25,650 --> 00:41:28,680
주제를 이해하도록 하겠습니다.

615
00:41:28,680 --> 00:41:35,670
그것은 계산 그래프와 역전파의 개념입니다.

616
00:41:35,670 --> 00:41:40,650
계산 그래프는 신경망의 모든 연산을

617
00:41:40,650 --> 00:41:43,470
모아 단계별로

618
00:41:43,470 --> 00:41:48,150
구성하고 입력과 기본적으로 필요한 모든

619
00:41:48,150 --> 00:41:52,050
매개변수에서 시작하여 최종

620
00:41:52,050 --> 00:41:55,410
출력, 즉 최종 레이어로

621
00:41:55,410 --> 00:41:59,080
손실을 얻는 것입니다.

622
00:41:59,080 --> 00:42:02,200
이 경우 손실 함수는 소프트맥스

623
00:42:02,200 --> 00:42:04,140
함수 또는 힌지 손실

624
00:42:04,140 --> 00:42:05,740
함수일 수 있습니다.

625
00:42:05,740 --> 00:42:07,870
무엇이든 간에,

626
00:42:07,870 --> 00:42:14,430
이는 정규화기 RW에 추가되는 손실 함수입니다.

627
00:42:14,430 --> 00:42:18,370
R은 W를 입력으로 가집니다.

628
00:42:18,370 --> 00:42:23,490
따라서 이 두 개를 더하여 손실을 계산하거나 생성합니다.

629
00:42:23,490 --> 00:42:28,780
손실을 계산하기 전에, 우리는 종종

630
00:42:28,780 --> 00:42:34,630
x와 W를 집계하고 점수를 생성해야 합니다.

631
00:42:34,630 --> 00:42:37,620
이것은 곱셈 함수입니다.

632
00:42:37,620 --> 00:42:41,040
이는 실제로 매우 유용합니다. 왜냐하면 우리가

633
00:42:41,040 --> 00:42:43,500
구축하는 대부분의 신경망은 그래픽 카드

634
00:42:43,500 --> 00:42:45,760
표현을 가지고 있기 때문입니다.

635
00:42:45,760 --> 00:42:48,310
모든 이러한 복잡한 함수는

636
00:42:48,310 --> 00:42:52,430
동일한 프레임워크로 표시될 수 있습니다.

637
00:42:52,430 --> 00:42:58,090
그런 다음 이를 사용하여 입력 이미지 또는 입력 데이터에서 시작하여

638
00:42:58,090 --> 00:43:01,160
계산 그래프를 구축할 수 있습니다.

639
00:43:01,160 --> 00:43:03,920
네트워크 전반에 걸쳐 많은 가중치가 있습니다.

640
00:43:03,920 --> 00:43:06,103
마지막으로 손실 함수가 있습니다.

641
00:43:09,730 --> 00:43:13,360
다시 말해, 이는 복잡한 신경망,

642
00:43:13,360 --> 00:43:18,130
예를 들어 이 신경 튜링 기계에

643
00:43:18,130 --> 00:43:19,220
유용합니다.

644
00:43:19,220 --> 00:43:23,960
이는 시간적 및 순차적 데이터에 사용됩니다.

645
00:43:23,960 --> 00:43:27,230
따라서 이 기계의 많은 언롤링이 있습니다.

646
00:43:27,230 --> 00:43:30,980
모든 작업을 수동으로

647
00:43:30,980 --> 00:43:39,382
수행해야 한다면 이는 다루기 힘들고 불가능할 것입니다.

648
00:43:39,382 --> 00:43:44,620
그래서 우리가 이 계산 그래프를 구축할

649
00:43:44,620 --> 00:43:49,830
때, 그 해결책이 역전파입니다.

650
00:43:49,830 --> 00:43:53,320
그리고 저는 매우 간단한 예로 시작하고 싶습니다.

651
00:43:53,320 --> 00:43:59,460
그래서 우리는 x, y, z의 함수 f로 시작합니다.

652
00:43:59,460 --> 00:44:03,220
이는 x 더하기 y 곱하기 z입니다.

653
00:44:03,220 --> 00:44:09,180
이 함수의 계산 그래프를 그리면,

654
00:44:09,180 --> 00:44:15,210
x와 y 사이의 덧셈 연산이

655
00:44:15,210 --> 00:44:17,440
있습니다.

656
00:44:17,440 --> 00:44:20,970
그리고 그 덧셈

657
00:44:20,970 --> 00:44:29,100
x와 y에 z를 곱하는 곱셈이 있습니다.

658
00:44:29,100 --> 00:44:35,560
입력 값이 x는 -2, y는 5, z는

659
00:44:35,560 --> 00:44:39,300
-4일 때, 이제

660
00:44:39,300 --> 00:44:41,550
실제로 모든

661
00:44:41,550 --> 00:44:47,310
계산을 하고 신경망에서 순전파를

662
00:44:47,310 --> 00:44:50,350
진행할 수 있습니다.

663
00:44:50,350 --> 00:44:56,670
첫 번째 단계는 x와 y를 더하는 것으로, 결과는 3입니다.

664
00:44:56,670 --> 00:44:59,970
단계별로 이해할 수 있도록

665
00:44:59,970 --> 00:45:02,950
이름을 붙이겠습니다.

666
00:45:02,950 --> 00:45:06,850
그래서 q는 x 더하기 y입니다.

667
00:45:06,850 --> 00:45:12,150
q에 대한 x와 y의 부분 미분을

668
00:45:12,150 --> 00:45:14,550
계산하려면 매우

669
00:45:14,550 --> 00:45:20,730
간단합니다. q와 x, y 사이의 공식이 있기

670
00:45:20,730 --> 00:45:22,330
때문입니다.

671
00:45:22,330 --> 00:45:24,220
공식이 있습니다.

672
00:45:24,220 --> 00:45:30,270
q에 대한 x의 부분 미분은 1이고, y에 대한

673
00:45:30,270 --> 00:45:33,160
부분 미분도 1입니다.

674
00:45:33,160 --> 00:45:35,830
그래서 이것은 간단한 설정입니다.

675
00:45:35,830 --> 00:45:37,480
우리는 그것이 존재한다는 것을 알고 있습니다.

676
00:45:37,480 --> 00:45:40,000
그러니 마음속에 두세요.

677
00:45:40,000 --> 00:45:46,820
그 다음 연산은 f는 q 곱하기 z입니다.

678
00:45:46,820 --> 00:45:48,980
다시 말해, 이 함수가

679
00:45:48,980 --> 00:45:54,490
있으므로 부분 미분을 쓰는 것이 매우 쉽습니다.

680
00:45:54,490 --> 00:45:56,710
f에 대한 q의 부분 미분은 z입니다.

681
00:45:56,710 --> 00:46:01,490
그리고 f에 대한 z의 부분 미분은 q입니다.

682
00:46:01,490 --> 00:46:04,305
그래서 z와 q가 서로 바뀝니다.

683
00:46:07,900 --> 00:46:09,640
모두가 선형 대수학에

684
00:46:09,640 --> 00:46:11,600
대해 알고 있기를 바랍니다.

685
00:46:11,600 --> 00:46:14,830
모르신다면 꼭 확인하고

686
00:46:14,830 --> 00:46:18,490
상기시켜야 합니다. 왜냐하면

687
00:46:18,490 --> 00:46:23,860
이것들은 이번 분기 나머지에 매우 중요한

688
00:46:23,860 --> 00:46:26,930
대수이기 때문입니다.

689
00:46:26,930 --> 00:46:30,860
우리가 이 설정에서 원하는

690
00:46:30,860 --> 00:46:34,070
것과 필요한 것은

691
00:46:34,070 --> 00:46:37,930
f에 대한 x, y,

692
00:46:37,930 --> 00:46:41,753
z의 부분 미분입니다.

693
00:46:44,400 --> 00:46:47,730
우리가 시작하는 방법과 역전파가 이를

694
00:46:47,730 --> 00:46:50,580
구현하는 방법은 네트워크의 앞쪽, 즉

695
00:46:50,580 --> 00:46:52,480
끝에서 시작하는 것입니다.

696
00:46:52,480 --> 00:46:58,290
그리고 우리는 뒤로 가서 모든 그래디언트를

697
00:46:58,290 --> 00:47:00,520
역전파합니다.

698
00:47:00,520 --> 00:47:04,920
이것은 기본적으로 실행될 재귀

699
00:47:04,920 --> 00:47:07,200
프로세스입니다.

700
00:47:07,200 --> 00:47:15,450
f에 대한 f의 미분은 무엇인가요?

701
00:47:15,450 --> 00:47:17,650
그것은 자기 자신에 대한 것입니다.

702
00:47:17,650 --> 00:47:23,100
그래서 항상 마지막 부분, 손실

703
00:47:23,100 --> 00:47:28,270
함수에 대한 미분은 항상 1입니다.

704
00:47:28,270 --> 00:47:36,070
역전파를 원한다면 가장 즉각적인 것은 z입니다.

705
00:47:36,070 --> 00:47:39,720
여기서 z가 있습니다.

706
00:47:39,720 --> 00:47:44,510
이 경우 f에 대한 z의 미분을 계산하면

707
00:47:44,510 --> 00:47:47,540
이미 가지고 있습니다.

708
00:47:47,540 --> 00:47:50,730
f에 대한 z는 q와 같습니다.

709
00:47:50,730 --> 00:48:00,020
따라서 q의 값이 무엇이든 이 그래디언트로

710
00:48:00,020 --> 00:48:01,400
전달됩니다.

711
00:48:01,400 --> 00:48:03,880
다음은 q입니다.

712
00:48:03,880 --> 00:48:08,700
Q는 다음 것, 즉 f와 직접 연결된 다음 것입니다.

713
00:48:08,700 --> 00:48:12,980
따라서 이는 q에 대한 f의 도함수를 가지고 있기 때문에

714
00:48:12,980 --> 00:48:14,310
계산하기 쉽습니다.

715
00:48:14,310 --> 00:48:18,620
우리는 이미 그것이 z와 같다는 것을 계산했습니다.

716
00:48:18,620 --> 00:48:23,870
z가 무엇이든, 여기서의 도함수 값은 -4입니다.

717
00:48:23,870 --> 00:48:29,570
다음은 q 바로 앞에 있는 y입니다.

718
00:48:29,570 --> 00:48:33,650
y와 f는 연결되어 있지 않지만,

719
00:48:33,650 --> 00:48:37,170
y에 대한 f의 도함수가

720
00:48:37,170 --> 00:48:39,220
필요합니다.

721
00:48:39,220 --> 00:48:44,640
그래서 우리는 체인 규칙을

722
00:48:44,640 --> 00:48:51,840
사용하여 중간 변수에 대한 도함수 계산을

723
00:48:51,840 --> 00:48:53,950
나눕니다.

724
00:48:53,950 --> 00:49:01,510
따라서 y에 대한 부분 f는 q에 대한 부분 f와 q에 대한 y의 곱입니다.

725
00:49:01,510 --> 00:49:09,580
이것이 이 경우 체인 규칙을 작성하는 방법입니다.

726
00:49:09,580 --> 00:49:13,890
이제 두 가지 중요한 새로운 용어를 소개하고 싶습니다: 로컬

727
00:49:13,890 --> 00:49:16,660
그래디언트와 업스트림 그래디언트입니다.

728
00:49:16,660 --> 00:49:18,930
업스트림

729
00:49:18,930 --> 00:49:27,930
그래디언트는 종종 네트워크의 끝에서 현재 노드로 오는 그래디언트입니다.

730
00:49:27,930 --> 00:49:29,140
그래디언트는 종종 네트워크의 끝에서 현재 노드로 오는 그래디언트입니다.

731
00:49:29,140 --> 00:49:35,270
그리고 로컬 그래디언트는

732
00:49:35,270 --> 00:49:39,380
노드의 입력과

733
00:49:39,380 --> 00:49:45,110
출력에 대한 그래디언트입니다.

734
00:49:45,110 --> 00:49:46,200
출력에 대한 그래디언트입니다.

735
00:49:46,200 --> 00:49:48,240
그래서 이것이 로컬 그래디언트입니다.

736
00:49:48,240 --> 00:49:53,420
이들을 정의하는 것은 실제로 어렵지 않습니다. 왜냐하면 q에 대한 f의 값이

737
00:49:53,420 --> 00:49:55,010
이미 있기 때문입니다.

738
00:49:55,010 --> 00:49:59,310
y에 대한 q의 값도 이미 있습니다.

739
00:49:59,310 --> 00:50:06,900
그래서 1에 z를 곱하면 값은 -4가 됩니다.

740
00:50:06,900 --> 00:50:13,860
같은 이야기이고, 다른 변수 x에 대해서도 마찬가지입니다.

741
00:50:13,860 --> 00:50:18,500
여기서 로컬 그래디언트 업스트림은 다시

742
00:50:18,500 --> 00:50:22,830
이 체인 규칙으로 작성될 수 있습니다.

743
00:50:22,830 --> 00:50:29,810
그리고 이것은 -4로 결과가 나오며, 두 경우

744
00:50:29,810 --> 00:50:36,400
모두 x 또는 y에 대한 그래디언트는 이미

745
00:50:36,400 --> 00:50:38,390
1이었습니다.

746
00:50:38,390 --> 00:50:41,050
그래서 두 값이 동일합니다.

747
00:50:41,050 --> 00:50:45,380
이 계산 설정과 계산 그래프 덕분에 우리가

748
00:50:45,380 --> 00:50:50,020
하고자 하는 일을 모듈화하기 매우 쉬워집니다.

749
00:50:50,020 --> 00:50:54,790
신경망의 모든 노드에 대해 x와 y를

750
00:50:54,790 --> 00:50:58,670
입력으로, 또는 다른 무엇이든,

751
00:50:58,670 --> 00:51:06,070
z를 출력으로 가질 때, 우리가 필요한 것은 먼저 로컬

752
00:51:06,070 --> 00:51:08,410
그래디언트입니다.

753
00:51:08,410 --> 00:51:11,180
우리는 항상 함수 f를 가지고 있습니다.

754
00:51:11,180 --> 00:51:13,520
x와 y의 함수입니다.

755
00:51:13,520 --> 00:51:19,520
따라서 각 입력에 대한 출력의 그래디언트를 계산하는 것은

756
00:51:19,520 --> 00:51:22,730
모든 노드에 대해 쉽습니다.

757
00:51:22,730 --> 00:51:27,220
그리고 우리가 역전파를 위해 필요한 것은

758
00:51:27,220 --> 00:51:30,280
업스트림 그래디언트입니다.

759
00:51:30,280 --> 00:51:33,450
역전파 과정은 이 업스트림 그래디언트를

760
00:51:33,450 --> 00:51:37,210
단계별로 얻을 수 있는 힘을 제공합니다.

761
00:51:37,210 --> 00:51:39,750
따라서 이 노드에 있을 때,

762
00:51:39,750 --> 00:51:41,880
우리는 이미 미래 노드에서

763
00:51:41,880 --> 00:51:45,700
계산된 업스트림 그래디언트를 가지고 있습니다.

764
00:51:45,700 --> 00:51:51,630
그리고 그것이 우리가 필요한 것입니다.

765
00:51:51,630 --> 00:51:54,330
이후에 할 수 있는 것은 업스트림

766
00:51:54,330 --> 00:51:58,650
그래디언트와 로컬 그래디언트를 곱하여 이제 우리가

767
00:51:58,650 --> 00:52:03,520
다운스트림 그래디언트라고 부르는 것을 만드는 것입니다.

768
00:52:03,520 --> 00:52:05,250
따라서 다운스트림 그래디언트는

769
00:52:05,250 --> 00:52:08,530
이전 레이어에 대한 업스트림 그래디언트가 될 것입니다.

770
00:52:08,530 --> 00:52:11,650
그래서 x에 대해 그렇게 계산하고,

771
00:52:11,650 --> 00:52:15,660
y에 대해서도 같은 이야기입니다.

772
00:52:15,660 --> 00:52:19,710
그래서 이 전체 과정은 우리가 모든 것을 완전히

773
00:52:19,710 --> 00:52:23,250
로컬에서 생성하고 계산할 수 있는 힘을

774
00:52:23,250 --> 00:52:26,340
주며, 단계별로 거꾸로 가서 이전

775
00:52:26,340 --> 00:52:28,460
노드에 전달하여 그들이

776
00:52:28,460 --> 00:52:30,840
계속할 수 있도록 합니다.

777
00:52:30,840 --> 00:52:36,770
그래서 다시 말하지만, 이것은 모든 신경망과

778
00:52:36,770 --> 00:52:41,570
여러 정보 층을 포함하는 많은 최적화

779
00:52:41,570 --> 00:52:47,000
과정에서 가장 기본적인 작업 중 하나입니다.

780
00:52:47,000 --> 00:52:50,040
질문을 제대로 이해했다면, 우리는 직관적으로 기울기가 무엇을

781
00:52:50,040 --> 00:52:51,740
하고 있는지 이해할 수 있는

782
00:52:51,740 --> 00:52:53,070
방법을 묻고 있는 것입니다.

783
00:52:53,070 --> 00:52:58,010
그래서 한 걸음 물러서서 우리가 왜 여기

784
00:52:58,010 --> 00:53:00,270
있는지 살펴보겠습니다.

785
00:53:00,270 --> 00:53:03,650
우리가 필요했던 것은 W1과

786
00:53:03,650 --> 00:53:06,050
W2, 일반적으로 W에

787
00:53:06,050 --> 00:53:09,690
대한 손실 함수의 기울기를 계산하는

788
00:53:09,690 --> 00:53:15,920
것이었고, 기울기의 반대 방향으로 최적의 값, 최적의

789
00:53:15,920 --> 00:53:17,960
손실을 찾기 위해 한

790
00:53:17,960 --> 00:53:21,750
걸음 나아갈 수 있어야 했습니다.

791
00:53:21,750 --> 00:53:26,980
그래서 이를 위해 우리는 모든 것에 대한 L 손실의

792
00:53:26,980 --> 00:53:28,850
기울기가 필요합니다.

793
00:53:28,850 --> 00:53:33,220
우리가 하는 것은 네트워크의 모든 변수에 대한 L의

794
00:53:33,220 --> 00:53:35,720
기울기를 네트워크의 모든 값으로

795
00:53:35,720 --> 00:53:39,380
이동시키는 것이며, 전체 네트워크에

796
00:53:39,380 --> 00:53:42,010
대한 함수를 앉아서 작성하지 않고도

797
00:53:42,010 --> 00:53:43,190
가능합니다.

798
00:53:43,190 --> 00:53:46,698
네트워크에 100개의 층이 있다면, 우리는 100개의

799
00:53:46,698 --> 00:53:49,240
층 각각에 대한 함수를 앉아서

800
00:53:49,240 --> 00:53:51,020
작성하지 않을 것입니다.

801
00:53:51,020 --> 00:53:53,590
이것이 우리가 단계별로

802
00:53:53,590 --> 00:53:57,130
역전파하여 네트워크에 통합될 모든

803
00:53:57,130 --> 00:54:00,190
가중치의 최적화 과정에 필요한

804
00:54:00,190 --> 00:54:02,380
값을 얻는 방법입니다.

805
00:54:02,380 --> 00:54:11,320
좋아요, 또 다른 예시입니다. 이것은 가중치와 x의

806
00:54:11,320 --> 00:54:15,860
조금 더 복잡한 함수입니다.

807
00:54:15,860 --> 00:54:20,440
그리고 우리는 1을 1 더하기 e의 x와 w의

808
00:54:20,440 --> 00:54:23,850
선형 조합의 거듭제곱으로 나누었습니다.

809
00:54:23,850 --> 00:54:30,460
그래서 곱셈, 덧셈, 부정, exp 함수가

810
00:54:30,460 --> 00:54:36,180
있으며, 궁극적으로 우리가 계산한

811
00:54:36,180 --> 00:54:40,510
함수의 1로 나누어집니다.

812
00:54:40,510 --> 00:54:47,070
이 모든 것을 가지고, W0, x0, W1, x1, W2에

813
00:54:47,070 --> 00:54:52,840
대한 특정 값을 가진 이 예제를 살펴보겠습니다.

814
00:54:52,840 --> 00:54:56,980
이 주어진 값으로 우리는 순방향 패스를 수행하고

815
00:54:56,980 --> 00:55:02,050
이 과정에서 우리가 가진 모든 값을 계산할 수 있습니다.

816
00:55:02,050 --> 00:55:05,940
그리고 여러분에게 상기시키기 위해,

817
00:55:05,940 --> 00:55:08,220
우리는 exp 함수에

818
00:55:08,220 --> 00:55:13,840
대한 몇 가지 세부 사항이 있으며, e의 x에 대한

819
00:55:13,840 --> 00:55:16,710
도함수가 x에 대한 상수 곱셈이

820
00:55:16,710 --> 00:55:19,570
무엇인지 알고 있습니다.

821
00:55:19,570 --> 00:55:21,990
항상 도함수는 상수

822
00:55:21,990 --> 00:55:27,500
값 자체인 1/x의 도함수는 -1/x²입니다.

823
00:55:27,500 --> 00:55:30,726
이것들은 다시 말하지만, 우리가 대수에서 아는 것입니다.

824
00:55:30,726 --> 00:55:34,130
그리고 상수 덧셈일 경우, 항상

825
00:55:34,130 --> 00:55:36,900
도함수는 1과 같습니다.

826
00:55:36,900 --> 00:55:41,330
그래서 제가 말했듯이, 네트워크의

827
00:55:41,330 --> 00:55:45,320
시작과 끝에서 L에 대한

828
00:55:45,320 --> 00:55:48,960
도함수는 항상 1과 같습니다.

829
00:55:48,960 --> 00:55:54,050
그래서 여기서 우리는 1/x 함수의 도함수를

830
00:55:54,050 --> 00:55:56,850
사용하는 것입니다.

831
00:55:56,850 --> 00:55:59,630
이제 우리는 계산할 수 있습니다.

832
00:55:59,630 --> 00:56:02,330
상류에서는 항상 1입니다.

833
00:56:02,330 --> 00:56:07,470
로컬 기울기는 -1/x²일 수 있습니다.

834
00:56:07,470 --> 00:56:09,170
x의 값은 무엇입니까?

835
00:56:09,170 --> 00:56:10,440
입력이 무엇이든 상관없습니다.

836
00:56:10,440 --> 00:56:16,650
그래서 이 계산의 결과는 -0.53입니다.

837
00:56:16,650 --> 00:56:20,370
-0.53은 다음 것에 대한 상류 기울기를

838
00:56:20,370 --> 00:56:22,960
정의하는 하류 기울기입니다.

839
00:56:22,960 --> 00:56:25,570
그리고 다음 단계에서, 여기의

840
00:56:25,570 --> 00:56:28,160
함수는 상수 덧셈이며, 우리는

841
00:56:28,160 --> 00:56:32,780
로컬 기울기가 1과 같다는 것을 알고 있습니다.

842
00:56:32,780 --> 00:56:39,010
그래서 1에 상류 기울기를 곱하면 같은 값이 다시 돌아갑니다.

843
00:56:39,010 --> 00:56:43,820
다음 단계는 exp 함수입니다.

844
00:56:43,820 --> 00:56:48,230
그래서 그에 대해, 다시 상류에서 우리는 이미 값을

845
00:56:48,230 --> 00:56:49,750
가지고 있습니다.

846
00:56:49,750 --> 00:56:54,770
로컬 기울기는 e의 x의 거듭제곱입니다.

847
00:56:54,770 --> 00:56:55,390
x는 무엇인가요?

848
00:56:55,390 --> 00:56:59,390
이 단계의 입력에서 1을 뺀 값입니다.

849
00:56:59,390 --> 00:57:03,860
그래서 이를 계산하면 -0.2가 됩니다.

850
00:57:03,860 --> 00:57:08,180
그리고 이것은 다음 단계로 돌아갑니다.

851
00:57:08,180 --> 00:57:11,950
여기서 우리는 다시 상수와의

852
00:57:11,950 --> 00:57:16,030
곱셈이 있으며, 이는 지역 기울기를

853
00:57:16,030 --> 00:57:21,220
그 숫자 또는 상수 값과 같게 정의하고

854
00:57:21,220 --> 00:57:26,290
새로운 기울기, 하류 기울기를 정의합니다.

855
00:57:26,290 --> 00:57:30,510
그리고 돌아가면,

856
00:57:30,510 --> 00:57:37,350
여기에는 두 개의 서로 다른 값의 입력을

857
00:57:37,350 --> 00:57:42,610
받는 덧셈 함수가 있습니다.

858
00:57:42,610 --> 00:57:45,660
다시 말해, 상류 기울기를 계산하고 싶다면 0.

859
00:57:45,660 --> 00:57:46,320
2입니다.

860
00:57:46,320 --> 00:57:47,490
이미 우리는 그것을 가지고 있습니다.

861
00:57:47,490 --> 00:57:54,720
하류, 지역 기울기는 두 값 간의 덧셈이므로

862
00:57:54,720 --> 00:57:57,240
1이 됩니다.

863
00:57:57,240 --> 00:58:00,180
x와 y에 대한

864
00:58:00,180 --> 00:58:04,660
x+y의 도함수는 항상 1입니다.

865
00:58:04,660 --> 00:58:06,960
그래서 두 입력은 동일할 것입니다.

866
00:58:06,960 --> 00:58:07,920
그런

867
00:58:07,920 --> 00:58:11,070
다음 우리는 상류 기울기와

868
00:58:11,070 --> 00:58:14,470
곱셈 연산을 가지고 있습니다.

869
00:58:14,470 --> 00:58:15,930
다시 우리는 값을 가지고 있습니다.

870
00:58:15,930 --> 00:58:21,500
곱셈에 대한 지역 기울기는 항상 예를

871
00:58:21,500 --> 00:58:24,870
들어 a와 x를 곱할

872
00:58:24,870 --> 00:58:28,880
때, x에 대한 도함수는

873
00:58:28,880 --> 00:58:31,850
항상 다른 변수입니다.

874
00:58:31,850 --> 00:58:37,880
그래서 여기서 첫 번째는 -1, 즉 x의

875
00:58:37,880 --> 00:58:39,240
값입니다.

876
00:58:39,240 --> 00:58:42,720
두 번째는 2, 즉 w의 값입니다.

877
00:58:42,720 --> 00:58:46,140
그래서 다른 변수는 어떤 값을 가지고 있든지 간에.

878
00:58:46,140 --> 00:58:50,000
그래서 이를 통해 모든 것을 계산할 수 있고

879
00:58:50,000 --> 00:58:54,060
W1과 x1에 대한 것도 계산할 수 있습니다.

880
00:58:54,060 --> 00:58:56,820
다시 말해, 우리는 이러한

881
00:58:56,820 --> 00:59:01,970
모든 계산을 했으므로 W가 네트워크의

882
00:59:01,970 --> 00:59:08,810
최적 지점으로 나아가기 위해 얼마나 변경되어야 하는지 식별할

883
00:59:08,810 --> 00:59:10,790
수 있습니다.

884
00:59:10,790 --> 00:59:13,350
그래서 이것은 또 다른 예였습니다.

885
00:59:13,350 --> 00:59:19,490
계산 그래프를 그리는 방법은 정말 많습니다.

886
00:59:19,490 --> 00:59:21,590
이것이 제가 설명한 유일한 방법은 아닙니다.

887
00:59:21,590 --> 00:59:24,400
그래서 우리는 실제로 모든 함수를 함께

888
00:59:24,400 --> 00:59:28,510
묶고 시그모이드를 정의할 수 있습니다. 왜냐하면 이것은 기본적으로

889
00:59:28,510 --> 00:59:31,310
선형 함수의 시그모이드이기 때문입니다.

890
00:59:31,310 --> 00:59:33,770
그래서 선형 함수는 여기 있을 수 있습니다.

891
00:59:33,770 --> 00:59:38,150
그리고 이러한 모든 연산은 시그모이드로 정의될 수 있습니다.

892
00:59:38,150 --> 00:59:41,080
실제로 시그모이드는 흥미롭고 매우 유용하게

893
00:59:41,080 --> 00:59:46,330
사용할 수 있습니다. 왜냐하면 시그모이드를 사용할 때의 지역 기울기는
시그모이드

894
00:59:46,330 --> 00:59:48,530
자체에 의존하기 때문입니다.

895
00:59:48,530 --> 00:59:53,750
그래서 x에 대한 시그모이드의 지역 기울기는

896
00:59:53,750 --> 00:59:56,600
계산하고 단순화하면 1 -

897
00:59:56,600 --> 01:00:01,900
시그모이드 곱하기 같은 x의 시그모이드입니다.

898
01:00:01,900 --> 01:00:10,030
그래서 이는 실제로 매우 유용한 프레임워크, 유용한 함수이며

899
01:00:10,030 --> 01:00:10,910
쉽습니다.

900
01:00:10,910 --> 01:00:18,040
그리고 하류 기울기를 계산하기 위해, 다시 말해 상류

901
01:00:18,040 --> 01:00:21,100
기울기가 1이었습니다.

902
01:00:21,100 --> 01:00:24,270
그리고 제가 지역 기울기를

903
01:00:24,270 --> 01:00:28,240
계산하면, 이 함수에서 x를

904
01:00:28,240 --> 01:00:34,050
입력값인 1로 대체하면, 이는 1로 곱해져 0.

905
01:00:34,050 --> 01:00:35,290
2가 됩니다.

906
01:00:35,290 --> 01:00:37,170
이는 실제로 이전에

907
01:00:37,170 --> 01:00:42,150
별도로 계산했을 때와 정확히 같은 값입니다.

908
01:00:42,150 --> 01:00:47,490
저는 요약하고 싶습니다. 데이터에는 몇

909
01:00:47,490 --> 01:00:55,980
가지 패턴이 있으며, 종종 우리가 실제로 기억할 수 있는

910
01:00:55,980 --> 01:00:59,970
노드에서 매우 많이 나타납니다.

911
01:00:59,970 --> 01:01:02,590
더하기 게이트를 위한 더하기 게이트가 있습니다.

912
01:01:02,590 --> 01:01:05,850
항상 기울기 분배기입니다.

913
01:01:05,850 --> 01:01:10,770
내가 설명한 덧셈의 특성 때문에,

914
01:01:10,770 --> 01:01:18,710
기울기는 입력이 무엇이든 동일하게 유지됩니다.

915
01:01:18,710 --> 01:01:23,850
곱셈 게이트의 경우, 스왑 함수입니다.

916
01:01:23,850 --> 01:01:27,890
다시 말하지만, xy에 대한 x의 기울기는 y이고,

917
01:01:27,890 --> 01:01:30,090
y에 대한 기울기는 x입니다.

918
01:01:30,090 --> 01:01:31,710
그래서 스왑입니다.

919
01:01:31,710 --> 01:01:37,040
그리고 복사 게이트가 있습니다.

920
01:01:37,040 --> 01:01:41,000
복사 게이트에서 발생하는 작업은

921
01:01:41,000 --> 01:01:47,390
네트워크에 들어오는 것의 덧셈입니다--

922
01:01:47,390 --> 01:01:49,980
노드 또는 게이트에.

923
01:01:49,980 --> 01:01:53,600
궁극적으로, 최대 게이트가 있으며, 이는 실제로

924
01:01:53,600 --> 01:01:55,910
우리가 자주 사용하는 것으로

925
01:01:55,910 --> 01:01:58,010
ReLU 함수와 매우 유사합니다.

926
01:01:58,010 --> 01:02:02,030
그 최대 게이트는-- 입력 간의 최대값을

927
01:02:02,030 --> 01:02:06,390
취하기 때문에 기울기를 가지고 있습니다.

928
01:02:06,390 --> 01:02:11,080
그래서 최대값이 무엇이든,

929
01:02:11,080 --> 01:02:15,880
기울기를 그 방향으로 라우팅합니다.

930
01:02:15,880 --> 01:02:21,520
그로 인해 이제 신경망을 구현하는 것이 매우 간단해졌습니다.

931
01:02:21,520 --> 01:02:25,880
순전파를 수행하고 모든 단계를 계산합니다.

932
01:02:25,880 --> 01:02:27,610
그리고 역전파에서

933
01:02:27,610 --> 01:02:29,480
기울기를 계산하기 시작합니다.

934
01:02:29,480 --> 01:02:32,110
단계별로, 손실 함수의

935
01:02:32,110 --> 01:02:38,120
기울기가 항상 1이라는 것을 설명했습니다.

936
01:02:38,120 --> 01:02:43,190
그리고 네트워크의 끝에서 시작하여 위로 올라갑니다.

937
01:02:43,190 --> 01:02:45,410
여기서 위로 올라가는 것을 볼 수 있습니다.

938
01:02:45,410 --> 01:02:49,360
그래서 이것은 기울기를 계산하는 시그모이드 함수입니다,

939
01:02:49,360 --> 01:02:50,900
그리고 위로 올라갑니다.

940
01:02:50,900 --> 01:02:52,760
그것이 더하기 게이트였습니다.

941
01:02:52,760 --> 01:02:54,500
또 다른 더하기 게이트가 있었습니다.

942
01:02:54,500 --> 01:02:58,190
그리고 두 개의 곱셈 게이트가 있었으며,

943
01:02:58,190 --> 01:03:03,710
이는 기본적으로 매우 간단하게 구현을 제공합니다.

944
01:03:03,710 --> 01:03:07,390
이러한 유형의 공식화로,

945
01:03:07,390 --> 01:03:12,960
우리는 신경망의 모든 기능을

946
01:03:12,960 --> 01:03:20,400
모듈화하고 필요한 모든 기능에 대한 순전파

947
01:03:20,400 --> 01:03:23,280
및 역전파

948
01:03:23,280 --> 01:03:25,350
API를 생성할

949
01:03:25,350 --> 01:03:27,640
수 있습니다.

950
01:03:27,640 --> 01:03:31,710
이 경우, 이것은 곱셈 게이트로--

951
01:03:31,710 --> 01:03:33,810
곱셈을 위해서는

952
01:03:33,810 --> 01:03:39,190
역전파에서 사용할 입력에 접근해야 합니다.

953
01:03:39,190 --> 01:03:42,750
우리는 종종 그것들을 저장하고 기억하지만,

954
01:03:42,750 --> 01:03:45,480
그런 다음 순전파 값을

955
01:03:45,480 --> 01:03:48,600
계산하고 역전파에서 기울기를 계산합니다.

956
01:03:48,600 --> 01:03:53,850
그래서 이것은 우리가 우리의 함수를 작성하고 모든 순전파 및 역전파를

957
01:03:53,850 --> 01:03:56,350
포함할 수 있음을 의미합니다.

958
01:03:56,350 --> 01:04:00,580
현재 PyTorch 연산자는 이렇게 생겼습니다.

959
01:04:00,580 --> 01:04:02,260
예를 들어, 시그모이드 레이어를

960
01:04:02,260 --> 01:04:04,120
보면, 그것은 단순히 순전파입니다.

961
01:04:04,120 --> 01:04:08,070
비록 이 특정 함수에서는 구현되지 않았습니다.

962
01:04:08,070 --> 01:04:11,630
C++ 코드의 다른 곳, C 코드에서 실제로

963
01:04:11,630 --> 01:04:14,520
PyTorch에 구현되어 있습니다.

964
01:04:14,520 --> 01:04:16,940
그러나 시그모이드의 역전파도

965
01:04:16,940 --> 01:04:18,950
우리가 방금 이야기한 동일한

966
01:04:18,950 --> 01:04:20,995
함수를 계산하고 있습니다.

967
01:04:27,500 --> 01:04:31,140
지금까지 우리가 말한 것--

968
01:04:31,140 --> 01:04:33,530
그리고 나는

969
01:04:33,530 --> 01:04:40,260
스칼라 값을 사용하는 대부분의 예제를 다루었습니다.

970
01:04:40,260 --> 01:04:43,560
모든 예제는 단지 스칼라 값이었습니다.

971
01:04:43,560 --> 01:04:48,410
하지만 우리는 이러한 모든 연산이 실제로

972
01:04:48,410 --> 01:04:52,500
벡터 또는 행렬 형태로 구현될 수 있다는

973
01:04:52,500 --> 01:04:58,700
것을 알고 있습니다, 여기서 그 부분을 확장합니다.

974
01:04:58,700 --> 01:05:03,080
우리는 지금까지 스칼라에서 스칼라 설정에 대해

975
01:05:03,080 --> 01:05:07,090
이야기했습니다. 입력 x와 y가 스칼라인

976
01:05:07,090 --> 01:05:08,240
경우입니다.

977
01:05:08,240 --> 01:05:13,070
따라서 도함수도 스칼라가 됩니다.

978
01:05:13,070 --> 01:05:17,830
즉, x를 조금 변화시키면 y의 값이 얼마나

979
01:05:17,830 --> 01:05:20,890
변하는지를 의미합니다.

980
01:05:20,890 --> 01:05:24,070
이제 벡터화되었고,

981
01:05:24,070 --> 01:05:28,360
x가 n 요소의 벡터이고 y가

982
01:05:28,360 --> 01:05:33,770
스칼라인 경우, 벡터에서 스칼라로의

983
01:05:33,770 --> 01:05:37,060
도함수는 이 경우에도 벡터가

984
01:05:37,060 --> 01:05:38,270
됩니다.

985
01:05:38,270 --> 01:05:42,760
그리고 그 벡터의 각 요소는 x를

986
01:05:42,760 --> 01:05:48,440
조금 변화시키면 y의 양이 얼마나 변하는지를

987
01:05:48,440 --> 01:05:51,740
의미하며, y의 전체 양은 단일

988
01:05:51,740 --> 01:05:53,770
값이기 때문에

989
01:05:53,770 --> 01:05:55,160
그렇습니다.

990
01:05:55,160 --> 01:05:59,380
또한 x와 y가 임의의 크기의

991
01:05:59,380 --> 01:06:05,430
벡터인 벡터에서 벡터로의 프레임워크도 있습니다.

992
01:06:05,430 --> 01:06:09,300
이 경우 도함수는 행렬을

993
01:06:09,300 --> 01:06:14,130
형성하며, 이를 야코비안이라고 부릅니다.

994
01:06:14,130 --> 01:06:20,250
x의 각 요소가 조금 변화하면 이

995
01:06:20,250 --> 01:06:23,520
도함수는 y의 각

996
01:06:23,520 --> 01:06:27,720
요소가 얼마나 변할지를

997
01:06:27,720 --> 01:06:29,830
알려줍니다.

998
01:06:29,830 --> 01:06:36,365
다시 말해, 여기 스크립트를 보면 완전히 같지

999
01:06:36,365 --> 01:06:37,240
않습니다.

1000
01:06:37,240 --> 01:06:38,770
서로 다를 수 있습니다.

1001
01:06:38,770 --> 01:06:42,360
이 야코비안의 각 요소에는

1002
01:06:42,360 --> 01:06:44,950
명확한 의미가 있습니다.

1003
01:06:44,950 --> 01:06:51,750
그리고 벡터로 역전파를 수행하는 방법은

1004
01:06:51,750 --> 01:06:57,190
x, y, z가 각각 dx,

1005
01:06:57,190 --> 01:07:04,790
dy, dz 크기의 벡터라고 가정할 때입니다.

1006
01:07:04,790 --> 01:07:11,090
다시 말해, 손실 도함수 L은 항상 스칼라입니다. 왜냐하면

1007
01:07:11,090 --> 01:07:13,040
우리가 최소화하고자

1008
01:07:13,040 --> 01:07:16,650
하는 값은 항상 하나이기 때문입니다.

1009
01:07:16,650 --> 01:07:21,080
그러나 상류 그래디언트를

1010
01:07:21,080 --> 01:07:29,360
계산하면 변수 z와 같은 크기의 벡터 dz가 생성됩니다.

1011
01:07:29,360 --> 01:07:34,940
하류 그래디언트에서도 같은 일이 발생합니다.

1012
01:07:34,940 --> 01:07:38,700
하류 그래디언트에서는--

1013
01:07:38,700 --> 01:07:41,000
사실 하류로

1014
01:07:41,000 --> 01:07:43,700
가기 전에, x와

1015
01:07:43,700 --> 01:07:52,730
y에 대한 z의 국소 그래디언트에 대해 조금 말씀드리겠습니다.

1016
01:07:52,730 --> 01:07:55,160
이 경우, 제가 말한

1017
01:07:55,160 --> 01:07:58,910
부분이 야코비안이 될 것이며, 이제

1018
01:07:58,910 --> 01:08:02,440
그래디언트는 행렬로 변환됩니다.

1019
01:08:02,440 --> 01:08:06,240
그래서 입력 크기와 출력

1020
01:08:06,240 --> 01:08:10,260
크기를 곱한 두 개의

1021
01:08:10,260 --> 01:08:14,220
야코비안 행렬이 정의됩니다.

1022
01:08:14,220 --> 01:08:21,540
그리고 이것은 상류와 국소 그래디언트의 곱으로

1023
01:08:21,540 --> 01:08:27,189
하류 그래디언트를 생성합니다.

1024
01:08:27,189 --> 01:08:33,220
그곳에서 우리는 입력 x 자체와 같은 크기를 얻습니다.

1025
01:08:33,220 --> 01:08:37,080
그래서 우리는 다시 벡터를 가지게

1026
01:08:37,080 --> 01:08:40,020
됩니다. 입력이 벡터였기

1027
01:08:40,020 --> 01:08:45,300
때문에 그래디언트 측면에서 같은 크기입니다.

1028
01:08:45,300 --> 01:08:48,660
변수의 그래디언트는 항상

1029
01:08:48,660 --> 01:08:51,990
원래 변수와 동일한 차원을

1030
01:08:51,990 --> 01:08:57,180
가지며, 이 슬라이드에서도 보여집니다.

1031
01:08:57,180 --> 01:09:07,830
그래서 벡터로 역전파는 이것이었습니다. 여기 하나의 예입니다.

1032
01:09:07,830 --> 01:09:11,550
함수 max(0, x)가 있다고 가정해 보겠습니다.

1033
01:09:11,550 --> 01:09:14,430
이것이 ReLU 함수입니다.

1034
01:09:14,430 --> 01:09:18,660
이것은 입력을 받아 0과의 최대값을 취하는 요소별

1035
01:09:18,660 --> 01:09:19,760
함수입니다.

1036
01:09:19,760 --> 01:09:21,090
그리고 비제로인

1037
01:09:21,090 --> 01:09:24,590
경우-- 비부정적인 경우에는 통과하고, 그렇지

1038
01:09:24,590 --> 01:09:26,729
않으면 0으로 대체합니다.

1039
01:09:26,729 --> 01:09:29,819
상류 그래디언트를 얻었다고 가정하고

1040
01:09:29,819 --> 01:09:34,010
이제 여기에서 야코비안 행렬을 구축해야 합니다.

1041
01:09:34,010 --> 01:09:36,479
이 야코비안 행렬은

1042
01:09:36,479 --> 01:09:40,140
요소별 연산이기 때문에 다른

1043
01:09:40,140 --> 01:09:45,890
입력에 대한 의존성이 없고, 오직 값 자체에

1044
01:09:45,890 --> 01:09:48,990
대한 의존성만 있습니다.

1045
01:09:48,990 --> 01:09:52,700
이것은 매우 희소한 행렬로, 주

1046
01:09:52,700 --> 01:09:55,050
대각선에만 값이 있습니다.

1047
01:09:55,050 --> 01:09:59,900
그 값은 실제로 0 또는 1이며,

1048
01:09:59,900 --> 01:10:04,400
최대값이 실제로 취해졌는지,

1049
01:10:04,400 --> 01:10:07,940
값이 통과했는지, 아니면

1050
01:10:07,940 --> 01:10:13,930
0으로 대체되었는지에 따라 다릅니다.

1051
01:10:13,930 --> 01:10:16,900
상류 그래디언트와 곱하면

1052
01:10:16,900 --> 01:10:19,700
하류 그래디언트를 얻습니다.

1053
01:10:19,700 --> 01:10:25,700
그리고 이렇게 계산이 이루어집니다.

1054
01:10:25,700 --> 01:10:31,100
제가 말했듯이, 여기에서 야코비안은 희소합니다. 왜냐하면 이

1055
01:10:31,100 --> 01:10:34,180
경우 연산이 요소별이기 때문입니다.

1056
01:10:34,180 --> 01:10:37,390
그래서 실제로는 역전파에서

1057
01:10:37,390 --> 01:10:40,870
그 거대한 희소 야코비안

1058
01:10:40,870 --> 01:10:44,590
행렬을 계산하는 대신, 우리는

1059
01:10:44,590 --> 01:10:48,100
이 최대 함수의 기울기를

1060
01:10:48,100 --> 01:10:50,960
규칙 기반으로 계산합니다.

1061
01:10:50,960 --> 01:10:53,800
그래서 우리는 그 행렬을

1062
01:10:53,800 --> 01:10:56,910
저장하지 않고 계산하지 않습니다.

1063
01:10:56,910 --> 01:11:01,020
함수가 어떻게 작동하는지 알기 때문입니다.

1064
01:11:01,020 --> 01:11:03,930
그리고 이것은 행렬과

1065
01:11:03,930 --> 01:11:07,870
텐서로도 확장될 수 있습니다.

1066
01:11:07,870 --> 01:11:09,840
입력이 벡터가

1067
01:11:09,840 --> 01:11:13,840
아니라면, 그것은 고차원 데이터입니다.

1068
01:11:13,840 --> 01:11:18,750
그래서 그런 경우에는 다시, 변수에

1069
01:11:18,750 --> 01:11:21,660
대한 기울기가 해당 특정

1070
01:11:21,660 --> 01:11:26,340
변수와 같은 크기가 될 것입니다.

1071
01:11:26,340 --> 01:11:34,260
상류 및 하류 행렬과 도함수를 계산하는 것은

1072
01:11:34,260 --> 01:11:39,810
이전에 벡터에 대해 논의하고 보여준

1073
01:11:39,810 --> 01:11:44,350
것과 동일하게 진행됩니다.

1074
01:11:44,350 --> 01:11:49,890
그리고 로컬 기울기에 관해서는, 비록-- 그러나, 출력으로

1075
01:11:49,890 --> 01:11:53,930
행렬이 있고 입력으로 행렬이 있기 때문에

1076
01:11:53,930 --> 01:11:56,840
거대한 행렬, 거대한 야코비안이

1077
01:11:56,840 --> 01:11:58,320
될 것입니다.

1078
01:11:58,320 --> 01:12:00,080
그리고 로컬

1079
01:12:00,080 --> 01:12:06,170
기울기는 입력 크기와 출력 크기의 곱과 같은 크기가

1080
01:12:06,170 --> 01:12:08,250
될 것입니다.

1081
01:12:08,250 --> 01:12:11,880
그래서 그것 자체로도 거대한 행렬이 될 것입니다.

1082
01:12:11,880 --> 01:12:13,500
예를 들어 드리겠습니다.

1083
01:12:13,500 --> 01:12:17,810
이것이 입력 x이고 w가 행렬

1084
01:12:17,810 --> 01:12:24,980
곱셈을 위한 노드의 입력으로, y가 출력으로

1085
01:12:24,980 --> 01:12:28,050
생성된다면, y에 대한

1086
01:12:28,050 --> 01:12:32,930
L의 도함수를 계산하면 이러한

1087
01:12:32,930 --> 01:12:36,050
야코비안 행렬을 얻습니다.

1088
01:12:36,050 --> 01:12:43,250
그리고 미니 배치 크기가 64이고 이러한 행렬의 차원

1089
01:12:43,250 --> 01:12:47,645
수가 4,096이라면, 이는 그

1090
01:12:47,645 --> 01:12:52,930
거대한 야코비안 행렬이 단일 행렬 곱셈에

1091
01:12:52,930 --> 01:13:00,100
대해 256기가바이트가 넘는다는 것을 의미합니다.

1092
01:13:00,100 --> 01:13:07,720
그래서 이를 단순화하기 위해, 우리는 값들이 서로에게

1093
01:13:07,720 --> 01:13:12,160
어떻게 영향을 미치는지

1094
01:13:12,160 --> 01:13:14,240
살펴보려고 합니다.

1095
01:13:14,240 --> 01:13:17,740
예를 들어, x의 한 요소가

1096
01:13:17,740 --> 01:13:24,770
영향을 받으면 y의 어떤 부분이 영향을 받을까요?

1097
01:13:24,770 --> 01:13:30,700
그래서 여기서 xn과 d, 이 특정 값은

1098
01:13:30,700 --> 01:13:35,720
종종 출력의 한 행에만 영향을 미칩니다.

1099
01:13:35,720 --> 01:13:41,560
그리고 이것은 기본적으로 이러한 각 노드를 계산하는 데

1100
01:13:41,560 --> 01:13:43,400
도움이 됩니다.

1101
01:13:43,400 --> 01:13:46,340
우리는 그 거대한 야코비안을 만들 필요가 없습니다.

1102
01:13:46,340 --> 01:13:50,160
우리는 사실 행렬 곱셈을 위한 역전파

1103
01:13:50,160 --> 01:13:53,610
함수를 더 효율적인 방식으로

1104
01:13:53,610 --> 01:13:56,170
작성할 수 있습니다.

1105
01:13:56,170 --> 01:13:59,530
그리고 거의 끝났습니다.

1106
01:13:59,530 --> 01:14:01,320
그래서 이 질문에 답해 보세요.

1107
01:14:01,320 --> 01:14:08,170
xn, d가 y와 m의 값에 얼마나 영향을 미칩니까?

1108
01:14:08,170 --> 01:14:15,480
그래서 이것은 xn, d로부터 영향을 받는 yn, m입니다.

1109
01:14:15,480 --> 01:14:17,400
얼마나 영향을 받습니까?

1110
01:14:17,400 --> 01:14:19,680
이는 특정 값

1111
01:14:19,680 --> 01:14:22,680
xn, d에 대한

1112
01:14:22,680 --> 01:14:30,720
기울기로 무엇을 배치해야 하는지를 의미합니다.

1113
01:14:30,720 --> 01:14:34,150
다시 말씀드리면, 이것은 곱셈 연산입니다.

1114
01:14:37,410 --> 01:14:41,010
곱셈 게이트에서는 스왑이 있어야 합니다.

1115
01:14:41,010 --> 01:14:47,820
그래서 이 질문에 대한 답이 W의

1116
01:14:47,820 --> 01:14:49,500
값인가요?

1117
01:14:49,500 --> 01:14:51,800
우리가 스왑 곱셈기인 이 곱셈 게이트를

1118
01:14:51,800 --> 01:14:53,520
가졌던 것을 기억하세요.

1119
01:14:53,520 --> 01:14:55,950
그래서 여기에서 스왑이 발생하고 있습니다.

1120
01:14:55,950 --> 01:15:07,730
따라서 x가 y에 영향을 미치는 값, y의 요소 중 하나는 x 행렬에 의해

1121
01:15:07,730 --> 01:15:13,040
정의된 행 D와 y 행렬에 의해

1122
01:15:13,040 --> 01:15:17,510
정의된 열 M에 있는 W에

1123
01:15:17,510 --> 01:15:19,800
따라 달라집니다.

1124
01:15:19,800 --> 01:15:22,470
그래서 값이 교환되고 있습니다.

1125
01:15:22,470 --> 01:15:23,910
같은 교환입니다.

1126
01:15:23,910 --> 01:15:27,650
하지만 여기서는 거대한 행렬을 살펴보고

1127
01:15:27,650 --> 01:15:31,950
어떤 특정 요소여야 하는지 찾아야 합니다.

1128
01:15:31,950 --> 01:15:34,580
그에 따라 우리는 실제로

1129
01:15:34,580 --> 01:15:40,940
행렬 곱셈과 행렬 연산으로 전체를 대체할 수

1130
01:15:40,940 --> 01:15:41,760
있습니다.

1131
01:15:41,760 --> 01:15:43,700
L의 x에 대한

1132
01:15:43,700 --> 01:15:47,690
기울기는 이 간단한 행렬 연산으로 정의됩니다.

1133
01:15:47,690 --> 01:15:51,130
그리고 L의 W에 대한 기울기는 이

1134
01:15:51,130 --> 01:15:54,080
매우 간단한 곱셈으로 정의됩니다.

1135
01:15:54,080 --> 01:15:58,940
다시 말해, 여기서 x에 대해 전체 w를 포함합니다.

1136
01:15:58,940 --> 01:16:05,420
w에 대해서는 전체 x를 포함하고 곱셈을 수행합니다.

1137
01:16:05,420 --> 01:16:09,220
이 공식들은 더 크고 어려운 연산을

1138
01:16:09,220 --> 01:16:14,860
구현하기 쉽게 만들어 주며, 역전파에서 구현할

1139
01:16:14,860 --> 01:16:17,470
수 있게 해줍니다.

1140
01:16:17,470 --> 01:16:20,020
좋습니다, 우리는 끝났습니다.

1141
01:16:20,020 --> 01:16:22,690
요약하자면, 오늘 우리는 완전 연결

1142
01:16:22,690 --> 01:16:24,860
신경망에 대해 이야기했습니다.

1143
01:16:24,860 --> 01:16:30,320
우리는 역전파에 필요한 모든 단계, 순전파와 역전파를

1144
01:16:30,320 --> 01:16:32,390
살펴보았습니다.

1145
01:16:32,390 --> 01:16:36,160
다음 세션에서는 합성곱 신경망

1146
01:16:36,160 --> 01:16:39,820
주제로 들어갈 것입니다.

1147
01:16:39,820 --> 01:16:41,820
감사합니다.
