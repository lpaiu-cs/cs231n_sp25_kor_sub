1
00:00:05,270 --> 00:00:09,140
이번 강의는 이번 코스의 마지막 게스트 강사와 함께합니다.

2
00:00:09,140 --> 00:00:12,410
오늘은 Dr. Yunzhu Li 선생님을 모셨습니다.

3
00:00:12,410 --> 00:00:14,870
그는 Columbia University 컴퓨터과학

4
00:00:14,870 --> 00:00:18,750
조교수로, Robotic Perception, Interaction, and
Learning

5
00:00:18,750 --> 00:00:20,160
Lab을 이끌고 있습니다.

6
00:00:20,160 --> 00:00:23,070
또한 모든 게스트 강사들처럼 CS231N의

7
00:00:23,070 --> 00:00:24,600
전임 강사 출신입니다.

8
00:00:24,600 --> 00:00:28,550
2023년에는 Stanford에서 Fei-Fei Li 교수님과 Jiajun

9
00:00:28,550 --> 00:00:31,770
Wu 교수님과 함께 포닥을 하면서 이 강의를 맡았습니다.

10
00:00:31,770 --> 00:00:33,680
그의 연구는 로보틱스,

11
00:00:33,680 --> 00:00:36,750
컴퓨터 비전, 머신러닝의 교차점에 있습니다.

12
00:00:36,750 --> 00:00:40,100
특히 로봇 학습에 집중하며, 로봇의 인지

13
00:00:40,100 --> 00:00:43,040
및 물리적 상호작용 능력을 크게

14
00:00:43,040 --> 00:00:45,180
확장하는 것을 목표로 합니다.

15
00:00:45,180 --> 00:00:47,750
오늘 강의에서는 바로 그 주제, 로봇 학습에

16
00:00:47,750 --> 00:00:49,020
대해 다룰 예정입니다.

17
00:00:49,020 --> 00:00:52,490
이제 Yunzhu 선생님께 강의를 넘기겠습니다.

18
00:00:52,490 --> 00:00:55,380
네, 친절한 소개 감사합니다, Dean.

19
00:00:55,380 --> 00:00:57,200
여기에 오게 되어 정말 기쁩니다.

20
00:00:57,200 --> 00:00:59,010
마지막으로 강의한 것은

21
00:00:59,010 --> 00:01:01,270
2년 전, 2023년이었습니다.

22
00:01:01,270 --> 00:01:05,489
최근에 많은 강의를 다시 보면서 준비했습니다.

23
00:01:05,489 --> 00:01:09,060
오늘은 제가 연구해온 몇 가지 주제에 대해

24
00:01:09,060 --> 00:01:10,570
말씀드리겠습니다.

25
00:01:10,570 --> 00:01:13,440
이 내용은 컴퓨터 비전을 위한 딥러닝

26
00:01:13,440 --> 00:01:16,290
전체 그림에서 매우 일관된 부분입니다.

27
00:01:16,290 --> 00:01:19,140
특히 로봇 학습에 관한 내용입니다.

28
00:01:19,140 --> 00:01:20,910
로봇이 물리적 세계를

29
00:01:20,910 --> 00:01:23,110
더 잘 인지하고

30
00:01:23,110 --> 00:01:26,340
상호작용할 수 있도록 하는 데

31
00:01:26,340 --> 00:01:28,270
필요한 흥미로운 고려사항과,

32
00:01:28,270 --> 00:01:30,150
일반적인

33
00:01:30,150 --> 00:01:33,420
컴퓨터 비전 과제 및 방법과 어떻게

34
00:01:33,420 --> 00:01:36,075
다른지 설명드리겠습니다.

35
00:01:36,075 --> 00:01:38,580
우선, 여러분은 이미

36
00:01:38,580 --> 00:01:42,095
감독 학습에 대해 많이 배웠습니다.

37
00:01:42,095 --> 00:01:44,850
감독 학습의 기본 설정은 데이터

38
00:01:44,850 --> 00:01:47,130
x와 y가 있습니다.

39
00:01:47,130 --> 00:01:49,750
x는 입력이고, y는 레이블입니다.

40
00:01:49,750 --> 00:01:53,220
입력 x에서 출력 y로 매핑하는 함수를

41
00:01:53,220 --> 00:01:55,057
학습하는 것이 목표입니다.

42
00:01:55,057 --> 00:01:56,890
분류, 회귀,

43
00:01:56,890 --> 00:02:01,295
객체 탐지 같은 예제를 이미 배웠죠.

44
00:02:01,295 --> 00:02:05,300
또한 레이블 없이 데이터만

45
00:02:05,300 --> 00:02:07,890
가지고 학습하는

46
00:02:07,890 --> 00:02:11,130
자기지도 학습도 배웠습니다.

47
00:02:11,130 --> 00:02:13,190
이 경우는 보조

48
00:02:13,190 --> 00:02:17,270
손실(auxiliary loss)을 설계해 데이터의

49
00:02:17,270 --> 00:02:19,680
숨겨진 구조를 추출하거나

50
00:02:19,680 --> 00:02:24,060
식별하는 학습 알고리즘을 만드는 것입니다.

51
00:02:24,060 --> 00:02:27,230
대표적인 예로 오토인코더가

52
00:02:27,230 --> 00:02:31,010
있고, 이 밖에도 많은

53
00:02:31,010 --> 00:02:34,160
비지도 학습 또는 자기지도

54
00:02:34,160 --> 00:02:37,520
학습 방법이 있습니다.

55
00:02:37,520 --> 00:02:40,010
로봇 학습의 특별하고

56
00:02:40,010 --> 00:02:42,140
독특한 점은 로봇이

57
00:02:42,140 --> 00:02:44,810
물리적으로 세계와 상호작용해야

58
00:02:44,810 --> 00:02:46,830
한다는 것입니다.

59
00:02:46,830 --> 00:02:49,010
단순히 입력과 출력,

60
00:02:49,010 --> 00:02:50,790
또는 잠재 표현 간

61
00:02:50,790 --> 00:02:52,620
매핑이 아닙니다.

62
00:02:52,620 --> 00:02:55,140
환경의 진화에 영향을 미치는

63
00:02:55,140 --> 00:02:56,610
행동을 해야 합니다.

64
00:02:56,610 --> 00:02:58,340
현실 세계에서 어떤

65
00:02:58,340 --> 00:03:00,440
행동을 취하면, 그

66
00:03:00,440 --> 00:03:03,270
행동의 결과로 환경이 변합니다.

67
00:03:03,270 --> 00:03:05,900
환경은 새로운 관측이나 보상을

68
00:03:05,900 --> 00:03:09,068
주어, 환경이 어떻게 변했는지, 그리고

69
00:03:09,068 --> 00:03:10,610
특정 작업

70
00:03:10,610 --> 00:03:13,740
수행이 얼마나 잘 되었는지 알려줍니다.

71
00:03:13,740 --> 00:03:16,520
목표는 환경으로부터 피드백을

72
00:03:16,520 --> 00:03:21,180
받으며 일련의 행동을 계획해, 보상을

73
00:03:21,180 --> 00:03:25,670
최대화하거나 비용을 최소화하는 것입니다.

74
00:03:25,670 --> 00:03:30,420
로봇 학습은 특히 최근 몇 년간 학계와

75
00:03:30,420 --> 00:03:34,190
산업계 모두에서 큰 관심을

76
00:03:34,190 --> 00:03:36,450
받고 있습니다.

77
00:03:36,450 --> 00:03:41,280
그래서 우리는 많은 스타트업 회사들을 보았고, 예를 들어 Tesla

78
00:03:41,280 --> 00:03:43,910
bots나 Figure 같은 물리적 지능

79
00:03:43,910 --> 00:03:45,200
분야도 포함됩니다.

80
00:03:45,200 --> 00:03:49,940
이들은 셔츠를 접거나 커피 원두를 조작하는 등 매우 복잡한

81
00:03:49,940 --> 00:03:54,240
작업을 수행하는 로봇의 멋지고 화려한 영상을 제작하고

82
00:03:54,240 --> 00:03:56,700
있습니다. 실제 물리적

83
00:03:56,700 --> 00:03:58,890
세계에서 흥미로운 작업을 하는

84
00:03:58,890 --> 00:04:01,920
인간과 같은 움직임을 시도하기도 하죠.

85
00:04:01,920 --> 00:04:05,580
이 분야는 제가 언급했듯이 많은 관심과

86
00:04:05,580 --> 00:04:07,510
투자도 받고 있습니다.

87
00:04:07,510 --> 00:04:10,140
여기 로봇 학습 분야에서 최근에 큰

88
00:04:10,140 --> 00:04:12,300
투자를 유치한 스타트업 몇

89
00:04:12,300 --> 00:04:15,090
가지 예시가 있습니다. 이들은 환경과

90
00:04:15,090 --> 00:04:17,370
물리적으로 상호작용할 수 있는

91
00:04:17,370 --> 00:04:19,050
범용 로봇을 만들려고

92
00:04:19,050 --> 00:04:20,200
노력하고 있습니다.

93
00:04:20,200 --> 00:04:23,550
물론, 이런 스타트업뿐만 아니라

94
00:04:23,550 --> 00:04:28,370
주로 대형 기업들도 자체 로봇 연구와

95
00:04:28,370 --> 00:04:30,420
프로젝트를 진행하며,

96
00:04:30,420 --> 00:04:32,520
환경과 범용적이고

97
00:04:32,520 --> 00:04:36,090
고성능의 물리적 상호작용이 가능한

98
00:04:36,090 --> 00:04:39,615
범용 로봇을 개발하려고 합니다.

99
00:04:39,615 --> 00:04:42,060
오늘 강의에서는 현재 로봇

100
00:04:42,060 --> 00:04:44,535
학습의 성공과 붐을

101
00:04:44,535 --> 00:04:47,160
가능하게 하는 주요 기술과

102
00:04:47,160 --> 00:04:50,910
요소들에 대해 개괄적으로 설명드리겠습니다.

103
00:04:50,910 --> 00:04:53,210
먼저 문제 정의부터 시작하겠습니다.

104
00:04:53,210 --> 00:04:55,840
우리가 구축해온 문제를 좀 더 구체적으로 어떻게

105
00:04:55,840 --> 00:04:57,620
정의할 수 있는지, 그리고

106
00:04:57,620 --> 00:05:00,490
환경과의 견고한 상호작용을 어떻게 공식적으로 생각할 수

107
00:05:00,490 --> 00:05:01,700
있는지 살펴보겠습니다.

108
00:05:01,700 --> 00:05:05,510
그다음에는 인지 측면에 대해 이야기하겠습니다.

109
00:05:05,510 --> 00:05:07,870
로봇이 환경을 인지하는 방식과

110
00:05:07,870 --> 00:05:10,270
컴퓨터 비전 커뮤니티에서

111
00:05:10,270 --> 00:05:13,480
일반적으로 고려하는 방식의 차이점,

112
00:05:13,480 --> 00:05:16,460
그리고 로봇 인지의 특별한 점에

113
00:05:16,460 --> 00:05:19,270
대해 말씀드리겠습니다. 이어서

114
00:05:19,270 --> 00:05:22,750
강화 학습, 모델 학습, 모델 기반 계획,

115
00:05:22,750 --> 00:05:25,970
모방 학습, 최근 로봇 기초 모델

116
00:05:25,970 --> 00:05:29,350
동향에 대해 다루고, 남은 시간에는

117
00:05:29,350 --> 00:05:34,330
앞으로 우리가 마주할 도전 과제들에 대해 논의하겠습니다.

118
00:05:34,330 --> 00:05:37,690
먼저 문제 정의부터 시작하겠습니다.

119
00:05:37,690 --> 00:05:40,930
일반적으로 문제는 적어도

120
00:05:40,930 --> 00:05:45,350
그래픽적으로 이렇게 보일 수 있습니다.

121
00:05:45,350 --> 00:05:47,300
중앙에는 에이전트가 있습니다.

122
00:05:47,300 --> 00:05:50,400
에이전트는 어떤 작업 목표를 부여받습니다.

123
00:05:50,400 --> 00:05:52,760
이 작업 목표는 예를 들어 인간의

124
00:05:52,760 --> 00:05:55,910
언어 지시일 수도 있고, 특정 작업을

125
00:05:55,910 --> 00:06:00,530
얼마나 잘 수행하는지 측정하는 목적 함수일 수도 있습니다.

126
00:06:00,530 --> 00:06:04,610
이 에이전트는 물리적 세계나 어떤 환경에서

127
00:06:04,610 --> 00:06:06,330
상태를 받아옵니다.

128
00:06:06,330 --> 00:06:09,050
그리고 에이전트는 여기서 어떤 행동을

129
00:06:09,050 --> 00:06:11,270
취할지 결정하는데, 이 행동은

130
00:06:11,270 --> 00:06:13,470
물리적 세계에서 실행되어야 합니다.

131
00:06:13,470 --> 00:06:17,450
그리고 이 물리적 세계는 업데이트되어 다음 상태 st+1과

132
00:06:17,450 --> 00:06:20,180
보상을 에이전트에게 제공합니다. 보상은

133
00:06:20,180 --> 00:06:24,000
에이전트가 얼마나 잘 임무를 수행하는지 알려줍니다.

134
00:06:24,000 --> 00:06:26,670
이것이 일반적인 프레임워크의 모습입니다.

135
00:06:26,670 --> 00:06:31,400
그래서 목표, 상태, 행동, 그리고 보상으로 구성된

136
00:06:31,400 --> 00:06:36,210
이런 유형의 공식화에 대해 명확히 이해해야 합니다.

137
00:06:36,210 --> 00:06:41,300
로봇 학습 시나리오 문제를 구체적으로 정의하는

138
00:06:41,300 --> 00:06:44,750
것은 컴퓨터 비전과 매우

139
00:06:44,750 --> 00:06:45,510
다릅니다.

140
00:06:45,510 --> 00:06:48,260
컴퓨터 비전은 주로 고차원 데이터

141
00:06:48,260 --> 00:06:50,600
같은 입력을 기반으로

142
00:06:50,600 --> 00:06:53,160
환경의 어떤 표현을 학습하려고

143
00:06:53,160 --> 00:06:54,540
하는 것입니다.

144
00:06:54,540 --> 00:06:56,870
하지만 로봇공학에서는

145
00:06:56,870 --> 00:06:59,750
기본적으로 물리적 환경이라는

146
00:06:59,750 --> 00:07:02,150
제약 조건이 있는 최적화

147
00:07:02,150 --> 00:07:04,105
문제를 푸는 것입니다.

148
00:07:04,105 --> 00:07:05,480
목표에 대해 정의된 목적

149
00:07:05,480 --> 00:07:06,810
함수를 가지고 있고,

150
00:07:06,810 --> 00:07:09,410
본질적으로 목적 함수를 최대화하거나

151
00:07:09,410 --> 00:07:12,320
최소화할 수 있는 행동 시퀀스를

152
00:07:12,320 --> 00:07:15,570
찾아 이 최적화 문제를 해결하려고 합니다.

153
00:07:15,570 --> 00:07:17,960
이것이 로봇 학습과 일반적으로

154
00:07:17,960 --> 00:07:22,070
사람들이 컴퓨터 비전에서 생각하는 것의 핵심 차이점입니다.

155
00:07:22,070 --> 00:07:24,440
이 문제의 구체적인 예로

156
00:07:24,440 --> 00:07:27,300
카트폴(cart-pole)이 있는데, 목표는 움직일

157
00:07:27,300 --> 00:07:31,380
수 있는 카트 위에 막대를 균형 있게 세우는 것입니다.

158
00:07:31,380 --> 00:07:33,200
이 환경의 상태는

159
00:07:33,200 --> 00:07:35,900
시스템의 물리적 상태를

160
00:07:35,900 --> 00:07:39,500
설명하며, 각도, 각속도, 위치,

161
00:07:39,500 --> 00:07:43,127
수평 속도 등을 포함할 수 있습니다.

162
00:07:43,127 --> 00:07:44,960
행동은 카트에

163
00:07:44,960 --> 00:07:47,580
가해지는 수평 힘이 됩니다.

164
00:07:47,580 --> 00:07:51,000
보상은 막대가 매 시간 단계마다

165
00:07:51,000 --> 00:07:56,100
똑바로 세워져 있으면 1로 표시할 수 있습니다.

166
00:07:56,100 --> 00:07:59,650
또 다른 예로는 로봇 보행이 있는데, 목표는

167
00:07:59,650 --> 00:08:03,310
이 로봇들이 앞으로 움직이게 하는 것입니다.

168
00:08:03,310 --> 00:08:06,720
그리고 상태는 이 로봇 내 모든 관절의 각도,

169
00:08:06,720 --> 00:08:09,160
위치, 속도를 포함할 수 있습니다.

170
00:08:09,160 --> 00:08:11,160
그리고 행동은 각 관절에

171
00:08:11,160 --> 00:08:13,390
가해지는 토크일 수 있습니다.

172
00:08:13,390 --> 00:08:16,380
그리고 보상은 매 시간마다 1일 수 있습니다.

173
00:08:16,380 --> 00:08:20,010
로봇은 앞으로 한 걸음 나아가면서도

174
00:08:20,010 --> 00:08:22,995
똑바른 자세를 유지해야 합니다.

175
00:08:22,995 --> 00:08:25,770
또한 Atari 게임을 포함한

176
00:08:25,770 --> 00:08:27,460
흥미로운 예들도 있습니다.

177
00:08:27,460 --> 00:08:31,470
목표는 가능한 한 높은 점수로 게임을

178
00:08:31,470 --> 00:08:33,549
완료하는 것입니다.

179
00:08:33,549 --> 00:08:35,880
상태는 게임 화면의 원시 픽셀

180
00:08:35,880 --> 00:08:37,480
입력이 될 것입니다.

181
00:08:37,480 --> 00:08:41,010
행동은 위, 아래, 왼쪽, 오른쪽 같은 게임 조작일 수

182
00:08:41,010 --> 00:08:41,710
있습니다.

183
00:08:41,710 --> 00:08:43,650
보상은 각 시간 단계에서

184
00:08:43,650 --> 00:08:47,170
점수의 증가나 감소가 될 수 있습니다.

185
00:08:47,170 --> 00:08:50,320
그리고 아마도 AlphaGo의 발전과

186
00:08:50,320 --> 00:08:53,050
함께 여러분이 이전에 눈치챘을 더

187
00:08:53,050 --> 00:08:55,240
유명한 예들도 있습니다.

188
00:08:55,240 --> 00:08:57,400
바둑의 정의와 문제도 비슷한

189
00:08:57,400 --> 00:09:00,190
방식으로 정의할 수 있는데, 목표는

190
00:09:00,190 --> 00:09:02,350
게임에서 이기는 것입니다.

191
00:09:02,350 --> 00:09:05,290
상태는 현재 바둑판 위에 놓여 있는 모든

192
00:09:05,290 --> 00:09:06,950
돌들이 될 것입니다.

193
00:09:06,950 --> 00:09:09,460
행동은 다음 돌을 어디에 놓을지

194
00:09:09,460 --> 00:09:10,520
결정하는 것입니다.

195
00:09:10,520 --> 00:09:13,670
보상은 마지막 턴에서 이기면

196
00:09:13,670 --> 00:09:19,150
1, 지면 0의 보상을 받는 것입니다.

197
00:09:19,150 --> 00:09:22,270
이것은 예를 들어 게임 분야뿐만 아니라 적용됩니다.

198
00:09:22,270 --> 00:09:25,900
최근 대형 언어 모델의 발전과 함께,

199
00:09:25,900 --> 00:09:29,090
특히 순차 생성 문제에 대해 비슷한

200
00:09:29,090 --> 00:09:32,890
방식으로 생각할 수 있습니다. 목표는

201
00:09:32,890 --> 00:09:34,960
다음 단어를 예측하는

202
00:09:34,960 --> 00:09:37,340
것이 될 수 있습니다.

203
00:09:37,340 --> 00:09:40,010
상태는 문장 내 현재 단어들이 될 수 있습니다.

204
00:09:40,010 --> 00:09:42,340
행동은 다음에 넣고자 하는 특정

205
00:09:42,340 --> 00:09:43,920
단어가 될 것입니다.

206
00:09:43,920 --> 00:09:46,830
맞으면 보상을 받습니다.

207
00:09:46,830 --> 00:09:51,230
틀리면 0의 보상을 받습니다.

208
00:09:51,230 --> 00:09:53,420
마찬가지로, 이제 여러분은 아마도

209
00:09:53,420 --> 00:09:56,870
많은 챗봇을 꽤 많이 사용해 보셨을 겁니다.

210
00:09:56,870 --> 00:09:59,450
그리고 문제를 비슷하게 정의할 수

211
00:09:59,450 --> 00:10:04,260
있는데, 목표는 인간 사용자에게 좋은 동반자가 되는 것입니다.

212
00:10:04,260 --> 00:10:06,860
상태는 현재 대화가 될 수 있습니다.

213
00:10:06,860 --> 00:10:10,460
챗봇이 생성해야 할 행동은 인간

214
00:10:10,460 --> 00:10:14,340
사용자에게 줄 다음 문장입니다.

215
00:10:14,340 --> 00:10:16,830
인간 평가에 따라, 사람이

216
00:10:16,830 --> 00:10:20,210
행복하면 보상을 정의할 수 있습니다.

217
00:10:20,210 --> 00:10:23,340
만족하면 1의 보상을 받습니다.

218
00:10:23,340 --> 00:10:25,740
행복하지 않거나 중립이면

219
00:10:25,740 --> 00:10:28,310
다른 보상을 받습니다.

220
00:10:28,310 --> 00:10:31,050
더 구체적으로, 예를 들어 로봇 공학 분야에서는,

221
00:10:31,050 --> 00:10:33,840
과제는 옷을 접는 것일 수 있습니다.

222
00:10:33,840 --> 00:10:36,500
우리는 옷을 깔끔하게 접고 싶습니다.

223
00:10:36,500 --> 00:10:39,020
상태는 로봇이 환경에서

224
00:10:39,020 --> 00:10:42,360
얻는 현재 관찰이며, 환경의 다중

225
00:10:42,360 --> 00:10:45,900
시점 RGB 또는 RGBD 관찰을

226
00:10:45,900 --> 00:10:47,820
포함할 수 있습니다.

227
00:10:47,820 --> 00:10:50,380
로봇은 엔드 이펙터를 어떻게 움직일지

228
00:10:50,380 --> 00:10:52,200
행동을 결정해야 합니다.

229
00:10:52,200 --> 00:10:54,720
이 천을 조작하기 위해 그리퍼를 닫아야

230
00:10:54,720 --> 00:10:56,830
할까요, 아니면 열어야 할까요?

231
00:10:56,830 --> 00:10:58,750
사람의 평가에 따르면,

232
00:10:58,750 --> 00:11:01,680
천이 제대로 접히면 로봇에게

233
00:11:01,680 --> 00:11:04,660
보상 1을 주고, 접히지 않으면

234
00:11:04,660 --> 00:11:06,610
보상 0을 줍니다.

235
00:11:06,610 --> 00:11:10,770
여기서 로봇 학습 문제를 좀 더 구체적으로 생각하는

236
00:11:10,770 --> 00:11:12,400
방법을 보여드리겠습니다.

237
00:11:12,400 --> 00:11:15,900
이것은 에이전트가 세상과 상호작용할 수 있게

238
00:11:15,900 --> 00:11:16,990
하는 방법입니다.

239
00:11:16,990 --> 00:11:19,050
행동의 결과와 연속적인

240
00:11:19,050 --> 00:11:21,840
의사결정 문제를 고려합니다.

241
00:11:21,840 --> 00:11:23,820
이것은 사람들이 일반적으로 컴퓨터 비전에서

242
00:11:23,820 --> 00:11:25,120
생각하는 것과 다릅니다.

243
00:11:25,120 --> 00:11:27,945
우리는 단지 출력을 예측하면 됩니다.

244
00:11:27,945 --> 00:11:30,880
목표, 상태, 행동, 보상,

245
00:11:30,880 --> 00:11:33,090
그리고 목적 함수는 이

246
00:11:33,090 --> 00:11:34,710
방향의 문제를 생각할

247
00:11:34,710 --> 00:11:38,490
때 항상 염두에 두어야 할 것들입니다.

248
00:11:38,490 --> 00:11:40,970
이것이 문제 공식화에 관한 내용입니다.

249
00:11:40,970 --> 00:11:45,490
그래서 질문은 보상이 얼마나 구체적으로 설계되어야 하느냐입니다.

250
00:11:45,490 --> 00:11:48,610
많은 작업에서 보상은 여러 가지 다른 방식으로

251
00:11:48,610 --> 00:11:49,875
지정될 수 있습니다.

252
00:11:49,875 --> 00:11:51,250
예를 들어,

253
00:11:51,250 --> 00:11:54,520
자율주행에서는 보상이 가능한 한 빠르게 하는

254
00:11:54,520 --> 00:11:58,810
것이 될 수도 있고, 승객이 편안함을 느끼도록 하는 것이

255
00:11:58,810 --> 00:12:00,290
될 수도 있습니다.

256
00:12:00,290 --> 00:12:02,320
옷 접기에서도 사용자 취향에

257
00:12:02,320 --> 00:12:04,030
따라 옷을 여러 가지

258
00:12:04,030 --> 00:12:05,810
방식으로 접을 수 있습니다.

259
00:12:05,810 --> 00:12:08,930
어떤 사람은 전체 면적이 최대한 작아지길 원하고,

260
00:12:08,930 --> 00:12:11,690
어떤 사람은 최대한 매끄럽게 접히길 원합니다.

261
00:12:11,690 --> 00:12:14,320
다양한 유형의 보상이 있을 수 있습니다.

262
00:12:14,320 --> 00:12:16,840
여기서는 일반적인 용어로 이야기하는 겁니다.

263
00:12:16,840 --> 00:12:18,310
사람이 옷을 보고, 이게

264
00:12:18,310 --> 00:12:20,300
접힌 것인지 아닌지 판단하는 거죠?

265
00:12:20,300 --> 00:12:22,970
하지만 보상 설계 측면에서는

266
00:12:22,970 --> 00:12:25,060
특정 애플리케이션의

267
00:12:25,060 --> 00:12:30,190
구체적인 요구를 만족시키는 데 많은 미묘한 차이가 있습니다.

268
00:12:30,190 --> 00:12:31,090
네.

269
00:12:31,090 --> 00:12:31,865
계속하겠습니다.

270
00:12:31,865 --> 00:12:34,240
이렇게 해서 우리는 에이전트가 물리적

271
00:12:34,240 --> 00:12:36,770
세계와 상호작용할 수 있게 하는 로봇

272
00:12:36,770 --> 00:12:38,540
학습 문제를 생각하고 있습니다.

273
00:12:38,540 --> 00:12:41,010
이제 로봇 인식으로 넘어가서,

274
00:12:41,010 --> 00:12:43,580
특히 로봇 학습 영역

275
00:12:43,580 --> 00:12:45,560
내에서 인식 문제가

276
00:12:45,560 --> 00:12:48,020
일반적인 컴퓨터 비전과 어떻게

277
00:12:48,020 --> 00:12:50,030
다른지 논의하겠습니다.

278
00:12:50,030 --> 00:12:51,440
이 이미지를 다시

279
00:12:51,440 --> 00:12:54,020
보여드리는데, 오늘 강의 내내 이 이미지를

280
00:12:54,020 --> 00:12:55,620
여러 번 보게 될 겁니다.

281
00:12:55,620 --> 00:12:58,040
본질적으로, 물리적

282
00:12:58,040 --> 00:13:02,450
세계에서 얻는 정보를 어떻게 처리할 것인가에

283
00:13:02,450 --> 00:13:04,185
관한 질문입니다.

284
00:13:04,185 --> 00:13:06,060
물리적 세계는 예를 들어

285
00:13:06,060 --> 00:13:09,840
고차원 RGB 관측이나 RGBD 관측을 줄 수 있습니다.

286
00:13:09,840 --> 00:13:12,230
또한 촉각 센싱 같은 다른 감각

287
00:13:12,230 --> 00:13:13,945
데이터도 포함할 수 있습니다.

288
00:13:13,945 --> 00:13:15,890
로봇 인식 문제는

289
00:13:15,890 --> 00:13:18,890
본질적으로 이런 고차원 데이터에서

290
00:13:18,890 --> 00:13:23,210
로봇이 후속 의사결정을 하는 데 유용한

291
00:13:23,210 --> 00:13:27,470
구조화된 지식을 추출하거나 활용하는 것입니다.

292
00:13:27,470 --> 00:13:30,770
본질적으로 우리가 해결하려는 질문은 이

293
00:13:30,770 --> 00:13:34,560
비구조화된 현실 세계를 이해하는 것입니다.

294
00:13:34,560 --> 00:13:37,660
현실 세계는 매우 복잡할 수 있습니다.

295
00:13:37,660 --> 00:13:39,660
로봇이 환경에서 얻는

296
00:13:39,660 --> 00:13:41,310
관측은 객체와

297
00:13:41,310 --> 00:13:45,000
환경에 대한 불완전한 지식만 포함할 수

298
00:13:45,000 --> 00:13:46,090
있습니다.

299
00:13:46,090 --> 00:13:47,590
가림 현상(occlusion)이 있을 수 있습니다.

300
00:13:47,590 --> 00:13:52,390
감각 데이터에서 오류가 발생할 수도 있습니다.

301
00:13:52,390 --> 00:13:56,020
그리고 불완전한 동작도 실패로 이어질 수 있습니다.

302
00:13:56,020 --> 00:13:59,590
예를 들어, 로봇이 어떤 물체를 잡으려고 시도하지만,

303
00:13:59,590 --> 00:14:02,650
잡는 동작이 항상 성공적이지 않을 수 있습니다.

304
00:14:02,650 --> 00:14:05,170
때때로 물체를 실수로 떨어뜨리기도

305
00:14:05,170 --> 00:14:09,060
하는데, 이것도 환경의 변화와 예상치 못한

306
00:14:09,060 --> 00:14:10,300
변화를 초래합니다.

307
00:14:10,300 --> 00:14:12,780
그래서 이런 상황들을 처리할 수 있는

308
00:14:12,780 --> 00:14:14,650
인지 시스템이 필요합니다.

309
00:14:14,650 --> 00:14:17,170
또한 이 환경은 변할 수 있습니다.

310
00:14:17,170 --> 00:14:20,170
환경은 동적이며, 단단한 물체뿐만 아니라 옷,

311
00:14:20,170 --> 00:14:23,940
로프, 입자 매체 같은 변형 가능한 물체도 포함합니다.

312
00:14:23,940 --> 00:14:26,820
개나 다른 아이들, 인간 같은 다른

313
00:14:26,820 --> 00:14:29,790
에이전트들도 같은 환경에 있을 수 있어서 세상을

314
00:14:29,790 --> 00:14:31,330
어지럽힐 수 있습니다.

315
00:14:31,330 --> 00:14:33,910
인지 시스템은 이런 모든

316
00:14:33,910 --> 00:14:37,870
변화에 대응할 수 있어야 합니다.

317
00:14:37,870 --> 00:14:40,430
그래서 로봇 분야에서는 보통

318
00:14:40,430 --> 00:14:43,900
카메라 데이터만 사용하는 것이 아닙니다.

319
00:14:43,900 --> 00:14:47,800
유용한 정보를 제공할 수 있다면 가능한 한

320
00:14:47,800 --> 00:14:51,460
많은 센서를 로봇에 추가하려고 합니다.

321
00:14:51,460 --> 00:14:54,470
예를 들어, 촉각 센싱, 오디오

322
00:14:54,470 --> 00:14:57,745
정보, 깊이 정보 등이 정말 고려됩니다.

323
00:14:57,745 --> 00:15:01,900
보통은 모든 센서를 함께 결합하여 서로

324
00:15:01,900 --> 00:15:04,330
보완할 수 있는

325
00:15:04,330 --> 00:15:07,730
시스템을 설계해야 합니다. 모든

326
00:15:07,730 --> 00:15:09,850
정보는 물리적 맥락에

327
00:15:09,850 --> 00:15:11,540
대해 알려주고,

328
00:15:11,540 --> 00:15:13,540
촉각 정보는 잡는 동작이

329
00:15:13,540 --> 00:15:15,980
안정적인지 여부를 알려주며,

330
00:15:15,980 --> 00:15:17,590
카메라 정보는

331
00:15:17,590 --> 00:15:19,850
환경의 전체 상태 같은

332
00:15:19,850 --> 00:15:22,330
더 높은 수준의 정보를

333
00:15:22,330 --> 00:15:23,570
제공합니다.

334
00:15:23,570 --> 00:15:25,870
이 센서들이 어떻게 함께

335
00:15:25,870 --> 00:15:28,180
구성되고 작동하는지가 실제

336
00:15:28,180 --> 00:15:31,430
물리 세계에서 작동하는 유능한 로봇

337
00:15:31,430 --> 00:15:34,010
시스템 설계에 매우 중요합니다.

338
00:15:34,010 --> 00:15:38,010
감각 모달리티 수 외에도, 로봇 비전과 컴퓨터 비전의

339
00:15:38,010 --> 00:15:40,010
매우 중요한 차이점은

340
00:15:40,010 --> 00:15:42,770
동작의 효과와 환경의 어포던스(affordance)를

341
00:15:42,770 --> 00:15:45,290
진정으로 이해하려고

342
00:15:45,290 --> 00:15:46,770
한다는 점입니다.

343
00:15:46,770 --> 00:15:49,100
왼쪽은 컴퓨터 비전에서

344
00:15:49,100 --> 00:15:51,950
흔히 보는 인스턴스

345
00:15:51,950 --> 00:15:53,760
세그멘테이션 예시입니다.

346
00:15:53,760 --> 00:15:56,010
주어진 것은 2D 이미지입니다.

347
00:15:56,010 --> 00:15:59,310
2D 이미지에서 픽셀 위에 윤곽선을

348
00:15:59,310 --> 00:16:03,445
그려 서로 다른 인스턴스를 분할합니다.

349
00:16:03,445 --> 00:16:05,820
하지만 로봇 분야에서 다른 점은,

350
00:16:05,820 --> 00:16:07,040
예를 들어 오른쪽처럼

351
00:16:07,040 --> 00:16:09,320
로봇이 하나의 물체를 받았을 때,

352
00:16:09,320 --> 00:16:14,060
이 물체가 단일 물체인지, 아니면 여러 조각이

353
00:16:14,060 --> 00:16:16,400
쌓여 있는 것인지 알아야

354
00:16:16,400 --> 00:16:17,640
한다는 겁니다.

355
00:16:17,640 --> 00:16:20,900
그래서 로봇은 어떤 동작이 환경에 대한

356
00:16:20,900 --> 00:16:23,720
더 나은 이해와 인지를 가능하게

357
00:16:23,720 --> 00:16:25,710
하는지 알아야 합니다.

358
00:16:25,710 --> 00:16:27,710
이것이 하나의 물체인지, 여러 조각이

359
00:16:27,710 --> 00:16:29,090
합쳐진 것인지 말이죠.

360
00:16:29,090 --> 00:16:31,560
그래서 로봇은 환경 상태에 대한

361
00:16:31,560 --> 00:16:34,650
더 나은 인지를 위해 환경을 교란하거나

362
00:16:34,650 --> 00:16:38,520
적극적으로 상호작용하는 동작을 고안해야 합니다.

363
00:16:38,520 --> 00:16:44,190
그래서 로봇 비전은 구현되어 있고, 능동적이며, 환경에 위치해

364
00:16:44,190 --> 00:16:46,410
있다고 할 수 있습니다.

365
00:16:46,410 --> 00:16:49,335
구현되어 있다는 것은 로봇이

366
00:16:49,335 --> 00:16:53,580
물리적 세계를 직접 경험하는 물리적 몸체를

367
00:16:53,580 --> 00:16:55,920
가지고 있다는 뜻입니다.

368
00:16:55,920 --> 00:16:58,050
로봇의 동작은 세계와의

369
00:16:58,050 --> 00:17:00,390
역동적인 상호작용의 일부이며,

370
00:17:00,390 --> 00:17:02,700
즉각적인 감각 피드백을 받습니다.

371
00:17:02,700 --> 00:17:06,720
능동적이라는 것은 로봇이 능동적인 인지자라는 의미입니다.

372
00:17:06,720 --> 00:17:09,960
로봇은 왜 감지하려 하는지 알고, 무엇을 감지할지

373
00:17:09,960 --> 00:17:12,210
선택하며, 언제, 어디서, 어떻게 그

374
00:17:12,210 --> 00:17:14,099
인지를 달성할지 결정합니다.

375
00:17:14,099 --> 00:17:15,430
머리를 움직일 수 있듯이,

376
00:17:15,430 --> 00:17:17,349
테이블 뒤에 무엇이 있는지 알고 싶으면,

377
00:17:17,349 --> 00:17:19,900
그냥 돌아다니면서 테이블 뒤를 볼 수 있습니다.

378
00:17:19,900 --> 00:17:21,565
이것이 바로 능동적인 부분인데,

379
00:17:21,565 --> 00:17:23,190
보통 사람들이 컴퓨터 비전에서

380
00:17:23,190 --> 00:17:24,609
생각하는 것과는 다릅니다.

381
00:17:24,609 --> 00:17:29,170
대부분은 수동적으로 수집된 데이터셋을 다루고 있습니다.

382
00:17:29,170 --> 00:17:31,270
세 번째는 situated, 즉

383
00:17:31,270 --> 00:17:33,620
로봇이 세상에 위치하는 문제입니다.

384
00:17:33,620 --> 00:17:37,480
로봇은 추상적인 설명이 아니라, 시스템의

385
00:17:37,480 --> 00:17:39,940
행동에 직접 영향을 주는 현재

386
00:17:39,940 --> 00:17:42,490
이 순간의 세상을 다룹니다.

387
00:17:42,490 --> 00:17:45,490
특히 로봇은 인지와 행동의 순환

388
00:17:45,490 --> 00:17:48,500
고리를 반드시 이해해야 합니다.

389
00:17:48,500 --> 00:17:51,310
로봇은 세상을 보고 목표를

390
00:17:51,310 --> 00:17:53,860
이해하며, 인지한 환경에서 행동할

391
00:17:53,860 --> 00:17:55,700
수 있어야 합니다.

392
00:17:55,700 --> 00:17:58,300
때로는 로봇이 환경의 전체 상태를 알

393
00:17:58,300 --> 00:17:59,270
필요가 없습니다.

394
00:17:59,270 --> 00:18:01,280
예를 들어, 셔츠 단추를

395
00:18:01,280 --> 00:18:04,510
채우는 경우, 단추 주변의 국소 영역만

396
00:18:04,510 --> 00:18:05,720
알면 충분합니다.

397
00:18:05,720 --> 00:18:07,660
그래서 인지는 작업과 하위

398
00:18:07,660 --> 00:18:10,540
의사결정 시스템과 밀접하게 결합되고

399
00:18:10,540 --> 00:18:11,980
공동 설계되어야 하며,

400
00:18:11,980 --> 00:18:14,740
로봇이 환경의 관련 영역이나 작업

401
00:18:14,740 --> 00:18:18,760
관련 영역에 집중할 수 있도록 해야 합니다. 이것이

402
00:18:18,760 --> 00:18:22,600
바로 인지와 행동의 가장 밀접한 순환 고리입니다.

403
00:18:22,600 --> 00:18:25,750
이것은 매우 구체적인 고려사항이며, 로봇의 인지가

404
00:18:25,750 --> 00:18:27,680
일반적으로 사람들이 컴퓨터 비전에서

405
00:18:27,680 --> 00:18:29,180
생각하는 것과 어떻게

406
00:18:29,180 --> 00:18:31,250
다를 수 있는지에 관한 내용입니다.

407
00:18:31,250 --> 00:18:34,370
이제 로봇이 세상을 보는 것뿐 아니라

408
00:18:34,370 --> 00:18:36,980
세상에서 행동할 수 있게 하는

409
00:18:36,980 --> 00:18:39,300
알고리즘에 대해 이야기하겠습니다.

410
00:18:39,300 --> 00:18:42,905
먼저 강화학습부터 시작하겠습니다.

411
00:18:42,905 --> 00:18:46,230
앞서 이 이미지를 본 적이 있죠.

412
00:18:46,230 --> 00:18:48,770
로봇은 이 환경에서

413
00:18:48,770 --> 00:18:51,690
행동하고 보상을 받아야 합니다.

414
00:18:51,690 --> 00:18:54,170
이 최적화 문제를 해결하는

415
00:18:54,170 --> 00:18:56,720
매우 일반적인 방법은

416
00:18:56,720 --> 00:19:01,310
로봇이 세상과 최대한 광범위하고 대규모로

417
00:19:01,310 --> 00:19:03,680
상호작용하도록 하는 것입니다.

418
00:19:03,680 --> 00:19:06,545
모든 경험 데이터를 수집하고

419
00:19:06,545 --> 00:19:09,660
시행착오를 반복하는 거죠.

420
00:19:09,660 --> 00:19:13,010
로봇이 어떤 행동이 높은 보상을 주고 어떤 행동이 낮은

421
00:19:13,010 --> 00:19:15,210
보상을 주는지 이해할 수 있도록 합니다.

422
00:19:15,210 --> 00:19:18,470
그리고 에이전트의 행동을 더 높은 보상을

423
00:19:18,470 --> 00:19:21,990
주는 행동 쪽으로 조정할 수 있습니다.

424
00:19:21,990 --> 00:19:25,050
즉, 강화학습의 일반적인 아이디어는

425
00:19:25,050 --> 00:19:29,100
에이전트가 환경과 지속적으로 상호작용하며

426
00:19:29,100 --> 00:19:30,960
시행착오를 통해 보상을

427
00:19:30,960 --> 00:19:34,960
최대화하거나 비용을 최소화하도록 하는 방법입니다.

428
00:19:34,960 --> 00:19:38,910
여기서 강화학습과 지도학습의 차이를

429
00:19:38,910 --> 00:19:41,460
좀 더 구체적으로

430
00:19:41,460 --> 00:19:43,410
설명하고자 합니다.

431
00:19:43,410 --> 00:19:46,110
이것이 강화학습의 전형적인

432
00:19:46,110 --> 00:19:46,870
프레임워크입니다.

433
00:19:46,870 --> 00:19:47,970
환경이 있습니다.

434
00:19:47,970 --> 00:19:49,840
환경은 에이전트에게 상태를 제공합니다.

435
00:19:49,840 --> 00:19:51,150
에이전트는 행동을 생성합니다.

436
00:19:51,150 --> 00:19:55,830
환경은 에이전트에게 보상이라는 피드백을 줍니다.

437
00:19:55,830 --> 00:19:58,290
환경은 변화하며, 에이전트에게

438
00:19:58,290 --> 00:20:01,120
다음 상태 st+1을

439
00:20:01,120 --> 00:20:04,140
주고, 본질적으로 시간적 순서가

440
00:20:04,140 --> 00:20:06,750
있는 시퀀스에서 에이전트가

441
00:20:06,750 --> 00:20:09,960
순차적 결정을 내려야 합니다.

442
00:20:09,960 --> 00:20:13,350
이것이 지도학습의 전형적인

443
00:20:13,350 --> 00:20:15,640
모습입니다.

444
00:20:15,640 --> 00:20:17,080
데이터셋이 있습니다.

445
00:20:17,080 --> 00:20:20,865
데이터셋이 모델에 입력 x를 제공합니다.

446
00:20:20,865 --> 00:20:23,550
모델은 예측 y를 생성합니다.

447
00:20:23,550 --> 00:20:26,190
모델의 예측과 데이터셋의

448
00:20:26,190 --> 00:20:28,680
정답을 비교해 손실을

449
00:20:28,680 --> 00:20:30,520
계산할 수 있습니다.

450
00:20:30,520 --> 00:20:34,103
이것이 지도학습의 전형적인 설정입니다.

451
00:20:34,103 --> 00:20:36,270
강화 학습과 지도 학습의

452
00:20:36,270 --> 00:20:38,760
주요 차이점 중 하나는

453
00:20:38,760 --> 00:20:43,230
환경이 확률적일 수 있다는 점입니다. 같은

454
00:20:43,230 --> 00:20:45,660
행동이라도 환경이 다르게

455
00:20:45,660 --> 00:20:47,770
변할 수 있습니다.

456
00:20:47,770 --> 00:20:50,110
예를 들어, 상자를 앞으로

457
00:20:50,110 --> 00:20:53,380
밀 때, 지지력의 분포에 따라

458
00:20:53,380 --> 00:20:56,850
같은 행동이 상자를 다른 각도로

459
00:20:56,850 --> 00:21:00,040
회전시킬 수 있습니다. 즉,

460
00:21:00,040 --> 00:21:02,760
환경에 불확실성과 확률성이

461
00:21:02,760 --> 00:21:05,790
존재해서 환경의 행동이 확률적으로

462
00:21:05,790 --> 00:21:08,970
나타나고, 이로 인해 에이전트가

463
00:21:08,970 --> 00:21:13,320
받는 보상도 확률적이어서 같은 행동이

464
00:21:13,320 --> 00:21:17,290
항상 같은 보상을 주지 않을 수 있습니다.

465
00:21:17,290 --> 00:21:19,840
이 점이 지도 학습과 매우 다릅니다.

466
00:21:19,840 --> 00:21:23,800
우리는 불확실한 동적 시스템을 다루고 있는 겁니다.

467
00:21:23,800 --> 00:21:27,870
두 번째는 크레딧 할당 문제에 관한 것입니다.

468
00:21:27,870 --> 00:21:30,350
지도 학습에서는 입력을 주고 출력을

469
00:21:30,350 --> 00:21:33,200
예측한 뒤, 바로 손실을 계산합니다.

470
00:21:33,200 --> 00:21:36,040
그래서 특정 예측에서 어떤

471
00:21:36,040 --> 00:21:38,560
실수와 오류가 있었는지 바로

472
00:21:38,560 --> 00:21:40,190
알 수 있습니다.

473
00:21:40,190 --> 00:21:41,710
하지만 강화

474
00:21:41,710 --> 00:21:43,640
학습이나 순차적 의사결정

475
00:21:43,640 --> 00:21:46,390
분야에서는 보상이 지연될 수

476
00:21:46,390 --> 00:21:50,380
있습니다. 예를 들어 바둑 게임을 할 때,

477
00:21:50,380 --> 00:21:53,950
에피소드가 끝날 때까지 내가 이겼는지

478
00:21:53,950 --> 00:21:56,100
졌는지 알 수 없습니다.

479
00:21:56,100 --> 00:21:58,150
그리고 보상은 거기 01입니다.

480
00:21:58,150 --> 00:22:02,410
[잘 들리지 않음]은 아주 초기 단계, 어쩌면 첫

481
00:22:02,410 --> 00:22:04,750
단계나 게임 중간 단계에서

482
00:22:04,750 --> 00:22:06,200
발생하는 것입니다.

483
00:22:06,200 --> 00:22:08,590
그래서 이 순차적 의사결정

484
00:22:08,590 --> 00:22:11,470
과정에서 얻는 보상을 모든

485
00:22:11,470 --> 00:22:15,370
행동에 적절히 할당하는 방법도 강화학습으로

486
00:22:15,370 --> 00:22:18,650
답을 찾고자 하는 매우 까다롭고

487
00:22:18,650 --> 00:22:20,840
중요한 질문입니다.

488
00:22:20,840 --> 00:22:25,250
세 번째는 이 동적 시스템의 비확산

489
00:22:25,250 --> 00:22:27,230
능력입니다.

490
00:22:27,230 --> 00:22:30,570
예를 들어, 지도학습에서는 입력이 있습니다.

491
00:22:30,570 --> 00:22:32,947
입력을 모델에 넣습니다.

492
00:22:32,947 --> 00:22:33,780
출력을 얻습니다.

493
00:22:33,780 --> 00:22:34,830
손실을 계산합니다.

494
00:22:34,830 --> 00:22:37,980
그래서 이 과정 전체가 미분 가능하다는 겁니다.

495
00:22:37,980 --> 00:22:41,540
그래서 모델 내 파라미터에 대한 손실 함수의

496
00:22:41,540 --> 00:22:44,430
기울기를 직접 구할 수 있습니다.

497
00:22:44,430 --> 00:22:46,280
하지만 강화학습에서는

498
00:22:46,280 --> 00:22:50,360
환경이 종종 미분 불가능한 경우가 많아서 보통

499
00:22:50,360 --> 00:22:51,840
그렇지 않습니다.

500
00:22:51,840 --> 00:22:53,780
그래서 보상에 대한 기울기를

501
00:22:53,780 --> 00:22:57,840
행동에 대해 제대로 구하는 것이 까다로울 수 있습니다.

502
00:22:57,840 --> 00:23:00,650
때로는 제대로 된 학습을 위해 기울기의

503
00:23:00,650 --> 00:23:04,400
0차 근사치를 구하기 위해 대규모 샘플링에

504
00:23:04,400 --> 00:23:06,140
의존해야 하기도 합니다.

505
00:23:06,140 --> 00:23:09,650
이것도 또 다른 차이점입니다.

506
00:23:09,650 --> 00:23:13,625
마지막 차이점은 환경의 상태 변화가

507
00:23:13,625 --> 00:23:17,700
여러분의 행동 결과라는

508
00:23:17,700 --> 00:23:19,350
비정상성(nonstationarity)에

509
00:23:19,350 --> 00:23:21,630
관한 것입니다.

510
00:23:21,630 --> 00:23:24,670
지도학습에서는 예측한 결과가

511
00:23:24,670 --> 00:23:27,090
데이터셋 내 다른 데이터 포인트에

512
00:23:27,090 --> 00:23:29,380
영향을 주지 않습니다.

513
00:23:29,380 --> 00:23:32,040
하지만 순차적 의사결정 문제에서는

514
00:23:32,040 --> 00:23:34,740
여러분의 행동이 다음 상태에 영향을

515
00:23:34,740 --> 00:23:35,530
미칩니다.

516
00:23:35,530 --> 00:23:38,400
이것이 강화학습 문제를

517
00:23:38,400 --> 00:23:40,920
지도학습보다 좀 더

518
00:23:40,920 --> 00:23:43,860
미묘하게 만드는 이유입니다.

519
00:23:43,860 --> 00:23:46,360
여기 좀 더 구체적인 예시들이 있습니다.

520
00:23:46,360 --> 00:23:48,880
예를 들어, 앞서 말한 Atari 게임을

521
00:23:48,880 --> 00:23:50,200
하는 경우입니다.

522
00:23:50,200 --> 00:23:52,980
목표는 최고 점수로 게임을 완료하는 것일 수

523
00:23:52,980 --> 00:23:53,610
있습니다.

524
00:23:53,610 --> 00:23:55,800
상태는 게임 화면에서 얻는

525
00:23:55,800 --> 00:23:57,370
원시 픽셀 입력입니다.

526
00:23:57,370 --> 00:23:59,110
행동은 키보드의 위, 아래,

527
00:23:59,110 --> 00:24:00,610
왼쪽, 오른쪽일 수 있습니다.

528
00:24:00,610 --> 00:24:01,755
보상은 각

529
00:24:01,755 --> 00:24:05,160
시간 단계에서 점수가

530
00:24:05,160 --> 00:24:07,800
오르거나 내리는 것입니다.

531
00:24:07,800 --> 00:24:10,620
이 분야 내의 전형적인 알고리즘들은 예를

532
00:24:10,620 --> 00:24:12,790
들어 Q-learning이나

533
00:24:12,790 --> 00:24:16,220
policy iterations 같은 분야에 속합니다.

534
00:24:16,220 --> 00:24:19,660
여기 제가 Q 함수를 학습하려고 시도한 예시들이 있습니다.

535
00:24:19,660 --> 00:24:25,570
Q 함수는 특정 상태 s에서 특정 행동 a를

536
00:24:25,570 --> 00:24:29,890
취할 때 할인된 기대 미래 누적 보상을

537
00:24:29,890 --> 00:24:31,630
측정합니다.

538
00:24:31,630 --> 00:24:33,370
이 Q 함수들은

539
00:24:33,370 --> 00:24:37,830
게임 환경과의 상호작용을 통해 얻을 수 있습니다.

540
00:24:37,830 --> 00:24:40,580
Q 함수를 학습한 후에는, 예를

541
00:24:40,580 --> 00:24:43,240
들어, 다양한 행동을 취했을

542
00:24:43,240 --> 00:24:46,850
때 얻는 Q 값을 평가할 수 있습니다.

543
00:24:46,850 --> 00:24:49,940
이 경우에는 왼쪽, 오른쪽, 위, 아래가 있습니다.

544
00:24:49,940 --> 00:24:52,280
그래서 잠재적으로 네 가지 행동이 있을 수 있습니다.

545
00:24:52,280 --> 00:24:55,570
이 네 가지 행동에 대해 Q 값을 보고 가장

546
00:24:55,570 --> 00:24:57,430
높은 Q 값을 주는 행동을

547
00:24:57,430 --> 00:24:59,120
실행할 수 있습니다.

548
00:24:59,120 --> 00:25:03,220
이것이 이 분야에서 이런 종류의 의사결정을

549
00:25:03,220 --> 00:25:06,160
가능하게 하는 겁니다.

550
00:25:06,160 --> 00:25:08,690
오늘은 다룰 내용이 많아서

551
00:25:08,690 --> 00:25:12,040
강화학습의 세부사항까지는 들어가지 않겠습니다.

552
00:25:12,040 --> 00:25:14,780
하지만 현재 최첨단 강화학습 알고리즘 중에는

553
00:25:14,780 --> 00:25:18,680
SAC, Soft Actor Critic, 그리고 PPO,

554
00:25:18,680 --> 00:25:22,040
Proximal Policy Optimization이 있습니다.

555
00:25:22,040 --> 00:25:24,140
관심 있으시면 이 알고리즘들을

556
00:25:24,140 --> 00:25:26,820
자세히 살펴보시는 걸 추천합니다.

557
00:25:26,820 --> 00:25:30,800
온라인에 많은 오픈 소스 구현과 튜토리얼이

558
00:25:30,800 --> 00:25:32,000
있습니다.

559
00:25:32,000 --> 00:25:33,770
여기서는 강화학습,

560
00:25:33,770 --> 00:25:36,020
특히 Q-learning

561
00:25:36,020 --> 00:25:39,320
과정을 통해 얻을 수 있는 결과

562
00:25:39,320 --> 00:25:42,215
몇 가지를 강조하고 싶습니다.

563
00:25:42,215 --> 00:25:44,840
이것은 Google DeepMind가

564
00:25:44,840 --> 00:25:47,030
개발한 것으로, Atari 세계에서

565
00:25:47,030 --> 00:25:50,900
Breakout 게임을 플레이하는 에이전트를 개발하려는

566
00:25:50,900 --> 00:25:51,810
시도입니다.

567
00:25:51,810 --> 00:25:55,650
훈련 10분 만에 에이전트가

568
00:25:55,650 --> 00:25:59,250
공을 맞출 수 있지만,

569
00:25:59,250 --> 00:26:03,710
자주 놓치기도 합니다.

570
00:26:03,710 --> 00:26:06,350
그리고 더 학습한

571
00:26:06,350 --> 00:26:10,400
후, 예를 들어 2시간 훈련

572
00:26:10,400 --> 00:26:14,070
후에는 에이전트가 훨씬

573
00:26:14,070 --> 00:26:18,100
더 신뢰성 있고 일관되게

574
00:26:18,100 --> 00:26:21,270
공을 거의 항상 잡아내고

575
00:26:21,270 --> 00:26:27,480
점점 더 많은 보상을 얻을 수 있습니다.

576
00:26:27,480 --> 00:26:31,650
완전한 몇 시간 훈련 후에는 흥미로운

577
00:26:31,650 --> 00:26:35,490
일이 일어나는데, 에이전트가 실제로

578
00:26:35,490 --> 00:26:39,400
많은 분들이 모를 수도 있는 새로운 전략을

579
00:26:39,400 --> 00:26:42,090
발견합니다. 즉, 왼쪽

580
00:26:42,090 --> 00:26:46,415
벽에 터널을 만들기 위해 공을 튕겨내고,

581
00:26:46,415 --> 00:26:50,010
그 다음에는 벽 위쪽으로 공을

582
00:26:50,010 --> 00:26:55,060
밀어 효율적으로 벽돌들을 제거하는 전략입니다.

583
00:26:55,060 --> 00:26:56,520
이런 전략은 강화학습을

584
00:26:56,520 --> 00:26:59,130
통해 발견될 수 있습니다.

585
00:26:59,130 --> 00:27:01,680
강화학습의 좋은 점은 에이전트가

586
00:27:01,680 --> 00:27:05,760
세계와 매우 광범위하고 포괄적인 탐색과

587
00:27:05,760 --> 00:27:09,280
상호작용을 할 수 있게 한다는 겁니다.

588
00:27:09,280 --> 00:27:12,640
그리고 이 강화학습 에이전트가 심지어

589
00:27:12,640 --> 00:27:15,100
최고의 인간 선수보다 더 나은

590
00:27:15,100 --> 00:27:18,620
전략을 발견하는 것도 충분히 가능합니다.

591
00:27:18,620 --> 00:27:22,260
전형적인 예가 바둑 게임입니다.

592
00:27:22,260 --> 00:27:26,480
AlphaGo가 2016년 1월에

593
00:27:26,480 --> 00:27:29,620
나왔을 때, 저도 연구

594
00:27:29,620 --> 00:27:32,650
방향을 결정하려던

595
00:27:32,650 --> 00:27:33,710
시기였습니다.

596
00:27:33,710 --> 00:27:36,550
그 전까지는 컴퓨터 비전을 위한

597
00:27:36,550 --> 00:27:37,990
딥러닝만 연구했었는데,

598
00:27:37,990 --> 00:27:40,610
AlphaGo가 나오자 저는 이런 의사결정

599
00:27:40,610 --> 00:27:43,010
문제를 연구해야겠다고 생각했습니다.

600
00:27:43,010 --> 00:27:46,280
그래서 강화학습, 모방학습을 접하기 시작했고,

601
00:27:46,280 --> 00:27:48,470
지금까지 로봇이 환경과

602
00:27:48,470 --> 00:27:50,710
물리적으로 상호작용할 수 있게

603
00:27:50,710 --> 00:27:53,190
하는 로봇 학습까지 이어졌습니다.

604
00:27:53,190 --> 00:27:55,510
단순히 수동적으로 수집된 데이터셋만

605
00:27:55,510 --> 00:27:57,380
다루는 것에 만족하지

606
00:27:57,380 --> 00:28:00,220
않고, 환경과 능동적으로 상호작용하는

607
00:28:00,220 --> 00:28:01,690
에이전트를 원했습니다.

608
00:28:01,690 --> 00:28:05,230
그렇다면 이 Q 함수가 구체적으로 어떻게

609
00:28:05,230 --> 00:28:07,220
작동하는지 질문이 생깁니다.

610
00:28:07,220 --> 00:28:10,010
보시다시피 Q 함수는 상태 s와

611
00:28:10,010 --> 00:28:12,450
행동 a를 입력으로 받습니다.

612
00:28:12,450 --> 00:28:14,870
이 데이터는 본질적으로 Q

613
00:28:14,870 --> 00:28:17,720
함수의 파라미터이며, Q는

614
00:28:17,720 --> 00:28:19,730
신경망으로 구현됩니다.

615
00:28:19,730 --> 00:28:21,960
특히 이 경우, 앞서

616
00:28:21,960 --> 00:28:24,680
말했듯이 상태는 게임 화면에서 실제로

617
00:28:24,680 --> 00:28:27,240
받는 원시 픽셀 입력입니다.

618
00:28:27,240 --> 00:28:29,820
그래서 입력은 이 네 단계, 네

619
00:28:29,820 --> 00:28:33,330
프레임이 이 Q 함수에 직접 입력될 수 있습니다.

620
00:28:33,330 --> 00:28:34,950
이미지를 다룰 때, 이 Q

621
00:28:34,950 --> 00:28:38,600
함수를 구현하는 가장 직관적인 방법은 convolutional

622
00:28:38,600 --> 00:28:41,010
neural networks를 사용하는 것입니다.

623
00:28:41,010 --> 00:28:43,040
그래서 이 오렌지색 블록처럼 convolutional

624
00:28:43,040 --> 00:28:44,490
layers가 있습니다.

625
00:28:44,490 --> 00:28:46,910
그리고 나서 fully connected

626
00:28:46,910 --> 00:28:51,030
layers를 거쳐서 이 Q 값을 직접 도출할 수 있습니다.

627
00:28:51,030 --> 00:28:54,878
이 경우, 네 개의 이산 행동이 있는데-- 아마

628
00:28:54,878 --> 00:28:56,670
좌우만 있을 겁니다.

629
00:28:56,670 --> 00:28:59,087
하지만 네 개의 이산 행동이 있다고 가정해 봅시다--

630
00:28:59,087 --> 00:28:59,790
상하, 좌우.

631
00:28:59,790 --> 00:29:02,240
그래서 특정 행동 a의 결과인

632
00:29:02,240 --> 00:29:06,060
서로 다른 Q 값 추정치를 가질 수 있습니다.

633
00:29:06,060 --> 00:29:08,700
이렇게 해서 가장 효과적이고 Q 값을

634
00:29:08,700 --> 00:29:11,490
최대화하는 행동을 결정하는 데 이 Q 값을

635
00:29:11,490 --> 00:29:12,798
사용할 수 있습니다.

636
00:29:12,798 --> 00:29:14,090
질문에 답이 되었나요?

637
00:29:17,835 --> 00:29:20,500
네, AlphaGo가 나왔을 때입니다.

638
00:29:20,500 --> 00:29:22,470
그리고 그 이후로 이 유형의

639
00:29:22,470 --> 00:29:24,420
게임 에이전트를 점점

640
00:29:24,420 --> 00:29:28,150
더 발전시키고 개선하는 많은 발전이 있었습니다.

641
00:29:28,150 --> 00:29:31,000
그 다음에 AlphaGo Zero가 나왔습니다.

642
00:29:31,000 --> 00:29:34,210
본질적으로 AlphaGo의 단순화된

643
00:29:34,210 --> 00:29:36,150
버전으로, 초기화에 어떤

644
00:29:36,150 --> 00:29:39,040
모방 학습도 사용하지 않습니다.

645
00:29:39,040 --> 00:29:42,540
그리고 당시 1위 선수인 Ke

646
00:29:42,540 --> 00:29:45,120
Jie를 이길 수 있었습니다.

647
00:29:45,120 --> 00:29:48,210
이것은 AI 커뮤니티에서 사람들이 배운

648
00:29:48,210 --> 00:29:51,690
한 가지 교훈인데, Rich Sutton이

649
00:29:51,690 --> 00:29:54,580
말한 bitter lesson이라고

650
00:29:54,580 --> 00:29:57,810
부를 수 있습니다. 때로는 가장 단순한

651
00:29:57,810 --> 00:30:01,740
방법이 확장성과 가장 잘 호환된다는 겁니다.

652
00:30:01,740 --> 00:30:04,820
규모 확장의 힘을 활용하고 싶을 때가 있다는 뜻입니다.

653
00:30:04,820 --> 00:30:07,480
그리고 때로는 이 방법을 더 단순하게

654
00:30:07,480 --> 00:30:09,670
만드는 것이, 확장에 사용할 수

655
00:30:09,670 --> 00:30:13,030
있는 인프라와 더 잘 호환되게 하여 실제로 더

656
00:30:13,030 --> 00:30:15,200
나은 성능을 낼 수 있습니다.

657
00:30:15,200 --> 00:30:18,790
그리고 나중에 그들은 Alpha Zero를

658
00:30:18,790 --> 00:30:21,670
개발했는데, 이 알고리즘은 체스뿐만

659
00:30:21,670 --> 00:30:23,200
아니라 바둑, 장기

660
00:30:23,200 --> 00:30:26,500
같은 다른 게임에도 일반화할 수 있습니다.

661
00:30:26,500 --> 00:30:30,085
그리고 그들은 MuZero를 설계했는데, 이

662
00:30:30,085 --> 00:30:32,680
모델은 단순한 모델 프리

663
00:30:32,680 --> 00:30:35,500
강화학습뿐만 아니라 잠재 공간 동역학 모델을

664
00:30:35,500 --> 00:30:40,090
학습해 더 나은 성능을 내는 계획을 할 수 있습니다.

665
00:30:40,090 --> 00:30:42,790
특히 게임 분야에서는

666
00:30:42,790 --> 00:30:47,380
이것이 강화학습 에이전트의 더 나은, 더

667
00:30:47,380 --> 00:30:51,220
효율적이고 확장 가능한 설계와

668
00:30:51,220 --> 00:30:54,280
개발을 가능하게 한다고

669
00:30:54,280 --> 00:30:56,305
말씀드릴 수 있습니다.

670
00:30:56,305 --> 00:31:01,950
2019년 11월, AlphaGo에게 패배한 이세돌 선수가

671
00:31:01,950 --> 00:31:03,780
은퇴를 발표했습니다.

672
00:31:03,780 --> 00:31:05,910
그는 당시 어떤

673
00:31:05,910 --> 00:31:09,560
인간 선수도 최고의 AI 에이전트를

674
00:31:09,560 --> 00:31:14,135
이길 수 없다는 것을 깨달았습니다.

675
00:31:14,135 --> 00:31:16,250
그리고 그 이후로

676
00:31:16,250 --> 00:31:20,000
스타크래프트, 도타 같은 더 복잡한

677
00:31:20,000 --> 00:31:23,720
게임들이 등장했는데, 충분한 컴퓨팅

678
00:31:23,720 --> 00:31:28,220
자원과 잘 설계된 알고리즘, 인프라가

679
00:31:28,220 --> 00:31:29,940
있다면 강화학습으로

680
00:31:29,940 --> 00:31:33,470
훨씬 더 복잡한 게임에서도

681
00:31:33,470 --> 00:31:35,900
매우 뛰어난 성능을 낼

682
00:31:35,900 --> 00:31:39,230
수 있다는 것을 보여줬습니다.

683
00:31:39,230 --> 00:31:42,530
그래서 적절히 설계된 게임이라면

684
00:31:42,530 --> 00:31:45,950
충분한 자원을 투입할 경우 매우 강력한

685
00:31:45,950 --> 00:31:48,300
게임 에이전트를 만들 수

686
00:31:48,300 --> 00:31:51,230
있다고 말씀드릴 수 있습니다.

687
00:31:51,230 --> 00:31:54,028
게임뿐만 아니라, 사람들은

688
00:31:54,028 --> 00:31:55,820
실제 물리적 세계에서

689
00:31:55,820 --> 00:31:58,760
작동하는 강화학습 알고리즘과

690
00:31:58,760 --> 00:32:01,125
에이전트도 개발해왔습니다.

691
00:32:01,125 --> 00:32:04,950
왼쪽에 보이는 것은 ETH에서 발표한 2020년

692
00:32:04,950 --> 00:32:08,140
Science Robotics 논문 작업입니다.

693
00:32:08,140 --> 00:32:11,700
이 논문은 실제 물리 로봇에 강화학습이 얼마나 유용할

694
00:32:11,700 --> 00:32:14,290
수 있는지에 대한 제 생각을 완전히

695
00:32:14,290 --> 00:32:15,930
바꿨습니다. 이전에는 주로

696
00:32:15,930 --> 00:32:17,580
게임 분야에만 적용됐었죠.

697
00:32:17,580 --> 00:32:19,950
게임에서는 가능한 한

698
00:32:19,950 --> 00:32:23,140
많은 게임을 생성할 수 있습니다.

699
00:32:23,140 --> 00:32:26,250
하지만 실제 세계에서는 시뮬레이션과 현실 간의 차이, 즉
sim-to-real

700
00:32:26,250 --> 00:32:28,060
갭이 항상 존재합니다. 게임에서는

701
00:32:28,060 --> 00:32:29,710
같은 게임에서 훈련하고 테스트하지만,

702
00:32:29,710 --> 00:32:32,920
로봇의 경우 시뮬레이션에서 훈련했을 때 이

703
00:32:32,920 --> 00:32:35,940
sim-to-real 갭이 실제 환경에 일반화하는 데 얼마나

704
00:32:35,940 --> 00:32:37,960
영향을 미치는지가 문제입니다.

705
00:32:37,960 --> 00:32:40,140
이 논문은 때로는 sim-to-real

706
00:32:40,140 --> 00:32:42,810
갭이 그렇게 크게 중요하지 않을 수도 있다는

707
00:32:42,810 --> 00:32:44,735
점을 저에게 확신시켜줬습니다.

708
00:32:44,735 --> 00:32:46,540
그래서 우리는 덤불을 시뮬레이션하지 않습니다.

709
00:32:46,540 --> 00:32:48,130
우리는 눈을 시뮬레이션하는 것이 아닙니다.

710
00:32:48,130 --> 00:32:50,647
하지만 강화학습을 사용하는 에이전트는

711
00:32:50,647 --> 00:32:52,230
그 훈련

712
00:32:52,230 --> 00:32:54,120
시뮬레이션을 통해 눈이나 매우

713
00:32:54,120 --> 00:32:58,230
미끄러운 표면 같은 실제 물리적 세계에서 매우 견고한

714
00:32:58,230 --> 00:33:00,420
성능을 보여줄 수 있습니다.

715
00:33:00,420 --> 00:33:02,370
오른쪽에는 Unitree가

716
00:33:02,370 --> 00:33:07,410
최근에 공개한 영상이 있는데, 시뮬레이션에서 실제로 전이하는

717
00:33:07,410 --> 00:33:11,470
sim-to-real 기술로 로봇이 매우 역동적이고 놀라운

718
00:33:11,470 --> 00:33:14,010
동작을 할 수 있는 또 다른 수준의

719
00:33:14,010 --> 00:33:15,820
운동 능력을 보여줍니다.

720
00:33:15,820 --> 00:33:19,090
이 로봇들은 매우 거칠고 도전적인 지형도 탐색할 수 있습니다.

721
00:33:19,090 --> 00:33:22,450
로봇 운동 분야에서는 거의 해결된

722
00:33:22,450 --> 00:33:26,170
문제에 가깝다고 할 수 있습니다.

723
00:33:26,170 --> 00:33:29,280
그리고 이 문제의 해결책은 바로

724
00:33:29,280 --> 00:33:30,670
강화학습입니다.

725
00:33:30,670 --> 00:33:33,420
이것은 운동에 관한 이야기입니다.

726
00:33:33,420 --> 00:33:37,140
다른 분야는 조작에 관한 것으로, 로봇이 실제

727
00:33:37,140 --> 00:33:40,910
물리적 세계에서 물체를 조작해야 하는 경우입니다.

728
00:33:40,910 --> 00:33:44,790
2019년에 OpenAI가 로보틱스에 처음

729
00:33:44,790 --> 00:33:48,150
도전했을 때, 루빅스 큐브를

730
00:33:48,150 --> 00:33:51,580
능숙하게 조작하는 시스템을 설계했습니다.

731
00:33:51,580 --> 00:33:54,000
그들은 시뮬레이션에서 강화학습을

732
00:33:54,000 --> 00:33:56,740
수행하고 sim-to-real 전이를

733
00:33:56,740 --> 00:34:00,860
통해 이런 로봇들이 루빅스 큐브를 해결할 수 있게 했습니다.

734
00:34:00,860 --> 00:34:04,328
하지만 한 가지 단점은 성공률이 매우 낮다는 점입니다.

735
00:34:04,328 --> 00:34:06,620
이 영상들은 매우 잘 만들어진 것처럼

736
00:34:06,620 --> 00:34:08,480
보이지만, 성공률은 매우 낮았습니다.

737
00:34:08,480 --> 00:34:10,840
논문을 자세히 보면,

738
00:34:10,840 --> 00:34:14,000
시험 횟수가 매우 제한적이었고,

739
00:34:14,000 --> 00:34:17,710
그 수치를 고려하면 신뢰성이 만족스럽지

740
00:34:17,710 --> 00:34:19,940
않을 수 있습니다.

741
00:34:19,940 --> 00:34:22,330
그럼에도 불구하고,

742
00:34:22,330 --> 00:34:27,130
이후 사람들은 이 능숙한 조작 문제를

743
00:34:27,130 --> 00:34:30,850
확장하여 로봇이 다양한 물체를

744
00:34:30,850 --> 00:34:33,850
더 정교하게 조작하고

745
00:34:33,850 --> 00:34:37,719
재배치할 수 있게 했습니다.

746
00:34:37,719 --> 00:34:40,750
이 모든 것은 강화학습의

747
00:34:40,750 --> 00:34:42,250
발전 덕분입니다.

748
00:34:42,250 --> 00:34:47,469
하지만 지금까지 운동과 손 조작 예시를 보면, 문제를

749
00:34:47,469 --> 00:34:49,612
완전히 해결하지는 못했습니다.

750
00:34:49,612 --> 00:34:51,820
그래도 로봇이 집에서

751
00:34:51,820 --> 00:34:53,903
옷을 개거나 세탁을

752
00:34:53,903 --> 00:34:55,790
대신해준다면 좋겠죠.

753
00:34:55,790 --> 00:34:59,490
조작 분야는 여전히 이런 고립된

754
00:34:59,490 --> 00:35:03,230
환경에서 작업하는 경우가 많습니다.

755
00:35:03,230 --> 00:35:07,010
이것이 기존의 모델 프리 강화학습의

756
00:35:07,010 --> 00:35:10,040
주요 도전과 병목 중 일부입니다.

757
00:35:10,040 --> 00:35:12,620
대부분 환경과의 시행착오를

758
00:35:12,620 --> 00:35:16,670
통해 학습하며, 세계와의 광범위한 상호작용이

759
00:35:16,670 --> 00:35:17,940
필요합니다.

760
00:35:17,940 --> 00:35:20,782
예를 들어 AlphaGo Zero는

761
00:35:20,782 --> 00:35:24,710
40일 동안 3,000년치 인간 지식을 학습했는데,

762
00:35:24,710 --> 00:35:26,070
정말 놀랍습니다.

763
00:35:26,070 --> 00:35:29,960
하지만 여전히 에이전트가

764
00:35:29,960 --> 00:35:32,930
학습하는 데 수년간의 계산이

765
00:35:32,930 --> 00:35:34,870
필요합니다.

766
00:35:34,870 --> 00:35:37,740
만약 시뮬레이션과 현실 간

767
00:35:37,740 --> 00:35:39,890
차이가 크고 실제

768
00:35:39,890 --> 00:35:43,130
물리 세계에서 강화학습을 하려면,

769
00:35:43,130 --> 00:35:45,620
이는 학습에 큰 병목이

770
00:35:45,620 --> 00:35:47,225
될 것입니다.

771
00:35:47,225 --> 00:35:50,450
또한 시뮬레이션과 현실 간 차이가 있다면,

772
00:35:50,450 --> 00:35:52,660
실제 환경에서만 학습하는

773
00:35:52,660 --> 00:35:54,870
경우 안전 문제도 많습니다.

774
00:35:54,870 --> 00:35:57,270
예를 들어, 여기 인간형 로봇을

775
00:35:57,270 --> 00:35:58,770
앞으로 움직이게

776
00:35:58,770 --> 00:36:02,250
하는 에이전트의 학습 진행 과정을 보여주는 예가

777
00:36:02,250 --> 00:36:03,010
있습니다.

778
00:36:03,010 --> 00:36:04,230
학습 과정

779
00:36:04,230 --> 00:36:05,700
중에, 마지막에는

780
00:36:05,700 --> 00:36:08,140
로봇이 앞으로 움직일 수

781
00:36:08,140 --> 00:36:10,590
있지만, 매우 이상한 행동들이

782
00:36:10,590 --> 00:36:12,450
많아 실제 로봇에

783
00:36:12,450 --> 00:36:14,890
적용하면 큰 실패가 날 수

784
00:36:14,890 --> 00:36:17,865
있음을 쉽게 상상할 수 있습니다.

785
00:36:17,865 --> 00:36:21,910
또한 해석 가능성이 매우 제한적입니다.

786
00:36:21,910 --> 00:36:24,360
문제가 생겼을 때

787
00:36:24,360 --> 00:36:26,280
수정하기도 어렵습니다.

788
00:36:26,280 --> 00:36:28,950
흥미로운 점은, 인간이 환경과

789
00:36:28,950 --> 00:36:31,020
상호작용하는 방식을

790
00:36:31,020 --> 00:36:34,150
생각해 보면, 순수 강화학습과 달리

791
00:36:34,150 --> 00:36:38,160
인간은 환경에 대해 매우 직관적인 이해를 가지고

792
00:36:38,160 --> 00:36:39,370
있습니다.

793
00:36:39,370 --> 00:36:41,340
특정 행동을 취하면 환경이

794
00:36:41,340 --> 00:36:44,530
어떻게 변할지 상상할 수 있습니다.

795
00:36:44,530 --> 00:36:47,220
이 예측 능력이 바로 인간이 특정

796
00:36:47,220 --> 00:36:50,470
목표를 달성하기 위해 행동을 계획할 수

797
00:36:50,470 --> 00:36:52,255
있게 하는 것입니다.

798
00:36:52,255 --> 00:36:55,600
그리고 이 예측 능력은 인간이

799
00:36:55,600 --> 00:36:57,940
실제 물리 세계와의

800
00:36:57,940 --> 00:37:02,140
상호작용과 일상 경험을 통해 배운 것입니다.

801
00:37:02,140 --> 00:37:04,010
강화 학습을 넘어서서,

802
00:37:04,010 --> 00:37:06,940
다음으로 논의하고 싶은 주제는

803
00:37:06,940 --> 00:37:10,960
로봇에게 자신의 행동이 미칠 영향을 상상하는

804
00:37:10,960 --> 00:37:13,760
능력과 모델 기반 계획을

805
00:37:13,760 --> 00:37:15,530
부여하는 방법입니다.

806
00:37:15,530 --> 00:37:18,230
구체적인 예로는 시뮬레이션이 있습니다.

807
00:37:18,230 --> 00:37:20,900
일반적으로 시뮬레이션으로 많이 사용하는 것은 예를 들어

808
00:37:20,900 --> 00:37:23,910
NVIDIA에서 개발한 Isaac Gym이나 Isaac Sim입니다.

809
00:37:23,910 --> 00:37:27,100
본질적으로 로봇이 바닥과

810
00:37:27,100 --> 00:37:30,340
같은 다각형 형태의 표현을

811
00:37:30,340 --> 00:37:34,790
접촉하는 강체 시뮬레이션들이죠.

812
00:37:34,790 --> 00:37:36,830
예를 들어, 덤불 같은 것은 시뮬레이션하지 않습니다.

813
00:37:36,830 --> 00:37:38,895
눈 같은 것도 시뮬레이션하지 않습니다.

814
00:37:38,895 --> 00:37:40,810
하지만 사람들은 시뮬레이션

815
00:37:40,810 --> 00:37:43,960
환경을 많이 무작위화하고 마찰력,

816
00:37:43,960 --> 00:37:47,200
지형, 그리고 환경 내 여러

817
00:37:47,200 --> 00:37:51,960
물리적 파라미터를 무작위화해서, 실제 물리 세계에서

818
00:37:51,960 --> 00:37:55,350
마주치는 상황이 시뮬레이션 내 무작위화된

819
00:37:55,350 --> 00:37:57,630
분포 안의 하나의 데이터

820
00:37:57,630 --> 00:37:59,800
포인트라고 가정합니다.

821
00:37:59,800 --> 00:38:02,370
그래서 정책이 그 분포 내에서 로봇을

822
00:38:02,370 --> 00:38:05,107
견고하게 제어할 수 있다면, 실제 세계가 그

823
00:38:05,107 --> 00:38:06,690
분포 내 하나의 데이터

824
00:38:06,690 --> 00:38:09,550
포인트이므로 정책이 일반화할 수 있습니다.

825
00:38:09,550 --> 00:38:12,130
지금까지 경험적 증거로 보면, 이런

826
00:38:12,130 --> 00:38:14,020
가정은 실제로 맞고 정책이

827
00:38:14,020 --> 00:38:16,530
실제 물리 세계에서도 매우 신뢰성

828
00:38:16,530 --> 00:38:18,390
있고 견고하게 작동합니다.

829
00:38:18,390 --> 00:38:20,800
그래서 실제 명령이 무엇인지에 관한 질문이 생깁니다.

830
00:38:20,800 --> 00:38:23,530
사실 기존 데모들 중 많은 경우에

831
00:38:23,530 --> 00:38:26,430
사람이 로봇에게 고수준 명령을

832
00:38:26,430 --> 00:38:27,100
제공합니다.

833
00:38:27,100 --> 00:38:29,490
예를 들어, 로봇이 어느 방향으로 걸어야 하는지 말이죠.

834
00:38:29,490 --> 00:38:32,430
로봇이 제자리에서 회전할지 아니면 계속 앞으로

835
00:38:32,430 --> 00:38:33,510
걸을지 결정합니다.

836
00:38:33,510 --> 00:38:37,270
사람이 제공한 고수준 명령에 조건을 두고, 로봇은

837
00:38:37,270 --> 00:38:40,000
저수준 행동을 결정해야 합니다.

838
00:38:40,000 --> 00:38:41,550
저수준 행동은

839
00:38:41,550 --> 00:38:45,480
보통 로봇의 각 관절에 적용되는

840
00:38:45,480 --> 00:38:47,730
관절 토크

841
00:38:47,730 --> 00:38:49,270
같은 것입니다.

842
00:38:49,270 --> 00:38:52,020
이런 식으로 보통 진행됩니다, 즉 사람이

843
00:38:52,020 --> 00:38:54,147
고수준 명령을 주고, 그 조건에

844
00:38:54,147 --> 00:38:55,480
따라 행동하는 겁니다.

845
00:38:55,480 --> 00:38:57,450
로봇은 관절 토크를 사용해

846
00:38:57,450 --> 00:39:00,570
구현된 이 정책을 사용해서 이런 종류의 저수준

847
00:39:00,570 --> 00:39:02,520
행동을 결정해야 합니다.

848
00:39:02,520 --> 00:39:05,850
제가 언급했듯이, 보행 연구에서 제가 배운

849
00:39:05,850 --> 00:39:08,727
가장 큰 교훈 중 하나는 시뮬레이션이

850
00:39:08,727 --> 00:39:10,810
완벽할 필요는 없다는 겁니다.

851
00:39:10,810 --> 00:39:12,460
충분히 무작위성을 주기만

852
00:39:12,460 --> 00:39:15,960
하면 실제 환경에서 매우 강건하게 일반화할 수 있습니다.

853
00:39:15,960 --> 00:39:19,200
하지만 이런 교훈은 조작 분야에서는 아직

854
00:39:19,200 --> 00:39:21,330
잘 일반화되지 않았습니다.

855
00:39:21,330 --> 00:39:23,850
그래서 조작 분야에서는 시뮬레이션이 얼마나

856
00:39:23,850 --> 00:39:26,460
정확해야 하는지, 그리고 시뮬레이션과

857
00:39:26,460 --> 00:39:29,950
현실 간 격차가 얼마나 중요한지는 여전히 연구 질문입니다.

858
00:39:29,950 --> 00:39:32,140
구체적인 예를 하나 드릴 수 있습니다.

859
00:39:32,140 --> 00:39:34,840
시뮬레이션에서 상자를 앞으로 밀 때,

860
00:39:34,840 --> 00:39:37,420
시뮬레이션에서는 상자가 10도 회전하는데

861
00:39:37,420 --> 00:39:39,670
실제로는 12도 회전한다면 크게

862
00:39:39,670 --> 00:39:41,170
문제되지 않을 수 있습니다.

863
00:39:41,170 --> 00:39:44,680
하지만 실제 세계에서 그립이 성공했다면,

864
00:39:44,680 --> 00:39:46,720
시뮬레이션에서는 어떤 수치적

865
00:39:46,720 --> 00:39:49,370
문제로 인해 물체가 그냥 날아가 버린다면,

866
00:39:49,370 --> 00:39:52,520
혹은 물체가 손가락 사이로 미끄러져

867
00:39:52,520 --> 00:39:54,555
나간다면, 그건 문제가 됩니다.

868
00:39:54,555 --> 00:39:56,912
그래서 시뮬레이션과 현실 간 격차가 중요한 영역이 있습니다.

869
00:39:56,912 --> 00:39:58,870
반면 조작 분야에서는 시뮬레이션과 현실 간

870
00:39:58,870 --> 00:40:01,070
격차가 크게 중요하지 않은 영역도 있습니다.

871
00:40:01,070 --> 00:40:02,830
사람들은 여전히 시뮬레이션과

872
00:40:02,830 --> 00:40:06,890
현실 간 격차가 어떻게 발생하는지, 그리고 가장 신뢰할

873
00:40:06,890 --> 00:40:09,280
수 있는 시뮬레이션-현실 전이를

874
00:40:09,280 --> 00:40:11,470
위해 시뮬레이션이 갖춰야 할

875
00:40:11,470 --> 00:40:15,220
중요한 요소들이 무엇인지 이해하려 노력하고 있습니다.

876
00:40:15,220 --> 00:40:17,660
질문을 제대로 이해했다면, 아직도

877
00:40:17,660 --> 00:40:18,790
사람이

878
00:40:18,790 --> 00:40:21,700
로봇에게 고수준 명령을 제공한다는

879
00:40:21,700 --> 00:40:22,305
말씀이시죠.

880
00:40:22,305 --> 00:40:23,680
그래서 질문은, 로봇이 인간보다

881
00:40:23,680 --> 00:40:25,760
더 나은 계획을 세울 수 있느냐는 겁니다.

882
00:40:25,760 --> 00:40:28,880
좀 더 미묘한 관점을 드릴 수 있습니다.

883
00:40:28,880 --> 00:40:31,400
이 영상들 중 많은 것들이 매우 좋아

884
00:40:31,400 --> 00:40:34,990
보이지만, 실제로는 인간 조작자가 로봇을 조작하여

885
00:40:34,990 --> 00:40:37,550
어느 경로로 갈지 선택하고 있습니다.

886
00:40:37,550 --> 00:40:40,430
예를 들어, 사람들이 일반적으로 하는 것은,

887
00:40:40,430 --> 00:40:42,370
예를 들어 거친 지형이나 바위

888
00:40:42,370 --> 00:40:43,920
더미가 있을 때입니다.

889
00:40:43,920 --> 00:40:47,480
그리고 인간은 실제로 로봇에게 앞으로 가서 그런

890
00:40:47,480 --> 00:40:50,010
바위를 오르도록 명령할 수 있습니다.

891
00:40:50,010 --> 00:40:52,010
만약 실패하면, 인간은 이 바위

892
00:40:52,010 --> 00:40:54,560
더미를 우회할 수 있는 다른 고수준 명령을

893
00:40:54,560 --> 00:40:55,850
제공할 수 있습니다.

894
00:40:55,850 --> 00:40:57,800
그래서 인간 쪽에서도

895
00:40:57,800 --> 00:40:59,960
로봇의 능력을 이해하는 학습이

896
00:40:59,960 --> 00:41:01,600
있을 수 있습니다.

897
00:41:01,600 --> 00:41:04,280
이것이 바로 이런 영상들이 매우

898
00:41:04,280 --> 00:41:08,180
좋아 보이는 이유 중 하나인데, 인간이 로봇의

899
00:41:08,180 --> 00:41:10,850
한계와 능력을 보여줄 수

900
00:41:10,850 --> 00:41:13,625
있는 경로를 선택하기 때문입니다.

901
00:41:13,625 --> 00:41:15,510
그럼 이걸 자율적으로 어떻게 할 수 있을까요?

902
00:41:15,510 --> 00:41:18,200
이것은 실제로 매우 흥미로운 질문이며, 사람들이

903
00:41:18,200 --> 00:41:19,930
연구하고 있는 분야입니다.

904
00:41:22,925 --> 00:41:25,230
그럼 계속하겠습니다.

905
00:41:25,230 --> 00:41:28,850
강화학습의 성공 사례와 강력함에 대해

906
00:41:28,850 --> 00:41:30,683
이야기했고, 강화학습의

907
00:41:30,683 --> 00:41:32,850
한계도 논의했습니다.

908
00:41:32,850 --> 00:41:35,720
그리고 아직 조작 분야에서 강화학습이

909
00:41:35,720 --> 00:41:38,330
매우 성공적이고 광범위하게 적용된

910
00:41:38,330 --> 00:41:40,140
사례는 보지 못했습니다.

911
00:41:40,140 --> 00:41:42,790
그리고 우리 인간은 단순히 시행착오로만 배우지 않습니다.

912
00:41:42,790 --> 00:41:45,130
우리는 실제로 내부 모델을 구축합니다.

913
00:41:45,130 --> 00:41:47,460
그래서 질문은, 로봇이 환경과

914
00:41:47,460 --> 00:41:49,800
상호작용하면서 모델을 학습하고,

915
00:41:49,800 --> 00:41:53,070
그 모델을 사용해 더 나은 물리적 상호작용을

916
00:41:53,070 --> 00:41:55,320
할 수 있을까 하는 것입니다.

917
00:41:55,320 --> 00:41:58,110
구체적으로, 다시 이 그림으로 돌아가서,

918
00:41:58,110 --> 00:42:00,120
실제 물리 세계의

919
00:42:00,120 --> 00:42:03,610
근사 모델을 어떻게 학습할 수 있고, 이 가상

920
00:42:03,610 --> 00:42:07,155
도메인에서 실행되는 근사 물리 세계가 로봇의

921
00:42:07,155 --> 00:42:10,140
행동을 안내하고 물리 세계에서 어떤 행동을

922
00:42:10,140 --> 00:42:12,720
취할지 결정하는 데 어떻게 도움이

923
00:42:12,720 --> 00:42:15,420
될 수 있는지에 관한 것입니다.

924
00:42:15,420 --> 00:42:17,410
예를 들어, 이미 모델이

925
00:42:17,410 --> 00:42:19,868
있다고 가정하면, 우리가

926
00:42:19,868 --> 00:42:24,160
인간처럼 정신적 환경에서 모델을 학습했다고 할 때,

927
00:42:24,160 --> 00:42:26,670
현재 상태 st와 행동 t가

928
00:42:26,670 --> 00:42:29,670
주어지면 환경 상태가 다음 상태 st+1로

929
00:42:29,670 --> 00:42:33,030
어떻게 변할지 예측할 수 있습니다.

930
00:42:33,030 --> 00:42:34,890
그리고 이것을 본질적으로 현재 상태와

931
00:42:34,890 --> 00:42:37,570
행동이 주어졌을 때 다음 상태를 예측하는 순방향

932
00:42:37,570 --> 00:42:38,935
모델로 사용할 수 있습니다.

933
00:42:38,935 --> 00:42:41,350
그렇다면 계획 문제, 즉

934
00:42:41,350 --> 00:42:43,600
이 순방향 모델의

935
00:42:43,600 --> 00:42:45,817
역 문제는 무엇일까요?

936
00:42:45,817 --> 00:42:48,400
계획은 현재 상태와 목표 상태가

937
00:42:48,400 --> 00:42:51,670
주어졌을 때, 로봇이 목표 상태를 달성할 수

938
00:42:51,670 --> 00:42:54,252
있도록 하는 행동을 찾아내는 것입니다.

939
00:42:54,252 --> 00:42:56,210
파란 점으로 표시된 현재 상태와

940
00:42:56,210 --> 00:42:58,735
빨간 점으로 표시된 목표가 있을 때,

941
00:42:58,735 --> 00:43:00,610
행동이 어떻게 생겼을지 초기

942
00:43:00,610 --> 00:43:02,060
추측을 할 수 있습니다.

943
00:43:02,060 --> 00:43:05,200
그리고 우리의 근사 학습 모델은

944
00:43:05,200 --> 00:43:07,150
상태의 연속적인

945
00:43:07,150 --> 00:43:09,460
변화를 예측할 수 있는데,

946
00:43:09,460 --> 00:43:13,270
이는 녹색 궤적으로 표시됩니다.

947
00:43:13,270 --> 00:43:16,510
그다음 이 녹색 점들과 빨간

948
00:43:16,510 --> 00:43:18,920
점들 사이의 거리를

949
00:43:18,920 --> 00:43:24,445
측정하고, 이 거리의 기울기를 행동 시퀀스 전체에

950
00:43:24,445 --> 00:43:27,350
대해 역전파하거나 최적화를

951
00:43:27,350 --> 00:43:29,330
수행하여 어떤 행동이

952
00:43:29,330 --> 00:43:32,110
빨간 점, 즉 목표에

953
00:43:32,110 --> 00:43:35,890
더 가까워지게 할지 최적화합니다.

954
00:43:35,890 --> 00:43:38,670
물론 모델이 충분히 정확하지 않을

955
00:43:38,670 --> 00:43:41,810
수 있어서, 보통 첫 번째 행동만 실행하고

956
00:43:41,810 --> 00:43:44,730
환경에서 새로운 상태를 얻습니다.

957
00:43:44,730 --> 00:43:47,210
그리고 다시 경사 하강법이나

958
00:43:47,210 --> 00:43:49,130
다른 최적화

959
00:43:49,130 --> 00:43:53,345
기법을 사용해 행동 시퀀스를 재최적화합니다.

960
00:43:53,345 --> 00:43:56,870
특히 최근 GPU와 신경 동역학

961
00:43:56,870 --> 00:44:00,480
모델의 발전으로, GPU를

962
00:44:00,480 --> 00:44:02,930
사용해 병렬로

963
00:44:02,930 --> 00:44:05,120
대규모 샘플링과 행동

964
00:44:05,120 --> 00:44:07,760
시퀀스 최적화를 동시에

965
00:44:07,760 --> 00:44:12,560
수행할 수 있어 매우 효율적입니다.

966
00:44:12,560 --> 00:44:14,490
이 일반적인 프레임워크에서

967
00:44:14,490 --> 00:44:17,430
모델은 순방향 프로세스이고,

968
00:44:17,430 --> 00:44:19,460
항상 순방향 모델을 사용해

969
00:44:19,460 --> 00:44:22,130
역 최적화를 수행하여 목표

970
00:44:22,130 --> 00:44:25,100
구성에 더 가까워지도록 행동을 찾아낼

971
00:44:25,100 --> 00:44:26,335
수 있습니다.

972
00:44:26,335 --> 00:44:28,340
항상 중요한 질문 중

973
00:44:28,340 --> 00:44:30,588
하나는 환경의 올바른 표현은

974
00:44:30,588 --> 00:44:31,380
무엇인가?

975
00:44:31,380 --> 00:44:33,230
가장 효과적인 상태 표현

976
00:44:33,230 --> 00:44:34,580
s는 무엇인가?

977
00:44:34,580 --> 00:44:36,370
그리고 상태 표현을 기반으로 이 모델을

978
00:44:36,370 --> 00:44:38,185
어떻게 학습할 수 있는가? 입니다.

979
00:44:38,185 --> 00:44:40,030
수년간 다양한

980
00:44:40,030 --> 00:44:43,330
상태 표현을 선택하거나 조사하는

981
00:44:43,330 --> 00:44:45,820
연구가 많이 있었습니다.

982
00:44:45,820 --> 00:44:48,500
초기 연구 중 일부는 예를 들어

983
00:44:48,500 --> 00:44:51,520
2D 이미지만을 상태의 표현으로 사용하고,

984
00:44:51,520 --> 00:44:53,600
특정 행동을 적용했을 때

985
00:44:53,600 --> 00:44:57,400
이미지가 어떻게 변할지 픽셀 동역학을 학습하려는

986
00:44:57,400 --> 00:44:58,160
시도입니다.

987
00:44:58,160 --> 00:45:00,520
이것은 deep visual

988
00:45:00,520 --> 00:45:02,620
foresight라는 연구로, 전체

989
00:45:02,620 --> 00:45:06,040
워드 모델 분야의 초기 연구들을 촉발시켰습니다.

990
00:45:06,040 --> 00:45:08,600
이런 픽셀 기반 동역학 모델을

991
00:45:08,600 --> 00:45:10,480
학습함으로써, 현재 관찰과

992
00:45:10,480 --> 00:45:12,250
목표(녹색으로

993
00:45:12,250 --> 00:45:14,920
표시된) 사이의 거리를 최소화하거나,

994
00:45:14,920 --> 00:45:18,040
물체를 회전시키고 밀어서 목표를 달성하는

995
00:45:18,040 --> 00:45:20,870
전략을 세울 수 있습니다. 현재

996
00:45:20,870 --> 00:45:23,950
상태는 빨간색으로 표시되어 있습니다.

997
00:45:23,950 --> 00:45:26,030
즉, 이것은 픽셀 동역학에 관한 내용입니다.

998
00:45:26,030 --> 00:45:29,050
사람들이 할 수 있는 또 다른 방법은 환경을

999
00:45:29,050 --> 00:45:31,180
표현하는 데 키포인트를 사용하여

1000
00:45:31,180 --> 00:45:33,890
키포인트 동역학 모델을 학습하는 것입니다.

1001
00:45:33,890 --> 00:45:37,520
여기서는 3D 공간 위 박스의

1002
00:45:37,520 --> 00:45:42,290
키포인트 움직임을 추적하고, 밀기 동작에 따른

1003
00:45:42,290 --> 00:45:46,160
키포인트의 신경 동역학 모델을 학습할 수

1004
00:45:46,160 --> 00:45:47,220
있습니다.

1005
00:45:47,220 --> 00:45:49,880
그 후 로봇이 이 예측

1006
00:45:49,880 --> 00:45:53,750
모델을 사용해 특정 궤적을

1007
00:45:53,750 --> 00:45:56,360
추적하며 박스를 목표

1008
00:45:56,360 --> 00:46:01,100
구성으로 밀도록 행동을 계획할 수 있습니다.

1009
00:46:01,100 --> 00:46:03,080
키포인트를 사용하는 것

1010
00:46:03,080 --> 00:46:06,890
외에, 만약 더 높은 자유도를 가진 물체를 만난다면

1011
00:46:06,890 --> 00:46:08,315
어떻게 할까요?

1012
00:46:08,315 --> 00:46:10,790
한 단계 더 세밀하게 들어가면,

1013
00:46:10,790 --> 00:46:15,090
본질적으로 점들의 집합인 입자 집합을 사용해서도 그 객체들을

1014
00:46:15,090 --> 00:46:16,855
표현할 수 있습니다.

1015
00:46:16,855 --> 00:46:19,610
여기 보이는 것은 제가 포닥으로

1016
00:46:19,610 --> 00:46:22,140
있을 때 진행한 연구인데,

1017
00:46:22,140 --> 00:46:25,050
입자 여러 개를 사용해 이 알갱이

1018
00:46:25,050 --> 00:46:27,080
더미를 표현하고 특정

1019
00:46:27,080 --> 00:46:30,050
행동을 가했을 때 입자들이 어떻게

1020
00:46:30,050 --> 00:46:32,650
움직일지 예측하려고 했습니다.

1021
00:46:32,650 --> 00:46:34,780
이 순방향 모델은 로봇이

1022
00:46:34,780 --> 00:46:37,540
다양한 크기의 알갱이 객체들을

1023
00:46:37,540 --> 00:46:40,120
다룰 수 있는 역방향 의사결정을

1024
00:46:40,120 --> 00:46:42,200
할 수 있게 해줍니다.

1025
00:46:42,200 --> 00:46:43,750
그리고 우리는 각

1026
00:46:43,750 --> 00:46:46,570
세그먼트의 모서리 같은 목표

1027
00:46:46,570 --> 00:46:48,950
영역(오른쪽 아래)에 그 조각들을

1028
00:46:48,950 --> 00:46:51,610
모으는 전략을 고안했습니다.

1029
00:46:51,610 --> 00:46:55,850
같은 모델이 환경으로부터 좋은 피드백을 받으면

1030
00:46:55,850 --> 00:46:59,410
모델의 오류를 수정하고 모든

1031
00:46:59,410 --> 00:47:02,800
조각을 목표 영역에 매우 신뢰성 있게

1032
00:47:02,800 --> 00:47:06,550
모으는 전략을 만들어낼 수 있습니다.

1033
00:47:06,550 --> 00:47:08,230
분명히 이 모델은 다양한

1034
00:47:08,230 --> 00:47:11,170
크기의 알갱이 조각들에 일반화할

1035
00:47:11,170 --> 00:47:12,920
수 있을 뿐만 아니라,

1036
00:47:12,920 --> 00:47:17,063
다른 목표 구성으로도 변경할 수 있습니다.

1037
00:47:17,063 --> 00:47:18,730
여기서 목표 구성이 무엇인지

1038
00:47:18,730 --> 00:47:20,710
아주 빠르게 알 수 있을 겁니다.

1039
00:47:20,710 --> 00:47:23,320
로봇은 입자 조각들을

1040
00:47:23,320 --> 00:47:27,620
비단순하게 재분배하는 전략을 세워야 합니다.

1041
00:47:27,620 --> 00:47:29,650
재분배 후에는 모양 같은

1042
00:47:29,650 --> 00:47:31,370
세밀한 부분들을 목표에

1043
00:47:31,370 --> 00:47:33,210
맞게 정렬해야 합니다.

1044
00:47:33,210 --> 00:47:36,660
이 작업은 마치 쌓여 있는 더미를 재배치하는 작업과 같습니다.

1045
00:47:36,660 --> 00:47:39,980
여기서 과제는 이 입자 조각들을

1046
00:47:39,980 --> 00:47:42,860
A부터 Z까지 다양한 글자

1047
00:47:42,860 --> 00:47:45,860
모양으로 재배열하는 것입니다. 이런 종류의 forward 모델을 사용하면,

1048
00:47:45,860 --> 00:47:47,960
환경으로부터 피드백을

1049
00:47:47,960 --> 00:47:49,920
받으면서 로봇이 이 조각들을

1050
00:47:49,920 --> 00:47:51,530
목표 영역으로 재배치할 수

1051
00:47:51,530 --> 00:47:54,260
있는 일련의 전략을 성공적으로

1052
00:47:54,260 --> 00:47:55,500
만들어낼 수 있습니다.

1053
00:47:55,500 --> 00:47:58,940
이 작업은 사실 매우 복잡한 과제입니다.

1054
00:47:58,940 --> 00:48:03,600
더 나아가, 제가 스탠포드에 있을 때

1055
00:48:03,600 --> 00:48:05,390
참여했던 후속

1056
00:48:05,390 --> 00:48:07,640
연구도 있습니다.

1057
00:48:07,640 --> 00:48:11,000
우리는 만두 만드는 로봇을 설계했는데,

1058
00:48:11,000 --> 00:48:15,440
로봇에 15가지 다른 3D 프린팅 도구를

1059
00:48:15,440 --> 00:48:16,140
장착했습니다.

1060
00:48:16,140 --> 00:48:19,310
네 대의 RGBD 카메라가 환경을 관찰하여

1061
00:48:19,310 --> 00:48:22,560
저장소의 기하학적 구조를 재구성합니다.

1062
00:48:22,560 --> 00:48:25,610
로봇은 어떤 도구를 사용하고

1063
00:48:25,610 --> 00:48:30,390
어떤 행동을 취해야 반죽을 만두로 만들 수

1064
00:48:30,390 --> 00:48:32,610
있을지 결정해야 합니다.

1065
00:48:32,610 --> 00:48:34,140
핵심 요소는

1066
00:48:34,140 --> 00:48:39,460
입자를 사용해 표현한 forward 예측 모델입니다.

1067
00:48:39,460 --> 00:48:41,970
여기서 빨간 점은 도구의

1068
00:48:41,970 --> 00:48:43,890
모양을, 파란 점은

1069
00:48:43,890 --> 00:48:46,330
물체의 모양을 나타냅니다.

1070
00:48:46,330 --> 00:48:49,600
첫 번째 행은 우리 모델의 오픈 루프

1071
00:48:49,600 --> 00:48:53,770
예측이고, 두 번째 행은 실제 환경에서 일어난 일입니다.

1072
00:48:53,770 --> 00:48:56,250
이 학습된 모델은 실제

1073
00:48:56,250 --> 00:48:58,545
상호작용에서 직접 학습하여, 다양한

1074
00:48:58,545 --> 00:49:02,100
도구와 행동을 적용할 때 반죽 모양 변화를

1075
00:49:02,100 --> 00:49:04,720
매우 정확하게 예측할 수

1076
00:49:04,720 --> 00:49:09,030
있습니다. 결국 이 통합 시스템 덕분에 반죽으로 만두를

1077
00:49:09,030 --> 00:49:10,962
만들 수 있게 됩니다.

1078
00:49:10,962 --> 00:49:12,420
이 영상에서 흥미로운

1079
00:49:12,420 --> 00:49:14,700
점은 사람이 계속해서 로봇의

1080
00:49:14,700 --> 00:49:17,130
작업을 방해한다는 것입니다.

1081
00:49:17,130 --> 00:49:20,580
로봇은 환경에서 실시간 시각 피드백을 받아

1082
00:49:20,580 --> 00:49:22,650
반죽의 모양을 실시간으로

1083
00:49:22,650 --> 00:49:24,900
이해하고, 현재 관찰과 환경이

1084
00:49:24,900 --> 00:49:28,180
어떻게 변할지 예측하는 학습된 동역학 모델을

1085
00:49:28,180 --> 00:49:30,970
사용합니다. 이 모델은 도구를 사용해

1086
00:49:30,970 --> 00:49:32,637
특정 행동을 취할 때

1087
00:49:32,637 --> 00:49:35,860
반죽 모양이 어떻게 변할지 예측하며, 이

1088
00:49:35,860 --> 00:49:37,700
forward 모델을

1089
00:49:37,700 --> 00:49:40,210
바탕으로 역방향 결정을 내립니다.

1090
00:49:40,210 --> 00:49:42,440
이 결정은 두 가지 수준에서

1091
00:49:42,440 --> 00:49:45,385
이루어집니다. 하나는 어떤 도구를 사용할지

1092
00:49:45,385 --> 00:49:48,620
결정하는 고수준, 즉 작업 수준의 의사결정입니다.

1093
00:49:48,620 --> 00:49:50,890
그리고 이 도구들이 주어지면,

1094
00:49:50,890 --> 00:49:53,770
로봇은 다음 작업 단계로 나아가기 위해 어떤

1095
00:49:53,770 --> 00:49:57,430
구체적인 행동을 취할지 결정하는 하위 수준, 감정

1096
00:49:57,430 --> 00:49:59,450
수준의 의사결정도 해야 합니다.

1097
00:49:59,450 --> 00:50:01,900
사람들은 정말 귀찮게도 반죽에 재료를 추가하고 접는

1098
00:50:01,900 --> 00:50:02,480
작업을 합니다.

1099
00:50:02,480 --> 00:50:05,500
로봇은 이런 외부 방해에도 매우 강인하게

1100
00:50:05,500 --> 00:50:08,295
작업을 계속 진행할 수 있습니다.

1101
00:50:08,295 --> 00:50:09,620
여기서 흥미로운 점이 있습니다.

1102
00:50:09,620 --> 00:50:13,490
로봇이 원을 그리면, 인간은 가차 없이 모든

1103
00:50:13,490 --> 00:50:15,130
것을 망가뜨립니다.

1104
00:50:15,130 --> 00:50:16,600
로봇은 이런 작업

1105
00:50:16,600 --> 00:50:18,460
목표를 진행하려면

1106
00:50:18,460 --> 00:50:20,680
실제로 처음부터 다시

1107
00:50:20,680 --> 00:50:23,770
시작해야 한다는 것을 알고 있습니다.

1108
00:50:23,770 --> 00:50:26,090
이것은 우리 시스템이 이런 외부

1109
00:50:26,090 --> 00:50:28,940
방해에도 얼마나 인내심 있고 강인한지를

1110
00:50:28,940 --> 00:50:30,180
보여줍니다.

1111
00:50:30,180 --> 00:50:32,810
이 모든 능력은 특정 행동을 취했을

1112
00:50:32,810 --> 00:50:34,760
때 반죽 모양이 어떻게

1113
00:50:34,760 --> 00:50:36,410
변할지 예측하는

1114
00:50:36,410 --> 00:50:39,360
신경 역학 모델 덕분에 가능해졌습니다.

1115
00:50:39,360 --> 00:50:41,900
결국 로봇은 만두 클립

1116
00:50:41,900 --> 00:50:45,170
위에 만두피를 올리고, 만두 속을

1117
00:50:45,170 --> 00:50:48,020
만두피 위에 올린 뒤, 후크를

1118
00:50:48,020 --> 00:50:50,120
사용해 만두 클립을

1119
00:50:50,120 --> 00:50:52,430
닫아 15개의 범용

1120
00:50:52,430 --> 00:50:57,860
도구가 장착된 범용 로봇으로 반죽에서 만두를 만듭니다.

1121
00:50:57,860 --> 00:50:59,810
이것은 우리가 모델을 어떻게

1122
00:50:59,810 --> 00:51:03,440
학습하고, 그 모델이 후속 모델 기반 계획에 어떻게 유용할 수

1123
00:51:03,440 --> 00:51:04,760
있는지에 관한 내용입니다.

1124
00:51:04,760 --> 00:51:08,630
이 특정 경우를 좀 더 엄밀히 설명하면,

1125
00:51:08,630 --> 00:51:10,580
우리는 강화학습을

1126
00:51:10,580 --> 00:51:12,710
사용하지 않습니다.

1127
00:51:12,710 --> 00:51:15,660
그저 모델을 학습하고 그 모델을 사용해 계획을

1128
00:51:15,660 --> 00:51:18,560
세우는 것이며, 그 계획은 실제 환경에서 더

1129
00:51:18,560 --> 00:51:21,170
효율적으로 실행할 수 있는 정책으로

1130
00:51:21,170 --> 00:51:22,290
증류될 수 있습니다.

1131
00:51:22,290 --> 00:51:25,280
하지만 어떤 사람들은 이를 모델 기반 강화학습이라고도

1132
00:51:25,280 --> 00:51:25,780
부릅니다.

1133
00:51:25,780 --> 00:51:27,850
어떤 배경에서 오셨는지에 따라 다릅니다.

1134
00:51:27,850 --> 00:51:29,400
모델 학습과 모델 기반 계획이라고

1135
00:51:29,400 --> 00:51:30,442
부를 수도 있습니다.

1136
00:51:30,442 --> 00:51:33,040
이것을 모델 기반 강화학습이라고도 부를 수 있습니다.

1137
00:51:33,040 --> 00:51:36,690
하지만 핵심 아이디어는 로봇이 실제 물리 세계와

1138
00:51:36,690 --> 00:51:39,280
상호작용하면서 모델을 학습하고,

1139
00:51:39,280 --> 00:51:40,980
그 학습된 모델을

1140
00:51:40,980 --> 00:51:42,870
사용해 로봇이 행동을

1141
00:51:42,870 --> 00:51:45,960
결정하여 과제 목표를 진행하는 데 매우

1142
00:51:45,960 --> 00:51:47,580
효과적이라는 점입니다.

1143
00:51:47,580 --> 00:51:49,920
이 특정 경우에는 고수준 계획과 저수준

1144
00:51:49,920 --> 00:51:52,870
의사결정이 두 개의 다른 모델에 의해 수행됩니다.

1145
00:51:52,870 --> 00:51:56,310
고수준에서는 현재 상태, 환경에 대한 현재 관찰,

1146
00:51:56,310 --> 00:51:58,810
그리고 로봇이 달성해야 할 목표가

1147
00:51:58,810 --> 00:52:00,810
주어지고, 본질적으로 어떤

1148
00:52:00,810 --> 00:52:04,050
도구를 사용할지 분류하는 분류기가 있습니다.

1149
00:52:04,050 --> 00:52:06,810
이 분류기, 즉 도구 레이블에 조건부로,

1150
00:52:06,810 --> 00:52:09,300
다음 작업 단계로 진행하기

1151
00:52:09,300 --> 00:52:11,820
위해 어떤 구체적인 행동을 취할지

1152
00:52:11,820 --> 00:52:14,160
결정하는 저수준 정책이 있습니다.

1153
00:52:14,160 --> 00:52:15,010
아주 좋은 질문입니다.

1154
00:52:15,010 --> 00:52:17,350
이 연구는 2023년에 수행되었습니다.

1155
00:52:17,350 --> 00:52:21,130
그 당시에는 비주얼 언어 모델이 그리 강력하지 않았습니다.

1156
00:52:21,130 --> 00:52:25,750
그래서 그때는 인간 조작자가 10번의 작업

1157
00:52:25,750 --> 00:52:28,360
시연을 통해 데이터를

1158
00:52:28,360 --> 00:52:30,530
수집하도록 했습니다.

1159
00:52:30,530 --> 00:52:33,190
그 데이터를 사용해 어떤 도구를 사용할지

1160
00:52:33,190 --> 00:52:35,090
분류하는 분류기를 학습시켰습니다.

1161
00:52:35,090 --> 00:52:38,390
이 덕분에 이 체인 내에서 앞뒤로 자유롭게 이동할 수 있었습니다.

1162
00:52:38,390 --> 00:52:41,330
앞서 말씀드렸듯이, 로봇이 원을 자른 후 인간이

1163
00:52:41,330 --> 00:52:42,650
모든 것을 파괴하면,

1164
00:52:42,650 --> 00:52:45,460
로봇은 현재 관찰에 맞는 이전

1165
00:52:45,460 --> 00:52:47,380
단계로 점프하여 외부

1166
00:52:47,380 --> 00:52:52,360
교란으로부터 적절히 복구할 수 있다는 것을 알고 있습니다.

1167
00:52:52,360 --> 00:52:54,670
이 특정 경우에 저희가 한

1168
00:52:54,670 --> 00:52:57,400
것은 샘플링 기반 궤적

1169
00:52:57,400 --> 00:53:00,178
최적화와 정책 학습의 결합입니다.

1170
00:53:00,178 --> 00:53:02,470
현재 반죽 상태를

1171
00:53:02,470 --> 00:53:03,470
주면,

1172
00:53:03,470 --> 00:53:05,662
저희는 앞으로 예측하는 모델을 가지고 있습니다.

1173
00:53:05,662 --> 00:53:07,870
우리는 여러 가지 행동과

1174
00:53:07,870 --> 00:53:11,620
도구를 샘플링해서 반죽 모양의 변화를

1175
00:53:11,620 --> 00:53:13,765
예측할 수 있습니다.

1176
00:53:13,765 --> 00:53:15,640
그다음 모델의 예측을 우리가

1177
00:53:15,640 --> 00:53:18,080
달성하고자 하는 목표와 비교할 겁니다. 이건

1178
00:53:18,080 --> 00:53:20,170
제가 앞서 보여드린 것과 비슷합니다.

1179
00:53:20,170 --> 00:53:23,010
예를 들어, 우리 모델은 반죽 모양이 초록

1180
00:53:23,010 --> 00:53:25,920
점들로 변할 거라고 예측하지만, 목표는 빨간

1181
00:53:25,920 --> 00:53:28,840
점들입니다. 그리고 이 둘의 거리를 비교합니다.

1182
00:53:28,840 --> 00:53:30,780
이렇게 해서 목표에 최대한

1183
00:53:30,780 --> 00:53:34,080
가까워질 수 있는 가장 효과적인 행동을

1184
00:53:34,080 --> 00:53:35,830
선택할 수 있습니다.

1185
00:53:35,830 --> 00:53:37,930
이런 식으로 많은 샘플을 만들 수 있습니다.

1186
00:53:37,930 --> 00:53:41,350
하지만 테스트 시에 샘플링하는 것은 매우 시간이 많이 걸립니다.

1187
00:53:41,350 --> 00:53:44,340
그래서 우리는 이런 샘플링을 오프라인 방식으로 수행해서

1188
00:53:44,340 --> 00:53:45,400
데이터 세트를 만듭니다.

1189
00:53:45,400 --> 00:53:48,330
그리고 그 데이터 세트를 사용해

1190
00:53:48,330 --> 00:53:50,945
테스트 시에 매우 짧은 시간

1191
00:53:50,945 --> 00:53:52,320
안에 추론할 수

1192
00:53:52,320 --> 00:53:54,360
있는 정책을 학습합니다.

1193
00:53:54,360 --> 00:53:56,470
정책은 여전히 신경망으로 구성되어 있습니다.

1194
00:53:56,470 --> 00:53:56,970
네.

1195
00:53:56,970 --> 00:54:01,200
정책은 모델의 예측을 대량의

1196
00:54:01,200 --> 00:54:04,320
샘플에서 증류하여 학습합니다.

1197
00:54:04,320 --> 00:54:07,920
이 연구에서는 물리 기반 시뮬레이션을 전혀 사용하지

1198
00:54:07,920 --> 00:54:08,680
않았습니다.

1199
00:54:08,680 --> 00:54:11,280
우리는 최첨단 변형 가능한 객체 시뮬레이터인

1200
00:54:11,280 --> 00:54:13,770
MPM(Material Point Methods)을 사용하는

1201
00:54:13,770 --> 00:54:15,250
기준 모델도 가지고 있습니다.

1202
00:54:15,250 --> 00:54:19,143
우리가 깨달은 것은, 물리 기반 변형 객체

1203
00:54:19,143 --> 00:54:21,060
시뮬레이터의 파라미터를

1204
00:54:21,060 --> 00:54:24,220
매우 정밀하게 추정해도, 실제

1205
00:54:24,220 --> 00:54:27,360
상호작용에서 직접 학습한 모델보다 정확도가

1206
00:54:27,360 --> 00:54:28,860
눈에 띄게

1207
00:54:28,860 --> 00:54:30,340
떨어진다는 점입니다.

1208
00:54:30,340 --> 00:54:31,870
앞서 보여드린 것처럼, 첫

1209
00:54:31,870 --> 00:54:34,120
번째 행은 우리 모델의 오픈 루프 예측입니다.

1210
00:54:34,120 --> 00:54:35,830
두 번째 행은 실제 정답입니다.

1211
00:54:35,830 --> 00:54:37,770
우리 모델의 예측은 실제 정답과

1212
00:54:37,770 --> 00:54:39,270
매우 잘 일치합니다.

1213
00:54:39,270 --> 00:54:42,840
이는 기존의 물리 기반 시뮬레이터보다 훨씬 더

1214
00:54:42,840 --> 00:54:43,370
정확합니다.

1215
00:54:46,100 --> 00:54:46,600
네.

1216
00:54:46,600 --> 00:54:49,860
더 이상 질문이 없으면 계속하겠습니다.

1217
00:54:49,860 --> 00:54:52,320
그럼 지금까지 모델 학습에서 무엇을 논의했고,

1218
00:54:52,320 --> 00:54:54,090
이렇게 학습된 모델이 어떻게

1219
00:54:54,090 --> 00:54:56,955
후속 모델 기반 계획에 효과적인지 살펴봤습니다.

1220
00:54:56,955 --> 00:55:01,235
다음 알고리즘 카테고리는 모방 학습입니다.

1221
00:55:01,235 --> 00:55:04,680
조금 복습하자면, 강화 학습에

1222
00:55:04,680 --> 00:55:06,420
대해 논의했습니다.

1223
00:55:06,420 --> 00:55:08,190
강화 학습은 환경과의 시행착오를

1224
00:55:08,190 --> 00:55:10,793
통해 직접 정책을 학습하는 방법인데,

1225
00:55:10,793 --> 00:55:13,210
샘플 효율성이나 안전 문제 같은 여러

1226
00:55:13,210 --> 00:55:14,385
어려움이 있습니다.

1227
00:55:14,385 --> 00:55:16,510
모델 학습의 경우,

1228
00:55:16,510 --> 00:55:19,660
사실 감독 학습 범주로 돌아가서

1229
00:55:19,660 --> 00:55:22,510
환경의 진화를 데이터로

1230
00:55:22,510 --> 00:55:23,470
사용합니다.

1231
00:55:23,470 --> 00:55:25,750
그 데이터를 이용해 감독 학습으로

1232
00:55:25,750 --> 00:55:27,850
모델을 훈련시키고, 이 모델을 사용해

1233
00:55:27,850 --> 00:55:29,830
모델 기반 계획을 수행합니다.

1234
00:55:29,830 --> 00:55:32,230
그리고 단순히 감독 학습으로 모델을

1235
00:55:32,230 --> 00:55:35,390
훈련하는 대신, 정책에도 감독 학습을

1236
00:55:35,390 --> 00:55:38,270
적용할 수 있을지 묻는 사람들이 있습니다.

1237
00:55:38,270 --> 00:55:41,140
이것이 바로 모방 학습의 일반적인

1238
00:55:41,140 --> 00:55:46,570
아이디어입니다. 즉, 작업 수행 방식을 보여주는 대규모 데이터셋을 가지고

1239
00:55:46,570 --> 00:55:49,270
이 데이터를 이용해 정책을 학습할

1240
00:55:49,270 --> 00:55:50,900
수 있느냐는 겁니다.

1241
00:55:50,900 --> 00:55:52,520
이 그림을 다시 보여드리겠습니다.

1242
00:55:52,520 --> 00:55:54,530
이 그림은 상태를 입력으로 받아

1243
00:55:54,530 --> 00:55:57,680
행동을 예측하는 정책을 학습하려는 시도입니다.

1244
00:55:57,680 --> 00:56:00,820
이 모든 학습 신호와 절차는

1245
00:56:00,820 --> 00:56:03,770
인간이 로봇에게 작업 수행

1246
00:56:03,770 --> 00:56:09,010
방식을 시연한 대규모 수집 데이터로 이루어집니다.

1247
00:56:09,010 --> 00:56:11,960
시연으로부터 학습하는 것은 물론 새로운 개념이 아닙니다.

1248
00:56:11,960 --> 00:56:14,010
수십 년 동안 연구되어 왔습니다.

1249
00:56:14,010 --> 00:56:16,130
인간이 아주 어릴 때부터

1250
00:56:16,130 --> 00:56:19,310
실제 세계에서 많은 신체적 상호작용이나

1251
00:56:19,310 --> 00:56:22,450
사회적 활동을 수행하는 법을

1252
00:56:22,450 --> 00:56:24,650
배우는 과정과도 같습니다.

1253
00:56:24,650 --> 00:56:28,760
가장 초기의 고전적인 모방 학습 알고리즘 중 하나는

1254
00:56:28,760 --> 00:56:32,750
행동 복제(behavior cloning)라고

1255
00:56:32,750 --> 00:56:35,870
불리며, 본질적으로 현재 관찰 o에서 행동

1256
00:56:35,870 --> 00:56:39,390
a로 매핑하는 함수를 배우려는 시도입니다.

1257
00:56:39,390 --> 00:56:42,410
이 정책은 파라미터 θ로

1258
00:56:42,410 --> 00:56:45,530
표현된 함수 π로 나타냅니다.

1259
00:56:45,530 --> 00:56:48,410
행동 복제의 주요 문제 중 하나는 연쇄

1260
00:56:48,410 --> 00:56:51,990
오류(cascading error)라고 하는데,

1261
00:56:51,990 --> 00:56:54,320
앞서 말했듯이 로봇이나 에이전트가

1262
00:56:54,320 --> 00:56:56,300
환경과 상호작용하는 것은

1263
00:56:56,300 --> 00:56:59,610
순차적 의사결정 문제라는 점에서 차이가 있습니다.

1264
00:56:59,610 --> 00:57:02,298
이는 일반적인 컴퓨터 비전 분야의 감독

1265
00:57:02,298 --> 00:57:03,840
학습과 달리, 오류가

1266
00:57:03,840 --> 00:57:06,320
시간이 지남에 따라 누적되고 증폭될

1267
00:57:06,320 --> 00:57:08,485
수 있다는 점에서 다릅니다.

1268
00:57:08,485 --> 00:57:09,860
예를 들어, 처음에 아주

1269
00:57:09,860 --> 00:57:11,730
작은 오류를 범했다고 가정해봅시다.

1270
00:57:11,730 --> 00:57:15,240
그 작은 오류가 모델을 학습시킨 데이터

1271
00:57:15,240 --> 00:57:17,490
분포에서 약간 벗어난 상태로

1272
00:57:17,490 --> 00:57:19,230
이어질 수 있습니다.

1273
00:57:19,230 --> 00:57:21,700
그 결과 정책은 더 큰 오류를 범하게 됩니다.

1274
00:57:21,700 --> 00:57:26,350
이 오류는 시간 축을 따라 점점 증폭됩니다.

1275
00:57:26,350 --> 00:57:27,930
그 결과 시연 궤적과

1276
00:57:27,930 --> 00:57:31,270
상당히 다른 경로를 따르게 됩니다.

1277
00:57:31,270 --> 00:57:34,680
이것이 행동 복제의 전형적인 문제입니다.

1278
00:57:34,680 --> 00:57:37,080
그래서 보통 모방 학습을

1279
00:57:37,080 --> 00:57:39,370
성공시키려 할 때, 사람들은

1280
00:57:39,370 --> 00:57:42,580
이런 파이프라인을 따릅니다. 위쪽에는

1281
00:57:42,580 --> 00:57:45,850
전문가가 수집한 시연 데이터가 있습니다.

1282
00:57:45,850 --> 00:57:49,470
그 데이터를 학습 데이터로 사용해 감독 학습으로

1283
00:57:49,470 --> 00:57:51,210
정책을 훈련합니다.

1284
00:57:51,210 --> 00:57:53,910
그리고 실제 환경에서 정책을

1285
00:57:53,910 --> 00:57:56,555
실행해 실패 사례를 관찰합니다.

1286
00:57:56,555 --> 00:57:59,070
그 후 추가 데이터를

1287
00:57:59,070 --> 00:58:01,560
수집하거나 교정 행동을

1288
00:58:01,560 --> 00:58:03,990
제공하여, 데이터 세트가

1289
00:58:03,990 --> 00:58:07,380
초기 시연뿐 아니라 정책의

1290
00:58:07,380 --> 00:58:11,170
오류를 원래 궤적로 되돌리는 교정

1291
00:58:11,170 --> 00:58:13,970
행동도 포함하도록 합니다.

1292
00:58:13,970 --> 00:58:16,520
이렇게 하면 정책이 여전히 성공적으로 과제를 수행할 수 있습니다.

1293
00:58:16,520 --> 00:58:18,910
이것이 실제로 전형적인 생명 주기입니다.

1294
00:58:18,910 --> 00:58:21,520
우리는 실제 물리적 세계에서

1295
00:58:21,520 --> 00:58:23,830
모방 학습 에이전트나 모방

1296
00:58:23,830 --> 00:58:26,665
학습 알고리즘을 개발하려고 합니다.

1297
00:58:26,665 --> 00:58:29,920
이와 관련해서, 사람들이 이런 종류의

1298
00:58:29,920 --> 00:58:32,030
모방 학습을 할 때,

1299
00:58:32,030 --> 00:58:35,110
실제 작업이 무엇인지에 대한

1300
00:58:35,110 --> 00:58:37,400
명확한 정의가 없습니다.

1301
00:58:37,400 --> 00:58:41,750
작업은 시연 속에 암묵적으로 숨겨져 있습니다.

1302
00:58:41,750 --> 00:58:44,365
그래서 역강화학습(inverse

1303
00:58:44,365 --> 00:58:47,020
reinforcement

1304
00:58:47,020 --> 00:58:49,780
learning)이라는 알고리즘 군이 있는데,

1305
00:58:49,780 --> 00:58:52,390
왼쪽은 사람들이 일반적으로

1306
00:58:52,390 --> 00:58:54,160
강화학습에 대해 생각하는

1307
00:58:54,160 --> 00:58:58,270
것이고, 오른쪽은 사람들이 이 역강화학습을 사용해

1308
00:58:58,270 --> 00:59:00,755
시연에서 보상을 요약하고 그 보상을

1309
00:59:00,755 --> 00:59:02,380
이용해 일반적인

1310
00:59:02,380 --> 00:59:04,020
강화학습을 하는 것입니다.

1311
00:59:04,020 --> 00:59:06,880
초기 성공 사례 중 일부는 스탠포드에서

1312
00:59:06,880 --> 00:59:08,860
Pieter Abbeel과

1313
00:59:08,860 --> 00:59:11,930
Andrew Ng가 개발했습니다.

1314
00:59:11,930 --> 00:59:16,400
이것은 헬리콥터를 매우, 매우 복잡한 행동을 하도록 제어할

1315
00:59:16,400 --> 00:59:17,720
수 있게 해줍니다.

1316
00:59:17,720 --> 00:59:19,810
그리고 이것은 사실 매우 오래된 연구입니다.

1317
00:59:19,810 --> 00:59:25,660
실제 물리적 헬리콥터에서 이런 민첩하고 효과적인 행동을

1318
00:59:25,660 --> 00:59:27,550
달성한 것은 당시

1319
00:59:27,550 --> 00:59:30,050
매우 인상적이었습니다.

1320
00:59:30,050 --> 00:59:32,830
이것이 바로 시연에서 배우고 그 시연을 통해

1321
00:59:32,830 --> 00:59:35,120
보상을 요약하는 학습의 힘입니다.

1322
00:59:35,120 --> 00:59:37,130
그리고 강화학습과 연결해서,

1323
00:59:37,130 --> 00:59:39,805
이것이 우리가 달성할 수 있는 것입니다.

1324
00:59:39,805 --> 00:59:42,550
분명히, 수년간 사람들은 모방

1325
00:59:42,550 --> 00:59:45,430
학습 알고리즘을 점점 더 효과적으로

1326
00:59:45,430 --> 00:59:47,560
만들었고, 특히 에너지 기반

1327
00:59:47,560 --> 00:59:50,465
모델과 연결하는 데 집중했습니다.

1328
00:59:50,465 --> 00:59:52,840
왼쪽에 보이는 명시적인 정책을 학습하는

1329
00:59:52,840 --> 00:59:55,660
대신, 관찰에서 바로 행동으로 매핑하는

1330
00:59:55,660 --> 00:59:56,960
방식을 사용합니다.

1331
00:59:56,960 --> 00:59:59,580
이런 암묵적인 정책을 만들 때,

1332
00:59:59,580 --> 01:00:01,840
에너지 기반 모델에서

1333
01:00:01,840 --> 01:00:04,000
관찰과 행동을 직접 받아

1334
01:00:04,000 --> 01:00:09,230
점수를 예측하고, 이 에너지 기반 모델을 이용해 추론하여

1335
01:00:09,230 --> 01:00:12,710
예측된 행동을 얻는 아이디어를 차용합니다.

1336
01:00:12,710 --> 01:00:17,150
이것은 로봇이 매우 다중 모달한 시연을 처리하거나

1337
01:00:17,150 --> 01:00:19,490
최적화 지형이 매끄럽지

1338
01:00:19,490 --> 01:00:21,530
않은 상황을 다루는 데

1339
01:00:21,530 --> 01:00:23,010
도움을 줍니다.

1340
01:00:23,010 --> 01:00:26,300
로봇은 이런 전략을 고안하고,

1341
01:00:26,300 --> 01:00:28,740
시연에서 정책을 추출하여 내용이

1342
01:00:28,740 --> 01:00:32,630
풍부한 조작 작업을 수행할 수 있습니다.

1343
01:00:32,630 --> 01:00:34,672
최근 로봇 학습의

1344
01:00:34,672 --> 01:00:37,730
매우 최근 성공 사례 중

1345
01:00:37,730 --> 01:00:42,230
일부는 diffusion policy라는

1346
01:00:42,230 --> 01:00:45,470
연구 결과인데, 이것 역시 생성

1347
01:00:45,470 --> 01:00:48,470
모델 분야의 진보를 활용한

1348
01:00:48,470 --> 01:00:49,670
것입니다.

1349
01:00:49,670 --> 01:00:52,290
이번에는 암묵적 행동 복제를 위해, 사람들이

1350
01:00:52,290 --> 01:00:54,290
에너지 기반 모델의 개발에서 영감을

1351
01:00:54,290 --> 01:00:55,320
얻고 있습니다.

1352
01:00:55,320 --> 01:00:58,010
에너지 기반 모델은 딥러닝 커뮤니티에서

1353
01:00:58,010 --> 01:01:00,310
개발된 생성 모델의 한 종류입니다.

1354
01:01:00,310 --> 01:01:03,650
그리고 딥러닝 커뮤니티에는 diffusion

1355
01:01:03,650 --> 01:01:07,030
모델이라 불리는 더 강력한 모델 클래스도 있습니다.

1356
01:01:07,030 --> 01:01:10,710
사람들은 이 diffusion 모델을

1357
01:01:10,710 --> 01:01:17,130
정책 함수 클래스로 사용하여 에이전트가 diffusion 모델의 이점과

1358
01:01:17,130 --> 01:01:20,640
특성을 물려받도록 시도하고 있습니다.

1359
01:01:20,640 --> 01:01:24,340
이 연구는 원래 콜롬비아에서 진행되었습니다.

1360
01:01:24,340 --> 01:01:27,100
제가 지금 있는 곳이 바로 그곳입니다.

1361
01:01:27,100 --> 01:01:31,500
이 연구의 책임자, 즉 PI는 현재 스탠포드로 오셨습니다.

1362
01:01:31,500 --> 01:01:35,550
제가 선택한 많은 연구들이 스탠포드에 뿌리를 두고 있음을

1363
01:01:35,550 --> 01:01:37,080
알 수 있습니다.

1364
01:01:37,080 --> 01:01:41,170
그분은 현재 스탠포드의 W 학과에 계십니다.

1365
01:01:41,170 --> 01:01:44,910
이 정책은 매우 다양한 능력을 보여주며,

1366
01:01:44,910 --> 01:01:47,280
로봇이 단순한 플래너

1367
01:01:47,280 --> 01:01:51,630
푸싱뿐 아니라 세밀한 조작 작업도 수행할 수

1368
01:01:51,630 --> 01:01:52,660
있게 합니다.

1369
01:01:52,660 --> 01:01:55,000
단순히 집고 놓는 것뿐 아니라,

1370
01:01:55,000 --> 01:01:57,000
예를 들어 여기서는

1371
01:01:57,000 --> 01:02:02,770
버터를 펴 바르고 스크램블 에그를 만들며 감자를 벗기고 책을

1372
01:02:02,770 --> 01:02:04,940
미끄러뜨리는 작업도 합니다.

1373
01:02:04,940 --> 01:02:06,580
즉, 여러 시연

1374
01:02:06,580 --> 01:02:09,730
데이터를 모으고 최고의 정책 학습

1375
01:02:09,730 --> 01:02:13,670
기법을 사용하면, 실제 물리적 환경에서 매우

1376
01:02:13,670 --> 01:02:17,350
효율적으로 작동하는 정책을 얻을 수 있다는

1377
01:02:17,350 --> 01:02:20,377
것을 보여줍니다. 아침에

1378
01:02:20,377 --> 01:02:21,710
데이터를 수집하고,

1379
01:02:21,710 --> 01:02:23,270
정책을 훈련시키며,

1380
01:02:23,270 --> 01:02:26,650
오후에는 실제 환경에서 작동하는 정책을

1381
01:02:26,650 --> 01:02:28,160
가질 수 있습니다.

1382
01:02:28,160 --> 01:02:30,310
물론 정책의 신뢰성,

1383
01:02:30,310 --> 01:02:33,340
일반화 가능성, 초기 구성의

1384
01:02:33,340 --> 01:02:37,690
다양성에 따라 정책이 얼마나 견고하게 작동하는지에

1385
01:02:37,690 --> 01:02:40,610
대한 여러 주의사항이 있습니다.

1386
01:02:40,610 --> 01:02:44,560
그럼에도 불구하고, 모방 학습은 실제 물리적 환경에서

1387
01:02:44,560 --> 01:02:46,600
흥미로운 작업을 수행할 수 있는

1388
01:02:46,600 --> 01:02:49,310
정책을 얻는 가장 효율적인 방법입니다.

1389
01:02:49,310 --> 01:02:52,885
정책이 실제 환경 변화에 효과적이고

1390
01:02:52,885 --> 01:02:55,150
견고하려면, 이러한

1391
01:02:55,150 --> 01:02:58,000
반복적 데이터 수집이

1392
01:02:58,000 --> 01:02:59,980
필요하며, 정책이

1393
01:02:59,980 --> 01:03:03,500
예상치 못한 행동이나 편차 행동을

1394
01:03:03,500 --> 01:03:06,530
포괄할 수 있어야 합니다.

1395
01:03:06,530 --> 01:03:08,430
이것이 모방 학습에 관한 내용입니다.

1396
01:03:08,430 --> 01:03:09,870
질문 있으신가요?

1397
01:03:13,040 --> 01:03:15,120
네, 질문이 없으면

1398
01:03:15,120 --> 01:03:17,660
남은 시간에 로봇 학습의

1399
01:03:17,660 --> 01:03:20,510
모든 열풍을 이끄는

1400
01:03:20,510 --> 01:03:23,480
최근 발전, 즉 로봇 기초

1401
01:03:23,480 --> 01:03:26,695
모델에 대해 이야기하겠습니다.

1402
01:03:26,695 --> 01:03:30,120
물론 이 분야는 매우 복잡합니다.

1403
01:03:30,120 --> 01:03:32,240
사실 이 항목들 각각에 대해

1404
01:03:32,240 --> 01:03:35,190
별도의 강의를 할 수 있을 정도입니다.

1405
01:03:35,190 --> 01:03:38,580
오늘 강의에서는 아주 빠르게

1406
01:03:38,580 --> 01:03:39,450
훑어보며,

1407
01:03:39,450 --> 01:03:42,140
이 용어들을 보면서 알아야 할

1408
01:03:42,140 --> 01:03:45,380
핵심적인 고수준 지식만 말씀드리겠습니다.

1409
01:03:45,380 --> 01:03:47,855
로봇 기초 모델은

1410
01:03:47,855 --> 01:03:53,690
기능적으로 강화 학습이나 모방 학습과

1411
01:03:53,690 --> 01:03:57,890
매우 유사한 모델 유형입니다.

1412
01:03:57,890 --> 01:03:59,970
이 모델들은 상태나 환경 모델에

1413
01:03:59,970 --> 01:04:01,670
대한 명시적 표현이 없습니다.

1414
01:04:01,670 --> 01:04:03,420
예를 들어, 이 로봇 기초

1415
01:04:03,420 --> 01:04:06,250
모델은 환경 모델을 학습하지 않습니다.

1416
01:04:06,250 --> 01:04:09,690
여전히 관찰을 입력으로 받아 행동을

1417
01:04:09,690 --> 01:04:11,880
출력하는 정책입니다.

1418
01:04:11,880 --> 01:04:13,380
이것은 다음

1419
01:04:13,380 --> 01:04:16,265
그림으로 잘 표현할 수 있습니다.

1420
01:04:16,265 --> 01:04:17,640
현재 상태와 목표를

1421
01:04:17,640 --> 01:04:19,530
입력으로 받는 정책인

1422
01:04:19,530 --> 01:04:21,060
에이전트가 있고,

1423
01:04:21,060 --> 01:04:23,340
실제 물리적 환경에서 실행할

1424
01:04:23,340 --> 01:04:26,592
수 있는 행동을 생성하려고 합니다.

1425
01:04:26,592 --> 01:04:28,050
하지만 이것이 모방 학습이나

1426
01:04:28,050 --> 01:04:30,640
강화 학습과 매우 비슷하다고 생각할 수도 있습니다.

1427
01:04:30,640 --> 01:04:34,300
그럼 이 로보틱스 foundation 모델이 특별한 점은 무엇일까요?

1428
01:04:34,300 --> 01:04:36,270
사실 이 모든 것은 foundation

1429
01:04:36,270 --> 01:04:39,700
모델 분야, 특히 언어 관련 foundation

1430
01:04:39,700 --> 01:04:42,120
모델과 비전-언어 관련 foundation

1431
01:04:42,120 --> 01:04:45,150
모델의 발전에 뿌리를 두고 있습니다. 즉,

1432
01:04:45,150 --> 01:04:47,980
이것은 일종의 정책(policy)이라는 겁니다.

1433
01:04:47,980 --> 01:04:51,660
하지만 단일 특정 작업에만 작동하는 정책보다 훨씬

1434
01:04:51,660 --> 01:04:54,240
더 잘 일반화할 수 있어야 합니다.

1435
01:04:54,240 --> 01:04:55,930
여기 제 정의가 있습니다.

1436
01:04:55,930 --> 01:04:59,350
저는 현재 비전-언어 모델의 발전에서 비유를

1437
01:04:59,350 --> 01:05:03,370
끌어오고 있습니다. 즉, 이들의 출력이 항상 완벽하지는 않지만,

1438
01:05:03,370 --> 01:05:06,100
이런 foundation 모델에

1439
01:05:06,100 --> 01:05:08,630
프롬프트를 주면 항상 합리적인 결과를

1440
01:05:08,630 --> 01:05:10,150
생성한다는 뜻입니다.

1441
01:05:10,150 --> 01:05:13,180
그래서 우리가 이 로보틱스 foundation

1442
01:05:13,180 --> 01:05:16,000
모델로 달성하고자 하는 것은, 합성된 행동이

1443
01:05:16,000 --> 01:05:18,400
관찰과 작업에 조건화된 최적의

1444
01:05:18,400 --> 01:05:19,940
행동이 아닐 수도 있지만,

1445
01:05:19,940 --> 01:05:21,790
일반적인 궤적은 실제 물리

1446
01:05:21,790 --> 01:05:24,130
세계에서 실행하기에 항상 아름답고

1447
01:05:24,130 --> 01:05:26,170
합리적일 것이라는 점입니다.

1448
01:05:26,170 --> 01:05:28,790
여기서 아름답다는 것은 흔들리는 동작(jiggling motion)을
사용하지 말아야 한다는 의미입니다.

1449
01:05:28,790 --> 01:05:30,650
부드럽고 연속적이어야 합니다.

1450
01:05:30,650 --> 01:05:32,890
합리적이라는 것은 로봇에게 주어진

1451
01:05:32,890 --> 01:05:35,595
언어 지시를 잘 들어야 한다는 뜻입니다.

1452
01:05:35,595 --> 01:05:39,280
분명히, 같은 것을 설명하는 다양한 이름들이

1453
01:05:39,280 --> 01:05:40,700
많이 있습니다.

1454
01:05:40,700 --> 01:05:42,820
어떤 사람들은 이를 Vision Language Action

1455
01:05:42,820 --> 01:05:44,150
Models, 즉 VLAs라고 부릅니다.

1456
01:05:44,150 --> 01:05:46,880
어떤 사람들은 이를 large behavior models라고 부르기도
합니다.

1457
01:05:46,880 --> 01:05:49,970
하지만 본질적으로 모두 같은 것을 설명하는 겁니다.

1458
01:05:49,970 --> 01:05:52,810
즉, 관찰과 언어 지시나 작업 명세 같은

1459
01:05:52,810 --> 01:05:55,100
것을 받아서 정책을 만드는 것이죠.

1460
01:05:55,100 --> 01:05:57,950
여러 다양한 시나리오에 걸쳐

1461
01:05:57,950 --> 01:06:01,420
일반화되는 행동을 생성하려는 것입니다.

1462
01:06:01,420 --> 01:06:04,650
그래서 이 분야는 사실 꽤 혼란스러운 편입니다.

1463
01:06:04,650 --> 01:06:07,100
Noisy라는 것은 다양한 종류의

1464
01:06:07,100 --> 01:06:10,610
로봇 기반 모델들의 진척도를 정량화하기가 매우,

1465
01:06:10,610 --> 01:06:12,308
매우 어렵다는 뜻입니다.

1466
01:06:12,308 --> 01:06:14,100
foundation model이라고 부르는데,

1467
01:06:14,100 --> 01:06:14,933
그게 무슨 의미일까요?

1468
01:06:14,933 --> 01:06:18,080
그 말은 이 모델이 매우 넓은 범위의 시나리오에서 일반화할

1469
01:06:18,080 --> 01:06:19,740
것으로 기대한다는 뜻입니다.

1470
01:06:19,740 --> 01:06:21,950
그런 기대를 한다면, 실제로 넓게

1471
01:06:21,950 --> 01:06:23,900
일반화한다는 것을 보여줄 상당한

1472
01:06:23,900 --> 01:06:24,930
증거가 필요합니다.

1473
01:06:24,930 --> 01:06:27,950
그래서 평가와 진척도의 정량적 측정이

1474
01:06:27,950 --> 01:06:30,000
매우 어렵다는 겁니다.

1475
01:06:30,000 --> 01:06:33,390
하지만 여전히 실험 영상을 보면

1476
01:06:33,390 --> 01:06:36,560
지난 2년간 매우 흥미롭고

1477
01:06:36,560 --> 01:06:39,240
구체적인 진전이 많이 보입니다.

1478
01:06:39,240 --> 01:06:41,180
초기 연구는 2022년

1479
01:06:41,180 --> 01:06:45,230
12월에 발표된 RT-1에서

1480
01:06:45,230 --> 01:06:47,545
시작하는 경우가 많습니다.

1481
01:06:47,545 --> 01:06:51,810
그 이후로 대략 반년마다 새로운 모델이 나오는데,

1482
01:06:51,810 --> 01:06:55,900
예를 들어 RT-2, RT-X, OpenVLA,

1483
01:06:55,900 --> 01:06:58,900
그리고 최근의 Pi-Zero 같은

1484
01:06:58,900 --> 01:07:02,970
모델들이 점점 더 일반화 가능한 로봇 기반 모델

1485
01:07:02,970 --> 01:07:06,070
개발에 구체적인 진전을 이루고 있습니다.

1486
01:07:06,070 --> 01:07:09,520
사실 올해는 큰 도약이 있었습니다.

1487
01:07:09,520 --> 01:07:13,690
Helix, Hi-Robot, Gemini Robotics, Pi-0.

1488
01:07:13,690 --> 01:07:17,500
5 등 많은 foundation model들이 처음 등장했습니다.

1489
01:07:17,500 --> 01:07:19,140
이 분야에서는 자본

1490
01:07:19,140 --> 01:07:21,330
투자뿐만 아니라 더

1491
01:07:21,330 --> 01:07:24,430
나은, 더 일반화 가능한 로봇

1492
01:07:24,430 --> 01:07:27,990
foundation model 개발을 위한

1493
01:07:27,990 --> 01:07:29,580
인재 투자도 활발히

1494
01:07:29,580 --> 01:07:31,625
이루어지고 있습니다.

1495
01:07:31,625 --> 01:07:34,980
시간 관계상 모든 모델의 세부 사항을 다룰

1496
01:07:34,980 --> 01:07:36,120
수는 없습니다.

1497
01:07:36,120 --> 01:07:37,620
관심 있으시면 두 달 전에

1498
01:07:37,620 --> 01:07:40,050
[INAUDIBLE]에서 제가 이 축을

1499
01:07:40,050 --> 01:07:42,570
따라 몇몇 모델을 설명하고 토론한 튜토리얼을

1500
01:07:42,570 --> 01:07:44,470
했으니 참고하시기 바랍니다.

1501
01:07:44,470 --> 01:07:47,320
관심 있으시면 꼭 시청해 주세요.

1502
01:07:47,320 --> 01:07:51,670
오늘은 주로 이런 foundation model에

1503
01:07:51,670 --> 01:07:55,420
필수적인 요소가 무엇인지, 그리고 실제로

1504
01:07:55,420 --> 01:07:58,810
어떤 모습인지 Pi-Zero의 예를 들어서

1505
01:07:58,810 --> 01:08:01,330
개괄적으로 설명드리겠습니다.

1506
01:08:01,330 --> 01:08:06,670
Pi-Zero는 2024년 10월에 처음 공개되었습니다.

1507
01:08:06,670 --> 01:08:09,220
저는 이 단어가 저를 설득했다고 생각합니다.

1508
01:08:09,220 --> 01:08:11,080
이 유형의 로봇 기반

1509
01:08:11,080 --> 01:08:14,860
모델이 실제 환경에서 매우 신뢰할 수 있는 섬세한 조작을

1510
01:08:14,860 --> 01:08:16,399
할 수 있다는 것을요.

1511
01:08:16,399 --> 01:08:18,729
이 모델은 옷 접기, 상자 접기

1512
01:08:18,729 --> 01:08:22,330
그리고 여러 가지 다른 조작 작업들을 매우 신뢰할 수

1513
01:08:22,330 --> 01:08:24,700
있는 방식으로 처리할 수 있습니다.

1514
01:08:24,700 --> 01:08:26,620
그리고 여기 이 프레임워크가

1515
01:08:26,620 --> 01:08:29,109
고수준에서 실제로 어떻게 생겼는지 보여드립니다.

1516
01:08:29,109 --> 01:08:32,369
왼쪽에는 데이터 세트가 있습니다.

1517
01:08:32,369 --> 01:08:34,870
어떤 모델이든지 파운데이션 모델이라고 불리려면 그

1518
01:08:34,870 --> 01:08:36,620
파운데이션 모델을 위한 연료가 필요합니다.

1519
01:08:36,620 --> 01:08:38,684
그 연료가 바로 데이터입니다.

1520
01:08:38,684 --> 01:08:41,470
그래서 그들은 학계 내외에서 그리고 자신들이

1521
01:08:41,470 --> 01:08:43,750
직접 여러 로봇 형태에서 수집한

1522
01:08:43,750 --> 01:08:45,513
많은 데이터를 모읍니다.

1523
01:08:45,513 --> 01:08:46,930
여기서 로봇은 실제

1524
01:08:46,930 --> 01:08:50,100
환경에서 흥미롭고 유용한 작업을 수행하고 있죠.

1525
01:08:50,100 --> 01:08:54,020
그리고 이 데이터를 사용해 사전 학습을 합니다.

1526
01:08:54,020 --> 01:08:56,060
이 사전 학습의 중요한

1527
01:08:56,060 --> 01:08:59,569
점은 이미 방대한 시각-언어 관련 데이터로

1528
01:08:59,569 --> 01:09:01,430
학습된 사전 학습된

1529
01:09:01,430 --> 01:09:05,689
비전 언어 모델에서 시작한다는 것입니다. 이 모델은

1530
01:09:05,689 --> 01:09:08,899
자연스럽게 그런 의미론적 지식을

1531
01:09:08,899 --> 01:09:10,410
적응할 수 있습니다.

1532
01:09:10,410 --> 01:09:13,399
그리고 코드 파인튜닝이라고 부르는

1533
01:09:13,399 --> 01:09:16,790
것을 하면서, 행동 예측 목적과 비전

1534
01:09:16,790 --> 01:09:19,279
질문 응답 같은 작업에서

1535
01:09:19,279 --> 01:09:22,200
적응된 목적을 모두 사용해, 먼저

1536
01:09:22,200 --> 01:09:25,490
그 모델 내 의미론적 지식을 보존할

1537
01:09:25,490 --> 01:09:26,550
수 있습니다.

1538
01:09:26,550 --> 01:09:30,560
하지만 동시에 로봇 행동을 예측할 수 있습니다.

1539
01:09:30,560 --> 01:09:32,430
이것이 사전 학습 단계입니다.

1540
01:09:32,430 --> 01:09:36,439
많은 기존 로봇 파운데이션 모델에서 매우 중요한

1541
01:09:36,439 --> 01:09:39,029
설계는 포스트 트레이닝이라고 불리는데,

1542
01:09:39,029 --> 01:09:42,200
이는 대형 언어 모델 커뮤니티에서

1543
01:09:42,200 --> 01:09:44,450
발전한 개념에서 영감을 받았습니다.

1544
01:09:44,450 --> 01:09:46,410
기본 모델이 있죠.

1545
01:09:46,410 --> 01:09:48,978
기본 모델은 어느 정도 합리적인 기본 성능을 제공합니다.

1546
01:09:48,978 --> 01:09:51,270
하지만 특정 작업에서 성능을

1547
01:09:51,270 --> 01:09:53,130
정말 좋게 하려면,

1548
01:09:53,130 --> 01:09:56,890
그 작업에 특화된 데이터를 수집해 모델을

1549
01:09:56,890 --> 01:09:59,730
파인튜닝, 즉 포스트 트레이닝해야 만족할

1550
01:09:59,730 --> 01:10:02,625
만한 성능을 얻을 수 있습니다.

1551
01:10:02,625 --> 01:10:04,740
그래서 그들은 전체 시스템을 세

1552
01:10:04,740 --> 01:10:06,520
가지 다른 범주에서 평가합니다.

1553
01:10:06,520 --> 01:10:09,160
첫 번째는 기본 모델을 직접 사용하는 경우입니다.

1554
01:10:09,160 --> 01:10:11,280
기본 모델은 이미 일부 매우

1555
01:10:11,280 --> 01:10:15,450
간단한 분포 내 데이터에 대해 충분히 좋을 수 있습니다.

1556
01:10:15,450 --> 01:10:18,432
이 작업은 사실 사전 학습

1557
01:10:18,432 --> 01:10:19,890
단계에서 이미 접했을

1558
01:10:19,890 --> 01:10:22,470
수도 있는 작업입니다.

1559
01:10:22,470 --> 01:10:25,540
조금 더 복잡한 다른 작업에 대해서는,

1560
01:10:25,540 --> 01:10:28,260
기본 모델이 분포 내 작업에서 더

1561
01:10:28,260 --> 01:10:32,445
향상할 수 있도록 포스트 트레이닝을 할 수 있습니다.

1562
01:10:32,445 --> 01:10:34,320
그리고 보지 못한 작업에

1563
01:10:34,320 --> 01:10:36,360
대해서는, 일반적으로 그 작업에

1564
01:10:36,360 --> 01:10:40,050
특화된 데이터를 수집해 사전 학습된 모델을 파인튜닝하는

1565
01:10:40,050 --> 01:10:43,170
포스트 트레이닝을 해야 성능을 낼 수 있습니다.

1566
01:10:43,170 --> 01:10:46,285
이 Pi-Zero 모델은 실제로 오픈 소스입니다.

1567
01:10:46,285 --> 01:10:48,800
체크포인트를 다운로드할 수 있습니다.

1568
01:10:48,800 --> 01:10:50,530
제 연구실 학생들이 이미 이

1569
01:10:50,530 --> 01:10:53,298
모델을 가지고 놀기 시작했고 포스트 트레이닝을

1570
01:10:53,298 --> 01:10:54,590
시도하고 있습니다.

1571
01:10:54,590 --> 01:10:58,010
그리고 매우 유망한 결과가 나오기 시작했습니다.

1572
01:10:58,010 --> 01:11:01,540
관심 있으시면 꼭 시도해 보시길 강력히 권합니다.

1573
01:11:01,540 --> 01:11:02,780
아주 좋은 질문입니다.

1574
01:11:02,780 --> 01:11:05,050
본질적으로 기존 로봇 파운데이션

1575
01:11:05,050 --> 01:11:07,190
모델의 효율성에 대해 묻는 거죠.

1576
01:11:07,190 --> 01:11:10,300
정책이 인간보다 느린 데에는 여러

1577
01:11:10,300 --> 01:11:11,630
이유가 있습니다.

1578
01:11:11,630 --> 01:11:14,590
그중 주요 이유 중 하나는 데이터 수집

1579
01:11:14,590 --> 01:11:17,170
방식, 즉 시연 데이터 수집

1580
01:11:17,170 --> 01:11:18,410
방식에서 비롯됩니다.

1581
01:11:18,410 --> 01:11:20,660
대부분의 시나리오에서

1582
01:11:20,660 --> 01:11:22,360
시연 데이터는 인간이

1583
01:11:22,360 --> 01:11:26,560
직접 로봇을 원격 조작하여 수집했습니다. 예를

1584
01:11:26,560 --> 01:11:30,340
들어 이 상자를 접는 작업을 위해서요.

1585
01:11:30,340 --> 01:11:32,500
인간의 원격 조작은 수

1586
01:11:32,500 --> 01:11:36,230
시간의 훈련을 거쳤다 해도, 인간이 손으로

1587
01:11:36,230 --> 01:11:38,930
직접 하는 것보다 실제로 느립니다.

1588
01:11:38,930 --> 01:11:41,410
이는 당신이 가장 익숙한

1589
01:11:41,410 --> 01:11:45,510
신체화(embodiment)와 다른 신체화를 사용하고 있기 때문이며,

1590
01:11:45,510 --> 01:11:48,920
동시에 로봇 팔이 여전히 당신과 일정 거리를

1591
01:11:48,920 --> 01:11:50,670
두고 있기 때문입니다.

1592
01:11:50,670 --> 01:11:51,840
가림 현상이 발생할 것입니다.

1593
01:11:51,840 --> 01:11:54,180
때로는 정말로 다음 작업 단계로

1594
01:11:54,180 --> 01:11:57,140
넘어갈 시기인지 이해하기 위해 머리를 움직이거나

1595
01:11:57,140 --> 01:11:58,830
시야 각도를 바꾸는 등

1596
01:11:58,830 --> 01:12:02,220
매우 주의 깊게 자세히 봐야 할 때가 있습니다.

1597
01:12:02,220 --> 01:12:05,900
현재 데이터 수집 방식에는 많은

1598
01:12:05,900 --> 01:12:08,520
주의사항과 비효율성이 있습니다.

1599
01:12:08,520 --> 01:12:11,180
그래서 그 데이터로 직접 훈련된 정책이

1600
01:12:11,180 --> 01:12:14,100
인간 속도보다 느리게 나왔던 겁니다.

1601
01:12:14,100 --> 01:12:16,400
그래서 인간 속도에 맞춰 더

1602
01:12:16,400 --> 01:12:18,860
효율적으로 데이터 수집을 할 수 있는

1603
01:12:18,860 --> 01:12:21,900
방법에 대한 많은 연구가 진행되고 있습니다.

1604
01:12:21,900 --> 01:12:24,857
이것은 사실 매우 활발한 연구 방향입니다.

1605
01:12:24,857 --> 01:12:26,190
아주 좋은 질문입니다.

1606
01:12:26,190 --> 01:12:28,610
이 박스 접기 작업은 이미 매우 긴 시간의

1607
01:12:28,610 --> 01:12:31,140
작업(horizon)이라고 주장할 수 있습니다.

1608
01:12:31,140 --> 01:12:35,660
이 한 가지 정책이 이렇게 긴 시간의 작업을 잘 처리하는

1609
01:12:35,660 --> 01:12:37,680
것이 매우 인상적이었습니다.

1610
01:12:37,680 --> 01:12:39,170
하지만 만약 이 정책이

1611
01:12:39,170 --> 01:12:42,887
더 큰 규모, 예를 들어 실제 환경이나

1612
01:12:42,887 --> 01:12:44,970
가정에서 유용하려면, 단순히

1613
01:12:44,970 --> 01:12:47,610
로봇이 박스를 접는 것뿐만 아니라,

1614
01:12:47,610 --> 01:12:50,030
셔츠를 접고 침대를 정리하며 바닥의 어지러운

1615
01:12:50,030 --> 01:12:52,320
것들을 치우는 것도 해야 할 것입니다.

1616
01:12:52,320 --> 01:12:54,590
그런 시나리오에서는—개인적으로는—하나의

1617
01:12:54,590 --> 01:12:56,900
거대한 정책이

1618
01:12:56,900 --> 01:12:59,870
그런 시나리오에 적응할 수 있다고

1619
01:12:59,870 --> 01:13:01,110
믿지 않습니다.

1620
01:13:01,110 --> 01:13:03,020
더 높은 수준의 추상화,

1621
01:13:03,020 --> 01:13:04,940
예를 들어 장면 그래프나

1622
01:13:04,940 --> 01:13:08,760
상징적 표현이 있어야 하며, 이것이 비전-언어-행동

1623
01:13:08,760 --> 01:13:10,940
모델의 조건으로 작용해야

1624
01:13:10,940 --> 01:13:13,610
다양한 작업을 수행하고 더 큰

1625
01:13:13,610 --> 01:13:15,590
환경과 복잡한 작업에

1626
01:13:15,590 --> 01:13:19,070
확장하는 데 가장 효과적이고 유용할 수 있습니다.

1627
01:13:19,070 --> 01:13:21,530
이것은 사전 학습된 비전-언어 모델에서

1628
01:13:21,530 --> 01:13:22,140
시작되었습니다.

1629
01:13:22,140 --> 01:13:24,890
그래서 이미 이 비전-언어 데이터를 사용한

1630
01:13:24,890 --> 01:13:27,800
대규모 사전 학습을 통해 많은 의미론적 지식이

1631
01:13:27,800 --> 01:13:29,130
학습되어 있습니다.

1632
01:13:29,130 --> 01:13:31,700
그래서 일부 일반화 능력은 공짜로

1633
01:13:31,700 --> 01:13:34,310
얻는 것과 같아서, 기본 모델이 의미론적

1634
01:13:34,310 --> 01:13:37,400
수준에서 놀라울 정도로 좋은 일반화

1635
01:13:37,400 --> 01:13:38,790
능력을 갖고 있습니다.

1636
01:13:38,790 --> 01:13:41,632
저는 이 모델을 로봇 데이터로 미세 조정하여

1637
01:13:41,632 --> 01:13:43,090
의미론적 수준뿐

1638
01:13:43,090 --> 01:13:46,777
아니라 행동 수준에서도 일반화할 수 있도록 해야 합니다.

1639
01:13:46,777 --> 01:13:48,610
시간이 거의 다 되어 아마 마지막에 질문을

1640
01:13:48,610 --> 01:13:50,090
받을 수 있을 것 같습니다.

1641
01:13:50,090 --> 01:13:51,340
아직 2~3분

1642
01:13:51,340 --> 01:13:53,210
정도 남았는데, 로봇

1643
01:13:53,210 --> 01:13:56,240
학습 모델 개발과 관련된 남은 도전

1644
01:13:56,240 --> 01:13:59,270
과제 몇 가지를 말씀드리겠습니다.

1645
01:13:59,270 --> 01:14:03,220
전체 커뮤니티가 인식하는 주요 도전 과제 중

1646
01:14:03,220 --> 01:14:05,080
하나는 평가입니다.

1647
01:14:05,080 --> 01:14:06,670
평가는 현재 주로 실제 환경에서

1648
01:14:06,670 --> 01:14:07,790
이루어지고 있습니다.

1649
01:14:07,790 --> 01:14:11,030
예를 들어, 이것은 Google Robotics의 사진입니다.

1650
01:14:11,030 --> 01:14:14,500
그들은 Aloha 시스템을 원격 조작하는 그리드

1651
01:14:14,500 --> 01:14:17,920
형태로 데이터 수집과 평가를 하고 있습니다.

1652
01:14:17,920 --> 01:14:21,230
실제 환경 평가에는 비용이 많이 들고 잡음이 많습니다.

1653
01:14:21,230 --> 01:14:24,428
그들이 저에게 한 말은, 평가를 위해 충분한 예산이

1654
01:14:24,428 --> 01:14:26,470
있어야 진전을 이룰 수 있다는

1655
01:14:26,470 --> 01:14:27,140
것이었습니다.

1656
01:14:27,140 --> 01:14:29,273
즉, 평가를 누가 하느냐에

1657
01:14:29,273 --> 01:14:31,940
따라 초기 설정이나 조명

1658
01:14:31,940 --> 01:14:34,280
조건이 어떻게 변하느냐에

1659
01:14:34,280 --> 01:14:36,700
따라 결과가 매우 달라질

1660
01:14:36,700 --> 01:14:38,600
수 있다는 뜻입니다.

1661
01:14:38,600 --> 01:14:40,600
마찰 계수 같은 어떤 요소도

1662
01:14:40,600 --> 01:14:43,090
후속 정책의 견고성에 큰 차이를

1663
01:14:43,090 --> 01:14:44,518
만들 수 있습니다.

1664
01:14:44,518 --> 01:14:46,060
그래서 비용이 많이

1665
01:14:46,060 --> 01:14:49,060
들고 결과를 받기까지 이틀을 기다려야 합니다.

1666
01:14:49,060 --> 01:14:52,960
현재 훈련 손실과 실제 성공률 사이에는

1667
01:14:52,960 --> 01:14:55,730
매우 약한 상관관계가 있습니다.

1668
01:14:55,730 --> 01:14:58,390
이것은 감독 학습과 순차적

1669
01:14:58,390 --> 01:15:00,063
의사결정, 즉 정책 학습

1670
01:15:00,063 --> 01:15:01,730
간의 또 다른

1671
01:15:01,730 --> 01:15:05,080
중요한 차이점입니다. 감독 학습에서는 훈련

1672
01:15:05,080 --> 01:15:08,578
손실이 모델의 성능을 직접 측정하지만,

1673
01:15:08,578 --> 01:15:10,120
정책 학습에서는

1674
01:15:10,120 --> 01:15:13,510
훈련 손실이 한 단계 예측의 정확도를

1675
01:15:13,510 --> 01:15:16,670
측정하는데, 이것이 항상, 사실

1676
01:15:16,670 --> 01:15:20,290
대부분은 긴 작업 시간 동안 정책 성능을 잘

1677
01:15:20,290 --> 01:15:23,990
나타내지 못합니다. 손실이 낮아도 말이죠.

1678
01:15:23,990 --> 01:15:25,700
하지만 장기 과제 수행에서는

1679
01:15:25,700 --> 01:15:28,860
정책이 오히려 더 나쁠 수 있습니다.

1680
01:15:28,860 --> 01:15:32,320
그리고 훈련 목표와 과제별 지표, 예를

1681
01:15:32,320 --> 01:15:34,690
들어 훈련과 테스트의 시간

1682
01:15:34,690 --> 01:15:36,630
범위 차이가 정책

1683
01:15:36,630 --> 01:15:39,170
성능을 측정할 근사 지표를 만드는

1684
01:15:39,170 --> 01:15:41,060
것이 매우 어려운

1685
01:15:41,060 --> 01:15:42,810
이유 중 하나입니다.

1686
01:15:42,810 --> 01:15:46,460
그래서 사람들은 실제 환경 평가에 의존할 수밖에 없습니다.

1687
01:15:46,460 --> 01:15:48,290
그렇다면 시뮬레이션 환경에서

1688
01:15:48,290 --> 01:15:50,580
평가를 하는 것은 어떨까요?

1689
01:15:50,580 --> 01:15:52,170
예를 들어, 스탠포드의

1690
01:15:52,170 --> 01:15:53,630
물리 실험실과 Meta의

1691
01:15:53,630 --> 01:15:56,100
Habitat 3.0에서 행동

1692
01:15:56,100 --> 01:15:58,225
연구가 많이 진행되고 있습니다.

1693
01:15:58,225 --> 01:15:59,600
사람들은 비용이

1694
01:15:59,600 --> 01:16:02,220
많이 드는 시뮬레이션 환경을 만들어

1695
01:16:02,220 --> 01:16:05,090
로봇 정책의 평가와 측정을 시도하고

1696
01:16:05,090 --> 01:16:06,030
있습니다.

1697
01:16:06,030 --> 01:16:09,240
물론 시뮬레이션과 실제 간의 격차, 즉

1698
01:16:09,240 --> 01:16:12,090
sim-to-real 갭과 관련된 문제들이 있습니다.

1699
01:16:12,090 --> 01:16:15,450
강체, 변형 가능한 물체, 옷 등을 매우 정확하게 시뮬레이션하는

1700
01:16:15,450 --> 01:16:16,770
방법은 무엇일까요?

1701
01:16:16,770 --> 01:16:19,830
이들이 실제 성능과 좋은 상관관계를 가지는지가 문제입니다.

1702
01:16:19,830 --> 01:16:22,925
또한 에셋(asset) 문제도 큰

1703
01:16:22,925 --> 01:16:25,340
이슈인데, 대규모 일반화와

1704
01:16:25,340 --> 01:16:27,920
에셋 생성이 매우 어렵습니다.

1705
01:16:27,920 --> 01:16:31,080
자세한 설명은 강의 후에 하겠습니다.

1706
01:16:31,080 --> 01:16:33,590
그리고 실제 세계를 어떻게 디지털화할지도 문제입니다.

1707
01:16:33,590 --> 01:16:35,170
현실적이고 다양한

1708
01:16:35,170 --> 01:16:37,870
것을 절차적으로 생성하는 방법도 로봇

1709
01:16:37,870 --> 01:16:41,020
학습 정책 평가를 위한 시뮬레이션 사용의

1710
01:16:41,020 --> 01:16:42,310
문제점입니다.

1711
01:16:42,310 --> 01:16:44,950
그리고 시뮬레이션과 실제 간의 상관관계를

1712
01:16:44,950 --> 01:16:48,810
찾았을 때, Embodied AI에서 ImageNet을 떠올리게

1713
01:16:48,810 --> 01:16:51,040
됩니다. ImageNet이 성공한 이유는

1714
01:16:51,040 --> 01:16:54,680
최소 몇 년간 ImageNet에서의 진전, 즉 딥러닝과

1715
01:16:54,680 --> 01:16:56,990
컴퓨터 비전의 진전이 모두 같은 목표를

1716
01:16:56,990 --> 01:16:58,157
향했기 때문입니다.

1717
01:16:58,157 --> 01:16:59,740
우리는 이런 플랫폼, 즉 그

1718
01:16:59,740 --> 01:17:02,260
벤치마크나 플랫폼에서의 진전이 곧 로봇 학습의

1719
01:17:02,260 --> 01:17:04,250
진전이 되는 그런 환경을 원합니다.

1720
01:17:04,250 --> 01:17:07,270
그것이 바로 우리가 정말로 원하는 것입니다.

1721
01:17:07,270 --> 01:17:10,540
그리고 저는 아마도 건너뛸 수 있습니다.

1722
01:17:10,540 --> 01:17:13,300
우리는 이 기초 정책을 구축하는 방법에 대해 이야기하는데,

1723
01:17:13,300 --> 01:17:15,130
이는 기초적인 세계 모델을 구축하는

1724
01:17:15,130 --> 01:17:16,940
방법에 대한 탐구가 될 수도 있습니다.

1725
01:17:16,940 --> 01:17:18,760
특히 지금은 사람들이 이 기초

1726
01:17:18,760 --> 01:17:21,040
정책을 훈련시키기 위해 대규모 행동 조건

1727
01:17:21,040 --> 01:17:23,180
로봇 상호작용 데이터를 수집하고 있습니다.

1728
01:17:23,180 --> 01:17:24,850
하지만 그 데이터에는 많은 동역학

1729
01:17:24,850 --> 01:17:26,180
지식이 내포되어 있습니다.

1730
01:17:26,180 --> 01:17:28,520
만약 우리가 단순히 그 데이터를 정책 학습에만 사용한다면,

1731
01:17:28,520 --> 01:17:29,750
정말 큰 낭비가 될 것입니다.

1732
01:17:29,750 --> 01:17:31,690
그래서 우리는 이미 수집된

1733
01:17:31,690 --> 01:17:33,820
이 대규모 행동 조건 로봇

1734
01:17:33,820 --> 01:17:36,310
상호작용 데이터를 어떻게 활용해 이 기초

1735
01:17:36,310 --> 01:17:38,950
정책을 훈련시키고, 동시에 이 기초

1736
01:17:38,950 --> 01:17:42,490
세계 모델을 훈련시키며 서로 상호작용할 수 있을지

1737
01:17:42,490 --> 01:17:43,790
고민하고 있습니다.

1738
01:17:43,790 --> 01:17:45,340
그리고 이런 기초

1739
01:17:45,340 --> 01:17:48,250
세계 모델을 구축하는 방향으로 생각하는

1740
01:17:48,250 --> 01:17:50,240
기존 연구들도 있습니다.

1741
01:17:50,240 --> 01:17:53,980
그리고 여러분이 생각할 수 있는 매우 흥미로운 특성들이

1742
01:17:53,980 --> 01:17:56,630
있는데, 예를 들어 3D로 만들고 싶은지,

1743
01:17:56,630 --> 01:17:57,930
구조적 사전 지식을 원하시는지,

1744
01:17:57,930 --> 01:17:59,680
학습과 물리 기반 모델 중 어느 쪽에 더 중점을 둘지,

1745
01:17:59,680 --> 01:18:03,340
그리고 실제 물리 세계와 어떻게 연관 지을 수 있을지 등이 있습니다.

1746
01:18:03,340 --> 01:18:06,800
그리고 아마도, 사실 시간이 다 된 것 같습니다.

1747
01:18:06,800 --> 01:18:08,630
그래서 여기서 마치겠습니다.

1748
01:18:08,630 --> 01:18:12,850
이것이 우리가 정말로 널리 작동하고 주변의 비구조화된

1749
01:18:12,850 --> 01:18:16,120
데이터 환경에서 잘 일반화할 수

1750
01:18:16,120 --> 01:18:19,090
있는 기초 로봇 모델을 구축하기를

1751
01:18:19,090 --> 01:18:20,660
바라는 미래입니다.

1752
01:18:20,660 --> 01:18:23,210
다음 강의는 인간 중심 AI에 관한 내용입니다.

1753
01:18:23,210 --> 01:18:25,880
그리고 오늘 강의는 여기서 마치겠습니다.

1754
01:18:25,880 --> 01:18:27,750
정말 감사합니다.
