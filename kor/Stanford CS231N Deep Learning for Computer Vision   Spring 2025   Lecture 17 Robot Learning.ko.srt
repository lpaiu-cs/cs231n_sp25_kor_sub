1
00:00:05,270 --> 00:00:09,140
우리는 이번 강좌의 마지막 초청 강사와 함께 있습니다.

2
00:00:09,140 --> 00:00:12,410
오늘은 Dr. Yunzhu Li입니다.

3
00:00:12,410 --> 00:00:14,870
그는 콜롬비아 대학교의 컴퓨터

4
00:00:14,870 --> 00:00:18,750
과학 조교수로, 로봇 인식, 상호작용 및 학습 연구실을

5
00:00:18,750 --> 00:00:20,160
이끌고 있습니다.

6
00:00:20,160 --> 00:00:23,070
그는 모든 초청 강사들처럼 CS231N의

7
00:00:23,070 --> 00:00:24,600
이전 강사이기도 합니다.

8
00:00:24,600 --> 00:00:28,550
그는 2023년에 스탠포드에서 Fei-Fei Li 교수와 Jiajun Wu

9
00:00:28,550 --> 00:00:31,770
교수와 함께 포스트닥을 수행하면서 이 강좌를 가르쳤습니다.

10
00:00:31,770 --> 00:00:33,680
그의 연구는 로봇 공학,

11
00:00:33,680 --> 00:00:36,750
컴퓨터 비전 및 기계 학습의 교차점에 있습니다.

12
00:00:36,750 --> 00:00:40,100
특히 그의 연구는 로봇 학습에 중점을 두고 있으며,

13
00:00:40,100 --> 00:00:43,040
로봇의 인식 및 물리적 상호작용 능력을 크게

14
00:00:43,040 --> 00:00:45,180
확장하는 것을 목표로 하고 있습니다.

15
00:00:45,180 --> 00:00:47,750
오늘 강의에서는 바로 그 주제인 로봇 학습에

16
00:00:47,750 --> 00:00:49,020
대해 논의할 것입니다.

17
00:00:49,020 --> 00:00:52,490
이제 오늘 강의를 위해 Yunzhu에게 넘기겠습니다.

18
00:00:52,490 --> 00:00:55,380
네, 친절한 소개에 감사드립니다, Dean.

19
00:00:55,380 --> 00:00:57,200
여기 오게 되어 매우 기쁩니다.

20
00:00:57,200 --> 00:00:59,010
제가 마지막으로 여기에서 강의를

21
00:00:59,010 --> 00:01:01,270
했던 것은 2년 전, 2023년이었습니다.

22
00:01:01,270 --> 00:01:05,489
최근에 많은 강의를 살펴보았습니다.

23
00:01:05,489 --> 00:01:09,060
오늘은 제가 작업해온 몇 가지에 대해

24
00:01:09,060 --> 00:01:10,570
이야기할 것입니다.

25
00:01:10,570 --> 00:01:13,440
그리고 이는 컴퓨터 비전을 위한 딥러닝의

26
00:01:13,440 --> 00:01:16,290
전체 그림과 매우 일관된 부분입니다.

27
00:01:16,290 --> 00:01:19,140
이것은 로봇 학습에 관한 것입니다.

28
00:01:19,140 --> 00:01:20,910
로봇이 물리적 세계를

29
00:01:20,910 --> 00:01:23,110
더 잘 인식하고 상호작용할

30
00:01:23,110 --> 00:01:26,340
수 있도록 하는 흥미로운 고려사항과

31
00:01:26,340 --> 00:01:28,270
이러한 고려사항이

32
00:01:28,270 --> 00:01:30,150
전형적인 컴퓨터 비전

33
00:01:30,150 --> 00:01:33,420
작업 및 방법과 어떻게 다를 수

34
00:01:33,420 --> 00:01:36,075
있는지에 대해 논의할 것입니다.

35
00:01:36,075 --> 00:01:38,580
우선, 여러분은 이미 감독

36
00:01:38,580 --> 00:01:42,095
학습에 대해 많은 것을 배웠습니다.

37
00:01:42,095 --> 00:01:44,850
감독 학습의 장면과 설정은 데이터

38
00:01:44,850 --> 00:01:47,130
x와 y가 있다는 것입니다.

39
00:01:47,130 --> 00:01:49,750
x는 입력이고, y는 레이블입니다.

40
00:01:49,750 --> 00:01:53,220
입력 x에서 출력 y로 매핑하는 방법을

41
00:01:53,220 --> 00:01:55,057
배우려고 합니다.

42
00:01:55,057 --> 00:01:56,890
여러분이 이미 배운

43
00:01:56,890 --> 00:02:01,295
예로는 분류, 회귀, 객체 탐지 등이 있습니다.

44
00:02:01,295 --> 00:02:05,300
여러분은 레이블이 없는 데이터로

45
00:02:05,300 --> 00:02:07,890
작업하는 자기 감독

46
00:02:07,890 --> 00:02:11,130
학습에 대해서도 배웠습니다.

47
00:02:11,130 --> 00:02:13,190
여기서 여러분이 하려는

48
00:02:13,190 --> 00:02:17,270
것은 보조 손실을 설계하거나 작업하여 데이터의

49
00:02:17,270 --> 00:02:19,680
기본 숨겨진 구조를 추출하거나

50
00:02:19,680 --> 00:02:24,060
식별할 수 있는 학습 알고리즘을 만드는 것입니다.

51
00:02:24,060 --> 00:02:27,230
자동 인코더를 포함한 몇 가지 전형적인

52
00:02:27,230 --> 00:02:31,010
예가 있으며, 이 방대한 양의 레이블 없는

53
00:02:31,010 --> 00:02:34,160
데이터에서 비감독 학습 또는 자기 감독

54
00:02:34,160 --> 00:02:37,520
학습을 시도하는 다른 많은 예가 있습니다.

55
00:02:37,520 --> 00:02:40,010
로봇 학습의 특별한 점은

56
00:02:40,010 --> 00:02:42,140
로봇이 세계와

57
00:02:42,140 --> 00:02:44,810
상호작용하는 물리적 상호작용을

58
00:02:44,810 --> 00:02:46,830
해야 한다는 것입니다.

59
00:02:46,830 --> 00:02:49,010
단순히 입력과 출력이 있고 입력 x에서

60
00:02:49,010 --> 00:02:50,790
y로 매핑하거나 어떤 종류의

61
00:02:50,790 --> 00:02:52,620
잠재 표현이 있는 것이 아닙니다.

62
00:02:52,620 --> 00:02:55,140
환경의 진화에 영향을

63
00:02:55,140 --> 00:02:56,610
미치는 것입니다.

64
00:02:56,610 --> 00:02:58,340
여러분이 실제 세계에서

65
00:02:58,340 --> 00:03:00,440
어떤 행동을 결정하든, 그

66
00:03:00,440 --> 00:03:03,270
행동의 결과로 세계는 변화할 것입니다.

67
00:03:03,270 --> 00:03:05,900
그리고 세계는 여러분에게 새로운 관찰이나

68
00:03:05,900 --> 00:03:09,068
보상을 제공하여 환경이 어떻게 변화했는지,

69
00:03:09,068 --> 00:03:10,610
특정 작업을 수행하는

70
00:03:10,610 --> 00:03:13,740
데 얼마나 잘하고 있는지를 알려줄 것입니다.

71
00:03:13,740 --> 00:03:16,520
목표는 환경으로부터 피드백을

72
00:03:16,520 --> 00:03:21,180
받아 보상을 극대화하거나 비용을 최소화하는

73
00:03:21,180 --> 00:03:25,670
일련의 행동을 실제로 고안하는 것입니다.

74
00:03:25,670 --> 00:03:30,420
로봇 학습, 특히 최근 몇 년 동안,

75
00:03:30,420 --> 00:03:34,190
학계와 산업 모두에서 상당한 주목을

76
00:03:34,190 --> 00:03:36,450
받고 있습니다.

77
00:03:36,450 --> 00:03:41,280
그래서 우리는 많은 스타트업 회사들을 보았고, 예를 들어 테슬라

78
00:03:41,280 --> 00:03:43,910
봇이나 피규어와 같은 물리적 지능을

79
00:03:43,910 --> 00:03:45,200
포함하고 있습니다.

80
00:03:45,200 --> 00:03:49,940
그들은 셔츠 접기, 커피콩 조작, 또는 실제 물리적

81
00:03:49,940 --> 00:03:54,240
세계에서 흥미로운 작업을 수행하는 인간 소음과

82
00:03:54,240 --> 00:03:56,700
같은 매우 복잡한 작업을

83
00:03:56,700 --> 00:03:58,890
수행하는 로봇의 멋지고

84
00:03:58,890 --> 00:04:01,920
화려한 비디오를 제작하고 있습니다.

85
00:04:01,920 --> 00:04:05,580
그래서 제가 언급했듯이 이 분야는 많은 주목과

86
00:04:05,580 --> 00:04:07,510
투자도 받고 있습니다.

87
00:04:07,510 --> 00:04:10,140
여기 최근 로봇 학습 분야에서 막대한

88
00:04:10,140 --> 00:04:12,300
투자를 유치하고 있는

89
00:04:12,300 --> 00:04:15,090
스타트업의 몇 가지 예가 있습니다. 이들은

90
00:04:15,090 --> 00:04:17,370
환경과 물리적 상호작용을 할

91
00:04:17,370 --> 00:04:19,050
수 있는 범용 로봇을

92
00:04:19,050 --> 00:04:20,200
만들려고 합니다.

93
00:04:20,200 --> 00:04:23,550
따라서 분명히 이러한 스타트업뿐만 아니라,

94
00:04:23,550 --> 00:04:28,370
주로 큰 기존 회사들도 자신들의 로봇 조사와

95
00:04:28,370 --> 00:04:30,420
이니셔티브를 가지고 있으며,

96
00:04:30,420 --> 00:04:32,520
환경과의 범용적이고

97
00:04:32,520 --> 00:04:36,090
고성능의 물리적 상호작용을 할 수 있는

98
00:04:36,090 --> 00:04:39,615
범용 로봇을 개발하려고 하고 있습니다.

99
00:04:39,615 --> 00:04:42,060
오늘 강의에서는 로봇

100
00:04:42,060 --> 00:04:44,535
학습의 현재 성공과 붐을

101
00:04:44,535 --> 00:04:47,160
가능하게 하는 주요 기술적

102
00:04:47,160 --> 00:04:50,910
요소에 대한 개요를 제공할 것입니다.

103
00:04:50,910 --> 00:04:53,210
문제 정의부터 시작하겠습니다.

104
00:04:53,210 --> 00:04:55,840
우리가 구축해온 문제를 더 구체적으로 어떻게

105
00:04:55,840 --> 00:04:57,620
정의할 수 있을까요? 그리고

106
00:04:57,620 --> 00:05:00,490
환경과의 강력한 상호작용에 대해 어떻게 공식적으로

107
00:05:00,490 --> 00:05:01,700
생각할 수 있을까요?

108
00:05:01,700 --> 00:05:05,510
그럼 더 인식 측면에 대해 논의하겠습니다.

109
00:05:05,510 --> 00:05:07,870
로봇이 환경을 인식하는 방식과

110
00:05:07,870 --> 00:05:10,270
사람들이 일반적으로 컴퓨터

111
00:05:10,270 --> 00:05:13,480
비전 커뮤니티에서 고려하는 방식의

112
00:05:13,480 --> 00:05:16,460
차이점, 그리고 로봇 인식의 특별한

113
00:05:16,460 --> 00:05:19,270
점에 대해 이야기하겠습니다.

114
00:05:19,270 --> 00:05:22,750
강화 학습, 모델 학습, 모델 기반 계획,

115
00:05:22,750 --> 00:05:25,970
모방 학습, 최근 로봇 기초 모델에

116
00:05:25,970 --> 00:05:29,350
대한 트렌드와 여유 시간을 사용하여 우리가

117
00:05:29,350 --> 00:05:34,330
여전히 직면하고 있는 도전 과제에 대해 논의하겠습니다.

118
00:05:34,330 --> 00:05:37,690
문제 정의부터 시작하겠습니다.

119
00:05:37,690 --> 00:05:40,930
일반적으로 문제는 이렇게 보여야

120
00:05:40,930 --> 00:05:45,350
하며, 최소한 그래픽적으로는 이렇게 표현됩니다.

121
00:05:45,350 --> 00:05:47,300
가운데에 에이전트가 있습니다.

122
00:05:47,300 --> 00:05:50,400
에이전트는 어떤 작업 목표를 부여받습니다.

123
00:05:50,400 --> 00:05:52,760
이 작업 목표는 예를 들어 인간의

124
00:05:52,760 --> 00:05:55,910
언어 지시사항이나 이 에이전트가 특정 작업을 수행하는

125
00:05:55,910 --> 00:06:00,530
데 얼마나 잘하고 있는지를 측정하는 어떤 목표 함수일 수 있습니다.

126
00:06:00,530 --> 00:06:04,610
이 에이전트는 물리적 세계나 어떤 환경에서

127
00:06:04,610 --> 00:06:06,330
상태를 가져옵니다.

128
00:06:06,330 --> 00:06:09,050
그리고 에이전트는 물리적

129
00:06:09,050 --> 00:06:11,270
세계에서 실행해야 할

130
00:06:11,270 --> 00:06:13,470
행동을 결정합니다.

131
00:06:13,470 --> 00:06:17,450
이 물리적 세계는 업데이트되고 이 상태의 st+1과

132
00:06:17,450 --> 00:06:20,180
보상이 주어져 에이전트에게

133
00:06:20,180 --> 00:06:24,000
작업 수행이 얼마나 잘 되고 있는지를 알려줍니다.

134
00:06:24,000 --> 00:06:26,670
이것이 일반적으로 프레임워크가 어떻게 생겼는지입니다.

135
00:06:26,670 --> 00:06:31,400
목표, 상태, 행동 및 보상으로 구성된 이러한

136
00:06:31,400 --> 00:06:36,210
유형의 정의를 매우 명확히 해야 합니다.

137
00:06:36,210 --> 00:06:41,300
로봇 학습 시나리오의 문제를 구체적으로 정의하는

138
00:06:41,300 --> 00:06:44,750
것은 컴퓨터 비전과 매우

139
00:06:44,750 --> 00:06:45,510
다릅니다.

140
00:06:45,510 --> 00:06:48,260
저는 컴퓨터 비전이 주로 입력, 즉

141
00:06:48,260 --> 00:06:50,600
고차원 데이터를 기반으로 환경의

142
00:06:50,600 --> 00:06:53,160
어떤 표현을 학습하려고 하는 것이라고

143
00:06:53,160 --> 00:06:54,540
말하고 싶습니다.

144
00:06:54,540 --> 00:06:56,870
하지만 로봇 공학은

145
00:06:56,870 --> 00:06:59,750
기본적으로 물리적 환경의 제약을

146
00:06:59,750 --> 00:07:02,150
가진 최적화 문제를

147
00:07:02,150 --> 00:07:04,105
해결하려고 합니다.

148
00:07:04,105 --> 00:07:05,480
목표에 대해 정의된

149
00:07:05,480 --> 00:07:06,810
목표 함수가 있습니다.

150
00:07:06,810 --> 00:07:09,410
그리고 본질적으로 목표 함수를

151
00:07:09,410 --> 00:07:12,320
최대화하거나 최소화할 수 있는 일련의 행동을

152
00:07:12,320 --> 00:07:15,570
제시함으로써 이 최적화 문제를 해결하려고 합니다.

153
00:07:15,570 --> 00:07:17,960
그래서 로봇 학습과 사람들이 일반적으로

154
00:07:17,960 --> 00:07:22,070
컴퓨터 비전에서 고려하는 것 사이의 주요 차이점입니다.

155
00:07:22,070 --> 00:07:24,440
이 문제의 특정 인스턴스 예를

156
00:07:24,440 --> 00:07:27,300
들면, 카트-폴에서 목표는 이동 가능한

157
00:07:27,300 --> 00:07:31,380
카트 위에 막대를 균형 있게 유지하는 것입니다.

158
00:07:31,380 --> 00:07:33,200
이 환경의 상태는

159
00:07:33,200 --> 00:07:35,900
시스템의 물리적 상태를

160
00:07:35,900 --> 00:07:39,500
설명하며, 각도, 각속도, 위치,

161
00:07:39,500 --> 00:07:43,127
수평 속도 등을 포함할 수 있습니다.

162
00:07:43,127 --> 00:07:44,960
행동은 카트에 적용되는

163
00:07:44,960 --> 00:07:47,580
수평 힘이 될 것입니다.

164
00:07:47,580 --> 00:07:51,000
그리고 보상은 각 시간 단계에서 막대가

165
00:07:51,000 --> 00:07:56,100
수직 위치를 유지하고 있는지를 나타내는 1이 될 수 있습니다.

166
00:07:56,100 --> 00:07:59,650
다른 예로는 로봇 보행이 포함될 수 있으며, 목표는

167
00:07:59,650 --> 00:08:03,310
이 로봇들이 앞으로 이동하도록 하는 것입니다.

168
00:08:03,310 --> 00:08:06,720
상태는 이 로봇의 모든 관절의 각도 위치

169
00:08:06,720 --> 00:08:09,160
속도를 포함할 수 있습니다.

170
00:08:09,160 --> 00:08:11,160
행동은 각 관절에

171
00:08:11,160 --> 00:08:13,390
적용되는 토크일 수 있습니다.

172
00:08:13,390 --> 00:08:16,380
보상은 매 시간마다 1일 수 있습니다.

173
00:08:16,380 --> 00:08:20,010
로봇은 앞으로 한 걸음을 내딛고 똑바로

174
00:08:20,010 --> 00:08:22,995
서 있는 상태를 유지합니다.

175
00:08:22,995 --> 00:08:25,770
그리고 아타리 게임을 포함한 흥미로운

176
00:08:25,770 --> 00:08:27,460
예시도 있습니다.

177
00:08:27,460 --> 00:08:31,470
목표는 가능한 한 높은 점수로 게임을

178
00:08:31,470 --> 00:08:33,549
완료하는 것입니다.

179
00:08:33,549 --> 00:08:35,880
상태는 게임 화면의 원시 픽셀

180
00:08:35,880 --> 00:08:37,480
입력이 될 것입니다.

181
00:08:37,480 --> 00:08:41,010
행동은 위, 아래, 왼쪽, 오른쪽과 같은 게임 조작일 수

182
00:08:41,010 --> 00:08:41,710
있습니다.

183
00:08:41,710 --> 00:08:43,650
보상은 매 시간 단계에서

184
00:08:43,650 --> 00:08:47,170
점수의 증가와 감소일 수 있습니다.

185
00:08:47,170 --> 00:08:50,320
그리고 여러분이 아마도 이전에 주목했을 유명한

186
00:08:50,320 --> 00:08:53,050
예시들이 있으며, 특히 AlphaGo의

187
00:08:53,050 --> 00:08:55,240
발전과 관련이 있습니다.

188
00:08:55,240 --> 00:08:57,400
바둑의 정의와 문제는 게임에서

189
00:08:57,400 --> 00:09:00,190
이기는 것이 목표인 유사한

190
00:09:00,190 --> 00:09:02,350
방식으로 정의될 수 있습니다.

191
00:09:02,350 --> 00:09:05,290
상태는 현재 바둑판에 있는 모든

192
00:09:05,290 --> 00:09:06,950
돌이 될 것입니다.

193
00:09:06,950 --> 00:09:09,460
행동은 이 보드에 다음 돌을 놓을 위치가

194
00:09:09,460 --> 00:09:10,520
될 수 있습니다.

195
00:09:10,520 --> 00:09:13,670
보상은 마지막 턴에서 이기면

196
00:09:13,670 --> 00:09:19,150
1의 보상을 받고, 지면 0의 보상을 받는 것입니다.

197
00:09:19,150 --> 00:09:22,270
이것은 예를 들어 게임 도메인에만 적용되는 것이 아닙니다.

198
00:09:22,270 --> 00:09:25,900
최근의 대형 언어 모델 발전과 함께,

199
00:09:25,900 --> 00:09:29,090
특히 순차 생성 문제에 대해

200
00:09:29,090 --> 00:09:32,890
유사한 방식으로 생각할 수 있으며, 목표는

201
00:09:32,890 --> 00:09:34,960
다음 단어를 예측하는

202
00:09:34,960 --> 00:09:37,340
것이 될 수 있습니다.

203
00:09:37,340 --> 00:09:40,010
상태는 문장에서 현재 단어들일 수 있습니다.

204
00:09:40,010 --> 00:09:42,340
행동은 여러분이 그곳에 넣고 싶은 특정

205
00:09:42,340 --> 00:09:43,920
다음 단어가 될 것입니다.

206
00:09:43,920 --> 00:09:46,830
정확하다면 보상을 받습니다.

207
00:09:46,830 --> 00:09:51,230
부정확하다면 0의 보상을 받습니다.

208
00:09:51,230 --> 00:09:53,420
유사하게, 이제 여러분은

209
00:09:53,420 --> 00:09:56,870
아마도 많은 챗봇과 많이 놀아보았을 것입니다.

210
00:09:56,870 --> 00:09:59,450
문제를 유사하게 정의할 수 있으며,

211
00:09:59,450 --> 00:10:04,260
목표는 인간 사용자에게 좋은 동반자가 되는 것입니다.

212
00:10:04,260 --> 00:10:06,860
상태는 현재 대화입니다.

213
00:10:06,860 --> 00:10:10,460
챗봇이 생성해야 할 행동은 인간 사용자에게

214
00:10:10,460 --> 00:10:14,340
주어지는 다음 문장이 될 것입니다.

215
00:10:14,340 --> 00:10:16,830
인간 평가에 따라, 사람이

216
00:10:16,830 --> 00:10:20,210
행복하면 보상을 정의할 수 있습니다.

217
00:10:20,210 --> 00:10:23,340
만족하면 1의 보상을 받습니다.

218
00:10:23,340 --> 00:10:25,740
행복하지 않거나 중립적이면

219
00:10:25,740 --> 00:10:28,310
다른 보상을 받습니다.

220
00:10:28,310 --> 00:10:31,050
더 구체적으로, 예를 들어 로봇 공학 분야에서.

221
00:10:31,050 --> 00:10:33,840
작업은 옷을 접는 것입니다.

222
00:10:33,840 --> 00:10:36,500
우리는 옷을 잘 접고 싶습니다.

223
00:10:36,500 --> 00:10:39,020
상태는 로봇이 이 환경에서

224
00:10:39,020 --> 00:10:42,360
얻는 현재 관찰로, 다중 시점

225
00:10:42,360 --> 00:10:45,900
RGB 또는 RGBD 관찰을

226
00:10:45,900 --> 00:10:47,820
포함할 수 있습니다.

227
00:10:47,820 --> 00:10:50,380
로봇은 자신의 행동, 즉 끝단 효과기를 어떻게

228
00:10:50,380 --> 00:10:52,200
움직일지를 결정해야 합니다.

229
00:10:52,200 --> 00:10:54,720
이 천을 조작하기 위해 그리퍼를 닫아야

230
00:10:54,720 --> 00:10:56,830
할까요, 아니면 열어야 할까요?

231
00:10:56,830 --> 00:10:58,750
인간 평가에 따르면,

232
00:10:58,750 --> 00:11:01,680
천이 제대로 접혀 있으면 로봇에게

233
00:11:01,680 --> 00:11:04,660
1의 보상을 주고, 접혀 있지 않으면

234
00:11:04,660 --> 00:11:06,610
0의 보상을 줍니다.

235
00:11:06,610 --> 00:11:10,770
그래서 여기서 로봇 학습 문제를 더 구체적으로 생각하는

236
00:11:10,770 --> 00:11:12,400
방법을 설명하겠습니다.

237
00:11:12,400 --> 00:11:15,900
이는 에이전트가 세상과 상호작용할 수 있게 해주는

238
00:11:15,900 --> 00:11:16,990
방법입니다.

239
00:11:16,990 --> 00:11:19,050
행동의 효과와 순차적

240
00:11:19,050 --> 00:11:21,840
의사결정 문제를 고려합니다.

241
00:11:21,840 --> 00:11:23,820
이는 사람들이 일반적으로 컴퓨터 비전에서

242
00:11:23,820 --> 00:11:25,120
고려하는 것과 다릅니다.

243
00:11:25,120 --> 00:11:27,945
우리는 단지 출력을 예측해야 합니다.

244
00:11:27,945 --> 00:11:30,880
목표, 상태, 행동, 보상 및

245
00:11:30,880 --> 00:11:33,090
목적 함수는 이 방향의

246
00:11:33,090 --> 00:11:34,710
문제를 생각할

247
00:11:34,710 --> 00:11:38,490
때 항상 염두에 두어야 할 사항입니다.

248
00:11:38,490 --> 00:11:40,970
이것은 문제 공식화에 관한 것입니다.

249
00:11:40,970 --> 00:11:45,490
질문은 보상이 얼마나 구체적으로 설계되어야 하는가입니다.

250
00:11:45,490 --> 00:11:48,610
많은 작업에서 보상은 다양한 유형의 사양을

251
00:11:48,610 --> 00:11:49,875
가질 수 있습니다.

252
00:11:49,875 --> 00:11:51,250
예를 들어,

253
00:11:51,250 --> 00:11:54,520
자율주행에서는 보상이 가능한 한 빨리 도착하는

254
00:11:54,520 --> 00:11:58,810
것이거나, 승객이 도로를 주행하는 동안 편안함을 느끼도록 하는

255
00:11:58,810 --> 00:12:00,290
것일 수 있습니다.

256
00:12:00,290 --> 00:12:02,320
옷 접기에서도 사용자의 선호에

257
00:12:02,320 --> 00:12:04,030
따라 옷을 여러 가지

258
00:12:04,030 --> 00:12:05,810
방법으로 접을 수 있습니다.

259
00:12:05,810 --> 00:12:08,930
어떤 사람들은 전체 면적이 가능한 한 작기를 원합니다.

260
00:12:08,930 --> 00:12:11,690
어떤 사람들은 가능한 한 매끄럽기를 원합니다.

261
00:12:11,690 --> 00:12:14,320
다양한 유형의 보상이 있을 수 있습니다.

262
00:12:14,320 --> 00:12:16,840
여기서는 일반적인 용어로 이야기하고 있습니다.

263
00:12:16,840 --> 00:12:18,310
사람이 옷을 보면, 이것이

264
00:12:18,310 --> 00:12:20,300
접혀 있다고 생각할까요, 아니면 아닐까요?

265
00:12:20,300 --> 00:12:22,970
하지만 보상 설계 측면에서 더

266
00:12:22,970 --> 00:12:25,060
구체적으로 말하자면, 특정

267
00:12:25,060 --> 00:12:30,190
애플리케이션의 특정 요구를 충족하는 데 많은 뉘앙스가 있습니다.

268
00:12:30,190 --> 00:12:31,090
좋습니다.

269
00:12:31,090 --> 00:12:31,865
계속하겠습니다.

270
00:12:31,865 --> 00:12:34,240
이것이 에이전트가 물리적 세계와

271
00:12:34,240 --> 00:12:36,770
상호작용할 수 있게 해주는 로봇 학습

272
00:12:36,770 --> 00:12:38,540
문제를 생각하는 방법입니다.

273
00:12:38,540 --> 00:12:41,010
이제 로봇 인식으로 넘어가겠습니다.

274
00:12:41,010 --> 00:12:43,580
특히 로봇 학습 영역 내에서 인식

275
00:12:43,580 --> 00:12:45,560
문제가 사람들이 일반적으로

276
00:12:45,560 --> 00:12:48,020
컴퓨터 비전에서 고려하는 것과

277
00:12:48,020 --> 00:12:50,030
어떻게 다른지 논의하겠습니다.

278
00:12:50,030 --> 00:12:51,440
이 이미지는

279
00:12:51,440 --> 00:12:54,020
오늘 강의에서 여러 번

280
00:12:54,020 --> 00:12:55,620
보게 될 것입니다.

281
00:12:55,620 --> 00:12:58,040
본질적으로, 이것은 우리가

282
00:12:58,040 --> 00:13:02,450
물리적 세계에서 얻는 정보를 어떻게 처리하는가에

283
00:13:02,450 --> 00:13:04,185
대한 질문입니다.

284
00:13:04,185 --> 00:13:06,060
물리적 세계는 예를 들어,

285
00:13:06,060 --> 00:13:09,840
고차원 RGB 관찰 또는 RGBD 관찰을 제공할 수 있습니다.

286
00:13:09,840 --> 00:13:12,230
또한 촉각 감지와 같은 다른 감각

287
00:13:12,230 --> 00:13:13,945
데이터를 포함할 수 있습니다.

288
00:13:13,945 --> 00:13:15,890
이 로봇 인식 문제는

289
00:13:15,890 --> 00:13:18,890
본질적으로 이러한 고차원 데이터에서

290
00:13:18,890 --> 00:13:23,210
로봇이 하위 의사결정을 하는 데 유용한

291
00:13:23,210 --> 00:13:27,470
구조화된 지식을 증류하거나 활용하려고 합니다.

292
00:13:27,470 --> 00:13:30,770
본질적으로 우리가 해결하려고 하는 질문은 이

293
00:13:30,770 --> 00:13:34,560
비구조적 현실 세계를 이해하려고 하는 것입니다.

294
00:13:34,560 --> 00:13:37,660
현실 세계는 매우 복잡할 수 있습니다.

295
00:13:37,660 --> 00:13:39,660
따라서 로봇이 환경에서

296
00:13:39,660 --> 00:13:41,310
얻는 관찰은 객체와

297
00:13:41,310 --> 00:13:45,000
환경에 대한 불완전한 지식만 포함할

298
00:13:45,000 --> 00:13:46,090
수 있습니다.

299
00:13:46,090 --> 00:13:47,590
차단이 있을 수 있습니다.

300
00:13:47,590 --> 00:13:52,390
감각 데이터에서 오류가 발생할 수 있습니다.

301
00:13:52,390 --> 00:13:56,020
불완전한 행동도 실패로 이어질 수 있습니다.

302
00:13:56,020 --> 00:13:59,590
예를 들어, 로봇은 물체를 잡으려고 시도할 수 있지만, 그들의

303
00:13:59,590 --> 00:14:02,650
잡기 행동이 항상 성공적이지는 않을 수 있습니다.

304
00:14:02,650 --> 00:14:05,170
때때로, 물체를 실수로 떨어뜨릴 수

305
00:14:05,170 --> 00:14:09,060
있으며, 이는 환경의 진화와 예상치 못한 변화를 초래할

306
00:14:09,060 --> 00:14:10,300
수 있습니다.

307
00:14:10,300 --> 00:14:12,780
그들은 이러한 시나리오를 처리할 수 있는

308
00:14:12,780 --> 00:14:14,650
인식 시스템을 가져야 합니다.

309
00:14:14,650 --> 00:14:17,170
또한 이 환경은 변할 수 있습니다.

310
00:14:17,170 --> 00:14:20,170
이 환경은 동적이며, 단단한 물체뿐만 아니라

311
00:14:20,170 --> 00:14:23,940
변형 가능한 물체(옷, 로프, 입자 매체)로 구성됩니다.

312
00:14:23,940 --> 00:14:26,820
개나 다른 아이들, 다른 인간과 같은 다른

313
00:14:26,820 --> 00:14:29,790
에이전트가 같은 환경에 존재하여 세상을

314
00:14:29,790 --> 00:14:31,330
어지럽힐 수 있습니다.

315
00:14:31,330 --> 00:14:33,910
따라서 당신의 인식 시스템은 이러한

316
00:14:33,910 --> 00:14:37,870
모든 종류의 변화에 대처할 수 있어야 합니다.

317
00:14:37,870 --> 00:14:40,430
그래서 로봇 공학 분야에서는 사람들이

318
00:14:40,430 --> 00:14:43,900
일반적으로 카메라 데이터만 가지고 작업하지 않습니다.

319
00:14:43,900 --> 00:14:47,800
그들은 가능한 한 많은 센서를 로봇에 추가하려고 하며,

320
00:14:47,800 --> 00:14:51,460
유용한 정보를 제공할 수 있는 한도 내에서 그렇게 합니다.

321
00:14:51,460 --> 00:14:54,470
예를 들어, 촉각 감지, 오디오

322
00:14:54,470 --> 00:14:57,745
정보, 깊이 정보 등을 고려합니다.

323
00:14:57,745 --> 00:15:01,900
일반적으로 우리는 모든 센서를 함께 결합하여

324
00:15:01,900 --> 00:15:04,330
서로 보완할 수 있는 시스템을

325
00:15:04,330 --> 00:15:07,730
설계해야 하며, 모든 정보는 물리적

326
00:15:07,730 --> 00:15:09,850
맥락에 대한 정보를

327
00:15:09,850 --> 00:15:11,540
제공할 수 있습니다.

328
00:15:11,540 --> 00:15:13,540
촉각 정보는 그래프가 안정적인지

329
00:15:13,540 --> 00:15:15,980
여부를 알려줄 수 있습니다.

330
00:15:15,980 --> 00:15:17,590
카메라 정보는 이

331
00:15:17,590 --> 00:15:19,850
환경의 전반적인 상태에

332
00:15:19,850 --> 00:15:22,330
대한 더 높은 수준의 정보를

333
00:15:22,330 --> 00:15:23,570
제공합니다.

334
00:15:23,570 --> 00:15:25,870
이러한 센서들이 어떻게 결합되어

335
00:15:25,870 --> 00:15:28,180
함께 작동할 수 있는지가 실제

336
00:15:28,180 --> 00:15:31,430
물리적 세계에서 작동하는 유능한 로봇

337
00:15:31,430 --> 00:15:34,010
시스템을 설계하는 데 매우 중요합니다.

338
00:15:34,010 --> 00:15:38,010
감각 양식의 수 외에도 로봇 비전과 컴퓨터

339
00:15:38,010 --> 00:15:40,010
비전의 중요한

340
00:15:40,010 --> 00:15:42,770
차이점은 행동의 효과와 이 환경의

341
00:15:42,770 --> 00:15:45,290
가능성을 이해하려고

342
00:15:45,290 --> 00:15:46,770
한다는 것입니다.

343
00:15:46,770 --> 00:15:49,100
왼쪽은 컴퓨터 비전에서 이미

344
00:15:49,100 --> 00:15:51,950
본 전형적인 예로, 인스턴스 분할을

345
00:15:51,950 --> 00:15:53,760
시도하는 것입니다.

346
00:15:53,760 --> 00:15:56,010
주어진 것은 이 2D 이미지입니다.

347
00:15:56,010 --> 00:15:59,310
이 2D 이미지에서 서로 다른 인스턴스를

348
00:15:59,310 --> 00:16:03,445
세그먼트로 나누며, 2D 픽셀 위에 윤곽선을 그립니다.

349
00:16:03,445 --> 00:16:05,820
하지만 로봇 공학 분야에서의 차이점은

350
00:16:05,820 --> 00:16:07,040
오른쪽과 같이 로봇이

351
00:16:07,040 --> 00:16:09,320
하나의 물체를 주어진 경우입니다.

352
00:16:09,320 --> 00:16:14,060
이 물체는 아마도 하나의 물체일 수도 있고, 서로

353
00:16:14,060 --> 00:16:16,400
쌓여 있는 여러 조각일

354
00:16:16,400 --> 00:16:17,640
수도 있습니다.

355
00:16:17,640 --> 00:16:20,900
따라서 로봇은 어떤 종류의 행동이 이 환경에

356
00:16:20,900 --> 00:16:23,720
대한 더 나은 이해와 인식을 가능하게

357
00:16:23,720 --> 00:16:25,710
할지를 알아야 합니다.

358
00:16:25,710 --> 00:16:27,710
이것이 하나의 물체인지 아니면 여러

359
00:16:27,710 --> 00:16:29,090
조각이 결합된 것인지?

360
00:16:29,090 --> 00:16:31,560
그래서 로봇은 환경과 능동적으로

361
00:16:31,560 --> 00:16:34,650
상호작용하고 방해하는 행동을 통해 이

362
00:16:34,650 --> 00:16:38,520
환경의 상태에 대한 더 나은 인식을 얻어야 합니다.

363
00:16:38,520 --> 00:16:44,190
그래서 로봇 비전은 구현되고, 능동적이며,

364
00:16:44,190 --> 00:16:46,410
환경에 적합합니다.

365
00:16:46,410 --> 00:16:49,335
구현된다는 것은 로봇이

366
00:16:49,335 --> 00:16:53,580
물리적 세계를 직접 경험하는 신체를 가지고

367
00:16:53,580 --> 00:16:55,920
있다는 의미입니다.

368
00:16:55,920 --> 00:16:58,050
그들의 행동은 즉각적인

369
00:16:58,050 --> 00:17:00,390
피드백을 받는 세계와의

370
00:17:00,390 --> 00:17:02,700
동적 관계의 일부입니다.

371
00:17:02,700 --> 00:17:06,720
능동적이라는 것은 로봇이 능동적인 인식자라는 의미입니다.

372
00:17:06,720 --> 00:17:09,960
로봇은 무엇을 감지하고 싶은지 알고, 무엇을 인식할지를

373
00:17:09,960 --> 00:17:12,210
선택하며, 그 인식을 달성하기 위해 어떻게,

374
00:17:12,210 --> 00:17:14,099
언제, 어디서 할지를 결정합니다.

375
00:17:14,099 --> 00:17:15,430
당신은 머리를 움직일 수 있습니다.

376
00:17:15,430 --> 00:17:17,349
이 테이블 뒤에 무엇이 있는지 알고 싶다면,

377
00:17:17,349 --> 00:17:19,900
주위를 돌아다니며 테이블 뒤를 볼 수 있습니다.

378
00:17:19,900 --> 00:17:21,565
그래서 이것이 활성 부분이며, 이는

379
00:17:21,565 --> 00:17:23,190
사람들이 일반적으로 컴퓨터

380
00:17:23,190 --> 00:17:24,609
비전에서 고려하는 것과 다릅니다.

381
00:17:24,609 --> 00:17:29,170
그들은 주로 수동적으로 수집된 데이터 세트로 작업하고 있습니다.

382
00:17:29,170 --> 00:17:31,270
세 번째 포인트는 로봇이 세계에

383
00:17:31,270 --> 00:17:33,620
위치하는 것에 관한 것입니다.

384
00:17:33,620 --> 00:17:37,480
그들은 추상적인 설명이 아니라 시스템의

385
00:17:37,480 --> 00:17:39,940
행동에 직접 영향을 미치는

386
00:17:39,940 --> 00:17:42,490
현재의 세계와 다루고 있습니다.

387
00:17:42,490 --> 00:17:45,490
로봇은 특히 의류에서 인식과

388
00:17:45,490 --> 00:17:48,500
행동 루프를 이해해야 합니다.

389
00:17:48,500 --> 00:17:51,310
로봇은 세상을 보고 목표를 이해하며

390
00:17:51,310 --> 00:17:53,860
인식에 따라 환경에서 행동할

391
00:17:53,860 --> 00:17:55,700
수 있어야 합니다.

392
00:17:55,700 --> 00:17:58,300
때때로 로봇은 환경의 전체 상태를 알

393
00:17:58,300 --> 00:17:59,270
필요가 없습니다.

394
00:17:59,270 --> 00:18:01,280
예를 들어, 내가 셔츠의

395
00:18:01,280 --> 00:18:04,510
단추를 채우고 있다면, 그 단추 근처의 지역만

396
00:18:04,510 --> 00:18:05,720
알면 됩니다.

397
00:18:05,720 --> 00:18:07,660
그래서 일부 인식은

398
00:18:07,660 --> 00:18:10,540
작업 및 하위 의사 결정

399
00:18:10,540 --> 00:18:11,980
시스템과 밀접하게

400
00:18:11,980 --> 00:18:14,740
결합되고 공동 설계되어

401
00:18:14,740 --> 00:18:18,760
로봇이 관련 지역 또는 작업 관련

402
00:18:18,760 --> 00:18:22,600
지역에 집중할 수 있도록 해야 합니다.

403
00:18:22,600 --> 00:18:25,750
그래서 이것은 매우 구체적인 고려 사항과 로봇의 인식이

404
00:18:25,750 --> 00:18:27,680
사람들이 일반적으로 컴퓨터

405
00:18:27,680 --> 00:18:29,180
비전에서 고려하는 것과 어떻게

406
00:18:29,180 --> 00:18:31,250
다를 수 있는지에 관한 것입니다.

407
00:18:31,250 --> 00:18:34,370
로봇이 세상을 보고 행동할 수

408
00:18:34,370 --> 00:18:36,980
있게 해주는 알고리즘에 대해

409
00:18:36,980 --> 00:18:39,300
논의하기 시작할 것입니다.

410
00:18:39,300 --> 00:18:42,905
그리고 우리는 강화 학습으로 시작할 것입니다.

411
00:18:42,905 --> 00:18:46,230
기억하세요, 이전에 우리는 이 이미지를 보았습니다.

412
00:18:46,230 --> 00:18:48,770
로봇은 이 환경에서 행동하고 이

413
00:18:48,770 --> 00:18:51,690
환경으로부터 보상을 받아야 합니다.

414
00:18:51,690 --> 00:18:54,170
이 최적화 문제를 해결하기

415
00:18:54,170 --> 00:18:56,720
위한 매우 일반적인 방법은

416
00:18:56,720 --> 00:19:01,310
로봇이 가능한 한 광범위하게 세계와

417
00:19:01,310 --> 00:19:03,680
상호작용하도록 하는 것입니다.

418
00:19:03,680 --> 00:19:06,545
모든 경험 데이터를 수집하고

419
00:19:06,545 --> 00:19:09,660
이러한 유형의 시행착오를 수행합니다.

420
00:19:09,660 --> 00:19:13,010
로봇이 이 행동이 높은 보상으로 이어지고 저 행동이 낮은

421
00:19:13,010 --> 00:19:15,210
보상으로 이어진다는 것을 이해하도록 합니다.

422
00:19:15,210 --> 00:19:18,470
그리고 우리는 에이전트의 행동을 에이전트에게

423
00:19:18,470 --> 00:19:21,990
더 높은 보상을 주는 행동으로 전환할 수 있습니다.

424
00:19:21,990 --> 00:19:25,050
그래서 강화 학습의 일반적인 아이디어는

425
00:19:25,050 --> 00:19:29,100
에이전트가 환경과 지속적으로 상호작용하고 보상을

426
00:19:29,100 --> 00:19:30,960
극대화하거나 비용을 최소화하기

427
00:19:30,960 --> 00:19:34,960
위해 시행착오를 수행할 수 있도록 하는 방법입니다.

428
00:19:34,960 --> 00:19:38,910
여기서 저는 강화 학습과 감독 학습의

429
00:19:38,910 --> 00:19:41,460
차이에 대해 좀 더 구체적으로

430
00:19:41,460 --> 00:19:43,410
논의하고 싶습니다.

431
00:19:43,410 --> 00:19:46,110
그래서 이것이 강화 학습의 전형적인

432
00:19:46,110 --> 00:19:46,870
프레임워크입니다.

433
00:19:46,870 --> 00:19:47,970
당신은 환경을 가지고 있습니다.

434
00:19:47,970 --> 00:19:49,840
환경은 에이전트에게 몇 가지 상태를 제공합니다.

435
00:19:49,840 --> 00:19:51,150
에이전트는 행동을 생성합니다.

436
00:19:51,150 --> 00:19:55,830
그리고 환경은 에이전트에게 피드백, 즉 보상을 제공합니다.

437
00:19:55,830 --> 00:19:58,290
환경은 변화하며, 환경은

438
00:19:58,290 --> 00:20:01,120
에이전트에게 새로운 상태 st+1을

439
00:20:01,120 --> 00:20:04,140
제공하고 본질적으로 로봇이

440
00:20:04,140 --> 00:20:06,750
연속적인 결정을 내려야 하는

441
00:20:06,750 --> 00:20:09,960
시간 도메인에서의 시퀀스와 같습니다.

442
00:20:09,960 --> 00:20:13,350
여기 감독 학습을 위한 전형적인

443
00:20:13,350 --> 00:20:15,640
이미지가 있습니다.

444
00:20:15,640 --> 00:20:17,080
당신은 데이터 세트를 가지고 있습니다.

445
00:20:17,080 --> 00:20:20,865
데이터 세트는 모델에 이 x를 입력합니다.

446
00:20:20,865 --> 00:20:23,550
그리고 이 모델은 예측 y를 생성합니다.

447
00:20:23,550 --> 00:20:26,190
그리고 당신은 모델의 예측과 이

448
00:20:26,190 --> 00:20:28,680
데이터 세트의 실제 값에 따라 손실을

449
00:20:28,680 --> 00:20:30,520
계산할 수 있습니다.

450
00:20:30,520 --> 00:20:34,103
그래서 이것이 감독 학습의 전형적인 설정입니다.

451
00:20:34,103 --> 00:20:36,270
강화 학습과 지도 학습의

452
00:20:36,270 --> 00:20:38,760
주요 차이점 중 하나는

453
00:20:38,760 --> 00:20:43,230
환경이 확률적일 수 있으며, 동일한 행동이

454
00:20:43,230 --> 00:20:45,660
환경을 다르게 변화시킬

455
00:20:45,660 --> 00:20:47,770
수 있다는 점입니다.

456
00:20:47,770 --> 00:20:50,110
예를 들어, 상자를 앞으로

457
00:20:50,110 --> 00:20:53,380
밀 때, 지지력의 분포에

458
00:20:53,380 --> 00:20:56,850
따라 동일한 행동이 상자가 다른 각도로

459
00:20:56,850 --> 00:21:00,040
회전하게 만들 수 있으며,

460
00:21:00,040 --> 00:21:02,760
이는 환경에서 불확실성과

461
00:21:02,760 --> 00:21:05,790
확률성이 존재할 수 있음을 의미합니다.

462
00:21:05,790 --> 00:21:08,970
이는 에이전트에게도 확률적

463
00:21:08,970 --> 00:21:13,320
보상을 주며, 동일한 행동이 항상 동일한

464
00:21:13,320 --> 00:21:17,290
보상으로 이어지지 않을 수 있습니다.

465
00:21:17,290 --> 00:21:19,840
그래서 이는 지도 학습과 매우 다릅니다.

466
00:21:19,840 --> 00:21:23,800
우리는 불확실한 동적 시스템을 다루고 있습니다.

467
00:21:23,800 --> 00:21:27,870
두 번째는 신용 할당에 관한 질문입니다.

468
00:21:27,870 --> 00:21:30,350
지도 학습에서는 입력을 제공하고

469
00:21:30,350 --> 00:21:33,200
출력을 예측하며 손실을 이미 계산합니다.

470
00:21:33,200 --> 00:21:36,040
이제 특정 예측을 통해 어떤 실수를 하고

471
00:21:36,040 --> 00:21:38,560
있는지, 어떤 오류를 범하고 있는지

472
00:21:38,560 --> 00:21:40,190
직접 알 수 있습니다.

473
00:21:40,190 --> 00:21:41,710
하지만 강화

474
00:21:41,710 --> 00:21:43,640
학습이나 순차적 의사결정

475
00:21:43,640 --> 00:21:46,390
영역에서는 보상이 지연될 수

476
00:21:46,390 --> 00:21:50,380
있습니다. 즉, 바둑 게임을 할 때, 이 에피소드의

477
00:21:50,380 --> 00:21:53,950
끝까지 가야만 이기고 있는지 지고

478
00:21:53,950 --> 00:21:56,100
있는지를 알 수 있습니다.

479
00:21:56,100 --> 00:21:58,150
그리고 그 보상은 01입니다.

480
00:21:58,150 --> 00:22:02,410
[듣기 어려움] 이는 매우 초기 단계, 아마도

481
00:22:02,410 --> 00:22:04,750
첫 단계나 게임 중간의 몇

482
00:22:04,750 --> 00:22:06,200
단계 때문입니다.

483
00:22:06,200 --> 00:22:08,590
따라서 이 순차적 의사결정

484
00:22:08,590 --> 00:22:11,470
과정에서 얻는 신용을 모든 행동에

485
00:22:11,470 --> 00:22:15,370
적절히 할당하는 방법도 사람들이 강화 학습을

486
00:22:15,370 --> 00:22:18,650
통해 답하고자 하는 또 다른 매우

487
00:22:18,650 --> 00:22:20,840
까다롭고 중요한 질문입니다.

488
00:22:20,840 --> 00:22:25,250
세 번째는 이 동적 시스템의 비확산

489
00:22:25,250 --> 00:22:27,230
능력입니다.

490
00:22:27,230 --> 00:22:30,570
예를 들어, 지도 학습에서는 입력이 있습니다.

491
00:22:30,570 --> 00:22:32,947
모델을 통해 입력을 전달합니다.

492
00:22:32,947 --> 00:22:33,780
출력을 얻습니다.

493
00:22:33,780 --> 00:22:34,830
손실을 계산합니다.

494
00:22:34,830 --> 00:22:37,980
따라서 이 과정의 모든 것은 미분 가능하다.

495
00:22:37,980 --> 00:22:41,540
그래서 모델 내의 매개변수에 대한 손실 함수의

496
00:22:41,540 --> 00:22:44,430
기울기를 직접 수집할 수 있다.

497
00:22:44,430 --> 00:22:46,280
하지만 강화

498
00:22:46,280 --> 00:22:50,360
학습에서는 환경이 종종 미분 가능하지 않은

499
00:22:50,360 --> 00:22:51,840
경우가 많다.

500
00:22:51,840 --> 00:22:53,780
따라서 행동에 대한 보상의

501
00:22:53,780 --> 00:22:57,840
기울기를 제대로 얻는 것은 까다로울 수 있다.

502
00:22:57,840 --> 00:23:00,650
때때로 사람들은 적절한 학습을 위해

503
00:23:00,650 --> 00:23:04,400
이러한 제로 차수 기울기 추정을 수행하기 위해 대규모

504
00:23:04,400 --> 00:23:06,140
샘플링에 의존해야 한다.

505
00:23:06,140 --> 00:23:09,650
그것도 또 다른 차이점이다.

506
00:23:09,650 --> 00:23:13,625
마지막 차이점은 환경의 진화와

507
00:23:13,625 --> 00:23:17,700
상태가 당신의 행동의 결과라는

508
00:23:17,700 --> 00:23:19,350
비정상성에

509
00:23:19,350 --> 00:23:21,630
관한 것이다.

510
00:23:21,630 --> 00:23:24,670
지도 학습에서는 예측한 것이 이 데이터

511
00:23:24,670 --> 00:23:27,090
세트에서 얻는 다른 데이터

512
00:23:27,090 --> 00:23:29,380
포인트에 영향을 미치지 않는다.

513
00:23:29,380 --> 00:23:32,040
하지만 당신의 행동은 이 순차적

514
00:23:32,040 --> 00:23:34,740
의사결정 문제에서 얻는 다음 상태에 영향을

515
00:23:34,740 --> 00:23:35,530
미친다.

516
00:23:35,530 --> 00:23:38,400
그것이 바로 이 강화 학습

517
00:23:38,400 --> 00:23:40,920
문제를 지도 학습보다 조금

518
00:23:40,920 --> 00:23:43,860
더 미묘하게 만드는 이유이다.

519
00:23:43,860 --> 00:23:46,360
여기 몇 가지 더 구체적인 예가 있다.

520
00:23:46,360 --> 00:23:48,880
예를 들어, 앞서 언급한 것처럼 이 아타리

521
00:23:48,880 --> 00:23:50,200
게임을 하는 것이다.

522
00:23:50,200 --> 00:23:52,980
목표는 가장 높은 점수로 게임을 완료하는

523
00:23:52,980 --> 00:23:53,610
것이다.

524
00:23:53,610 --> 00:23:55,800
상태는 게임 화면의 원시 픽셀

525
00:23:55,800 --> 00:23:57,370
입력이 될 것이다.

526
00:23:57,370 --> 00:23:59,110
행동은 키보드에서 위, 아래,

527
00:23:59,110 --> 00:24:00,610
왼쪽, 오른쪽이 될 수 있다.

528
00:24:00,610 --> 00:24:01,755
우리는 각

529
00:24:01,755 --> 00:24:05,160
시간 단계에서 점수의 증가와 감소가

530
00:24:05,160 --> 00:24:07,800
보상인 것을 시도하고 있다.

531
00:24:07,800 --> 00:24:10,620
이 분야의 전형적인 알고리즘은

532
00:24:10,620 --> 00:24:12,790
예를 들어 Q-학습이나

533
00:24:12,790 --> 00:24:16,220
정책 반복과 같은 분야에 있습니다.

534
00:24:16,220 --> 00:24:19,660
여기 Q 함수를 학습하려는 제 예시가 있습니다.

535
00:24:19,660 --> 00:24:25,570
Q 함수는 본질적으로 특정 상태 s에서 특정 행동 a를

536
00:24:25,570 --> 00:24:29,890
적용할 때 할인된 기대 미래 누적

537
00:24:29,890 --> 00:24:31,630
보상을 측정합니다.

538
00:24:31,630 --> 00:24:33,370
이 게임 환경과의

539
00:24:33,370 --> 00:24:37,830
상호작용을 통해 이러한 Q 함수를 얻을 수 있습니다.

540
00:24:37,830 --> 00:24:40,580
이 Q 함수를 학습한 후에는

541
00:24:40,580 --> 00:24:43,240
다양한 행동을 적용하여

542
00:24:43,240 --> 00:24:46,850
얻는 Q 값을 평가할 수 있습니다.

543
00:24:46,850 --> 00:24:49,940
이 경우 왼쪽과 오른쪽, 위와 아래가 있습니다.

544
00:24:49,940 --> 00:24:52,280
따라서 잠재적으로 네 가지 행동이 있을 수 있습니다.

545
00:24:52,280 --> 00:24:55,570
이 네 가지 행동을 고려하여 Q 값을 보고

546
00:24:55,570 --> 00:24:57,430
가장 높은 Q 값을 주는 행동을

547
00:24:57,430 --> 00:24:59,120
실행할 수 있습니다.

548
00:24:59,120 --> 00:25:03,220
이것이 이 분야에서 이러한 유형의

549
00:25:03,220 --> 00:25:06,160
의사 결정을 가능하게 합니다.

550
00:25:06,160 --> 00:25:08,690
오늘 많은 자료를 다룰 것이기 때문에

551
00:25:08,690 --> 00:25:12,040
강화 학습의 세부 사항에 들어가지 않을 것입니다.

552
00:25:12,040 --> 00:25:14,780
하지만 SAC, 소프트 액터 비평가

553
00:25:14,780 --> 00:25:18,680
및 PPO, 근접 정책 최적화와 같은 현재의

554
00:25:18,680 --> 00:25:22,040
최첨단 강화 학습 알고리즘이 있습니다.

555
00:25:22,040 --> 00:25:24,140
관심이 있으시면 이러한

556
00:25:24,140 --> 00:25:26,820
알고리즘을 자세히 살펴보시기 바랍니다.

557
00:25:26,820 --> 00:25:30,800
온라인에는 많은 오픈 소스 구현 및 튜토리얼이

558
00:25:30,800 --> 00:25:32,000
있습니다.

559
00:25:32,000 --> 00:25:33,770
하지만 여기서는

560
00:25:33,770 --> 00:25:36,020
강화 학습, 특히 Q-학습

561
00:25:36,020 --> 00:25:39,320
과정을 통해 얻을 수 있는 결과

562
00:25:39,320 --> 00:25:42,215
몇 가지를 강조하고 싶습니다.

563
00:25:42,215 --> 00:25:44,840
이것은 Google DeepMind에서

564
00:25:44,840 --> 00:25:47,030
개발한 것으로, Atari

565
00:25:47,030 --> 00:25:50,900
세계에서 Breakout 게임을 하려고 하는 에이전트를 개발하고

566
00:25:50,900 --> 00:25:51,810
있습니다.

567
00:25:51,810 --> 00:25:55,650
훈련 10분 후, 로봇 에이전트는

568
00:25:55,650 --> 00:25:59,250
이미 공을 터치할 수 있지만,

569
00:25:59,250 --> 00:26:03,710
종종 공을 놓치는 경우가 많습니다.

570
00:26:03,710 --> 00:26:06,350
그리고 더 많은 학습 후,

571
00:26:06,350 --> 00:26:10,400
예를 들어 2시간의 훈련을 통해

572
00:26:10,400 --> 00:26:14,070
에이전트는 훨씬 더 신뢰할 수

573
00:26:14,070 --> 00:26:18,100
있고 일관된 방식으로 경로를 제어할

574
00:26:18,100 --> 00:26:21,270
수 있으며, 거의 항상

575
00:26:21,270 --> 00:26:27,480
공을 잡고 지속적으로 더 많은 보상을 받을 수 있습니다.

576
00:26:27,480 --> 00:26:31,650
훈련이 4시간이 지나면 흥미로운 일이

577
00:26:31,650 --> 00:26:35,490
발생하는데, 에이전트가 사실상 많은

578
00:26:35,490 --> 00:26:39,400
사람들에게 알려지지 않은 새로운 전략을

579
00:26:39,400 --> 00:26:42,090
고안하게 됩니다. 즉, 공을

580
00:26:42,090 --> 00:26:46,415
밀어 벽의 왼쪽에 터널을 만드는 것입니다.

581
00:26:46,415 --> 00:26:50,010
그리고 벽의 윗부분에서 이 공을

582
00:26:50,010 --> 00:26:55,060
밀어 이 벽돌들을 매우 효율적으로 제거합니다.

583
00:26:55,060 --> 00:26:56,520
이것은 강화 학습을

584
00:26:56,520 --> 00:26:59,130
통해 발견할 수 있는 전략의 유형입니다.

585
00:26:59,130 --> 00:27:01,680
그래서 강화 학습의 장점은

586
00:27:01,680 --> 00:27:05,760
에이전트가 세계와 매우 광범위하고 포괄적인 탐색 및

587
00:27:05,760 --> 00:27:09,280
상호작용을 할 수 있게 한다는 것입니다.

588
00:27:09,280 --> 00:27:12,640
이 강화 학습 에이전트가 최고의 인간

589
00:27:12,640 --> 00:27:15,100
플레이어보다 더 나은 전략을

590
00:27:15,100 --> 00:27:18,620
발견하는 것이 전혀 불가능하지 않습니다.

591
00:27:18,620 --> 00:27:22,260
매우 전형적인 예는 바둑 게임입니다.

592
00:27:22,260 --> 00:27:26,480
2016년 1월에 AlphaGo가

593
00:27:26,480 --> 00:27:29,620
등장했을 때, 제가 어떤 연구

594
00:27:29,620 --> 00:27:32,650
방향을 정할지 고민하던

595
00:27:32,650 --> 00:27:33,710
시기였습니다.

596
00:27:33,710 --> 00:27:36,550
그 이전에는 컴퓨터 비전을 위한 딥러닝

597
00:27:36,550 --> 00:27:37,990
작업만 하고 있었습니다.

598
00:27:37,990 --> 00:27:40,610
하지만 AlphaGo가 나왔을 때, 저는 이러한 의사

599
00:27:40,610 --> 00:27:43,010
결정 문제에 대해 작업해야겠다고 생각했습니다.

600
00:27:43,010 --> 00:27:46,280
그래서 강화 학습, 모방 학습에 대해 다루기

601
00:27:46,280 --> 00:27:48,470
시작했고, 지금까지 로봇 학습을

602
00:27:48,470 --> 00:27:50,710
통해 로봇이 환경과 물리적 상호작용을

603
00:27:50,710 --> 00:27:53,190
할 수 있도록 하고 있습니다.

604
00:27:53,190 --> 00:27:55,510
저는 단순히 수집된 데이터 세트로

605
00:27:55,510 --> 00:27:57,380
작업하는 것에 만족하지 않았고,

606
00:27:57,380 --> 00:28:00,220
환경과 능동적으로 상호작용할 수 있는

607
00:28:00,220 --> 00:28:01,690
에이전트를 원했습니다.

608
00:28:01,690 --> 00:28:05,230
그래서 질문은 이 Q 함수가 구체적으로

609
00:28:05,230 --> 00:28:07,220
어떻게 작동하는가였습니다.

610
00:28:07,220 --> 00:28:10,010
보시다시피, 이 Q는 상태 s와

611
00:28:10,010 --> 00:28:12,450
행동을 입력으로 받습니다.

612
00:28:12,450 --> 00:28:14,870
이 데이터는 본질적으로 Q

613
00:28:14,870 --> 00:28:17,720
함수의 매개변수가 되며, Q는

614
00:28:17,720 --> 00:28:19,730
신경망으로 인스턴스화됩니다.

615
00:28:19,730 --> 00:28:21,960
이 특정 경우, 제가 앞서

616
00:28:21,960 --> 00:28:24,680
언급한 것처럼 상태는 실제로 게임

617
00:28:24,680 --> 00:28:27,240
화면에서 얻는 원시 픽셀 입력입니다.

618
00:28:27,240 --> 00:28:29,820
따라서 입력은 이 네 단계, 네

619
00:28:29,820 --> 00:28:33,330
프레임이 이 Q 함수에 직접 입력될 수 있습니다.

620
00:28:33,330 --> 00:28:34,950
이미지를 다룰 때, 이

621
00:28:34,950 --> 00:28:38,600
Q 함수를 구현하는 매우 직관적인 방법은 합성곱

622
00:28:38,600 --> 00:28:41,010
신경망을 사용하는 것입니다.

623
00:28:41,010 --> 00:28:43,040
이런 종류의 주황색 블록처럼

624
00:28:43,040 --> 00:28:44,490
합성곱 층이 있습니다.

625
00:28:44,490 --> 00:28:46,910
그런 다음 완전 연결 층을

626
00:28:46,910 --> 00:28:51,030
통해 이 Q 값을 직접 도출할 수 있습니다.

627
00:28:51,030 --> 00:28:54,878
이 경우, 네 개의 이산 행동이 있기 때문에-- 아마도

628
00:28:54,878 --> 00:28:56,670
왼쪽과 오른쪽입니다.

629
00:28:56,670 --> 00:28:59,087
하지만 네 개의 이산 행동이 있다고 가정해 봅시다-- 위와 아래,

630
00:28:59,087 --> 00:28:59,790
왼쪽과 오른쪽.

631
00:28:59,790 --> 00:29:02,240
따라서 특정 행동 a의 결과로

632
00:29:02,240 --> 00:29:06,060
서로 다른 Q 값 추정치를 가질 수 있습니다.

633
00:29:06,060 --> 00:29:08,700
이렇게 Q 값을 사용하여 가장

634
00:29:08,700 --> 00:29:11,490
효과적이고 Q 값을 극대화하는 행동을

635
00:29:11,490 --> 00:29:12,798
결정할 수 있습니다.

636
00:29:12,798 --> 00:29:14,090
이 질문에 대한 답이 되었나요?

637
00:29:17,835 --> 00:29:20,500
네, 이것이 AlphaGo가 등장했을 때입니다.

638
00:29:20,500 --> 00:29:22,470
그리고 그 이후로 이러한

639
00:29:22,470 --> 00:29:24,420
유형의 게임 에이전트를 더

640
00:29:24,420 --> 00:29:28,150
나아지게 하는 많은 발전과 진화가 있었습니다.

641
00:29:28,150 --> 00:29:31,000
그 후 AlphaGo Zero가 나왔습니다.

642
00:29:31,000 --> 00:29:34,210
본질적으로 AlphaGo의 단순화된

643
00:29:34,210 --> 00:29:36,150
버전으로, 더 이상

644
00:29:36,150 --> 00:29:39,040
모방 학습을 사용하지 않습니다.

645
00:29:39,040 --> 00:29:42,540
그 당시 1위 선수인 커제에게

646
00:29:42,540 --> 00:29:45,120
이길 수 있었습니다.

647
00:29:45,120 --> 00:29:48,210
이것은 AI 커뮤니티에서 사람들이

648
00:29:48,210 --> 00:29:51,690
배운 한 가지 교훈으로, 리치 서튼의

649
00:29:51,690 --> 00:29:54,580
쓴 교훈이라고 부를 수

650
00:29:54,580 --> 00:29:57,810
있습니다. 때때로 가장 간단한 레시피를

651
00:29:57,810 --> 00:30:01,740
찾는 것이 가장 잘 확장성과 호환됩니다.

652
00:30:01,740 --> 00:30:04,820
확장의 힘을 활용하고 싶습니다.

653
00:30:04,820 --> 00:30:07,480
때때로 이 방법을

654
00:30:07,480 --> 00:30:09,670
단순화하면 더

655
00:30:09,670 --> 00:30:13,030
나은 성능을 제공할

656
00:30:13,030 --> 00:30:15,200
수 있습니다.

657
00:30:15,200 --> 00:30:18,790
그 후 Alpha Zero를 개발하여

658
00:30:18,790 --> 00:30:21,670
동일한 알고리즘 세트를 체스와 쇼기와

659
00:30:21,670 --> 00:30:23,200
같은 다른

660
00:30:23,200 --> 00:30:26,500
게임으로 일반화할 수 있게 되었습니다.

661
00:30:26,500 --> 00:30:30,085
그런 다음 모델 없는 강화 학습을 수행할

662
00:30:30,085 --> 00:30:32,680
뿐만 아니라, 더 나은 성능을

663
00:30:32,680 --> 00:30:35,500
제공하는 잠재 공간 동역학

664
00:30:35,500 --> 00:30:40,090
모델을 학습할 수 있는 MuZero를 설계했습니다.

665
00:30:40,090 --> 00:30:42,790
이 특정 도메인에서는 특히

666
00:30:42,790 --> 00:30:47,380
게임 개발에서 사람들이 더 나은 설계와 효율적인

667
00:30:47,380 --> 00:30:51,220
조립 및 확장 가능한 강화 학습 에이전트를

668
00:30:51,220 --> 00:30:54,280
설계하는 데 많은 힘을 주었다고

669
00:30:54,280 --> 00:30:56,305
말할 수 있습니다.

670
00:30:56,305 --> 00:31:01,950
2019년 11월, AlphaGo에게 패배한 이세돌이

671
00:31:01,950 --> 00:31:03,780
은퇴를 발표했습니다.

672
00:31:03,780 --> 00:31:05,910
그는 당시 어떤

673
00:31:05,910 --> 00:31:09,560
인간 플레이어도 최고의 AI 에이전트를

674
00:31:09,560 --> 00:31:14,135
이길 수 없다는 것을 깨달았습니다.

675
00:31:14,135 --> 00:31:16,250
그 이후로 스타크래프트와

676
00:31:16,250 --> 00:31:20,000
도타와 같은 더 복잡한

677
00:31:20,000 --> 00:31:23,720
게임이 등장했으며, 충분한 컴퓨팅

678
00:31:23,720 --> 00:31:28,220
자원과 잘 설계된 알고리즘 및 인프라가

679
00:31:28,220 --> 00:31:29,940
있다면,

680
00:31:29,940 --> 00:31:33,470
실제로 바둑보다 훨씬 더 복잡한

681
00:31:33,470 --> 00:31:35,900
게임에서도 매우

682
00:31:35,900 --> 00:31:39,230
좋은 성능을 얻을 수 있습니다.

683
00:31:39,230 --> 00:31:42,530
따라서 합리적으로 설계된 게임이 있다면,

684
00:31:42,530 --> 00:31:45,950
충분한 자원을 투입하면 매우

685
00:31:45,950 --> 00:31:48,300
강력한 게임 에이전트를 가질

686
00:31:48,300 --> 00:31:51,230
수 있는 좋은 기회가 있습니다.

687
00:31:51,230 --> 00:31:54,028
게임뿐만 아니라, 사람들은 실제

688
00:31:54,028 --> 00:31:55,820
물리적 세계에서 직접

689
00:31:55,820 --> 00:31:58,760
작동할 수 있는 강화 학습 알고리즘과

690
00:31:58,760 --> 00:32:01,125
에이전트도 개발하고 있습니다.

691
00:32:01,125 --> 00:32:04,950
왼쪽은 2020년 Science Robotics에

692
00:32:04,950 --> 00:32:08,140
발표된 ETH의 작업입니다.

693
00:32:08,140 --> 00:32:11,700
이는 강화 학습이 실제 물리적 로봇에 얼마나 유용할

694
00:32:11,700 --> 00:32:14,290
수 있는지에 대한 제 생각을 바꿨습니다.

695
00:32:14,290 --> 00:32:15,930
이전에는 대부분 게임에

696
00:32:15,930 --> 00:32:17,580
국한되었기 때문입니다.

697
00:32:17,580 --> 00:32:19,950
게임에서는 가능한 한

698
00:32:19,950 --> 00:32:23,140
많은 게임을 생성할 수 있습니다.

699
00:32:23,140 --> 00:32:26,250
하지만 실제 세계에서는 항상 시뮬레이션과 실제 간의 격차가

700
00:32:26,250 --> 00:32:28,060
존재합니다. 동일한 게임에서 훈련하고

701
00:32:28,060 --> 00:32:29,710
동일한 게임에서 테스트합니다.

702
00:32:29,710 --> 00:32:32,920
하지만 로봇의 경우, 시뮬레이션에서 훈련하면

703
00:32:32,920 --> 00:32:35,940
시뮬레이션과 실제 간의 격차가 실제 환경에

704
00:32:35,940 --> 00:32:37,960
일반화하는 데 얼마나 중요한가요?

705
00:32:37,960 --> 00:32:40,140
이 논문은 때때로 시뮬레이션과 실제

706
00:32:40,140 --> 00:32:42,810
간의 격차가 그리 중요하지 않을 수 있다는

707
00:32:42,810 --> 00:32:44,735
것을 저에게 확신시켰습니다.

708
00:32:44,735 --> 00:32:46,540
따라서 우리는 덤불을 시뮬레이션하고 있지 않습니다.

709
00:32:46,540 --> 00:32:48,130
우리는 눈을 시뮬레이션하고 있는 것이 아닙니다.

710
00:32:48,130 --> 00:32:50,647
하지만 강화 학습을 사용하는 에이전트는

711
00:32:50,647 --> 00:32:52,230
이러한 훈련 시뮬레이션을

712
00:32:52,230 --> 00:32:54,120
통해 실제 물리적

713
00:32:54,120 --> 00:32:58,230
세계에서 매우 강력한 성능을 발휘할 수 있습니다. 예를 들어

714
00:32:58,230 --> 00:33:00,420
눈이나 매우 미끄러운 표면에서요.

715
00:33:00,420 --> 00:33:02,370
오른쪽은 Unitree에서

716
00:33:02,370 --> 00:33:07,410
최근에 발표한 비디오로, 시뮬레이션에서 실제로 전이할 수

717
00:33:07,410 --> 00:33:11,470
있는 이동성의 또 다른 수준을 보여줍니다. 이

718
00:33:11,470 --> 00:33:14,010
로봇들은 매우 미친 듯이 역동적인

719
00:33:14,010 --> 00:33:15,820
행동을 할 수 있습니다.

720
00:33:15,820 --> 00:33:19,090
그들은 매우 거칠고 도전적인 지형을 탐색할 수 있습니다.

721
00:33:19,090 --> 00:33:22,450
로봇 이동성 분야에서는 거의 해결된

722
00:33:22,450 --> 00:33:26,170
문제에 가깝다고 할 수 있습니다.

723
00:33:26,170 --> 00:33:29,280
이 문제의 해결책은 바로 강화

724
00:33:29,280 --> 00:33:30,670
학습입니다.

725
00:33:30,670 --> 00:33:33,420
이것은 이동성에 관한 것입니다.

726
00:33:33,420 --> 00:33:37,140
다른 분야는 조작에 관한 것으로, 로봇이

727
00:33:37,140 --> 00:33:40,910
실제 물리적 세계에서 물체를 조작해야 합니다.

728
00:33:40,910 --> 00:33:44,790
2019년, OpenAI가 로봇 공학에 대해

729
00:33:44,790 --> 00:33:48,150
다루고 있을 때, 그들은 루빅스 큐브의 능숙한

730
00:33:48,150 --> 00:33:51,580
조작을 시도하는 시스템을 설계했습니다.

731
00:33:51,580 --> 00:33:54,000
그들은 시뮬레이션에서 강화 학습을

732
00:33:54,000 --> 00:33:56,740
수행하고, 이러한 로봇들이 루빅스 큐브를

733
00:33:56,740 --> 00:34:00,860
해결할 수 있도록 하는 시뮬레이션에서 실제로 전이할 수 있습니다.

734
00:34:00,860 --> 00:34:04,328
하지만 한 가지 주의할 점은 그들의 성공률이 매우 낮다는 것입니다.

735
00:34:04,328 --> 00:34:06,620
이 비디오들이 매우 아름답게 제작된 것처럼

736
00:34:06,620 --> 00:34:08,480
보이지만, 그들의 성공률은 매우 낮았습니다.

737
00:34:08,480 --> 00:34:10,840
그리고 논문을 자세히 살펴보면,

738
00:34:10,840 --> 00:34:14,000
그들은 매우 제한된 수의 실험만을 수행했습니다.

739
00:34:14,000 --> 00:34:17,710
그 숫자를 고려할 때, 신뢰성이 그리

740
00:34:17,710 --> 00:34:19,940
만족스럽지 않을 수 있습니다.

741
00:34:19,940 --> 00:34:22,330
하지만 그 이후로

742
00:34:22,330 --> 00:34:27,130
사람들은 이러한 능숙한 조작 문제를

743
00:34:27,130 --> 00:34:30,850
확장하여 로봇들이 다양한 유형의

744
00:34:30,850 --> 00:34:33,850
물체를 향상된 능숙한 조작과

745
00:34:33,850 --> 00:34:37,719
재배치할 수 있도록 했습니다.

746
00:34:37,719 --> 00:34:40,750
모두 강화 학습의 발전

747
00:34:40,750 --> 00:34:42,250
덕분입니다.

748
00:34:42,250 --> 00:34:47,469
하지만 지금까지의 이동성과 손 조작의 예를 보면, 그들에게

749
00:34:47,469 --> 00:34:49,612
문제를 해결하지는 못하지만,

750
00:34:49,612 --> 00:34:51,820
로봇이 당신을 위해

751
00:34:51,820 --> 00:34:53,903
옷을 개거나 세탁을 해줄

752
00:34:53,903 --> 00:34:55,790
수 있다면 좋겠습니다.

753
00:34:55,790 --> 00:34:59,490
조작의 경우, 여전히 이러한 고립된 환경에서

754
00:34:59,490 --> 00:35:03,230
작업하는 것과 같은 매우 제한된 영역에 있습니다.

755
00:35:03,230 --> 00:35:07,010
이것은 기존의 모델 없는 강화 학습의 주요

756
00:35:07,010 --> 00:35:10,040
도전 과제와 병목 현상 중 일부입니다.

757
00:35:10,040 --> 00:35:12,620
주로 환경과의 시행착오를

758
00:35:12,620 --> 00:35:16,670
통해 학습하며, 세계와의 광범위한 상호작용이

759
00:35:16,670 --> 00:35:17,940
필요합니다.

760
00:35:17,940 --> 00:35:20,782
예를 들어, AlphaGo Zero는

761
00:35:20,782 --> 00:35:24,710
40일 동안 3,000년의 인간 지식으로부터 학습합니다.

762
00:35:24,710 --> 00:35:26,070
이는 놀랍습니다.

763
00:35:26,070 --> 00:35:29,960
하지만 여전히 에이전트가 학습하기

764
00:35:29,960 --> 00:35:32,930
위해서는 수년의 계산이

765
00:35:32,930 --> 00:35:34,870
필요합니다.

766
00:35:34,870 --> 00:35:37,740
시뮬레이션과 실제 간의 큰 격차가 있는

767
00:35:37,740 --> 00:35:39,890
분야에서 실제 물리적 세계에서 강화

768
00:35:39,890 --> 00:35:43,130
학습을 수행하고자 한다면, 이는 강화 학습 에이전트를

769
00:35:43,130 --> 00:35:45,620
매우 효과적으로 학습하는 데 큰

770
00:35:45,620 --> 00:35:47,225
병목 현상이 될 것입니다.

771
00:35:47,225 --> 00:35:50,450
물론, 시뮬레이션과 실제 간의 격차가 있다면,

772
00:35:50,450 --> 00:35:52,660
실제 환경에서만 모델을 학습할 수

773
00:35:52,660 --> 00:35:54,870
있다면 많은 안전 문제가 발생합니다.

774
00:35:54,870 --> 00:35:57,270
예를 들어, 여기에는 이러한 휴머노이드

775
00:35:57,270 --> 00:35:58,770
로봇이 앞으로 이동하도록

776
00:35:58,770 --> 00:36:02,250
제어하는 에이전트의 학습 진행 상황을 보여주는

777
00:36:02,250 --> 00:36:03,010
예가 있습니다.

778
00:36:03,010 --> 00:36:04,230
이 학습 과정에서

779
00:36:04,230 --> 00:36:05,700
로봇이 결국 앞으로

780
00:36:05,700 --> 00:36:08,140
이동할 수 있지만, 매우 이상한

781
00:36:08,140 --> 00:36:10,590
행동이 많이 발생하는 것을 볼

782
00:36:10,590 --> 00:36:12,450
수 있습니다. 이 에이전트를

783
00:36:12,450 --> 00:36:14,890
실제 로봇에 적용하면 재앙적으로

784
00:36:14,890 --> 00:36:17,865
실패할 것이라고 상상할 수 있습니다.

785
00:36:17,865 --> 00:36:21,910
그리고 해석 가능성도 매우 제한적입니다.

786
00:36:21,910 --> 00:36:24,360
때때로, 일이 잘못될 때

787
00:36:24,360 --> 00:36:26,280
수정하기가 매우 어렵습니다.

788
00:36:26,280 --> 00:36:28,950
흥미로운 점은, 인간이 환경과

789
00:36:28,950 --> 00:36:31,020
상호작용하는 방법과 순수한

790
00:36:31,020 --> 00:36:34,150
강화 학습을 생각해보면, 우리는 이

791
00:36:34,150 --> 00:36:38,160
환경에 대한 매우 직관적인 이해를 가지고

792
00:36:38,160 --> 00:36:39,370
있다는 것입니다.

793
00:36:39,370 --> 00:36:41,340
우리는 특정 행동을 적용할

794
00:36:41,340 --> 00:36:44,530
경우 환경이 어떻게 변화할지를 상상할 수 있습니다.

795
00:36:44,530 --> 00:36:47,220
바로 이러한 예측 능력이 우리가

796
00:36:47,220 --> 00:36:50,470
특정 목표를 달성하기 위해 행동을 계획할

797
00:36:50,470 --> 00:36:52,255
수 있게 해줍니다.

798
00:36:52,255 --> 00:36:55,600
이러한 예측 능력은 실제 물리적

799
00:36:55,600 --> 00:36:57,940
세계와의 인간의 물리적

800
00:36:57,940 --> 00:37:02,140
상호작용과 일상적인 경험에서 학습된 것입니다.

801
00:37:02,140 --> 00:37:04,010
강화 학습을 넘어,

802
00:37:04,010 --> 00:37:06,940
제가 논의하고 싶은 다음 주제는 로봇이

803
00:37:06,940 --> 00:37:10,960
자신의 행동의 결과를 상상하고 모델 기반 계획을

804
00:37:10,960 --> 00:37:13,760
수행할 수 있는 유사한 능력을

805
00:37:13,760 --> 00:37:15,530
부여하는 방법입니다.

806
00:37:15,530 --> 00:37:18,230
구체적인 예로, 우리는 시뮬레이션을 가지고 있습니다.

807
00:37:18,230 --> 00:37:20,900
일반적으로 사람들이 사용하는 시뮬레이션은 NVIDIA에서

808
00:37:20,900 --> 00:37:23,910
개발한 Isaac Gym 또는 Isaac Sim과 같은 것입니다.

809
00:37:23,910 --> 00:37:27,100
본질적으로 로봇이 바닥의 표현과

810
00:37:27,100 --> 00:37:30,340
같은 다각형 형태의 표현에

811
00:37:30,340 --> 00:37:34,790
접촉하는 강체 시뮬레이션이 있습니다.

812
00:37:34,790 --> 00:37:36,830
예를 들어, 덤불을 시뮬레이션하지 않습니다.

813
00:37:36,830 --> 00:37:38,895
눈을 시뮬레이션하지 않습니다.

814
00:37:38,895 --> 00:37:40,810
하지만 사람들이 하는 것은

815
00:37:40,810 --> 00:37:43,960
시뮬레이션된 환경을 많이 무작위화하고

816
00:37:43,960 --> 00:37:47,200
마찰, 기하학 및 이 환경 내의 많은 다른

817
00:37:47,200 --> 00:37:51,960
물리적 매개변수를 무작위화하여 사람들이 실제 물리적 세계에서

818
00:37:51,960 --> 00:37:55,350
만나는 것은 시뮬레이션 내에서 무작위화한 분포

819
00:37:55,350 --> 00:37:57,630
내의 하나의 데이터 포인트라고

820
00:37:57,630 --> 00:37:59,800
가정하게 만드는 것입니다.

821
00:37:59,800 --> 00:38:02,370
따라서 정책이 그 분포 내에서 로봇을

822
00:38:02,370 --> 00:38:05,107
제어하는 데 강력할 수 있고, 실제 세계가

823
00:38:05,107 --> 00:38:06,690
그 분포 내의 하나의 데이터

824
00:38:06,690 --> 00:38:09,550
포인트라면 정책은 일반화할 수 있습니다.

825
00:38:09,550 --> 00:38:12,130
지금까지 이 경험적 증거에 따르면, 이러한

826
00:38:12,130 --> 00:38:14,020
유형의 가정은 실제로 성립하며,

827
00:38:14,020 --> 00:38:16,530
정책은 실제 물리적 세계에서 매우 신뢰할

828
00:38:16,530 --> 00:38:18,390
수 있고 강력하게 작동합니다.

829
00:38:18,390 --> 00:38:20,800
그래서 질문은 실제 명령이 무엇인가입니다.

830
00:38:20,800 --> 00:38:23,530
실제로 많은 기존 데모에서는 사람이

831
00:38:23,530 --> 00:38:26,430
로봇에 고수준 명령을 제공할 수

832
00:38:26,430 --> 00:38:27,100
있습니다.

833
00:38:27,100 --> 00:38:29,490
예를 들어, 로봇이 어떤 방향으로 걸어야 하는지입니다.

834
00:38:29,490 --> 00:38:32,430
그래서 로봇은 제자리에서 회전하거나 계속

835
00:38:32,430 --> 00:38:33,510
앞으로 걸어갑니다.

836
00:38:33,510 --> 00:38:37,270
사람이 제공한 고수준 행동에 따라 로봇은 이러한

837
00:38:37,270 --> 00:38:40,000
저수준 행동을 결정해야 합니다.

838
00:38:40,000 --> 00:38:41,550
저수준 행동은

839
00:38:41,550 --> 00:38:45,480
일반적으로 이 로봇의 모든 관절에

840
00:38:45,480 --> 00:38:47,730
적용되는 관절 토크와

841
00:38:47,730 --> 00:38:49,270
같은 것입니다.

842
00:38:49,270 --> 00:38:52,020
이것이 일반적으로 어떻게 보이는지를 의미하며,

843
00:38:52,020 --> 00:38:54,147
인간이 고수준 명령, 조건부 고수준

844
00:38:54,147 --> 00:38:55,480
명령을 제공합니다.

845
00:38:55,480 --> 00:38:57,450
로봇은 이 정책을 사용하여

846
00:38:57,450 --> 00:39:00,570
관절 토크를 사용하여 구현되는 저수준

847
00:39:00,570 --> 00:39:02,520
행동을 결정해야 합니다.

848
00:39:02,520 --> 00:39:05,850
제가 언급했듯이, 이 보행 작업에서 배운 가장

849
00:39:05,850 --> 00:39:08,727
큰 교훈 중 하나는 시뮬레이션이 완벽할

850
00:39:08,727 --> 00:39:10,810
필요는 없다는 것입니다.

851
00:39:10,810 --> 00:39:12,460
충분히 무작위화하기만 하면

852
00:39:12,460 --> 00:39:15,960
실제 환경에서 매우 강력하게 일반화할 수 있습니다.

853
00:39:15,960 --> 00:39:19,200
하지만 이러한 교훈은 조작 영역에서 잘

854
00:39:19,200 --> 00:39:21,330
일반화되지 않았습니다.

855
00:39:21,330 --> 00:39:23,850
조작 영역에서 시뮬레이션이 얼마나 정확해야

856
00:39:23,850 --> 00:39:26,460
하는지와 시뮬레이션과 실제 간의 차이가 얼마나

857
00:39:26,460 --> 00:39:29,950
중요한지는 여전히 사람들이 답하고자 하는 연구 질문입니다.

858
00:39:29,950 --> 00:39:32,140
하나의 구체적인 예를 드릴 수 있습니다.

859
00:39:32,140 --> 00:39:34,840
시뮬레이션에서 상자를 앞으로 밀고 있을 때,

860
00:39:34,840 --> 00:39:37,420
시뮬레이션에서 상자가 10도 회전하지만

861
00:39:37,420 --> 00:39:39,670
실제로는 12도 회전한다면, 그리

862
00:39:39,670 --> 00:39:41,170
중요하지 않을 수 있습니다.

863
00:39:41,170 --> 00:39:44,680
하지만 실제 세계에서 당신의 잡기가 성공적이었다면.

864
00:39:44,680 --> 00:39:46,720
그러나 시뮬레이션에서는 물체가 어떤

865
00:39:46,720 --> 00:39:49,370
수치적 문제로 인해 그냥 날아가 버립니다.

866
00:39:49,370 --> 00:39:52,520
또는 물체가 손가락 사이에서 미끄러져

867
00:39:52,520 --> 00:39:54,555
나간다면, 이는 문제가 됩니다.

868
00:39:54,555 --> 00:39:56,912
따라서 시뮬레이션과 실제 간의 차이가 중요한 영역이 있습니다.

869
00:39:56,912 --> 00:39:58,870
조작 영역에서는 시뮬레이션과 실제 간의 차이가

870
00:39:58,870 --> 00:40:01,070
그리 중요하지 않을 수 있는 다른 영역도 있습니다.

871
00:40:01,070 --> 00:40:02,830
사람들은 여전히 시뮬레이션과

872
00:40:02,830 --> 00:40:06,890
실제 간의 차이가 어떻게 발생할 수 있는지, 그리고 가장 신뢰할

873
00:40:06,890 --> 00:40:09,280
수 있는 시뮬레이션-실제 전이를 위해

874
00:40:09,280 --> 00:40:11,470
이 시뮬레이션이 가져야 할 가장

875
00:40:11,470 --> 00:40:15,220
중요한 레시피와 특성이 무엇인지 이해하려고 노력하고 있습니다.

876
00:40:15,220 --> 00:40:17,660
그래서 제가 당신의 질문을 올바르게

877
00:40:17,660 --> 00:40:18,790
이해했다면, 여전히

878
00:40:18,790 --> 00:40:21,700
사람이 로봇에 고수준 명령을 제공하고 있다는

879
00:40:21,700 --> 00:40:22,305
것입니다.

880
00:40:22,305 --> 00:40:23,680
그래서 질문은 로봇이 인간보다

881
00:40:23,680 --> 00:40:25,760
더 나은 계획을 세울 수 있는가입니다.

882
00:40:25,760 --> 00:40:28,880
그래서 저는 실제로 더 미묘한 관점을 제공할 수 있습니다.

883
00:40:28,880 --> 00:40:31,400
이 비디오들이 매우 멋져

884
00:40:31,400 --> 00:40:34,990
보이지만, 로봇이 어떤 경로를 선택할지

885
00:40:34,990 --> 00:40:37,550
결정하는 인간 조작자가 있습니다.

886
00:40:37,550 --> 00:40:40,430
예를 들어, 사람들이 일반적으로 하는 것은

887
00:40:40,430 --> 00:40:42,370
거칠거나 바위 더미가 있는

888
00:40:42,370 --> 00:40:43,920
지형을 가정하는 것입니다.

889
00:40:43,920 --> 00:40:47,480
인간은 실제로 로봇에게 앞으로 나아가고 그 바위를

890
00:40:47,480 --> 00:40:50,010
오르도록 명령할 수 있습니다.

891
00:40:50,010 --> 00:40:52,010
이것이 실패하면, 인간은 바위

892
00:40:52,010 --> 00:40:54,560
더미를 피하기 위한 다른 고급 명령을

893
00:40:54,560 --> 00:40:55,850
제공할 수 있습니다.

894
00:40:55,850 --> 00:40:57,800
따라서 로봇의 능력을 이해하는

895
00:40:57,800 --> 00:40:59,960
데 있어 인간 측에서도 어떤

896
00:40:59,960 --> 00:41:01,600
학습이 있을 수 있습니다.

897
00:41:01,600 --> 00:41:04,280
이것이 일부 비디오가 매우 멋져 보일

898
00:41:04,280 --> 00:41:08,180
수 있는 이유이기도 합니다. 왜냐하면 인간이 알고 있는

899
00:41:08,180 --> 00:41:10,850
경로를 선택하여 이러한 저수준 제어기의

900
00:41:10,850 --> 00:41:13,625
한계와 능력을 보여주기 때문입니다.

901
00:41:13,625 --> 00:41:15,510
그렇다면 이를 자율적으로 어떻게 할 수 있을까요?

902
00:41:15,510 --> 00:41:18,200
사람들이 연구하고 있는 매우

903
00:41:18,200 --> 00:41:19,930
흥미로운 질문입니다.

904
00:41:22,925 --> 00:41:25,230
그럼 계속하겠습니다.

905
00:41:25,230 --> 00:41:28,850
저는 강화 학습의 성공적인 사례와 힘에 대해

906
00:41:28,850 --> 00:41:30,683
논의했으며, 강화 학습의

907
00:41:30,683 --> 00:41:32,850
한계에 대해서도 논의했습니다.

908
00:41:32,850 --> 00:41:35,720
그리고 우리는 아직 조작에서 강화

909
00:41:35,720 --> 00:41:38,330
학습의 매우 성공적이고 대규모

910
00:41:38,330 --> 00:41:40,140
배포를 보지 못했습니다.

911
00:41:40,140 --> 00:41:42,790
우리는 인간이 단순히 시행착오로 배우지 않습니다.

912
00:41:42,790 --> 00:41:45,130
우리는 실제로 이러한 내부 모델을 구축합니다.

913
00:41:45,130 --> 00:41:47,460
그래서 우리는 로봇이 환경과

914
00:41:47,460 --> 00:41:49,800
상호작용하면서 모델을 학습하고, 그 모델을

915
00:41:49,800 --> 00:41:53,070
사용하여 로봇이 더 나은 물리적 상호작용을

916
00:41:53,070 --> 00:41:55,320
할 수 있을지 질문하고 있습니다.

917
00:41:55,320 --> 00:41:58,110
구체적으로, 우리가 다시 이 그림을

918
00:41:58,110 --> 00:42:00,120
언급하는 것은 우리가 실제

919
00:42:00,120 --> 00:42:03,610
물리적 세계를 어떻게 학습하고 근사할 수 있는지,

920
00:42:03,610 --> 00:42:07,155
그리고 이 근사된 물리적 세계가 가상 도메인에서

921
00:42:07,155 --> 00:42:10,140
어떻게 로봇의 행동을 안내하고 물리적

922
00:42:10,140 --> 00:42:12,720
세계에서 어떤 행동을 취할지를

923
00:42:12,720 --> 00:42:15,420
결정하는 데 도움이 될 수 있는지입니다.

924
00:42:15,420 --> 00:42:17,410
예를 들어, 이미 모델이

925
00:42:17,410 --> 00:42:19,868
있다고 가정해 보겠습니다. 예를

926
00:42:19,868 --> 00:42:24,160
들어, 우리가 인간처럼 정신적 환경에서 모델을 이미

927
00:42:24,160 --> 00:42:26,670
학습했다면, 현재 상태 st와 행동

928
00:42:26,670 --> 00:42:29,670
t를 주었을 때 환경의 상태가 t+1

929
00:42:29,670 --> 00:42:33,030
상태로 어떻게 변화할지를 예측할 수 있습니다.

930
00:42:33,030 --> 00:42:34,890
그리고 우리는 이것을 본질적으로 현재

931
00:42:34,890 --> 00:42:37,570
상태와 행동을 주어 다음 상태를 예측하는 전방

932
00:42:37,570 --> 00:42:38,935
모델로 사용할 수 있습니다.

933
00:42:38,935 --> 00:42:41,350
그렇다면 우리가 계획을 세우는 데

934
00:42:41,350 --> 00:42:43,600
있어 문제는 무엇일까요? 이는

935
00:42:43,600 --> 00:42:45,817
본질적으로 이 전방 모델의 역입니다.

936
00:42:45,817 --> 00:42:48,400
계획은 현재 상태와 목표 상태를

937
00:42:48,400 --> 00:42:51,670
주고 로봇이 목표 상태를 달성할 수 있도록

938
00:42:51,670 --> 00:42:54,252
하는 행동을 제시하는 것입니다.

939
00:42:54,252 --> 00:42:56,210
파란 점으로 표시된 현재 상태를 고려할

940
00:42:56,210 --> 00:42:58,735
때, 빨간 점으로 표시된 목표가 있습니다.

941
00:42:58,735 --> 00:43:00,610
행동이 어떻게 보일지에 대한 초기

942
00:43:00,610 --> 00:43:02,060
추측을 할 수 있습니다.

943
00:43:02,060 --> 00:43:05,200
우리의 모델, 즉 근사된 학습

944
00:43:05,200 --> 00:43:07,150
모델은 상태의 순차적

945
00:43:07,150 --> 00:43:09,460
진화를 예측할 수 있으며,

946
00:43:09,460 --> 00:43:13,270
이는 이 녹색 궤적에 나타납니다.

947
00:43:13,270 --> 00:43:16,510
그런 다음 이 녹색 점과 빨간 점

948
00:43:16,510 --> 00:43:18,920
사이의 거리를 측정하고, 궤적을

949
00:43:18,920 --> 00:43:24,445
따라 모든 행동에 대한 거리의 기울기를 사용하여 역전파하거나

950
00:43:24,445 --> 00:43:27,350
최적화를 수행하여 이러한 유형의

951
00:43:27,350 --> 00:43:29,330
최적화를 수행하여

952
00:43:29,330 --> 00:43:32,110
빨간 점으로 표시된 목표에 더

953
00:43:32,110 --> 00:43:35,890
가까워질 수 있는 행동을 알 수 있습니다.

954
00:43:35,890 --> 00:43:38,670
명백히도 모델이 충분히 정확하지 않을 수

955
00:43:38,670 --> 00:43:41,810
있으므로, 우리는 일반적으로 첫 번째 행동만

956
00:43:41,810 --> 00:43:44,730
실행하고 환경에서 새로운 상태를 얻습니다.

957
00:43:44,730 --> 00:43:47,210
그리고 우리는 경량 하강법이나

958
00:43:47,210 --> 00:43:49,130
다른 최적화 기술을

959
00:43:49,130 --> 00:43:53,345
사용하여 행동 시퀀스를 다시 최적화할 수 있습니다.

960
00:43:53,345 --> 00:43:56,870
특히 최근 GPU와 신경 동역학 모델의

961
00:43:56,870 --> 00:44:00,480
발전으로 인해, GPU를 사용하여

962
00:44:00,480 --> 00:44:02,930
대규모 샘플링 및 행동

963
00:44:02,930 --> 00:44:05,120
시퀀스의 최적화를 수행할

964
00:44:05,120 --> 00:44:07,760
수 있는 병렬 및 동시

965
00:44:07,760 --> 00:44:12,560
샘플링이 가능하다는 것이 주요 이점 중 하나입니다.

966
00:44:12,560 --> 00:44:14,490
따라서 이를 고려할 때, 일반적인

967
00:44:14,490 --> 00:44:17,430
프레임워크는 모델이 이 전방 프로세스라는 것입니다.

968
00:44:17,430 --> 00:44:19,460
그리고 항상 이 전방 모델을

969
00:44:19,460 --> 00:44:22,130
사용하여 이러한 역 최적화를 수행하여

970
00:44:22,130 --> 00:44:25,100
목표 구성에 더 가까워질 수 있는 행동을

971
00:44:25,100 --> 00:44:26,335
제시할 수 있습니다.

972
00:44:26,335 --> 00:44:28,340
그리고 항상 중요한 질문 중 하나는

973
00:44:28,340 --> 00:44:30,588
환경의 올바른 표현이 무엇이어야

974
00:44:30,588 --> 00:44:31,380
하는가입니다.

975
00:44:31,380 --> 00:44:33,230
올바르고 가장 효과적인 상태 표현

976
00:44:33,230 --> 00:44:34,580
s는 무엇이어야 하는가?

977
00:44:34,580 --> 00:44:36,370
그리고 상태 표현을 기반으로 이

978
00:44:36,370 --> 00:44:38,185
모델을 어떻게 학습할 수 있는가.

979
00:44:38,185 --> 00:44:40,030
수년 동안 다양한

980
00:44:40,030 --> 00:44:43,330
상태 표현을 선택하거나 조사하는

981
00:44:43,330 --> 00:44:45,820
다양한 연구가 있었습니다.

982
00:44:45,820 --> 00:44:48,500
이전 작업 중 일부는 예를 들어 2D

983
00:44:48,500 --> 00:44:51,520
이미지를 상태의 표현으로 사용하고 픽셀 동역학을

984
00:44:51,520 --> 00:44:53,600
학습하는 방법에 관한 것입니다. 즉,

985
00:44:53,600 --> 00:44:57,400
특정 행동을 적용했을 때 이미지가 어떻게 변할 수 있는지를

986
00:44:57,400 --> 00:44:58,160
의미합니다.

987
00:44:58,160 --> 00:45:00,520
이 작업은 딥 비주얼 포어사이트라고

988
00:45:00,520 --> 00:45:02,620
하며, 전체 단어 모델

989
00:45:02,620 --> 00:45:06,040
영역에서 초기 작업 중 일부를 시작했습니다.

990
00:45:06,040 --> 00:45:08,600
이러한 픽셀 기반 동역학 모델을

991
00:45:08,600 --> 00:45:10,480
학습함으로써 사람들은

992
00:45:10,480 --> 00:45:12,250
현재 관찰과 초록색으로

993
00:45:12,250 --> 00:45:14,920
표시된 목표 간의 거리를

994
00:45:14,920 --> 00:45:18,040
최소화하고, 물체를 회전시키고

995
00:45:18,040 --> 00:45:20,870
밀어내는 전략을 고안할 수 있습니다.

996
00:45:20,870 --> 00:45:23,950
현재 상태는 빨간색으로 표시됩니다.

997
00:45:23,950 --> 00:45:26,030
이것은 픽셀 동역학에 관한 것입니다.

998
00:45:26,030 --> 00:45:29,050
사람들이 할 수 있는 또 다른 방법은 환경의

999
00:45:29,050 --> 00:45:31,180
표현으로 키포인트를 사용하여

1000
00:45:31,180 --> 00:45:33,890
키포인트 동역학 모델을 학습하는 것입니다.

1001
00:45:33,890 --> 00:45:37,520
여기서 우리는 이 상자 위의 키포인트의

1002
00:45:37,520 --> 00:45:42,290
움직임을 3D 공간에서 추적하고, 특정 밀기 행동의 결과로서

1003
00:45:42,290 --> 00:45:46,160
이러한 키포인트의 신경 동역학 모델을 추적할

1004
00:45:46,160 --> 00:45:47,220
수 있습니다.

1005
00:45:47,220 --> 00:45:49,880
그런 다음 로봇이 이러한 전방

1006
00:45:49,880 --> 00:45:53,750
예측 모델을 사용하여 특정 궤적을 추적하고

1007
00:45:53,750 --> 00:45:56,360
이 상자를 밀어 목표

1008
00:45:56,360 --> 00:46:01,100
구성을 달성하도록 로봇의 행동을 계획할 수 있습니다.

1009
00:46:01,100 --> 00:46:03,080
키포인트를 사용하는 것

1010
00:46:03,080 --> 00:46:06,890
외에, 더 높은 자유도를 가진 물체를 만난다면

1011
00:46:06,890 --> 00:46:08,315
어떻게 될까요?

1012
00:46:08,315 --> 00:46:10,790
한 단계 더 세분화하면, 이러한

1013
00:46:10,790 --> 00:46:15,090
물체를 본질적으로 점의 집합인 입자의 집합으로

1014
00:46:15,090 --> 00:46:16,855
표현할 수 있습니다.

1015
00:46:16,855 --> 00:46:19,610
여기서, 제가 박사후 연구원으로

1016
00:46:19,610 --> 00:46:22,140
있을 때 수행한 작업으로,

1017
00:46:22,140 --> 00:46:25,050
우리는 이 입자들의 집합을

1018
00:46:25,050 --> 00:46:27,080
사용하여 이 입자들이

1019
00:46:27,080 --> 00:46:30,050
특정 행동을 적용했을 때 어떻게

1020
00:46:30,050 --> 00:46:32,650
움직일지를 예측하려고 했습니다.

1021
00:46:32,650 --> 00:46:34,780
이 전방 모델은 로봇이

1022
00:46:34,780 --> 00:46:37,540
다양한 크기의 입자 물체를

1023
00:46:37,540 --> 00:46:40,120
처리하는 역 결정 만들기를

1024
00:46:40,120 --> 00:46:42,200
수행할 수 있게 합니다.

1025
00:46:42,200 --> 00:46:43,750
우리는 이러한 조각들을

1026
00:46:43,750 --> 00:46:46,570
오른쪽 하단에 표시된 목표 영역으로

1027
00:46:46,570 --> 00:46:48,950
모으는 전략을 고안했습니다. 각

1028
00:46:48,950 --> 00:46:51,610
세그먼트의 모서리와 같은 곳입니다.

1029
00:46:51,610 --> 00:46:55,850
환경으로부터의 좋은 피드백을 받는 동일한 모델은

1030
00:46:55,850 --> 00:46:59,410
로봇이 모델의 오류를 수정하고 모든 물체

1031
00:46:59,410 --> 00:47:02,800
조각을 목표 영역으로 매우 신뢰성 있게

1032
00:47:02,800 --> 00:47:06,550
집합할 수 있는 전략을 고안할 수 있게 합니다.

1033
00:47:06,550 --> 00:47:08,230
분명히 이 모델은

1034
00:47:08,230 --> 00:47:11,170
서로 다른 크기의 다양한 입자 조각에

1035
00:47:11,170 --> 00:47:12,920
일반화할 수 있습니다.

1036
00:47:12,920 --> 00:47:17,063
또한 서로 다른 목표 구성으로 변경할 수 있습니다.

1037
00:47:17,063 --> 00:47:18,730
여기서 여러분은 목표 구성이

1038
00:47:18,730 --> 00:47:20,710
무엇인지 매우 빠르게 깨닫게 될 것입니다.

1039
00:47:20,710 --> 00:47:23,320
로봇은 미세한 조각의 비트리비얼

1040
00:47:23,320 --> 00:47:27,620
재분배를 수행할 전략을 세워야 합니다.

1041
00:47:27,620 --> 00:47:29,650
재분배 후에는 형태와 같은

1042
00:47:29,650 --> 00:47:31,370
목표에 맞춰 세밀한 세부

1043
00:47:31,370 --> 00:47:33,210
사항을 정렬해야 합니다.

1044
00:47:33,210 --> 00:47:36,660
이것을 더미 재배치 작업처럼 수행하고 싶습니다.

1045
00:47:36,660 --> 00:47:39,980
여기서의 작업은 이 미세한 조각들을

1046
00:47:39,980 --> 00:47:42,860
A자에서 Z자까지 다양한

1047
00:47:42,860 --> 00:47:45,860
문자 형태로 재배치하는 것입니다. 이러한 종류의 전방 모델을 통해 우리는

1048
00:47:45,860 --> 00:47:47,960
환경의 피드백을 받아 로봇이

1049
00:47:47,960 --> 00:47:49,920
이러한 물체 조각들을 목표

1050
00:47:49,920 --> 00:47:51,530
지역으로 재배치할 수

1051
00:47:51,530 --> 00:47:54,260
있는 일련의 전략을 성공적으로 개발할

1052
00:47:54,260 --> 00:47:55,500
수 있습니다.

1053
00:47:55,500 --> 00:47:58,940
이것은 실제로 매우 비트리비얼한 작업입니다.

1054
00:47:58,940 --> 00:48:03,600
그것을 넘어, 저는 여기 스탠포드에 있을 때

1055
00:48:03,600 --> 00:48:05,390
참여하고 수행한

1056
00:48:05,390 --> 00:48:07,640
후속 작업도 있습니다.

1057
00:48:07,640 --> 00:48:11,000
우리는 15개의 서로 다른 3D

1058
00:48:11,000 --> 00:48:15,440
프린트 도구를 장착한 만두 제조 로봇을

1059
00:48:15,440 --> 00:48:16,140
설계했습니다.

1060
00:48:16,140 --> 00:48:19,310
우리는 환경을 관찰하여 매장의 기하학을 재구성하기

1061
00:48:19,310 --> 00:48:22,560
위해 4개의 RGBD 카메라를 사용합니다.

1062
00:48:22,560 --> 00:48:25,610
로봇은 어떤 도구를 사용할지,

1063
00:48:25,610 --> 00:48:30,390
이 반죽을 만두로 만들기 위해 어떤 행동을

1064
00:48:30,390 --> 00:48:32,610
취할지를 결정해야 합니다.

1065
00:48:32,610 --> 00:48:34,140
핵심 요소는

1066
00:48:34,140 --> 00:48:39,460
입자를 사용하여 표현된 이러한 전방 예측 모델입니다.

1067
00:48:39,460 --> 00:48:41,970
여기서 빨간 점은 도구의 형태를

1068
00:48:41,970 --> 00:48:43,890
나타내고, 파란

1069
00:48:43,890 --> 00:48:46,330
점은 물체의 형태를 나타냅니다.

1070
00:48:46,330 --> 00:48:49,600
첫 번째 행은 우리의 모델의 개방 루프

1071
00:48:49,600 --> 00:48:53,770
예측이고, 두 번째 행은 실제 환경에서 발생하는 일입니다.

1072
00:48:53,770 --> 00:48:56,250
이 학습된 모델은 실제 세계의

1073
00:48:56,250 --> 00:48:58,545
상호작용에서 직접 학습하여 다양한

1074
00:48:58,545 --> 00:49:02,100
도구를 사용하고 다양한 행동을 적용할 때 반죽의

1075
00:49:02,100 --> 00:49:04,720
형태 변화에 대해 매우 정확하게

1076
00:49:04,720 --> 00:49:09,030
예측할 수 있으며, 결국 반죽으로 만두를 만들 수 있는

1077
00:49:09,030 --> 00:49:10,962
통합 시스템을 제공합니다.

1078
00:49:10,962 --> 00:49:12,420
이 비디오에서 흥미로운

1079
00:49:12,420 --> 00:49:14,700
점은 누군가가 로봇이 작업을 수행하는

1080
00:49:14,700 --> 00:49:17,130
것을 지속적으로 방해하고 있다는 것입니다.

1081
00:49:17,130 --> 00:49:20,580
로봇은 이 환경에서 실시간 시각적 피드백을

1082
00:49:20,580 --> 00:49:22,650
받아 반죽의 형태를

1083
00:49:22,650 --> 00:49:24,900
실시간으로 이해하고, 현재 관찰과

1084
00:49:24,900 --> 00:49:28,180
환경 변화 예측을 위한 학습된 동역학

1085
00:49:28,180 --> 00:49:30,970
모델을 사용하여 도구를 사용해

1086
00:49:30,970 --> 00:49:32,637
특정 행동을 적용할

1087
00:49:32,637 --> 00:49:35,860
경우 반죽의 형태가 어떻게 변할지를

1088
00:49:35,860 --> 00:49:37,700
예측하며, 이 전방 모델을

1089
00:49:37,700 --> 00:49:40,210
기반으로 역결정을 내립니다.

1090
00:49:40,210 --> 00:49:42,440
이 결정은 두 가지 수준에서

1091
00:49:42,440 --> 00:49:45,385
이루어지며, 높은 수준에서는 어떤 도구를 사용할지

1092
00:49:45,385 --> 00:49:48,620
결정하는 작업 수준의 의사결정이 포함됩니다.

1093
00:49:48,620 --> 00:49:50,890
그리고 이러한 도구를 고려할 때,

1094
00:49:50,890 --> 00:49:53,770
로봇은 다음 작업 단계로 진행하기 위해

1095
00:49:53,770 --> 00:49:57,430
어떤 특정 행동을 취할지를 결정하는 낮은 수준의

1096
00:49:57,430 --> 00:49:59,450
감정적 결정을 내려야 합니다.

1097
00:49:59,450 --> 00:50:01,900
인간은 정말 성가십니다. 조각을 추가하고 반죽을

1098
00:50:01,900 --> 00:50:02,480
접습니다.

1099
00:50:02,480 --> 00:50:05,500
로봇은 작업을 수행하는 데 있어 이러한

1100
00:50:05,500 --> 00:50:08,295
외부 방해에 매우 강인합니다.

1101
00:50:08,295 --> 00:50:09,620
여기서 흥미로운 점은.

1102
00:50:09,620 --> 00:50:13,490
로봇이 원을 그린 후, 인간은 자비 없이 모든

1103
00:50:13,490 --> 00:50:15,130
것을 파괴합니다.

1104
00:50:15,130 --> 00:50:16,600
로봇은 실제로 처음부터

1105
00:50:16,600 --> 00:50:18,460
다시 시작해야 하며, 이러한

1106
00:50:18,460 --> 00:50:20,680
작업 목표를 진행하기 위해 처음부터

1107
00:50:20,680 --> 00:50:23,770
작업을 다시 수행해야 한다는 것을 알고 있습니다.

1108
00:50:23,770 --> 00:50:26,090
이것은 외부 방해에 대한

1109
00:50:26,090 --> 00:50:28,940
우리의 시스템의 인내와 강인함을

1110
00:50:28,940 --> 00:50:30,180
보여줍니다.

1111
00:50:30,180 --> 00:50:32,810
이 모든 능력은 특정 행동을 적용할

1112
00:50:32,810 --> 00:50:34,760
경우 반죽의 형태가 어떻게

1113
00:50:34,760 --> 00:50:36,410
변화할지를 예측하는

1114
00:50:36,410 --> 00:50:39,360
신경 역학 모델에 의해 가능해집니다.

1115
00:50:39,360 --> 00:50:41,900
결국 로봇은 이 만두 클립

1116
00:50:41,900 --> 00:50:45,170
위에 피를 놓고, 이 만두 피 위에

1117
00:50:45,170 --> 00:50:48,020
속을 옮기고, 훅을 사용하여

1118
00:50:48,020 --> 00:50:50,120
만두 클립을 닫아

1119
00:50:50,120 --> 00:50:52,430
15개의 다목적 도구가 장착된

1120
00:50:52,430 --> 00:50:57,860
이 일반 목적 로봇을 사용하여 반죽으로 만두를 만듭니다.

1121
00:50:57,860 --> 00:50:59,810
이것은 우리가 모델을 어떻게

1122
00:50:59,810 --> 00:51:03,440
학습하고 그 모델이 하류 모델 기반 계획에 어떻게 유용할 수

1123
00:51:03,440 --> 00:51:04,760
있는지에 관한 것입니다.

1124
00:51:04,760 --> 00:51:08,630
따라서 이 특정 경우에 대해 더 엄밀하게 설명하고자

1125
00:51:08,630 --> 00:51:10,580
한다면, 우리는 강화

1126
00:51:10,580 --> 00:51:12,710
학습을 사용하지 않습니다.

1127
00:51:12,710 --> 00:51:15,660
우리는 단순히 모델을 학습하고 그 모델을 사용하여

1128
00:51:15,660 --> 00:51:18,560
계획을 세우지만, 그 계획은 실제 환경에서

1129
00:51:18,560 --> 00:51:21,170
보다 효율적으로 실행할 수 있는 정책으로

1130
00:51:21,170 --> 00:51:22,290
정제될 수 있습니다.

1131
00:51:22,290 --> 00:51:25,280
하지만 일부 사람들은 이것을 모델 기반 강화 학습이라고도

1132
00:51:25,280 --> 00:51:25,780
부릅니다.

1133
00:51:25,780 --> 00:51:27,850
어떤 배경에서 오는지 결정하세요.

1134
00:51:27,850 --> 00:51:29,400
모델 학습 및 모델 기반 계획이라고

1135
00:51:29,400 --> 00:51:30,442
부를 수 있습니다.

1136
00:51:30,442 --> 00:51:33,040
모델 기반 강화 학습이라고 부를 수도 있습니다.

1137
00:51:33,040 --> 00:51:36,690
하지만 핵심 아이디어는 로봇이 실제 물리적 세계와의

1138
00:51:36,690 --> 00:51:39,280
물리적 상호작용을 통해 모델을

1139
00:51:39,280 --> 00:51:40,980
학습하고, 그 학습된

1140
00:51:40,980 --> 00:51:42,870
모델이 로봇이 작업 목표를

1141
00:51:42,870 --> 00:51:45,960
진행하기 위한 행동을 결정하는 데 매우

1142
00:51:45,960 --> 00:51:47,580
효과적이라는 것입니다.

1143
00:51:47,580 --> 00:51:49,920
따라서 이 특정 경우에 높은 수준의 계획과 낮은

1144
00:51:49,920 --> 00:51:52,870
수준의 의사결정은 두 개의 서로 다른 모델에 의해 수행됩니다.

1145
00:51:52,870 --> 00:51:56,310
높은 수준에서는 현재 상태, 환경의 현재 관찰

1146
00:51:56,310 --> 00:51:58,810
및 로봇이 달성할 수 있는 목표가

1147
00:51:58,810 --> 00:52:00,810
주어지며, 본질적으로

1148
00:52:00,810 --> 00:52:04,050
어떤 도구를 사용할지를 분류하는 분류기입니다.

1149
00:52:04,050 --> 00:52:06,810
그리고 이 분류기, 도구 레이블에 조건부로,

1150
00:52:06,810 --> 00:52:09,300
이는 다음 작업 단계로 진행하기

1151
00:52:09,300 --> 00:52:11,820
위해 어떤 특정 행동을 취할지를

1152
00:52:11,820 --> 00:52:14,160
결정하는 낮은 수준의 정책입니다.

1153
00:52:14,160 --> 00:52:15,010
아주 좋은 질문입니다.

1154
00:52:15,010 --> 00:52:17,350
그 당시 이 작업은 2023년에 수행되었습니다.

1155
00:52:17,350 --> 00:52:21,130
그때 시각 언어 모델은 그리 강력하지 않았습니다.

1156
00:52:21,130 --> 00:52:25,750
그래서 그때 우리가 한 것은 인간 운영자가 10번

1157
00:52:25,750 --> 00:52:28,360
작업을 데이터 수집 및 시연하도록

1158
00:52:28,360 --> 00:52:30,530
허용하는 것이었습니다.

1159
00:52:30,530 --> 00:52:33,190
우리는 그 데이터를 사용하여 어떤 도구를 사용할지를

1160
00:52:33,190 --> 00:52:35,090
분류하는 분류기를 학습했습니다.

1161
00:52:35,090 --> 00:52:38,390
그것은 우리가 실제로 이 체인에서 앞뒤로 이동할 수 있게 해줍니다.

1162
00:52:38,390 --> 00:52:41,330
앞서 언급했듯이, 로봇이 원을 자른 후 인간이

1163
00:52:41,330 --> 00:52:42,650
모든 것을 파괴합니다.

1164
00:52:42,650 --> 00:52:45,460
로봇은 외부 방해로부터 적절한 복구를

1165
00:52:45,460 --> 00:52:47,380
수행하기 위해 현재

1166
00:52:47,380 --> 00:52:52,360
관찰에 맞는 이전 단계로 되돌아가야 한다는 것을 알고 있습니다.

1167
00:52:52,360 --> 00:52:54,670
따라서 이 특정 경우에 우리가

1168
00:52:54,670 --> 00:52:57,400
하고 있는 것은 샘플링 기반 궤적

1169
00:52:57,400 --> 00:53:00,178
최적화와 정책 학습의 조합입니다.

1170
00:53:00,178 --> 00:53:02,470
우리가 하고 있는 것은 이 반죽의 현재 상태를

1171
00:53:02,470 --> 00:53:03,470
제공하는 것입니다.

1172
00:53:03,470 --> 00:53:05,662
우리는 우리의 전방 예측 모델을 가지고 있습니다.

1173
00:53:05,662 --> 00:53:07,870
우리는 여러 가지 행동을 샘플링하고

1174
00:53:07,870 --> 00:53:11,620
여러 가지 도구를 샘플링하여 그 반죽의 형태가 어떻게

1175
00:53:11,620 --> 00:53:13,765
진화할지를 예측할 수 있습니다.

1176
00:53:13,765 --> 00:53:15,640
그리고 나서 우리는 모델의 예측을

1177
00:53:15,640 --> 00:53:18,080
우리가 달성하고자 하는 목표와 비교할 것입니다.

1178
00:53:18,080 --> 00:53:20,170
이는 제가 이전에 보여준 것과 유사합니다.

1179
00:53:20,170 --> 00:53:23,010
예를 들어, 우리 모델은 반죽의 형태가 이

1180
00:53:23,010 --> 00:53:25,920
초록 점으로 갈 것이라고 예측하지만, 목표는

1181
00:53:25,920 --> 00:53:28,840
이 빨간 점이며, 그 거리를 비교합니다.

1182
00:53:28,840 --> 00:53:30,780
그리고 이는 우리가 목표에

1183
00:53:30,780 --> 00:53:34,080
최대한 가깝게 도달할 수 있는 가장 효과적인 행동을

1184
00:53:34,080 --> 00:53:35,830
선택할 수 있게 해줍니다.

1185
00:53:35,830 --> 00:53:37,930
우리는 이렇게 많은 샘플을 할 수 있습니다.

1186
00:53:37,930 --> 00:53:41,350
하지만 테스트 시간 동안 샘플링하는 것은 매우 시간이 많이 소요됩니다.

1187
00:53:41,350 --> 00:53:44,340
그래서 우리는 오프라인 방식으로 이러한 샘플링을 수행하여 데이터

1188
00:53:44,340 --> 00:53:45,400
세트를 제공합니다.

1189
00:53:45,400 --> 00:53:48,330
그리고 우리는 그 데이터 세트를 사용하여

1190
00:53:48,330 --> 00:53:50,945
테스트 시간 동안 매우 짧은 시간에

1191
00:53:50,945 --> 00:53:52,320
추론할 수 있는

1192
00:53:52,320 --> 00:53:54,360
정책을 훈련할 수 있습니다.

1193
00:53:54,360 --> 00:53:56,470
정책으로서 여전히 신경망이 있습니다.

1194
00:53:56,470 --> 00:53:56,970
네.

1195
00:53:56,970 --> 00:54:01,200
정책은 방대한 샘플에 대한 모델의

1196
00:54:01,200 --> 00:54:04,320
예측을 증류하여 학습됩니다.

1197
00:54:04,320 --> 00:54:07,920
이 특정 작업에서는 물리 기반 시뮬레이션이 전혀

1198
00:54:07,920 --> 00:54:08,680
없습니다.

1199
00:54:08,680 --> 00:54:11,280
우리는 실제로 MPM(Material Point

1200
00:54:11,280 --> 00:54:13,770
Methods)라는 최첨단 변형 물체 시뮬레이터를

1201
00:54:13,770 --> 00:54:15,250
사용하는 기준선을 가지고 있습니다.

1202
00:54:15,250 --> 00:54:19,143
우리가 깨달은 것은 매우 광범위한 시스템 식별을 수행하더라도,

1203
00:54:19,143 --> 00:54:21,060
즉 물리 기반 변형 물체

1204
00:54:21,060 --> 00:54:24,220
시뮬레이터의 매개변수를 추정하더라도, 식별된

1205
00:54:24,220 --> 00:54:27,360
모델이 실제 세계 상호작용에서 직접 학습한

1206
00:54:27,360 --> 00:54:28,860
모델보다 눈에 띄게 정확도가

1207
00:54:28,860 --> 00:54:30,340
떨어진다는 것입니다.

1208
00:54:30,340 --> 00:54:31,870
제가 이전에 보여준 것처럼, 예를

1209
00:54:31,870 --> 00:54:34,120
들어 첫 번째 행은 우리 모델의 개방 루프 예측입니다.

1210
00:54:34,120 --> 00:54:35,830
두 번째 행은 실제 값입니다.

1211
00:54:35,830 --> 00:54:37,770
우리 모델의 예측은 실제

1212
00:54:37,770 --> 00:54:39,270
값과 매우 잘 일치하며,

1213
00:54:39,270 --> 00:54:42,840
이는 어떤 물리 기반 시뮬레이터보다 훨씬 더

1214
00:54:42,840 --> 00:54:43,370
정확합니다.

1215
00:54:46,100 --> 00:54:46,600
좋습니다.

1216
00:54:46,600 --> 00:54:49,860
더 이상 질문이 없다면 계속하겠습니다.

1217
00:54:49,860 --> 00:54:52,320
우리가 이 모델 학습에서 논의한 내용과 이

1218
00:54:52,320 --> 00:54:54,090
학습된 모델이 하류 모델 기반 계획에

1219
00:54:54,090 --> 00:54:56,955
어떻게 효과적일 수 있는지에 대해 이야기하겠습니다.

1220
00:54:56,955 --> 00:55:01,235
다음 알고리즘 범주는 모방 학습입니다.

1221
00:55:01,235 --> 00:55:04,680
간단히 요약하자면, 우리는 강화 학습에

1222
00:55:04,680 --> 00:55:06,420
대해 논의했습니다.

1223
00:55:06,420 --> 00:55:08,190
이는 환경과의 시행착오를

1224
00:55:08,190 --> 00:55:10,793
통해 정책을 직접 학습하는 것으로, 샘플

1225
00:55:10,793 --> 00:55:13,210
효율성, 안전 문제 등 많은 문제를

1226
00:55:13,210 --> 00:55:14,385
가지고 있습니다.

1227
00:55:14,385 --> 00:55:16,510
모델 학습의 경우, 우리가

1228
00:55:16,510 --> 00:55:19,660
하고 있는 것은 실제로 환경의 진화를

1229
00:55:19,660 --> 00:55:22,510
가진 감독 학습 범주로 강제로 돌아가는

1230
00:55:22,510 --> 00:55:23,470
것입니다.

1231
00:55:23,470 --> 00:55:25,750
그 데이터를 사용하여 감독 학습을 수행하여

1232
00:55:25,750 --> 00:55:27,850
이 모델을 훈련하고 이 모델을

1233
00:55:27,850 --> 00:55:29,830
사용하여 모델 기반 계획을 수행합니다.

1234
00:55:29,830 --> 00:55:32,230
그리고 단순히 감독 학습을 사용하여

1235
00:55:32,230 --> 00:55:35,390
모델을 훈련하는 대신, 사람들은 정책에 대해서도

1236
00:55:35,390 --> 00:55:38,270
감독 학습을 할 수 있는지 묻고 있습니다.

1237
00:55:38,270 --> 00:55:41,140
이것이 모방 학습의 일반적인 아이디어로,

1238
00:55:41,140 --> 00:55:46,570
작업이 어떻게 수행되어야 하는지를 보여주는 대규모 데이터 세트를

1239
00:55:46,570 --> 00:55:49,270
가지고 이 정책을 훈련할 수

1240
00:55:49,270 --> 00:55:50,900
있는지를 의미합니다.

1241
00:55:50,900 --> 00:55:52,520
이 그림을 다시 보여주고 있습니다.

1242
00:55:52,520 --> 00:55:54,530
이는 상태를 입력으로 받아

1243
00:55:54,530 --> 00:55:57,680
행동을 예측하는 정책을 학습하려고 합니다.

1244
00:55:57,680 --> 00:56:00,820
이 모든 학습 신호와 학습 절차는

1245
00:56:00,820 --> 00:56:03,770
인간이 작업을 수행하는 방법을

1246
00:56:03,770 --> 00:56:09,010
로봇에게 시연하는 대규모 수집 데이터로 수행됩니다.

1247
00:56:09,010 --> 00:56:11,960
따라서 시연을 통한 학습은 물론 새로운 것이 아닙니다.

1248
00:56:11,960 --> 00:56:14,010
수십 년 동안 조사되어 왔습니다.

1249
00:56:14,010 --> 00:56:16,130
우리는 매우 어릴 때부터

1250
00:56:16,130 --> 00:56:19,310
실제 세계에서 많은 신체적

1251
00:56:19,310 --> 00:56:22,450
상호작용이나 사회적 활동을 수행하는

1252
00:56:22,450 --> 00:56:24,650
방법을 배우고 있습니다.

1253
00:56:24,650 --> 00:56:28,760
가장 초기의 고전적인 모방 학습 알고리즘

1254
00:56:28,760 --> 00:56:32,750
중 하나는 행동 복제라고 불리며,

1255
00:56:32,750 --> 00:56:35,870
현재 관찰 o에서 행동 a로

1256
00:56:35,870 --> 00:56:39,390
매핑하는 것을 배우려고 합니다.

1257
00:56:39,390 --> 00:56:42,410
이 정책은 세타로 매개변수화된

1258
00:56:42,410 --> 00:56:45,530
함수 pi를 사용하여 표현됩니다.

1259
00:56:45,530 --> 00:56:48,410
행동 복제의 주요 문제 중 하나는

1260
00:56:48,410 --> 00:56:51,990
연쇄 오류라고 불리며, 로봇 학습이나

1261
00:56:51,990 --> 00:56:54,320
에이전트의 환경과의 상호작용의

1262
00:56:54,320 --> 00:56:56,300
주요 차이점은 이것이

1263
00:56:56,300 --> 00:56:59,610
순차적 의사결정 문제라는 것입니다.

1264
00:56:59,610 --> 00:57:02,298
이는 일반적인 컴퓨터 비전 도메인에서의

1265
00:57:02,298 --> 00:57:03,840
감독 학습과 다르며,

1266
00:57:03,840 --> 00:57:06,320
오류가 시간이 지남에 따라

1267
00:57:06,320 --> 00:57:08,485
누적되고 증폭될 수 있습니다.

1268
00:57:08,485 --> 00:57:09,860
예를 들어, 처음에 매우

1269
00:57:09,860 --> 00:57:11,730
작은 오류를 범했다고 가정해 보겠습니다.

1270
00:57:11,730 --> 00:57:15,240
그 작은 오류는 모델을 훈련하는 데 사용된 데이터

1271
00:57:15,240 --> 00:57:17,490
분포에서 약간 벗어난 상태로

1272
00:57:17,490 --> 00:57:19,230
이어질 수 있습니다.

1273
00:57:19,230 --> 00:57:21,700
그것은 정책이 더 큰 오류를 범하게 할 것입니다.

1274
00:57:21,700 --> 00:57:26,350
그리고 이 오류는 시간의 경과에 따라 증폭됩니다.

1275
00:57:26,350 --> 00:57:27,930
그로 인해 시연 궤적과

1276
00:57:27,930 --> 00:57:31,270
많이 벗어난 궤적이 발생할 수 있습니다.

1277
00:57:31,270 --> 00:57:34,680
그래서 이것이 행동 복제의 전형적인 문제입니다.

1278
00:57:34,680 --> 00:57:37,080
그래서 사람들이 모방 학습을

1279
00:57:37,080 --> 00:57:39,370
작동시키려고 할 때, 종종

1280
00:57:39,370 --> 00:57:42,580
전문가가 수집한 시연이 맨 위에 있는

1281
00:57:42,580 --> 00:57:45,850
이러한 유형의 파이프라인을 따릅니다.

1282
00:57:45,850 --> 00:57:49,470
그런 다음 우리는 그것을 훈련 데이터로 사용하여 감독 학습을

1283
00:57:49,470 --> 00:57:51,210
통해 이 정책을 훈련합니다.

1284
00:57:51,210 --> 00:57:53,910
그리고 우리는 실제 환경에서 정책을

1285
00:57:53,910 --> 00:57:56,555
실행하고 실패 사례를 관찰합니다.

1286
00:57:56,555 --> 00:57:59,070
그리고 우리는 추가 데이터를

1287
00:57:59,070 --> 00:58:01,560
수집하거나 오류를

1288
00:58:01,560 --> 00:58:03,990
수정하는 행동을 제공하여

1289
00:58:03,990 --> 00:58:07,380
이러한 데이터 세트가 초기 시연뿐만

1290
00:58:07,380 --> 00:58:11,170
아니라 오류를 표준 궤도로 되돌리는

1291
00:58:11,170 --> 00:58:13,970
수정 행동도 포함하도록 합니다.

1292
00:58:13,970 --> 00:58:16,520
그들은 여전히 성공적으로 작업을 수행할 수 있습니다.

1293
00:58:16,520 --> 00:58:18,910
그래서 이것은 실제 물리적 세계에서 모방 학습 에이전트나 알고리즘을
개발하려는 전형적인 생애 주기입니다.

1294
00:58:18,910 --> 00:58:21,520
그리고 이러한 맥락에서 사람들이 이러한

1295
00:58:21,520 --> 00:58:23,830
종류의 모방 학습을 수행하면 실제로

1296
00:58:23,830 --> 00:58:26,665
작업이 무엇인지에 대한 명확한 정의가 없습니다.

1297
00:58:26,665 --> 00:58:29,920
작업은 이러한

1298
00:58:29,920 --> 00:58:32,030
시연 내에

1299
00:58:32,030 --> 00:58:35,110
암묵적으로 숨겨져

1300
00:58:35,110 --> 00:58:37,400
있습니다.

1301
00:58:37,400 --> 00:58:41,750
그래서 역 강화 학습이라는 알고리즘 클래스가 있으며, 왼쪽은 사람들이
일반적으로 강화 학습에 대해 생각하는 것이고, 오른쪽은 사람들이 역 강화
학습을 사용하여 시연에서 보상을 요약하고 그 보상을 사용하여 일반적인 강화
학습을 수행하여 이러한 알고리즘을 배우려고 하는 것입니다.

1302
00:58:41,750 --> 00:58:44,365
초기 성공

1303
00:58:44,365 --> 00:58:47,020
사례 중 일부는

1304
00:58:47,020 --> 00:58:49,780
실제로 스탠포드에서

1305
00:58:49,780 --> 00:58:52,390
Pieter

1306
00:58:52,390 --> 00:58:54,160
Ab

1307
00:58:54,160 --> 00:58:58,270
beel과 Andrew

1308
00:58:58,270 --> 00:59:00,755
Ng에 의해

1309
00:59:00,755 --> 00:59:02,380
개발되

1310
00:59:02,380 --> 00:59:04,020
었습니다.

1311
00:59:04,020 --> 00:59:06,880
그들은 헬리콥터를 제어하여

1312
00:59:06,880 --> 00:59:08,860
매우 미친 행동을

1313
00:59:08,860 --> 00:59:11,930
수행할 수 있게 되었습니다.

1314
00:59:11,930 --> 00:59:16,400
그리고 이것은 실제로 매우 오래된

1315
00:59:16,400 --> 00:59:17,720
작업입니다.

1316
00:59:17,720 --> 00:59:19,810
이러한 실제 물리적 헬리콥터에서 이러한 민첩하고 효과적인 행동을 달성할 수
있는 것은 그 당시 매우 인상적이었습니다.

1317
00:59:19,810 --> 00:59:25,660
그래서 이것은 시연에서 배우고 그 시연을

1318
00:59:25,660 --> 00:59:27,550
사용하여 보상을

1319
00:59:27,550 --> 00:59:30,050
요약하는 힘입니다.

1320
00:59:30,050 --> 00:59:32,830
강화 학습과의 연결에서, 이것이 우리가

1321
00:59:32,830 --> 00:59:35,120
달성할 수 있는 것입니다.

1322
00:59:35,120 --> 00:59:37,130
분명히, 수년 동안 사람들은 모방 학습 알고리즘을 점점 더

1323
00:59:37,130 --> 00:59:39,805
효과적으로 만들고 있으며, 특히 에너지 기반 모델과 연결하는 데 있어
그렇습니다.

1324
00:59:39,805 --> 00:59:42,550
왼쪽에 표시된 명시적

1325
00:59:42,550 --> 00:59:45,430
정책을 배우는 대신, 그들은

1326
00:59:45,430 --> 00:59:47,560
관찰에서 행동으로

1327
00:59:47,560 --> 00:59:50,465
직접 매핑을 수행합니다.

1328
00:59:50,465 --> 00:59:52,840
이러한 암묵적 정책을 생각해내면, 에너지 기반 모델에서 아이디어를 가져와

1329
00:59:52,840 --> 00:59:55,660
관찰과 행동을 직접 사용하여 점수를 예측하고 이러한 에너지 기반 모델을
사용하여 추론하여

1330
00:59:55,660 --> 00:59:56,960
예측된 행동 a를 얻을 수 있습니다.

1331
00:59:56,960 --> 00:59:59,580
이것은 로봇이

1332
00:59:59,580 --> 01:00:01,840
매우 다중

1333
01:00:01,840 --> 01:00:04,000
모드의

1334
01:00:04,000 --> 01:00:09,230
시연을 처리하거나 최적화

1335
01:00:09,230 --> 01:00:12,710
경관이 매우 부드럽지

1336
01:00:12,710 --> 01:00:17,150
않을 수 있는 시나리오를

1337
01:00:17,150 --> 01:00:19,490
처리할

1338
01:00:19,490 --> 01:00:21,530
수 있게

1339
01:00:21,530 --> 01:00:23,010
합니다.

1340
01:00:23,010 --> 01:00:26,300
로봇은 이러한 전략을 생각해내고

1341
01:00:26,300 --> 01:00:28,740
시연에서 이러한 정책을 증류하여

1342
01:00:28,740 --> 01:00:32,630
콘텐츠가 풍부한 조작 작업을 수행합니다.

1343
01:00:32,630 --> 01:00:34,672
최근 로봇 학습의

1344
01:00:34,672 --> 01:00:37,730
성공 중 일부는

1345
01:00:37,730 --> 01:00:42,230
확산 정책이라는 작업의 결과이며,

1346
01:00:42,230 --> 01:00:45,470
이는 다시 생성 모델

1347
01:00:45,470 --> 01:00:48,470
커뮤니티의 발전을

1348
01:00:48,470 --> 01:00:49,670
반영합니다.

1349
01:00:49,670 --> 01:00:52,290
이 경우, 암묵적 행동 복제를 위해 사람들은

1350
01:00:52,290 --> 01:00:54,290
에너지 기반 모델의 개발에서 영감을

1351
01:00:54,290 --> 01:00:55,320
얻고 있습니다.

1352
01:00:55,320 --> 01:00:58,010
에너지 기반 모델은 딥 러닝 커뮤니티에서

1353
01:00:58,010 --> 01:01:00,310
개발된 생성 모델의 일종입니다.

1354
01:01:00,310 --> 01:01:03,650
딥 러닝 커뮤니티에는 확산 모델이라고 불리는

1355
01:01:03,650 --> 01:01:07,030
더 강력한 모델의 또 다른 클래스가 있습니다.

1356
01:01:07,030 --> 01:01:10,710
사람들은 또한 확산 모델을 정책 함수

1357
01:01:10,710 --> 01:01:17,130
클래스로 사용하여 에이전트가 이러한 확산 모델의 이점과 특성을

1358
01:01:17,130 --> 01:01:20,640
상속받도록 하려고 하고 있습니다.

1359
01:01:20,640 --> 01:01:24,340
이 작업은 원래 컬럼비아에서 수행되었습니다.

1360
01:01:24,340 --> 01:01:27,100
제가 지금 있는 곳입니다.

1361
01:01:27,100 --> 01:01:31,500
이 작업의 주 연구자는 현재 스탠포드로 왔습니다.

1362
01:01:31,500 --> 01:01:35,550
제가 선택한 많은 작업이 스탠포드에서 뿌리를 두고 있다는

1363
01:01:35,550 --> 01:01:37,080
것을 알 수 있습니다.

1364
01:01:37,080 --> 01:01:41,170
그녀는 현재 스탠포드의 W 부서에 있습니다.

1365
01:01:41,170 --> 01:01:44,910
이 정책은 로봇이 단순한 계획 추진뿐만

1366
01:01:44,910 --> 01:01:47,280
아니라 많은 세밀한 조작

1367
01:01:47,280 --> 01:01:51,630
작업을 수행할 수 있도록 매우 다양한 능력을

1368
01:01:51,630 --> 01:01:52,660
보여줍니다.

1369
01:01:52,660 --> 01:01:55,000
단순한 집기와 놓기뿐만

1370
01:01:55,000 --> 01:01:57,000
아니라, 예를 들어 여기서

1371
01:01:57,000 --> 01:02:02,770
버터를 펴고 스크램블 에그를 만들고 감자를 껍질을 벗기고 책을

1372
01:02:02,770 --> 01:02:04,940
미는 작업도 포함됩니다.

1373
01:02:04,940 --> 01:02:06,580
따라서 여러 가지 시연을

1374
01:02:06,580 --> 01:02:09,730
수집하고 최상의 정책과 학습 메커니즘을

1375
01:02:09,730 --> 01:02:13,670
사용하여 실제 물리적 세계에서 매우 효율적으로

1376
01:02:13,670 --> 01:02:17,350
작동하는 정책을 얻을 수 있는 이러한 유형의 레시피가

1377
01:02:17,350 --> 01:02:20,377
정말로 보여줍니다. 즉, 아침에

1378
01:02:20,377 --> 01:02:21,710
데이터를 수집합니다.

1379
01:02:21,710 --> 01:02:23,270
정책을 미세하게 훈련합니다.

1380
01:02:23,270 --> 01:02:26,650
오후에는 실제 물리적 세계에서 작동하는 정책을

1381
01:02:26,650 --> 01:02:28,160
가질 수 있습니다.

1382
01:02:28,160 --> 01:02:30,310
따라서 정책의 신뢰성과

1383
01:02:30,310 --> 01:02:33,340
일반화 가능성, 정책이 여전히

1384
01:02:33,340 --> 01:02:37,690
매우 강력하게 작동할 수 있는 초기 구성의

1385
01:02:37,690 --> 01:02:40,610
다양성에 많은 주의사항이 있습니다.

1386
01:02:40,610 --> 01:02:44,560
하지만 여전히 모방 학습은 실제 물리적 세계에서 흥미로운

1387
01:02:44,560 --> 01:02:46,600
작업을 수행할 수 있는

1388
01:02:46,600 --> 01:02:49,310
정책을 얻는 가장 효율적인 방법입니다.

1389
01:02:49,310 --> 01:02:52,885
정책이 실제 세계의 변동성에 매우 효과적이고

1390
01:02:52,885 --> 01:02:55,150
강력하기 위해서는

1391
01:02:55,150 --> 01:02:58,000
이러한 유형의 반복 데이터 수집이

1392
01:02:58,000 --> 01:02:59,980
필요하며, 정책이

1393
01:02:59,980 --> 01:03:03,500
예상치 못한 행동이나 어떤 종류의 이탈

1394
01:03:03,500 --> 01:03:06,530
행동을 포괄할 수 있어야 합니다.

1395
01:03:06,530 --> 01:03:08,430
그래서 이것은 모방 학습에 관한 것입니다.

1396
01:03:08,430 --> 01:03:09,870
질문이 있습니까?

1397
01:03:13,040 --> 01:03:15,120
좋습니다, 더 이상 질문이

1398
01:03:15,120 --> 01:03:17,660
없다면 남은 시간을

1399
01:03:17,660 --> 01:03:20,510
사용하여 로봇 학습에 대한 모든

1400
01:03:20,510 --> 01:03:23,480
열광을 이끄는 최근 개발 사항인

1401
01:03:23,480 --> 01:03:26,695
로봇 기초 모델에 대해 논의하겠습니다.

1402
01:03:26,695 --> 01:03:30,120
물론, 이것은 매우 복잡한 분야입니다.

1403
01:03:30,120 --> 01:03:32,240
사실, 이러한 항목 각각에

1404
01:03:32,240 --> 01:03:35,190
대해 강의를 진행할 수 있습니다.

1405
01:03:35,190 --> 01:03:38,580
그래서 오늘 강의에서는 매우 빠르게

1406
01:03:38,580 --> 01:03:39,450
훑어보겠습니다.

1407
01:03:39,450 --> 01:03:42,140
저는 여러분이 이러한 용어를 보면서 알아야

1408
01:03:42,140 --> 01:03:45,380
할 핵심적인 내용, 고급 지식만을 말씀드리겠습니다.

1409
01:03:45,380 --> 01:03:47,855
로봇 기초 모델은

1410
01:03:47,855 --> 01:03:53,690
기능 클래스에서 강화 학습이나 모방 학습과

1411
01:03:53,690 --> 01:03:57,890
매우 유사한 모델 유형입니다.

1412
01:03:57,890 --> 01:03:59,970
이러한 모델의 상태에 대한

1413
01:03:59,970 --> 01:04:01,670
명시적인 표현은 없습니다.

1414
01:04:01,670 --> 01:04:03,420
예를 들어, 이 로봇 기초

1415
01:04:03,420 --> 01:04:06,250
모델은 환경의 모델을 학습하지 않습니다.

1416
01:04:06,250 --> 01:04:09,690
여전히 관찰에서 행동으로

1417
01:04:09,690 --> 01:04:11,880
매핑하는 정책입니다.

1418
01:04:11,880 --> 01:04:13,380
이것은 여전히 대표적이며,

1419
01:04:13,380 --> 01:04:16,265
이러한 도형을 사용하여 매우 잘 표현될 수 있습니다.

1420
01:04:16,265 --> 01:04:17,640
현재 상태와 목표를

1421
01:04:17,640 --> 01:04:19,530
입력으로 받아들이는 정책인 이

1422
01:04:19,530 --> 01:04:21,060
에이전트가 있습니다.

1423
01:04:21,060 --> 01:04:23,340
그리고 여러분은 실제 물리적 세계에서

1424
01:04:23,340 --> 01:04:26,592
실행할 수 있는 이러한 행동을 생성하려고 합니다.

1425
01:04:26,592 --> 01:04:28,050
하지만 이것이 모방 학습 및

1426
01:04:28,050 --> 01:04:30,640
강화 학습과 매우 유사하다고 말할 수 있습니다.

1427
01:04:30,640 --> 01:04:34,300
이 로봇 기초 모델의 특별한 점은 무엇인가요?

1428
01:04:34,300 --> 01:04:36,270
이는 사실 기초 모델

1429
01:04:36,270 --> 01:04:39,700
분야 내의 모든 발전에 뿌리를 두고 있으며,

1430
01:04:39,700 --> 01:04:42,120
특히 언어 관련 기초

1431
01:04:42,120 --> 01:04:45,150
모델과 비전 언어 관련 기초 모델을

1432
01:04:45,150 --> 01:04:47,980
의미합니다. 이는 정책입니다.

1433
01:04:47,980 --> 01:04:51,660
하지만 이는 특정 작업에만 적용되는 정책보다

1434
01:04:51,660 --> 01:04:54,240
훨씬 더 잘 일반화되어야 합니다.

1435
01:04:54,240 --> 01:04:55,930
여기 제 정의가 있습니다.

1436
01:04:55,930 --> 01:04:59,350
저는 비전 언어 모델의 현재 발전에서 비유를

1437
01:04:59,350 --> 01:05:03,370
끌어오고 있습니다. 이는 그들의 출력이 항상 완벽하지는

1438
01:05:03,370 --> 01:05:06,100
않지만, 이 기초 모델을 통해

1439
01:05:06,100 --> 01:05:08,630
합리적인 무언가를 항상 생성할

1440
01:05:08,630 --> 01:05:10,150
것이라는 의미입니다.

1441
01:05:10,150 --> 01:05:13,180
우리가 이 로봇 기초 모델로 달성하고자 하는

1442
01:05:13,180 --> 01:05:16,000
것은, 합성된 행동이 관찰과 작업에

1443
01:05:16,000 --> 01:05:18,400
의해 조건지어진 최적의 행동이 아닐

1444
01:05:18,400 --> 01:05:19,940
수 있다는 것입니다.

1445
01:05:19,940 --> 01:05:21,790
하지만 일반적인 궤적은 항상

1446
01:05:21,790 --> 01:05:24,130
아름답고 실제 물리적 세계에서

1447
01:05:24,130 --> 01:05:26,170
실행하기에 합리적일 것입니다.

1448
01:05:26,170 --> 01:05:28,790
아름답다는 것은 흔들리는 동작을 사용하지 말라는 의미입니다.

1449
01:05:28,790 --> 01:05:30,650
부드럽고 연속적이어야 합니다.

1450
01:05:30,650 --> 01:05:32,890
합리적이라는 것은 로봇에게 주어진

1451
01:05:32,890 --> 01:05:35,595
언어 지침을 들어야 한다는 의미입니다.

1452
01:05:35,595 --> 01:05:39,280
따라서 분명히 같은 것을 설명하는 다양한

1453
01:05:39,280 --> 01:05:40,700
이름이 존재합니다.

1454
01:05:40,700 --> 01:05:42,820
어떤 사람들은 이를 비전 언어 행동

1455
01:05:42,820 --> 01:05:44,150
모델(VLAs)이라고 부릅니다.

1456
01:05:44,150 --> 01:05:46,880
어떤 사람들은 이를 대규모 행동 모델이라고 부릅니다.

1457
01:05:46,880 --> 01:05:49,970
하지만 본질적으로 그들은 모두 같은 것을 설명하고

1458
01:05:49,970 --> 01:05:52,810
있으며, 이는 관찰과 언어 지침 또는 작업

1459
01:05:52,810 --> 01:05:55,100
사양을 받아들이는 정책을 의미합니다.

1460
01:05:55,100 --> 01:05:57,950
당신은 다양한 시나리오에 걸쳐

1461
01:05:57,950 --> 01:06:01,420
널리 일반화되는 행동을 생성하려고 합니다.

1462
01:06:01,420 --> 01:06:04,650
이 분야는 사실 꽤 시끄럽습니다.

1463
01:06:04,650 --> 01:06:07,100
시끄럽다는 것은 다양한 종류의

1464
01:06:07,100 --> 01:06:10,610
로봇 기초 모델의 진행 상황을 정량화하기 매우

1465
01:06:10,610 --> 01:06:12,308
어렵다는 의미입니다.

1466
01:06:12,308 --> 01:06:14,100
당신이 이를 기초 모델이라고 부르고 있다면, 그것은

1467
01:06:14,100 --> 01:06:14,933
무엇을 의미하나요?

1468
01:06:14,933 --> 01:06:18,080
그것은 이 모델이 다양한 시나리오에 걸쳐 매우 널리 일반화되기를

1469
01:06:18,080 --> 01:06:19,740
기대한다는 것을 의미합니다.

1470
01:06:19,740 --> 01:06:21,950
그것이 당신의 기대라면, 실제로 널리

1471
01:06:21,950 --> 01:06:23,900
일반화된다는 것을 보여줄 상당한

1472
01:06:23,900 --> 01:06:24,930
증거가 필요합니다.

1473
01:06:24,930 --> 01:06:27,950
그래서 그들의 진행 상황에 대한 평가와 정량적

1474
01:06:27,950 --> 01:06:30,000
측정이 매우 도전적입니다.

1475
01:06:30,000 --> 01:06:33,390
하지만 여전히 그들의 경험적 비디오를 보면

1476
01:06:33,390 --> 01:06:36,560
지난 2년 동안 매우 흥미롭고 구체적인

1477
01:06:36,560 --> 01:06:39,240
진행 상황을 많이 볼 수 있습니다.

1478
01:06:39,240 --> 01:06:41,180
많은 초기 조사는

1479
01:06:41,180 --> 01:06:45,230
2022년 12월에 출시된

1480
01:06:45,230 --> 01:06:47,545
RT-1에서 시작됩니다.

1481
01:06:47,545 --> 01:06:51,810
그 이후로, 대략 매 반년마다 새로운 모델이 등장한다고 할

1482
01:06:51,810 --> 01:06:55,900
수 있습니다. 예를 들어 RT-2, RT-X,

1483
01:06:55,900 --> 01:06:58,900
OpenVLA, 최근의 Pi-Zero와 같은

1484
01:06:58,900 --> 01:07:02,970
모델들이 더 일반화 가능한 로봇 기초 모델을 개발하는

1485
01:07:02,970 --> 01:07:06,070
방향으로 구체적인 진전을 이루고 있습니다.

1486
01:07:06,070 --> 01:07:09,520
실제로 올해는 큰 발전이 있었습니다.

1487
01:07:09,520 --> 01:07:13,690
많은 기초 모델의 첫 번째 부스트가 있었으며, Helix, Hi-Robot,

1488
01:07:13,690 --> 01:07:17,500
Gemini Robotics, Pi-0.5 등이 포함됩니다.

1489
01:07:17,500 --> 01:07:19,140
이 분야에는 자본

1490
01:07:19,140 --> 01:07:21,330
투자뿐만 아니라 더

1491
01:07:21,330 --> 01:07:24,430
나은, 더 나은, 더 일반화 가능한

1492
01:07:24,430 --> 01:07:27,990
로봇 기초 모델을 개발하기 위한

1493
01:07:27,990 --> 01:07:29,580
인재 투자도 많이

1494
01:07:29,580 --> 01:07:31,625
이루어지고 있습니다.

1495
01:07:31,625 --> 01:07:34,980
시간 관계상 이 모든 모델의 세부 사항에 대해 자세히

1496
01:07:34,980 --> 01:07:36,120
설명할 수는 없습니다.

1497
01:07:36,120 --> 01:07:37,620
그래서 관심이 있으시다면,

1498
01:07:37,620 --> 01:07:40,050
두 달 전에 [INAUDIBLE]에서 이

1499
01:07:40,050 --> 01:07:42,570
축을 따라 몇 가지 모델을 구체적으로 설명하고

1500
01:07:42,570 --> 01:07:44,470
논의하는 튜토리얼을 진행했습니다.

1501
01:07:44,470 --> 01:07:47,320
관심이 있으시면 꼭 시청해 주세요.

1502
01:07:47,320 --> 01:07:51,670
오늘은 이러한 기초 모델에 실제로 필수적인 것이

1503
01:07:51,670 --> 01:07:55,420
무엇인지, 그리고 Pi-Zero의 예를 통해

1504
01:07:55,420 --> 01:07:58,810
그것이 실제로 어떻게 생겼는지에 대한 고급

1505
01:07:58,810 --> 01:08:01,330
개요를 주로 제공할 것입니다.

1506
01:08:01,330 --> 01:08:06,670
Pi-Zero는 2024년 10월에 처음 출시되었습니다.

1507
01:08:06,670 --> 01:08:09,220
이 단어가 이 유형의 로봇 기초 모델이

1508
01:08:09,220 --> 01:08:11,080
실제 환경에서 매우 신뢰할

1509
01:08:11,080 --> 01:08:14,860
수 있는 섬세한 조작을 할 수 있다는 것을 저를

1510
01:08:14,860 --> 01:08:16,399
설득시켰다고 생각합니다.

1511
01:08:16,399 --> 01:08:18,729
이 모델은 신뢰할 수 있는

1512
01:08:18,729 --> 01:08:22,330
방식으로 옷 접기, 상자 접기 및 다양한 조작

1513
01:08:22,330 --> 01:08:24,700
작업을 처리할 수 있습니다.

1514
01:08:24,700 --> 01:08:26,620
여기서 프레임워크의 전반적인

1515
01:08:26,620 --> 01:08:29,109
모습이 어떻게 생겼는지 보여줍니다.

1516
01:08:29,109 --> 01:08:32,369
왼쪽은 데이터 세트입니다.

1517
01:08:32,369 --> 01:08:34,870
어떤 모델이 기초 모델이라고 불리려면 그 기초

1518
01:08:34,870 --> 01:08:36,620
모델을 위한 연료가 필요합니다.

1519
01:08:36,620 --> 01:08:38,684
그 연료는 데이터입니다.

1520
01:08:38,684 --> 01:08:41,470
그들은 학계 내에서 수집한 데이터와 여러

1521
01:08:41,470 --> 01:08:43,750
다른 구현을 통해 스스로 수집한

1522
01:08:43,750 --> 01:08:45,513
많은 데이터를 집계하여 로봇이

1523
01:08:45,513 --> 01:08:46,930
실제 환경에서 흥미롭고

1524
01:08:46,930 --> 01:08:50,100
유용한 작업을 수행하는 데이터를 모았습니다.

1525
01:08:50,100 --> 01:08:54,020
그리고 이 데이터를 사용하여 사전 훈련을 수행합니다.

1526
01:08:54,020 --> 01:08:56,060
이 사전 훈련의

1527
01:08:56,060 --> 01:08:59,569
중요한 주의사항 중 하나는 이미

1528
01:08:59,569 --> 01:09:01,430
방대한 양의

1529
01:09:01,430 --> 01:09:05,689
비전-언어 관련 데이터로 훈련된 사전 훈련된

1530
01:09:05,689 --> 01:09:08,899
시각 언어 모델로 시작한다는

1531
01:09:08,899 --> 01:09:10,410
것입니다.

1532
01:09:10,410 --> 01:09:13,399
그리고 함께 코드 미세 조정을 수행함으로써,

1533
01:09:13,399 --> 01:09:16,790
그들이 코드 미세 조정이라고 부르는 것을

1534
01:09:16,790 --> 01:09:19,279
통해, 행동 예측을 위한 목표와 이러한

1535
01:09:19,279 --> 01:09:22,200
비전 질문 응답 작업에서 적응된 목표를

1536
01:09:22,200 --> 01:09:25,490
사용하여, 모델 내의 의미적 지식을 보존할

1537
01:09:25,490 --> 01:09:26,550
수 있습니다.

1538
01:09:26,550 --> 01:09:30,560
하지만 동시에 로봇의 행동을 예측할 수 있습니다.

1539
01:09:30,560 --> 01:09:32,430
이것이 사전 훈련 단계입니다.

1540
01:09:32,430 --> 01:09:36,439
기존의 많은 로봇 기초 모델에 대한 매우

1541
01:09:36,439 --> 01:09:39,029
중요한 설계는 포스트 훈련이라고

1542
01:09:39,029 --> 01:09:42,200
불리며, 이는 대형 언어 모델

1543
01:09:42,200 --> 01:09:44,450
커뮤니티의 많은 발전에서

1544
01:09:44,450 --> 01:09:46,410
영감을 받았습니다.

1545
01:09:46,410 --> 01:09:48,978
기본 모델은 합리적인 기준 성능을 제공합니다.

1546
01:09:48,978 --> 01:09:51,270
하지만 특정 작업에서 성능이 매우

1547
01:09:51,270 --> 01:09:53,130
좋기를 원한다면, 실제로는

1548
01:09:53,130 --> 01:09:56,890
모델을 미세 조정하기 위해 작업 특정 데이터를 수집해야

1549
01:09:56,890 --> 01:09:59,730
하며, 그 특정 작업에 대한 데이터로 포스트

1550
01:09:59,730 --> 01:10:02,625
훈련을 수행해야 성능이 만족스러워집니다.

1551
01:10:02,625 --> 01:10:04,740
그래서 그들은 세 가지 다른 범주에서

1552
01:10:04,740 --> 01:10:06,520
전체 시스템을 평가하고 있습니다.

1553
01:10:06,520 --> 01:10:09,160
첫 번째는 그들의 기본 모델을 직접 사용하는 것입니다.

1554
01:10:09,160 --> 01:10:11,280
그들의 기본 모델은 이미 매우

1555
01:10:11,280 --> 01:10:15,450
간단한 분포 내 데이터에 대해 충분히 좋을 수 있습니다.

1556
01:10:15,450 --> 01:10:18,432
이 작업은 실제로 사전 훈련

1557
01:10:18,432 --> 01:10:19,890
단계에서 이미

1558
01:10:19,890 --> 01:10:22,470
접했을 수 있는 작업입니다.

1559
01:10:22,470 --> 01:10:25,540
조금 더 복잡한 다른 작업의 경우,

1560
01:10:25,540 --> 01:10:28,260
기본 모델이 이러한 분포 내 작업에서

1561
01:10:28,260 --> 01:10:32,445
더 개선될 수 있도록 후속 훈련을 할 수 있습니다.

1562
01:10:32,445 --> 01:10:34,320
보지 못한 작업의 경우,

1563
01:10:34,320 --> 01:10:36,360
일반적으로 해당 작업에 특화된

1564
01:10:36,360 --> 01:10:40,050
데이터를 수집하고 이러한 작업에 대해 사전 훈련된

1565
01:10:40,050 --> 01:10:43,170
모델을 미세 조정하여 성능을 높여야 합니다.

1566
01:10:43,170 --> 01:10:46,285
그리고 이 Pi-Zero 모델은 실제로 오픈 소스입니다.

1567
01:10:46,285 --> 01:10:48,800
체크포인트를 다운로드할 수 있습니다.

1568
01:10:48,800 --> 01:10:50,530
내 연구실의 학생들은 이미

1569
01:10:50,530 --> 01:10:53,298
그들의 모델로 실험을 시작하고 후속 훈련을

1570
01:10:53,298 --> 01:10:54,590
시도하고 있습니다.

1571
01:10:54,590 --> 01:10:58,010
그리고 우리는 매우 유망한 결과를 보기 시작했습니다.

1572
01:10:58,010 --> 01:11:01,540
그래서 관심이 있다면, 시도해 보시기를 강력히 권장합니다.

1573
01:11:01,540 --> 01:11:02,780
정말 좋은 질문입니다.

1574
01:11:02,780 --> 01:11:05,050
기존 로봇 기초 모델의 효율성에

1575
01:11:05,050 --> 01:11:07,190
대해 질문하고 계신 것입니다.

1576
01:11:07,190 --> 01:11:10,300
정책이 실제로 인간보다 느린 이유는 여러

1577
01:11:10,300 --> 01:11:11,630
가지가 있습니다.

1578
01:11:11,630 --> 01:11:14,590
주요 이유 중 하나는 데이터 수집

1579
01:11:14,590 --> 01:11:17,170
방식, 즉 시연 데이터 수집 방식에서

1580
01:11:17,170 --> 01:11:18,410
기인합니다.

1581
01:11:18,410 --> 01:11:20,660
일반적으로 이러한

1582
01:11:20,660 --> 01:11:22,360
시나리오의

1583
01:11:22,360 --> 01:11:26,560
경우, 시연 데이터는 로봇을

1584
01:11:26,560 --> 01:11:30,340
원격 조작하여 수집되었습니다.

1585
01:11:30,340 --> 01:11:32,500
그리고 인간의 원격 조작은

1586
01:11:32,500 --> 01:11:36,230
인간이 손으로 이러한 작업을 수행하는 것보다 느립니다,

1587
01:11:36,230 --> 01:11:38,930
비록 몇 시간의 훈련을 받았더라도.

1588
01:11:38,930 --> 01:11:41,410
이는 당신이 가장 익숙한

1589
01:11:41,410 --> 01:11:45,510
구현체와 다른 구현체를 사용하고 있으며, 동시에

1590
01:11:45,510 --> 01:11:48,920
로봇 팔이 당신과 일정 거리를 두고

1591
01:11:48,920 --> 01:11:50,670
있기 때문입니다.

1592
01:11:50,670 --> 01:11:51,840
차폐가 발생할 것입니다.

1593
01:11:51,840 --> 01:11:54,180
때때로, 정말로 이해하기 위해서는 매우 가까이,

1594
01:11:54,180 --> 01:11:57,140
조심스럽게 바라봐야 하며, 머리를 바꾸거나 시각 각도를

1595
01:11:57,140 --> 01:11:58,830
바꿔야 할 수도 있습니다.

1596
01:11:58,830 --> 01:12:02,220
다음 작업 단계로 진행할 시간인지 아닌지를 판단하기 위해서입니다.

1597
01:12:02,220 --> 01:12:05,900
현재 데이터 수집 방식에는 많은

1598
01:12:05,900 --> 01:12:08,520
주의사항과 비효율성이 있습니다.

1599
01:12:08,520 --> 01:12:11,180
그래서 이러한 데이터에 직접 훈련된

1600
01:12:11,180 --> 01:12:14,100
정책이 인간의 속도보다 느리게 나타났습니다.

1601
01:12:14,100 --> 01:12:16,400
그래서 우리는 이러한 데이터 수집을 더

1602
01:12:16,400 --> 01:12:18,860
효율적으로, 인간의 속도로 수행할 수

1603
01:12:18,860 --> 01:12:21,900
있는 방법에 대한 많은 조사가 이루어지고 있습니다.

1604
01:12:21,900 --> 01:12:24,857
이는 실제로 매우 활발한 연구 방향입니다.

1605
01:12:24,857 --> 01:12:26,190
그래서 이것은 매우 좋은 질문입니다.

1606
01:12:26,190 --> 01:12:28,610
이 상자 접기 작업에 대해, 저는 이것이

1607
01:12:28,610 --> 01:12:31,140
이미 매우 긴 수평 작업이라고 주장할 것입니다.

1608
01:12:31,140 --> 01:12:35,660
저는 이 단일 정책이 이 긴 수평 작업을 얼마나 잘 처리할 수

1609
01:12:35,660 --> 01:12:37,680
있는지에 매우 감명받았습니다.

1610
01:12:37,680 --> 01:12:39,170
하지만 만약 이 정책이 더

1611
01:12:39,170 --> 01:12:42,887
큰 규모, 즉 그들의 야외 시나리오나 집에서 유용하게

1612
01:12:42,887 --> 01:12:44,970
사용되기를 원한다면, 로봇이 상자만

1613
01:12:44,970 --> 01:12:47,610
접는 것이 아니라는 점을 주장할 수 있습니다.

1614
01:12:47,610 --> 01:12:50,030
셔츠를 접고 침대를 정리하며 바닥의

1615
01:12:50,030 --> 01:12:52,320
모든 엉망을 치우기를 원합니다.

1616
01:12:52,320 --> 01:12:54,590
이러한 유형의 시나리오에서는 현재

1617
01:12:54,590 --> 01:12:56,900
개인적으로 저는 하나의 거대한

1618
01:12:56,900 --> 01:12:59,870
정책이 이러한 시나리오에 적응할 수 있다고

1619
01:12:59,870 --> 01:13:01,110
믿지 않습니다.

1620
01:13:01,110 --> 01:13:03,020
더 높은 수준의

1621
01:13:03,020 --> 01:13:04,940
추상화나 어떤 종류의

1622
01:13:04,940 --> 01:13:08,760
장면 그래프, 상징적 표현이 이러한 정책이

1623
01:13:08,760 --> 01:13:10,940
다양한 작업으로 유도되고

1624
01:13:10,940 --> 01:13:13,610
더 큰 환경과 더 복잡한

1625
01:13:13,610 --> 01:13:15,590
작업으로 확장될 수

1626
01:13:15,590 --> 01:13:19,070
있도록 하기 위한 조건으로 필요합니다.

1627
01:13:19,070 --> 01:13:21,530
그래서 이는 사전 훈련된 비전 언어 모델로

1628
01:13:21,530 --> 01:13:22,140
시작되었습니다.

1629
01:13:22,140 --> 01:13:24,890
그래서 이미 이 비전 언어 데이터를 사용한

1630
01:13:24,890 --> 01:13:27,800
대규모 사전 훈련을 통해 많은 의미적

1631
01:13:27,800 --> 01:13:29,130
지식이 학습되었습니다.

1632
01:13:29,130 --> 01:13:31,700
그래서 일부 일반화가 무료로 발생하는

1633
01:13:31,700 --> 01:13:34,310
이유는 기본 모델이 의미적 수준에서

1634
01:13:34,310 --> 01:13:37,400
놀라울 정도로 좋은 일반화 수준을 가질

1635
01:13:37,400 --> 01:13:38,790
수 있기 때문입니다.

1636
01:13:38,790 --> 01:13:41,632
그리고 저는 이 모델을 로봇 데이터로 미세 조정해야

1637
01:13:41,632 --> 01:13:43,090
하며, 이는 의미적 수준뿐만

1638
01:13:43,090 --> 01:13:46,777
아니라 행동 수준에서도 일반화할 수 있도록 하기 위해서입니다.

1639
01:13:46,777 --> 01:13:48,610
질문은 아마도 끝에 할 수 있을 것입니다.

1640
01:13:48,610 --> 01:13:50,090
이미 시간이 다 되어가고 있습니다.

1641
01:13:50,090 --> 01:13:51,340
아직도 마지막으로

1642
01:13:51,340 --> 01:13:53,210
두세 분 정도 남았으니,

1643
01:13:53,210 --> 01:13:56,240
로봇 학습 모델 개발과 관련된

1644
01:13:56,240 --> 01:13:59,270
남은 도전 과제에 대해 논의하겠습니다.

1645
01:13:59,270 --> 01:14:03,220
전체 커뮤니티가 인식하는 주요 도전 과제 중

1646
01:14:03,220 --> 01:14:05,080
하나는 평가입니다.

1647
01:14:05,080 --> 01:14:06,670
현재 평가는 주로 실제

1648
01:14:06,670 --> 01:14:07,790
세계에서 이루어집니다.

1649
01:14:07,790 --> 01:14:11,030
예를 들어, 이것은 구글 로보틱스의 사진입니다.

1650
01:14:11,030 --> 01:14:14,500
그들은 데이터 수집과 평가를 수행하는 이러한 종류의

1651
01:14:14,500 --> 01:14:17,920
원격 조작 Aloha 시스템의 그리드를 가지고 있습니다.

1652
01:14:17,920 --> 01:14:21,230
실제 세계에서의 평가는 비용이 많이 들고 소음이 많습니다.

1653
01:14:21,230 --> 01:14:24,428
그들이 저에게 한 정확한 말은, 평가를 위해 그들은 여전히 진전을

1654
01:14:24,428 --> 01:14:26,470
이룰 수 있을 만큼 충분한 예산을 가지고 있다는

1655
01:14:26,470 --> 01:14:27,140
것이었습니다.

1656
01:14:27,140 --> 01:14:29,273
이것은 그들의 정확한 말로,

1657
01:14:29,273 --> 01:14:31,940
만약 당신이 평가를 하거나 제가 평가를

1658
01:14:31,940 --> 01:14:34,280
한다면, 초기 구성과 조명 조건이 어떻게

1659
01:14:34,280 --> 01:14:36,700
변할지에 따라 결과가 매우 다를

1660
01:14:36,700 --> 01:14:38,600
수 있다는 것을 의미합니다.

1661
01:14:38,600 --> 01:14:40,600
어떤 요소의 마찰 매개변수조차도

1662
01:14:40,600 --> 01:14:43,090
다운스트림 정책의 강건성에 큰 차이를

1663
01:14:43,090 --> 01:14:44,518
만들 수 있습니다.

1664
01:14:44,518 --> 01:14:46,060
그래서 이것은 매우 비용이 많이

1665
01:14:46,060 --> 01:14:49,060
들며, 그들은 결과가 돌아오기를 위해 이틀을 기다려야 합니다.

1666
01:14:49,060 --> 01:14:52,960
현재 훈련 손실과 실제 세계 성공률 간의

1667
01:14:52,960 --> 01:14:55,730
상관관계는 매우 약합니다.

1668
01:14:55,730 --> 01:14:58,390
그래서 이것은 감독 학습과 이와 같은

1669
01:14:58,390 --> 01:15:00,063
순차적 의사 결정, 이

1670
01:15:00,063 --> 01:15:01,730
정책 학습 간의 또 다른

1671
01:15:01,730 --> 01:15:05,080
매우 중요한 주의사항과 차이점입니다. 감독

1672
01:15:05,080 --> 01:15:08,578
학습에서는 훈련 손실이 모델의 성능을 직접 측정합니다.

1673
01:15:08,578 --> 01:15:10,120
하지만 이 정책

1674
01:15:10,120 --> 01:15:13,510
학습에서는 훈련 손실이 이 일단계 예측의 성능을

1675
01:15:13,510 --> 01:15:16,670
측정하며, 이는 때때로 그렇지 않을 수

1676
01:15:16,670 --> 01:15:20,290
있고, 실제로는 종종 긴 작업 수평에서 정책의

1677
01:15:20,290 --> 01:15:23,990
성능을 나타내지 않습니다, 비록 손실이 낮더라도.

1678
01:15:23,990 --> 01:15:25,700
하지만 긴 시간 동안의 작업

1679
01:15:25,700 --> 01:15:28,860
실행에서는 정책이 실제로 더 나쁠 수 있습니다.

1680
01:15:28,860 --> 01:15:32,320
훈련 목표와 작업별 지표, 예를 들어

1681
01:15:32,320 --> 01:15:34,690
훈련과 테스트 수평선 간의

1682
01:15:34,690 --> 01:15:36,630
차이 등이 정책 성능을

1683
01:15:36,630 --> 01:15:39,170
측정하기 위한 근사적 지표를

1684
01:15:39,170 --> 01:15:41,060
제시하기 어렵게 만드는

1685
01:15:41,060 --> 01:15:42,810
이유 중 일부입니다.

1686
01:15:42,810 --> 01:15:46,460
사람들은 실제 평가에 의존해야 합니다.

1687
01:15:46,460 --> 01:15:48,290
그렇다면 시뮬레이션 환경에서

1688
01:15:48,290 --> 01:15:50,580
평가를 수행하는 것은 어떨까요?

1689
01:15:50,580 --> 01:15:52,170
예를 들어, 스탠포드의

1690
01:15:52,170 --> 01:15:53,630
물리 실험실과 메타의

1691
01:15:53,630 --> 01:15:56,100
Habitat 3.0에서 수행된 행동에

1692
01:15:56,100 --> 01:15:58,225
대한 많은 연구가 있었습니다.

1693
01:15:58,225 --> 01:15:59,600
현재 사람들은 이러한

1694
01:15:59,600 --> 01:16:02,220
비싼 시뮬레이션 환경을 만들어

1695
01:16:02,220 --> 01:16:05,090
로봇 정책의 평가와 측정을 시도하고

1696
01:16:05,090 --> 01:16:06,030
있습니다.

1697
01:16:06,030 --> 01:16:09,240
그리고 분명히, 시뮬레이션과 실제 간의 격차와

1698
01:16:09,240 --> 01:16:12,090
관련하여 그들만의 문제가 있습니다.

1699
01:16:12,090 --> 01:16:15,450
강체, 변형 가능한 물체, 의류의 매우 정확한 시뮬레이션을

1700
01:16:15,450 --> 01:16:16,770
어떻게 수행할 수 있을까요?

1701
01:16:16,770 --> 01:16:19,830
그들은 실제 성능과 좋은 상관관계를 가지고 있습니다.

1702
01:16:19,830 --> 01:16:22,925
자산도 또 다른 주요 문제로,

1703
01:16:22,925 --> 01:16:25,340
대규모 일반화 및 자산

1704
01:16:25,340 --> 01:16:27,920
생성이 큰 어려움입니다.

1705
01:16:27,920 --> 01:16:31,080
자세히 설명할 수 있지만, 아마도 강의 후에 하겠습니다.

1706
01:16:31,080 --> 01:16:33,590
그리고 실제 세계를 디지털화하는 방법도 문제입니다.

1707
01:16:33,590 --> 01:16:35,170
현실적이고 다양한 것들의

1708
01:16:35,170 --> 01:16:37,870
절차적 생성을 수행하는 방법은 로봇

1709
01:16:37,870 --> 01:16:41,020
학습 정책을 평가하기 위해 시뮬레이션을 사용하는

1710
01:16:41,020 --> 01:16:42,310
모든 문제입니다.

1711
01:16:42,310 --> 01:16:44,950
그리고 실제와 시뮬레이션 간의 상관관계를

1712
01:16:44,950 --> 01:16:48,810
발견했을 때, 이는 Embodied AI의 ImageNet을

1713
01:16:48,810 --> 01:16:51,040
호출하게 되며, ImageNet이 성공한

1714
01:16:51,040 --> 01:16:54,680
이유는 적어도 몇 년 동안 ImageNet의 모든 진전이

1715
01:16:54,680 --> 01:16:56,990
심층 학습과 컴퓨터 비전의 진전을

1716
01:16:56,990 --> 01:16:58,157
의미했기 때문입니다.

1717
01:16:58,157 --> 01:16:59,740
우리는 이 플랫폼을 원합니다.

1718
01:16:59,740 --> 01:17:02,260
즉, 그 기준선이나 플랫폼에서의 모든 진전은

1719
01:17:02,260 --> 01:17:04,250
로봇 학습의 진전을 의미합니다.

1720
01:17:04,250 --> 01:17:07,270
그래서 우리가 정말로 원하는 것입니다.

1721
01:17:07,270 --> 01:17:10,540
그리고 저는 아마도 건너뛰어도 될 것 같습니다.

1722
01:17:10,540 --> 01:17:13,300
우리는 이러한 기초 정책을 구축하는 방법과

1723
01:17:13,300 --> 01:17:15,130
기초 세계 모델을 구축하는 방법에

1724
01:17:15,130 --> 01:17:16,940
대한 조사를 이야기합니다.

1725
01:17:16,940 --> 01:17:18,760
특히 지금, 사람들은 이 기초

1726
01:17:18,760 --> 01:17:21,040
정책을 훈련하기 위해 대규모의 행동 조건

1727
01:17:21,040 --> 01:17:23,180
로봇 상호작용 데이터를 수집하고 있습니다.

1728
01:17:23,180 --> 01:17:24,850
하지만 그 데이터에는 많은 동역학

1729
01:17:24,850 --> 01:17:26,180
지식이 내재되어 있습니다.

1730
01:17:26,180 --> 01:17:28,520
그 데이터를 정책 학습에만 사용한다면, 정말

1731
01:17:28,520 --> 01:17:29,750
아쉬운 일이 될 것입니다.

1732
01:17:29,750 --> 01:17:31,690
그래서 우리는 이미 수집된 이

1733
01:17:31,690 --> 01:17:33,820
대규모 행동 조건 로봇 상호작용

1734
01:17:33,820 --> 01:17:36,310
데이터를 어떻게 활용하여 기초 정책을

1735
01:17:36,310 --> 01:17:38,950
훈련하고 기초 세계 모델을 훈련할 수

1736
01:17:38,950 --> 01:17:42,490
있을지, 그리고 서로 어떻게 상호작용할 수 있을지를

1737
01:17:42,490 --> 01:17:43,790
고민하고 있습니다.

1738
01:17:43,790 --> 01:17:45,340
그리고 이러한 기초

1739
01:17:45,340 --> 01:17:48,250
세계 모델을 구축하는 방향에 대해 생각하는

1740
01:17:48,250 --> 01:17:50,240
기존 연구들이 있습니다.

1741
01:17:50,240 --> 01:17:53,980
그리고 당신이 생각할 수 있는 매우 흥미로운 특성들이 있습니다.

1742
01:17:53,980 --> 01:17:56,630
예를 들어, 3D로 만들고 싶으신가요?

1743
01:17:56,630 --> 01:17:57,930
구조적 우선순위를 원하나요?

1744
01:17:57,930 --> 01:17:59,680
얼마나 학습하고 얼마나 물리학을 적용할 것인가?

1745
01:17:59,680 --> 01:18:03,340
그리고 실제 물리적 세계와 어떻게 상관관계를 맺을 수 있을까요?

1746
01:18:03,340 --> 01:18:06,800
그리고 아마도, 실제로 우리는 시간이 다 되어가는 것 같습니다.

1747
01:18:06,800 --> 01:18:08,630
그래서 여기서 마치겠습니다.

1748
01:18:08,630 --> 01:18:12,850
그리고 이것이 우리가 달성하고자 하는 미래로, 매우

1749
01:18:12,850 --> 01:18:16,120
널리 작동하고 비구조적 데이터 환경에서 잘

1750
01:18:16,120 --> 01:18:19,090
일반화할 수 있는 기초 로봇 모델을

1751
01:18:19,090 --> 01:18:20,660
구축하는 것입니다.

1752
01:18:20,660 --> 01:18:23,210
다음 강의는 인간 중심 AI가 될 것입니다.

1753
01:18:23,210 --> 01:18:25,880
그리고 오늘 강의는 여기서 마치겠습니다.

1754
01:18:25,880 --> 01:18:27,750
정말 감사합니다.
