1
00:00:05,600 --> 00:00:12,000
이번 주 화요일에 우리는 GPU와 그 사용법,

2
00:00:12,000 --> 00:00:14,380
여러 GPU를

3
00:00:14,380 --> 00:00:18,700
사용한 훈련 방법에 대한

4
00:00:18,700 --> 00:00:20,920
강의를 했습니다.

5
00:00:20,920 --> 00:00:23,440
올해 이 수업에

6
00:00:23,440 --> 00:00:26,400
추가된 흥미롭고 새로운

7
00:00:26,400 --> 00:00:29,360
주제로, AI 모델의

8
00:00:29,360 --> 00:00:34,960
크기와 응용이 증가하는 시점에서

9
00:00:34,960 --> 00:00:39,840
시의적절하고 매우 중요하다고 생각합니다.

10
00:00:39,840 --> 00:00:47,800
그 전에 우리는 분류, 의미론적 분할, 객체 탐지, 인스턴스

11
00:00:47,800 --> 00:00:50,660
분할 등 컴퓨터

12
00:00:50,660 --> 00:00:53,400
비전의 모든 주요

13
00:00:53,400 --> 00:00:56,880
작업에 대해 이야기했습니다.

14
00:00:56,880 --> 00:00:59,810
오늘 우리가 이야기할 모델의 결과 중

15
00:00:59,810 --> 00:01:02,790
일부 주제를 다시 살펴볼 것입니다.

16
00:01:02,790 --> 00:01:09,410
이러한 작업은 여전히 매우 중요합니다.

17
00:01:09,410 --> 00:01:12,650
그리고 우리는 모델을 시각화하고

18
00:01:12,650 --> 00:01:19,490
이해하며 모델이 무엇을 배우고 있는지 살펴보았습니다.

19
00:01:19,490 --> 00:01:21,130
우리가 논의한

20
00:01:21,130 --> 00:01:27,830
것 중 하나는 초기 세션에서 최근접 이웃과 픽셀 공간을

21
00:01:27,830 --> 00:01:31,130
사용하여 픽셀 기반 거리만으로

22
00:01:31,130 --> 00:01:37,690
이미지의 클래스를 찾는 방법에 대해 이야기한

23
00:01:37,690 --> 00:01:40,550
것입니다. 그리고 그것이

24
00:01:40,550 --> 00:01:45,330
실제로 비효율적이라는 점을 논의했습니다.

25
00:01:45,330 --> 00:01:47,250
우리가 이야기한

26
00:01:47,250 --> 00:01:53,130
것 중 하나는 임베딩 레이어 또는 특징 공간, 특징

27
00:01:53,130 --> 00:01:56,730
레이어를 사용하는 것이며,

28
00:01:56,730 --> 00:02:03,060
이는 컨볼루션 신경망 또는 우리가 사용하는 다른 네트워크

29
00:02:03,060 --> 00:02:05,780
아키텍처의 완전 연결

30
00:02:05,780 --> 00:02:10,020
레이어에서 이미지의 좋은 표현이 될 수

31
00:02:10,020 --> 00:02:11,280
있습니다.

32
00:02:11,280 --> 00:02:14,520
우리는 특징 공간에서 최근접

33
00:02:14,520 --> 00:02:19,260
이웃의 메트릭으로 L2 거리를

34
00:02:19,260 --> 00:02:23,540
사용하는 것에 대해 이야기했습니다.

35
00:02:23,540 --> 00:02:25,300
이는 이러한 특징들이 우리가

36
00:02:25,300 --> 00:02:28,380
다루고 있는 특정 작업에 대해 매우 의미가

37
00:02:28,380 --> 00:02:30,260
있다는 것을 의미합니다.

38
00:02:30,260 --> 00:02:34,460
특히, 신경망, CNN,

39
00:02:34,460 --> 00:02:39,660
ResNet 또는 트랜스포머 모델을

40
00:02:39,660 --> 00:02:46,800
실행하고 다양한 맥락에서 학습된 표현을 살펴보면,

41
00:02:46,800 --> 00:02:49,200
이러한 표현이

42
00:02:49,200 --> 00:02:54,740
다른 이름으로 언급될 수 있습니다.

43
00:02:54,740 --> 00:02:56,890
학습된 대규모 표현, 특징,

44
00:02:56,890 --> 00:02:59,810
임베딩, 잠재 공간 등입니다.

45
00:02:59,810 --> 00:03:04,230
하지만 이러한 학습된 표현이나 특징은

46
00:03:04,230 --> 00:03:07,590
이미지의 매우 좋은 대표입니다.

47
00:03:07,590 --> 00:03:12,970
우리가 그것들을 추출할 방법이 있다면, 간단한

48
00:03:12,970 --> 00:03:17,470
선형 모델을 통해 이러한 특징에서

49
00:03:17,470 --> 00:03:21,350
클래스 레이블을 항상 얻을 수

50
00:03:21,350 --> 00:03:22,430
있습니다.

51
00:03:22,430 --> 00:03:24,910
하지만 대규모로 이러한

52
00:03:24,910 --> 00:03:30,190
네트워크를 훈련하거나 구축하는 주요

53
00:03:30,190 --> 00:03:33,530
도전 과제가 항상 존재합니다.

54
00:03:33,530 --> 00:03:37,350
여기서 왜 도전이 있는지 말씀해 주실 수 있나요?

55
00:03:37,350 --> 00:03:40,690
문제는 대규모에서는 많은 레이블이 있는

56
00:03:40,690 --> 00:03:45,110
데이터가 필요하다는 것입니다. 이 네트워크는 이미지에서

57
00:03:45,110 --> 00:03:49,270
시작하여 최종적으로 클래스 레이블을 갖습니다.

58
00:03:49,270 --> 00:03:51,550
이 네트워크를 훈련하면,

59
00:03:51,550 --> 00:03:56,190
이러한 특징이 클래스 레이블을 얻는 데 매우 유용할

60
00:03:56,190 --> 00:03:57,370
것입니다.

61
00:03:57,370 --> 00:04:02,410
하지만 대규모에서는 이미지를 하나하나 수동으로 레이블링하는

62
00:04:02,410 --> 00:04:05,148
많은 노력이 필요합니다.

63
00:04:05,148 --> 00:04:06,690
세그멘테이션

64
00:04:06,690 --> 00:04:10,850
작업이라면, 모든 이미지에서 픽셀을 하나하나

65
00:04:10,850 --> 00:04:14,650
레이블링해야 하며, 이는 매우 도전적입니다.

66
00:04:14,650 --> 00:04:18,130
그래서 질문은 대규모 수동 레이블

67
00:04:18,130 --> 00:04:22,490
데이터 세트 없이 신경망을 훈련할 수

68
00:04:22,490 --> 00:04:25,190
있는 방법이 있는가입니다.

69
00:04:25,190 --> 00:04:29,470
이러한 수동 레이블이 도전 과제이며,

70
00:04:29,470 --> 00:04:32,970
우리는 신경망을 훈련하여 매우

71
00:04:32,970 --> 00:04:36,290
좋은 특징을 얻는 방법을 우회할

72
00:04:36,290 --> 00:04:39,410
수 있는지 보고 싶습니다.

73
00:04:39,410 --> 00:04:44,330
그와 함께 자기 지도 학습의 주제가

74
00:04:44,330 --> 00:04:45,270
부각됩니다.

75
00:04:45,270 --> 00:04:48,570
오늘 우리가 다룰 내용입니다.

76
00:04:48,570 --> 00:04:58,700
예를 들어 레이블이 없는 이미지의 대규모 데이터 세트를 가지고, 우리의

77
00:04:58,700 --> 00:05:02,020
가설은 좋은 특징을 얻기

78
00:05:02,020 --> 00:05:08,460
위해 목표 함수, 전제 작업을 사용하여 신경망을

79
00:05:08,460 --> 00:05:12,600
훈련할 수 있다는 것입니다.

80
00:05:12,600 --> 00:05:17,860
그리고 레이블이 있는 소규모 데이터 세트에서

81
00:05:17,860 --> 00:05:23,860
학습할 때, 우리는 기본적으로 이 훈련된 인코더를

82
00:05:23,860 --> 00:05:28,740
전이하여 다운스트림 작업이나 다운스트림 목표를

83
00:05:28,740 --> 00:05:32,260
위해 특징을 추출하는 데

84
00:05:32,260 --> 00:05:34,500
사용할 수 있습니다.

85
00:05:34,500 --> 00:05:38,780
여기서 우리는 전제 작업을 정의하고,

86
00:05:38,780 --> 00:05:43,540
이미지에서 좋은 특징을 학습할 수

87
00:05:43,540 --> 00:05:47,500
있을 만큼 일반적인 작업을

88
00:05:47,500 --> 00:05:53,750
정의하고, 그 인코더를 사용하여 우리가 관심 있는

89
00:05:53,750 --> 00:05:56,870
문제, 즉 다운스트림

90
00:05:56,870 --> 00:05:59,990
작업이나 다운스트림 목표를

91
00:05:59,990 --> 00:06:02,330
해결하고자 합니다.

92
00:06:02,330 --> 00:06:06,630
예를 들어, 우리는 인터넷에서 다운로드한 많은 자연 이미지를

93
00:06:06,630 --> 00:06:08,130
가지고 있습니다.

94
00:06:08,130 --> 00:06:10,330
그것으로 무언가를 훈련할 수 있습니다.

95
00:06:10,330 --> 00:06:13,290
그리고 우리는 레이블이 몇 개 있는 산업

96
00:06:13,290 --> 00:06:14,910
응용 프로그램이나 의료

97
00:06:14,910 --> 00:06:18,870
응용 프로그램의 소규모 데이터 세트를 가지고 있습니다.

98
00:06:18,870 --> 00:06:21,990
이제 우리는 그 전이된 지식을

99
00:06:21,990 --> 00:06:26,270
사용하여 특징을 추출하고 우리가 관심 있는 작업을

100
00:06:26,270 --> 00:06:29,350
분류하거나 수행할 수 있습니다.

101
00:06:29,350 --> 00:06:32,390
그래서 우리는 이 주제를 깊이

102
00:06:32,390 --> 00:06:35,430
파고들어 다양한 구성 요소를

103
00:06:35,430 --> 00:06:37,590
이해하고자 합니다.

104
00:06:37,590 --> 00:06:40,810
간단히 말해, 자기 지도 학습이란

105
00:06:40,810 --> 00:06:45,590
레이블이 없는 데이터 세트에서 이 전제 작업을

106
00:06:45,590 --> 00:06:47,110
정의하는 것입니다.

107
00:06:47,110 --> 00:06:55,140
인코더는 종종 학습된 표현을 제공합니다.

108
00:06:55,140 --> 00:06:59,640
그리고 같은 신경망의 또 다른 모듈이 학습된

109
00:06:59,640 --> 00:07:04,080
표현을 출력 공간으로 생성하거나 전이합니다.

110
00:07:04,080 --> 00:07:06,240
이 출력 공간은

111
00:07:06,240 --> 00:07:10,440
레이블이나 데이터에서 자동으로 생성된 출력일

112
00:07:10,440 --> 00:07:11,980
수 있습니다.

113
00:07:11,980 --> 00:07:14,880
이들은 수동 주석이 아닙니다.

114
00:07:14,880 --> 00:07:22,720
이렇게 할 수 있다면, 우리는 목표 함수, 손실 함수,

115
00:07:22,720 --> 00:07:25,400
그리고 손실 함수로

116
00:07:25,400 --> 00:07:29,280
훈련될 신경망을 갖게 됩니다.

117
00:07:29,280 --> 00:07:33,680
여기서 보시다시피, 우리는 두 번째 부분을 때때로 디코더, 분류기,

118
00:07:33,680 --> 00:07:36,600
회귀기로 부릅니다. 이는 우리가 전제 작업을 어떻게

119
00:07:36,600 --> 00:07:38,420
정의하느냐에 따라 다릅니다.

120
00:07:38,420 --> 00:07:40,920
몇 가지 예를 드리겠지만, 이는

121
00:07:40,920 --> 00:07:44,120
어떤 형태의 프레임워크일 수 있습니다.

122
00:07:44,120 --> 00:07:47,230
하지만 인코더와 디코더가 있을 때,

123
00:07:47,230 --> 00:07:49,810
이는 제가 간단히 설명할

124
00:07:49,810 --> 00:07:52,570
자동 인코딩 프레임워크입니다.

125
00:07:52,570 --> 00:07:59,810
전제 작업으로 훈련을 마친 후, 이제 우리는 인코더와 학습된 표현을

126
00:07:59,810 --> 00:08:03,890
다운스트림 작업에 사용할 수 있습니다.

127
00:08:03,890 --> 00:08:07,050
이를 위해 한 층을 추가하거나 완전

128
00:08:07,050 --> 00:08:12,830
연결 신경망, 선형 함수 또는 레이블을 예측하는 완전

129
00:08:12,830 --> 00:08:16,010
연결 신경망을 추가하면 됩니다.

130
00:08:16,010 --> 00:08:18,410
이 레이블은 이제 데이터

131
00:08:18,410 --> 00:08:20,570
세트에서 나옵니다.

132
00:08:20,570 --> 00:08:24,450
그래서 이것이 자기 지도 학습의

133
00:08:24,450 --> 00:08:31,230
주요 개념입니다. 전제 버전의 일부는 훈련을

134
00:08:31,230 --> 00:08:35,929
위해 레이블이 있는 데이터가 필요하지

135
00:08:35,929 --> 00:08:37,289
않습니다.

136
00:08:37,289 --> 00:08:39,210
하지만 전제 작업 자체를

137
00:08:39,210 --> 00:08:43,169
정의하는 것은 그렇게 간단하지 않습니다.

138
00:08:43,169 --> 00:08:46,560
이를 정의하는 다양한 방법이 있습니다.

139
00:08:46,560 --> 00:08:50,740
예를 들어, 우리는 전제 작업을 정의할

140
00:08:50,740 --> 00:08:56,500
때, 첫째, 좋은 특징을 얻을 수 있을 만큼 일반적이어야

141
00:08:56,500 --> 00:09:02,020
하며 수동 레이블링이 필요하지 않아야 한다는 점을

142
00:09:02,020 --> 00:09:04,120
염두에 두어야 합니다.

143
00:09:04,120 --> 00:09:07,492
따라서 레이블은 데이터 자체에서 나와야 합니다, 맞죠?

144
00:09:07,492 --> 00:09:14,540
하나의 예는 이미지 완성입니다. 여기서 우리는 이미지의 절반 또는

145
00:09:14,540 --> 00:09:16,360
일부를 가립니다.

146
00:09:16,360 --> 00:09:21,460
그리고 가려지지 않은 부분을 주어 가려진 부분을

147
00:09:21,460 --> 00:09:23,960
예측하는 작업을 정의합니다.

148
00:09:23,960 --> 00:09:28,740
또는 예를 들어, 특정 각도로 이미지를

149
00:09:28,740 --> 00:09:32,660
회전시키고, 작업은 이미지를 입력으로

150
00:09:32,660 --> 00:09:37,340
받아 회전 각도를 예측하는 것입니다.

151
00:09:37,340 --> 00:09:41,740
또 다른 예는 지그소 퍼즐로, 이미지의

152
00:09:41,740 --> 00:09:44,850
패치가 순서가 없지만,

153
00:09:44,850 --> 00:09:50,590
작업은 이 패치의 올바른 순서를 출력하는 것입니다.

154
00:09:50,590 --> 00:09:53,550
그리고 색칠하기는 인기 있는 작업 중

155
00:09:53,550 --> 00:09:55,070
하나로, 오늘 우리가

156
00:09:55,070 --> 00:09:58,350
매우 빠르게 다룰 네 가지 중 하나입니다.

157
00:09:58,350 --> 00:10:04,850
흑백 이미지의 경우 각 픽셀의

158
00:10:04,850 --> 00:10:08,350
색상을 예측합니다.

159
00:10:08,350 --> 00:10:12,350
전제 작업을 해결하면 모델이 좋은 특징을 학습할

160
00:10:12,350 --> 00:10:13,810
수 있습니다.

161
00:10:13,810 --> 00:10:15,730
그것이 우리가 원하는 것입니다.

162
00:10:15,730 --> 00:10:18,070
우리는 전제 작업에 대한 레이블을

163
00:10:18,070 --> 00:10:20,210
자동으로 생성할 수 있습니다.

164
00:10:20,210 --> 00:10:22,430
그래서 제가 언급한 두

165
00:10:22,430 --> 00:10:27,350
가지 포인트는 자기 지도 학습을 위한 좋은 전제 작업으로

166
00:10:27,350 --> 00:10:30,270
자격을 갖추기 위해 필요합니다.

167
00:10:30,270 --> 00:10:34,470
자기 지도 학습 프레임워크를 평가할 때 항상

168
00:10:34,470 --> 00:10:38,510
염두에 두어야 할 몇 가지 빠른 고려 사항이

169
00:10:38,510 --> 00:10:39,510
있습니다.

170
00:10:39,510 --> 00:10:44,560
전제 작업 자체를 살펴볼 수 있는 다양한 요소와

171
00:10:44,560 --> 00:10:47,240
영역이 있습니다. 우리는

172
00:10:47,240 --> 00:10:50,160
레이블을 생성하고 있습니다.

173
00:10:50,160 --> 00:10:53,880
이것은 모델이 전제 작업을 해결하는

174
00:10:53,880 --> 00:10:59,080
능력을 평가할 수 있는 힘을 제공합니다.

175
00:10:59,080 --> 00:11:03,700
그래서 이것이 하나의 요소입니다.

176
00:11:03,700 --> 00:11:06,120
그런 다음 표현 품질 자체가

177
00:11:06,120 --> 00:11:08,840
때때로 매우 중요합니다.

178
00:11:08,840 --> 00:11:14,200
예를 들어, 미세 조정 없이 표현만을 살펴보거나,

179
00:11:14,200 --> 00:11:15,960
표현을

180
00:11:15,960 --> 00:11:17,760
클러스터링하여

181
00:11:17,760 --> 00:11:20,680
표현에서 패턴을 볼 수

182
00:11:20,680 --> 00:11:23,160
있는지 확인합니다.

183
00:11:23,160 --> 00:11:27,000
때때로 좋은 차원 축소 알고리즘이

184
00:11:27,000 --> 00:11:28,060
있습니다.

185
00:11:28,060 --> 00:11:33,120
여기서 제가 언급하는 t-SNE는 우리가 많이

186
00:11:33,120 --> 00:11:35,240
이야기하지 않았지만,

187
00:11:35,240 --> 00:11:37,880
이는 학습된 표현의 차원을

188
00:11:37,880 --> 00:11:41,940
줄이고 2D 또는 3D로 시각화하여

189
00:11:41,940 --> 00:11:45,060
표현에서 패턴을 찾을 수 있는

190
00:11:45,060 --> 00:11:48,140
차원 축소 프레임워크입니다.

191
00:11:48,140 --> 00:11:52,440
따라서 강건성, 일반화, 계산 효율성은

192
00:11:52,440 --> 00:11:54,540
모두 매우 중요합니다.

193
00:11:54,540 --> 00:11:59,660
하지만 가장 중요한 것은 우리가 추구하는

194
00:11:59,660 --> 00:12:04,780
가장 중요한 측면은 다운스트림 작업에서의

195
00:12:04,780 --> 00:12:06,880
성능입니다.

196
00:12:06,880 --> 00:12:11,380
우리는 전체 자기 지도 학습을 수행하고 작업,

197
00:12:11,380 --> 00:12:14,020
전제 작업 등을 정의하여 관심

198
00:12:14,020 --> 00:12:17,180
있는 작업이나 우리가 중요하게

199
00:12:17,180 --> 00:12:21,100
여기는 작업의 결과를 개선할 수 있습니다.

200
00:12:21,100 --> 00:12:25,700
이것이 어떻게 이루어질 수 있는지 몇 가지 간단한 예를 살펴보겠습니다.

201
00:12:25,700 --> 00:12:31,260
이것은 이미지를 회전시키고 회전

202
00:12:31,260 --> 00:12:36,280
각도를 출력으로 예측하는 예입니다.

203
00:12:36,280 --> 00:12:39,270
따라서 우리는 이미지의 객체에 대한 레이블

204
00:12:39,270 --> 00:12:43,110
없이 자기 지도 방식으로 이를 훈련할 수 있습니다.

205
00:12:43,110 --> 00:12:45,350
이 예제에는 여러 개의

206
00:12:45,350 --> 00:12:48,510
합성곱 층이 있고, 마지막에는 회귀 또는

207
00:12:48,510 --> 00:12:52,550
분류 작업을 수행하기 위한 완전 연결 신경망이

208
00:12:52,550 --> 00:12:53,590
있습니다.

209
00:12:53,590 --> 00:12:56,430
이는 우리가 좋은 특징

210
00:12:56,430 --> 00:13:02,270
집합, 좋은 특징 추출기를 제공하며, 마지막에

211
00:13:02,270 --> 00:13:07,990
이러한 작업, 전제 작업, 특정 부분인 FC 층을

212
00:13:07,990 --> 00:13:13,630
제거하고 하나의 층 또는 경우에 따라 여러

213
00:13:13,630 --> 00:13:17,990
층을 추가하여 특징을 객체 레이블로

214
00:13:17,990 --> 00:13:21,490
분류할 수 있음을 의미합니다.

215
00:13:21,490 --> 00:13:26,070
이번에는 객체 레이블을 사용하여 예측하고

216
00:13:26,070 --> 00:13:30,370
이 선형 함수 자체를 훈련합니다.

217
00:13:30,370 --> 00:13:33,770
우리는 종종 얕은 네트워크를

218
00:13:33,770 --> 00:13:35,960
찾습니다. 왜냐하면

219
00:13:35,960 --> 00:13:42,760
특징이 충분히 좋다면 클래스 레이블을 얻기 위해 많은

220
00:13:42,760 --> 00:13:47,000
훈련을 할 필요가 없기 때문입니다.

221
00:13:47,000 --> 00:13:52,360
이것이 일반적인 자기 지도 학습입니다.

222
00:13:52,360 --> 00:13:56,200
우리는 컴퓨터 비전 응용 프로그램에 대해

223
00:13:56,200 --> 00:13:57,480
이야기하고 있지만,

224
00:13:57,480 --> 00:14:01,360
이 자기 지도 학습 패러다임이

225
00:14:01,360 --> 00:14:07,080
바로 이러한 대형 언어 모델을 가능하게 했습니다.

226
00:14:07,080 --> 00:14:10,000
GPT-4와 이러한

227
00:14:10,000 --> 00:14:18,440
프레임워크는 대부분 수동 레이블링 없이 원시 데이터로 훈련됩니다.

228
00:14:18,440 --> 00:14:21,940
언어 모델뿐만 아니라 음성,

229
00:14:21,940 --> 00:14:25,840
그리고 요즘에는 로봇 및 강화

230
00:14:25,840 --> 00:14:29,640
학습에서도 많이 사용됩니다.

231
00:14:29,640 --> 00:14:34,390
우리가 수동 레이블링 데이터가 필요하지 않을 때,

232
00:14:34,390 --> 00:14:41,410
우리는 수동 레이블링 없이 원시 데이터를 캡처하고 이를 훈련에

233
00:14:41,410 --> 00:14:43,310
사용할 수 있습니다.

234
00:14:43,310 --> 00:14:47,410
그래서 베이 지역에서 데이터를 수집하는

235
00:14:47,410 --> 00:14:51,550
자율주행차가 많은 이유입니다. 데이터 수집이 가능하고,

236
00:14:51,550 --> 00:14:53,790
실제로 데이터를 주석

237
00:14:53,790 --> 00:14:57,690
달 필요 없이 모델을 훈련할 수 있습니다.

238
00:14:57,690 --> 00:15:03,410
그래서 오늘의 agenda는 이미지 변환에서의

239
00:15:03,410 --> 00:15:06,990
몇 가지 전제 작업을

240
00:15:06,990 --> 00:15:08,570
다룰 것이고,

241
00:15:08,570 --> 00:15:13,490
그 다음에는 이러한 이미지 변환

242
00:15:13,490 --> 00:15:19,330
기반 전제 작업과는 약간 다른 대조적 표현

243
00:15:19,330 --> 00:15:22,730
학습에 관한 알고리즘

244
00:15:22,730 --> 00:15:26,450
세트에 대해 이야기하겠습니다.

245
00:15:26,450 --> 00:15:30,450
그럼 첫 번째 부분부터 시작하겠습니다.

246
00:15:30,450 --> 00:15:36,460
그곳에서 우리는 작업을 하나씩 다룰 것입니다.

247
00:15:36,460 --> 00:15:41,720
회전과 회전 예측에 대해 많이 이야기했습니다.

248
00:15:41,720 --> 00:15:45,820
무작위 또는 임의의 각도로

249
00:15:45,820 --> 00:15:54,300
이미지를 실제로 회전시키고 모델로 회전 각도를 예측할 수

250
00:15:54,300 --> 00:15:56,840
있는지 봅시다.

251
00:15:56,840 --> 00:16:00,860
여기서 우리의 가설은 모델이 객체의 올바른

252
00:16:00,860 --> 00:16:05,380
회전을 인식할 수 있는 것은 객체가 방해받지

253
00:16:05,380 --> 00:16:09,940
않은 상태에서 어떻게 보여야 하는지에 대한

254
00:16:09,940 --> 00:16:14,660
시각적 상식이 있을 때만 가능하다는 것입니다.

255
00:16:14,660 --> 00:16:19,940
이 모델들은 대부분 시각적 상식 개념을

256
00:16:19,940 --> 00:16:23,420
중심으로 설계되었습니다.

257
00:16:23,420 --> 00:16:28,700
모델이 이를 포착할 수 있다면,

258
00:16:28,700 --> 00:16:32,870
전체 이미지를

259
00:16:32,870 --> 00:16:40,050
유용한 특징 집합으로 요약할 수 있다는

260
00:16:40,050 --> 00:16:42,150
의미입니다.

261
00:16:42,150 --> 00:16:48,590
2018년에 발표된 이 논문은 0, 19,

262
00:16:48,590 --> 00:16:53,350
90, 180, 270의 네 가지

263
00:16:53,350 --> 00:17:03,750
다른 각도를 탐색하여 이러한 각도 중 하나로 이미지를 회전시키고, 컨볼루션

264
00:17:03,750 --> 00:17:08,790
신경망을 사용하여 각 회전의

265
00:17:08,790 --> 00:17:12,150
출력을 예측하는 방식으로

266
00:17:12,150 --> 00:17:14,190
구현되었습니다.

267
00:17:14,190 --> 00:17:17,210
네 가지 다른 출력을 생성했기 때문에,

268
00:17:17,210 --> 00:17:19,510
이는 네 가지 다른 경우만

269
00:17:19,510 --> 00:17:21,230
있는 분류 작업입니다.

270
00:17:21,230 --> 00:17:23,990
정확한 각도 값을 예측할

271
00:17:23,990 --> 00:17:26,089
필요는 없습니다.

272
00:17:26,089 --> 00:17:31,280
실제로는 0, 1, 2 또는 3

273
00:17:31,280 --> 00:17:38,016
중 하나의 클래스만 예측하는 것입니다.

274
00:17:38,016 --> 00:17:42,000
y0, 1, 2 또는 3입니다.

275
00:17:42,000 --> 00:17:49,040
그래서 저자들은 좋은 표현을 학습할 수

276
00:17:49,040 --> 00:17:50,920
있었습니다.

277
00:17:50,920 --> 00:17:55,840
그 표현을 가지고, 그들은

278
00:17:55,840 --> 00:17:57,880
기본적으로

279
00:17:57,880 --> 00:18:00,640
인코더와 분류기를

280
00:18:00,640 --> 00:18:07,520
미세 조정하여 신경망을 하위 응용

281
00:18:07,520 --> 00:18:12,740
프로그램에서 훈련하기 시작했습니다.

282
00:18:12,740 --> 00:18:19,560
사실 이 경우, 그들은 첫 번째와 두 번째 층을 고정하고

283
00:18:19,560 --> 00:18:23,560
마지막 합성곱 층과 선형 층을 미세

284
00:18:23,560 --> 00:18:25,060
조정했습니다.

285
00:18:25,060 --> 00:18:32,010
그래서 전체 네트워크를 완전히 미세 조정하는 것은 아니지만,

286
00:18:32,010 --> 00:18:35,910
매우 좋은 결과를 얻을 수 있었습니다.

287
00:18:35,910 --> 00:18:39,890
이것은 우리가 이전에 이야기했던 데이터 세트 중 하나인

288
00:18:39,890 --> 00:18:42,330
CIFAR10 데이터 세트입니다.

289
00:18:42,330 --> 00:18:45,350
모델이 사전 훈련되면 처음부터 좋은

290
00:18:45,350 --> 00:18:48,430
정확도로 시작하는 것을 볼 수 있습니다.

291
00:18:48,430 --> 00:18:52,650
즉, 이미 좋은 상태에 있으며,

292
00:18:52,650 --> 00:18:57,330
처음 몇 번의 반복에서도 객체에 대한

293
00:18:57,330 --> 00:19:01,810
좋은 이해를 가지고 있다는 뜻입니다.

294
00:19:01,810 --> 00:19:04,150
하지만 작업이 충분히 간단하다면,

295
00:19:04,150 --> 00:19:09,250
CIFAR10은 실제로 모델을 훈련하기에 그리 어렵지 않습니다.

296
00:19:09,250 --> 00:19:12,470
작업이 충분히 간단하다면, 감독된 버전,

297
00:19:12,470 --> 00:19:14,810
완전히 감독된 버전과 사전 훈련으로

298
00:19:14,810 --> 00:19:17,510
시작하는 버전은 종종 같은 수치,

299
00:19:17,510 --> 00:19:19,450
같은 정확도로 수렴합니다.

300
00:19:19,450 --> 00:19:22,130
하지만 다시 말하지만, 작업이 충분히

301
00:19:22,130 --> 00:19:25,130
간단하다면, 매우 어려운 응용 프로그램에서는 감독

302
00:19:25,130 --> 00:19:27,390
학습 프레임워크가 사전 훈련,

303
00:19:27,390 --> 00:19:29,590
대규모 사전 훈련을 하지 않으면

304
00:19:29,590 --> 00:19:31,270
좋은 결과를 얻지 못합니다.

305
00:19:31,270 --> 00:19:34,270
알겠습니다.

306
00:19:34,270 --> 00:19:42,590
그들은 또한 분류, 탐지 및 분할을 포함한 여러 작업이

307
00:19:42,590 --> 00:19:51,110
포함된 PASCAL VOC 2017 데이터 세트에

308
00:19:51,110 --> 00:19:53,890
대한 몇 가지

309
00:19:53,890 --> 00:19:59,510
응용 프로그램과 실험을 수행했습니다.

310
00:19:59,510 --> 00:20:03,910
이 세 가지 작업 세트 중에서, 그들은

311
00:20:03,910 --> 00:20:08,470
분류, 탐지 및 분할 작업을 위해 몇

312
00:20:08,470 --> 00:20:10,870
개의 완전 연결 층

313
00:20:10,870 --> 00:20:13,870
또는 모든 층을 훈련하는 다양한

314
00:20:13,870 --> 00:20:16,550
설정을 사용했습니다.

315
00:20:16,550 --> 00:20:19,390
ImageNet 레이블을 살펴보면,

316
00:20:19,390 --> 00:20:23,360
우리가 거대한 레이블 데이터 세트를 가지고

317
00:20:23,360 --> 00:20:27,160
있고 그 데이터 세트에서 사전 훈련을 한다면.

318
00:20:27,160 --> 00:20:29,340
우리는 이미 매우 높은 정확도를 얻습니다.

319
00:20:29,340 --> 00:20:31,440
하지만 다시 말하지만, 이것은

320
00:20:31,440 --> 00:20:34,080
사전 훈련을 위한 모든 레이블이 포함된

321
00:20:34,080 --> 00:20:36,360
ImageNet이라는 점을 명심하세요.

322
00:20:36,360 --> 00:20:40,880
하지만 감독된 사전 훈련을 하지 않고,

323
00:20:40,880 --> 00:20:44,140
사전 훈련이 모두 자기 감독

324
00:20:44,140 --> 00:20:49,760
기반이라면, 이 회전 프레임워크가 다른 많은

325
00:20:49,760 --> 00:20:54,780
방법들보다 훨씬 더 나은 성과를 내고 있다는

326
00:20:54,780 --> 00:21:01,560
것을 보여줍니다. 우리는 그들 중 많은 세부 사항에 대해

327
00:21:01,560 --> 00:21:04,540
깊이 들어가지 않지만,

328
00:21:04,540 --> 00:21:09,720
이 회전 전제 작업에 대한 효능을 보여줍니다.

329
00:21:09,720 --> 00:21:12,800
그리고 가중치의 무작위 초기화로

330
00:21:12,800 --> 00:21:17,960
시작할 때 얼마나 다른지, 얼마나 더 나은지 확인하세요.

331
00:21:17,960 --> 00:21:21,530
그래서 무작위 초기화와 이 회전, 전제

332
00:21:21,530 --> 00:21:24,130
작업으로 사전 훈련을 비교합니다.

333
00:21:24,130 --> 00:21:29,970
차이는 크고, 이 회전 전제 작업은 같지는 않지만

334
00:21:29,970 --> 00:21:33,610
전체 ImageNet에서의 사전

335
00:21:33,610 --> 00:21:35,730
훈련에 가깝습니다.

336
00:21:35,730 --> 00:21:40,610
이 논문에서 그들이 살펴본 또 다른 점은

337
00:21:40,610 --> 00:21:43,410
특징과 학습된 특징이

338
00:21:43,410 --> 00:21:48,850
어떻게 의미가 있는지를 살펴보는 것이었습니다.

339
00:21:48,850 --> 00:21:54,330
전제 작업을 평가하는 방법 중 하나는 일반적으로 자기 지도 학습

340
00:21:54,330 --> 00:21:56,930
프레임워크에서 특징을 살펴보는

341
00:21:56,930 --> 00:21:59,070
것이라고 언급했습니다.

342
00:21:59,070 --> 00:22:01,730
완전 연결층의 특징에서

343
00:22:01,730 --> 00:22:03,730
항상 갈 수 있습니다.

344
00:22:03,730 --> 00:22:07,770
우리는 grad cam과 다른 주의 기반 프레임워크에 대해 이야기했으며,

345
00:22:07,770 --> 00:22:10,690
어떻게 특징에서 이미지 공간으로 돌아갈 수

346
00:22:10,690 --> 00:22:12,330
있는지에 대해 이야기했습니다.

347
00:22:12,330 --> 00:22:15,730
이 평가는 그것들을 이미지 공간으로 투영하고

348
00:22:15,730 --> 00:22:19,380
모델이 무엇을 보고 있는지 확인하는 것을 포함합니다.

349
00:22:19,380 --> 00:22:22,760
감독 모델의 주의 맵을 보면,

350
00:22:22,760 --> 00:22:25,180
감독 모델은 종종 더

351
00:22:25,180 --> 00:22:29,900
집중된 맵을 가지는데, 이는 단일 분류 작업만

352
00:22:29,900 --> 00:22:33,080
해결하려고 하기 때문입니다.

353
00:22:33,080 --> 00:22:36,460
그래서 눈과 그 주변 형태를 포착하면 다른

354
00:22:36,460 --> 00:22:39,460
부분에 대해서는 그다지 신경 쓰지 않습니다.

355
00:22:39,460 --> 00:22:42,640
하지만 자기 지도 학습의 경우, 종종

356
00:22:42,640 --> 00:22:45,940
더 많은 특징과 더 많은 영역이 커버되는데,

357
00:22:45,940 --> 00:22:50,580
이는 우리가 하류 작업이 무엇인지 모르기 때문에

358
00:22:50,580 --> 00:22:52,340
이미지에 대한 더

359
00:22:52,340 --> 00:22:55,460
전체적인 이해가 필요하기 때문입니다.

360
00:22:55,460 --> 00:23:00,060
목표는 많은 작업에서 동등하게 잘 수행하는 것입니다.

361
00:23:00,060 --> 00:23:02,940
그래서 그것이 하나의 작업입니다.

362
00:23:02,940 --> 00:23:05,240
질문이 있다면, 계속해 주세요.

363
00:23:05,240 --> 00:23:08,540
몇 가지 작업을 설명한 후에 멈추겠습니다.

364
00:23:08,540 --> 00:23:10,980
질문이 답변되지 않았다면

365
00:23:10,980 --> 00:23:13,940
기꺼이 답변하겠습니다.

366
00:23:13,940 --> 00:23:16,710
좋습니다.

367
00:23:16,710 --> 00:23:21,910
또 다른 인기 있는 전제 작업은 기본적으로

368
00:23:21,910 --> 00:23:26,910
3x3 그리드를 만들고 네트워크를

369
00:23:26,910 --> 00:23:33,030
사용하여 각 주어진 패치의 위치를 센서 패치에

370
00:23:33,030 --> 00:23:36,310
대해 예측하는 것이었습니다.

371
00:23:36,310 --> 00:23:40,790
따라서 이 패치의 경우, 여기서 출력은 3이어야

372
00:23:40,790 --> 00:23:46,950
합니다. 왜냐하면 우리는 8개만 가지고 있기 때문입니다. 이것은

373
00:23:46,950 --> 00:23:50,450
3x3이고, 중앙 패치는 기준입니다.

374
00:23:50,450 --> 00:23:56,270
그래서 이것은 8방향 분류 작업으로 판명되었습니다.

375
00:23:56,270 --> 00:23:59,270
이 패치 중 하나를

376
00:23:59,270 --> 00:24:04,630
가져와서 주어진 패치의 위치가 중앙

377
00:24:04,630 --> 00:24:11,300
패치에 대해 어떤 것인지 출력하려고 합니다.

378
00:24:16,200 --> 00:24:18,780
그래서 이것은 또 다른 예였습니다.

379
00:24:18,780 --> 00:24:25,040
하지만 이 후속 논문, 즉 이것을 직소 퍼즐

380
00:24:25,040 --> 00:24:29,520
프레임워크로 바꾼 논문은 모델에게 이 8개

381
00:24:29,520 --> 00:24:34,480
패치 중 어떤 것인지 예측하라고 요청하는

382
00:24:34,480 --> 00:24:37,240
대신, 정확한 순열,

383
00:24:37,240 --> 00:24:41,060
올바른 순열을 예측하려고

384
00:24:41,060 --> 00:24:42,020
했습니다.

385
00:24:42,020 --> 00:24:46,160
그들이 한 것은 동일한 3x3

386
00:24:46,160 --> 00:24:50,960
그리드를 사용하고, 모든 패치를 무작위로

387
00:24:50,960 --> 00:24:59,080
섞은 다음, 신경망에게 어떤 것이 올바른 순열인지 말하도록

388
00:24:59,080 --> 00:25:01,480
요청한 것입니다.

389
00:25:01,480 --> 00:25:04,277
그래서 그들은 기본적으로 올바른 순열을 예측합니다.

390
00:25:04,277 --> 00:25:06,360
이 설정에 대해 가질

391
00:25:06,360 --> 00:25:11,840
수 있는 순열의 수가 얼마인지 말해줄 수 있나요?

392
00:25:11,840 --> 00:25:13,250
다시 말해보세요?

393
00:25:13,250 --> 00:25:14,110
9 팩토리얼.

394
00:25:14,110 --> 00:25:14,750
네, 정확히 그렇습니다.

395
00:25:14,750 --> 00:25:16,570
그래서 엄청난 숫자죠?

396
00:25:16,570 --> 00:25:19,930
30만 정도인 것 같아요.

397
00:25:19,930 --> 00:25:22,410
하지만 그들이 한

398
00:25:22,410 --> 00:25:32,130
것은 64개의 그럴듯한 가능한 순열로 구성된 조회 테이블을 만든

399
00:25:32,130 --> 00:25:33,410
것입니다.

400
00:25:33,410 --> 00:25:38,030
그리고 그들은 64개의 순열만 고려합니다.

401
00:25:38,030 --> 00:25:41,450
그리고 그들은 그것을 섞고, 이 64개 중

402
00:25:41,450 --> 00:25:43,230
하나를 기준으로 섞습니다.

403
00:25:43,230 --> 00:25:48,330
그리고 출력은 64 크기의 벡터가 될 것입니다.

404
00:25:48,330 --> 00:25:52,050
그래서 다시 말해, 이것은 64개의 출력

405
00:25:52,050 --> 00:25:56,090
클래스를 가진 간단한 분류 작업으로 판명되었습니다.

406
00:25:56,090 --> 00:25:59,170
그들은 이것이 사전 텍스트 작업을

407
00:25:59,170 --> 00:26:04,470
정의하는 데 훌륭한 아이디어라는 것을 보여주었고, 제가

408
00:26:04,470 --> 00:26:09,130
이야기한 유사한 유형의 작업이 있는 동일한 데이터

409
00:26:09,130 --> 00:26:14,110
세트에서 감독이 어떻게 이루어지는지에 대해 이야기했습니다.

410
00:26:14,110 --> 00:26:17,550
그들은 그들의 방법이 이전의

411
00:26:17,550 --> 00:26:25,190
일부 모델, 이전의 프레임워크보다 성능이 뛰어났음을 보여주었습니다.

412
00:26:25,190 --> 00:26:30,870
그리고 다시 말하지만, 이것은 2016년에 발표되었습니다.

413
00:26:30,870 --> 00:26:36,590
그래서 다음 사전 텍스트 작업은 결손 부분을 예측하는

414
00:26:36,590 --> 00:26:38,710
인페인팅입니다.

415
00:26:38,710 --> 00:26:44,310
그들이 여기서 한 것은 간단한 마스킹 전략으로, 이미지의 일부를

416
00:26:44,310 --> 00:26:47,590
마스킹한 다음 모델에게 마스킹된

417
00:26:47,590 --> 00:26:51,230
부분을 인페인팅하도록 요청하는 것입니다.

418
00:26:51,230 --> 00:26:57,010
어떻게 수행되었는지, 입력 이미지에 간단한 마스킹을 했습니다.

419
00:26:57,010 --> 00:27:00,170
하지만 우리는 모든 이미지를 가지고 있기 때문에

420
00:27:00,170 --> 00:27:02,790
실제로 원하는 출력을 가지고 있습니다.

421
00:27:02,790 --> 00:27:06,610
그래서 인코더가 이것을 특징 공간으로

422
00:27:06,610 --> 00:27:08,816
변환하고, 그 특징 공간에는

423
00:27:08,816 --> 00:27:12,300
중간에 몇 개의 완전 연결 층이 있으며,

424
00:27:12,300 --> 00:27:14,840
결손된 부분을 디코딩하는

425
00:27:14,840 --> 00:27:16,920
디코더가 있습니다.

426
00:27:16,920 --> 00:27:22,760
그리고 손실 함수는 출력을

427
00:27:22,760 --> 00:27:27,720
실제 정답과 비교합니다.

428
00:27:27,720 --> 00:27:32,680
이것은 기본적으로 결손 픽셀을 재구성하는 것을

429
00:27:32,680 --> 00:27:34,560
배우는 것입니다.

430
00:27:34,560 --> 00:27:38,800
다시 말해, 우리는 이전에 몇 번

431
00:27:38,800 --> 00:27:43,360
오토인코더에 대해 이야기했으며, 이것은 입력

432
00:27:43,360 --> 00:27:47,720
이미지를 디코드할 출력을 원하는 표현으로

433
00:27:47,720 --> 00:27:51,860
인코딩하는 오토인코더의 일종입니다.

434
00:27:51,860 --> 00:27:57,600
하지만 이 오토인코더는 마스킹 전략, 마스킹 목표로

435
00:27:57,600 --> 00:27:58,760
훈련됩니다.

436
00:27:58,760 --> 00:28:03,080
그래서 몇 가지 예를 보여주기 위해, 인페인팅

437
00:28:03,080 --> 00:28:07,930
평가가 조금 흥미롭고 까다로운데, 왜냐하면 인페인팅을

438
00:28:07,930 --> 00:28:10,710
원할 때 이 이미지를 인페인팅하는

439
00:28:10,710 --> 00:28:13,630
방법이 너무 많기 때문입니다.

440
00:28:13,630 --> 00:28:26,570
그리고 우리는 이 경우에 단 하나의 출력을 할 수 있다고 말할 수 있습니다.

441
00:28:26,570 --> 00:28:28,770
재구성 기반

442
00:28:28,770 --> 00:28:31,370
프레임워크는 초기 재구성

443
00:28:31,370 --> 00:28:42,410
기반 프레임워크가 실제로 많은 흐릿하고 매우 부드러운 출력을 생성하고
있었습니다.

444
00:28:42,410 --> 00:28:49,530
그래서 제가 여기서 언급하는 이 논문은 추가적인 적대적 목표 함수를

445
00:28:49,530 --> 00:28:54,250
사용하고 있었는데, 이는 다음 강의의 주제인

446
00:28:54,250 --> 00:28:56,890
생성 모델에 대한 논의이기

447
00:28:56,890 --> 00:29:00,950
때문에 자세히 설명하지 않겠습니다.

448
00:29:00,950 --> 00:29:05,730
하지만 일반적으로 이러한 프레임워크가 작동하는 방식은

449
00:29:05,730 --> 00:29:07,680
재구성 손실이 있습니다.

450
00:29:07,680 --> 00:29:09,580
재구성 손실은

451
00:29:09,580 --> 00:29:13,620
기본적으로 패치, 즉

452
00:29:13,620 --> 00:29:18,660
이미지 x와 인코더를 통과한

453
00:29:18,660 --> 00:29:24,340
후의 이미지 간의 차이를 계산합니다.

454
00:29:24,340 --> 00:29:33,340
그리고 이것은 요소별 곱셈입니다.

455
00:29:33,340 --> 00:29:35,740
우리는 손실 함수, 즉 목표

456
00:29:35,740 --> 00:29:38,300
함수를 마스크된 영역에서만 계산하고

457
00:29:38,300 --> 00:29:41,520
싶기 때문에 여기에도 마스크가 있습니다.

458
00:29:41,520 --> 00:29:45,180
그래서 마스크와 요소별 곱셈을

459
00:29:45,180 --> 00:29:46,020
수행합니다.

460
00:29:46,020 --> 00:29:51,460
이것은 우리가 가진 마스크의 해당 부분에

461
00:29:51,460 --> 00:29:55,800
대한 재구성 손실을 제공합니다.

462
00:29:55,800 --> 00:29:59,260
제가 말했듯이, 이는 생성된

463
00:29:59,260 --> 00:30:02,820
이미지가 실제처럼 보이도록

464
00:30:02,820 --> 00:30:09,070
보장하는 적대적 목표 적대적 학습 손실 함수로

465
00:30:09,070 --> 00:30:10,910
보완됩니다.

466
00:30:10,910 --> 00:30:17,350
그래서 이를 통해 재구성된 부분이

467
00:30:17,350 --> 00:30:24,830
조금 더 나아 보이도록 개선할 수 있었습니다.

468
00:30:24,830 --> 00:30:31,630
하지만 다시 말하지만, 세부 사항은 다음 강의에서 논의될 것입니다.

469
00:30:31,630 --> 00:30:35,990
이 재구성 프레임워크는 실제로

470
00:30:35,990 --> 00:30:39,550
동일한 데이터 세트에서

471
00:30:39,550 --> 00:30:43,190
동일한 분류, 탐지 및

472
00:30:43,190 --> 00:30:46,830
분할 작업을 수행할 때

473
00:30:46,830 --> 00:30:52,710
추가적인 이점을 제공할 수 있었습니다.

474
00:30:52,710 --> 00:30:56,710
이 재구성 기반 프레임워크와 마스킹에 대해

475
00:30:56,710 --> 00:30:59,110
다시 이야기할 것이며,

476
00:30:59,110 --> 00:31:05,440
이는 요즘 사전 훈련에 사용되는 가장 많이 사용되는 모델 또는

477
00:31:05,440 --> 00:31:07,800
전제 작업 중 하나입니다.

478
00:31:07,800 --> 00:31:10,140
그래서 다시 돌아오겠습니다.

479
00:31:10,140 --> 00:31:15,520
하지만 그 전에 이미지 색칠이라는 다른

480
00:31:15,520 --> 00:31:19,200
전제 작업을 소개하겠습니다.

481
00:31:19,200 --> 00:31:24,400
그리고 이것은 우리가 주로 색상 이미지로 구성된 데이터 세트를

482
00:31:24,400 --> 00:31:30,023
가지고 있기 때문에 색상 이미지를 변환하는 매우 간단한 프레임워크

483
00:31:30,023 --> 00:31:30,940
설정입니다.

484
00:31:30,940 --> 00:31:36,680
우리는 그 컬러 이미지를 밝기와 색상을

485
00:31:36,680 --> 00:31:41,880
분리하는 구성 요소 또는 채널로 변환합니다.

486
00:31:41,880 --> 00:31:42,500
분리하는 구성 요소 또는 채널로 변환합니다.

487
00:31:42,500 --> 00:31:44,560
컴퓨터 그래픽스나 CS-131,

488
00:31:44,560 --> 00:31:47,160
다른 컴퓨터 비전 수업을

489
00:31:47,160 --> 00:31:51,060
들었다면 여러 색 공간이 있다는 것을 알 것입니다.

490
00:31:51,060 --> 00:31:54,900
다양한 색 공간이 많이 있다는 것을 알고 있습니다.

491
00:31:54,900 --> 00:31:57,960
주로 컴퓨터 비전에서는 RGB를 사용합니다.

492
00:31:57,960 --> 00:32:02,130
하지만 밝기와 색상을 분리하고 싶다면

493
00:32:02,130 --> 00:32:04,830
다른 색 공간이 있습니다.

494
00:32:04,830 --> 00:32:08,330
예를 들어, LAB 색 공간은

495
00:32:08,330 --> 00:32:12,930
밝기를 색상과 분리하는 색 공간 중

496
00:32:12,930 --> 00:32:14,410
하나입니다.

497
00:32:14,410 --> 00:32:18,170
그래서 우리는 밝기를 위한 하나의 채널과 실제

498
00:32:18,170 --> 00:32:21,970
색상을 정의하기 위한 두 개의 채널을 가집니다.

499
00:32:21,970 --> 00:32:27,897
그리고 L, A, B 이 세 채널을 모두 더하면 실제로

500
00:32:27,897 --> 00:32:29,730
컬러 이미지를

501
00:32:29,730 --> 00:32:31,850
얻을 수 있습니다.

502
00:32:31,850 --> 00:32:34,770
그래서 여기서의 전제 작업은 간단합니다.

503
00:32:34,770 --> 00:32:40,690
L 채널이 주어지면 A와 B 채널을 예측하는 것입니다, 맞죠?

504
00:32:40,690 --> 00:32:44,310
다시 말해, 우리는 수동 주석을 할 필요가 없습니다.

505
00:32:44,310 --> 00:32:47,530
이미 데이터에 포함되어 있습니다.

506
00:32:47,530 --> 00:32:52,150
그리고 이것은 다른 프레임워크로 확장되었습니다.

507
00:32:52,150 --> 00:32:56,730
왜 L을 주고 A와 B를 예측하는 것만 봐야 할까요?

508
00:32:56,730 --> 00:32:58,860
우리는 반대로도 할 수 있습니다, 맞죠?

509
00:32:58,860 --> 00:33:03,180
그리고 이것은 우리가 분할이라고 부르는

510
00:33:03,180 --> 00:33:06,660
것을 이끌어냈습니다. 분할 뇌

511
00:33:06,660 --> 00:33:12,300
오토인코더에서 입력 이미지는 분할되어 하나로 변환됩니다.

512
00:33:12,300 --> 00:33:16,140
L 채널, 밝기 채널, 색상 채널입니다.

513
00:33:16,140 --> 00:33:18,480
이 두 이미지는 하나의 채널입니다.

514
00:33:18,480 --> 00:33:20,340
이것은 두 개의 색상 채널입니다.

515
00:33:20,340 --> 00:33:24,120
그리고 우리는 두 개의 함수, 두 개의 신경망, 레이어

516
00:33:24,120 --> 00:33:26,780
집합을 훈련시켜 다른 하나를 예측합니다.

517
00:33:26,780 --> 00:33:30,780
그리고 마지막에 손실 함수를 계산하고

518
00:33:30,780 --> 00:33:34,300
역전파를 하기 위해, 우리는 이 두 개를

519
00:33:34,300 --> 00:33:39,600
병합하여 실제 이미지를 생성하고 L2 손실을 계산합니다.

520
00:33:39,600 --> 00:33:42,900
어떤 거리 함수도 이 신경망의 훈련에

521
00:33:42,900 --> 00:33:45,180
도움이 될 수 있습니다.

522
00:33:45,180 --> 00:33:49,560
보다 일반적인 프레임워크나 공식에서, 주어진

523
00:33:49,560 --> 00:33:55,280
하나의 채널 또는 채널 집합으로 다른 채널을 예측하고

524
00:33:55,280 --> 00:34:00,920
X2에 대해서도 동일하게 수행하는 아이디어입니다.

525
00:34:00,920 --> 00:34:04,760
따라서 채널 집합 X1, 채널 집합 X2입니다.

526
00:34:04,760 --> 00:34:07,440
하나가 주어지면 다른 하나를 예측할 수 있으며,

527
00:34:07,440 --> 00:34:09,360
이것들이 해당 신경망입니다.

528
00:34:09,360 --> 00:34:16,159
병합하면 이미지를 얻고 손실 함수는

529
00:34:16,159 --> 00:34:18,800
간단해질 것입니다.

530
00:34:18,800 --> 00:34:21,120
따라서 이러한 프레임워크가 있다면

531
00:34:21,120 --> 00:34:25,470
색상과 조명뿐만 아니라 모든 것에 적용할 수 있습니다.

532
00:34:25,470 --> 00:34:32,040
우리는 RGB 채널과 깊이 채널이 있는 RGBD 센서의 데이터를 가질

533
00:34:32,040 --> 00:34:35,980
수 있습니다. 예를 들어, Kinect와

534
00:34:35,980 --> 00:34:38,880
로봇 공학에서 사용하는 다른

535
00:34:38,880 --> 00:34:40,719
센서들이 있습니다.

536
00:34:40,719 --> 00:34:46,400
RGB 채널을 주면 깊이를 예측하고 그 반대도

537
00:34:46,400 --> 00:34:47,320
가능합니다.

538
00:34:47,320 --> 00:34:51,000
그리고 이것은 다양한 응용 프로그램에

539
00:34:51,000 --> 00:34:54,800
사용된 매우 성공적인 하위 작업이었습니다.

540
00:34:54,800 --> 00:34:59,770
그리고 이 모델과 제가 방금

541
00:34:59,770 --> 00:35:06,330
언급한 논문, 즉 색상을 예측하여 이미지를

542
00:35:06,330 --> 00:35:12,150
색칠하는 모델은 그 특징들 자체입니다.

543
00:35:12,150 --> 00:35:14,330
실제로 클래스 레이블을

544
00:35:14,330 --> 00:35:19,110
예측하는 데 매우 높은 정확도를 가지고 있습니다.

545
00:35:19,110 --> 00:35:22,650
그리고 비교 측면에서도

546
00:35:22,650 --> 00:35:29,250
사용되는 다양한 다른 프레임워크가 많이 있습니다.

547
00:35:29,250 --> 00:35:32,290
다시 말하지만, 여기에는

548
00:35:32,290 --> 00:35:36,170
레이블이 없기 때문에 감독 학습만큼

549
00:35:36,170 --> 00:35:40,450
좋지 않습니다. 이는 F1과 F2의

550
00:35:40,450 --> 00:35:43,690
연결된 특징을 기반으로 합니다.

551
00:35:43,690 --> 00:35:49,090
좋습니다, 그래서 이미지 색칠의 전제 작업은 실제로

552
00:35:49,090 --> 00:35:55,100
매우 흥미로웠습니다. 왜냐하면 이제 우리는 신경망을

553
00:35:55,100 --> 00:35:57,520
사전 훈련하는 데만 사용할

554
00:35:57,520 --> 00:35:59,600
수 있는 것이 아니라,

555
00:35:59,600 --> 00:36:03,460
색상이 없는 이미지를 색칠할 수

556
00:36:03,460 --> 00:36:05,560
있었기 때문입니다.

557
00:36:05,560 --> 00:36:09,900
그래서 우리는 색상이 없는

558
00:36:09,900 --> 00:36:15,100
이미지와 비디오를 색칠할 수 있었습니다.

559
00:36:15,100 --> 00:36:18,420
그리고 그뿐만 아니라, 그들이

560
00:36:18,420 --> 00:36:21,220
논문에서 보여준 또 다른

561
00:36:21,220 --> 00:36:27,900
흥미로운 결과 중 하나는 색칠된 요세미티와 하프 돔의 이미지였습니다.

562
00:36:27,900 --> 00:36:32,540
이 이미지에서 볼 수 있는 흥미로운 점은

563
00:36:32,540 --> 00:36:36,720
실제 물체인 하프 돔이나 나무,

564
00:36:36,720 --> 00:36:40,100
다리와 물속의 반사 사이의

565
00:36:40,100 --> 00:36:41,420
일관성입니다.

566
00:36:41,420 --> 00:36:44,700
그래서 모델은 이 반사가 어떻게

567
00:36:44,700 --> 00:36:47,380
훈련되었는지에 따라

568
00:36:47,380 --> 00:36:51,430
색상을 보존해야 한다는 것을 이해할

569
00:36:51,430 --> 00:36:53,430
수 있었습니다.

570
00:36:53,430 --> 00:36:57,510
다시 말하지만, 이 모델들은 모두 대규모 언어,

571
00:36:57,510 --> 00:37:01,690
대규모 비전 모델의 사전 훈련 모델입니다.

572
00:37:01,690 --> 00:37:04,390
그리고 그들은 특정 작업에 대해 훈련되었습니다.

573
00:37:04,390 --> 00:37:09,830
그래서 그들은 모든 것을 해결하기 위해 훈련되지 않았습니다.

574
00:37:09,830 --> 00:37:14,250
따라서 이것은 실제로 비디오 설정으로 확장될 수 있습니다.

575
00:37:14,250 --> 00:37:17,590
왜냐하면 이제 비디오가 있다면 색상이

576
00:37:17,590 --> 00:37:21,270
있는 참조 프레임을 가지고 후속 프레임의 색칠을

577
00:37:21,270 --> 00:37:23,330
할 수 있기 때문입니다.

578
00:37:23,330 --> 00:37:26,750
이것은 매우 간단합니다.

579
00:37:26,750 --> 00:37:32,350
왜냐하면 비디오에서 미래 프레임을 색칠하는 것은

580
00:37:32,350 --> 00:37:34,510
기본적으로 픽셀과

581
00:37:34,510 --> 00:37:41,870
객체를 추적하려고 하기 때문입니다. 모델은 이러한 추적이

582
00:37:41,870 --> 00:37:44,390
어떻게 형성되어야 하는지를

583
00:37:44,390 --> 00:37:47,570
암묵적으로 학습합니다.

584
00:37:47,570 --> 00:37:51,040
따라서 가설은 비디오 프레임을 색칠하는 것을 학습하면

585
00:37:51,040 --> 00:37:54,200
모델이 레이블 없이 영역이나 객체를 추적하는 방법을

586
00:37:54,200 --> 00:37:56,610
학습할 수 있어야 한다는 것입니다.

587
00:37:59,720 --> 00:38:02,600
비디오를 색칠하는 것은 많은

588
00:38:02,600 --> 00:38:06,740
대응 관계가 있기 때문에 흥미로운 작업입니다.

589
00:38:06,740 --> 00:38:10,660
자세한 내용을 살펴보는 것을 추천합니다.

590
00:38:10,660 --> 00:38:13,760
간단히 그들에 대해 이야기하겠습니다.

591
00:38:13,760 --> 00:38:16,960
참조 프레임이 있다면, 입력

592
00:38:16,960 --> 00:38:20,180
프레임의 색상을 지정하기 위해

593
00:38:20,180 --> 00:38:23,360
특정 객체나 픽셀이 어디에

594
00:38:23,360 --> 00:38:26,520
있는지 포인터를 찾아야 합니다.

595
00:38:26,520 --> 00:38:28,680
그런 다음 이를

596
00:38:28,680 --> 00:38:31,760
바탕으로 색상을 확인하고

597
00:38:31,760 --> 00:38:36,280
해당 픽셀의 출력 색상으로 복사합니다.

598
00:38:36,280 --> 00:38:39,600
이것이 이루어지는 방식은 우리가 이야기한

599
00:38:39,600 --> 00:38:43,680
주의(attention) 주제와 매우 유사합니다.

600
00:38:43,680 --> 00:38:48,130
각 입력 프레임, 즉 참조 프레임과 목표

601
00:38:48,130 --> 00:38:51,730
프레임의 각 픽셀에 대한 주의를

602
00:38:51,730 --> 00:38:53,610
형성하는 것입니다.

603
00:38:53,610 --> 00:38:58,450
우리는 종종 CNN을 실행하여 해당 픽셀 주변의 어떤 특징을 사용할지

604
00:38:58,450 --> 00:38:59,550
확인합니다.

605
00:38:59,550 --> 00:39:02,970
그 특징을 사용하여 각

606
00:39:02,970 --> 00:39:05,330
목표 픽셀에 대해

607
00:39:05,330 --> 00:39:09,370
참조 프레임의 모든 픽셀과의 주의

608
00:39:09,370 --> 00:39:13,970
또는 거리를 계산할 수 있습니다.

609
00:39:13,970 --> 00:39:18,530
그런 다음 관심 있는

610
00:39:18,530 --> 00:39:25,930
픽셀에 대한 주의를 정의한 후 참조

611
00:39:25,930 --> 00:39:31,510
프레임의 모든 픽셀과 비교합니다.

612
00:39:31,510 --> 00:39:37,050
이제 이러한 주의 모듈을 기반으로 모든 픽셀의 평균 색상을

613
00:39:37,050 --> 00:39:38,830
구할 수 있습니다.

614
00:39:38,830 --> 00:39:42,170
주요 개념은 두 개체 간의 유사성입니다.

615
00:39:42,170 --> 00:39:45,130
어쨌든 이를 통해

616
00:39:45,130 --> 00:39:48,180
우리는 평균으로

617
00:39:48,180 --> 00:39:55,300
출력 색상을 얻고, 결국 색상의 값이 데이터에

618
00:39:55,300 --> 00:40:00,360
있는 올바른 색상으로 손실

619
00:40:00,360 --> 00:40:04,580
함수를 계산할 수 있습니다.

620
00:40:04,580 --> 00:40:08,220
이 참조 프레임을 통해 이미지를

621
00:40:08,220 --> 00:40:10,280
색칠할 수 있었습니다.

622
00:40:10,280 --> 00:40:15,480
색칠하는 데 있어 얼마나 일관성이 있는지 볼 수 있습니다.

623
00:40:15,480 --> 00:40:18,620
시간에 따라 일관성이

624
00:40:18,620 --> 00:40:25,100
없으면, 예를 들어 사람의 셔츠나

625
00:40:25,100 --> 00:40:32,180
의상이 색상이 바뀌는 것을 자주 볼 수

626
00:40:32,180 --> 00:40:34,220
있습니다.

627
00:40:34,220 --> 00:40:38,780
또한 참조 프레임에 대한 주의를 계산함으로써

628
00:40:38,780 --> 00:40:42,500
객체를 추적하고, 비디오에서

629
00:40:42,500 --> 00:40:44,750
세그먼트를 추적하며,

630
00:40:44,750 --> 00:40:48,670
심지어 비디오의 주요 포인트를 식별할

631
00:40:48,670 --> 00:40:54,710
수 있는 매우 흥미로운 응용 프로그램이 있었습니다.

632
00:40:54,710 --> 00:40:56,010
좋은 질문입니다.

633
00:40:56,010 --> 00:41:03,030
당신이 묻는 질문은 이 슬라이드에 관한 것이며,

634
00:41:03,030 --> 00:41:09,270
인코더가 처음에 데이터를 어떻게 알고 좋은 학습

635
00:41:09,270 --> 00:41:13,450
표현을 얻는지에 대한 것입니다.

636
00:41:13,450 --> 00:41:18,870
제가 제시하고 정의한 모든 작업은

637
00:41:18,870 --> 00:41:26,030
디코딩, 분류 또는 회귀를 사용하여 출력을

638
00:41:26,030 --> 00:41:30,950
생성하여 이 인코더를 훈련시키려는

639
00:41:30,950 --> 00:41:32,730
것입니다.

640
00:41:32,730 --> 00:41:34,830
만약 원본 이미지가

641
00:41:34,830 --> 00:41:37,990
인터넷이나 ImageNet 등에서 가져온

642
00:41:37,990 --> 00:41:41,350
자연 이미지라면, 당신은 이러한

643
00:41:41,350 --> 00:41:45,730
유형의 이미지에서 특징을 추출할 수 있는 인코더를

644
00:41:45,730 --> 00:41:47,330
학습하고 있습니다.

645
00:41:47,330 --> 00:41:50,410
그런 다음 디코더를 제거하고 끝에 이

646
00:41:50,410 --> 00:41:52,990
분류기를 추가하면, 이 부분만

647
00:41:52,990 --> 00:41:56,310
훈련하면 됩니다. 왜냐하면 이 인코더는

648
00:41:56,310 --> 00:41:58,930
제가 방금 이야기한 모든 사전

649
00:41:58,930 --> 00:42:02,467
훈련 작업으로 이미 훈련되었기 때문입니다.

650
00:42:02,467 --> 00:42:04,050
당신은 레이블이

651
00:42:04,050 --> 00:42:07,290
인코더의 사전 훈련을 위해 디코더에서 오는지

652
00:42:07,290 --> 00:42:09,950
묻고 있으며, 그 대답은 예입니다.

653
00:42:09,950 --> 00:42:12,550
그래서 우리는 사전 텍스트 작업을 정의합니다.

654
00:42:12,550 --> 00:42:16,030
왜냐하면 우리는 레이블, 즉 출력을 원하기 때문입니다.

655
00:42:16,030 --> 00:42:20,130
그런 다음 이러한 출력을 바탕으로 이 전체

656
00:42:20,130 --> 00:42:23,270
네트워크를 훈련하려고 합니다.

657
00:42:23,270 --> 00:42:28,490
올바른 레이블을 예측하는 과정에서 이

658
00:42:28,490 --> 00:42:31,290
인코더도 훈련됩니다.

659
00:42:31,290 --> 00:42:32,070
좋은 질문입니다.

660
00:42:32,070 --> 00:42:34,690
당신은 인코더와 디코더가 하나의 큰

661
00:42:34,690 --> 00:42:37,610
신경망인지 아니면 차이가 있는지 묻고 있습니다.

662
00:42:37,610 --> 00:42:41,120
다양한 논문과 작업에서

663
00:42:41,120 --> 00:42:44,380
완전히 다릅니다.

664
00:42:44,380 --> 00:42:47,800
어떤 경우에는 인코더와 디코더가 다르기 때문에 저는

665
00:42:47,800 --> 00:42:49,700
이를 분류기라고 부릅니다.

666
00:42:49,700 --> 00:42:54,940
제가 예측한 정도에 대한 예시에서, 이것은

667
00:42:54,940 --> 00:42:57,060
단순한 신경망입니다.

668
00:42:57,060 --> 00:42:59,680
디코더는 이러한 FC 레이어입니다.

669
00:42:59,680 --> 00:43:02,700
그래서 이것은 하나의 전체 네트워크가 될 수 있습니다.

670
00:43:02,700 --> 00:43:04,980
그런 다음 이를 다운스트림 작업을 위해

671
00:43:04,980 --> 00:43:06,580
다른 것으로 교체합니다.

672
00:43:06,580 --> 00:43:09,580
하지만 어떤 경우에는, 예를 들어

673
00:43:09,580 --> 00:43:14,500
이미지를 자동 인코딩하고 다른 이미지를 디코딩할 때,

674
00:43:14,500 --> 00:43:16,620
중간의 표현 공간을

675
00:43:16,620 --> 00:43:19,580
활용하고자 하므로 종종 끝에서 끝으로

676
00:43:19,580 --> 00:43:21,500
훈련된 두 개의 신경망이

677
00:43:21,500 --> 00:43:22,660
있습니다.

678
00:43:22,660 --> 00:43:26,200
그리고 제가 다음에 이야기하고 싶은 것은

679
00:43:26,200 --> 00:43:31,260
마스크 자동 인코더인데, 인코더와 디코더 사이에 대칭이

680
00:43:31,260 --> 00:43:32,320
없습니다.

681
00:43:32,320 --> 00:43:35,660
그들은 단순히 두 개의 다른 프레임워크,

682
00:43:35,660 --> 00:43:39,590
두 개의 다른 신경망일 수 있으며,

683
00:43:39,590 --> 00:43:43,330
작업을 훈련하기 위한 대칭이 없어도 됩니다.

684
00:43:43,330 --> 00:43:45,490
그래서 이것은 매우 작업 의존적이며,

685
00:43:45,490 --> 00:43:47,150
전제 작업에 의존적입니다.

686
00:43:47,150 --> 00:43:51,470
하지만 우리가 아는 동일한 아키텍처, 예를 들어

687
00:43:51,470 --> 00:43:55,230
CNN이나 ResNet에 속할 수도

688
00:43:55,230 --> 00:43:58,310
있고, 대칭 없이 두 개의 다른

689
00:43:58,310 --> 00:44:00,390
아키텍처일 수도 있습니다.

690
00:44:00,390 --> 00:44:05,990
이것들은 자기 지도 학습을 위한 최초의 방법이라는

691
00:44:05,990 --> 00:44:08,270
것을 기억하세요.

692
00:44:08,270 --> 00:44:11,070
그래서 모든 것을 해결할 수 있는 것은 아닙니다.

693
00:44:11,070 --> 00:44:15,070
그것은 간단한 면책 조항이지만, 여기서의

694
00:44:15,070 --> 00:44:17,230
아이디어는, 모델이 이것이

695
00:44:17,230 --> 00:44:20,250
90도 회전되었다고 말할 수

696
00:44:20,250 --> 00:44:22,790
있다면, 암묵적으로 올바른

697
00:44:22,790 --> 00:44:28,970
회전, 즉 올바른 방향과 방향성을 이해하고 있다는 것을 의미합니다.

698
00:44:28,970 --> 00:44:33,870
그리고 나서 회전되지 않은 이미지를

699
00:44:33,870 --> 00:44:39,360
주면, 그 안에 무엇이 있는지 인식할

700
00:44:39,360 --> 00:44:42,260
수 있게 됩니다.

701
00:44:42,260 --> 00:44:46,440
하지만 이것은 그 자체로 제한된 작업이며, 저도 동의합니다.

702
00:44:46,440 --> 00:44:49,240
여기서 그들이 64를 사용하는 이유는 무엇인가요?

703
00:44:52,680 --> 00:44:54,480
좋은 질문입니다.

704
00:44:54,480 --> 00:44:59,000
하지만 그것은 거의 임의의 선택이기도 합니다.

705
00:44:59,000 --> 00:45:01,520
제가 말했듯이, 여기에는

706
00:45:01,520 --> 00:45:05,920
9!의 다양한 종류의 다른 순열이 있습니다.

707
00:45:05,920 --> 00:45:07,520
그래서 이는 매우 큰 숫자입니다.

708
00:45:07,520 --> 00:45:10,620
우리가 모든 것을 예측하는 것은 의미가 없습니다.

709
00:45:10,620 --> 00:45:12,800
저자들이 여기서 한

710
00:45:12,800 --> 00:45:16,400
것은, 충분한 변동성이 있는 몇 가지

711
00:45:16,400 --> 00:45:19,080
교란을 선택하기로 결정한

712
00:45:19,080 --> 00:45:24,420
것입니다. 많은 교란은 단지 하나의 패치만 전환된 것입니다.

713
00:45:24,420 --> 00:45:30,200
그래서 그들은 서로 간의 차이가 가장 큰 64개를

714
00:45:30,200 --> 00:45:31,660
선택했고, 다른

715
00:45:31,660 --> 00:45:33,840
유형의 작업 대신

716
00:45:33,840 --> 00:45:36,290
분류 문제를 해결하고자

717
00:45:36,290 --> 00:45:38,960
64개를 선택했습니다.

718
00:45:41,570 --> 00:45:42,410
좋습니다.

719
00:45:42,410 --> 00:45:47,730
그래서 저는 종종 이미지나 비디오에 어떤

720
00:45:47,730 --> 00:45:54,130
변환을 수행하는 이러한 프레임워크에 대해 이야기하고

721
00:45:54,130 --> 00:45:55,070
있었습니다.

722
00:45:55,070 --> 00:45:58,630
그리고 이것은 2021년에 발표된 새로운

723
00:45:58,630 --> 00:46:00,950
프레임워크로 이어집니다.

724
00:46:00,950 --> 00:46:05,250
그리고 이에 대한 많은 후속 연구가 있었으며,

725
00:46:05,250 --> 00:46:11,670
이는 많은 작업을 위한 사전 훈련에 훌륭한 프레임워크가 되었습니다.

726
00:46:11,670 --> 00:46:14,250
요즘 원시 데이터

727
00:46:14,250 --> 00:46:21,850
세트에서 사전 훈련을 원할 때, 우리는 종종 이 MAE 프레임워크를
사용합니다.

728
00:46:21,850 --> 00:46:24,410
이것은 마스크 자동 인코더라고 불립니다.

729
00:46:24,410 --> 00:46:27,610
이것은 제가 언급한 페인팅의

730
00:46:27,610 --> 00:46:30,670
마스킹 전략과 유사한 재구성

731
00:46:30,670 --> 00:46:34,900
기반 프레임워크이지만, 훨씬 더 자세합니다.

732
00:46:34,900 --> 00:46:39,100
그리고 보시다시피, 이 프레임워크는 단지 하나의

733
00:46:39,100 --> 00:46:41,680
마스크를 선택하는 것이 아닙니다.

734
00:46:41,680 --> 00:46:44,460
그들은 훨씬 더 공격적인

735
00:46:44,460 --> 00:46:48,940
샘플링 비율인 50% 마스킹 또는 75%

736
00:46:48,940 --> 00:46:54,900
마스킹 비율로 마스킹하는 다양한 패치와 장소가 있습니다.

737
00:46:54,900 --> 00:46:58,960
그리고 대규모 훈련을 통해, 그들은

738
00:46:58,960 --> 00:47:00,900
마스킹된 모든 영역을

739
00:47:00,900 --> 00:47:05,140
재구성할 수 있을 뿐만 아니라,

740
00:47:05,140 --> 00:47:11,220
이미지를 좋은 특징으로 요약할 수 있는 매우 좋은 인코더를

741
00:47:11,220 --> 00:47:14,340
얻고 있음을 보여주었습니다.

742
00:47:14,340 --> 00:47:20,340
이것이 어떻게 이루어졌는지는 인코더와 디코더를 정의함으로써

743
00:47:20,340 --> 00:47:22,358
이루어졌습니다.

744
00:47:22,358 --> 00:47:23,900
그리고 이것이 제가

745
00:47:23,900 --> 00:47:26,700
말한 비대칭적인 예 중 하나입니다.

746
00:47:26,700 --> 00:47:30,900
인코더와 디코더는 입력 마스크의 큰

747
00:47:30,900 --> 00:47:32,750
부분을 차지합니다.

748
00:47:32,750 --> 00:47:37,110
입력 패치가 마스킹되고, 마스킹되지

749
00:47:37,110 --> 00:47:43,470
않은 것들은 인코더에 주어져 특징으로 인코딩된

750
00:47:43,470 --> 00:47:49,430
후 디코더를 통해 완전한 이미지를 생성합니다.

751
00:47:49,430 --> 00:47:53,890
하지만 이것이 의미하는 바에 대해 조금 더 자세히 들어가 보겠습니다.

752
00:47:53,890 --> 00:47:56,510
여기서 이러한 모델들이 어떻게

753
00:47:56,510 --> 00:47:59,030
훈련되는지에 대한 세부사항이 있습니다.

754
00:47:59,030 --> 00:48:04,590
하지만 저는 이러한 모델들이 어떻게 작동하는지

755
00:48:04,590 --> 00:48:07,750
간단히 설명했습니다.

756
00:48:07,750 --> 00:48:13,350
여기 인코더는 ViT와 매우 유사합니다.

757
00:48:13,350 --> 00:48:16,990
모두 우리가 이야기한 ViT를 기반으로

758
00:48:16,990 --> 00:48:18,870
한 트랜스포머입니다.

759
00:48:18,870 --> 00:48:23,090
ViT와 유사하게 이미지는 패치로 나뉩니다.

760
00:48:23,090 --> 00:48:29,560
패치는 샘플링되며, 균일 샘플링이

761
00:48:29,560 --> 00:48:32,960
이루어졌고, 실험에서

762
00:48:32,960 --> 00:48:40,240
75% 샘플링이 꽤 효율적임을 보여주었습니다.

763
00:48:40,240 --> 00:48:45,520
그리고 높은 마스킹 비율을 사용하여

764
00:48:45,520 --> 00:48:50,440
예측 작업을 매우 도전적으로 만듭니다.

765
00:48:50,440 --> 00:48:54,040
자기 지도 학습의 전제

766
00:48:54,040 --> 00:48:56,880
작업에서 이 작업은 의미가

767
00:48:56,880 --> 00:48:58,700
있습니다.

768
00:48:58,700 --> 00:49:00,640
이 작업은 모델이 이를

769
00:49:00,640 --> 00:49:05,560
재구성할 수 있는 좋은 특징을 학습해야 하기 때문에 매우 좋습니다.

770
00:49:05,560 --> 00:49:09,960
따라서 높은 샘플링 비율로 인해, 데이터가

771
00:49:09,960 --> 00:49:14,040
많이 증강될 수 있습니다. 매번

772
00:49:14,040 --> 00:49:17,280
75%의 데이터를 마스킹하므로

773
00:49:17,280 --> 00:49:20,840
훈련 중에 동일한 이미지를 여러 번

774
00:49:20,840 --> 00:49:23,220
재사용할 수 있습니다.

775
00:49:23,220 --> 00:49:29,620
따라서 이 인코더를 훈련시키기 위한 데이터가 매우 많아질 것입니다.

776
00:49:29,620 --> 00:49:36,460
그래서 그들은 큰 ViT를 인코더로

777
00:49:36,460 --> 00:49:38,040
사용합니다.

778
00:49:38,040 --> 00:49:43,320
이 인코더 자체는 샘플의 25%만 봅니다.

779
00:49:43,320 --> 00:49:49,620
패치는 첫 번째 선형 프로젝션을 통해 두 개의 임베딩 공간에

780
00:49:49,620 --> 00:49:51,320
임베딩됩니다.

781
00:49:51,320 --> 00:49:55,420
그런 다음 위치 임베딩이 ViT에서

782
00:49:55,420 --> 00:49:59,700
언급한 것과 정확히 동일하게 추가됩니다.

783
00:49:59,700 --> 00:50:02,140
이 모든 것은 트랜스포머 블록입니다.

784
00:50:02,140 --> 00:50:09,600
그리고 인코더는 매우 큽니다. 제가 방금 언급한 것입니다.

785
00:50:09,600 --> 00:50:13,500
디코딩 부분으로 넘어가면,

786
00:50:13,500 --> 00:50:17,060
마스킹되거나 누락된 패치에

787
00:50:17,060 --> 00:50:21,820
대해 존재했던 모든 패치의 임베딩이

788
00:50:21,820 --> 00:50:23,260
있습니다.

789
00:50:23,260 --> 00:50:26,530
그에 대해 학습 가능한 매개변수가 있으며, 우리가

790
00:50:26,530 --> 00:50:29,230
가졌던 클래스 토큰과 매우 유사합니다.

791
00:50:29,230 --> 00:50:34,270
공유된 마스크 토큰은 기본적으로 어떤

792
00:50:34,270 --> 00:50:36,990
평균의 형태입니다.

793
00:50:36,990 --> 00:50:39,390
우리는 그것을 평균

794
00:50:39,390 --> 00:50:43,750
패치, 누락되거나 마스킹된 것에

795
00:50:43,750 --> 00:50:48,950
대해 설정된 평균 표현으로 간주할 수

796
00:50:48,950 --> 00:50:54,270
있으며, 디코더는 이를 전체 이미지의

797
00:50:54,270 --> 00:51:02,190
이미지 패치로 변환해야 하며, 전체 이미지는 출력 대상입니다.

798
00:51:02,190 --> 00:51:06,430
이것을 어떻게 훈련하는지, 이는

799
00:51:06,430 --> 00:51:16,150
이미지와 재구성된 이미지 간의 평균 제곱 오차(MSE) 기반 손실
함수입니다. 손실

800
00:51:16,150 --> 00:51:18,790
함수는 마스킹된 패치에

801
00:51:18,790 --> 00:51:21,830
대해서만 계산됩니다. 제가

802
00:51:21,830 --> 00:51:25,960
방금 언급한 이전 것과 유사합니다.

803
00:51:25,960 --> 00:51:30,960
그리고 훈련을 할 때, 그들은

804
00:51:30,960 --> 00:51:33,520
논문에서 선형

805
00:51:33,520 --> 00:51:42,192
프로빙 또는 전체 미세 조정을 통해 다운스트림 작업에

806
00:51:42,192 --> 00:51:44,900
사용할 수 있다고

807
00:51:44,900 --> 00:51:47,560
보여주었습니다.

808
00:51:47,560 --> 00:51:50,680
선형 프로빙에서는 인코더가

809
00:51:50,680 --> 00:51:55,000
고정되고 학습된 표현을 사용하여

810
00:51:55,000 --> 00:51:58,800
최종 작업을 위한 선형 함수만

811
00:51:58,800 --> 00:52:00,740
학습합니다.

812
00:52:00,740 --> 00:52:04,240
이 마크는 훈련 중임을 의미하지만,

813
00:52:04,240 --> 00:52:06,460
전체 미세 조정에서는

814
00:52:06,460 --> 00:52:08,320
사전 훈련된

815
00:52:08,320 --> 00:52:14,560
인코더도 미세 조정됩니다. 모두 또는 몇 개의 트랜스포머 블록만.

816
00:52:14,560 --> 00:52:19,420
이것이 미세 조정 프레임워크입니다.

817
00:52:19,420 --> 00:52:22,570
선형 프로빙은 표현 품질의

818
00:52:22,570 --> 00:52:25,890
척도를 제공하며, 이러한 표현

819
00:52:25,890 --> 00:52:31,890
특징이 어떻게 되는지를 보여주고, 미세 조정은 항상 새로운

820
00:52:31,890 --> 00:52:37,250
작업에 적응하기 위해 모델의 잠재력을 활용합니다.

821
00:52:37,250 --> 00:52:43,250
좋습니다, 이 주제에 관심이 있고 이 논문을 사용할

822
00:52:43,250 --> 00:52:45,310
계획이라면, 후속 논문을

823
00:52:45,310 --> 00:52:49,030
살펴보는 것을 강력히 권장합니다.

824
00:52:49,030 --> 00:52:53,210
모델 선택, 하이퍼파라미터, 마스킹 비율 등 다양한

825
00:52:53,210 --> 00:52:56,490
측면에 대한 논의가 많이 있습니다.

826
00:52:56,490 --> 00:53:00,450
그들은 마스킹 비율이 75%일 때 매우 높은

827
00:53:00,450 --> 00:53:04,150
정확도를 제공한다는 것을 보여주었습니다.

828
00:53:04,150 --> 00:53:07,810
그래서 75%가 선택된 이유입니다.

829
00:53:07,810 --> 00:53:13,530
디코더 깊이, 디코더 너비, 마스크 토큰, 재구성 목표, 데이터

830
00:53:13,530 --> 00:53:18,530
증강이 어떻게 도움이 되는지, 마스크 샘플링 방법에

831
00:53:18,530 --> 00:53:22,160
대한 결과를 다시 보여주고 있습니다.

832
00:53:22,160 --> 00:53:25,180
마스크 샘플링 방법은 주로 무작위

833
00:53:25,180 --> 00:53:30,500
마스킹 블록을 사용할지 그리드 유형의 마스킹을 사용할지에 관한

834
00:53:30,500 --> 00:53:31,520
것입니다.

835
00:53:31,520 --> 00:53:35,660
여기 예시가 보이며, 그들은 이 무작위 마스킹이

836
00:53:35,660 --> 00:53:39,620
최선의 선택이라는 결론에 도달했습니다.

837
00:53:39,620 --> 00:53:43,500
마지막으로 MAE가 사용된

838
00:53:43,500 --> 00:53:49,340
다른 많은 방법들에 비해 훨씬 더 잘 작동한다는

839
00:53:49,340 --> 00:53:53,960
것을 보여줄 수 있었습니다.

840
00:53:53,960 --> 00:53:56,460
그래서 다른 최첨단 방법 중 일부는 실제로

841
00:53:56,460 --> 00:53:58,760
Dino와 Moco V3였습니다.

842
00:53:58,760 --> 00:54:00,980
시간이 있다면 간단히 설명하겠습니다.

843
00:54:00,980 --> 00:54:06,540
하지만 이 프레임워크는 당시 더

844
00:54:06,540 --> 00:54:13,660
발전된 대조 학습 프레임워크보다 성능이

845
00:54:13,660 --> 00:54:16,220
뛰어났습니다.

846
00:54:16,220 --> 00:54:22,850
질문이 있으시면 몇 분간 멈추겠습니다.

847
00:54:22,850 --> 00:54:28,050
우리가 이야기한 내용을 요약하겠습니다.

848
00:54:28,050 --> 00:54:30,970
전제 작업은 실제로 매우 중요했습니다.

849
00:54:30,970 --> 00:54:33,910
그리고 제가 말했듯이 그들의 초점은

850
00:54:33,910 --> 00:54:36,230
시각적 상식을 이해하는 것입니다.

851
00:54:36,230 --> 00:54:40,630
그리고 질문과 관련된 것 중 하나는

852
00:54:40,630 --> 00:54:44,150
개별 전제 작업을 만드는

853
00:54:44,150 --> 00:54:48,790
것이 종종 도전적이라는 것입니다.

854
00:54:48,790 --> 00:54:52,270
학습된 표현이 정의한 작업의

855
00:54:52,270 --> 00:54:58,190
유형 때문에 일반적이지 않을 수 있습니다.

856
00:54:58,190 --> 00:55:05,170
예를 들어, 완성, 회전 예측, 또는 직소 퍼즐이나 색상화

857
00:55:05,170 --> 00:55:08,070
작업을 사용하는 경우,

858
00:55:08,070 --> 00:55:11,670
학습된 표현은 이러한 특정 작업을

859
00:55:11,670 --> 00:55:14,030
해결하는 데 좋지만

860
00:55:14,030 --> 00:55:19,680
일반 전제 작업에는 그리 좋지 않을 수 있습니다.

861
00:55:19,680 --> 00:55:22,420
그래서 질문은 분할 뇌

862
00:55:22,420 --> 00:55:28,880
오토인코더에서 모델이 예를 들어 L 채널, 조명 채널, 밝기 채널을

863
00:55:28,880 --> 00:55:33,340
주었을 때 다른 채널을 어떻게 예측하는지를

864
00:55:33,340 --> 00:55:34,720
아는 것입니다.

865
00:55:34,720 --> 00:55:39,040
질문으로 질문에 답하겠습니다.

866
00:55:39,040 --> 00:55:43,000
이미지에서 객체의 클래스를 예측하기 위해

867
00:55:43,000 --> 00:55:45,400
모델을 훈련할 때, 인코더는

868
00:55:45,400 --> 00:55:51,880
클래스 모델을 예측하기 위해 어떤 특징을 추출해야 하는지 어떻게 알까요?

869
00:55:51,880 --> 00:55:55,960
레이블 데이터입니다. 당신이 하는 것은

870
00:55:55,960 --> 00:55:59,280
그 레이블을 기준으로 계산된

871
00:55:59,280 --> 00:56:02,280
손실 값을 역전파하는 것입니다.

872
00:56:02,280 --> 00:56:03,660
여기서도 같은 이야기입니다.

873
00:56:03,660 --> 00:56:07,760
우리는 하나의 채널을 입력으로 받고 다른 채널을 출력하는

874
00:56:07,760 --> 00:56:09,480
네트워크를 정의합니다.

875
00:56:09,480 --> 00:56:13,040
이것이 어떻게 훈련되었는지는 출력이 무엇이어야 하는지를

876
00:56:13,040 --> 00:56:15,060
역전파하여 훈련되었습니다.

877
00:56:15,060 --> 00:56:17,400
출력은 다른 채널이었습니다.

878
00:56:17,400 --> 00:56:20,020
우리는 데이터에 다른 채널이 있습니다.

879
00:56:20,020 --> 00:56:23,700
그래서 객체의 클래스를 예측하는

880
00:56:23,700 --> 00:56:26,460
작업을 정의하는 대신,

881
00:56:26,460 --> 00:56:28,900
여기서는 픽셀의 색상을

882
00:56:28,900 --> 00:56:32,100
예측하는 작업을 정의합니다.

883
00:56:32,100 --> 00:56:33,740
픽셀의 색상은 이미

884
00:56:33,740 --> 00:56:36,980
데이터 세트에 있으므로 손실 함수는

885
00:56:36,980 --> 00:56:40,220
여전히 계산되고 역전파될 수 있습니다.

886
00:56:40,220 --> 00:56:42,540
그래서 질문은 이러한 출력이 디코더에 대한

887
00:56:42,540 --> 00:56:44,540
입력으로 어떻게 사용되는가입니다.

888
00:56:44,540 --> 00:56:51,020
다시 말해, 이것은 모든 입력을 토큰으로 변환하는

889
00:56:51,020 --> 00:56:58,660
VIT 트랜스포머 스타일의 인코더입니다. 이 출력은

890
00:56:58,660 --> 00:57:02,700
특정 입력 패치의 표현입니다.

891
00:57:02,700 --> 00:57:04,700
우리는 이것에 대해 이야기했습니다.

892
00:57:04,700 --> 00:57:10,040
하지만 우리는 이것이 모든 패치의 목록이 아니라는 것을 알고 있습니다.

893
00:57:10,040 --> 00:57:12,740
일부 패치는 마스킹되어 있습니다.

894
00:57:12,740 --> 00:57:15,830
마스킹된 것들에 대해서는 이

895
00:57:15,830 --> 00:57:20,350
인코더가 공유 마스크 토큰을 출력하도록

896
00:57:20,350 --> 00:57:25,490
훈련합니다. 기본적으로 평균 토큰인 토큰입니다.

897
00:57:25,490 --> 00:57:27,110
이것은 학습 가능한 매개변수입니다.

898
00:57:27,110 --> 00:57:29,290
우리는 반드시 그것을 해석할

899
00:57:29,290 --> 00:57:31,510
수는 없지만, 아마도 평균

900
00:57:31,510 --> 00:57:35,670
토큰과 유사한 마스크일 것이라고 말할 수 있습니다.

901
00:57:35,670 --> 00:57:40,990
그래서 그 공유 마스크 토큰은 누락된 것들의 자리에

902
00:57:40,990 --> 00:57:42,130
놓입니다.

903
00:57:42,130 --> 00:57:46,230
그리고 이 긴 벡터, 긴 시퀀스가 생성됩니다.

904
00:57:46,230 --> 00:57:49,550
디코더, 또 다른 트랜스포머

905
00:57:49,550 --> 00:57:55,990
프레임워크는 이 긴 토큰 집합을 받아서 출력

906
00:57:55,990 --> 00:57:58,540
픽셀 값을 투영합니다.

907
00:58:01,790 --> 00:58:12,600
완벽합니다. 우리는 15분밖에 없고 다룰 것이

908
00:58:12,600 --> 00:58:14,880
많습니다.

909
00:58:14,880 --> 00:58:21,300
하지만 제가 이 세션에서 얻고 싶었던 것은 여러분이 프리텍스트 작업이
무엇인지,

910
00:58:21,300 --> 00:58:24,800
그리고 그것을 어떻게 정의하는지를 이해하는

911
00:58:24,800 --> 00:58:26,020
것이었습니다.

912
00:58:26,020 --> 00:58:29,400
현재 가장 많이 사용되는 프레임워크

913
00:58:29,400 --> 00:58:32,720
중 하나는 마스크드 오토인코더로,

914
00:58:32,720 --> 00:58:36,120
우리는 실제로 어느 정도 다뤘습니다.

915
00:58:36,120 --> 00:58:42,200
어쨌든, 우리는 이러한 변환을 살펴보았고, 이

916
00:58:42,200 --> 00:58:45,040
모든 변환이 원본

917
00:58:45,040 --> 00:58:49,280
이미지와 동일한 객체라는 것을 알고

918
00:58:49,280 --> 00:58:52,940
있습니다. 단지 다른 형태일

919
00:58:52,940 --> 00:58:54,540
뿐입니다.

920
00:58:54,540 --> 00:58:59,360
하지만 데이터 세트에는 완전히 다른 모습의 다른

921
00:58:59,360 --> 00:59:02,660
객체들이 있다는 것도 알고 있습니다.

922
00:59:02,660 --> 00:59:08,720
따라서 같은 객체에 속하는 것들, 같은

923
00:59:08,720 --> 00:59:11,990
픽셀을 정의하는 작업을

924
00:59:11,990 --> 00:59:15,510
설정하고, 표현 공간에서

925
00:59:15,510 --> 00:59:18,210
서로 가까워지도록

926
00:59:18,210 --> 00:59:21,890
끌어당기고, 같은

927
00:59:21,890 --> 00:59:27,930
객체에 속하지 않는 것들은 잠재 공간에서의

928
00:59:27,930 --> 00:59:30,610
거리를 최대화하여

929
00:59:30,610 --> 00:59:34,650
서로 밀어내는 작업을 정의할

930
00:59:34,650 --> 00:59:37,130
수 있습니다.

931
00:59:37,130 --> 00:59:40,530
이것은 종종 대조 학습,

932
00:59:40,530 --> 00:59:44,330
대조 표현 학습이라고 불리는 또

933
00:59:44,330 --> 00:59:46,290
다른 작업이며,

934
00:59:46,290 --> 00:59:52,530
살펴볼 만한 매우 흥미로운 방법들이 많이 있습니다.

935
00:59:52,530 --> 00:59:56,830
2019년에서 2020년

936
00:59:56,830 --> 01:00:04,530
사이에 이 분야에서 많은 논문들이

937
01:00:04,530 --> 01:00:07,010
샘플링되었습니다.

938
01:00:07,010 --> 01:00:10,840
Sinclair McCaw, CBC, 그리고 궁극적으로

939
01:00:10,840 --> 01:00:13,900
dyno는 대조 학습의 개념을 차용하지만 반드시

940
01:00:13,900 --> 01:00:16,440
대조 학습 프레임워크는 아닙니다.

941
01:00:19,060 --> 01:00:26,940
우리가 하는 것은 끌어당기고 밀어내는 함수, 특성을 정의하거나

942
01:00:26,940 --> 01:00:33,260
모델을 정규화하기 위해 참조 이미지를 x로

943
01:00:33,260 --> 01:00:38,740
정의하고, 같은 변환의 모든 것들을

944
01:00:38,740 --> 01:00:40,500
긍정 샘플로,

945
01:00:40,500 --> 01:00:44,140
데이터 세트 또는 배치의

946
01:00:44,140 --> 01:00:46,540
다른 객체들을 부정

947
01:00:46,540 --> 01:00:49,940
샘플로 정의하는 것입니다.

948
01:00:49,940 --> 01:00:52,300
이 긍정 샘플과

949
01:00:52,300 --> 01:00:55,260
부정 샘플은 손실 함수를

950
01:00:55,260 --> 01:00:58,500
계산하는 방법을 정의합니다.

951
01:00:58,500 --> 01:01:00,740
어떻게 할 수 있을까요?

952
01:01:00,740 --> 01:01:02,720
점수 함수가 있다고 가정해 보겠습니다.

953
01:01:02,720 --> 01:01:04,420
우리는 참조

954
01:01:04,420 --> 01:01:09,290
이미지 인코딩 버전의 점수와 긍정

955
01:01:09,290 --> 01:01:11,550
샘플의 특성이

956
01:01:11,550 --> 01:01:15,810
참조 이미지와 부정 샘플을 비교하는

957
01:01:15,810 --> 01:01:24,150
점수보다 커야 한다고 말하는 점수 함수를 얻고자 합니다.

958
01:01:24,150 --> 01:01:30,910
이러한 유형의 점수 함수를 정의하기 위해,

959
01:01:30,910 --> 01:01:35,110
우리는 이를 기반으로 손실

960
01:01:35,110 --> 01:01:38,370
함수를 정의합니다.

961
01:01:38,370 --> 01:01:43,750
점수 함수 S를 훈련한 후, 이 S는 이전 슬라이드의

962
01:01:43,750 --> 01:01:46,150
점수와 동일하다는

963
01:01:46,150 --> 01:01:48,510
것을 볼 수 있습니다.

964
01:01:48,510 --> 01:01:50,590
따라서 이 점수

965
01:01:50,590 --> 01:01:54,350
함수가 있다면, 이제

966
01:01:54,350 --> 01:01:58,830
끌어당기고 밀어내기 위해 이 점수를

967
01:01:58,830 --> 01:02:05,400
확률 값으로 변환하는 소프트맥스 설정을 사용할

968
01:02:05,400 --> 01:02:07,260
수 있습니다.

969
01:02:07,260 --> 01:02:09,920
그리고 분모에는

970
01:02:09,920 --> 01:02:15,040
사용된 모든 다른 부정 샘플이

971
01:02:15,040 --> 01:02:17,480
고려됩니다.

972
01:02:17,480 --> 01:02:18,920
실제로 구현하기 위해

973
01:02:18,920 --> 01:02:21,780
우리는 배치 학습 프레임워크를 사용합니다.

974
01:02:21,780 --> 01:02:23,680
배치의 다른 객체에

975
01:02:23,680 --> 01:02:27,800
속하는 모든 다른 부정 샘플은 부정

976
01:02:27,800 --> 01:02:31,120
샘플로 취급되고, 하나의 변환은

977
01:02:31,120 --> 01:02:33,540
긍정 샘플로 사용됩니다.

978
01:02:33,540 --> 01:02:36,600
따라서 우리는 손실 함수를 이렇게 정의합니다.

979
01:02:36,600 --> 01:02:39,440
긍정 쌍에 대한 점수,

980
01:02:39,440 --> 01:02:43,880
모든 다른 부정 쌍에 대한 점수.

981
01:02:43,880 --> 01:02:48,080
이 함수는 우리가 실제로 이전에 논의한

982
01:02:48,080 --> 01:02:50,160
것과 매우 유사합니다.

983
01:02:50,160 --> 01:02:51,840
아이디어가 있나요?

984
01:02:51,840 --> 01:02:54,520
이것은 다중 클래스에 대한 교차 엔트로피입니다.

985
01:02:54,520 --> 01:02:57,960
따라서 n 샘플이

986
01:02:57,960 --> 01:03:04,990
있다면, 이 경우 n 샘플이 있습니다.

987
01:03:04,990 --> 01:03:09,190
소프트맥스는 여러 클래스가 있을 때, 출력으로

988
01:03:09,190 --> 01:03:11,430
10개의 클래스가 있으면

989
01:03:11,430 --> 01:03:15,210
그 중 하나의 점수를 최대화하고 나머지는

990
01:03:15,210 --> 01:03:17,070
최소화하려고 합니다.

991
01:03:17,070 --> 01:03:19,070
여기서도 같은 이야기입니다.

992
01:03:19,070 --> 01:03:23,170
우리는 이 점수를 최대화하고, 부정 샘플과 참조

993
01:03:23,170 --> 01:03:26,050
간의 점수를 최소화하고자 합니다.

994
01:03:26,050 --> 01:03:29,530
따라서 이것은 다중 클래스 분류에

995
01:03:29,530 --> 01:03:31,910
대해 논의했던 것과

996
01:03:31,910 --> 01:03:34,690
같은 개념이지만, 이 손실

997
01:03:34,690 --> 01:03:37,450
함수를 대조 학습의 형태로

998
01:03:37,450 --> 01:03:39,610
공식화한 것입니다.

999
01:03:39,610 --> 01:03:44,850
이 함수는 info NCE 또는 정보 노이즈

1000
01:03:44,850 --> 01:03:47,930
대조 추정 손실이라고 불리며,

1001
01:03:47,930 --> 01:03:51,170
이 논문에서 제안되었습니다.

1002
01:03:51,170 --> 01:03:55,370
이 목표, 이 손실

1003
01:03:55,370 --> 01:03:58,890
함수는 의존성을

1004
01:03:58,890 --> 01:04:04,630
측정하며, 상호 정보의 하한을

1005
01:04:04,630 --> 01:04:07,790
나타냅니다.

1006
01:04:07,790 --> 01:04:12,070
상호 정보란 두 이미지를 가지고 그들 사이의

1007
01:04:12,070 --> 01:04:14,790
상호 정보를 계산할 때, 기본적으로

1008
01:04:14,790 --> 01:04:18,270
이 두 이미지 간의 의존성 또는

1009
01:04:18,270 --> 01:04:21,090
공유 정보를 측정하는 것입니다.

1010
01:04:21,090 --> 01:04:23,790
우리가 하고 싶은 것은 x와 x

1011
01:04:23,790 --> 01:04:27,430
플러스 간의 공유 정보를 극대화하고,

1012
01:04:27,430 --> 01:04:32,330
x와 x 마이너스 간의 공유 정보는 최소화하는 것입니다.

1013
01:04:32,330 --> 01:04:38,230
논문에서는 다시 말하지만, 이 내용을 설명하는 데 반 시간

1014
01:04:38,230 --> 01:04:40,130
정도 걸릴 것입니다.

1015
01:04:40,130 --> 01:04:42,750
관심이 있다면 논문을 꼭

1016
01:04:42,750 --> 01:04:47,830
살펴보아야 하며, 이 손실 함수 info

1017
01:04:47,830 --> 01:04:52,750
NCE의 음수는 x와 x 플러스 간의 상호 정보의

1018
01:04:52,750 --> 01:04:54,070
하한입니다.

1019
01:04:54,070 --> 01:04:57,390
하한의 음수가

1020
01:04:57,390 --> 01:05:03,400
상호 정보의 하한이라는 것입니다.

1021
01:05:03,400 --> 01:05:07,240
info NCE를 최소화하면 기본적으로

1022
01:05:07,240 --> 01:05:10,280
x와 x 플러스 간의 상호 정보를

1023
01:05:10,280 --> 01:05:11,940
극대화하는 것입니다.

1024
01:05:11,940 --> 01:05:14,000
이것이 제가 정말로 원하는 것입니다.

1025
01:05:14,000 --> 01:05:17,920
그래서 우리는 이것을 손실 함수로

1026
01:05:17,920 --> 01:05:23,480
사용하고 그 값을 최소화하기 시작합니다.

1027
01:05:23,480 --> 01:05:25,800
info NCE 논문에는

1028
01:05:25,800 --> 01:05:29,920
부정 샘플의 수가 많을수록 경계가 더 엄격해진다는

1029
01:05:29,920 --> 01:05:33,300
또 다른 이론적 측면이 있습니다.

1030
01:05:33,300 --> 01:05:36,600
부정 샘플의 수에 따라 경계가

1031
01:05:36,600 --> 01:05:38,520
더 엄격해집니다.

1032
01:05:38,520 --> 01:05:43,680
그래서 이러한 유형의 손실 함수로 신경망을 훈련시키기

1033
01:05:43,680 --> 01:05:47,880
위해서는 큰 배치 크기가 필요합니다.

1034
01:05:47,880 --> 01:05:51,040
부정 샘플의 수가

1035
01:05:51,040 --> 01:05:59,770
많을수록 더 나은 훈련 수렴을 더 빠르게 얻을 수 있습니다.

1036
01:05:59,770 --> 01:06:03,570
그리고 이 손실 함수는 여러 가지 다른

1037
01:06:03,570 --> 01:06:06,230
프레임워크에서 사용되었습니다.

1038
01:06:06,230 --> 01:06:09,210
다음 몇 분 동안 그 프레임워크가

1039
01:06:09,210 --> 01:06:12,190
무엇인지 말씀드리겠습니다.

1040
01:06:12,190 --> 01:06:16,570
예를 들어, Sinclair는 대조 학습을 위한

1041
01:06:16,570 --> 01:06:22,210
간단한 프레임워크로, 기본적으로 각 이미지를 가져와 같은 이미지의

1042
01:06:22,210 --> 01:06:24,550
두 가지 변환을 수행하여

1043
01:06:24,550 --> 01:06:27,110
표현 공간으로 전환합니다.

1044
01:06:27,110 --> 01:06:29,650
그리고 그것이 하는

1045
01:06:29,650 --> 01:06:36,390
것은 임베딩 표현 간의 코사인 유사성을 계산하는 것입니다.

1046
01:06:36,390 --> 01:06:40,530
하지만 그렇게 하기 전에, 선형 또는

1047
01:06:40,530 --> 01:06:47,690
비선형 투영을 통해 특징 집합 z로 변환하여 이 공간에서의

1048
01:06:47,690 --> 01:06:49,970
거리를 계산합니다.

1049
01:06:49,970 --> 01:06:55,280
이것이 긍정 샘플을 생성하는

1050
01:06:55,280 --> 01:06:57,580
방법입니다.

1051
01:06:57,580 --> 01:07:00,920
긍정 샘플을 생성하기 위해서는

1052
01:07:00,920 --> 01:07:05,020
모든 종류의 변환이 의미가 있습니다.

1053
01:07:05,020 --> 01:07:09,700
세부 사항은 기본적으로 여기에서 다루어집니다.

1054
01:07:09,700 --> 01:07:14,500
데이터 증강 함수를 샘플링하여 긍정 쌍을

1055
01:07:14,500 --> 01:07:15,580
생성합니다.

1056
01:07:15,580 --> 01:07:20,060
그래서 우리는 그 중 몇 개를 샘플링한 다음 쌍에

1057
01:07:20,060 --> 01:07:22,660
대해 역수 및 손실을 계산합니다.

1058
01:07:22,660 --> 01:07:29,580
그리고 이것이 우리가 반복하는 것입니다. 각 샘플은 우리가 생성한

1059
01:07:29,580 --> 01:07:32,940
2n 곱하기 n 샘플을 가집니다.

1060
01:07:32,940 --> 01:07:39,980
그래서 일어나는 일은 미니 배치에 있는 이미지 목록을

1061
01:07:39,980 --> 01:07:43,140
가져와서 같은

1062
01:07:43,140 --> 01:07:48,620
이미지의 두 가지 변형에 대해 인코더를

1063
01:07:48,620 --> 01:07:49,820
통과시킵니다.

1064
01:07:49,820 --> 01:07:55,310
각 이미지는 기본적으로 변형된 버전을

1065
01:07:55,310 --> 01:07:56,110
가집니다.

1066
01:07:56,110 --> 01:08:02,830
그리고 다음으로, 이제 배치에 2n 샘플이 있습니다.

1067
01:08:02,830 --> 01:08:05,950
그리고 이것은 각 샘플에 대해 다음

1068
01:08:05,950 --> 01:08:10,550
샘플이 이 두 개가 긍정 샘플이고 나머지는 부정이라는

1069
01:08:10,550 --> 01:08:12,050
것을 의미합니다.

1070
01:08:12,050 --> 01:08:16,010
그래서 첫 번째 샘플의 경우 두 번째 이미지는 긍정입니다.

1071
01:08:16,010 --> 01:08:17,569
나머지는 모두 부정입니다.

1072
01:08:17,569 --> 01:08:19,710
두 번째 샘플의 경우 첫 번째는 긍정이고

1073
01:08:19,710 --> 01:08:21,310
나머지는 모두 부정입니다.

1074
01:08:21,310 --> 01:08:24,990
이것은 모든 샘플에 대해 반복됩니다.

1075
01:08:24,990 --> 01:08:31,710
그래서 이것이 SimCLR의 높은 수준의 정의입니다.

1076
01:08:31,710 --> 01:08:36,870
과제 3에는 SimCLR과 관련된

1077
01:08:36,870 --> 01:08:42,510
질문이 있으니 이 프레임워크를 좀 더 탐구할

1078
01:08:42,510 --> 01:08:44,069
것입니다.

1079
01:08:44,069 --> 01:08:47,350
하지만 정의가 제가 여기서 제시한

1080
01:08:47,350 --> 01:08:49,790
표준 정의와 약간 다르다는

1081
01:08:49,790 --> 01:08:51,120
점에 주의하세요.

1082
01:08:51,120 --> 01:08:57,080
그리고 과제의 지침을 반드시 따르세요.

1083
01:08:57,080 --> 01:08:59,620
그래서 SimCLR은 실제로 매우 성공적이었습니다.

1084
01:08:59,620 --> 01:09:05,160
라벨 없이, 그리고 특징 위에 선형

1085
01:09:05,160 --> 01:09:07,600
분류기를

1086
01:09:07,600 --> 01:09:13,279
훈련시키면서, 이전의 모든 작업을 초월하고

1087
01:09:13,279 --> 01:09:19,240
기본적으로 감독 학습과 비교할 수

1088
01:09:19,240 --> 01:09:22,939
있는 결과를 생성할

1089
01:09:22,939 --> 01:09:25,140
수 있었습니다.

1090
01:09:25,140 --> 01:09:28,080
우리는 이제 더 일반적인 특징을 배우고 있기

1091
01:09:28,080 --> 01:09:30,420
때문에 더 큰 신경망이 필요합니다.

1092
01:09:30,420 --> 01:09:32,120
하지만 정확도

1093
01:09:32,120 --> 01:09:38,760
측면에서는 감독 학습에서 우리가 가졌던 것과 비교할 수 없었습니다.

1094
01:09:38,760 --> 01:09:44,040
SimCLR과 그 주변의 몇 가지

1095
01:09:44,040 --> 01:09:47,479
결과에서 흥미로운

1096
01:09:47,479 --> 01:09:53,630
점은 몇 가지 선택이 있다는 것입니다.

1097
01:09:53,630 --> 01:09:58,530
사실, 주요 선택 사항에 대해 시간을 할애하겠습니다.

1098
01:09:58,530 --> 01:10:02,850
왜 같은 표현을 사용하지 않고 특징을 새로운 변수로

1099
01:10:02,850 --> 01:10:06,850
투영했는지에 대한 질문이 있을 수 있습니다.

1100
01:10:06,850 --> 01:10:10,910
이것은 SimCLR에서 그들이 한 설계

1101
01:10:10,910 --> 01:10:16,050
선택으로, 샘플 간의 대비를 수행하는 목적

1102
01:10:16,050 --> 01:10:17,810
함수가 있을 때,

1103
01:10:17,810 --> 01:10:20,490
종종 대비 학습 프레임워크에

1104
01:10:20,490 --> 01:10:24,110
도움이 되지 않는 추가 정보를

1105
01:10:24,110 --> 01:10:26,050
잃게 된다고

1106
01:10:26,050 --> 01:10:28,070
올바르게 가정했습니다.

1107
01:10:28,070 --> 01:10:34,490
따라서 이러한 모든 추가 특징을 보존하기 위해

1108
01:10:34,490 --> 01:10:36,830
표현은 h로

1109
01:10:36,830 --> 01:10:40,010
정의되지만, 그들의

1110
01:10:40,010 --> 01:10:42,770
논문에서는 비선형 투영을

1111
01:10:42,770 --> 01:10:49,540
사용하여 z 값을 계산할 수 있도록 합니다.

1112
01:10:49,540 --> 01:10:52,280
그래서 이것이 중요한 설계 선택 중 하나입니다.

1113
01:10:52,280 --> 01:10:56,680
또 다른 하나는 제가 앞서 언급한 대규모 배치 크기입니다.

1114
01:10:56,680 --> 01:10:59,300
더 나은 SimCLR 성능을

1115
01:10:59,300 --> 01:11:04,040
얻기 위해서는 대규모 배치 크기가 필요합니다.

1116
01:11:04,040 --> 01:11:08,100
우리는 이것이 왜 그런지에 대해 이야기했습니다.

1117
01:11:08,100 --> 01:11:11,340
하지만 메모리 등의 제약으로

1118
01:11:11,340 --> 01:11:13,580
인해 우리가 가진

1119
01:11:13,580 --> 01:11:20,060
많은 작업에 대해 항상 큰 배치 크기를 사용할 수는 없습니다.

1120
01:11:20,060 --> 01:11:23,640
그래서 여러 후속 연구가 있었고,

1121
01:11:23,640 --> 01:11:29,920
예를 들어 모카가 제안된 모멘텀 대조 학습이 그 이유입니다.

1122
01:11:29,920 --> 01:11:34,980
샘플 대신 배치의 모든 부정 샘플을

1123
01:11:34,980 --> 01:11:39,260
사용하는 대신, 큐를

1124
01:11:39,260 --> 01:11:46,440
생성하고 모델에서 시간에 따라 배치 간 부정 샘플의

1125
01:11:46,440 --> 01:11:49,260
이력을 유지합니다.

1126
01:11:49,260 --> 01:11:53,840
그래서 배치의 부정 샘플에만

1127
01:11:53,840 --> 01:11:57,360
의존하지 않고, 정의된

1128
01:11:57,360 --> 01:12:01,640
별도의 큐가 있어 여러 부정

1129
01:12:01,640 --> 01:12:11,560
샘플을 유지하고 이를 시간에 따라 업데이트하여 대조 손실을 훈련하고

1130
01:12:11,560 --> 01:12:13,360
추론합니다.

1131
01:12:13,360 --> 01:12:17,240
하지만 이 큐가 있기 때문에 더 이상 배치에 없는 샘플에

1132
01:12:17,240 --> 01:12:19,540
대해 역전파를 할 수 없습니다.

1133
01:12:19,540 --> 01:12:25,260
그래서 부정 샘플에 대해 역전파를 할 수 없고, 그래서 이제

1134
01:12:25,260 --> 01:12:27,560
쿼리라고 불리는 긍정 샘플을

1135
01:12:27,560 --> 01:12:30,800
위한 인코더를 분리해야 했습니다.

1136
01:12:30,800 --> 01:12:32,600
그리고 이

1137
01:12:32,600 --> 01:12:38,240
아키텍처에서 이제 키라고 불리는 부정 샘플입니다.

1138
01:12:38,240 --> 01:12:42,760
그래서 훈련은 인코더에만 영향을 미치고,

1139
01:12:42,760 --> 01:12:51,590
시간이 지남에 따라 Q 인코더가 모멘텀 m을 사용하여 키 인코더, 모멘텀
인코더를

1140
01:12:51,590 --> 01:12:53,130
업데이트합니다.

1141
01:12:53,130 --> 01:12:57,370
그래서 이것은 실제로 구현 측면에서

1142
01:12:57,370 --> 01:13:01,490
매우 성공적인 프레임워크이며, 후속

1143
01:13:01,490 --> 01:13:03,670
버전도 있습니다.

1144
01:13:03,670 --> 01:13:07,930
흥미로운 결과가 많지만, 그들이 한

1145
01:13:07,930 --> 01:13:13,170
것은 기본적으로 비선형 프로젝션 헤드와 SimCLR의

1146
01:13:13,170 --> 01:13:19,530
데이터 증강을 사용하고, 이 미니 배치 스타일,

1147
01:13:19,530 --> 01:13:22,210
MoCo의 미니 배치와

1148
01:13:22,210 --> 01:13:26,010
부정 샘플의 분리를 하이브리드

1149
01:13:26,010 --> 01:13:29,170
버전으로 시도한 것입니다. 그리고

1150
01:13:29,170 --> 01:13:34,490
실제로 이렇게 함께 하면 MoCo 버전 2가

1151
01:13:34,490 --> 01:13:39,950
성능을 크게 향상시킨다는 것을 보여주었습니다.

1152
01:13:39,950 --> 01:13:45,740
그래서 여기서 멈추겠지만, 슬라이드에서 볼

1153
01:13:45,740 --> 01:13:50,140
수 있는 또 다른 예로 대조 예측

1154
01:13:50,140 --> 01:13:55,020
코딩(CPC)의 개념이 있었습니다.

1155
01:13:55,020 --> 01:14:00,580
그리고 MoCo의 더 나은 버전인 MoCo 버전 3과

1156
01:14:00,580 --> 01:14:04,580
DINO는 MoCo와 유사한 아키텍처를

1157
01:14:04,580 --> 01:14:11,220
가진 널리 사용되는 프레임워크 중 하나입니다. 하지만 이제 학생과

1158
01:14:11,220 --> 01:14:14,020
교사 네트워크가 있기

1159
01:14:14,020 --> 01:14:17,640
때문에 반드시 대조 학습은 아닙니다.

1160
01:14:17,640 --> 01:14:20,280
그래서 그것은 별도의 논의로

1161
01:14:20,280 --> 01:14:24,220
남기고, 관심이 있다면 미래의 슬라이드나

1162
01:14:24,220 --> 01:14:27,520
강의에서 논의할 수 있습니다.

1163
01:14:27,520 --> 01:14:32,380
어쨌든, 이것은 이미지와 때때로 비디오에서 특징을

1164
01:14:32,380 --> 01:14:35,140
추출하는 데 널리 사용되는 프레임워크

1165
01:14:35,140 --> 01:14:36,990
중 하나입니다.
