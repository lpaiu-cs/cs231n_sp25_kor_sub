1
00:00:05,600 --> 00:00:12,000
지난 화요일, 이번 주에 우리는 GPU에 관한 강의를 했습니다. GPU를
어떻게

2
00:00:12,000 --> 00:00:14,380
사용하고, 훈련에 어떻게

3
00:00:14,380 --> 00:00:18,700
활용하며, 여러 GPU를 사용해 더 큰 규모로 훈련하는

4
00:00:18,700 --> 00:00:20,920
방법 등에 대해 다뤘습니다.

5
00:00:20,920 --> 00:00:23,440
이번 학기에 새로

6
00:00:23,440 --> 00:00:26,400
추가한 흥미로운 주제가

7
00:00:26,400 --> 00:00:29,360
있는데요, 이는 모델 크기와

8
00:00:29,360 --> 00:00:34,960
AI 모델의 응용이 증가하는 현 시점에

9
00:00:34,960 --> 00:00:39,840
매우 시기적절하고 중요하다고 생각합니다.

10
00:00:39,840 --> 00:00:47,800
그 전에 우리는 컴퓨터 비전의 주요 과제들을 모두 다뤘습니다.

11
00:00:47,800 --> 00:00:50,660
분류, 의미론적

12
00:00:50,660 --> 00:00:53,400
분할, 객체 탐지,

13
00:00:53,400 --> 00:00:56,880
인스턴스 분할 등 말이죠.

14
00:00:56,880 --> 00:00:59,810
오늘은 그 중 일부 주제와 오늘 다룰

15
00:00:59,810 --> 00:01:02,790
모델들의 결과를 다시 살펴볼 예정입니다.

16
00:01:02,790 --> 00:01:09,410
이 과제들은 여전히 매우 중요합니다.

17
00:01:09,410 --> 00:01:12,650
그리고 우리는 모델을 시각화하고

18
00:01:12,650 --> 00:01:19,490
이해하며, 모델이 무엇을 학습하는지 보는 방법에 대해 이야기했습니다.

19
00:01:19,490 --> 00:01:21,130
예를 들어,

20
00:01:21,130 --> 00:01:27,830
초기 세션에서 픽셀 공간에서 최근접 이웃을

21
00:01:27,830 --> 00:01:31,130
사용하는 방법과 픽셀

22
00:01:31,130 --> 00:01:37,690
기반 거리만으로 이미지의 클래스를 찾는 방법을

23
00:01:37,690 --> 00:01:40,550
다뤘고, 이것이

24
00:01:40,550 --> 00:01:45,330
비효율적이라는 점을 논의했습니다.

25
00:01:45,330 --> 00:01:47,250
우리가 이야기한

26
00:01:47,250 --> 00:01:53,130
것 중 하나는 임베딩 층이나 특징 공간,

27
00:01:53,130 --> 00:01:56,730
즉 합성곱 신경망이나

28
00:01:56,730 --> 00:02:03,060
다른 네트워크 아키텍처의 완전 연결층에서 추출한

29
00:02:03,060 --> 00:02:05,780
특징들이 이미지의

30
00:02:05,780 --> 00:02:10,020
좋은 표현이 될 수 있다는

31
00:02:10,020 --> 00:02:11,280
점입니다.

32
00:02:11,280 --> 00:02:14,520
그리고 만약 이 특징 공간에서 최근접

33
00:02:14,520 --> 00:02:19,260
이웃을 찾는 데 L2 거리를 사용한다면, 이 모델들의

34
00:02:19,260 --> 00:02:23,540
특징들이 매우 의미 있다는 것을 알 수 있습니다.

35
00:02:23,540 --> 00:02:25,300
즉, 이 특징들은 우리가

36
00:02:25,300 --> 00:02:28,380
다루는 특정 과제에 대해 상당히

37
00:02:28,380 --> 00:02:30,260
유의미하다는 뜻입니다.

38
00:02:30,260 --> 00:02:34,460
특히 CNN, ResNet,

39
00:02:34,460 --> 00:02:39,660
또는 트랜스포머 모델을

40
00:02:39,660 --> 00:02:46,800
실행하고, 다양한 맥락에서 학습된 표현을 보면,

41
00:02:46,800 --> 00:02:49,200
이 표현들은

42
00:02:49,200 --> 00:02:54,740
다양한 이름으로 불릴 수 있습니다.

43
00:02:54,740 --> 00:02:56,890
학습된 대규모 표현,

44
00:02:56,890 --> 00:02:59,810
특징, 임베딩, 잠재 공간 등이죠.

45
00:02:59,810 --> 00:03:04,230
하지만 이 학습된 표현이나 특징들은 이미지의

46
00:03:04,230 --> 00:03:07,590
매우 좋은 대표자 역할을 합니다.

47
00:03:07,590 --> 00:03:12,970
그리고 만약 우리가 이들을 추출할 방법이 있다면, 간단한

48
00:03:12,970 --> 00:03:17,470
선형 모델로도 이 특징들에서 클래스 레이블을

49
00:03:17,470 --> 00:03:21,350
얻을 수 있습니다, 여기 마지막에 보시는

50
00:03:21,350 --> 00:03:22,430
것처럼요.

51
00:03:22,430 --> 00:03:24,910
하지만 가장 큰 도전 과제는

52
00:03:24,910 --> 00:03:30,190
이러한 네트워크를 더 큰 규모로 훈련하거나 구축하는

53
00:03:30,190 --> 00:03:33,530
것이 항상 어렵다는 점입니다.

54
00:03:33,530 --> 00:03:37,350
왜 이런 도전 과제가 있는지 말씀해 주실 수 있나요?

55
00:03:37,350 --> 00:03:40,690
그래서 문제는 더 큰 규모에서는 많은 라벨된

56
00:03:40,690 --> 00:03:45,110
데이터가 필요하다는 겁니다. 이 네트워크는 이미지에서 시작해서

57
00:03:45,110 --> 00:03:49,270
마지막에는 클래스 라벨을 얻도록 훈련되기 때문입니다.

58
00:03:49,270 --> 00:03:51,550
이 네트워크를 훈련하면, 네,

59
00:03:51,550 --> 00:03:56,190
이 특징들은 클래스 라벨을 얻는 데 매우 유용할

60
00:03:56,190 --> 00:03:57,370
것입니다, 그렇죠?

61
00:03:57,370 --> 00:04:02,410
하지만 규모가 커지면, 이미지를 하나하나 수작업으로

62
00:04:02,410 --> 00:04:05,148
라벨링하는 노력이 많이 필요합니다.

63
00:04:05,148 --> 00:04:06,690
만약 작업이

64
00:04:06,690 --> 00:04:10,850
세그멘테이션이라면, 모든 이미지에서 픽셀 하나하나를

65
00:04:10,850 --> 00:04:14,650
라벨링해야 해서 매우 어려운 일이 됩니다.

66
00:04:14,650 --> 00:04:18,130
그래서 질문은, 엄청난 수작업

67
00:04:18,130 --> 00:04:22,490
라벨 데이터 없이 신경망을 훈련할 수 있는

68
00:04:22,490 --> 00:04:25,190
방법이 있느냐는 겁니다.

69
00:04:25,190 --> 00:04:29,470
이 수작업 라벨이 문제이고, 이를

70
00:04:29,470 --> 00:04:32,970
우회해서 좋은 특징을 얻을

71
00:04:32,970 --> 00:04:36,290
수 있는 신경망을 훈련할

72
00:04:36,290 --> 00:04:39,410
수 있는지 보고 싶습니다.

73
00:04:39,410 --> 00:04:44,330
그래서 여기서 자기지도 학습(self-supervised
learning)이라는

74
00:04:44,330 --> 00:04:45,270
주제가 등장합니다.

75
00:04:45,270 --> 00:04:48,570
오늘 우리가 다룰 내용이 바로 그것입니다.

76
00:04:48,570 --> 00:04:58,700
라벨 없는 큰 이미지 데이터셋이 있다고 할 때, 우리의 가설은 이미지에서
좋은 특징을

77
00:04:58,700 --> 00:05:02,020
얻을 수 있는 목적 함수,

78
00:05:02,020 --> 00:05:08,460
즉 사전 과제(pretext task)를 사용해

79
00:05:08,460 --> 00:05:12,600
신경망을 훈련할 수 있다는 겁니다.

80
00:05:12,600 --> 00:05:17,860
그리고 라벨이 있는 더 작은 특정

81
00:05:17,860 --> 00:05:23,860
데이터셋에서 학습할 때, 이 훈련된 인코더를

82
00:05:23,860 --> 00:05:28,740
전이 학습해서 다운스트림 작업이나

83
00:05:28,740 --> 00:05:32,260
목적에 필요한 특징을 추출할

84
00:05:32,260 --> 00:05:34,500
수 있습니다.

85
00:05:34,500 --> 00:05:38,780
여기서 우리는 사전 과제, 즉

86
00:05:38,780 --> 00:05:43,540
이미지에서 좋은 특징을 학습할 수

87
00:05:43,540 --> 00:05:47,500
있을 만큼 일반적인 작업을

88
00:05:47,500 --> 00:05:53,750
정의하고, 그 인코더를 사용해 여러분이 관심

89
00:05:53,750 --> 00:05:56,870
있는 다운스트림 작업,

90
00:05:56,870 --> 00:05:59,990
즉 실제 응용 문제를

91
00:05:59,990 --> 00:06:02,330
해결하려고 합니다.

92
00:06:02,330 --> 00:06:06,630
예를 들어, 인터넷에서 다운로드한 많은 자연 이미지가

93
00:06:06,630 --> 00:06:08,130
있다고 합시다.

94
00:06:08,130 --> 00:06:10,330
그걸로 뭔가를 훈련할 수 있습니다.

95
00:06:10,330 --> 00:06:13,290
그리고 산업용이나 의료용

96
00:06:13,290 --> 00:06:14,910
같은 소규모 라벨된

97
00:06:14,910 --> 00:06:18,870
이미지 데이터셋이 있다고 할 때,

98
00:06:18,870 --> 00:06:21,990
이 전이된 지식을 사용해

99
00:06:21,990 --> 00:06:26,270
특징을 추출하고, 관심 있는 작업을

100
00:06:26,270 --> 00:06:29,350
분류하거나 수행할 수 있습니다.

101
00:06:29,350 --> 00:06:32,390
그래서 이 주제를 좀 더 깊이

102
00:06:32,390 --> 00:06:35,430
파고들어 다양한 구성 요소를

103
00:06:35,430 --> 00:06:37,590
이해해 보려고 합니다.

104
00:06:37,590 --> 00:06:40,810
간단히 말해, self-supervised

105
00:06:40,810 --> 00:06:45,590
learning은 라벨이 없는 데이터셋에 대해 이 pretext task를

106
00:06:45,590 --> 00:06:47,110
정의하는 것입니다.

107
00:06:47,110 --> 00:06:55,140
Encoder는 보통 학습된 표현을 얻어냅니다.

108
00:06:55,140 --> 00:06:59,640
그리고 같은 신경망의 또 다른 모듈이 학습된

109
00:06:59,640 --> 00:07:04,080
표현을 출력 공간으로 변환하거나 생성하는데,

110
00:07:04,080 --> 00:07:06,240
이 출력은 라벨일

111
00:07:06,240 --> 00:07:10,440
수도 있고 데이터에서 자동으로 생성된 출력일

112
00:07:10,440 --> 00:07:11,980
수도 있습니다.

113
00:07:11,980 --> 00:07:14,880
이것들은 수동 주석이 아닙니다.

114
00:07:14,880 --> 00:07:22,720
만약 우리가 이것을 할 수 있다면, 목적 함수, 손실 함수,

115
00:07:22,720 --> 00:07:25,400
그리고 손실 함수로

116
00:07:25,400 --> 00:07:29,280
학습할 신경망이 있게 됩니다.

117
00:07:29,280 --> 00:07:33,680
여기서 보시다시피 두 번째 부분을 때때로 decoder,
classifier,

118
00:07:33,680 --> 00:07:36,600
regressor라고 부르는데, 이는 pretext task를

119
00:07:36,600 --> 00:07:38,420
어떻게 정의하느냐에 따라 다릅니다.

120
00:07:38,420 --> 00:07:40,920
몇 가지 예를 드리겠지만, 이것들은

121
00:07:40,920 --> 00:07:44,120
어떤 형태의 프레임워크도 될 수 있습니다.

122
00:07:44,120 --> 00:07:47,230
하지만 encoder와 decoder가 있을 때,

123
00:07:47,230 --> 00:07:49,810
이것은 제가 잠시 설명할

124
00:07:49,810 --> 00:07:52,570
autoencoding 프레임워크에 더 가깝습니다.

125
00:07:52,570 --> 00:07:59,810
그래서 pretext task로 학습을 마친 후에는 encoder와 학습된

126
00:07:59,810 --> 00:08:03,890
표현을 downstream task에 사용할

127
00:08:03,890 --> 00:08:07,050
수 있는데, 여기서는 한 층

128
00:08:07,050 --> 00:08:12,830
또는 완전 연결 신경망, 선형 함수, 혹은 완전 연결 신경망을

129
00:08:12,830 --> 00:08:16,010
추가해서 라벨을 예측하면 됩니다.

130
00:08:16,010 --> 00:08:18,410
이 라벨은 이제 데이터셋에서

131
00:08:18,410 --> 00:08:20,570
나오는 것입니다.

132
00:08:20,570 --> 00:08:24,450
이것이 self-supervised

133
00:08:24,450 --> 00:08:31,230
learning의 주요 개념입니다. 즉, pretext 버전은

134
00:08:31,230 --> 00:08:35,929
일부가 라벨이 없는 데이터로 학습할 수 있다는

135
00:08:35,929 --> 00:08:37,289
점입니다.

136
00:08:37,289 --> 00:08:39,210
하지만 pretext

137
00:08:39,210 --> 00:08:43,169
task 자체를 정의하는 것은 그렇게 간단하지 않습니다.

138
00:08:43,169 --> 00:08:46,560
정의하는 방법이 여러 가지가 있습니다.

139
00:08:46,560 --> 00:08:50,740
예를 들어, pretext task를

140
00:08:50,740 --> 00:08:56,500
정의할 때는 첫째, 충분히 일반적이어야 하고 좋은 특징을

141
00:08:56,500 --> 00:09:02,020
얻을 수 있어야 하며, 수동 라벨링이 필요 없어야 한다는

142
00:09:02,020 --> 00:09:04,120
점을 기억하세요.

143
00:09:04,120 --> 00:09:07,492
라벨은 데이터 자체에서 나와야 합니다.

144
00:09:07,492 --> 00:09:14,540
예를 들어 이미지 완성(image completion)이 있는데, 이미지의
절반

145
00:09:14,540 --> 00:09:16,360
또는 일부를 가리고,

146
00:09:16,360 --> 00:09:21,460
가려지지 않은 부분을 주면 가려진 부분을 예측하는

147
00:09:21,460 --> 00:09:23,960
작업을 정의하는 것입니다.

148
00:09:23,960 --> 00:09:28,740
예를 들어, 이미지를 특정 각도로 회전시키고,

149
00:09:28,740 --> 00:09:32,660
입력으로 이미지를 받아서

150
00:09:32,660 --> 00:09:37,340
회전된 각도를 예측하는 작업이 있습니다.

151
00:09:37,340 --> 00:09:41,740
또 다른 예로는 퍼즐 조각처럼 이미지의

152
00:09:41,740 --> 00:09:44,850
패치들이 순서 없이 주어지고, 이

153
00:09:44,850 --> 00:09:50,590
패치들의 올바른 순서를 출력하는 작업이 있을 수 있습니다.

154
00:09:50,590 --> 00:09:53,550
그리고 컬러화(colorization)는 인기

155
00:09:53,550 --> 00:09:55,070
있는 작업 중 하나로,

156
00:09:55,070 --> 00:09:58,350
오늘 빠르게 다룰 네 가지 작업 중 하나입니다.

157
00:09:58,350 --> 00:10:04,850
흑백 이미지가 주어졌을 때, 각 픽셀의

158
00:10:04,850 --> 00:10:08,350
색상을 예측하는 겁니다.

159
00:10:08,350 --> 00:10:12,350
이렇게 사전 과제(pretext task)를 해결하면 모델이 좋은

160
00:10:12,350 --> 00:10:13,810
특징을 학습할 수 있습니다.

161
00:10:13,810 --> 00:10:15,730
바로 그게 우리가 원했던 것입니다.

162
00:10:15,730 --> 00:10:18,070
그리고 사전 과제에 대한 레이블을

163
00:10:18,070 --> 00:10:20,210
자동으로 생성할 수 있습니다.

164
00:10:20,210 --> 00:10:22,430
그래서 제가 언급한 두 가지

165
00:10:22,430 --> 00:10:27,350
포인트는 자기지도 학습(self-supervised learning)에

166
00:10:27,350 --> 00:10:30,270
적합한 좋은 사전 과제 조건입니다.

167
00:10:30,270 --> 00:10:34,470
항상 염두에 둬야 할 다른 빠른 고려사항은

168
00:10:34,470 --> 00:10:38,510
자기지도 학습 프레임워크를 어떻게 평가할

169
00:10:38,510 --> 00:10:39,510
것인가입니다.

170
00:10:39,510 --> 00:10:44,560
사전 과제 자체를 살펴볼 수 있는 다양한 부분과 영역이

171
00:10:44,560 --> 00:10:47,240
있습니다. 왜냐하면 레이블을

172
00:10:47,240 --> 00:10:50,160
우리가 생성하기 때문입니다.

173
00:10:50,160 --> 00:10:53,880
이것이 모델이 사전 과제를 얼마나

174
00:10:53,880 --> 00:10:59,080
잘 해결하는지 평가할 수 있는 힘을 줍니다.

175
00:10:59,080 --> 00:11:03,700
이것이 한 가지 평가 요소입니다.

176
00:11:03,700 --> 00:11:06,120
그리고 표현(representation)

177
00:11:06,120 --> 00:11:08,840
품질 자체도 때때로 매우 중요합니다.

178
00:11:08,840 --> 00:11:14,200
예를 들어, 미세 조정(fine tuning) 없이 표현만 보고,

179
00:11:14,200 --> 00:11:15,960
또는 제가 곧 이야기할

180
00:11:15,960 --> 00:11:17,760
것들 없이, 심지어

181
00:11:17,760 --> 00:11:20,680
표현을 클러스터링해서 표현에서

182
00:11:20,680 --> 00:11:23,160
패턴이 보이는지 확인하는 겁니다.

183
00:11:23,160 --> 00:11:27,000
때때로 좋은 차원 축소 알고리즘도

184
00:11:27,000 --> 00:11:28,060
있습니다.

185
00:11:28,060 --> 00:11:33,120
여기서 제가 말하는 것은 t-SNE인데, 이건

186
00:11:33,120 --> 00:11:35,240
우리가 많이 다루지

187
00:11:35,240 --> 00:11:37,880
않았지만, 학습된 표현의

188
00:11:37,880 --> 00:11:41,940
차원을 축소해서 2D나 3D로 시각화하고

189
00:11:41,940 --> 00:11:45,060
표현에서 패턴을 찾을 수 있는

190
00:11:45,060 --> 00:11:48,140
차원 축소 프레임워크입니다.

191
00:11:48,140 --> 00:11:52,440
그래서 견고성, 일반화, 계산 효율성, 이 모든

192
00:11:52,440 --> 00:11:54,540
것이 꽤 중요합니다.

193
00:11:54,540 --> 00:11:59,660
하지만 우리가 가장 중요하게 생각하는

194
00:11:59,660 --> 00:12:04,780
것은 바로 다운스트림 작업에서의

195
00:12:04,780 --> 00:12:06,880
성능입니다.

196
00:12:06,880 --> 00:12:11,380
왜냐하면 우리는 전체적인 self-supervised learning을

197
00:12:11,380 --> 00:12:14,020
수행하고, 관심 있는 작업이나

198
00:12:14,020 --> 00:12:17,180
우리가 신경 쓰는 작업의 결과를 향상시키기

199
00:12:17,180 --> 00:12:21,100
위해 pretext task 등을 정의하기 때문입니다.

200
00:12:21,100 --> 00:12:25,700
이것이 어떻게 이루어질 수 있는지 간단한 예를 살펴보겠습니다.

201
00:12:25,700 --> 00:12:31,260
이 예는 이미지를 회전시키고 회전

202
00:12:31,260 --> 00:12:36,280
각도를 출력으로 예측하는 예입니다.

203
00:12:36,280 --> 00:12:39,270
그래서 이미지 내 객체의 레이블이 필요 없이

204
00:12:39,270 --> 00:12:43,110
self-supervised 방식으로 학습할 수 있습니다.

205
00:12:43,110 --> 00:12:45,350
이 예에서는 여러 개의

206
00:12:45,350 --> 00:12:48,510
convolutional layer와 마지막에 fully

207
00:12:48,510 --> 00:12:52,550
connected neural network가 있어 회귀나 분류

208
00:12:52,550 --> 00:12:53,590
작업을 수행합니다.

209
00:12:53,590 --> 00:12:56,430
즉, 이것은 좋은 특징

210
00:12:56,430 --> 00:13:02,270
추출기, 좋은 feature extractor를

211
00:13:02,270 --> 00:13:07,990
제공하며, 마지막에 pretext task에 특화된

212
00:13:07,990 --> 00:13:13,630
FC layer를 제거하고 한 개 또는 경우에

213
00:13:13,630 --> 00:13:17,990
따라 여러 개의 층을 추가해 특징을 객체

214
00:13:17,990 --> 00:13:21,490
레이블로 분류할 수 있습니다.

215
00:13:21,490 --> 00:13:26,070
이번에는 객체 레이블을 사용해 예측하고

216
00:13:26,070 --> 00:13:30,370
이 선형 함수 자체를 학습합니다.

217
00:13:30,370 --> 00:13:33,770
그래서 특징이 충분히 좋다면,

218
00:13:33,770 --> 00:13:35,960
클래스 레이블을

219
00:13:35,960 --> 00:13:42,760
뽑아내는 데 많은 학습이 필요 없기 때문에 얕은

220
00:13:42,760 --> 00:13:47,000
네트워크를 찾는 경우가 많습니다.

221
00:13:47,000 --> 00:13:52,360
이것이 일반적인 self-supervised learning입니다.

222
00:13:52,360 --> 00:13:56,200
비록 우리가 컴퓨터 비전 응용에 대해

223
00:13:56,200 --> 00:13:57,480
이야기하고 있지만,

224
00:13:57,480 --> 00:14:01,360
이 self-supervised learning

225
00:14:01,360 --> 00:14:07,080
패러다임이 사실 모든 대형 언어 모델을 가능하게 했습니다.

226
00:14:07,080 --> 00:14:10,000
GPT-4와 이 모든

227
00:14:10,000 --> 00:14:18,440
프레임워크는 대부분 수동 레이블링 없이 원시 데이터로 학습됩니다.

228
00:14:18,440 --> 00:14:21,940
그리고 언어 모델뿐 아니라 음성,

229
00:14:21,940 --> 00:14:25,840
그리고 요즘에는 로봇 공학과

230
00:14:25,840 --> 00:14:29,640
강화 학습에서도 많이 활용됩니다.

231
00:14:29,640 --> 00:14:34,390
왜냐하면 수동 레이블링 데이터가 필요 없으면,

232
00:14:34,390 --> 00:14:41,410
수동 레이블링 없이 원시 데이터를 수집하고 이를 학습에 사용할 수

233
00:14:41,410 --> 00:14:43,310
있기 때문입니다.

234
00:14:43,310 --> 00:14:47,410
그래서 베이 지역에서 자율주행차가 데이터를 수집하는 걸

235
00:14:47,410 --> 00:14:51,550
많이 볼 수 있는 이유가 바로 그것 때문입니다. 데이터를

236
00:14:51,550 --> 00:14:53,790
얻는 것이고, 데이터를 일일이

237
00:14:53,790 --> 00:14:57,690
주석 달지 않아도 모델을 학습시킬 수 있기 때문입니다.

238
00:14:57,690 --> 00:15:03,410
그럼 오늘의 일정은 이미지 변환을 이용한 몇 가지 사전 학습

239
00:15:03,410 --> 00:15:06,990
과제를 다루고, 이어서 이미지 변환

240
00:15:06,990 --> 00:15:08,570
기반 사전 학습

241
00:15:08,570 --> 00:15:13,490
과제와는 약간 다른 대조 표현 학습(contrastive

242
00:15:13,490 --> 00:15:19,330
representation learning) 알고리즘에

243
00:15:19,330 --> 00:15:22,730
대해 조금 이야기하겠습니다. 이

244
00:15:22,730 --> 00:15:26,450
알고리즘들은 가능성을 보여주고 있습니다.

245
00:15:26,450 --> 00:15:30,450
그럼 첫 번째 부분부터 시작하겠습니다.

246
00:15:30,450 --> 00:15:36,460
거기서 과제들을 하나씩 다루겠습니다.

247
00:15:36,460 --> 00:15:41,720
회전 예측에 대해 꽤 많이 이야기했죠.

248
00:15:41,720 --> 00:15:45,820
이미지를 임의의 각도로

249
00:15:45,820 --> 00:15:54,300
회전시키고, 모델로 그 회전 각도를 예측할 수 있는지

250
00:15:54,300 --> 00:15:56,840
보겠습니다.

251
00:15:56,840 --> 00:16:00,860
여기서 가설은 모델이 객체가 원래 어떻게

252
00:16:00,860 --> 00:16:05,380
보여야 하는지에 대한 시각적 상식(visual

253
00:16:05,380 --> 00:16:09,940
common sense)을 가지고

254
00:16:09,940 --> 00:16:14,660
있어야만 올바른 회전을 인식할 수 있다는 겁니다.

255
00:16:14,660 --> 00:16:19,940
이 모델들은 주로 이런 시각적 상식 개념을

256
00:16:19,940 --> 00:16:23,420
중심으로 설계되었습니다.

257
00:16:23,420 --> 00:16:28,700
그리고 모델이 그것을 잘 포착할

258
00:16:28,700 --> 00:16:32,870
수 있다면, 이미지 전체를

259
00:16:32,870 --> 00:16:40,050
유용한 특징 집합으로 요약할 수 있다는

260
00:16:40,050 --> 00:16:42,150
의미입니다.

261
00:16:42,150 --> 00:16:48,590
2018년에 발표된 이 논문은 0도,

262
00:16:48,590 --> 00:16:53,350
90도, 180도, 270도 네

263
00:16:53,350 --> 00:17:03,750
가지 각도만 탐색하며 이미지를 회전시키고, 합성곱 신경망(CNN)을

264
00:17:03,750 --> 00:17:08,790
사용해 각 회전이 무엇인지

265
00:17:08,790 --> 00:17:12,150
예측하는 방식을

266
00:17:12,150 --> 00:17:14,190
구현했습니다.

267
00:17:14,190 --> 00:17:17,210
네 가지 출력만 만들었기 때문에, 이건

268
00:17:17,210 --> 00:17:19,510
분류 과제입니다. 네 가지 경우

269
00:17:19,510 --> 00:17:21,230
중 하나를 맞추는 거죠.

270
00:17:21,230 --> 00:17:23,990
정확한 각도 값을 예측할

271
00:17:23,990 --> 00:17:26,089
필요는 없습니다.

272
00:17:26,089 --> 00:17:31,280
단지 0, 1, 2, 3 중

273
00:17:31,280 --> 00:17:38,016
하나의 클래스를 예측하는 겁니다.

274
00:17:38,016 --> 00:17:42,000
0, 1, 2, 3 중 하나입니다.

275
00:17:42,000 --> 00:17:49,040
이렇게 해서 저자들은 좋은 표현을 학습할 수

276
00:17:49,040 --> 00:17:50,920
있었습니다.

277
00:17:50,920 --> 00:17:55,840
그리고 그 표현을 바탕으로 신경망을

278
00:17:55,840 --> 00:17:57,880
다운스트림 응용에

279
00:17:57,880 --> 00:18:00,640
맞춰 미세 조정(fine

280
00:18:00,640 --> 00:18:07,520
tuning)하기 시작했습니다. 인코더와

281
00:18:07,520 --> 00:18:12,740
분류기를 함께 미세 조정하는 방식입니다.

282
00:18:12,740 --> 00:18:19,560
사실 이 경우에는 첫 번째와 두 번째 레이어를 고정시키고, 마지막

283
00:18:19,560 --> 00:18:23,560
컨볼루션 레이어와 선형 레이어만 미세

284
00:18:23,560 --> 00:18:25,060
조정했습니다.

285
00:18:25,060 --> 00:18:32,010
그래서 전체 네트워크를 완전히 미세 조정한 것은 아니지만, 매우

286
00:18:32,010 --> 00:18:35,910
좋은 결과를 얻을 수 있었습니다.

287
00:18:35,910 --> 00:18:39,890
이것은 우리가 앞서 이야기한 데이터셋 중 하나인

288
00:18:39,890 --> 00:18:42,330
CIFAR10 데이터셋에서의 결과입니다.

289
00:18:42,330 --> 00:18:45,350
모델이 사전 학습되어 있을 때, 처음부터

290
00:18:45,350 --> 00:18:48,430
좋은 정확도로 시작하는 것을 볼 수 있습니다.

291
00:18:48,430 --> 00:18:52,650
즉, 아주 첫 번째 반복에서도

292
00:18:52,650 --> 00:18:57,330
이미 좋은 상태이고, 객체에

293
00:18:57,330 --> 00:19:01,810
대한 이해도가 높다는 의미입니다.

294
00:19:01,810 --> 00:19:04,150
하지만 CIFAR10은

295
00:19:04,150 --> 00:19:09,250
모델을 학습시키기에 그렇게 어렵지 않은 간단한 작업입니다.

296
00:19:09,250 --> 00:19:12,470
작업이 충분히 간단하다면, 완전 감독

297
00:19:12,470 --> 00:19:14,810
학습 버전과 사전 학습으로

298
00:19:14,810 --> 00:19:17,510
시작하는 버전은 종종 같은

299
00:19:17,510 --> 00:19:19,450
정확도로 수렴합니다.

300
00:19:19,450 --> 00:19:22,130
하지만 다시 말하지만, 작업이

301
00:19:22,130 --> 00:19:25,130
매우 어려운 경우, 대규모 사전

302
00:19:25,130 --> 00:19:27,390
학습을 하지 않은 감독 학습

303
00:19:27,390 --> 00:19:29,590
프레임워크는 좋은 결과를

304
00:19:29,590 --> 00:19:31,270
얻기 어렵습니다.

305
00:19:31,270 --> 00:19:34,270
네.

306
00:19:34,270 --> 00:19:42,590
또한 PASCAL VOC 2017

307
00:19:42,590 --> 00:19:51,110
데이터셋에서 분류, 검출, 분할을 포함한

308
00:19:51,110 --> 00:19:53,890
여러 작업에

309
00:19:53,890 --> 00:19:59,510
대한 실험도 진행했습니다.

310
00:19:59,510 --> 00:20:03,910
이 세 가지 작업에 대해, 분류,

311
00:20:03,910 --> 00:20:08,470
검출, 분할 작업별로 몇 개의 완전

312
00:20:08,470 --> 00:20:10,870
연결층만 학습하거나 모든

313
00:20:10,870 --> 00:20:13,870
레이어를 학습하는 다양한

314
00:20:13,870 --> 00:20:16,550
설정을 사용했습니다.

315
00:20:16,550 --> 00:20:19,390
ImageNet 레이블을

316
00:20:19,390 --> 00:20:23,360
보면, 방대한 레이블 데이터셋이 있고

317
00:20:23,360 --> 00:20:27,160
그 데이터셋으로 사전 학습을 하면,

318
00:20:27,160 --> 00:20:29,340
이미 매우 높은 정확도를 얻을 수 있습니다.

319
00:20:29,340 --> 00:20:31,440
하지만 이 점을 기억하세요,

320
00:20:31,440 --> 00:20:34,080
이건 사전 학습에 모든 ImageNet

321
00:20:34,080 --> 00:20:36,360
레이블이 포함된 경우입니다.

322
00:20:36,360 --> 00:20:40,880
만약 감독 사전 학습을 하지 않고, 사전

323
00:20:40,880 --> 00:20:44,140
학습이 모두 자기지도 학습

324
00:20:44,140 --> 00:20:49,760
기반이라면, 이 회전(rotation) 프레임워크가

325
00:20:49,760 --> 00:20:54,780
다른 많은 방법들보다 훨씬 더 좋은 성능을

326
00:20:54,780 --> 00:21:01,560
보이고 있습니다. 많은 방법들의 세부사항까지는 다루지

327
00:21:01,560 --> 00:21:04,540
않지만, 이 회전 사전

328
00:21:04,540 --> 00:21:09,720
과제(pretext task)의 효능을 보여줍니다.

329
00:21:09,720 --> 00:21:12,800
그리고 무작위 초기화로 시작했을 때와

330
00:21:12,800 --> 00:21:17,960
비교해 얼마나 차이가 나는지, 얼마나 더 나은지 확인해 보세요.

331
00:21:17,960 --> 00:21:21,530
그래서 무작위 초기화와 회전이라는 pretext task를

332
00:21:21,530 --> 00:21:24,130
이용한 사전학습의 차이가 있습니다.

333
00:21:24,130 --> 00:21:29,970
그 차이는 매우 크고, 이 회전 pretext task는 완전히

334
00:21:29,970 --> 00:21:33,610
같지는 않지만 전체 ImageNet에서의

335
00:21:33,610 --> 00:21:35,730
사전학습과 거의 비슷합니다.

336
00:21:35,730 --> 00:21:40,610
이 논문에서 또 살펴본 한 가지는

337
00:21:40,610 --> 00:21:43,410
학습된 특징들이

338
00:21:43,410 --> 00:21:48,850
얼마나 의미 있는지 보는 것이었습니다.

339
00:21:48,850 --> 00:21:54,330
앞서 말씀드렸듯이, pretext task나 일반적인
self-supervised

340
00:21:54,330 --> 00:21:56,930
learning 프레임워크를 평가하는 방법

341
00:21:56,930 --> 00:21:59,070
중 하나는 특징을 보는 것입니다.

342
00:21:59,070 --> 00:22:01,730
그리고 항상 완전 연결층의 특징에서

343
00:22:01,730 --> 00:22:03,730
시작할 수 있습니다.

344
00:22:03,730 --> 00:22:07,770
우리는 grad cam이나 다른 주의(attention) 기반

345
00:22:07,770 --> 00:22:10,690
프레임워크들을 이야기했는데, 특징에서 이미지

346
00:22:10,690 --> 00:22:12,330
공간으로 되돌아가는 방법들이죠.

347
00:22:12,330 --> 00:22:15,730
그래서 이 평가는 특징을 이미지 공간으로 투영해서

348
00:22:15,730 --> 00:22:19,380
모델이 어디를 보고 있는지 확인하는 것입니다.

349
00:22:19,380 --> 00:22:22,760
감독 학습 모델의 주의 맵을

350
00:22:22,760 --> 00:22:25,180
보면, 보통 감독 학습

351
00:22:25,180 --> 00:22:29,900
모델은 분류라는 단일 과제만 해결하기 때문에

352
00:22:29,900 --> 00:22:33,080
더 집중된 맵을 가집니다.

353
00:22:33,080 --> 00:22:36,460
그래서 눈과 그 주변 형태를 잡으면 다른

354
00:22:36,460 --> 00:22:39,460
부분은 크게 신경 쓰지 않습니다.

355
00:22:39,460 --> 00:22:42,640
하지만 self-supervised

356
00:22:42,640 --> 00:22:45,940
learning의 경우, 더 많은 특징과 영역이

357
00:22:45,940 --> 00:22:50,580
포함되는 경우가 많습니다. 왜냐하면 downstream task가

358
00:22:50,580 --> 00:22:52,340
무엇인지 모르기 때문에

359
00:22:52,340 --> 00:22:55,460
이미지에 대해 더 전체적인 이해가 필요하고,

360
00:22:55,460 --> 00:23:00,060
여러 과제에서 모두 잘 수행하는 것이 목표이기 때문입니다.

361
00:23:00,060 --> 00:23:02,940
이것이 바로 그 과제 중 하나입니다.

362
00:23:02,940 --> 00:23:05,240
질문이 있으면 계속 적어두세요.

363
00:23:05,240 --> 00:23:08,540
몇 가지 과제를 다룬 후에 멈추겠습니다.

364
00:23:08,540 --> 00:23:10,980
그리고 질문이 답변되지

365
00:23:10,980 --> 00:23:13,940
않았다면 기꺼이 답변해 드리겠습니다.

366
00:23:13,940 --> 00:23:16,710
좋습니다.

367
00:23:16,710 --> 00:23:21,910
또 다른 인기 있는 pretext task는

368
00:23:21,910 --> 00:23:26,910
기본적으로 3x3 격자를 만들고 네트워크를

369
00:23:26,910 --> 00:23:33,030
사용해 센서 패치에 대해 주어진 각 패치의

370
00:23:33,030 --> 00:23:36,310
위치를 예측하는 것이었습니다.

371
00:23:36,310 --> 00:23:40,790
그래서 여기서 나온 이 패치에 대해서는 출력이

372
00:23:40,790 --> 00:23:46,950
3이어야 합니다. 왜냐하면 8개만 있기 때문입니다-- 이건

373
00:23:46,950 --> 00:23:50,450
3x3이고, 가운데 패치가 기준입니다.

374
00:23:50,450 --> 00:23:56,270
그래서 이것도 8방향 분류 작업이 됩니다.

375
00:23:56,270 --> 00:23:59,270
이 패치들 중 하나를

376
00:23:59,270 --> 00:24:04,630
받아서, 그 패치가 가운데 패치에

377
00:24:04,630 --> 00:24:11,300
대해 어느 위치인지 출력하려고 하는 거죠.

378
00:24:16,200 --> 00:24:18,780
이것도 또 다른 예시였습니다.

379
00:24:18,780 --> 00:24:25,040
하지만 이 후속 논문에서는, 이걸 퍼즐 맞추기(jigsaw

380
00:24:25,040 --> 00:24:29,520
puzzle) 프레임워크로 바꿨는데,

381
00:24:29,520 --> 00:24:34,480
모델에게 단순히 8개 패치 중 어느 것인지

382
00:24:34,480 --> 00:24:37,240
맞추라고 하는 대신, 정확한

383
00:24:37,240 --> 00:24:41,060
순열, 올바른 순열을 예측하도록

384
00:24:41,060 --> 00:24:42,020
했습니다.

385
00:24:42,020 --> 00:24:46,160
그래서 그들은 같은 3x3

386
00:24:46,160 --> 00:24:50,960
그리드를 사용해서 모든 패치를

387
00:24:50,960 --> 00:24:59,080
무작위로 섞고, 신경망에게 어떤 순열이 올바른지

388
00:24:59,080 --> 00:25:01,480
맞추라고 했습니다.

389
00:25:01,480 --> 00:25:04,277
즉, 올바른 순열을 예측하는 거죠.

390
00:25:04,277 --> 00:25:06,360
이 설정에서 가능한

391
00:25:06,360 --> 00:25:11,840
순열의 개수가 몇 개인지 말할 수 있나요?

392
00:25:11,840 --> 00:25:13,250
다시요?

393
00:25:13,250 --> 00:25:14,110
9 팩토리얼입니다.

394
00:25:14,110 --> 00:25:14,750
네, 맞습니다.

395
00:25:14,750 --> 00:25:16,570
정말 큰 숫자죠?

396
00:25:16,570 --> 00:25:19,930
30만 몇 천 정도였던 것 같아요.

397
00:25:19,930 --> 00:25:22,410
하지만 그들은

398
00:25:22,410 --> 00:25:32,130
64개의 가능한 순열만 포함하는 조회 테이블을

399
00:25:32,130 --> 00:25:33,410
만들었습니다.

400
00:25:33,410 --> 00:25:38,030
그래서 64개의 순열만 고려합니다.

401
00:25:38,030 --> 00:25:41,450
그리고 그 중 하나를 기반으로

402
00:25:41,450 --> 00:25:43,230
섞는 작업을 합니다.

403
00:25:43,230 --> 00:25:48,330
그리고 출력도 60, 64 크기의 벡터가 됩니다.

404
00:25:48,330 --> 00:25:52,050
다시 말해, 이것은 64개의 출력

405
00:25:52,050 --> 00:25:56,090
클래스를 가진 단순한 분류 문제로 나타납니다.

406
00:25:56,090 --> 00:25:59,170
이들은 이것이 사전학습(pretext)

407
00:25:59,170 --> 00:26:04,470
과제로 정의하는 데도 훌륭한 아이디어임을

408
00:26:04,470 --> 00:26:09,130
보여주었고, 제가 말한 것과 유사한 유형의 작업과

409
00:26:09,130 --> 00:26:14,110
감독 방식으로 같은 데이터셋에서 수행했습니다.

410
00:26:14,110 --> 00:26:17,550
그들의 방법이 이전의 몇몇

411
00:26:17,550 --> 00:26:25,190
모델이나 프레임워크보다 성능이 뛰어남을 입증했습니다.

412
00:26:25,190 --> 00:26:30,870
그리고 다시 한 번, 이 연구가 2016년에 발표되었다는 점을 기억하세요.

413
00:26:30,870 --> 00:26:36,590
다음 사전학습 과제는 인페인팅(inpainting), 즉 결손된

414
00:26:36,590 --> 00:26:38,710
부분을 예측하는 것입니다.

415
00:26:38,710 --> 00:26:44,310
여기서 그들이 한 것은 간단한 마스킹 전략으로, 이미지의

416
00:26:44,310 --> 00:26:47,590
일부를 가리고 모델에게 그

417
00:26:47,590 --> 00:26:51,230
가려진 부분을 복원하도록 하는 겁니다.

418
00:26:51,230 --> 00:26:57,010
어떻게 했냐면, 입력 이미지에 단순히 마스킹을 적용했습니다.

419
00:26:57,010 --> 00:27:00,170
하지만 모든 이미지가 있기 때문에 원하는

420
00:27:00,170 --> 00:27:02,790
출력도 실제로 가지고 있습니다.

421
00:27:02,790 --> 00:27:06,610
인코더가 이를 특징 공간(feature

422
00:27:06,610 --> 00:27:08,816
space)으로

423
00:27:08,816 --> 00:27:12,300
변환하고, 중간에 완전 연결층들이

424
00:27:12,300 --> 00:27:14,840
있으며, 디코더가 가려진

425
00:27:14,840 --> 00:27:16,920
부분을 복원합니다.

426
00:27:16,920 --> 00:27:22,760
손실 함수는 출력과 실제 정답을

427
00:27:22,760 --> 00:27:27,720
비교하는 방식입니다.

428
00:27:27,720 --> 00:27:32,680
기본적으로 결손된 픽셀을 재구성하는 학습을

429
00:27:32,680 --> 00:27:34,560
하는 거죠.

430
00:27:34,560 --> 00:27:38,800
우리가 이전에 오토인코더(autoencoder)에

431
00:27:38,800 --> 00:27:43,360
대해 몇 번 이야기했는데, 이것도

432
00:27:43,360 --> 00:27:47,720
입력 이미지를 인코딩해서 원하는 출력을

433
00:27:47,720 --> 00:27:51,860
디코딩하는 일종의 오토인코더입니다.

434
00:27:51,860 --> 00:27:57,600
하지만 이 오토인코더는 마스킹 전략, 즉 마스킹 목적함수로

435
00:27:57,600 --> 00:27:58,760
학습됩니다.

436
00:27:58,760 --> 00:28:03,080
몇 가지 예를 보여드리자면, 인페인팅 평가는

437
00:28:03,080 --> 00:28:07,930
조금 흥미롭고 까다로운데, 인페인팅을 할 때 이

438
00:28:07,930 --> 00:28:10,710
이미지에는 복원할 수 있는

439
00:28:10,710 --> 00:28:13,630
방법이 매우 다양하기 때문입니다.

440
00:28:13,630 --> 00:28:26,570
그리고 이 경우에 단 하나의 출력만 있다고 말할 수 있습니다.

441
00:28:26,570 --> 00:28:28,770
재구성 기반

442
00:28:28,770 --> 00:28:31,370
프레임워크, 초기 재구성

443
00:28:31,370 --> 00:28:42,410
기반 프레임워크들은 실제로 매우 흐릿하고 부드러운 결과물을 많이
만들어냈습니다.

444
00:28:42,410 --> 00:28:49,530
그래서 제가 여기서 언급하는 논문은 추가적인 적대적 목적
함수(adversarial

445
00:28:49,530 --> 00:28:54,250
objective function)를 사용했는데,

446
00:28:54,250 --> 00:28:56,890
이 부분은 다음 강의인 생성

447
00:28:56,890 --> 00:29:00,950
모델에서 다룰 주제라 자세히 설명하지 않겠습니다.

448
00:29:00,950 --> 00:29:05,730
일반적으로 이런 프레임워크가 작동하는 방식은 재구성
손실(reconstruction

449
00:29:05,730 --> 00:29:07,680
loss)을 갖는 것입니다.

450
00:29:07,680 --> 00:29:09,580
재구성 손실은

451
00:29:09,580 --> 00:29:13,620
기본적으로 패치, 즉

452
00:29:13,620 --> 00:29:18,660
이미지 x와 인코더를 통과한 후의

453
00:29:18,660 --> 00:29:24,340
이미지 간 차이를 계산하는 겁니다.

454
00:29:24,340 --> 00:29:33,340
그리고 이것은 요소별 곱셈(element wise
multiplication)입니다.

455
00:29:33,340 --> 00:29:35,740
또한 손실 함수, 목적

456
00:29:35,740 --> 00:29:38,300
함수를 마스크된 영역에만 계산하기

457
00:29:38,300 --> 00:29:41,520
위해 여기 마스크도 사용합니다.

458
00:29:41,520 --> 00:29:45,180
그래서 마스크와 요소별 곱셈을 수행하는

459
00:29:45,180 --> 00:29:46,020
거죠.

460
00:29:46,020 --> 00:29:51,460
이것이 우리가 가진 마스크 부분에 대한

461
00:29:51,460 --> 00:29:55,800
재구성 손실을 계산해 줍니다.

462
00:29:55,800 --> 00:29:59,260
말씀드렸듯이, 여기에

463
00:29:59,260 --> 00:30:02,820
적대적 학습 손실 함수가

464
00:30:02,820 --> 00:30:09,070
추가되어 생성된 이미지가 실제처럼 보이도록

465
00:30:09,070 --> 00:30:10,910
보장합니다.

466
00:30:10,910 --> 00:30:17,350
이 덕분에 재구성된 부분이 좀 더

467
00:30:17,350 --> 00:30:24,830
나아 보이도록 개선할 수 있었습니다.

468
00:30:24,830 --> 00:30:31,630
하지만 자세한 내용은 다음 강의에서 다루겠습니다.

469
00:30:31,630 --> 00:30:35,990
이 재구성 프레임워크는 동일한

470
00:30:35,990 --> 00:30:39,550
분류, 검출, 세분화

471
00:30:39,550 --> 00:30:43,190
작업과 동일한

472
00:30:43,190 --> 00:30:46,830
데이터셋에서 실행했을 때

473
00:30:46,830 --> 00:30:52,710
추가적인 이점을 제공할 수 있었습니다.

474
00:30:52,710 --> 00:30:56,710
이 재구성 기반 프레임워크와 마스킹에 대해서는

475
00:30:56,710 --> 00:30:59,110
조금 후에 다시 다룰

476
00:30:59,110 --> 00:31:05,440
텐데, 요즘 사전 학습에 가장 많이 사용되는 모델 또는 프리텍스트 작업

477
00:31:05,440 --> 00:31:07,800
중 하나이기 때문입니다.

478
00:31:07,800 --> 00:31:10,140
그래서 다시 돌아올 겁니다.

479
00:31:10,140 --> 00:31:15,520
하지만 그 전에, 이미지 컬러링이라는 또 다른

480
00:31:15,520 --> 00:31:19,200
프리텍스트 작업을 소개하겠습니다.

481
00:31:19,200 --> 00:31:24,400
이것은 또 다른 매우 간단한 프레임워크 설정으로, 우리의

482
00:31:24,400 --> 00:31:30,023
데이터셋이 대부분 컬러 이미지이기 때문에 컬러 이미지를 변환하는

483
00:31:30,023 --> 00:31:30,940
작업입니다.

484
00:31:30,940 --> 00:31:36,680
우리는 그 컬러 이미지를 밝기, 조명과 색상

485
00:31:36,680 --> 00:31:41,880
자체를 분리하는 구성 요소나 채널로

486
00:31:41,880 --> 00:31:42,500
나눕니다.

487
00:31:42,500 --> 00:31:44,560
컴퓨터 그래픽스나 CS-131,

488
00:31:44,560 --> 00:31:47,160
다른 컴퓨터 비전 수업을 들으셨다면

489
00:31:47,160 --> 00:31:51,060
여러 가지 색 공간이 있다는 것을 아실 겁니다.

490
00:31:51,060 --> 00:31:54,900
색 공간이 정말 다양하다는 것을 알고 계시죠.

491
00:31:54,900 --> 00:31:57,960
대부분 컴퓨터 비전에서는 RGB를 사용합니다.

492
00:31:57,960 --> 00:32:02,130
하지만 밝기, 조명을 색상과 분리하고 싶다면

493
00:32:02,130 --> 00:32:04,830
다른 색 공간들이 있습니다.

494
00:32:04,830 --> 00:32:08,330
예를 들어, LAB 색 공간은

495
00:32:08,330 --> 00:32:12,930
밝기와 색상을 분리하는 색 공간 중

496
00:32:12,930 --> 00:32:14,410
하나입니다.

497
00:32:14,410 --> 00:32:18,170
그래서 밝기를 위한 한 채널과 실제

498
00:32:18,170 --> 00:32:21,970
색상을 정의하는 두 채널이 있습니다.

499
00:32:21,970 --> 00:32:27,897
그리고 이 세 채널 L, A, B를 모두 합치면

500
00:32:27,897 --> 00:32:29,730
컬러 이미지를

501
00:32:29,730 --> 00:32:31,850
얻을 수 있습니다.

502
00:32:31,850 --> 00:32:34,770
여기서 사전 학습 과제는 간단합니다.

503
00:32:34,770 --> 00:32:40,690
L 채널이 주어졌을 때 A와 B 채널을 예측하는 거죠.

504
00:32:40,690 --> 00:32:44,310
다시 말해, 수동으로 주석을 달 필요가 없습니다.

505
00:32:44,310 --> 00:32:47,530
이미 데이터 안에 포함되어 있습니다.

506
00:32:47,530 --> 00:32:52,150
이것은 다른 프레임워크로도 확장되었습니다.

507
00:32:52,150 --> 00:32:56,730
왜 L이 주어졌을 때 A와 B를 예측하는 것만 보아야 할까요?

508
00:32:56,730 --> 00:32:58,860
반대로도 할 수 있습니다, 맞죠?

509
00:32:58,860 --> 00:33:03,180
이것이 우리가 split brain autoencoder라고

510
00:33:03,180 --> 00:33:06,660
부르는 것을 만들게 된

511
00:33:06,660 --> 00:33:12,300
계기입니다, 입력 이미지를 분할해서 기본적으로 하나로 만드는 거죠.

512
00:33:12,300 --> 00:33:16,140
L 채널, 밝기 채널, 그리고 색상 채널입니다.

513
00:33:16,140 --> 00:33:18,480
이 두 이미지, 이것이 한 채널입니다.

514
00:33:18,480 --> 00:33:20,340
이것은 두 개의 색상 채널입니다.

515
00:33:20,340 --> 00:33:24,120
그리고 우리는 두 개의 함수, 두 개의 신경망, 즉 다른

516
00:33:24,120 --> 00:33:26,780
하나를 예측하는 레이어 집합을 훈련합니다.

517
00:33:26,780 --> 00:33:30,780
그리고 마지막에 손실 함수와 역전파를

518
00:33:30,780 --> 00:33:34,300
계산하기 위해 이 두 개를 합쳐

519
00:33:34,300 --> 00:33:39,600
실제 이미지를 생성하고 L2 손실을 계산합니다.

520
00:33:39,600 --> 00:33:42,900
어떤 거리 함수든 이 신경망 훈련에

521
00:33:42,900 --> 00:33:45,180
도움이 될 수 있습니다.

522
00:33:45,180 --> 00:33:49,560
더 일반적인 프레임워크나 공식에서는, 한

523
00:33:49,560 --> 00:33:55,280
채널 또는 채널 집합이 주어지면 다른 채널들을

524
00:33:55,280 --> 00:34:00,920
예측하고 X2에 대해서도 동일하게 수행하는 것입니다.

525
00:34:00,920 --> 00:34:04,760
즉, 채널 집합 X1, 채널 집합 X2입니다.

526
00:34:04,760 --> 00:34:07,440
하나가 주어지면 다른 하나를 예측할 수

527
00:34:07,440 --> 00:34:09,360
있고, 이것이 그 신경망들입니다.

528
00:34:09,360 --> 00:34:16,159
이들을 합치면 이미지 값을 얻고 손실 함수는

529
00:34:16,159 --> 00:34:18,800
간단해집니다.

530
00:34:18,800 --> 00:34:21,120
이런 프레임워크가 있으면

531
00:34:21,120 --> 00:34:25,470
색상과 조명뿐 아니라 모든 것에 적용할 수 있습니다.

532
00:34:25,470 --> 00:34:32,040
RGB 채널과 깊이 채널을 가진 RGBD 센서들, 예를 들어

533
00:34:32,040 --> 00:34:35,980
로보틱스에서 사용하는 Kinect

534
00:34:35,980 --> 00:34:38,880
같은 센서의 데이터를 사용할

535
00:34:38,880 --> 00:34:40,719
수 있습니다.

536
00:34:40,719 --> 00:34:46,400
RGB 채널이 주어지면 깊이를 예측하고, 반대로도

537
00:34:46,400 --> 00:34:47,320
가능합니다.

538
00:34:47,320 --> 00:34:51,000
이것은 다양한 응용에 사용된

539
00:34:51,000 --> 00:34:54,800
매우 성공적인 후속 작업이었습니다.

540
00:34:54,800 --> 00:34:59,770
보시다시피 이 모델과 제가 방금 언급한

541
00:34:59,770 --> 00:35:06,330
split brain 논문, 그리고 이미지를

542
00:35:06,330 --> 00:35:12,150
색칠하는 모델 논문은 그 특징들 자체입니다.

543
00:35:12,150 --> 00:35:14,330
이들은 실제로 클래스

544
00:35:14,330 --> 00:35:19,110
레이블을 예측하는 데 매우 좋은 정확도를 가지고 있습니다.

545
00:35:19,110 --> 00:35:22,650
비교 대상으로 사용되는

546
00:35:22,650 --> 00:35:29,250
다른 다양한 프레임워크들도 볼 수 있습니다.

547
00:35:29,250 --> 00:35:32,290
다시 말하지만, 이것은 감독

548
00:35:32,290 --> 00:35:36,170
학습만큼 좋지는 않은데, 왜냐하면 라벨이

549
00:35:36,170 --> 00:35:40,450
없고 F1과 F2에서 추출한 특징을 연결한

550
00:35:40,450 --> 00:35:43,690
학습된 특징에 기반하기 때문입니다.

551
00:35:43,690 --> 00:35:49,090
좋습니다, 이미지 색칠 사전 학습 과제는

552
00:35:49,090 --> 00:35:55,100
매우 흥미로웠습니다. 왜냐하면 이제 신경망 사전

553
00:35:55,100 --> 00:35:57,520
학습에만 쓰이는 것이

554
00:35:57,520 --> 00:35:59,600
아니라, 색상이

555
00:35:59,600 --> 00:36:03,460
없는 이미지도 색칠할 수 있게

556
00:36:03,460 --> 00:36:05,560
되었기 때문입니다.

557
00:36:05,560 --> 00:36:09,900
즉, 색상이 없는 이미지와

558
00:36:09,900 --> 00:36:15,100
비디오를 색칠할 수 있습니다.

559
00:36:15,100 --> 00:36:18,420
뿐만 아니라, 논문에서 보여준

560
00:36:18,420 --> 00:36:21,220
또 다른 흥미로운 결과는

561
00:36:21,220 --> 00:36:27,900
요세미티의 Half Dome 이미지를 색칠한 것입니다.

562
00:36:27,900 --> 00:36:32,540
이 이미지에서 흥미로운 점은 실제 객체인

563
00:36:32,540 --> 00:36:36,720
Half Dome, 나무, 다리와

564
00:36:36,720 --> 00:36:40,100
그 물에 비친 반영 사이의

565
00:36:40,100 --> 00:36:41,420
일관성입니다.

566
00:36:41,420 --> 00:36:44,700
모델은 이 반영도 어떻게든 색상을

567
00:36:44,700 --> 00:36:47,380
보존해야 한다는 것을

568
00:36:47,380 --> 00:36:51,430
방대한 데이터로 학습했기 때문에 이해할

569
00:36:51,430 --> 00:36:53,430
수 있었습니다.

570
00:36:53,430 --> 00:36:57,510
다시 말하지만, 이 모델들은 모두 대형 언어

571
00:36:57,510 --> 00:37:01,690
모델이나 대형 비전 모델 이전에 훈련된 것입니다.

572
00:37:01,690 --> 00:37:04,390
그리고 특정 과제에 맞춰 훈련되었습니다.

573
00:37:04,390 --> 00:37:09,830
그래서 모든 문제를 해결하도록 훈련된 것은 아닙니다.

574
00:37:09,830 --> 00:37:14,250
이것은 실제로 비디오 설정으로 확장할 수 있습니다.

575
00:37:14,250 --> 00:37:17,590
비디오가 있으면 색상이 있는 참조 프레임을 가지고

576
00:37:17,590 --> 00:37:21,270
후속 프레임을 색칠할 수 있습니다. 이 방법이 어떻게

577
00:37:21,270 --> 00:37:23,330
이루어지는지 설명드리겠습니다.

578
00:37:23,330 --> 00:37:26,750
이것은 매우 간단합니다. 또한

579
00:37:26,750 --> 00:37:32,350
매우 유용한데, 비디오의 미래 프레임을 색칠할 때

580
00:37:32,350 --> 00:37:34,510
우리는 기본적으로

581
00:37:34,510 --> 00:37:41,870
비디오 내 픽셀과 객체를 추적하려고 하며, 모델은 이러한 추적이

582
00:37:41,870 --> 00:37:44,390
어떻게 형성되어야

583
00:37:44,390 --> 00:37:47,570
하는지 암묵적으로 학습합니다.

584
00:37:47,570 --> 00:37:51,040
가설은 비디오 프레임을 색칠하는 학습이 라벨

585
00:37:51,040 --> 00:37:54,200
없이 영역이나 객체를 추적하는 법을

586
00:37:54,200 --> 00:37:56,610
모델이 배우도록 한다는 것입니다.

587
00:37:59,720 --> 00:38:02,600
그리고 비디오를 색칠하는 학습은 많은

588
00:38:02,600 --> 00:38:06,740
대응 관계가 있기 때문에 그 자체로 흥미로운 과제입니다.

589
00:38:06,740 --> 00:38:10,660
자세한 내용은 살펴보시길 권합니다.

590
00:38:10,660 --> 00:38:13,760
간단히 말씀드리겠습니다.

591
00:38:13,760 --> 00:38:16,960
참조 프레임이 있다면, 입력

592
00:38:16,960 --> 00:38:20,180
프레임에 색을 입히기 위해서는

593
00:38:20,180 --> 00:38:23,360
특정 객체나 픽셀이 어디에

594
00:38:23,360 --> 00:38:26,520
있는지 포인터를 찾아야 합니다.

595
00:38:26,520 --> 00:38:28,680
그다음 그 위치의

596
00:38:28,680 --> 00:38:31,760
색상을 확인하고 그 색상을

597
00:38:31,760 --> 00:38:36,280
출력, 즉 목표 색상으로 복사하는 겁니다.

598
00:38:36,280 --> 00:38:39,600
이 과정은 우리가 전에 이야기한

599
00:38:39,600 --> 00:38:43,680
attention과 매우 유사합니다.

600
00:38:43,680 --> 00:38:48,130
즉, 각 입력 프레임, 죄송합니다, 참조 프레임과 목표

601
00:38:48,130 --> 00:38:51,730
프레임의 각 픽셀에 대해 attention을

602
00:38:51,730 --> 00:38:53,610
형성하는 것입니다.

603
00:38:53,610 --> 00:38:58,450
우리는 종종 CNN을 사용해 그 픽셀 주변의 어떤 특징들이 사용되어야

604
00:38:58,450 --> 00:38:59,550
하는지 확인합니다.

605
00:38:59,550 --> 00:39:02,970
그 특징들을 이용해 이제 목표

606
00:39:02,970 --> 00:39:05,330
픽셀 각각에 대해

607
00:39:05,330 --> 00:39:09,370
참조 프레임의 모든 픽셀과의 attention

608
00:39:09,370 --> 00:39:13,970
또는 거리를 계산할 수 있습니다.

609
00:39:13,970 --> 00:39:18,530
그리고 관심 있는 목표

610
00:39:18,530 --> 00:39:25,930
프레임의 픽셀과 참조 프레임의 모든 픽셀 간의

611
00:39:25,930 --> 00:39:31,510
attention을 정의한 후에,

612
00:39:31,510 --> 00:39:37,050
이 attention 모듈들을 기반으로 모든 픽셀의 색상을

613
00:39:37,050 --> 00:39:38,830
평균 낼 수 있습니다.

614
00:39:38,830 --> 00:39:42,170
attention은 기본적으로 두 대상 간의 유사성입니다.

615
00:39:42,170 --> 00:39:45,130
어쨌든 그렇게 해서,

616
00:39:45,130 --> 00:39:48,180
우리는 attention을

617
00:39:48,180 --> 00:39:55,300
이용해 출력 색상을 평균값으로 얻고, 결국에는 데이터에

618
00:39:55,300 --> 00:40:00,360
있는 픽셀의 올바른 색상 값을 이용해

619
00:40:00,360 --> 00:40:04,580
손실 함수를 계산할 수 있습니다.

620
00:40:04,580 --> 00:40:08,220
이 방법으로 참조 프레임을 이용해 이미지를

621
00:40:08,220 --> 00:40:10,280
색칠할 수 있었습니다.

622
00:40:10,280 --> 00:40:15,480
색칠이 얼마나 일관되게 되는지 보실 수 있습니다.

623
00:40:15,480 --> 00:40:18,620
만약 시간에 따른 일관성 없이

624
00:40:18,620 --> 00:40:25,100
각각 따로 색칠하면, 예를 들어 사람의 셔츠나 옷 색깔이

625
00:40:25,100 --> 00:40:32,180
자주 바뀌는 걸 볼 수 있는데, 이는 일관성을 유지하는 제약이

626
00:40:32,180 --> 00:40:34,220
없기 때문입니다.

627
00:40:34,220 --> 00:40:38,780
또한 매우 흥미로운 응용도 있습니다. 참조

628
00:40:38,780 --> 00:40:42,500
프레임에 대한 attention을

629
00:40:42,500 --> 00:40:44,750
계산하기 때문에, 실제로

630
00:40:44,750 --> 00:40:48,670
영상에서 객체를 추적하거나 세그먼트를

631
00:40:48,670 --> 00:40:54,710
추적하고, 심지어 영상 내 주요 지점을 식별할 수도 있습니다.

632
00:40:54,710 --> 00:40:56,010
좋은 질문입니다.

633
00:40:56,010 --> 00:41:03,030
질문하신 내용은 이 슬라이드에 관한 것이고,

634
00:41:03,030 --> 00:41:09,270
인코더가 처음에 데이터를 어떻게 알고 좋은 표현을

635
00:41:09,270 --> 00:41:13,450
학습하는지에 관한 것입니다.

636
00:41:13,450 --> 00:41:18,870
제가 제시하고 정의한 모든 작업들은

637
00:41:18,870 --> 00:41:26,030
디코딩, 분류, 또는 회귀를 통해 출력을

638
00:41:26,030 --> 00:41:30,950
생성하여 이 인코더를 학습시키려는

639
00:41:30,950 --> 00:41:32,730
시도입니다.

640
00:41:32,730 --> 00:41:34,830
원본 이미지가 인터넷이나

641
00:41:34,830 --> 00:41:37,990
ImageNet 등에서 가져온 자연

642
00:41:37,990 --> 00:41:41,350
이미지라면, 이 사전 학습 과제를 통해

643
00:41:41,350 --> 00:41:45,730
그런 유형의 이미지에서 특징을 추출할 수 있는 인코더를

644
00:41:45,730 --> 00:41:47,330
학습하는 겁니다.

645
00:41:47,330 --> 00:41:50,410
그 후 디코더를 제거하고 끝에 분류기를

646
00:41:50,410 --> 00:41:52,990
추가하면, 이 부분만

647
00:41:52,990 --> 00:41:56,310
학습하면 됩니다. 왜냐하면 이 인코더는 제가

648
00:41:56,310 --> 00:41:58,930
방금 말씀드린 모든 사전

649
00:41:58,930 --> 00:42:02,467
학습 과제로 이미 학습되어 있기 때문입니다.

650
00:42:02,467 --> 00:42:04,050
사전 학습을 위해

651
00:42:04,050 --> 00:42:07,290
인코더에 라벨이 디코더에서 오는지

652
00:42:07,290 --> 00:42:09,950
질문하셨는데, 답은 그렇습니다.

653
00:42:09,950 --> 00:42:12,550
그래서 사전 학습 과제를 정의하는

654
00:42:12,550 --> 00:42:16,030
이유가 라벨, 즉 출력이 필요하기 때문입니다.

655
00:42:16,030 --> 00:42:20,130
그 출력들을 바탕으로 전체 네트워크를

656
00:42:20,130 --> 00:42:23,270
학습시키려는 거죠.

657
00:42:23,270 --> 00:42:28,490
올바른 라벨을 예측하는 과정에서 이

658
00:42:28,490 --> 00:42:31,290
인코더도 함께 학습됩니다.

659
00:42:31,290 --> 00:42:32,070
좋은 질문입니다.

660
00:42:32,070 --> 00:42:34,690
인코더와 디코더가 하나의 큰 신경망인지,

661
00:42:34,690 --> 00:42:37,610
아니면 차이가 있는지 물으셨는데,

662
00:42:37,610 --> 00:42:41,120
논문이나 연구마다

663
00:42:41,120 --> 00:42:44,380
완전히 다릅니다.

664
00:42:44,380 --> 00:42:47,800
어떤 경우에는 인코더와 디코더뿐 아니라, 그래서

665
00:42:47,800 --> 00:42:49,700
제가 분류기라고 부르는 것도,

666
00:42:49,700 --> 00:42:54,940
제가 보여드린 예에서 각도를 예측하는 것은

667
00:42:54,940 --> 00:42:57,060
단순한 신경망입니다.

668
00:42:57,060 --> 00:42:59,680
디코더는 이 FC 레이어들입니다.

669
00:42:59,680 --> 00:43:02,700
그래서 이 전체가 하나의 네트워크일 수 있습니다.

670
00:43:02,700 --> 00:43:04,980
그리고 나중에 다운스트림 작업을 위해 이것을

671
00:43:04,980 --> 00:43:06,580
다른 것으로 교체하는 겁니다.

672
00:43:06,580 --> 00:43:09,580
하지만 어떤 경우에는, 예를 들어

673
00:43:09,580 --> 00:43:14,500
이미지의 오토인코딩을 하고 다른 이미지를 디코딩하는 경우,

674
00:43:14,500 --> 00:43:16,620
중간의 표현 공간을 활용하기

675
00:43:16,620 --> 00:43:19,580
위해 두 개의 신경망을 엔드 투

676
00:43:19,580 --> 00:43:21,500
엔드로 학습시키는

677
00:43:21,500 --> 00:43:22,660
경우가 많습니다.

678
00:43:22,660 --> 00:43:26,200
그리고 다음에 이야기할 마스크드

679
00:43:26,200 --> 00:43:31,260
오토인코더에서는 인코더와 디코더 사이에 대칭성이 전혀 없을 수도

680
00:43:31,260 --> 00:43:32,320
있습니다.

681
00:43:32,320 --> 00:43:35,660
이들은 단지 두 개의 다른 프레임워크,

682
00:43:35,660 --> 00:43:39,590
두 개의 다른 신경망일 수 있고,

683
00:43:39,590 --> 00:43:43,330
대칭성 없이도 작업을 학습할 수 있습니다.

684
00:43:43,330 --> 00:43:45,490
그래서 이것은 매우 작업에 의존적이고,

685
00:43:45,490 --> 00:43:47,150
프리텍스트 작업에 의존적입니다.

686
00:43:47,150 --> 00:43:51,470
하지만 우리가 아는 같은 아키텍처, 예를 들어

687
00:43:51,470 --> 00:43:55,230
CNN이나 ResNet에 속할 수도 있고,

688
00:43:55,230 --> 00:43:58,310
대칭성 없이도 두 개의 다른

689
00:43:58,310 --> 00:44:00,390
아키텍처일 수도 있습니다.

690
00:44:00,390 --> 00:44:05,990
이것들이 자기지도 학습을 위한 아주 초기 방법들이라는

691
00:44:05,990 --> 00:44:08,270
점을 기억하세요.

692
00:44:08,270 --> 00:44:11,070
그래서 이 방법들이 모든 문제를 해결할 거라고 기대해서는 안 됩니다.

693
00:44:11,070 --> 00:44:15,070
간단한 주의사항이지만, 여기서 가설은

694
00:44:15,070 --> 00:44:17,230
모델이 90도

695
00:44:17,230 --> 00:44:20,250
회전된 것을 맞출 수 있다면,

696
00:44:20,250 --> 00:44:22,790
암묵적으로 올바른 회전,

697
00:44:22,790 --> 00:44:28,970
즉 올바른 방향과 위치를 이해하고 있다는 뜻입니다.

698
00:44:28,970 --> 00:44:33,870
그리고 만약 올바른, 회전되지 않은

699
00:44:33,870 --> 00:44:39,360
이미지가 주어진다면, 그것이 무엇인지

700
00:44:39,360 --> 00:44:42,260
인식할 수 있습니다.

701
00:44:42,260 --> 00:44:46,440
하지만 이것 자체로는 제한된 작업입니다, 저도 동의합니다.

702
00:44:46,440 --> 00:44:49,240
여기서 왜 64를 사용하는지에 대한 질문이 있죠.

703
00:44:52,680 --> 00:44:54,480
좋은 질문입니다.

704
00:44:54,480 --> 00:44:59,000
하지만 그것도 거의 임의로 선택한 것입니다.

705
00:44:59,000 --> 00:45:01,520
말씀드렸듯이, 여기에는 9

706
00:45:01,520 --> 00:45:05,920
팩토리얼처럼 다양한 순열의 종류가 많습니다.

707
00:45:05,920 --> 00:45:07,520
그래서 매우 큰 숫자입니다.

708
00:45:07,520 --> 00:45:10,620
그 모든 것을 예측하는 것은 의미가 없습니다.

709
00:45:10,620 --> 00:45:12,800
저자들이 여기서 한 일은,

710
00:45:12,800 --> 00:45:16,400
많은 변형들이 단지 한 패치만 바뀐

711
00:45:16,400 --> 00:45:19,080
것과 같기 때문에 충분한

712
00:45:19,080 --> 00:45:24,420
변이가 있는 몇 가지 변형을 선택하기로 결정한 것입니다.

713
00:45:24,420 --> 00:45:30,200
그래서 그들은 서로 간에 가장 큰 차이 거리를 가진

714
00:45:30,200 --> 00:45:31,660
64개를

715
00:45:31,660 --> 00:45:33,840
선택했는데, 분류 문제를

716
00:45:33,840 --> 00:45:36,290
해결하려고 했기 때문에

717
00:45:36,290 --> 00:45:38,960
64개만 선택했습니다.

718
00:45:41,570 --> 00:45:42,410
네.

719
00:45:42,410 --> 00:45:47,730
그래서 저는 이미지나 비디오 등에 어떤

720
00:45:47,730 --> 00:45:54,130
변환을 하는 이런 프레임워크들에 대해 이야기하고

721
00:45:54,130 --> 00:45:55,070
있었습니다.

722
00:45:55,070 --> 00:45:58,630
그리고 이것이 2021년에 발표된 이 새로운

723
00:45:58,630 --> 00:46:00,950
프레임워크로 이어집니다.

724
00:46:00,950 --> 00:46:05,250
그리고 이 이후로 많은 후속 연구가 있었고,

725
00:46:05,250 --> 00:46:11,670
많은 작업에서 사전 학습에 훌륭한 프레임워크가 되었습니다.

726
00:46:11,670 --> 00:46:14,250
요즘 원시 데이터

727
00:46:14,250 --> 00:46:21,850
세트에서 사전 학습을 할 때도 종종 이 MAE 프레임워크를 사용합니다.

728
00:46:21,850 --> 00:46:24,410
이것은 Masked Auto Encoders라고 불립니다.

729
00:46:24,410 --> 00:46:27,610
제가 언급한 페인팅에서의 마스킹

730
00:46:27,610 --> 00:46:30,670
전략과 유사한 재구성 기반

731
00:46:30,670 --> 00:46:34,900
프레임워크이지만, 훨씬 더 세밀합니다.

732
00:46:34,900 --> 00:46:39,100
보시다시피, 이 프레임워크는 단지 하나의

733
00:46:39,100 --> 00:46:41,680
마스크만 선택하는 것이 아닙니다.

734
00:46:41,680 --> 00:46:44,460
더 공격적인 샘플링 비율,

735
00:46:44,460 --> 00:46:48,940
예를 들어 50% 또는 75% 마스킹

736
00:46:48,940 --> 00:46:54,900
비율로 마스킹을 하는 다양한 패치와 위치가 아주 많습니다.

737
00:46:54,900 --> 00:46:58,960
대규모 학습을 통해, 이들은 마스킹된

738
00:46:58,960 --> 00:47:00,900
모든 영역을

739
00:47:00,900 --> 00:47:05,140
복원할 수 있을 뿐만 아니라, 이미지를

740
00:47:05,140 --> 00:47:11,220
잘 요약하는 좋은 특징을 추출하는 인코더도 얻을

741
00:47:11,220 --> 00:47:14,340
수 있음을 보여주었습니다.

742
00:47:14,340 --> 00:47:20,340
이것이 어떻게 이루어졌냐면, 인코더와 디코더를

743
00:47:20,340 --> 00:47:22,358
정의함으로써입니다.

744
00:47:22,358 --> 00:47:23,900
그리고 이것이 제가

745
00:47:23,900 --> 00:47:26,700
말한 비대칭적인 예 중 하나입니다.

746
00:47:26,700 --> 00:47:30,900
인코더와 디코더에서, 입력

747
00:47:30,900 --> 00:47:32,750
마스크의 큰 부분이

748
00:47:32,750 --> 00:47:37,110
입력 패치가 마스킹되고, 마스킹되지 않은

749
00:47:37,110 --> 00:47:43,470
패치들은 인코더에 주어져 특징으로 인코딩되며, 그 특징들은

750
00:47:43,470 --> 00:47:49,430
디코더를 통해 완전한 이미지를 생성하는 데 사용됩니다.

751
00:47:49,430 --> 00:47:53,890
하지만 이것이 무슨 의미인지 좀 더 자세히 살펴보겠습니다.

752
00:47:53,890 --> 00:47:56,510
여기 이 모델들이 어떻게 학습되는지에

753
00:47:56,510 --> 00:47:59,030
대한 세부 사항이 일부 있습니다.

754
00:47:59,030 --> 00:48:04,590
하지만 아주 간단히 이 모델들이 어떻게

755
00:48:04,590 --> 00:48:07,750
작동하는지 설명드렸습니다.

756
00:48:07,750 --> 00:48:13,350
여기 인코더는 ViT와 매우 유사합니다.

757
00:48:13,350 --> 00:48:16,990
모두 우리가 이야기한 ViT처럼

758
00:48:16,990 --> 00:48:18,870
트랜스포머 기반입니다.

759
00:48:18,870 --> 00:48:23,090
ViT와 비슷하게 이미지를 패치로 나눕니다.

760
00:48:23,090 --> 00:48:29,560
그 패치들을 샘플링하는데, 균일 샘플링을

761
00:48:29,560 --> 00:48:32,960
사용했고 실험에서

762
00:48:32,960 --> 00:48:40,240
75% 샘플링이 꽤 효율적임을 보여줬습니다.

763
00:48:40,240 --> 00:48:45,520
높은 마스킹 비율을 사용하고, 이로

764
00:48:45,520 --> 00:48:50,440
인해 예측 과제가 매우 어려워집니다.

765
00:48:50,440 --> 00:48:54,040
자기지도 학습에서 전처리

766
00:48:54,040 --> 00:48:56,880
과제는 의미 있는 과제라는

767
00:48:56,880 --> 00:48:58,700
뜻입니다.

768
00:48:58,700 --> 00:49:00,640
모델이 좋은 특징을

769
00:49:00,640 --> 00:49:05,560
학습해야 복원할 수 있기 때문에 이 과제는 매우 좋습니다.

770
00:49:05,560 --> 00:49:09,960
높은 샘플링 비율 덕분에 데이터를

771
00:49:09,960 --> 00:49:14,040
많이 증강할 수도 있는데, 매번

772
00:49:14,040 --> 00:49:17,280
75%를 마스킹하므로

773
00:49:17,280 --> 00:49:20,840
같은 이미지를 여러 번

774
00:49:20,840 --> 00:49:23,220
재사용할 수 있습니다.

775
00:49:23,220 --> 00:49:29,620
그래서 이 인코더를 학습할 데이터가 매우 많아집니다.

776
00:49:29,620 --> 00:49:36,460
그래서 큰 ViT를 인코더로

777
00:49:36,460 --> 00:49:38,040
사용합니다.

778
00:49:38,040 --> 00:49:43,320
이 인코더는 전체 샘플 중 25%만 봅니다.

779
00:49:43,320 --> 00:49:49,620
패치들은 먼저 선형 투영을 통해 임베딩 공간으로

780
00:49:49,620 --> 00:49:51,320
변환됩니다.

781
00:49:51,320 --> 00:49:55,420
그리고 ViT에서 말한 것과

782
00:49:55,420 --> 00:49:59,700
똑같이 위치 임베딩이 추가됩니다.

783
00:49:59,700 --> 00:50:02,140
모두 트랜스포머 블록으로 구성되어 있습니다.

784
00:50:02,140 --> 00:50:09,600
인코더가 매우 크다는 점을 방금 말씀드렸습니다.

785
00:50:09,600 --> 00:50:13,500
디코딩 부분으로 넘어가면,

786
00:50:13,500 --> 00:50:17,060
마스킹되거나

787
00:50:17,060 --> 00:50:21,820
누락된 패치들의 임베딩이

788
00:50:21,820 --> 00:50:23,260
있습니다.

789
00:50:23,260 --> 00:50:26,530
그 패치들에는 학습 가능한 파라미터가 있는데,

790
00:50:26,530 --> 00:50:29,230
우리가 봤던 클래스 토큰과 매우 비슷합니다.

791
00:50:29,230 --> 00:50:34,270
공유된 마스크 토큰으로, 일종의 평균 패치, 평균

792
00:50:34,270 --> 00:50:36,990
표현으로 생각할 수 있습니다.

793
00:50:36,990 --> 00:50:39,390
이것이

794
00:50:39,390 --> 00:50:43,750
마스킹된 패치들에 할당되고,

795
00:50:43,750 --> 00:50:48,950
디코더는 이를 전체

796
00:50:48,950 --> 00:50:54,270
이미지 패치로 변환해야

797
00:50:54,270 --> 00:51:02,190
합니다. 전체 이미지가 출력 목표입니다.

798
00:51:02,190 --> 00:51:06,430
학습 방법은 간단한 MSE 기반

799
00:51:06,430 --> 00:51:16,150
평균 제곱 오차 손실 함수로, 이미지와 복원된 이미지 간의 손실을
계산합니다.

800
00:51:16,150 --> 00:51:18,790
손실은 마스킹된

801
00:51:18,790 --> 00:51:21,830
패치에 대해서만 계산됩니다.

802
00:51:21,830 --> 00:51:25,960
앞서 설명한 것과 유사합니다.

803
00:51:25,960 --> 00:51:30,960
학습 시 논문에서는 선형

804
00:51:30,960 --> 00:51:33,520
프로빙이나

805
00:51:33,520 --> 00:51:42,192
전체 파인튜닝을 통해 다운스트림 작업에 사용할

806
00:51:42,192 --> 00:51:44,900
수 있다고

807
00:51:44,900 --> 00:51:47,560
보여줬습니다.

808
00:51:47,560 --> 00:51:50,680
선형 프로빙은 보통

809
00:51:50,680 --> 00:51:55,000
인코더를 고정시키고 학습된 표현만 사용해

810
00:51:55,000 --> 00:51:58,800
최종 작업을 위한 선형 함수만

811
00:51:58,800 --> 00:52:00,740
학습합니다.

812
00:52:00,740 --> 00:52:04,240
이 표시가 학습 중임을 의미합니다.

813
00:52:04,240 --> 00:52:06,460
반면 파인튜닝은

814
00:52:06,460 --> 00:52:08,320
사전학습된

815
00:52:08,320 --> 00:52:14,560
인코더를 전체 또는 일부 트랜스포머 블록만 미세 조정합니다.

816
00:52:14,560 --> 00:52:19,420
이것이 파인튜닝 프레임워크입니다.

817
00:52:19,420 --> 00:52:22,570
선형 프로빙은 표현

818
00:52:22,570 --> 00:52:25,890
품질을 측정하고,

819
00:52:25,890 --> 00:52:31,890
파인튜닝은 모델의 잠재력을 최대한

820
00:52:31,890 --> 00:52:37,250
활용해 새로운 작업에 적응시킵니다.

821
00:52:37,250 --> 00:52:43,250
이 주제에 관심 있고 이 논문을 사용하려면

822
00:52:43,250 --> 00:52:45,310
후속 연구들도

823
00:52:45,310 --> 00:52:49,030
꼭 살펴보시길 권합니다.

824
00:52:49,030 --> 00:52:53,210
모델 선택, 하이퍼파라미터, 마스킹

825
00:52:53,210 --> 00:52:56,490
비율 등 다양한 논의가 많습니다.

826
00:52:56,490 --> 00:53:00,450
마스킹 비율에서는 75%가 매우

827
00:53:00,450 --> 00:53:04,150
높은 정확도를 준다고 밝혔습니다.

828
00:53:04,150 --> 00:53:07,810
그래서 75%가 선택된 이유입니다.

829
00:53:07,810 --> 00:53:13,530
디코더 깊이, 너비, 마스크 토큰, 복원 목표, 데이터

830
00:53:13,530 --> 00:53:18,530
증강이 어떻게 도움이 되는지, 마스크 샘플링 방법에

831
00:53:18,530 --> 00:53:22,160
대한 결과도 다시 보여드립니다.

832
00:53:22,160 --> 00:53:25,180
마스크 샘플링 방법은 무작위

833
00:53:25,180 --> 00:53:30,500
마스킹 블록을 쓸지, 격자형 마스킹을 쓸지에 관한

834
00:53:30,500 --> 00:53:31,520
것입니다.

835
00:53:31,520 --> 00:53:35,660
여기 예시를 보시면, 무작위 마스킹이

836
00:53:35,660 --> 00:53:39,620
최선의 선택이라는 결론에 도달했습니다.

837
00:53:39,620 --> 00:53:43,500
마지막으로, MAE가 다른

838
00:53:43,500 --> 00:53:49,340
많은 방법들에 비해 훨씬 더 좋은 성과를

839
00:53:49,340 --> 00:53:53,960
내고 있다는 것을 보여주었습니다.

840
00:53:53,960 --> 00:53:56,460
그래서 다른 최첨단 방법들 중에는 실제로

841
00:53:56,460 --> 00:53:58,760
Dino와 Moco V3가 있었습니다.

842
00:53:58,760 --> 00:54:00,980
시간이 되면 간단히 설명드리겠습니다.

843
00:54:00,980 --> 00:54:06,540
하지만 이 프레임워크가 당시 더

844
00:54:06,540 --> 00:54:13,660
발전된 대조 학습 프레임워크들을 능가하고

845
00:54:13,660 --> 00:54:16,220
있었습니다.

846
00:54:16,220 --> 00:54:22,850
질문이 있으면 잠시 멈추겠습니다.

847
00:54:22,850 --> 00:54:28,050
지금까지 이야기한 내용을 요약해 보겠습니다.

848
00:54:28,050 --> 00:54:30,970
Pretext tasks가 정말 중요했습니다.

849
00:54:30,970 --> 00:54:33,910
그리고 말씀드렸듯이, 이들의 초점은 시각적 상식(visual

850
00:54:33,910 --> 00:54:36,230
common sense)을 이해하는 데 있습니다.

851
00:54:36,230 --> 00:54:40,630
또 질문과도 관련된 부분인데, 개별

852
00:54:40,630 --> 00:54:44,150
pretext task를 만드는

853
00:54:44,150 --> 00:54:48,790
것이 종종 어려운 이유는, 정의한

854
00:54:48,790 --> 00:54:52,270
작업 유형 때문에 학습된

855
00:54:52,270 --> 00:54:58,190
표현이 충분히 일반적이지 않을 수 있기 때문입니다.

856
00:54:58,190 --> 00:55:05,170
예를 들어, completion, rotation prediction,
jigsaw

857
00:55:05,170 --> 00:55:08,070
puzzle, colorization

858
00:55:08,070 --> 00:55:11,670
같은 작업을 사용하면, 학습된 표현은 그

859
00:55:11,670 --> 00:55:14,030
특정 작업을 푸는 데는 좋지만,

860
00:55:14,030 --> 00:55:19,680
일반적인 pretext task에는 적합하지 않을 수 있습니다.

861
00:55:19,680 --> 00:55:22,420
그래서 split brain

862
00:55:22,420 --> 00:55:28,880
autoencoder에서 모델이 예를 들어 L 채널, 즉 조명 채널, 밝기

863
00:55:28,880 --> 00:55:33,340
채널을 주었을 때 다른 채널을 어떻게 예측하는지

864
00:55:33,340 --> 00:55:34,720
궁금하실 텐데요.

865
00:55:34,720 --> 00:55:39,040
질문에 질문으로 답해 보겠습니다.

866
00:55:39,040 --> 00:55:43,000
이미지 내 객체의 클래스를 예측하도록

867
00:55:43,000 --> 00:55:45,400
모델을 훈련할 때,

868
00:55:45,400 --> 00:55:51,880
인코더가 어떤 특징을 추출해야 하는지 어떻게 알까요?

869
00:55:51,880 --> 00:55:55,960
라벨이 있기 때문입니다. 왜냐하면

870
00:55:55,960 --> 00:55:59,280
라벨을 기반으로 손실

871
00:55:59,280 --> 00:56:02,280
값을 역전파하기 때문이죠.

872
00:56:02,280 --> 00:56:03,660
여기서도 같은 원리입니다.

873
00:56:03,660 --> 00:56:07,760
네트워크가 한 채널을 받아서 다른 채널을

874
00:56:07,760 --> 00:56:09,480
출력하도록 정의합니다.

875
00:56:09,480 --> 00:56:13,040
훈련은 출력이 무엇이어야 하는지를 역전파하는

876
00:56:13,040 --> 00:56:15,060
방식으로 이루어집니다.

877
00:56:15,060 --> 00:56:17,400
출력은 다른 채널입니다.

878
00:56:17,400 --> 00:56:20,020
데이터에 다른 채널이 이미 있기 때문이죠.

879
00:56:20,020 --> 00:56:23,700
그래서 객체 클래스 예측 분류

880
00:56:23,700 --> 00:56:26,460
작업을 정의하는 대신,

881
00:56:26,460 --> 00:56:28,900
여기서는 픽셀 색상을

882
00:56:28,900 --> 00:56:32,100
예측하는 작업으로 정의합니다.

883
00:56:32,100 --> 00:56:33,740
픽셀 색상은

884
00:56:33,740 --> 00:56:36,980
데이터셋에 이미 있으므로 손실 함수는

885
00:56:36,980 --> 00:56:40,220
계산되고 역전파될 수 있습니다.

886
00:56:40,220 --> 00:56:42,540
그럼 이 출력들이 디코더의 입력으로

887
00:56:42,540 --> 00:56:44,540
어떻게 사용되는지 궁금할 텐데요.

888
00:56:44,540 --> 00:56:51,020
다시 말해, 이건 VIT 트랜스포머 스타일의

889
00:56:51,020 --> 00:56:58,660
프레임워크 인코더로, 모든 입력을 토큰으로 바꾸고, 그 출력은

890
00:56:58,660 --> 00:57:02,700
특정 입력 패치의 표현입니다.

891
00:57:02,700 --> 00:57:04,700
이 부분은 이미 설명드렸습니다.

892
00:57:04,700 --> 00:57:10,040
하지만 모든 패치가 있는 게 아니라 일부는 마스킹되어

893
00:57:10,040 --> 00:57:12,740
있다는 점도 알고 있습니다.

894
00:57:12,740 --> 00:57:15,830
마스킹된 패치에 대해서는

895
00:57:15,830 --> 00:57:20,350
인코더가 공유 마스크 토큰(shared

896
00:57:20,350 --> 00:57:25,490
mask token)을 출력하도록 훈련합니다.

897
00:57:25,490 --> 00:57:27,110
이 토큰은 기본적으로 평균 토큰 같은 것으로, 학습 가능한 파라미터입니다.

898
00:57:27,110 --> 00:57:29,290
명확히 해석할 수는

899
00:57:29,290 --> 00:57:31,510
없지만, 아마 마스크와

900
00:57:31,510 --> 00:57:35,670
비슷한 평균 토큰이라고 볼 수 있습니다.

901
00:57:35,670 --> 00:57:40,990
이 공유 마스크 토큰이 누락된 부분에

902
00:57:40,990 --> 00:57:42,130
들어갑니다.

903
00:57:42,130 --> 00:57:46,230
그렇게 긴 벡터, 긴 시퀀스가 만들어집니다.

904
00:57:46,230 --> 00:57:49,550
디코더, 또 다른 트랜스포머

905
00:57:49,550 --> 00:57:55,990
프레임워크가 이 긴 토큰 시퀀스를 받아서 출력 픽셀 값으로

906
00:57:55,990 --> 00:57:58,540
투영된 결과를 출력합니다.

907
00:58:01,790 --> 00:58:12,600
좋습니다, 시간이 15분밖에 없고 다룰 내용이

908
00:58:12,600 --> 00:58:14,880
많네요.

909
00:58:14,880 --> 00:58:21,300
하지만 이번 세션에서 제가 여러분께 전달하고 싶었던 것은, pretext
task가

910
00:58:21,300 --> 00:58:24,800
무엇인지, 그리고 그것을 어떻게 정의하는지 이해하는

911
00:58:24,800 --> 00:58:26,020
것이었습니다.

912
00:58:26,020 --> 00:58:29,400
현재 가장 많이 사용되는 프레임워크 중

913
00:58:29,400 --> 00:58:32,720
하나는 masked autoencoder인데,

914
00:58:32,720 --> 00:58:36,120
저희가 어느 정도 다뤘던 내용입니다.

915
00:58:36,120 --> 00:58:42,200
어쨌든 여기서 우리는 이러한 변환들을 살펴봤고,

916
00:58:42,200 --> 00:58:45,040
이 모든 변환들이

917
00:58:45,040 --> 00:58:49,280
원본 이미지와 같은 객체이지만

918
00:58:49,280 --> 00:58:52,940
다른 형태일 뿐이라는 것을

919
00:58:52,940 --> 00:58:54,540
알고 있습니다.

920
00:58:54,540 --> 00:58:59,360
하지만 데이터셋에는 완전히 다른 모습의 다른

921
00:58:59,360 --> 00:59:02,660
객체들도 있다는 사실도 알고 있습니다.

922
00:59:02,660 --> 00:59:08,720
그래서 만약 제가 같은 객체, 같은

923
00:59:08,720 --> 00:59:11,990
픽셀에 속하는

924
00:59:11,990 --> 00:59:15,510
것들은 표현 공간에서

925
00:59:15,510 --> 00:59:18,210
가깝게, 즉 서로

926
00:59:18,210 --> 00:59:21,890
끌어당기고, 그렇지

927
00:59:21,890 --> 00:59:27,930
않은 것들은 잠재 공간에서 거리를

928
00:59:27,930 --> 00:59:30,610
최대화해서 서로

929
00:59:30,610 --> 00:59:34,650
밀어내도록 하는 작업을

930
00:59:34,650 --> 00:59:37,130
정의한다면,

931
00:59:37,130 --> 00:59:40,530
이것이 흔히 contrastive

932
00:59:40,530 --> 00:59:44,330
learning, 즉 대조 학습 또는 대조

933
00:59:44,330 --> 00:59:46,290
표현 학습이라고

934
00:59:46,290 --> 00:59:52,530
불리는 또 다른 작업입니다. 매우 흥미로운 여러 방법들이 있습니다.

935
00:59:52,530 --> 00:59:56,830
2019년에서 2020년대

936
00:59:56,830 --> 01:00:04,530
초반을 중심으로 이 분야에 정말 많은 논문들이

937
01:00:04,530 --> 01:00:07,010
나왔습니다.

938
01:00:07,010 --> 01:00:10,840
Sinclair McCaw, CBC, 그리고 결국에는 dyno가

939
01:00:10,840 --> 01:00:13,900
있는데, dyno는 대조 학습 개념을 차용했지만

940
01:00:13,900 --> 01:00:16,440
반드시 대조 학습 프레임워크는 아닙니다.

941
01:00:19,060 --> 01:00:26,940
여기서 우리는 끌어당기고 밀어내는 함수, 즉 특성을 정의하거나

942
01:00:26,940 --> 01:00:33,260
모델을 정규화하기 위해, 기준 이미지를 x로

943
01:00:33,260 --> 01:00:38,740
정의하고, 같은 객체의 변환들은 positive

944
01:00:38,740 --> 01:00:40,500
샘플로,

945
01:00:40,500 --> 01:00:44,140
데이터셋이나 배치 내 다른

946
01:00:44,140 --> 01:00:46,540
객체들은 negative

947
01:00:46,540 --> 01:00:49,940
샘플로 정의합니다.

948
01:00:49,940 --> 01:00:52,300
이 positive와

949
01:00:52,300 --> 01:00:55,260
negative 샘플들이 손실

950
01:00:55,260 --> 01:00:58,500
함수를 계산하는 방식을 결정합니다.

951
01:00:58,500 --> 01:01:00,740
어떻게 할 수 있을까요?

952
01:01:00,740 --> 01:01:02,720
점수 함수가 있다고 가정해봅시다.

953
01:01:02,720 --> 01:01:04,420
기준 이미지의

954
01:01:04,420 --> 01:01:09,290
인코딩된 특징과 positive

955
01:01:09,290 --> 01:01:11,550
샘플의 특징 간 점수가,

956
01:01:11,550 --> 01:01:15,810
기준 이미지와 negative

957
01:01:15,810 --> 01:01:24,150
샘플 간 점수보다 커야 한다는 점수 함수를 얻고 싶습니다.

958
01:01:24,150 --> 01:01:30,910
이런 유형의 점수 함수를 정의하기 위해,

959
01:01:30,910 --> 01:01:35,110
우리는 그에 기반한 손실

960
01:01:35,110 --> 01:01:38,370
함수를 정의합니다.

961
01:01:38,370 --> 01:01:43,750
점수 함수 S를 학습한 후, 이 S는

962
01:01:43,750 --> 01:01:46,150
이전 슬라이드의

963
01:01:46,150 --> 01:01:48,510
점수와 동일합니다.

964
01:01:48,510 --> 01:01:50,590
이 점수 함수가

965
01:01:50,590 --> 01:01:54,350
있다면, 끌어당기고 밀어내기

966
01:01:54,350 --> 01:01:58,830
위해, exp를 사용해 점수를

967
01:01:58,830 --> 01:02:05,400
확률 값으로 변환하는 소프트맥스 구조를 사용할

968
01:02:05,400 --> 01:02:07,260
수 있습니다.

969
01:02:07,260 --> 01:02:09,920
분모에는 모든

970
01:02:09,920 --> 01:02:15,040
다른 negative 샘플들이

971
01:02:15,040 --> 01:02:17,480
포함되어 있습니다.

972
01:02:17,480 --> 01:02:18,920
실제로 구현할 때는

973
01:02:18,920 --> 01:02:21,780
배치 학습 프레임워크를 사용합니다.

974
01:02:21,780 --> 01:02:23,680
배치 내 다른

975
01:02:23,680 --> 01:02:27,800
객체에 속하는 모든 negative

976
01:02:27,800 --> 01:02:31,120
샘플들과 한 변환이 positive

977
01:02:31,120 --> 01:02:33,540
샘플로 사용됩니다.

978
01:02:33,540 --> 01:02:36,600
그래서 손실 함수를 이렇게 정의합니다.

979
01:02:36,600 --> 01:02:39,440
positive 쌍의 점수와

980
01:02:39,440 --> 01:02:43,880
모든 negative 쌍의 점수를 사용합니다.

981
01:02:43,880 --> 01:02:48,080
이 함수는 우리가 전에 논의했던 것과

982
01:02:48,080 --> 01:02:50,160
매우 유사합니다.

983
01:02:50,160 --> 01:02:51,840
어떤 생각이 드시나요?

984
01:02:51,840 --> 01:02:54,520
이것은 다중 클래스용 크로스 엔트로피입니다.

985
01:02:54,520 --> 01:02:57,960
만약 n개의 샘플이

986
01:02:57,960 --> 01:03:04,990
있다면, 네, 여기서는 n개의 샘플이 있습니다.

987
01:03:04,990 --> 01:03:09,190
소프트맥스는 여러 클래스가 있을 때, 예를 들어

988
01:03:09,190 --> 01:03:11,430
10개의 클래스가 출력일 때,

989
01:03:11,430 --> 01:03:15,210
그 중 하나의 점수를 최대화하고 나머지는

990
01:03:15,210 --> 01:03:17,070
최소화하려고 합니다.

991
01:03:17,070 --> 01:03:19,070
여기서도 같은 이야기입니다.

992
01:03:19,070 --> 01:03:23,170
우리는 이 점수를 최대화하고, negative와

993
01:03:23,170 --> 01:03:26,050
기준 간 점수는 최소화하려고 합니다.

994
01:03:26,050 --> 01:03:29,530
즉, 다중 클래스 분류에서

995
01:03:29,530 --> 01:03:31,910
논의한 개념을

996
01:03:31,910 --> 01:03:34,690
대조 학습 손실

997
01:03:34,690 --> 01:03:37,450
함수의 형태로

998
01:03:37,450 --> 01:03:39,610
적용한 것입니다.

999
01:03:39,610 --> 01:03:44,850
이 함수는 info NCE, 즉 정보 Noise Contrastive

1000
01:03:44,850 --> 01:03:47,930
Estimation 손실이라고

1001
01:03:47,930 --> 01:03:51,170
불리며, 이 논문에서 제안되었습니다.

1002
01:03:51,170 --> 01:03:55,370
논문에는 이 목적 함수, 이 손실

1003
01:03:55,370 --> 01:03:58,890
함수가 의존성을 측정한다는, 아니

1004
01:03:58,890 --> 01:04:04,630
mutual information의 하한이라는

1005
01:04:04,630 --> 01:04:07,790
이론적 논의가 많이 있습니다.

1006
01:04:07,790 --> 01:04:12,070
mutual information이란 두 이미지

1007
01:04:12,070 --> 01:04:14,790
간의 상호 정보를 계산하는

1008
01:04:14,790 --> 01:04:18,270
것으로, 두 이미지 간의 의존성이나

1009
01:04:18,270 --> 01:04:21,090
공유 정보를 측정하는 것입니다.

1010
01:04:21,090 --> 01:04:23,790
그래서 우리가 하고 싶은 것은,

1011
01:04:23,790 --> 01:04:27,430
x와 x plus 사이의 공유 정보를 최대화하되,

1012
01:04:27,430 --> 01:04:32,330
x와 x minuses 사이의 공유 정보는 최소화하는 것입니다.

1013
01:04:32,330 --> 01:04:38,230
그래서 논문에서는, 그리고 이것 자체만으로도 설명하는 데

1014
01:04:38,230 --> 01:04:40,130
30분은 걸릴 겁니다.

1015
01:04:40,130 --> 01:04:42,750
관심이 있다면 꼭 논문을

1016
01:04:42,750 --> 01:04:47,830
보셔야 하는데, 이 손실 함수 info NCE의 음수

1017
01:04:47,830 --> 01:04:52,750
값이 x와 x plus 사이의 상호 정보에 대한

1018
01:04:52,750 --> 01:04:54,070
하한선이라는 겁니다.

1019
01:04:54,070 --> 01:04:57,390
즉, 음수 값이 상호

1020
01:04:57,390 --> 01:05:03,400
정보에 대한 하한선이라는 뜻입니다.

1021
01:05:03,400 --> 01:05:07,240
info NCE를 최소화하면, 사실상 x와

1022
01:05:07,240 --> 01:05:10,280
x plus 사이의 상호 정보를

1023
01:05:10,280 --> 01:05:11,940
최대화하는 것입니다.

1024
01:05:11,940 --> 01:05:14,000
그래서 이게 제가 정말 원하는 겁니다.

1025
01:05:14,000 --> 01:05:17,920
그래서 이걸 손실 함수로

1026
01:05:17,920 --> 01:05:23,480
삼아 그 값을 최소화하기 시작하는 거죠.

1027
01:05:23,480 --> 01:05:25,800
info NCE 논문에는 또

1028
01:05:25,800 --> 01:05:29,920
다른 이론적 측면이 있는데, 음수 샘플 수가 많을수록

1029
01:05:29,920 --> 01:05:33,300
이 하한선이 더 타이트해진다는 겁니다.

1030
01:05:33,300 --> 01:05:36,600
즉, 음수 샘플 수에 따라 하한선이

1031
01:05:36,600 --> 01:05:38,520
더 엄밀해지는 거죠.

1032
01:05:38,520 --> 01:05:43,680
그래서 이런 유형의 손실 함수로 신경망을 훈련할

1033
01:05:43,680 --> 01:05:47,880
때는 아주 큰 배치 크기가 필요합니다.

1034
01:05:47,880 --> 01:05:51,040
음수 샘플 수가

1035
01:05:51,040 --> 01:05:59,770
많을수록 더 좋고 훨씬 빠른 훈련 수렴을 얻을 수 있습니다.

1036
01:05:59,770 --> 01:06:03,570
그리고 이 손실 함수는 여러 다른

1037
01:06:03,570 --> 01:06:06,230
프레임워크에서 사용되었습니다.

1038
01:06:06,230 --> 01:06:09,210
다음 몇 분 동안은 그 프레임워크들이

1039
01:06:09,210 --> 01:06:12,190
무엇인지 간단히 말씀드리겠습니다.

1040
01:06:12,190 --> 01:06:16,570
예를 들어, Sinclair는 대조 학습을

1041
01:06:16,570 --> 01:06:22,210
위한 간단한 프레임워크로, 각 이미지를 두 번 변환하여

1042
01:06:22,210 --> 01:06:24,550
같은 이미지의 두 표현을

1043
01:06:24,550 --> 01:06:27,110
표현 공간으로 옮깁니다.

1044
01:06:27,110 --> 01:06:29,650
그리고 그 후에

1045
01:06:29,650 --> 01:06:36,390
임베딩 표현 간의 코사인 유사도를 계산합니다.

1046
01:06:36,390 --> 01:06:40,530
하지만 그 전에 선형 또는 비선형

1047
01:06:40,530 --> 01:06:47,690
투영을 통해 특징 집합 z로 변환하여 이 공간에서

1048
01:06:47,690 --> 01:06:49,970
거리 계산을 합니다.

1049
01:06:49,970 --> 01:06:55,280
이것이 바로 positive 샘플을

1050
01:06:55,280 --> 01:06:57,580
생성하는 방법입니다.

1051
01:06:57,580 --> 01:07:00,920
positive 샘플을 생성할

1052
01:07:00,920 --> 01:07:05,020
때는 다양한 변환이 의미가 있습니다.

1053
01:07:05,020 --> 01:07:09,700
세부 사항은 여기에서 기본적으로 다루고 있습니다.

1054
01:07:09,700 --> 01:07:14,500
데이터 증강 함수를 샘플링하여 positive 쌍을

1055
01:07:14,500 --> 01:07:15,580
생성합니다.

1056
01:07:15,580 --> 01:07:20,060
몇 개를 샘플링한 후 역변환을 계산하고

1057
01:07:20,060 --> 01:07:22,660
쌍에 대한 손실을 봅니다.

1058
01:07:22,660 --> 01:07:29,580
이 과정을 반복하는데, 각 샘플마다 2n 곱하기 n개의

1059
01:07:29,580 --> 01:07:32,940
샘플이 생성되기 때문입니다.

1060
01:07:32,940 --> 01:07:39,980
미니 배치에 있는 이미지 리스트를

1061
01:07:39,980 --> 01:07:43,140
가져와 같은 이미지의

1062
01:07:43,140 --> 01:07:48,620
두 변형을 인코더에 통과시킵니다.

1063
01:07:48,620 --> 01:07:49,820
두 변형을 인코더에 통과시킵니다.

1064
01:07:49,820 --> 01:07:55,310
각 이미지마다 변환된 버전이 하나씩 있게

1065
01:07:55,310 --> 01:07:56,110
됩니다.

1066
01:07:56,110 --> 01:08:02,830
이제 배치에 2n개의 샘플이 있습니다.

1067
01:08:02,830 --> 01:08:05,950
각 샘플에 대해 다음 샘플이

1068
01:08:05,950 --> 01:08:10,550
positive이고 나머지는 모두 negative라는

1069
01:08:10,550 --> 01:08:12,050
의미입니다.

1070
01:08:12,050 --> 01:08:16,010
첫 번째 샘플에 대해 두 번째 이미지는 positive입니다.

1071
01:08:16,010 --> 01:08:17,569
나머지는 모두 negative입니다.

1072
01:08:17,569 --> 01:08:19,710
두 번째 샘플에 대해 첫 번째가 positive이고

1073
01:08:19,710 --> 01:08:21,310
나머지는 모두 negative입니다.

1074
01:08:21,310 --> 01:08:24,990
이 과정이 모든 샘플에 대해 반복됩니다.

1075
01:08:24,990 --> 01:08:31,710
이것이 SimCLR의 높은 수준의 정의입니다.

1076
01:08:31,710 --> 01:08:36,870
과제 3에 SimCLR과 관련된 질문이

1077
01:08:36,870 --> 01:08:42,510
있으니 이 프레임워크를 좀 더 탐구해 보시기

1078
01:08:42,510 --> 01:08:44,069
바랍니다.

1079
01:08:44,069 --> 01:08:47,350
하지만 여기서 제가 제시한 표준

1080
01:08:47,350 --> 01:08:49,790
정의와는 약간 다르다는 점을

1081
01:08:49,790 --> 01:08:51,120
주의해야 합니다.

1082
01:08:51,120 --> 01:08:57,080
그리고 과제 지침을 꼭 따라야 합니다.

1083
01:08:57,080 --> 01:08:59,620
그래서 SimCLR은 실제로 매우 성공적이었습니다.

1084
01:08:59,620 --> 01:09:05,160
레이블을 사용하지 않고, 그리고 특징

1085
01:09:05,160 --> 01:09:07,600
위에 선형

1086
01:09:07,600 --> 01:09:13,279
분류기를 학습시켜서 이전의 모든 연구를

1087
01:09:13,279 --> 01:09:19,240
능가했고, 사실상 완전 감독 학습

1088
01:09:19,240 --> 01:09:22,939
프레임워크와 견줄 만한 결과를

1089
01:09:22,939 --> 01:09:25,140
만들어냈습니다.

1090
01:09:25,140 --> 01:09:28,080
더 일반적인 특징을 학습하기 때문에

1091
01:09:28,080 --> 01:09:30,420
더 큰 신경망이 필요합니다.

1092
01:09:30,420 --> 01:09:32,120
하지만 정확도

1093
01:09:32,120 --> 01:09:38,760
측면에서는 감독 학습과 비슷한 수준이었습니다.

1094
01:09:38,760 --> 01:09:44,040
SimCLR과 그 주변 결과에서

1095
01:09:44,040 --> 01:09:47,479
흥미로운 점은 몇

1096
01:09:47,479 --> 01:09:53,630
가지 선택 사항이 있다는 겁니다.

1097
01:09:53,630 --> 01:09:58,530
사실 주요 선택 사항에 대해 시간을 좀 쓰겠습니다.

1098
01:09:58,530 --> 01:10:02,850
왜 같은 표현을 사용하지 않고 특징을 새로운

1099
01:10:02,850 --> 01:10:06,850
변수로 투영했는지 궁금할 수 있습니다.

1100
01:10:06,850 --> 01:10:10,910
이것은 SimCLR에서 내린 설계

1101
01:10:10,910 --> 01:10:16,050
선택인데, 대조 학습 프레임워크에서는 샘플 간

1102
01:10:16,050 --> 01:10:17,810
대조를 수행하는

1103
01:10:17,810 --> 01:10:20,490
목적 함수 때문에 대조에

1104
01:10:20,490 --> 01:10:24,110
도움이 되지 않는 추가 정보가

1105
01:10:24,110 --> 01:10:26,050
일부 손실된다고

1106
01:10:26,050 --> 01:10:28,070
가정했기 때문입니다.

1107
01:10:28,070 --> 01:10:34,490
그래서 모든 추가 특징을 보존하기 위해 표현은

1108
01:10:34,490 --> 01:10:36,830
h로 정의하지만,

1109
01:10:36,830 --> 01:10:40,010
논문에서는 비선형

1110
01:10:40,010 --> 01:10:42,770
투영을 사용해 z

1111
01:10:42,770 --> 01:10:49,540
값을 얻고 이를 통해 인플루언서를 계산합니다.

1112
01:10:49,540 --> 01:10:52,280
이것이 중요한 설계 선택 중 하나입니다.

1113
01:10:52,280 --> 01:10:56,680
그리고 또 하나는 앞서 언급한 큰 배치 크기입니다.

1114
01:10:56,680 --> 01:10:59,300
더 나은 SimCLR 성능을

1115
01:10:59,300 --> 01:11:04,040
위해서는 매우 큰 배치 크기가 필요합니다.

1116
01:11:04,040 --> 01:11:08,100
왜 그런지, 어떻게 그런지에 대해 이야기했습니다.

1117
01:11:08,100 --> 01:11:11,340
하지만 메모리 제약 등으로

1118
01:11:11,340 --> 01:11:13,580
인해 우리가 다루는

1119
01:11:13,580 --> 01:11:20,060
많은 작업에서 항상 큰 배치 사이즈를 사용할 수는 없습니다.

1120
01:11:20,060 --> 01:11:23,640
그래서 여러 후속 연구들이 나왔는데, 예를 들어

1121
01:11:23,640 --> 01:11:29,920
MoCo는 momentum contrastive learning으로
제안되었습니다.

1122
01:11:29,920 --> 01:11:34,980
배치 내 모든 음성 샘플을

1123
01:11:34,980 --> 01:11:39,260
사용하는 대신, 큐를 만들어

1124
01:11:39,260 --> 01:11:46,440
모델 내에서 배치 간 음성 샘플의 히스토리를

1125
01:11:46,440 --> 01:11:49,260
유지합니다.

1126
01:11:49,260 --> 01:11:53,840
즉, 배치 내 음성 샘플에만

1127
01:11:53,840 --> 01:11:57,360
의존하지 않고 별도의

1128
01:11:57,360 --> 01:12:01,640
큐를 정의해 음성 샘플을

1129
01:12:01,640 --> 01:12:11,560
일정 수 유지하며 시간이 지남에 따라 업데이트하면서 대조 손실을

1130
01:12:11,560 --> 01:12:13,360
학습합니다.

1131
01:12:13,360 --> 01:12:17,240
하지만 큐를 사용하기 때문에 그 샘플들은 더 이상 배치에

1132
01:12:17,240 --> 01:12:19,540
없어서 역전파를 할 수 없습니다.

1133
01:12:19,540 --> 01:12:25,260
그래서 음성 샘플에 대해서는 역전파가 불가능하고, 긍정 샘플(현재는
query라

1134
01:12:25,260 --> 01:12:27,560
부름)과 음성 샘플(현재는 key라

1135
01:12:27,560 --> 01:12:30,800
부름)을 위한 인코더를 분리해야 했습니다.

1136
01:12:30,800 --> 01:12:32,600
이 아키텍처에서는

1137
01:12:32,600 --> 01:12:38,240
긍정 샘플용 인코더와 음성 샘플용 인코더가 분리되어 있습니다.

1138
01:12:38,240 --> 01:12:42,760
학습은 오직 인코더에만 영향을 미치고, 시간이

1139
01:12:42,760 --> 01:12:51,590
지나면서 momentum m을 사용해 Q 인코더가 key 인코더(모멘텀
인코더)를

1140
01:12:51,590 --> 01:12:53,130
업데이트합니다.

1141
01:12:53,130 --> 01:12:57,370
이 프레임워크는 실제 구현과

1142
01:12:57,370 --> 01:13:01,490
후속 버전에서 매우

1143
01:13:01,490 --> 01:13:03,670
성공적이었습니다.

1144
01:13:03,670 --> 01:13:07,930
흥미로운 결과가 많았고,

1145
01:13:07,930 --> 01:13:13,170
SimCLR의 비선형 프로젝션 헤드와

1146
01:13:13,170 --> 01:13:19,530
데이터 증강, 그리고 MoCo의 미니

1147
01:13:19,530 --> 01:13:22,210
배치와 음성 샘플

1148
01:13:22,210 --> 01:13:26,010
분리 방식을 혼합한

1149
01:13:26,010 --> 01:13:29,170
하이브리드 버전을

1150
01:13:29,170 --> 01:13:34,490
시도했는데, MoCo 버전 2에서는

1151
01:13:34,490 --> 01:13:39,950
성능이 크게 향상됨을 보여주었습니다.

1152
01:13:39,950 --> 01:13:45,740
여기서 멈추겠지만, 슬라이드에서 볼 수 있는 또 다른

1153
01:13:45,740 --> 01:13:50,140
예로 CPC(Contrastive

1154
01:13:50,140 --> 01:13:55,020
Predictive Coding) 개념도 있습니다.

1155
01:13:55,020 --> 01:14:00,580
그리고 MoCo의 개선된 버전인 MoCo 버전

1156
01:14:00,580 --> 01:14:04,580
3와 DINO도 널리 사용되는

1157
01:14:04,580 --> 01:14:11,220
프레임워크인데, MoCo와 비슷한 아키텍처를 가지지만

1158
01:14:11,220 --> 01:14:14,020
학생-교사 네트워크를 사용해

1159
01:14:14,020 --> 01:14:17,640
반드시 대조 학습은 아닙니다.

1160
01:14:17,640 --> 01:14:20,280
이 부분은 별도의 논의로

1161
01:14:20,280 --> 01:14:24,220
남기고, 관심 있으시면 앞으로 강의나

1162
01:14:24,220 --> 01:14:27,520
슬라이드에서 다룰 수 있습니다.

1163
01:14:27,520 --> 01:14:32,380
어쨌든 이것도 이미지나 때로는 비디오에서 특징을

1164
01:14:32,380 --> 01:14:35,140
추출하는 데 널리 사용되는 프레임워크

1165
01:14:35,140 --> 01:14:36,990
중 하나입니다.
