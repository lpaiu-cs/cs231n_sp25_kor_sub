2
00:00:05,240 --> 00:00:07,737
Welcome back to
CS231n lecture 11.

3
00:00:07,737 --> 00:00:10,070
Today, we're going to talk
about large-scale distributed

4
00:00:10,070 --> 00:00:10,770
training.

5
00:00:10,770 --> 00:00:12,020
And this is a pretty
exciting topic,

6
00:00:12,020 --> 00:00:14,228
because this is basically
how all neural networks get

7
00:00:14,228 --> 00:00:15,510
trained in practice today.

8
00:00:15,510 --> 00:00:18,420
When you look at large models
from startups, from industries,

9
00:00:18,420 --> 00:00:20,630
even in academia,
really large-scale

10
00:00:20,630 --> 00:00:22,850
is the new norm in
deep learning nowadays.

11
00:00:22,850 --> 00:00:24,350
And that's actually
something that's

12
00:00:24,350 --> 00:00:26,190
changed quite a lot
in the last 10 years

13
00:00:26,190 --> 00:00:27,800
since we started this class.

14
00:00:27,800 --> 00:00:30,619
10 years ago, it
was actually pretty

15
00:00:30,619 --> 00:00:32,119
common to train all
models basically

16
00:00:32,119 --> 00:00:34,550
on one GPU, one
device, and it was

17
00:00:34,550 --> 00:00:36,960
fairly uncommon to train
on multiple devices.

18
00:00:36,960 --> 00:00:38,690
But as we'll see
nowadays, the new norm

19
00:00:38,690 --> 00:00:41,960
is to train models on tens,
hundreds, thousands, even

20
00:00:41,960 --> 00:00:43,820
tens of thousands of
devices concurrently.

21
00:00:43,820 --> 00:00:45,320
So we need to develop
new algorithms

22
00:00:45,320 --> 00:00:49,130
and new ways of thinking
in order to do that.

23
00:00:49,130 --> 00:00:51,900
And as a bit of running example
through today's lecture,

24
00:00:51,900 --> 00:00:55,040
we're going to be talking
a lot about Llama3-405B,

25
00:00:55,040 --> 00:00:57,560
not because this is the best
model or the most interesting

26
00:00:57,560 --> 00:00:59,920
model, but because this is a
model that this is a fairly

27
00:00:59,920 --> 00:01:02,500
close to state-of-the-art model
that actually shares a lot

28
00:01:02,500 --> 00:01:04,700
of the implementation details
of how it was trained,

29
00:01:04,700 --> 00:01:06,680
the model architecture,
everything like that.

30
00:01:06,680 --> 00:01:08,680
There's a lot of really
amazing, powerful models

31
00:01:08,680 --> 00:01:10,805
that have been trained in
the last couple of years,

32
00:01:10,805 --> 00:01:13,162
from Google, from OpenAI,
from Anthropic, from others.

33
00:01:13,162 --> 00:01:15,370
But basically they don't
share any details whatsoever

34
00:01:15,370 --> 00:01:16,880
about their models anymore.

35
00:01:16,880 --> 00:01:19,750
There's a very famous quote
that marked a sea change

36
00:01:19,750 --> 00:01:23,960
in the industry to me that was
in the GPT4 paper back in 2023.

37
00:01:23,960 --> 00:01:26,553
So when they released the
GPT4 model, they said,

38
00:01:26,553 --> 00:01:28,720
"Given both the competitive
landscape and the safety

39
00:01:28,720 --> 00:01:32,150
implications of large-scale
models like GPT4, this report,

40
00:01:32,150 --> 00:01:34,370
meaning the paper
they wrote about GPT4,

41
00:01:34,370 --> 00:01:36,980
contains no further details
about the architecture,

42
00:01:36,980 --> 00:01:39,470
including model size,
hardware, training compute,

43
00:01:39,470 --> 00:01:42,080
data set construction,
training method, or similar."

44
00:01:42,080 --> 00:01:43,400
And that's basically the news.

45
00:01:43,400 --> 00:01:45,910
That's basically been the
state-of-the-art for large-scale

46
00:01:45,910 --> 00:01:49,040
models the last three
years since GPT4.

47
00:01:49,040 --> 00:01:50,902
They don't tell you
anything about anything.

48
00:01:50,902 --> 00:01:52,610
They'll tell you
nothing about the model.

49
00:01:52,610 --> 00:01:54,902
You'll be lucky if they'll
tell you it's a transformer.

50
00:01:54,902 --> 00:01:56,260
They might tell you that much.

51
00:01:56,260 --> 00:01:58,420
So Llama3 is notable,
not because it's

52
00:01:58,420 --> 00:02:00,170
the best model out
there, but because it's

53
00:02:00,170 --> 00:02:01,950
one of the most open
models out there.

54
00:02:01,950 --> 00:02:04,670
So this is a large language
model that was trained by Meta

55
00:02:04,670 --> 00:02:08,340
and released open source about
a year ago, in April 2024.

56
00:02:08,340 --> 00:02:10,370
And unlike OpenAI,
the paper actually

57
00:02:10,370 --> 00:02:13,440
does share a lot of the details
about the model training.

58
00:02:13,440 --> 00:02:15,080
Not too much about
the data set, a lot

59
00:02:15,080 --> 00:02:18,000
about the system infrastructure
that was used to train it.

60
00:02:18,000 --> 00:02:19,640
So this is something that we--

61
00:02:19,640 --> 00:02:23,030
and this gives us a peek
into how large-scale LLMs are

62
00:02:23,030 --> 00:02:24,800
actually trained these days.

63
00:02:24,800 --> 00:02:27,110
And by the way, there
is a new Llama4 model

64
00:02:27,110 --> 00:02:30,850
that just came out from
Meta last month, April 2025.

65
00:02:30,850 --> 00:02:33,350
So there are slightly better
models out there in open source

66
00:02:33,350 --> 00:02:35,872
already, but there's
no paper on Llama4 yet.

67
00:02:35,872 --> 00:02:37,580
So I'm excited to read
that one hopefully

68
00:02:37,580 --> 00:02:38,750
when it comes out in
a couple of months

69
00:02:38,750 --> 00:02:41,420
and see what can we learn from
the new generation of Llama

70
00:02:41,420 --> 00:02:42,000
training.

71
00:02:42,000 --> 00:02:44,415
But just as a running example
through today's lecture,

72
00:02:44,415 --> 00:02:46,040
we'll be pointing
out a lot of examples

73
00:02:46,040 --> 00:02:50,252
from the Llama3-405B
model for this reason.

74
00:02:50,252 --> 00:02:51,710
OK, so there's
basically two things

75
00:02:51,710 --> 00:02:53,270
that I want to talk about today.

76
00:02:53,270 --> 00:02:56,110
One is a bit about GPU
hardware, and the other

77
00:02:56,110 --> 00:02:57,860
is how to train on lots of GPUs.

78
00:02:57,860 --> 00:02:59,920
So I want to give you
a sense both of what

79
00:02:59,920 --> 00:03:02,260
actually is the hardware
that these things execute on,

80
00:03:02,260 --> 00:03:04,510
as well as the algorithms
that we need to use in order

81
00:03:04,510 --> 00:03:06,020
to train on a lot of them.

82
00:03:06,020 --> 00:03:08,990
So first, we're going to talk a
little bit about GPU hardware.

83
00:03:08,990 --> 00:03:11,230
So GPU, for those of
you that don't know,

84
00:03:11,230 --> 00:03:12,860
is Graphics Processing Unit.

85
00:03:12,860 --> 00:03:14,650
These were specialized
coprocessors

86
00:03:14,650 --> 00:03:17,060
that were originally developed
for computer graphics.

87
00:03:17,060 --> 00:03:19,400
And they turned out
to be very useful

88
00:03:19,400 --> 00:03:21,335
generalizable
parallel processors.

89
00:03:21,335 --> 00:03:22,960
It's actually very
fitting to be giving

90
00:03:22,960 --> 00:03:24,850
this lecture in this
room, because this

91
00:03:24,850 --> 00:03:26,690
is the Huang auditorium.

92
00:03:26,690 --> 00:03:28,660
Jen-Hsun Huang is
the CEO and founder

93
00:03:28,660 --> 00:03:32,050
of NVIDIA, which is the
biggest company right now

94
00:03:32,050 --> 00:03:35,090
and has been for the last couple
of decades in producing GPUs,

95
00:03:35,090 --> 00:03:37,790
both for gaming and for ML.

96
00:03:37,790 --> 00:03:39,952
So these things started
off basically for graphics.

97
00:03:39,952 --> 00:03:41,410
Because if you
think about it, when

98
00:03:41,410 --> 00:03:42,570
you're doing computer
graphics, you

99
00:03:42,570 --> 00:03:44,528
need to generate a lot
of pixels on the screen.

100
00:03:44,528 --> 00:03:46,540
You need to process
lots of little pieces

101
00:03:46,540 --> 00:03:48,770
of primitive geometry
to produce those pixels.

102
00:03:48,770 --> 00:03:51,670
So it's very natural to do a lot
of computation all in parallel

103
00:03:51,670 --> 00:03:53,380
when you're doing
computer graphics.

104
00:03:53,380 --> 00:03:57,200
So people quickly figured
out that this hardware that

105
00:03:57,200 --> 00:03:59,600
had been built, intended
to use in computer graphics

106
00:03:59,600 --> 00:04:01,100
could actually be
used for much more

107
00:04:01,100 --> 00:04:04,970
general pieces of parallel
computation as well.

108
00:04:04,970 --> 00:04:08,068
So in the early days
in the early 2000s,

109
00:04:08,068 --> 00:04:10,610
researchers figured out how they
could contort these graphics

110
00:04:10,610 --> 00:04:13,910
cards into doing generalizable
parallel programming.

111
00:04:13,910 --> 00:04:16,040
And then moving on towards
the end of the 2000s

112
00:04:16,040 --> 00:04:18,950
and into the 2010s, NVIDIA
really picked up this

113
00:04:18,950 --> 00:04:21,082
and developed these
things, marketed them,

114
00:04:21,082 --> 00:04:23,540
built them with the intention
of being generalized parallel

115
00:04:23,540 --> 00:04:24,375
processors.

116
00:04:24,375 --> 00:04:26,750
They didn't quite know at the
time what exactly they were

117
00:04:26,750 --> 00:04:27,930
going to be used for, I think.

118
00:04:27,930 --> 00:04:30,350
I think they had this general
idea that parallel processing

119
00:04:30,350 --> 00:04:32,960
was going to be important, and
they really capitalized on deep

120
00:04:32,960 --> 00:04:36,302
learning when it started to
take off in the early 2010s.

121
00:04:36,302 --> 00:04:38,510
Much to NVIDIA's credit, I
think they really realized

122
00:04:38,510 --> 00:04:41,650
the potential of this
research area very early,

123
00:04:41,650 --> 00:04:44,240
even in the early 2010s
and started putting a ton

124
00:04:44,240 --> 00:04:46,820
of resources into making sure
that their hardware was really

125
00:04:46,820 --> 00:04:49,230
useful for deep
learning training.

126
00:04:49,230 --> 00:04:52,190
And it's basically
been the main way

127
00:04:52,190 --> 00:04:54,840
that people train large
scale deep learning models

128
00:04:54,840 --> 00:04:56,452
for more than a decade now.

129
00:04:56,452 --> 00:04:58,660
That's starting to change,
as we'll see a little bit,

130
00:04:58,660 --> 00:05:03,690
but their chips are the
main one that people use.

131
00:05:03,690 --> 00:05:06,030
So I always like looking
inside these things

132
00:05:06,030 --> 00:05:07,240
and seeing what's in them.

133
00:05:07,240 --> 00:05:09,370
So this is a picture
of the NVIDIA H100,

134
00:05:09,370 --> 00:05:12,930
which is the mainstay of
deep learning training

135
00:05:12,930 --> 00:05:14,035
right now, today.

136
00:05:14,035 --> 00:05:15,910
There's a next generation
that just came out,

137
00:05:15,910 --> 00:05:17,368
but it's not really
accessible yet.

138
00:05:17,368 --> 00:05:19,180
I haven't trained
anything on it yet.

139
00:05:19,180 --> 00:05:21,870
So this is the
state-of-the-art right now.

140
00:05:21,870 --> 00:05:25,702
Inside this NVIDIA GPU, inside
this H100 GPU in the middle

141
00:05:25,702 --> 00:05:26,910
here are these compute cores.

142
00:05:26,910 --> 00:05:29,400
And surrounding that
are 80 gigabytes of HBM

143
00:05:29,400 --> 00:05:30,907
memory, high-bandwidth memory.

144
00:05:30,907 --> 00:05:32,490
So you can see the
memory is separated

145
00:05:32,490 --> 00:05:33,460
from the compute cores.

146
00:05:33,460 --> 00:05:34,360
They need to move.

147
00:05:34,360 --> 00:05:36,790
They need to talk to
each other over this bus

148
00:05:36,790 --> 00:05:40,030
to move data back and forth from
the GPU memory into the cores.

149
00:05:40,030 --> 00:05:43,180
And it can do that at a speed
of about 3 terabytes per second,

150
00:05:43,180 --> 00:05:45,330
which is a lot of
bits moving around.

151
00:05:45,330 --> 00:05:48,640
Now, if we dive deeper
inside the GPU cores,

152
00:05:48,640 --> 00:05:51,240
we see that in the middle
in that compute core part,

153
00:05:51,240 --> 00:05:52,920
we've got a smaller
piece of memory,

154
00:05:52,920 --> 00:05:55,190
about 50 megabytes
of L2 cache that

155
00:05:55,190 --> 00:05:58,890
is much, much smaller than the
80 gigabytes of HBM memory.

156
00:05:58,890 --> 00:06:02,032
But it's very, very close to
the actual computing elements.

157
00:06:02,032 --> 00:06:03,740
So they can be accessed
much more quickly

158
00:06:03,740 --> 00:06:05,300
from the compute cores.

159
00:06:05,300 --> 00:06:06,950
And then the real
heart of the thing

160
00:06:06,950 --> 00:06:10,940
are these 132 Streaming
Multiprocessors or SMs.

161
00:06:10,940 --> 00:06:13,760
These are independent
parallel cores.

162
00:06:13,760 --> 00:06:15,290
They're a little
bit more powerful

163
00:06:15,290 --> 00:06:17,498
in some ways than
a typical CPU core,

164
00:06:17,498 --> 00:06:19,290
because they can do a
lot more parallelism.

165
00:06:19,290 --> 00:06:21,410
But they're a lot weaker
than a typical CPU

166
00:06:21,410 --> 00:06:23,360
core also in a lot of
ways, because they tend

167
00:06:23,360 --> 00:06:24,585
to have slower clock speeds.

168
00:06:24,585 --> 00:06:26,460
They can't do as much
instruction prediction,

169
00:06:26,460 --> 00:06:27,600
as much branch prediction.

170
00:06:27,600 --> 00:06:29,540
So it's really hard
to make exact apples

171
00:06:29,540 --> 00:06:32,450
to apples comparisons between
these GPU cores and the CPU

172
00:06:32,450 --> 00:06:32,970
cores.

173
00:06:32,970 --> 00:06:35,600
But I usually think of these
streaming multiprocessors

174
00:06:35,600 --> 00:06:38,480
as roughly akin to a CPU core.

175
00:06:38,480 --> 00:06:41,030
Also, I know someone's going
to go back home and actually

176
00:06:41,030 --> 00:06:43,020
count all the little
boxes on this screen,

177
00:06:43,020 --> 00:06:45,810
and you'll see that there
are actually 144 of them

178
00:06:45,810 --> 00:06:47,460
and when I've said
there's only 132.

179
00:06:47,460 --> 00:06:48,270
Why is that?

180
00:06:48,270 --> 00:06:51,070
It's because all GPU hardware
uses a process called binning,

181
00:06:51,070 --> 00:06:53,640
where making these things, they
have so many transistors, so

182
00:06:53,640 --> 00:06:54,940
many little computing elements.

183
00:06:54,940 --> 00:06:57,107
No matter how much money
they pour into the process,

184
00:06:57,107 --> 00:06:58,600
they just don't
come out perfectly.

185
00:06:58,600 --> 00:07:00,683
Some of them always end
up a little bit messed up.

186
00:07:00,683 --> 00:07:03,340
So they plan for that in the
development of their products.

187
00:07:03,340 --> 00:07:05,530
And they say, we're going
to try to make a chip.

188
00:07:05,530 --> 00:07:08,250
The full chip in theory has
144, but none of the chips

189
00:07:08,250 --> 00:07:08,890
are perfect.

190
00:07:08,890 --> 00:07:10,973
But we know we'll get a
reasonable number of those

191
00:07:10,973 --> 00:07:13,360
that have at least 132
that are functioning.

192
00:07:13,360 --> 00:07:15,400
So they tend to use
this process of binning.

193
00:07:15,400 --> 00:07:17,810
So then they actually
only can sell

194
00:07:17,810 --> 00:07:19,560
a much more larger
proportion of the chips

195
00:07:19,560 --> 00:07:24,825
they tried to produce by only
promising that 132 of them

196
00:07:24,825 --> 00:07:27,840
will be turned on.

197
00:07:27,840 --> 00:07:30,270
Then we can dive even deeper
inside one of those streaming

198
00:07:30,270 --> 00:07:31,180
multiprocessors.

199
00:07:31,180 --> 00:07:33,600
And then we can see
even more what's

200
00:07:33,600 --> 00:07:35,260
going on inside of these GPUs.

201
00:07:35,260 --> 00:07:38,070
So this is just one of
the 132 active streaming

202
00:07:38,070 --> 00:07:40,120
multiprocessors inside an H100.

203
00:07:40,120 --> 00:07:43,050
And there's a couple interesting
elements in here to look at.

204
00:07:43,050 --> 00:07:46,530
First, we see we have
256 kilobytes of L1 cache

205
00:07:46,530 --> 00:07:47,810
and register files.

206
00:07:47,810 --> 00:07:51,030
So this continues the trend of
the memory hierarchy in the GPU.

207
00:07:51,030 --> 00:07:53,583
We saw that in general,
you thought you

208
00:07:53,583 --> 00:07:54,750
were learning deep learning?

209
00:07:54,750 --> 00:07:56,708
You're actually learning
computer architecture.

210
00:07:56,708 --> 00:07:57,710
Sorry, it's a surprise.

211
00:07:57,710 --> 00:07:59,480
And it turns out
that memory hierarchy

212
00:07:59,480 --> 00:08:01,070
is really important
for deep learning

213
00:08:01,070 --> 00:08:03,415
and for all kind of high
performance computing.

214
00:08:03,415 --> 00:08:04,790
And the general
trend is that you

215
00:08:04,790 --> 00:08:06,860
have larger bits of
memory that are farther

216
00:08:06,860 --> 00:08:08,220
away from the compute cores.

217
00:08:08,220 --> 00:08:10,080
And the closer you get
to the compute cores,

218
00:08:10,080 --> 00:08:12,690
you have smaller bits of memory
but are much, much faster.

219
00:08:12,690 --> 00:08:14,390
And it's really
important to write--

220
00:08:14,390 --> 00:08:16,803
well, if you're writing the
low-level algorithms that

221
00:08:16,803 --> 00:08:18,470
run on these things,
it's very important

222
00:08:18,470 --> 00:08:20,240
to be aware of this
memory hierarchy

223
00:08:20,240 --> 00:08:21,890
and to be very
diligent in passing

224
00:08:21,890 --> 00:08:24,813
data between different phases
of this memory hierarchy.

225
00:08:24,813 --> 00:08:26,730
And if you're writing
performance GPU kernels,

226
00:08:26,730 --> 00:08:29,460
you spend a lot of time
trying to optimize that.

227
00:08:29,460 --> 00:08:31,910
So just to give you
a flavor of that,

228
00:08:31,910 --> 00:08:35,309
we see the three levels of
memory hierarchy in the H100.

229
00:08:35,309 --> 00:08:40,650
You've got 256 kilobytes of L1
cache, 50 megabytes of L2 cache,

230
00:08:40,650 --> 00:08:43,169
and then 80 gigabytes
of HBM memory.

231
00:08:43,169 --> 00:08:44,750
So those are the
three primary levels

232
00:08:44,750 --> 00:08:47,680
of memory hierarchy in the H100.

233
00:08:47,680 --> 00:08:51,430
Then we've also got 128
of these FP32 cores.

234
00:08:51,430 --> 00:08:53,260
These are little
arithmetic units

235
00:08:53,260 --> 00:08:56,060
that can do generalized
floating point operations.

236
00:08:56,060 --> 00:08:59,980
And in particular, each
one of these 128 FP32 cores

237
00:08:59,980 --> 00:09:03,890
can compute ax plus b, where
a, x, and b are all scalars.

238
00:09:03,890 --> 00:09:06,950
It can perform that bit of
computation in one clock cycle.

239
00:09:06,950 --> 00:09:10,030
So then if you add this
all up, that ax plus b

240
00:09:10,030 --> 00:09:12,400
is basically 1
multiply 1 addition,

241
00:09:12,400 --> 00:09:15,190
and you've got 128
of these cores.

242
00:09:15,190 --> 00:09:20,170
This whole SM can do 256
floating point operations per SM

243
00:09:20,170 --> 00:09:22,690
per clock cycle of the device.

244
00:09:22,690 --> 00:09:24,940
Then we'll also see
that in red, we've

245
00:09:24,940 --> 00:09:27,880
got-- this is where the real
magic happens, that in addition

246
00:09:27,880 --> 00:09:31,990
to these FP32 cores, there are
also these four tensor cores.

247
00:09:31,990 --> 00:09:34,910
I think the name is a
little bit of a misnomer.

248
00:09:34,910 --> 00:09:37,210
These are actually matrix cores.

249
00:09:37,210 --> 00:09:39,250
What each of these
little tensor cores does

250
00:09:39,250 --> 00:09:41,990
is they're special
circuits that are designed.

251
00:09:41,990 --> 00:09:43,130
They only do one thing.

252
00:09:43,130 --> 00:09:44,340
They do matrix multiply.

253
00:09:44,340 --> 00:09:46,190
So each one of these
little tensor cores

254
00:09:46,190 --> 00:09:48,890
can do a single chunk
of matrix multiply.

255
00:09:48,890 --> 00:09:52,940
In particular, I believe
the H100 ones can do a 16--

256
00:09:52,940 --> 00:09:57,180
input matrix A is 16 by 4,
input matrix B is 4 by 8,

257
00:09:57,180 --> 00:10:01,440
and then plus a bias
matrix of a size 16 by 8.

258
00:10:01,440 --> 00:10:03,920
So it basically does
ax plus b, where

259
00:10:03,920 --> 00:10:06,900
a, x, and b are little matrix
chunks of this fixed size.

260
00:10:06,900 --> 00:10:09,320
And it can do that one
little chunk of matrix

261
00:10:09,320 --> 00:10:12,560
multiply once per tensor
core per clock cycle.

262
00:10:12,560 --> 00:10:15,390
So then if you multiply
all these numbers out,

263
00:10:15,390 --> 00:10:19,850
you see that little matrix
multiply of ax plus b of that

264
00:10:19,850 --> 00:10:23,450
particular size is 1,024
floating point operations,

265
00:10:23,450 --> 00:10:25,130
where that's counting
each multiply,

266
00:10:25,130 --> 00:10:27,300
each add as a single
floating point operation.

267
00:10:27,300 --> 00:10:30,360
We multiply that by the
4 tensor cores in the SM.

268
00:10:30,360 --> 00:10:34,310
And we see that the entire SM,
if it's going through the tensor

269
00:10:34,310 --> 00:10:39,350
cores, can do 4,096 floating
point operations per SM per

270
00:10:39,350 --> 00:10:40,460
clock cycle.

271
00:10:40,460 --> 00:10:42,660
And this we need to
compare with the 256

272
00:10:42,660 --> 00:10:44,590
that we can get
from the FP32 cores.

273
00:10:44,590 --> 00:10:46,980
And here we see that
just like the tensor

274
00:10:46,980 --> 00:10:48,622
cores are where all
the magic happens.

275
00:10:48,622 --> 00:10:50,580
This is where the main
throughput of the device

276
00:10:50,580 --> 00:10:51,163
comes from.

277
00:10:51,163 --> 00:10:53,580
And if you're writing code
that wants to run on these GPUs

278
00:10:53,580 --> 00:10:56,290
and they can make
maximum usage of them,

279
00:10:56,290 --> 00:10:59,297
you need to make maximum
usage of these tensor cores.

280
00:10:59,297 --> 00:11:01,380
Another interesting thing
about these tensor cores

281
00:11:01,380 --> 00:11:03,452
is that they actually
operate in mixed precision.

282
00:11:03,452 --> 00:11:05,410
Rather than traditional
floating point numbers,

283
00:11:05,410 --> 00:11:08,760
which are normally 32-bit, the
tensor cores tend to use a mixed

284
00:11:08,760 --> 00:11:11,903
precision procedure, where
the inputs are usually 16-bit.

285
00:11:11,903 --> 00:11:14,070
And there's a couple of
different interesting 16-bit

286
00:11:14,070 --> 00:11:17,320
formats that they can use
that we can't get into today.

287
00:11:17,320 --> 00:11:20,100
And they'll do the
multiplications in this lower

288
00:11:20,100 --> 00:11:22,450
precision 16-bit and
then do the additions,

289
00:11:22,450 --> 00:11:24,820
the accumulations in a
higher precision 32-bit.

290
00:11:24,820 --> 00:11:29,170
So these tensor cores take a
low precision, 16-bit input,

291
00:11:29,170 --> 00:11:31,170
and then do some of the
intermediate computation

292
00:11:31,170 --> 00:11:34,290
and produce the outputs in
a higher precision 32-bit.

293
00:11:34,290 --> 00:11:37,660
And this is important
because at the PyTorch layer,

294
00:11:37,660 --> 00:11:40,760
if you forget to cast
your model into 16-bit,

295
00:11:40,760 --> 00:11:42,580
it will run on the
floating point cores.

296
00:11:42,580 --> 00:11:45,080
Instead it will be 20 times
slower than you expect.

297
00:11:45,080 --> 00:11:47,690
So this seems like a
little bit of minutia,

298
00:11:47,690 --> 00:11:50,530
but it becomes very tangible
when you mess up those data

299
00:11:50,530 --> 00:11:53,290
types in your PyTorch code.

300
00:11:53,290 --> 00:11:55,910
And then note, so
GPUs are really fast,

301
00:11:55,910 --> 00:11:58,930
and it's really crazy just
how much faster they've

302
00:11:58,930 --> 00:12:02,200
gotten over the past
decade or 15 years or so.

303
00:12:02,200 --> 00:12:05,470
So when I first started my
PhD and was working on deep

304
00:12:05,470 --> 00:12:08,110
learning, the state-of-the-art
GPU that we were all using was

305
00:12:08,110 --> 00:12:12,290
this K40 GPU, which was
released back in 2013.

306
00:12:12,290 --> 00:12:16,420
And this thing could do just
a 5 teraflops of FP32 compute

307
00:12:16,420 --> 00:12:17,662
for the whole device.

308
00:12:17,662 --> 00:12:19,370
All right, so I should
explain the graph.

309
00:12:19,370 --> 00:12:23,260
So the x-axis is time
ranging from about 2013

310
00:12:23,260 --> 00:12:24,380
up to present day.

311
00:12:24,380 --> 00:12:26,500
And then the y-axis
is the peak throughput

312
00:12:26,500 --> 00:12:31,180
of each of these devices
measured in terms of teraflops

313
00:12:31,180 --> 00:12:33,040
per second per device.

314
00:12:33,040 --> 00:12:35,870
And you can see the
graph goes up a lot.

315
00:12:35,870 --> 00:12:37,900
But there's something
salient to notice here

316
00:12:37,900 --> 00:12:40,600
is that from the
K40 to the P100,

317
00:12:40,600 --> 00:12:42,820
something really amazing
happened in the V100,

318
00:12:42,820 --> 00:12:47,200
which came out towards the end
of my PhD and around 2016, 2017,

319
00:12:47,200 --> 00:12:49,290
and that the V100 was
the first device that

320
00:12:49,290 --> 00:12:50,970
introduced these tensor cores.

321
00:12:50,970 --> 00:12:54,960
And since then,
more recent devices

322
00:12:54,960 --> 00:12:57,970
have gotten more tensor
cores, bigger tensor cores,

323
00:12:57,970 --> 00:13:00,310
more of the device area
allocated to tensor cores,

324
00:13:00,310 --> 00:13:02,775
and this has resulted
in a gigantic increase

325
00:13:02,775 --> 00:13:04,900
in the throughput of these
devices over the past 10

326
00:13:04,900 --> 00:13:06,270
or 15 years.

327
00:13:06,270 --> 00:13:09,000
And the most recent
device is this B200

328
00:13:09,000 --> 00:13:10,750
that was formally announced.

329
00:13:10,750 --> 00:13:12,310
It's slowly rolling out now.

330
00:13:12,310 --> 00:13:21,600
This one in theory has about
83.3 teraflops per second

331
00:13:21,600 --> 00:13:25,560
of FP32 compute and 5,000
teraflops per second in theory

332
00:13:25,560 --> 00:13:28,270
of mixed precision compute
on the tensor cores.

333
00:13:28,270 --> 00:13:31,290
So if you step back, this is
like literally we've been living

334
00:13:31,290 --> 00:13:35,580
through a 1,000 fold increase
in computation over the past 12

335
00:13:35,580 --> 00:13:36,640
years.

336
00:13:36,640 --> 00:13:38,625
And that's just that
the per device level.

337
00:13:38,625 --> 00:13:42,220
So one explanation of why AI has
gotten so good in the last 10

338
00:13:42,220 --> 00:13:43,660
years, what has happened?

339
00:13:43,660 --> 00:13:44,840
This is the answer.

340
00:13:44,840 --> 00:13:47,110
There's now a source of
computation that we're taking

341
00:13:47,110 --> 00:13:50,390
advantage of, and it's gone up
by 1,000x in the last decade.

342
00:13:50,390 --> 00:13:53,375
Anytime anything in the
world changes by 1,000x,

343
00:13:53,375 --> 00:13:55,750
you should step up and pay
attention because that's going

344
00:13:55,750 --> 00:13:58,700
to cause major changes in our
technological capabilities.

345
00:13:58,700 --> 00:14:01,010
And this 1,000x
improvement, I think,

346
00:14:01,010 --> 00:14:03,610
is the major driver of
improvement in deep learning

347
00:14:03,610 --> 00:14:05,290
over the past decade.

348
00:14:05,290 --> 00:14:08,840
So it does not have
5,000 tensor cores.

349
00:14:08,840 --> 00:14:12,660
That's 5,000 teraflops of
compute on the tensor cores.

350
00:14:12,660 --> 00:14:13,160
Yeah.

351
00:14:13,160 --> 00:14:15,285
So we always try to
distinguish between the compute

352
00:14:15,285 --> 00:14:19,330
on the tensor cores versus
the compute on the FP32 cores.

353
00:14:19,330 --> 00:14:20,960
So this is already crazy.

354
00:14:20,960 --> 00:14:23,650
It's already crazy that
there's been a 1,000 increase

355
00:14:23,650 --> 00:14:25,790
in a device that you
can hold in your hands.

356
00:14:25,790 --> 00:14:29,620
I've held a K40 in my hands,
and I've not had the opportunity

357
00:14:29,620 --> 00:14:30,620
to hold a B100.

358
00:14:30,620 --> 00:14:33,110
But they feel like the
same physical object.

359
00:14:33,110 --> 00:14:35,490
It's like about the same
size, about the same weight,

360
00:14:35,490 --> 00:14:38,490
looks the same, but the one from
today is 1,000 times faster than

361
00:14:38,490 --> 00:14:39,640
the one from 12 years ago.

362
00:14:39,640 --> 00:14:40,830
That's insane.

363
00:14:40,830 --> 00:14:44,980
But it gets even crazier because
we don't train on one GPU.

364
00:14:44,980 --> 00:14:48,250
I said that when the K40
first came out in 2013,

365
00:14:48,250 --> 00:14:51,400
it actually was common to train
a lot of models on just one GPU.

366
00:14:51,400 --> 00:14:53,740
But today, we're training
not just on one GPU.

367
00:14:53,740 --> 00:14:55,780
We're training on thousands,
tens of thousands,

368
00:14:55,780 --> 00:14:57,730
sometimes hundreds
of thousands of GPUs,

369
00:14:57,730 --> 00:15:00,482
all working together
to train one model.

370
00:15:00,482 --> 00:15:03,870
So stack that on top of this
1,000 fold increase in per

371
00:15:03,870 --> 00:15:04,720
device throughput.

372
00:15:04,720 --> 00:15:08,520
And something truly insane has
happened in the past decade.

373
00:15:08,520 --> 00:15:13,030
So we've looked inside the GPU.

374
00:15:13,030 --> 00:15:16,360
Now from here, I want to zoom
out and put that GPU in context,

375
00:15:16,360 --> 00:15:17,980
not looking at
individual devices,

376
00:15:17,980 --> 00:15:19,860
but thinking about the
modern GPU clusters

377
00:15:19,860 --> 00:15:22,300
that we build that stitch a
lot of these things together.

378
00:15:22,300 --> 00:15:26,427
So we've already seen
a single H100 GPU.

379
00:15:26,427 --> 00:15:28,260
And here we can think
of it as another level

380
00:15:28,260 --> 00:15:29,670
of memory hierarchy.

381
00:15:29,670 --> 00:15:31,680
We already saw inside
the H100, there

382
00:15:31,680 --> 00:15:33,742
were three layers
of memory hierarchy

383
00:15:33,742 --> 00:15:35,450
as we got closer to
the compute elements.

384
00:15:35,450 --> 00:15:37,520
And as you got farther away
from the compute elements,

385
00:15:37,520 --> 00:15:39,020
the bandwidth, the
memory bandwidth,

386
00:15:39,020 --> 00:15:41,020
the ability of the device
to move bits around

387
00:15:41,020 --> 00:15:43,460
between different parts
of the system gets slower.

388
00:15:43,460 --> 00:15:45,430
And this trend
actually continues

389
00:15:45,430 --> 00:15:47,650
once you escape the
bounds of a single device

390
00:15:47,650 --> 00:15:50,480
and imagine these in the
context of a full data center.

391
00:15:50,480 --> 00:15:54,250
So here we saw a single H100
GPU gets about 3 terabytes

392
00:15:54,250 --> 00:15:55,640
of memory bandwidth.

393
00:15:55,640 --> 00:15:59,260
That's the GPU memory talking
from its own HBM memory

394
00:15:59,260 --> 00:16:01,640
to its own compute elements,
3 terabytes per second.

395
00:16:01,640 --> 00:16:03,100
It can move bits around.

396
00:16:03,100 --> 00:16:05,890
But these things typically
live inside a GPU server.

397
00:16:05,890 --> 00:16:10,120
Almost all GPU servers have
eight devices in one big box,

398
00:16:10,120 --> 00:16:12,560
and those GPUs can
talk to each other.

399
00:16:12,560 --> 00:16:14,140
And they typically
talk to each other

400
00:16:14,140 --> 00:16:16,540
at a rate of about 900
gigabytes per second

401
00:16:16,540 --> 00:16:20,060
from any one GPU in the server
to any other GPU in the server.

402
00:16:20,060 --> 00:16:23,860
So you can see that's like a
3x less memory communication

403
00:16:23,860 --> 00:16:29,050
bandwidth compared to the GPU
talking inside one device.

404
00:16:29,050 --> 00:16:32,580
And here we again
turn to Llama3.

405
00:16:32,580 --> 00:16:34,597
A lot of major
players don't publish

406
00:16:34,597 --> 00:16:36,430
a lot of details on
their training clusters,

407
00:16:36,430 --> 00:16:38,340
but the Llama3 technical
report did actually

408
00:16:38,340 --> 00:16:40,300
give a lot of details around
their training clusters.

409
00:16:40,300 --> 00:16:41,790
So from here, some
of the specifics

410
00:16:41,790 --> 00:16:44,260
probably vary a little bit
from cluster to cluster.

411
00:16:44,260 --> 00:16:46,710
But these are now numbers
from the Llama3 cluster that

412
00:16:46,710 --> 00:16:49,860
was used to train their models.

413
00:16:49,860 --> 00:16:52,660
So they give in one GPU box.

414
00:16:52,660 --> 00:16:55,530
They stack two of those
box into one server rack.

415
00:16:55,530 --> 00:16:57,280
And a server rack, if
you haven't seen it,

416
00:16:57,280 --> 00:17:00,120
they're about 6 feet tall, like
about the size of a person,

417
00:17:00,120 --> 00:17:02,680
to just get a mental picture
of one of those things.

418
00:17:02,680 --> 00:17:04,740
So one server rack
has two servers inside

419
00:17:04,740 --> 00:17:07,410
of it, a total of 16 GPUs.

420
00:17:07,410 --> 00:17:11,550
Then we connect a lot of server
racks together into a GPU pod.

421
00:17:11,550 --> 00:17:16,230
The Llama3 cluster has GPU pods
that are composed of 192 racks,

422
00:17:16,230 --> 00:17:19,210
which is a total of 3,072 GPUs.

423
00:17:19,210 --> 00:17:21,480
And these things have really
high bandwidth connectors

424
00:17:21,480 --> 00:17:23,589
between all the different racks.

425
00:17:23,589 --> 00:17:27,599
And as a result, any pair
of GPUs inside that pod

426
00:17:27,599 --> 00:17:30,140
can talk to each other at a
rate of about 50 gigabytes

427
00:17:30,140 --> 00:17:30,870
per second.

428
00:17:30,870 --> 00:17:34,100
And now, you see, this is
another 20x decrease in memory

429
00:17:34,100 --> 00:17:37,220
traffic between what an
individual server can talk

430
00:17:37,220 --> 00:17:40,490
and then what any GPU across
an entire rack can talk to each

431
00:17:40,490 --> 00:17:41,330
other.

432
00:17:41,330 --> 00:17:44,100
So 3,072 GPUs seems
like a lot of compute,

433
00:17:44,100 --> 00:17:46,310
but it's nowhere near enough.

434
00:17:46,310 --> 00:17:48,590
So we're going to stack
those GPU pods together

435
00:17:48,590 --> 00:17:50,390
into a full GPU cluster.

436
00:17:50,390 --> 00:17:52,520
So this is actually
the full GPU cluster

437
00:17:52,520 --> 00:17:55,650
that Meta built to train
their Llama3 models.

438
00:17:55,650 --> 00:17:58,670
This thing combines eight
GPU pods together for a total

439
00:17:58,670 --> 00:18:01,790
of 24,576 GPUs.

440
00:18:01,790 --> 00:18:04,610
I could not find exact
numbers on the memory traffic

441
00:18:04,610 --> 00:18:06,530
between these things,
but it's definitely

442
00:18:06,530 --> 00:18:08,870
less than 50
gigabytes per second.

443
00:18:08,870 --> 00:18:12,050
And by the way, this is not the
largest GPU cluster in the world

444
00:18:12,050 --> 00:18:13,522
by a long shot.

445
00:18:13,522 --> 00:18:15,230
It's the biggest one
that I could quickly

446
00:18:15,230 --> 00:18:16,270
find precise numbers on.

447
00:18:16,270 --> 00:18:18,770
But there are definitely GPU
clusters out there in the world

448
00:18:18,770 --> 00:18:21,930
that are 50,000
GPUs, 100,000 GPUs.

449
00:18:21,930 --> 00:18:24,555
They exist, and people
train models on them.

450
00:18:24,555 --> 00:18:27,180
And the way that this works is
it sort of scales out naturally.

451
00:18:27,180 --> 00:18:29,730
So you would just cluster
together more pods together

452
00:18:29,730 --> 00:18:31,818
to create a bigger
cluster, or you

453
00:18:31,818 --> 00:18:33,360
might have another
level of hierarchy

454
00:18:33,360 --> 00:18:35,220
where you might have
a super pod that

455
00:18:35,220 --> 00:18:38,310
connects to other super pods
to get you another level up.

456
00:18:38,310 --> 00:18:40,560
How long do they train
with that GPU cluster?

457
00:18:40,560 --> 00:18:43,300
I don't remember offhand
for the Llama3 models,

458
00:18:43,300 --> 00:18:45,030
but there's been kind
of a rule of thumb

459
00:18:45,030 --> 00:18:48,090
for the past decade is that the
longest models that people train

460
00:18:48,090 --> 00:18:49,972
are usually on the
order of months.

461
00:18:49,972 --> 00:18:51,930
And that I think has less
to do with technology

462
00:18:51,930 --> 00:18:53,070
and more to do with people.

463
00:18:53,070 --> 00:18:57,120
When it comes to having
progress, making plans, having

464
00:18:57,120 --> 00:18:59,130
people work on things,
it's very difficult

465
00:18:59,130 --> 00:19:01,543
to have training runs
that are very, very long.

466
00:19:01,543 --> 00:19:03,960
So the longest training runs,
the biggest state-of-the-art

467
00:19:03,960 --> 00:19:06,360
models, I think are
typically measured in months.

468
00:19:06,360 --> 00:19:09,010
I would not be surprised if
the very, very largest models,

469
00:19:09,010 --> 00:19:13,140
the GPT-4.5, the GPT-5, if those
are pushing closer to a year

470
00:19:13,140 --> 00:19:14,340
at this point.

471
00:19:14,340 --> 00:19:16,350
But it's pretty common
to see training runs that

472
00:19:16,350 --> 00:19:18,960
are on the order of a couple
of months on these really

473
00:19:18,960 --> 00:19:20,760
big training clusters.

474
00:19:20,760 --> 00:19:23,190
The question is, why do you
organize servers into a rack

475
00:19:23,190 --> 00:19:24,310
rather than in a pod?

476
00:19:24,310 --> 00:19:25,660
You got to put them somewhere.

477
00:19:25,660 --> 00:19:27,640
There's physical
constraints on these things.

478
00:19:27,640 --> 00:19:30,820
So server racks have
been a standard unit

479
00:19:30,820 --> 00:19:33,500
in just data centers for
decades at this point.

480
00:19:33,500 --> 00:19:36,970
So when these new devices
came onto the scene of GPUs,

481
00:19:36,970 --> 00:19:38,737
that gives you a
different kind of server.

482
00:19:38,737 --> 00:19:40,070
They're a lot physically bigger.

483
00:19:40,070 --> 00:19:41,770
They have a lot
more power, but you

484
00:19:41,770 --> 00:19:45,470
can't redesign the whole data
center from scratch overnight.

485
00:19:45,470 --> 00:19:48,700
So as a result, the server
rack has been a standard unit

486
00:19:48,700 --> 00:19:50,620
with standard hardware
sizes and everything

487
00:19:50,620 --> 00:19:53,380
that the data centers are
typically built around.

488
00:19:53,380 --> 00:19:56,360
How much physical space does
like a cluster or something

489
00:19:56,360 --> 00:19:56,860
[INAUDIBLE]?

490
00:19:56,860 --> 00:19:59,800
Oh, that's a great question.

491
00:19:59,800 --> 00:20:02,560
So you should think
of a single server

492
00:20:02,560 --> 00:20:06,410
rack as being like around 6, 8
feet tall, something like that,

493
00:20:06,410 --> 00:20:07,580
like, about this big.

494
00:20:07,580 --> 00:20:10,600
So maybe a server rack would be
around the size of this podium

495
00:20:10,600 --> 00:20:13,120
and about as tall as me.

496
00:20:13,120 --> 00:20:15,530
Then you've got
192 racks in a pod.

497
00:20:15,530 --> 00:20:17,733
So imagine 200 of these podiums.

498
00:20:17,733 --> 00:20:18,650
How big would that be?

499
00:20:18,650 --> 00:20:20,320
And then multiply that by 8.

500
00:20:20,320 --> 00:20:22,660
But that's actually a little
bit of an underestimate,

501
00:20:22,660 --> 00:20:24,820
because you typically
organize these things in rows

502
00:20:24,820 --> 00:20:26,780
so people can actually
walk between them.

503
00:20:26,780 --> 00:20:28,197
And there's more
hardware that you

504
00:20:28,197 --> 00:20:31,600
need to pack into the cluster,
not just the compute racks.

505
00:20:31,600 --> 00:20:34,330
So in addition to the compute
racks that have the physical GPU

506
00:20:34,330 --> 00:20:35,980
servers, there will
be other racks that

507
00:20:35,980 --> 00:20:37,310
contain networking hardware.

508
00:20:37,310 --> 00:20:40,270
We've got a lot of bits
that need to fly around

509
00:20:40,270 --> 00:20:41,403
between all these devices.

510
00:20:41,403 --> 00:20:42,820
So they'll be
dedicated racks that

511
00:20:42,820 --> 00:20:44,070
only hold networking hardware.

512
00:20:44,070 --> 00:20:45,487
There will also
be dedicated racks

513
00:20:45,487 --> 00:20:47,350
that only hold storage
hardware, because you

514
00:20:47,350 --> 00:20:49,060
need to store that training
data somewhere and get that

515
00:20:49,060 --> 00:20:50,030
into your devices.

516
00:20:50,030 --> 00:20:52,860
So these things can take
up quite a lot of space.

517
00:20:52,860 --> 00:20:53,360
Oh yeah.

518
00:20:53,360 --> 00:20:55,985
Question is, when you go
to these big clusters,

519
00:20:55,985 --> 00:20:58,610
do the smaller units of compute
maintain the higher throughput?

520
00:20:58,610 --> 00:20:59,510
And yes, they do.

521
00:20:59,510 --> 00:21:02,740
And that's part of the secret
and the challenge of designing

522
00:21:02,740 --> 00:21:04,340
for these systems,
because you ideally

523
00:21:04,340 --> 00:21:06,340
want to take advantage
of the fast communication

524
00:21:06,340 --> 00:21:08,043
when you can get it,
but also fall back

525
00:21:08,043 --> 00:21:10,210
gracefully to the slower
communication on the larger

526
00:21:10,210 --> 00:21:12,850
units as you scale up.

527
00:21:12,850 --> 00:21:14,410
Oh, how hot does it get?

528
00:21:14,410 --> 00:21:16,165
Pretty hot.

529
00:21:16,165 --> 00:21:19,165
If any of you is a gamer
and has a 4090 GPU or 5080

530
00:21:19,165 --> 00:21:23,540
GPU in your desktop at
home, a single 4090 GPU,

531
00:21:23,540 --> 00:21:25,540
if you're playing games,
will heat up your room,

532
00:21:25,540 --> 00:21:27,250
make you want to
open the window.

533
00:21:27,250 --> 00:21:29,830
It will make the room
physically warmer.

534
00:21:29,830 --> 00:21:31,780
So imagine if that's
what a single gaming

535
00:21:31,780 --> 00:21:33,620
GPU can do to an
average-sized room,

536
00:21:33,620 --> 00:21:35,230
yeah, there's some serious
cooling requirements

537
00:21:35,230 --> 00:21:37,605
for these things once you
stack tens of thousands of them

538
00:21:37,605 --> 00:21:38,625
in a big data center.

540
00:21:41,515 --> 00:21:43,390
Although another
interesting thing is about--

541
00:21:43,390 --> 00:21:44,810
I mean, the cooling gets crazy.

542
00:21:44,810 --> 00:21:46,360
So a gaming desktop
will typically

543
00:21:46,360 --> 00:21:48,020
be air cooled,
sometimes water cooled.

544
00:21:48,020 --> 00:21:50,452
And then you can design
different cooling systems.

545
00:21:50,452 --> 00:21:51,910
And you can go nuts
on the hardware

546
00:21:51,910 --> 00:21:54,400
here to try to optimize
all this stuff.

547
00:21:54,400 --> 00:21:54,950
All right.

548
00:21:54,950 --> 00:21:56,450
So I think this
stuff is super cool.

549
00:21:56,450 --> 00:21:59,355
It's just imagining like
these GPUs are not just

550
00:21:59,355 --> 00:22:01,730
mythical creatures that are
floating around in the cloud.

551
00:22:01,730 --> 00:22:04,313
These are actual physical atoms
that someone built and stacked

552
00:22:04,313 --> 00:22:05,390
up in a room somewhere.

553
00:22:05,390 --> 00:22:08,290
And it's really interesting to
imagine what they look like.

554
00:22:08,290 --> 00:22:12,070
And so basically one
kind of mindset shift

555
00:22:12,070 --> 00:22:13,780
when we moved to
these big GPU clusters

556
00:22:13,780 --> 00:22:16,580
is actually thinking not so much
about the individual devices,

557
00:22:16,580 --> 00:22:17,930
about the individual servers.

558
00:22:17,930 --> 00:22:20,080
I basically tried to
think of the entire data

559
00:22:20,080 --> 00:22:22,250
center as one big computer.

560
00:22:22,250 --> 00:22:25,860
And this big computer in
this case has 24,000 GPUs,

561
00:22:25,860 --> 00:22:29,040
1.8 terabytes of HBM
memory on the GPUs,

562
00:22:29,040 --> 00:22:33,750
415 million FP32 cores,
13 million tensor cores.

563
00:22:33,750 --> 00:22:37,830
And this whole thing can do 24
exaflops of compute per second.

564
00:22:37,830 --> 00:22:39,710
That's 24 times 10 to the 18.

565
00:22:39,710 --> 00:22:41,070
That's a lot of flops.

566
00:22:41,070 --> 00:22:42,745
It's a lot of flops,
but I guarantee you

567
00:22:42,745 --> 00:22:44,120
five years from
today it will not

568
00:22:44,120 --> 00:22:47,270
feel like a lot of flops,
which is the even crazier part.

569
00:22:47,270 --> 00:22:50,480
And our goal here is actually
to think of this entire block

570
00:22:50,480 --> 00:22:54,120
of 24,000 GPUs as one
giant supercomputer.

571
00:22:54,120 --> 00:22:56,120
And then the question
is, how can we

572
00:22:56,120 --> 00:22:58,190
train one neural
network for months

573
00:22:58,190 --> 00:23:00,700
at a time on this one
giant supercomputer?

574
00:23:00,700 --> 00:23:02,450
And train a really
gigantic neural network

575
00:23:02,450 --> 00:23:05,390
that's really powerful, that can
soak up tons and tons of data.

576
00:23:05,390 --> 00:23:07,610
And that's basically the
question and the paradigm

577
00:23:07,610 --> 00:23:10,640
that we've moved to
in deep learning.

578
00:23:10,640 --> 00:23:12,955
And by the way, I
keep saying GPU,

579
00:23:12,955 --> 00:23:14,330
I keep saying
NVIDIA because they

580
00:23:14,330 --> 00:23:18,090
are the most dominant training
architecture and hardware today.

581
00:23:18,090 --> 00:23:20,240
But there are some others
that have sprung up.

582
00:23:20,240 --> 00:23:23,810
The biggest competitor, I think,
right now to NVIDIA training

583
00:23:23,810 --> 00:23:24,900
hardware is Google.

584
00:23:24,900 --> 00:23:26,810
Google has their
own hardware called

585
00:23:26,810 --> 00:23:28,800
Tensor Processing Units, TPUs.

586
00:23:28,800 --> 00:23:30,890
And these are really good.

587
00:23:30,890 --> 00:23:34,220
They've gone through six
generations of these already.

588
00:23:34,220 --> 00:23:37,130
These are the stats of
the v5p TPU, which you

589
00:23:37,130 --> 00:23:39,020
can rent in Google Cloud today.

590
00:23:39,020 --> 00:23:41,780
And it's roughly same
order of magnitude,

591
00:23:41,780 --> 00:23:44,150
similar specs as the H100
that we just talked about.

592
00:23:44,150 --> 00:23:46,400
There are some interesting
design decisions in the TPU

593
00:23:46,400 --> 00:23:48,350
that are quite different
from the GPUs, which

594
00:23:48,350 --> 00:23:50,308
I find fascinating, but
we just don't have time

595
00:23:50,308 --> 00:23:51,058
to get into today.

596
00:23:51,058 --> 00:23:53,100
And someone was asking,
how big are these things?

597
00:23:53,100 --> 00:23:54,500
This is an actual picture.

598
00:23:54,500 --> 00:23:57,920
Just like GPUs, these TPUs
are arranged into pods,

599
00:23:57,920 --> 00:24:05,030
and the v5p TPUs can be arranged
in pods of up to 8960 chips.

600
00:24:05,030 --> 00:24:07,670
And this is a picture,
actually, of a V2 TPU

601
00:24:07,670 --> 00:24:10,440
pod, which has only 256 chips.

602
00:24:10,440 --> 00:24:13,290
So then that gives you a sense
of how big these things are,

603
00:24:13,290 --> 00:24:14,220
each one of those.

604
00:24:14,220 --> 00:24:15,930
Then you see there's
four racks here.

605
00:24:15,930 --> 00:24:19,080
Those racks are, like I said,
maybe about a little bit taller

606
00:24:19,080 --> 00:24:19,830
than me.

607
00:24:19,830 --> 00:24:22,938
And there's four of them side
by side for 256 TPU chips.

608
00:24:22,938 --> 00:24:25,230
And now imagine this thing
is going to get a lot bigger

609
00:24:25,230 --> 00:24:29,015
in the more recent pods that
have up to almost 9,000 chips.

610
00:24:29,015 --> 00:24:31,140
Yes, so Google's Gemini
models are almost certainly

611
00:24:31,140 --> 00:24:32,898
trained on TPUs.

612
00:24:32,898 --> 00:24:34,440
Of course, they
don't tell you, but I

613
00:24:34,440 --> 00:24:38,520
would be astounded, absolutely
astounded if they were not.

614
00:24:38,520 --> 00:24:41,760
And like I said, the TPUs
are actually very good.

615
00:24:41,760 --> 00:24:43,620
I assume that most
large-scale Google models

616
00:24:43,620 --> 00:24:45,203
are trained on these
things, and those

617
00:24:45,203 --> 00:24:46,500
are very competitive models.

618
00:24:46,500 --> 00:24:48,812
So this is really good
training hardware.

619
00:24:48,812 --> 00:24:50,770
The difference with NVIDIA
is you can't buy it.

620
00:24:50,770 --> 00:24:53,580
The only way you can access TPUs
are either by working at Google

621
00:24:53,580 --> 00:24:55,408
or by renting them
on Google Cloud.

622
00:24:55,408 --> 00:24:57,450
But it is very good
hardware, and a lot of people

623
00:24:57,450 --> 00:24:58,950
are making use of
it, but I think

624
00:24:58,950 --> 00:25:03,600
it's still a little bit less
popular today than NVIDIA GPUs.

625
00:25:03,600 --> 00:25:05,310
And of course, other
companies obviously

626
00:25:05,310 --> 00:25:07,090
know that this is a
very important thing.

627
00:25:07,090 --> 00:25:08,160
So there's a lot
of other companies

628
00:25:08,160 --> 00:25:10,452
that are trying to build
competitive training hardware.

629
00:25:10,452 --> 00:25:12,570
But I think my honest
assessment right now

630
00:25:12,570 --> 00:25:16,715
is that probably NVIDIA and
TPUs are the two big ones.

631
00:25:16,715 --> 00:25:18,590
They're way ahead of
everyone else right now,

632
00:25:18,590 --> 00:25:21,640
today, in terms of usability,
performance, just like market

633
00:25:21,640 --> 00:25:22,160
share.

634
00:25:22,160 --> 00:25:23,577
But there are a
lot of others that

635
00:25:23,577 --> 00:25:25,040
are trying to catch up here.

636
00:25:25,040 --> 00:25:27,340
Two notable ones are AMD.

637
00:25:27,340 --> 00:25:29,680
AMD has been the
second major GPU

638
00:25:29,680 --> 00:25:31,340
manufacturer for many decades.

639
00:25:31,340 --> 00:25:35,530
They also have a training
accelerator called the MI325X.

640
00:25:35,530 --> 00:25:37,330
On paper, it actually
has really good stats

641
00:25:37,330 --> 00:25:39,170
that are pretty
comparable to an H100,

642
00:25:39,170 --> 00:25:43,840
but it just has not had the same
impact as the H100 right now.

643
00:25:43,840 --> 00:25:45,820
AWS also has their
own training chip

644
00:25:45,820 --> 00:25:48,070
that they've developed
called Trainium.

645
00:25:48,070 --> 00:25:49,723
I don't know too
much about this one.

646
00:25:49,723 --> 00:25:51,140
I've never tried
to use it myself,

647
00:25:51,140 --> 00:25:53,630
but I know that Anthropic uses
it for some of their training.

648
00:25:53,630 --> 00:25:55,880
I don't know to what extent
their training is entirely

649
00:25:55,880 --> 00:25:58,570
Trainium versus GPUs.

650
00:25:58,570 --> 00:25:59,960
So we should expect to see more.

651
00:25:59,960 --> 00:26:03,860
But today, I think NVIDIA GPUs
are probably the most dominant.

652
00:26:03,860 --> 00:26:05,427
And Google TPUs are right there.

653
00:26:05,427 --> 00:26:07,510
They're really good as
well but probably not quite

654
00:26:07,510 --> 00:26:10,340
as widely used as
GPUs from NVIDIA.

655
00:26:10,340 --> 00:26:10,840
OK.

656
00:26:10,840 --> 00:26:12,320
So that's basically part one.

657
00:26:12,320 --> 00:26:13,338
What are GPUs?

658
00:26:13,338 --> 00:26:14,880
How do we arrange
them into clusters?

659
00:26:14,880 --> 00:26:17,090
Just give you a sense of the
physicality of these machines

660
00:26:17,090 --> 00:26:18,740
that we're building
and training on.

661
00:26:18,740 --> 00:26:21,110
Then the second question
is, how do we actually

662
00:26:21,110 --> 00:26:25,190
write algorithms that can make
use of this giant GPU cluster

663
00:26:25,190 --> 00:26:27,032
with tens of thousands of GPUs?

664
00:26:27,032 --> 00:26:28,490
It's going to
require us to develop

665
00:26:28,490 --> 00:26:31,710
new algorithms, new ways of
thinking about our compute,

666
00:26:31,710 --> 00:26:33,800
and new ways of parallelizing
and splitting up

667
00:26:33,800 --> 00:26:34,920
our neural networks.

668
00:26:34,920 --> 00:26:36,710
So the basic strategy
here is going

669
00:26:36,710 --> 00:26:38,340
to be split up your computation.

670
00:26:38,340 --> 00:26:40,410
These things are giant
parallel devices.

671
00:26:40,410 --> 00:26:43,610
They have a lot of-- we saw they
have a lot of GPUs, a lot of CPU

672
00:26:43,610 --> 00:26:45,150
cores, a lot of GPU cores.

673
00:26:45,150 --> 00:26:46,760
They can all operate
independently,

674
00:26:46,760 --> 00:26:48,662
and they can't talk to
each other too much.

675
00:26:48,662 --> 00:26:50,870
If you think about what a
computer really does, like,

676
00:26:50,870 --> 00:26:53,190
at a high level, a computer
basically does two things.

677
00:26:53,190 --> 00:26:55,610
It does computation,
which is taking input bits

678
00:26:55,610 --> 00:26:57,665
and computing new
output bits from those,

679
00:26:57,665 --> 00:27:00,290
and it does communication, which
is taking bits and moving them

680
00:27:00,290 --> 00:27:01,850
from one bit of
memory in one place

681
00:27:01,850 --> 00:27:03,900
to some bit of memory
in some other place.

682
00:27:03,900 --> 00:27:05,450
And the whole
trick is, how do we

683
00:27:05,450 --> 00:27:09,590
make use of all of these
multiple scales of memory

684
00:27:09,590 --> 00:27:11,390
hierarchy across
the entire cluster

685
00:27:11,390 --> 00:27:14,870
to overlap the communication
with the computation?

686
00:27:14,870 --> 00:27:16,690
And also to split
up the computation

687
00:27:16,690 --> 00:27:20,260
and parallelize it so that
in the process of training

688
00:27:20,260 --> 00:27:22,840
a giant neural network, we
have useful work for all

689
00:27:22,840 --> 00:27:25,700
of those tens of thousands
of individual GPUs.

690
00:27:25,700 --> 00:27:28,100
All of those millions of
individual compute elements,

691
00:27:28,100 --> 00:27:30,605
we have useful work for all of
them to be doing in parallel,

692
00:27:30,605 --> 00:27:32,980
and then get them to communicate
their work to each other

693
00:27:32,980 --> 00:27:35,740
in a way that achieves
training a giant neural network

694
00:27:35,740 --> 00:27:37,720
on this giant cluster.

695
00:27:37,720 --> 00:27:40,330
So to that end, one way
I like to think about it

696
00:27:40,330 --> 00:27:43,185
is there's basically five
degrees of parallelism

697
00:27:43,185 --> 00:27:44,560
that people exploit
when training

698
00:27:44,560 --> 00:27:47,350
neural networks, the large-scale
neural networks today.

699
00:27:47,350 --> 00:27:49,100
A lot of this is
specific to transformers,

700
00:27:49,100 --> 00:27:51,558
because those are the dominant
architecture that people are

701
00:27:51,558 --> 00:27:53,000
using for large scale training.

702
00:27:53,000 --> 00:27:54,650
So if you think
about a transformer,

703
00:27:54,650 --> 00:27:58,310
a transformer is basically
a stack of L layers.

704
00:27:58,310 --> 00:28:00,190
And each one of
those L layers is

705
00:28:00,190 --> 00:28:01,870
operating on a
three-dimensional tensor

706
00:28:01,870 --> 00:28:05,613
of size, where one dimension
is the minibatch dimension.

707
00:28:05,613 --> 00:28:07,030
We've got a bunch
of sequences all

708
00:28:07,030 --> 00:28:09,610
operating in a minibatch,
a sequence dimension.

709
00:28:09,610 --> 00:28:12,140
We're operating on
sequences or sets of tokens,

710
00:28:12,140 --> 00:28:13,620
and a dim dimension.

711
00:28:13,620 --> 00:28:17,250
So each of those tokens itself
is a vector with some dimension.

712
00:28:17,250 --> 00:28:21,582
So our transformers
are operating

713
00:28:21,582 --> 00:28:23,040
on these
three-dimensional tensors.

714
00:28:23,040 --> 00:28:25,320
And they operate through
a stack of layers.

715
00:28:25,320 --> 00:28:28,130
So that gives us four
axes to parallelize on.

716
00:28:28,130 --> 00:28:30,540
We can parallelize
on the layers axis,

717
00:28:30,540 --> 00:28:32,150
which is pipeline parallelism.

718
00:28:32,150 --> 00:28:35,247
We can parallelize on
the batch dimension,

719
00:28:35,247 --> 00:28:36,330
which is data parallelism.

720
00:28:36,330 --> 00:28:37,970
We can split on the
sequence dimension, which

721
00:28:37,970 --> 00:28:39,360
is called context parallelism.

722
00:28:39,360 --> 00:28:41,330
And we can split on that
dim dimension, which

723
00:28:41,330 --> 00:28:42,870
is called tensor parallelism.

724
00:28:42,870 --> 00:28:44,587
So all of these
have funny names.

725
00:28:44,587 --> 00:28:46,170
But if you think
about it in this way,

726
00:28:46,170 --> 00:28:47,900
they're basically
all different ways

727
00:28:47,900 --> 00:28:50,240
of splitting up your
computation across these four

728
00:28:50,240 --> 00:28:52,160
axes of compute inside
your transformer.

729
00:28:52,160 --> 00:28:54,410
And then we're going to step
through each one of these

730
00:28:54,410 --> 00:28:56,810
in more detail, because there's
a lot of interesting nuances

731
00:28:56,810 --> 00:28:59,090
with all of these different
mechanisms of distributed

732
00:28:59,090 --> 00:29:00,680
training.

733
00:29:00,680 --> 00:29:03,920
So the first one is
Data Parallelism, or DP.

734
00:29:03,920 --> 00:29:06,888
And the basic idea
here is simple.

735
00:29:06,888 --> 00:29:08,930
So remember, when we're
training neural networks,

736
00:29:08,930 --> 00:29:12,300
we're always operating on
minibatches of samples.

737
00:29:12,300 --> 00:29:14,677
We're always taking a
minibatch of elements.

738
00:29:14,677 --> 00:29:17,010
We're computing a loss for
every entry in our minibatch,

739
00:29:17,010 --> 00:29:19,050
depending on whatever
our training task is.

740
00:29:19,050 --> 00:29:21,920
Then we compute a gradient,
where the gradient is actually

741
00:29:21,920 --> 00:29:23,912
typically an average
of the gradients

742
00:29:23,912 --> 00:29:26,370
of the losses for the individual
elements in the minibatch.

743
00:29:26,370 --> 00:29:28,650
So in most neural
network architectures,

744
00:29:28,650 --> 00:29:30,433
the computation of
computing the loss

745
00:29:30,433 --> 00:29:31,850
and then computing
the gradient is

746
00:29:31,850 --> 00:29:34,260
independent for each of the
elements in the minibatch.

747
00:29:34,260 --> 00:29:37,230
So this is something that
seems trivially parallelizable.

748
00:29:37,230 --> 00:29:40,100
So the basic idea
is that if you can

749
00:29:40,100 --> 00:29:43,160
fit a minibatch of N
examples on a single GPU

750
00:29:43,160 --> 00:29:45,530
and you have access
to M GPUs, then

751
00:29:45,530 --> 00:29:48,710
we're going to train our model
with a giant minibatch of M

752
00:29:48,710 --> 00:29:52,220
times N examples, where we
split up that giant minibatch

753
00:29:52,220 --> 00:29:55,160
into a little tiny, smaller
minibatch of N samples

754
00:29:55,160 --> 00:29:57,218
that goes on each GPU.

755
00:29:57,218 --> 00:29:58,760
And if you think
about mathematically

756
00:29:58,760 --> 00:30:01,530
why this makes sense, it's
because gradients are linear.

757
00:30:01,530 --> 00:30:04,940
So in practice, if you're
computing a single scalar

758
00:30:04,940 --> 00:30:07,100
loss L, which is going
to be the average

759
00:30:07,100 --> 00:30:10,355
of some individual losses
computed on each of our--

760
00:30:10,355 --> 00:30:14,510
so these xi,j are all the
entries across your entire macro

761
00:30:14,510 --> 00:30:15,980
batch I guess we'll call it.

762
00:30:15,980 --> 00:30:19,292
And then the W are the weight
matrices of the entire network.

763
00:30:19,292 --> 00:30:20,750
Then typically the
loss that you're

764
00:30:20,750 --> 00:30:22,820
computing at the end
of the forward pass

765
00:30:22,820 --> 00:30:24,770
is an average of
the losses on each

766
00:30:24,770 --> 00:30:26,395
of the individual
minibatch elements.

767
00:30:26,395 --> 00:30:27,770
And then if you
take the gradient

768
00:30:27,770 --> 00:30:30,497
of the loss with respect to
the weights of the network,

769
00:30:30,497 --> 00:30:32,330
that's the thing we
need to compute in order

770
00:30:32,330 --> 00:30:33,510
to make a weight update.

771
00:30:33,510 --> 00:30:36,020
Then that is actually
going to split.

772
00:30:36,020 --> 00:30:37,850
Because gradients
are linear, you

773
00:30:37,850 --> 00:30:40,060
get to choose in what order
do we want to do the sum?

774
00:30:40,060 --> 00:30:41,310
Do we want to do the gradient?

775
00:30:41,310 --> 00:30:42,602
Do we want to do the averaging?

776
00:30:42,602 --> 00:30:44,720
So in particular, it becomes
convenient to arrange

777
00:30:44,720 --> 00:30:46,820
the gradient in this
particular formulation

778
00:30:46,820 --> 00:30:49,340
where there's this inner
term that we've highlighted

779
00:30:49,340 --> 00:30:52,940
in blue, which is basically
a normal for backward

780
00:30:52,940 --> 00:30:54,600
pass on N elements.

781
00:30:54,600 --> 00:30:58,710
And these can be computed in
parallel on different GPUs.

782
00:30:58,710 --> 00:31:00,298
And then there's
an outer sum where

783
00:31:00,298 --> 00:31:02,090
we need to take an
average of the gradients

784
00:31:02,090 --> 00:31:07,630
across our M different devices
that we're operating on.

785
00:31:07,630 --> 00:31:10,988
So that's what's happening from
a mathematical perspective.

786
00:31:10,988 --> 00:31:13,280
And we see that this is
perfectly mathematically sound.

787
00:31:13,280 --> 00:31:15,490
This is basically exactly
the same mathematically

788
00:31:15,490 --> 00:31:18,020
as training on a single device.

789
00:31:18,020 --> 00:31:19,660
We've just been clever
with our algebra

790
00:31:19,660 --> 00:31:22,790
and changed the order of doing
our averages and our summations.

791
00:31:22,790 --> 00:31:24,230
But this is not
an approximation.

792
00:31:24,230 --> 00:31:25,730
This is exactly the
same computation

793
00:31:25,730 --> 00:31:28,300
as we would have done
on a single larger GPU.

794
00:31:28,300 --> 00:31:30,910
So what this looks like
at the GPU perspective

795
00:31:30,910 --> 00:31:33,680
is that we have M GPUs.

796
00:31:33,680 --> 00:31:36,250
Here I'm showing M equals
3 because that's all that

797
00:31:36,250 --> 00:31:37,820
can sensibly fit on the slide.

798
00:31:37,820 --> 00:31:40,640
But think that this is much
larger than 3 in practice.

799
00:31:40,640 --> 00:31:42,910
Then each one of those
GPUs actually maintains

800
00:31:42,910 --> 00:31:45,010
its own separate copy
of the neural network

801
00:31:45,010 --> 00:31:48,860
weights of the optimizer
state and of the gradients.

802
00:31:48,860 --> 00:31:50,740
So then what we're
going to do, each GPU

803
00:31:50,740 --> 00:31:53,830
will load in parallel a
different minibatch of data.

804
00:31:53,830 --> 00:31:56,230
Here we're showing
each GPU loading

805
00:31:56,230 --> 00:31:57,590
a minibatch of three elements.

806
00:31:57,590 --> 00:32:00,190
And crucially,
the different GPUs

807
00:32:00,190 --> 00:32:02,290
need to load different
minibatches of data.

808
00:32:02,290 --> 00:32:04,348
I've had bugs in my
code and students

809
00:32:04,348 --> 00:32:06,140
code, where they actually
accidentally load

810
00:32:06,140 --> 00:32:07,860
the same minibatch
on all the GPUs.

811
00:32:07,860 --> 00:32:08,640
That's not going to help you.

812
00:32:08,640 --> 00:32:09,807
That's not going to be good.

813
00:32:09,807 --> 00:32:11,150
Don't make that mistake.

814
00:32:11,150 --> 00:32:14,270
So it's crucially important that
your different GPUs actually

815
00:32:14,270 --> 00:32:16,580
load different
minibatches of data.

816
00:32:16,580 --> 00:32:20,000
Then each GPU will independently
do its own forward pass

817
00:32:20,000 --> 00:32:23,090
on its own minibatch of data
to compute its own local loss

818
00:32:23,090 --> 00:32:25,050
on its own local
minibatch of data.

819
00:32:25,050 --> 00:32:27,840
And these can all operate
totally independently.

820
00:32:27,840 --> 00:32:30,540
It does not require any
communication between GPUs.

821
00:32:30,540 --> 00:32:33,860
Then each network will
do its own backward pass

822
00:32:33,860 --> 00:32:36,290
to compute the gradient
of its own local loss

823
00:32:36,290 --> 00:32:38,520
with respect to all the
weights of the model.

824
00:32:38,520 --> 00:32:40,520
And again, this can happen
totally independently

825
00:32:40,520 --> 00:32:42,440
because each model,
remember, has its--

826
00:32:42,440 --> 00:32:45,450
each GPU has its own independent
copy of the model weights.

827
00:32:45,450 --> 00:32:47,510
It can do its own
forward backward pass

828
00:32:47,510 --> 00:32:49,550
completely independently.

829
00:32:49,550 --> 00:32:51,587
But now after the
backward pass is done,

830
00:32:51,587 --> 00:32:52,920
this is where things get tricky.

831
00:32:52,920 --> 00:32:54,830
Remember, we said
we needed to compute

832
00:32:54,830 --> 00:32:57,650
an average of those gradients
across all the devices that are

833
00:32:57,650 --> 00:32:59,130
participating in our training.

834
00:32:59,130 --> 00:33:00,960
So then we need communication.

835
00:33:00,960 --> 00:33:04,360
So this is where we do
an all reduce operation.

836
00:33:04,360 --> 00:33:09,580
And every GPU needs to send its
gradients to all the other GPUs.

837
00:33:09,580 --> 00:33:11,950
So there's two things
happening simultaneously.

838
00:33:11,950 --> 00:33:15,910
One, each GPU needs to broadcast
its gradients to all the GPUs.

839
00:33:15,910 --> 00:33:18,900
And then two, each GPU needs
to collect the gradients

840
00:33:18,900 --> 00:33:22,050
from all the GPUs that are
participating in the training.

841
00:33:22,050 --> 00:33:24,130
So this is an
allreduce operation.

842
00:33:24,130 --> 00:33:28,590
And this happens in logarithmic
time typically depending

843
00:33:28,590 --> 00:33:30,490
on in the number of GPUs.

844
00:33:30,490 --> 00:33:32,430
But at the end of this
allreduce operation,

845
00:33:32,430 --> 00:33:37,650
then each GPU now has an
average of all the gradients

846
00:33:37,650 --> 00:33:39,040
across all the devices.

847
00:33:39,040 --> 00:33:41,290
So at this point, the
communication has happened.

848
00:33:41,290 --> 00:33:43,590
Each GPU now has
an identical copy

849
00:33:43,590 --> 00:33:45,420
of the gradients that
have been all reduced

850
00:33:45,420 --> 00:33:46,630
across all the devices.

851
00:33:46,630 --> 00:33:49,030
So now at the beginning
of the training iteration,

852
00:33:49,030 --> 00:33:51,780
we assumed that each GPU
had its own independent copy

853
00:33:51,780 --> 00:33:52,660
of the model weights.

854
00:33:52,660 --> 00:33:55,410
Now, at this point, each GPU
has its own independent but

855
00:33:55,410 --> 00:33:58,980
identical copy of the gradients
across the entire macro

856
00:33:58,980 --> 00:33:59,920
batch of data.

857
00:33:59,920 --> 00:34:03,620
So now at this point, each
GPU can make a weight update

858
00:34:03,620 --> 00:34:05,370
on its own local
copy of the weights.

859
00:34:05,370 --> 00:34:07,781
And because they started
with the same weights

860
00:34:07,781 --> 00:34:09,239
and they applied
the same gradient,

861
00:34:09,239 --> 00:34:11,781
they're going to have the same
weights after the local weight

862
00:34:11,781 --> 00:34:15,620
update, assuming the
arithmetic was deterministic.

863
00:34:15,620 --> 00:34:19,340
And also, by the way, then
this is really important.

864
00:34:19,340 --> 00:34:22,110
Steps 4 and 5 can actually
happen in parallel.

865
00:34:22,110 --> 00:34:24,650
We said that there's
actually two things here

866
00:34:24,650 --> 00:34:25,920
that can happen in parallel.

867
00:34:25,920 --> 00:34:28,790
One is the backward pass
where each GPU computes

868
00:34:28,790 --> 00:34:30,804
its own backward pass
to compute gradients,

869
00:34:30,804 --> 00:34:32,929
and the other is the
communication of the gradients

870
00:34:32,929 --> 00:34:34,048
across the GPUs.

871
00:34:34,048 --> 00:34:35,840
And these things in
practice will typically

872
00:34:35,840 --> 00:34:37,260
happen simultaneously.

873
00:34:37,260 --> 00:34:39,590
So that means that each
model will start off

874
00:34:39,590 --> 00:34:42,080
doing backward pass over the
last layer in the network

875
00:34:42,080 --> 00:34:43,889
and then compute its
own local gradient.

876
00:34:43,889 --> 00:34:45,508
Now the model will
move its compute

877
00:34:45,508 --> 00:34:47,300
onto computing backward
pass for the second

878
00:34:47,300 --> 00:34:49,380
to last layer of the model.

879
00:34:49,380 --> 00:34:51,080
And while the
compute elements are

880
00:34:51,080 --> 00:34:54,179
busy computing the backward pass
on the second to last layer,

881
00:34:54,179 --> 00:34:56,600
then the GPUs will
simultaneously

882
00:34:56,600 --> 00:34:59,500
be doing an all reduce of the
gradients of the last layer.

883
00:34:59,500 --> 00:35:02,700
So this means that these things
chunk along communication

884
00:35:02,700 --> 00:35:05,520
for layer L plus 1 and
backward pass for layer L.

885
00:35:05,520 --> 00:35:07,978
And they can just chunk along
in parallel so that hopefully

886
00:35:07,978 --> 00:35:10,062
by the time we've gotten
to the end of the network

887
00:35:10,062 --> 00:35:11,950
and by the time the
backward pass is done,

888
00:35:11,950 --> 00:35:13,980
the gradients have
already been all reduced

889
00:35:13,980 --> 00:35:15,340
across all the devices.

890
00:35:15,340 --> 00:35:17,520
And we can make our
weight update all at once

891
00:35:17,520 --> 00:35:18,600
without waiting.

892
00:35:18,600 --> 00:35:21,150
This is really important
because like we said,

893
00:35:21,150 --> 00:35:22,853
the communication
is relatively slow.

894
00:35:22,853 --> 00:35:24,270
So the whole trick
in these things

895
00:35:24,270 --> 00:35:26,940
is figuring out ways to
hide the communication costs

896
00:35:26,940 --> 00:35:29,040
and do them at the same
time as the compute.

897
00:35:29,040 --> 00:35:31,540
The question is, is four or
five going to be the bottleneck?

898
00:35:31,540 --> 00:35:32,640
And the answer is yes.

899
00:35:32,640 --> 00:35:35,440
It depends entirely on
how fast your device is.

900
00:35:35,440 --> 00:35:36,610
How big is your model?

901
00:35:36,610 --> 00:35:37,780
How big is your minibatch?

902
00:35:37,780 --> 00:35:40,270
How fast is the interconnect
between the devices?

903
00:35:40,270 --> 00:35:42,520
When you get to this large-scale
distributed training,

904
00:35:42,520 --> 00:35:44,730
the answer is always, it
depends on your situation.

905
00:35:44,730 --> 00:35:47,220
And you need to benchmark
for your situation.

906
00:35:47,220 --> 00:35:49,720
Oh, why not take M different
gradient steps on each of them?

907
00:35:49,720 --> 00:35:51,210
That's actually a
really cool idea.

908
00:35:51,210 --> 00:35:54,330
There actually was a
popular set of algorithms

909
00:35:54,330 --> 00:35:57,767
that people used a while back
called asynchronous SGD where

910
00:35:57,767 --> 00:35:59,600
they would basically
do that, then basically

911
00:35:59,600 --> 00:36:01,260
have a bunch of
different model replicas,

912
00:36:01,260 --> 00:36:03,052
all take a bunch of
independent model steps

913
00:36:03,052 --> 00:36:05,420
and then try to average
them every once in a while.

914
00:36:05,420 --> 00:36:07,528
And those were popular.

915
00:36:07,528 --> 00:36:10,070
Actually Google used to do this
before they developed the TPU

916
00:36:10,070 --> 00:36:10,970
pods.

917
00:36:10,970 --> 00:36:13,610
And some of their earlier
networks in the early 2010s were

918
00:36:13,610 --> 00:36:14,930
trained in this way.

919
00:36:14,930 --> 00:36:17,880
But one, it tends to just
be a lot more unstable.

920
00:36:17,880 --> 00:36:20,790
And two, it's very hard
to debug and reproduce.

921
00:36:20,790 --> 00:36:23,640
And it just tends to
work a little bit worse.

922
00:36:23,640 --> 00:36:26,190
So it does feel like a
more scalable approach.

923
00:36:26,190 --> 00:36:28,890
But in practice, if you can
do everything synchronously,

924
00:36:28,890 --> 00:36:30,890
then your algorithms are
easier to debug, easier

925
00:36:30,890 --> 00:36:32,790
to understand, easier
to reason about.

926
00:36:32,790 --> 00:36:34,820
And basically, if
you can get away

927
00:36:34,820 --> 00:36:36,237
with synchronous
gradient updates,

928
00:36:36,237 --> 00:36:37,695
it's probably going
to work better.

929
00:36:37,695 --> 00:36:39,260
But actually, I
would personally not

930
00:36:39,260 --> 00:36:42,850
be too surprised if we see a
resurgence of async SGD methods

931
00:36:42,850 --> 00:36:44,600
in the next couple of
years at some point,

932
00:36:44,600 --> 00:36:47,060
because I think they are a lot
more friendly to distributed

933
00:36:47,060 --> 00:36:47,630
training.

934
00:36:47,630 --> 00:36:50,130
There's no one computer that
can orchestrate all this stuff.

935
00:36:50,130 --> 00:36:51,470
All these things are
independent devices

936
00:36:51,470 --> 00:36:52,845
with their own
independent stuff.

937
00:36:52,845 --> 00:36:56,440
There's no driver that
can take a God's eye view

938
00:36:56,440 --> 00:36:58,480
and take those steps.

939
00:36:58,480 --> 00:37:00,880
All that computation
has to happen somewhere.

940
00:37:00,880 --> 00:37:01,700
Oh, great question.

941
00:37:01,700 --> 00:37:04,220
I said, as you're overlapping
communication and compute,

942
00:37:04,220 --> 00:37:05,140
do you need to
write code for this,

943
00:37:05,140 --> 00:37:06,890
or does the hardware
do this automatically?

944
00:37:06,890 --> 00:37:08,270
You definitely got to
write code for this.

945
00:37:08,270 --> 00:37:09,700
The hardware is not smart
enough to understand

946
00:37:09,700 --> 00:37:10,837
what you want to do.

947
00:37:10,837 --> 00:37:13,420
The hardware, like we said, it
understands these little matrix

948
00:37:13,420 --> 00:37:14,300
multiply chunks.

949
00:37:14,300 --> 00:37:16,210
It understands pretty
low-level stuff.

950
00:37:16,210 --> 00:37:19,375
Anything that you want to do
to schedule that communication,

951
00:37:19,375 --> 00:37:21,230
you need to take
care of in software.

952
00:37:21,230 --> 00:37:24,110
But thankfully for a lot
of these common use cases,

953
00:37:24,110 --> 00:37:25,760
PyTorch ships with it for you.

954
00:37:25,760 --> 00:37:27,580
So for example, in
this case, there's

955
00:37:27,580 --> 00:37:29,890
a PyTorch class called
DistributedDataParallel

956
00:37:29,890 --> 00:37:33,190
that will do this for you and
make this happen relatively

957
00:37:33,190 --> 00:37:35,230
transparently on
top of otherwise

958
00:37:35,230 --> 00:37:37,747
straightforward PyTorch
code that you've written.

959
00:37:37,747 --> 00:37:39,580
Although actually that
is really interesting

960
00:37:39,580 --> 00:37:41,390
to contrast with the
individual devices,

961
00:37:41,390 --> 00:37:45,160
because if you're programming
an individual GPU in CUDA, which

962
00:37:45,160 --> 00:37:47,450
is NVIDIA's language
for programming GPUs,

963
00:37:47,450 --> 00:37:49,000
then actually the
hardware does take

964
00:37:49,000 --> 00:37:51,890
care of a lot of this async
transfer for you automatically.

965
00:37:51,890 --> 00:37:54,075
But at the cluster level,
it typically doesn't.

966
00:37:54,075 --> 00:37:55,950
Then you typically need
to do it in software.

967
00:37:55,950 --> 00:37:56,960
So there actually
is a little bit

968
00:37:56,960 --> 00:37:59,335
of interesting asymmetry
here between parallelism

969
00:37:59,335 --> 00:38:01,460
on the individual device
level, where a lot of that

970
00:38:01,460 --> 00:38:04,100
does happen automatically in
hardware versus at the cluster

971
00:38:04,100 --> 00:38:06,590
level where it needs to be
orchestrated in software.

972
00:38:06,590 --> 00:38:09,062
Yeah, so typically these
are heterogeneous systems

973
00:38:09,062 --> 00:38:10,520
where different
parts of the system

974
00:38:10,520 --> 00:38:12,150
are written in different
programming languages.

975
00:38:12,150 --> 00:38:13,730
So there's going to
be low-level device

976
00:38:13,730 --> 00:38:15,355
kernels that actually
are the code that

977
00:38:15,355 --> 00:38:16,873
executes inside the GPU.

978
00:38:16,873 --> 00:38:18,540
And those are typically
written in CUDA,

979
00:38:18,540 --> 00:38:21,020
which is it's a
C-like language that

980
00:38:21,020 --> 00:38:23,990
is NVIDIA's language for
programming their own GPUs.

981
00:38:23,990 --> 00:38:26,970
But then those individual GPU
kernels will get wrapped up.

982
00:38:26,970 --> 00:38:29,310
And you can call those
GPU kernels from Python.

983
00:38:29,310 --> 00:38:31,200
And this is basically
how PyTorch works.

984
00:38:31,200 --> 00:38:34,910
PyTorch is like a collection of
a lot of GPU kernels that can do

985
00:38:34,910 --> 00:38:36,660
lots of interesting
stuff on the GPU,

986
00:38:36,660 --> 00:38:39,920
and then a lot of C++ and Python
code that wraps around those GPU

987
00:38:39,920 --> 00:38:42,560
kernels and makes it more
user-friendly to program.

988
00:38:42,560 --> 00:38:45,680
So in this picture, each GPU
is computing its own gradients

989
00:38:45,680 --> 00:38:48,205
in black by itself.

990
00:38:48,205 --> 00:38:49,580
And then the
gradients in red get

991
00:38:49,580 --> 00:38:53,520
computed via an allreduce
across all the GPUs in parallel.

992
00:38:53,520 --> 00:38:55,200
Oh, the backward pass
at the lower layer

993
00:38:55,200 --> 00:38:57,450
is dependent on the gradients
from the previous layer.

994
00:38:57,450 --> 00:39:02,160
But crucially, each GPU is only
doing backward pass locally

995
00:39:02,160 --> 00:39:03,250
on its own minibatch.

996
00:39:03,250 --> 00:39:05,250
So then there's basically
two different variants

997
00:39:05,250 --> 00:39:06,540
of the gradient at
each layer that you

998
00:39:06,540 --> 00:39:07,560
need to think about now.

999
00:39:07,560 --> 00:39:09,990
There's the local gradients
about the gradient

1000
00:39:09,990 --> 00:39:12,240
of the local loss of my
minibatch with respect

1001
00:39:12,240 --> 00:39:13,207
to my network weights.

1002
00:39:13,207 --> 00:39:14,790
And there's the
global gradient, which

1003
00:39:14,790 --> 00:39:16,785
is the derivative
of the total loss

1004
00:39:16,785 --> 00:39:20,790
of the macrobatch with respect
to the network weights.

1005
00:39:20,790 --> 00:39:22,300
In order to compute
a backward pass,

1006
00:39:22,300 --> 00:39:24,030
each GPU only needs
the local version

1007
00:39:24,030 --> 00:39:25,120
of its upstream gradient.

1008
00:39:25,120 --> 00:39:27,930
But then computing the global
version of the upstream gradient

1009
00:39:27,930 --> 00:39:31,530
requires communication.

1010
00:39:31,530 --> 00:39:33,280
So this is data parallelism.

1011
00:39:33,280 --> 00:39:35,100
And there's actually
a bit of problem

1012
00:39:35,100 --> 00:39:36,600
here, which is,
this is a great way

1013
00:39:36,600 --> 00:39:38,107
to parallelize GPU computation.

1014
00:39:38,107 --> 00:39:39,690
And this was the
first way that people

1015
00:39:39,690 --> 00:39:41,982
started parallelizing GPU
computation in neural network

1016
00:39:41,982 --> 00:39:42,550
training.

1017
00:39:42,550 --> 00:39:46,000
But we quickly hit a
bottleneck on the model size.

1018
00:39:46,000 --> 00:39:48,660
So here, remember that
each GPU is keeping

1019
00:39:48,660 --> 00:39:50,880
its own independent copy
of the model parameters,

1020
00:39:50,880 --> 00:39:52,160
and this becomes a
bottleneck when you

1021
00:39:52,160 --> 00:39:53,490
want to have really big models.

1022
00:39:53,490 --> 00:39:56,257
So in particular, now each
weight in your neural network,

1023
00:39:56,257 --> 00:39:58,340
you basically need to keep
track of four numbers--

1024
00:39:58,340 --> 00:40:02,240
the weight itself, the
gradient of that weight,

1025
00:40:02,240 --> 00:40:03,453
and the optimizer state.

1026
00:40:03,453 --> 00:40:05,120
So if we're using
Adam, that's typically

1027
00:40:05,120 --> 00:40:07,760
a beta, that's a beta 1
and a beta 2 per parameter

1028
00:40:07,760 --> 00:40:08,560
in the network.

1029
00:40:08,560 --> 00:40:11,060
And sometimes you'll also have
an exponential moving average

1030
00:40:11,060 --> 00:40:12,600
of the model parameters as well.

1031
00:40:12,600 --> 00:40:15,162
So typically, you'll
have four to five scalars

1032
00:40:15,162 --> 00:40:17,120
that you need to keep
track of for every weight

1033
00:40:17,120 --> 00:40:18,320
in your network.

1034
00:40:18,320 --> 00:40:20,790
And if you're training
with 16-bit precision,

1035
00:40:20,790 --> 00:40:22,867
which is pretty
common these days,

1036
00:40:22,867 --> 00:40:25,200
some of these you'll sometimes
keep in higher precision.

1037
00:40:25,200 --> 00:40:27,150
But let's talk about
16-bit as a lower bound.

1038
00:40:27,150 --> 00:40:29,340
Then you need two
bytes for each number.

1039
00:40:29,340 --> 00:40:31,520
So that means that
we need four numbers,

1040
00:40:31,520 --> 00:40:34,670
2 bytes we need 8
bytes per scalar

1041
00:40:34,670 --> 00:40:37,350
in the network to keep track of.

1042
00:40:37,350 --> 00:40:40,910
Which means that 1
billion model parameters

1043
00:40:40,910 --> 00:40:43,310
is going to take about 8
gigabytes of GPU memory

1044
00:40:43,310 --> 00:40:44,730
to store all that stuff.

1045
00:40:44,730 --> 00:40:46,820
And we said the whole
GPU only has 80 gigabytes

1046
00:40:46,820 --> 00:40:48,150
of memory for an H100.

1047
00:40:48,150 --> 00:40:50,275
So that means that the
biggest model you could ever

1048
00:40:50,275 --> 00:40:52,090
hope to train in this
scenario is something

1049
00:40:52,090 --> 00:40:53,690
like 10 billion parameters.

1050
00:40:53,690 --> 00:40:54,830
And that's not big enough.

1051
00:40:54,830 --> 00:40:56,000
We want really big models.

1052
00:40:56,000 --> 00:40:58,667
We don't want to be constrained
by the tyranny of our GPU memory

1053
00:40:58,667 --> 00:41:01,490
size in telling us how big of
models we're allowed to train.

1054
00:41:01,490 --> 00:41:03,580
So we need to fix this somehow.

1055
00:41:03,580 --> 00:41:06,423
And the fix for this is
actually relatively easy.

1056
00:41:06,423 --> 00:41:07,840
We need to split
the model weights

1057
00:41:07,840 --> 00:41:09,410
across the different GPUs.

1058
00:41:09,410 --> 00:41:12,410
So in addition to splitting
the batch of data across GPUs,

1059
00:41:12,410 --> 00:41:15,560
we're also going to split our
model weights across the GPUs.

1060
00:41:15,560 --> 00:41:17,890
And this leads to a variant
of data parallelism called

1061
00:41:17,890 --> 00:41:20,950
Fully Sharded Data
Parallelism, or FSDP.

1062
00:41:20,950 --> 00:41:23,362
And this is relatively simple.

1063
00:41:23,362 --> 00:41:24,820
Conceptually what
we're going to do

1064
00:41:24,820 --> 00:41:27,610
is each model weight in
the network, each weight

1065
00:41:27,610 --> 00:41:31,460
Wi we're going to assign
it to a owner GPU.

1066
00:41:31,460 --> 00:41:35,020
So each weight will be owned by
a unique GPU among the M GPUs

1067
00:41:35,020 --> 00:41:36,040
that we're training on.

1068
00:41:36,040 --> 00:41:38,560
And that the GPU
that owns each weight

1069
00:41:38,560 --> 00:41:40,780
will also be
responsible for managing

1070
00:41:40,780 --> 00:41:42,350
the global gradients
for that weight

1071
00:41:42,350 --> 00:41:44,802
and the optimizer
state for that weight.

1072
00:41:44,802 --> 00:41:46,760
And typically you would
split this up by layer.

1073
00:41:46,760 --> 00:41:48,510
You're not managing
individual scalars.

1074
00:41:48,510 --> 00:41:50,873
This W you should think
of like the weight

1075
00:41:50,873 --> 00:41:52,915
matrix for an entire layer
of the neural network.

1077
00:41:55,820 --> 00:41:58,230
So now the picture on the
right changes a little bit.

1078
00:41:58,230 --> 00:42:00,647
Here we're only showing two
GPUs because, spoiler, there's

1079
00:42:00,647 --> 00:42:02,647
going to be a lot more
arrows flying around here

1080
00:42:02,647 --> 00:42:03,460
in just a moment.

1081
00:42:03,460 --> 00:42:05,210
So here we're showing
a four layer network

1082
00:42:05,210 --> 00:42:07,560
that we're distributing
across two different GPUs.

1083
00:42:07,560 --> 00:42:11,360
We've assigned the weights
for the first two network

1084
00:42:11,360 --> 00:42:12,840
layers W1 and W2.

1085
00:42:12,840 --> 00:42:14,690
Those are owned by GPU 1.

1086
00:42:14,690 --> 00:42:18,660
The weights W3 and W4 are
owned by GPU 3 or GPU 2.

1087
00:42:18,660 --> 00:42:21,780
So that means that at
the start of each batch,

1088
00:42:21,780 --> 00:42:25,080
the network weights are split
up across the GPUs in this way.

1089
00:42:25,080 --> 00:42:26,820
But it's still data parallelism.

1090
00:42:26,820 --> 00:42:28,968
It's still the same
basic idea that each GPU

1091
00:42:28,968 --> 00:42:31,260
is going to load its own
independent batch of elements,

1092
00:42:31,260 --> 00:42:33,470
do a full forward backward
pass on that batch

1093
00:42:33,470 --> 00:42:35,010
to compute its own
local gradients,

1094
00:42:35,010 --> 00:42:37,320
then all reduce the gradients
and take a gradient step.

1095
00:42:37,320 --> 00:42:39,320
Same basic algorithm,
but it gets tricky

1096
00:42:39,320 --> 00:42:41,130
now because the model
weights are split up.

1097
00:42:41,130 --> 00:42:43,565
So here we need to introduce
extra communication.

1098
00:42:43,565 --> 00:42:45,190
So when you're doing
fully sharded data

1099
00:42:45,190 --> 00:42:48,230
parallelism, now at the
beginning of forward,

1100
00:42:48,230 --> 00:42:51,580
before you start doing the
forward pass of the first layer,

1101
00:42:51,580 --> 00:42:54,100
whoever owns that weight
for the first layer

1102
00:42:54,100 --> 00:42:56,590
needs to broadcast that weight
matrix to all the other GPUs

1103
00:42:56,590 --> 00:42:57,590
that you're training on.

1104
00:42:57,590 --> 00:43:02,810
So in this case, GPU 1 owns W1,
so it broadcasts that to GPU 2.

1105
00:43:02,810 --> 00:43:05,240
So GPU 2 now has a copy of W1.

1106
00:43:05,240 --> 00:43:07,575
Now that all the GPUs
have a copy of W1,

1107
00:43:07,575 --> 00:43:09,700
they can run a forward pass
through the first layer

1108
00:43:09,700 --> 00:43:11,450
of the network and
compute the activations

1109
00:43:11,450 --> 00:43:13,120
at the first layer
of the network.

1110
00:43:13,120 --> 00:43:15,880
And now after you
run the forward pass,

1111
00:43:15,880 --> 00:43:18,580
each GPU that does
not own that W1

1112
00:43:18,580 --> 00:43:21,400
is going to delete its local
copy of the W1 weight matrix

1113
00:43:21,400 --> 00:43:22,430
to save memory.

1114
00:43:22,430 --> 00:43:25,592
So then after we run the forward
pass for the first layer,

1115
00:43:25,592 --> 00:43:27,550
we're back in the state
where the model weights

1116
00:43:27,550 --> 00:43:29,150
are split up across the GPUs.

1117
00:43:29,150 --> 00:43:33,050
But now all the GPUs also have
an activations in GPU memory

1118
00:43:33,050 --> 00:43:35,780
that is the result of running
the first layer of the network.

1119
00:43:35,780 --> 00:43:37,960
And now it's time to
do the second layer,

1120
00:43:37,960 --> 00:43:39,290
and we do the exact same thing.

1121
00:43:39,290 --> 00:43:42,730
So then the GPU that owns
the weight matrix for layer 2

1122
00:43:42,730 --> 00:43:44,622
is going to broadcast
that to all the GPUs

1123
00:43:44,622 --> 00:43:45,580
that we're training on.

1124
00:43:45,580 --> 00:43:48,820
Now they all have their
own local copy of W2,

1125
00:43:48,820 --> 00:43:50,710
and they can go forward.

1126
00:43:50,710 --> 00:43:52,630
And by the way, we also
have an opportunity

1127
00:43:52,630 --> 00:43:55,210
to interleave computation
and communication here

1128
00:43:55,210 --> 00:43:57,130
as well, so that
while we are computing

1129
00:43:57,130 --> 00:43:59,830
the forward pass for layer
i, we can be prefetching

1130
00:43:59,830 --> 00:44:01,220
the weights for the next layer.

1131
00:44:01,220 --> 00:44:03,430
So in practice, this
will happen in parallel

1132
00:44:03,430 --> 00:44:06,020
during the forward
pass of an FSDP run.

1133
00:44:06,020 --> 00:44:08,775
So then we'll be
computing layer 2.

1134
00:44:08,775 --> 00:44:10,150
At the same time,
we are fetching

1135
00:44:10,150 --> 00:44:11,590
the weights for layer 3.

1136
00:44:11,590 --> 00:44:15,800
And once we get to layer 3, note
that now GPU 1 owns layer 3.

1137
00:44:15,800 --> 00:44:18,130
So then GPU 1 will be
broadcasting the weights

1138
00:44:18,130 --> 00:44:19,850
to all the GPUs that
we're training on.

1139
00:44:19,850 --> 00:44:21,280
And this will repeat
until we've gotten

1140
00:44:21,280 --> 00:44:22,363
to the end of the network.

1141
00:44:22,363 --> 00:44:25,180
And now at the end of the
network, then all models,

1142
00:44:25,180 --> 00:44:27,500
now we know each model has
done a full forward pass,

1143
00:44:27,500 --> 00:44:29,893
computed its local loss
on its own local batch,

1144
00:44:29,893 --> 00:44:32,560
and has all the activations for
all the layers in memory already

1145
00:44:32,560 --> 00:44:33,347
for backward.

1146
00:44:33,347 --> 00:44:35,680
And now we need to do the
same thing in reverse in order

1147
00:44:35,680 --> 00:44:36,950
to compute the backward pass.

1148
00:44:36,950 --> 00:44:39,490
So at the beginning
of the backward pass

1149
00:44:39,490 --> 00:44:42,600
for the last layer, whoever
owns that last layer weight

1150
00:44:42,600 --> 00:44:44,770
will broadcast it
to all the devices.

1151
00:44:44,770 --> 00:44:46,720
Once the devices
have that weight,

1152
00:44:46,720 --> 00:44:49,000
then they can perform
the backward pass.

1153
00:44:49,000 --> 00:44:52,030
And this whole will do a similar
procedure in the backward pass.

1154
00:44:52,030 --> 00:44:53,970
Now there is a little
bit of optimization

1155
00:44:53,970 --> 00:44:56,580
we can do on the very last
layer in the network, which

1156
00:44:56,580 --> 00:45:00,510
is, have all the
GPUs keep the weights

1157
00:45:00,510 --> 00:45:01,870
for the last layer in memory.

1158
00:45:01,870 --> 00:45:04,203
So this is something that
you'll usually do in practice.

1159
00:45:04,203 --> 00:45:06,330
Because at the end,
all the GPUs already

1160
00:45:06,330 --> 00:45:08,830
have a copy of the last layer
weights from the forward pass.

1161
00:45:08,830 --> 00:45:09,940
They'll just keep it in memory.

1162
00:45:09,940 --> 00:45:11,340
And just because
they know they're

1163
00:45:11,340 --> 00:45:13,298
about to reuse it for
the backward pass anyway,

1164
00:45:13,298 --> 00:45:15,000
so we just won't
delete the weights

1165
00:45:15,000 --> 00:45:16,085
from the very last layer.

1167
00:45:18,247 --> 00:45:19,830
But now there's
basically three things

1168
00:45:19,830 --> 00:45:22,440
that need to happen
during the backward pass.

1169
00:45:22,440 --> 00:45:25,080
One is that once the
GPUs have computed

1170
00:45:25,080 --> 00:45:27,690
the backward pass for their
last layer of the network,

1171
00:45:27,690 --> 00:45:30,190
now that they have a copy of
the weights, now at that point,

1172
00:45:30,190 --> 00:45:34,380
each GPU has computed
its own local gradients

1173
00:45:34,380 --> 00:45:38,100
for its local loss with respect
to that last layer weights.

1174
00:45:38,100 --> 00:45:41,810
Then we need to communicate
those gradients back.

1175
00:45:41,810 --> 00:45:44,590
And we said that the GPU
that owns the weight matrix

1176
00:45:44,590 --> 00:45:47,200
is also going to be responsible
for managing the gradients

1177
00:45:47,200 --> 00:45:48,290
for that weight matrix.

1178
00:45:48,290 --> 00:45:50,830
So now, rather than all
reducing the gradients

1179
00:45:50,830 --> 00:45:52,790
as we did in the data
parallelism case,

1180
00:45:52,790 --> 00:45:53,890
instead we're going to--

1182
00:45:56,920 --> 00:46:00,100
just the one GPU that owns that
one last layer, weights, is

1183
00:46:00,100 --> 00:46:03,670
going to gather and take a sum
across all the local gradients

1184
00:46:03,670 --> 00:46:05,030
across all our devices.

1185
00:46:05,030 --> 00:46:08,020
So in this case, GPU 1 is
going to send its last layer

1186
00:46:08,020 --> 00:46:11,770
local gradient to GPU
2, which will then

1187
00:46:11,770 --> 00:46:16,720
have the full gradient dL/dW
4 of the entire macrobatch

1188
00:46:16,720 --> 00:46:18,925
with respect to the
last layer weights.

1189
00:46:18,925 --> 00:46:20,300
What happens during
the downtime?

1190
00:46:20,300 --> 00:46:23,748
You got to get all this
stuff happening in parallel.

1191
00:46:23,748 --> 00:46:25,540
So then there's basically
three things that

1192
00:46:25,540 --> 00:46:26,832
need to happen during backward.

1193
00:46:26,832 --> 00:46:29,480
During backward, we need
to communicate the weights.

1194
00:46:29,480 --> 00:46:33,940
So whatever GPU owns the layer,
owns the weights for that layer

1195
00:46:33,940 --> 00:46:35,210
has to broadcast them.

1196
00:46:35,210 --> 00:46:38,150
Two, we need all the GPUs
once they get that weight,

1197
00:46:38,150 --> 00:46:40,450
you need to compute a
backward pass for that layer.

1198
00:46:40,450 --> 00:46:44,500
And then three, after each GPU
computes its backward pass,

1199
00:46:44,500 --> 00:46:47,460
then it needs to send the result
of the gradients with respect

1200
00:46:47,460 --> 00:46:49,200
to the weights of that
backward pass back

1201
00:46:49,200 --> 00:46:51,670
to the GPU that owns it.

1202
00:46:51,670 --> 00:46:55,830
And then after that, then
once the owner of the weights

1203
00:46:55,830 --> 00:46:58,140
has that full
gradient, then only

1204
00:46:58,140 --> 00:47:00,720
the owner of the weight matrix
can now make a gradient update

1205
00:47:00,720 --> 00:47:02,820
on that one weight matrix.

1206
00:47:02,820 --> 00:47:04,710
But I think at this
point, we actually

1207
00:47:04,710 --> 00:47:08,220
do not need to communicate the
updated weight matrix because it

1208
00:47:08,220 --> 00:47:10,080
will get recommunicated
to all the GPUs

1209
00:47:10,080 --> 00:47:11,550
on the next forward pass.

1210
00:47:11,550 --> 00:47:15,930
So that's a little bit different
from the DP case, maybe.

1211
00:47:15,930 --> 00:47:17,790
And then basically
all of these things

1212
00:47:17,790 --> 00:47:19,720
can actually happen
in parallel as well.

1213
00:47:19,720 --> 00:47:22,270
So we'll repeat this for
every layer of the network.

1214
00:47:22,270 --> 00:47:25,150
And then basically in the steady
state of a very deep network,

1215
00:47:25,150 --> 00:47:28,245
all three of these things will
be happening simultaneously.

1216
00:47:28,245 --> 00:47:32,050
So while we are computing the
backward pass for layer L,

1217
00:47:32,050 --> 00:47:34,440
we will be aggregating
the gradients

1218
00:47:34,440 --> 00:47:38,230
and performing a weight
update on layer L plus 1.

1219
00:47:38,230 --> 00:47:42,085
And we will be prefetching the
weights for layer L minus 1.

1220
00:47:42,085 --> 00:47:44,210
So I said there's three
things that need to happen.

1221
00:47:44,210 --> 00:47:48,938
We need to get the weight,
run the backward pass,

1222
00:47:48,938 --> 00:47:51,230
and then aggregate the
gradient, and update the weight.

1223
00:47:51,230 --> 00:47:53,000
And these things can
all happen in parallel.

1224
00:47:53,000 --> 00:47:54,010
So basically in
general, we'll be

1225
00:47:54,010 --> 00:47:56,740
operating on three consecutive
layers and doing all three

1226
00:47:56,740 --> 00:47:59,460
of these things in parallel over
the course of the backward pass.

1228
00:48:03,880 --> 00:48:06,410
And then as we chunk
backwards over the network,

1229
00:48:06,410 --> 00:48:09,130
then by the time we-- then
hopefully if you were properly

1230
00:48:09,130 --> 00:48:11,960
able to overlap all that
communication and computation,

1231
00:48:11,960 --> 00:48:14,540
then by the time you
finish your backward pass,

1232
00:48:14,540 --> 00:48:16,790
then all the gradients have
already been communicated.

1233
00:48:16,790 --> 00:48:18,280
All the GPUs have
already finished

1234
00:48:18,280 --> 00:48:20,690
doing their update on all
the weights, and we're ready.

1235
00:48:20,690 --> 00:48:23,410
And also hopefully your Data
Loader that's loading data

1236
00:48:23,410 --> 00:48:25,540
is also happening
asynchronously, usually

1237
00:48:25,540 --> 00:48:27,320
on the CPU cores of our servers.

1238
00:48:27,320 --> 00:48:29,980
So then the CPU is ready
with a fresh batch of data

1239
00:48:29,980 --> 00:48:31,070
to go forward again.

1240
00:48:31,070 --> 00:48:33,405
So these things are basically
parallelization machines.

1241
00:48:33,405 --> 00:48:34,780
We have a lot of
stuff that needs

1242
00:48:34,780 --> 00:48:37,928
to happen both within
a GPU and across GPUs,

1243
00:48:37,928 --> 00:48:40,220
and we need to overlap all
of that as much as possible.

1244
00:48:40,220 --> 00:48:41,920
So we can always feed
the GPUs and have

1245
00:48:41,920 --> 00:48:44,460
them running on those tensor
cores as densely as possible.

1247
00:48:47,305 --> 00:48:51,550
So then we're basically ready
to do our next batch after that.

1248
00:48:51,550 --> 00:48:52,700
So this is great.

1249
00:48:52,700 --> 00:48:54,440
This is Fully Sharded
Data Parallelism.

1250
00:48:54,440 --> 00:48:56,420
And this can get you a long way.

1251
00:48:56,420 --> 00:48:59,322
But there's actually a
slightly fancier variant

1252
00:48:59,322 --> 00:49:01,030
of data parallelism
that people sometimes

1253
00:49:01,030 --> 00:49:05,650
use called Hybrid Sharded
Data Parallelism, or HSDP.

1254
00:49:05,650 --> 00:49:07,270
And in this case,
we're actually going

1255
00:49:07,270 --> 00:49:09,700
to imagine conceptually
dividing our GPUs

1256
00:49:09,700 --> 00:49:11,210
into a two-dimensional grid.

1257
00:49:11,210 --> 00:49:14,690
So in the previous examples,
we said we had N GPUs.

1258
00:49:14,690 --> 00:49:18,170
And the way that we parallelized
our computation was the same.

1259
00:49:18,170 --> 00:49:20,890
We had one axis
of parallelization

1260
00:49:20,890 --> 00:49:23,090
in the previous variants
of data parallelism.

1261
00:49:23,090 --> 00:49:25,280
Once we get to Hybrid
Sharded Data Parallelism,

1262
00:49:25,280 --> 00:49:28,090
we now are going to have two
separate axes of parallelism

1263
00:49:28,090 --> 00:49:29,870
that we will do
at the same time.

1264
00:49:29,870 --> 00:49:34,090
So the first axis is we will do
typical FSDP, Fully Sharded Data

1265
00:49:34,090 --> 00:49:36,700
Parallelism, along one axis
that we just talked about.

1266
00:49:36,700 --> 00:49:40,000
So we'll have groups of K GPUs.

1267
00:49:40,000 --> 00:49:43,587
And each group of K GPUs will
be doing Fully Sharded Data

1268
00:49:43,587 --> 00:49:45,170
Parallelism that we
just talked about.

1269
00:49:45,170 --> 00:49:47,920
So within each group of
K GPUs, the model weights

1270
00:49:47,920 --> 00:49:50,010
will be split
across those K GPUs.

1271
00:49:50,010 --> 00:49:52,510
And they will be interleaving,
sending weights and gradients

1272
00:49:52,510 --> 00:49:54,040
back and forth to each
other during the forward

1273
00:49:54,040 --> 00:49:55,250
and backward passes.

1274
00:49:55,250 --> 00:50:00,490
But we will have now M
copies of those K groups

1275
00:50:00,490 --> 00:50:01,700
operating in parallel.

1276
00:50:01,700 --> 00:50:05,240
So in this case, we have
two groups of four GPUs.

1277
00:50:05,240 --> 00:50:07,510
So each group of
four GPUs you see

1278
00:50:07,510 --> 00:50:10,820
has the weights split
across the four GPUs.

1279
00:50:10,820 --> 00:50:15,550
But we have the entire setup
duplicated a second time

1280
00:50:15,550 --> 00:50:19,060
on a second group of two GPUs.

1281
00:50:19,060 --> 00:50:21,610
And then when you do
this, now then they

1282
00:50:21,610 --> 00:50:23,960
do typical data parallelism
across the groups.

1283
00:50:23,960 --> 00:50:26,680
So within a group, we're
going to do forward/backward.

1284
00:50:26,680 --> 00:50:28,690
And at the end of the
backward, each group

1285
00:50:28,690 --> 00:50:31,320
will have computed its
own local gradients.

1286
00:50:31,320 --> 00:50:33,540
And then after the
backward, then each group

1287
00:50:33,540 --> 00:50:36,570
needs to all reduce the
gradients across the groups

1288
00:50:36,570 --> 00:50:40,110
so that we now have the full
macro macrobatch gradients

1289
00:50:40,110 --> 00:50:41,290
across the two groups.

1290
00:50:41,290 --> 00:50:44,310
And then each group can make a
gradient update independently

1291
00:50:44,310 --> 00:50:46,590
once they've received the
full gradients for the macro

1292
00:50:46,590 --> 00:50:49,440
macrobatch.

1293
00:50:49,440 --> 00:50:51,570
So this are called
multi-dimensional parallelism,

1294
00:50:51,570 --> 00:50:53,873
because now there's basically
two different axes, two

1295
00:50:53,873 --> 00:50:56,040
different strategies that
we're using to parallelize

1296
00:50:56,040 --> 00:50:58,200
our computation simultaneously.

1297
00:50:58,200 --> 00:51:01,103
And why this might be
useful is because there's

1298
00:51:01,103 --> 00:51:02,520
different amounts
of communication

1299
00:51:02,520 --> 00:51:04,870
required for these two
different kinds of parallelism.

1300
00:51:04,870 --> 00:51:07,398
So if we think about
fully sharded parallelism,

1301
00:51:07,398 --> 00:51:08,940
we actually need
to-- what do we need

1302
00:51:08,940 --> 00:51:11,250
to communicate during fully
sharded data parallelism?

1303
00:51:11,250 --> 00:51:13,480
During FSDP, during
the forward pass,

1304
00:51:13,480 --> 00:51:16,433
remember we were copying
the weights all over.

1305
00:51:16,433 --> 00:51:17,850
During the forward
pass, we end up

1306
00:51:17,850 --> 00:51:20,058
doing a communication of
one full copy of the network

1307
00:51:20,058 --> 00:51:20,590
weights.

1308
00:51:20,590 --> 00:51:22,140
Then during the
backward pass, we

1309
00:51:22,140 --> 00:51:24,103
need to re communicate
the network weights,

1310
00:51:24,103 --> 00:51:26,020
and we also need to
communicate the gradients.

1311
00:51:26,020 --> 00:51:29,960
So basically when you use
fully sharded data parallelism

1312
00:51:29,960 --> 00:51:31,700
during a single
forward backward pass,

1313
00:51:31,700 --> 00:51:33,250
you need to
communicate three times

1314
00:51:33,250 --> 00:51:35,410
the network weights across
everything participating

1315
00:51:35,410 --> 00:51:36,650
in an FSDP group.

1316
00:51:36,650 --> 00:51:40,335
But when you do normal
data parallelism, where

1317
00:51:40,335 --> 00:51:42,710
each group keeps its own
independent copy of the weights,

1318
00:51:42,710 --> 00:51:44,460
you only need to all
reduce the gradients.

1319
00:51:44,460 --> 00:51:47,110
So that means that across
multiple data parallelism

1320
00:51:47,110 --> 00:51:49,120
groups, you only need to
communicate the network

1321
00:51:49,120 --> 00:51:51,560
weights once over a
forward backward pass.

1322
00:51:51,560 --> 00:51:54,520
And this plays into this idea
of multiple levels of hierarchy

1323
00:51:54,520 --> 00:51:56,060
inside of our GPU clusters.

1324
00:51:56,060 --> 00:51:57,890
So what you might
do, for example,

1325
00:51:57,890 --> 00:52:00,460
is have a GPU
server where it has

1326
00:52:00,460 --> 00:52:02,140
eight GPUs with
higher interconnect

1327
00:52:02,140 --> 00:52:03,440
inside a single machine.

1328
00:52:03,440 --> 00:52:05,470
Those might be an
FSDP group because it

1329
00:52:05,470 --> 00:52:07,943
requires more communication
inside an FSDP group.

1330
00:52:07,943 --> 00:52:09,610
But then you could
have multiple servers

1331
00:52:09,610 --> 00:52:11,870
that are on this other axis.

1332
00:52:11,870 --> 00:52:14,483
So you have one server with a
full copy of the model weights,

1333
00:52:14,483 --> 00:52:17,150
then another server with another
full copy of the model weights.

1334
00:52:17,150 --> 00:52:18,900
And remember, communication
across servers

1335
00:52:18,900 --> 00:52:21,380
is going to be slower than
communication inside a server.

1336
00:52:21,380 --> 00:52:26,260
So then this is our first
example of designing algorithms

1337
00:52:26,260 --> 00:52:28,370
to take advantage of
the network topology

1338
00:52:28,370 --> 00:52:30,680
that we know our devices
are connected into.

1339
00:52:30,680 --> 00:52:33,335
The question is, would
you rather have--

1340
00:52:33,335 --> 00:52:35,188
these things are
impossible to tune.

1341
00:52:35,188 --> 00:52:36,355
It's very, very hard to say.

1343
00:52:40,502 --> 00:52:43,080
And then basically, but once
you have data parallelism,

1344
00:52:43,080 --> 00:52:45,720
once you have this
DP, FSDP, and HSDP,

1345
00:52:45,720 --> 00:52:48,060
this is actually a recipe
that can take you a long ways.

1346
00:52:48,060 --> 00:52:51,860
So for example, a model
with 100 billion parameters

1347
00:52:51,860 --> 00:52:54,860
would take 800 GB
of memory to store.

1348
00:52:54,860 --> 00:52:56,970
And if you split
that over 80 GPUs,

1349
00:52:56,970 --> 00:52:59,070
it only takes 10 gigabytes
of memory per GPU.

1350
00:52:59,070 --> 00:53:03,560
So you can have a pretty big
model once you have FSDP.

1351
00:53:03,560 --> 00:53:06,320
But there's another problem that
the model activations themselves

1352
00:53:06,320 --> 00:53:07,590
now start to fill up memory.

1353
00:53:07,590 --> 00:53:11,510
So if we go back to Llama3-405B,
it's a transformer with 126

1354
00:53:11,510 --> 00:53:15,740
layers, model dimension of
16,000, sequence length 4,096.

1355
00:53:15,740 --> 00:53:18,050
So if you imagine
how much GPU memory

1356
00:53:18,050 --> 00:53:19,910
it takes to just store
the hidden states

1357
00:53:19,910 --> 00:53:22,560
during the forward pass,
that's going to be a lot.

1358
00:53:22,560 --> 00:53:24,577
So that's going to
quickly cause your GPU

1359
00:53:24,577 --> 00:53:27,160
to run out of memory once your
models and sequences get really

1360
00:53:27,160 --> 00:53:27,890
big.

1361
00:53:27,890 --> 00:53:29,650
So that leads to
another trick is

1362
00:53:29,650 --> 00:53:31,387
called activation
checkpointing, which

1363
00:53:31,387 --> 00:53:33,220
means that we're actually
not going to store

1364
00:53:33,220 --> 00:53:34,918
all the activations in memory.

1365
00:53:34,918 --> 00:53:37,210
We're going to recompute them
during the backward pass.

1366
00:53:37,210 --> 00:53:40,720
So to see how this works,
it's useful to think

1367
00:53:40,720 --> 00:53:42,470
of your neural network
in a different way,

1368
00:53:42,470 --> 00:53:44,905
where there's actually two
different layers in the neural.

1369
00:53:44,905 --> 00:53:47,300
Each layer in the neural
network does two things.

1370
00:53:47,300 --> 00:53:49,030
It does a forward
pass that computes

1371
00:53:49,030 --> 00:53:50,390
activations for the next layer.

1372
00:53:50,390 --> 00:53:52,098
Then it has a backward
pass that computes

1373
00:53:52,098 --> 00:53:54,940
gradients that take both
the upstream gradients

1374
00:53:54,940 --> 00:53:56,540
and the activations.

1375
00:53:56,540 --> 00:54:00,170
So normally, how much compute
and memory does this all take?

1376
00:54:00,170 --> 00:54:02,800
If we assume that all of these
are constant, then typically

1377
00:54:02,800 --> 00:54:07,195
a forward backward pass will
take 1, 2, 3, 4, four steps

1378
00:54:07,195 --> 00:54:07,820
during forward.

1379
00:54:07,820 --> 00:54:09,220
You'll remember
those activations

1380
00:54:09,220 --> 00:54:10,400
during the forward pass.

1381
00:54:10,400 --> 00:54:13,550
Then 1, 2, 3, 4 steps
during backward.

1382
00:54:13,550 --> 00:54:15,620
So in a normal
forward backward pass,

1383
00:54:15,620 --> 00:54:20,367
it takes O(N) compute and O(N)
memory for an N layer network.

1384
00:54:20,367 --> 00:54:22,700
But as we just said, this is
going to run out of memory.

1385
00:54:22,700 --> 00:54:24,500
So instead what we
can do is imagine

1386
00:54:24,500 --> 00:54:27,000
recomputing the activations
during the backward pass.

1387
00:54:27,000 --> 00:54:29,220
So what that looks like
is something like this.

1388
00:54:29,220 --> 00:54:30,750
So we'll start with
the activations.

1389
00:54:30,750 --> 00:54:33,125
We'll run the first layer and
then immediately throw away

1390
00:54:33,125 --> 00:54:34,742
the activations for the first--

1391
00:54:34,742 --> 00:54:36,450
run the forward pass
for the first layer.

1392
00:54:36,450 --> 00:54:38,492
And then immediately throw
away those activations

1393
00:54:38,492 --> 00:54:39,930
and so do this four times.

1394
00:54:39,930 --> 00:54:41,820
So now we've gone
through the network once,

1395
00:54:41,820 --> 00:54:43,590
got the activations
at the last layer.

1396
00:54:43,590 --> 00:54:46,290
At this point, we can compute
our backward for the last layer,

1397
00:54:46,290 --> 00:54:48,210
but now we're kind out of luck.

1398
00:54:48,210 --> 00:54:50,450
We don't have the
activations from A3

1399
00:54:50,450 --> 00:54:53,390
to compute the next backward
pass, but we can recompute them.

1400
00:54:53,390 --> 00:54:54,947
So we recompute them.

1401
00:54:54,947 --> 00:54:56,280
Now we can do the backward pass.

1402
00:54:56,280 --> 00:54:57,630
Now recompute some more.

1403
00:54:57,630 --> 00:54:58,800
Now do the backward pass.

1404
00:54:58,800 --> 00:54:59,610
Now recompute.

1405
00:54:59,610 --> 00:55:01,050
Now do another backward pass.

1406
00:55:01,050 --> 00:55:03,710
So if you add this
all up, this ends up

1407
00:55:03,710 --> 00:55:07,160
being N squared compute and
constant memory for a network

1408
00:55:07,160 --> 00:55:11,030
with N layers, because it's
sum N, N minus 1, N minus 2,

1409
00:55:11,030 --> 00:55:12,780
N minus 3, blah, blah,
blah down to one.

1410
00:55:12,780 --> 00:55:14,450
That's quadratic time.

1411
00:55:14,450 --> 00:55:15,840
And you can split this up.

1412
00:55:15,840 --> 00:55:18,510
N squared compute is pretty
bad for deep networks.

1413
00:55:18,510 --> 00:55:20,880
So instead, let's not
recompute everything.

1414
00:55:20,880 --> 00:55:23,560
Let's instead imagine taking
a checkpoint of activations

1415
00:55:23,560 --> 00:55:24,680
every C layers.

1416
00:55:24,680 --> 00:55:30,290
So we'll only recompute within
tinier blocks of the network.

1417
00:55:30,290 --> 00:55:33,010
Then in that case, if
you take C checkpoints

1418
00:55:33,010 --> 00:55:35,080
where you remember
your activations C

1419
00:55:35,080 --> 00:55:36,760
times over the course
of your network,

1420
00:55:36,760 --> 00:55:39,010
then it's going to take N
squared over C compute and O

1421
00:55:39,010 --> 00:55:40,143
of C memory.

1422
00:55:40,143 --> 00:55:41,560
And a pretty common
thing to do is

1423
00:55:41,560 --> 00:55:44,440
to set C equal to
root N, in which case

1424
00:55:44,440 --> 00:55:47,915
this becomes N root N compute
and O of root N memory.

1425
00:55:47,915 --> 00:55:49,540
So this is a pretty
common way that you

1426
00:55:49,540 --> 00:55:51,580
can trade-off
computation and memory

1427
00:55:51,580 --> 00:55:53,795
to train even bigger models.

1428
00:55:53,795 --> 00:55:54,295
OK.

1429
00:55:54,295 --> 00:55:55,960
So now at this
point, once we have

1430
00:55:55,960 --> 00:56:00,540
FSDP, activation checkpointing,
HSDP, we can do a lot of damage

1431
00:56:00,540 --> 00:56:01,040
here.

1432
00:56:01,040 --> 00:56:03,130
We can start to train
some really big models.

1433
00:56:03,130 --> 00:56:05,740
And the recipe for that
is basically as following.

1434
00:56:05,740 --> 00:56:07,930
So your scaling recipe
that will take you

1435
00:56:07,930 --> 00:56:11,570
quite a long way from here is
first, use data parallelism,

1436
00:56:11,570 --> 00:56:15,430
just raw data parallelism,
roughly up to 128 GPUs

1437
00:56:15,430 --> 00:56:17,900
and roughly up to models of
around a billion parameters.

1438
00:56:17,900 --> 00:56:19,750
You can just do normal
data parallelism

1439
00:56:19,750 --> 00:56:20,810
for models of this size.

1440
00:56:20,810 --> 00:56:22,700
It tends to work pretty well.

1441
00:56:22,700 --> 00:56:24,650
And another thing
that you almost always

1442
00:56:24,650 --> 00:56:27,470
want to set the local
batch size per GPU

1443
00:56:27,470 --> 00:56:28,770
to max out the GPU memory.

1444
00:56:28,770 --> 00:56:31,250
That's almost always
the right thing to do.

1445
00:56:31,250 --> 00:56:33,180
And then once your
model starts to get big,

1446
00:56:33,180 --> 00:56:34,850
then the model
itself will take up

1447
00:56:34,850 --> 00:56:36,750
a lot of memory inside your GPU.

1448
00:56:36,750 --> 00:56:38,970
So that will start
to give you problems.

1449
00:56:38,970 --> 00:56:42,750
So it depends on how
much memory your GPU has,

1450
00:56:42,750 --> 00:56:44,280
how fast your interconnects are.

1451
00:56:44,280 --> 00:56:46,100
But in general, once
your model starts

1452
00:56:46,100 --> 00:56:47,760
to be more than a
billion parameters,

1453
00:56:47,760 --> 00:56:49,160
that's when you want
to start thinking

1454
00:56:49,160 --> 00:56:50,720
about switching from
data parallelism

1455
00:56:50,720 --> 00:56:53,210
to fully sharded
data parallelism.

1456
00:56:53,210 --> 00:56:55,463
And then at this point, you
can scale up quite a bit,

1457
00:56:55,463 --> 00:56:57,380
but then you'll run into
the memory bottleneck

1458
00:56:57,380 --> 00:56:58,380
for your activations.

1459
00:56:58,380 --> 00:57:00,360
And that's when you turn on
activation check pointing.

1460
00:57:00,360 --> 00:57:02,180
Activation check
pointing sucks because it

1461
00:57:02,180 --> 00:57:04,160
makes everything a lot
slower, but it does let

1462
00:57:04,160 --> 00:57:07,640
you train much bigger models.

1463
00:57:07,640 --> 00:57:10,523
And this will scale up
to several hundred GPUs.

1464
00:57:10,523 --> 00:57:12,440
And then there's some
point, usually depending

1465
00:57:12,440 --> 00:57:15,530
on your cluster topology,
maybe around 256 GPUs, maybe

1466
00:57:15,530 --> 00:57:18,440
around 512 GPUs, once
you get to on the order

1467
00:57:18,440 --> 00:57:22,880
of multiple hundreds of devices,
then FSDP becomes too expensive.

1468
00:57:22,880 --> 00:57:25,580
And you need to start
switching to HSDP.

1469
00:57:25,580 --> 00:57:27,860
And then this is basically
going to let you get up

1470
00:57:27,860 --> 00:57:31,140
to models that are roughly
tens of billions of parameters,

1471
00:57:31,140 --> 00:57:34,190
training on maybe
a thousand GPUs

1472
00:57:34,190 --> 00:57:36,060
on pretty long sequence lengths.

1473
00:57:36,060 --> 00:57:37,490
So that's pretty good.

1474
00:57:37,490 --> 00:57:40,530
But if you have maybe
more than thousand GPUs,

1475
00:57:40,530 --> 00:57:42,390
more than 50 billion
parameter models,

1476
00:57:42,390 --> 00:57:46,020
sequence lengths more than
10,000 or so, this is when you

1477
00:57:46,020 --> 00:57:48,020
need to turn to these
more advanced strategies--

1478
00:57:48,020 --> 00:57:49,940
context parallelism,
pipeline parallelism,

1479
00:57:49,940 --> 00:57:51,707
or tensor parallelism.

1480
00:57:51,707 --> 00:57:53,040
And then there's a big question.

1481
00:57:53,040 --> 00:57:53,700
It's like, oh, my God.

1482
00:57:53,700 --> 00:57:55,110
There's a lot of
knobs to tune here.

1483
00:57:55,110 --> 00:57:56,568
How am I supposed
to optimize this?

1484
00:57:56,568 --> 00:57:59,120
I need to set the global
batch size, the local batch

1485
00:57:59,120 --> 00:58:01,860
size, the HSDP dimension,
the FSDP dimension,

1486
00:58:01,860 --> 00:58:03,500
how much to recompute.

1487
00:58:03,500 --> 00:58:04,168
I'm lost here.

1488
00:58:04,168 --> 00:58:04,710
What do I do?

1489
00:58:04,710 --> 00:58:07,880
There's so many knobs, I
don't know what do I do?

1490
00:58:07,880 --> 00:58:10,340
The answer is to optimize
a very important metric

1491
00:58:10,340 --> 00:58:12,320
called Model Flops
Utilization, MFU.

1492
00:58:12,320 --> 00:58:15,680
Whenever you get lost in
the sea of GPU parallelism,

1493
00:58:15,680 --> 00:58:17,630
Model Flops Utilization
is your guiding light.

1494
00:58:17,630 --> 00:58:19,047
Follow this, it
will tell you what

1495
00:58:19,047 --> 00:58:21,740
to do to optimize
your training stack.

1496
00:58:21,740 --> 00:58:23,820
But before we get to
Model Flops Utilization,

1497
00:58:23,820 --> 00:58:25,980
we need to talk about
Hardware Flops Utilization.

1498
00:58:25,980 --> 00:58:28,580
So remember we said
in theory, an H100 can

1499
00:58:28,580 --> 00:58:34,142
do 989.4 TFLOPs per second of
compute on the tensor cores.

1500
00:58:34,142 --> 00:58:35,100
But that's theoretical.

1501
00:58:35,100 --> 00:58:36,560
How much can you actually get?

1502
00:58:36,560 --> 00:58:38,390
The question is, how
much can you actually

1503
00:58:38,390 --> 00:58:40,190
achieve in practice?

1504
00:58:40,190 --> 00:58:42,450
And that's the metric of
Hardware Flops Utilization.

1505
00:58:42,450 --> 00:58:44,340
You're running some
compute on the device.

1506
00:58:44,340 --> 00:58:46,070
How much compute do
you actually realize

1507
00:58:46,070 --> 00:58:47,750
of that theoretical maximum?

1508
00:58:47,750 --> 00:58:48,980
And this is not hard to do.

1509
00:58:48,980 --> 00:58:50,605
You can write a couple
lines of PyTorch

1510
00:58:50,605 --> 00:58:52,170
code and just like
benchmark this.

1511
00:58:52,170 --> 00:58:54,710
So this is a benchmark that
I wrote that I ran on an H100

1512
00:58:54,710 --> 00:58:55,820
yesterday.

1513
00:58:55,820 --> 00:58:58,410
And you can see what it
does is basically x-axis.

1514
00:58:58,410 --> 00:59:00,960
It just does dense
matrix multiply in a loop

1515
00:59:00,960 --> 00:59:03,390
and then times how long did
the matrix multiply happen?

1516
00:59:03,390 --> 00:59:05,420
How long did the
matrix multiply take?

1517
00:59:05,420 --> 00:59:07,890
We can compute how many flops
a matrix multiply takes.

1518
00:59:07,890 --> 00:59:10,910
Then on the x-axis, we're
plotting the size of our matrix

1519
00:59:10,910 --> 00:59:13,980
going from 512 up to 32,000.

1520
00:59:13,980 --> 00:59:16,673
And then the y-axis is this
hardware flops utilization,

1521
00:59:16,673 --> 00:59:19,090
which is basically the fraction
of the theoretical maximum

1522
00:59:19,090 --> 00:59:21,220
throughput of the device
that we actually realize

1523
00:59:21,220 --> 00:59:22,700
from these matrix multiplies.

1524
00:59:22,700 --> 00:59:25,870
And you can see that on this
pretty straightforward PyTorch

1525
00:59:25,870 --> 00:59:29,560
loop, we're getting about 80%
HFU on an H100 once we get

1526
00:59:29,560 --> 00:59:33,020
to large matrix multiplies
of around 8,000 by 8,000.

1527
00:59:33,020 --> 00:59:34,250
So that's pretty good.

1528
00:59:34,250 --> 00:59:36,100
But the problem is
that HFU does not

1529
00:59:36,100 --> 00:59:39,220
account for all the other stuff
that your model needs to do.

1530
00:59:39,220 --> 00:59:41,570
We're maybe doing
activation computation.

1531
00:59:41,570 --> 00:59:43,730
We're maybe running some
other models on the side.

1532
00:59:43,730 --> 00:59:45,680
We're maybe doing data
loading, data augmentation.

1533
00:59:45,680 --> 00:59:47,222
There's a lot of
other stuff your GPU

1534
00:59:47,222 --> 00:59:49,850
is doing other than just forward
backward on your raw model.

1535
00:59:49,850 --> 00:59:52,330
And that's where we move from
Hardware Flops Utilization

1536
00:59:52,330 --> 00:59:54,290
to Model Flops Utilization.

1537
00:59:54,290 --> 00:59:56,530
So Model Flops
Utilization is basically

1538
00:59:56,530 --> 01:00:00,250
saying, what fraction of
the GPU's theoretical TFLOPs

1539
01:00:00,250 --> 01:00:03,610
are being used for forward
backward in my model?

1540
01:00:03,610 --> 01:00:06,050
And this is the thing you
always want to optimize for.

1541
01:00:06,050 --> 01:00:09,340
So then to make this more
concrete, you basically compute,

1542
01:00:09,340 --> 01:00:10,740
based on your
model architecture,

1543
01:00:10,740 --> 01:00:13,240
the size of the number of the
layers, the size of the layers

1544
01:00:13,240 --> 01:00:15,020
you compute, how
many flops does it

1545
01:00:15,020 --> 01:00:16,610
take to do a full
forward backward

1546
01:00:16,610 --> 01:00:19,520
pass of your architecture
on your minibatch of data?

1547
01:00:19,520 --> 01:00:22,787
Then you look up somewhere,
the peak theoretical throughput

1548
01:00:22,787 --> 01:00:24,120
of the device you're running on.

1549
01:00:24,120 --> 01:00:25,370
And then you divide those two.

1550
01:00:25,370 --> 01:00:28,070
And that tells you how long
should a full forward backward

1551
01:00:28,070 --> 01:00:31,070
pass take if you were achieving
the theoretical maximum

1552
01:00:31,070 --> 01:00:32,160
throughput of the device?

1553
01:00:32,160 --> 01:00:34,760
That's the theoretical fastest
you could ever do a forward

1554
01:00:34,760 --> 01:00:36,726
backward pass on your model.

1555
01:00:36,726 --> 01:00:40,828
Then you actually time a forward
backward pass of your model.

1556
01:00:40,828 --> 01:00:42,870
Your training loop is
doing all this other stuff.

1557
01:00:42,870 --> 01:00:44,270
It's doing data loading.

1558
01:00:44,270 --> 01:00:45,360
It's doing augmentation.

1559
01:00:45,360 --> 01:00:46,590
It's doing communication.

1560
01:00:46,590 --> 01:00:48,900
It's doing maybe
activation checkpointing.

1561
01:00:48,900 --> 01:00:50,760
So it's doing recomputation,
doing backward.

1562
01:00:50,760 --> 01:00:52,552
Your training loop is
doing a lot of stuff.

1563
01:00:52,552 --> 01:00:54,323
Just times see how
long it actually takes.

1564
01:00:54,323 --> 01:00:55,740
And then divide
those two numbers.

1565
01:00:55,740 --> 01:00:58,340
That gives you a number
between 0 and 1, which is like,

1566
01:00:58,340 --> 01:01:00,560
what fraction of that
theoretical maximum

1567
01:01:00,560 --> 01:01:03,770
are you actually achieving
in your training loop?

1568
01:01:03,770 --> 01:01:06,530
And that's your MFU, your
Model Flops Utilization.

1569
01:01:06,530 --> 01:01:09,080
And again, we can benchmark
this with some relatively simple

1570
01:01:09,080 --> 01:01:09,970
PyTorch code.

1571
01:01:09,970 --> 01:01:11,720
Here's an example
running forward backward

1572
01:01:11,720 --> 01:01:15,790
on just a short multi-layer
perceptron with a ReLU

1573
01:01:15,790 --> 01:01:19,420
nonlinearity and with really
big, with really wide MLP layers

1574
01:01:19,420 --> 01:01:22,460
and a gigantic batch
size on a single H100.

1575
01:01:22,460 --> 01:01:24,555
This is getting around 50% MFU.

1577
01:01:27,220 --> 01:01:28,870
And then in general,
whenever you're

1578
01:01:28,870 --> 01:01:30,980
trying to tune knobs for
distributed training,

1579
01:01:30,980 --> 01:01:32,650
you always want to
try to tune whatever

1580
01:01:32,650 --> 01:01:35,202
knobs you can to maximize
MFU, because that's the one

1581
01:01:35,202 --> 01:01:37,660
metric that we typically care
about when trying to optimize

1582
01:01:37,660 --> 01:01:40,090
training throughput.

1583
01:01:40,090 --> 01:01:44,440
And in general, an MFU these
days generally above 30%

1584
01:01:44,440 --> 01:01:45,440
is pretty good.

1585
01:01:45,440 --> 01:01:47,117
If you're way under
30%, you've probably

1586
01:01:47,117 --> 01:01:48,700
got some gigantic
bottleneck somewhere

1587
01:01:48,700 --> 01:01:50,030
and something is going wrong.

1588
01:01:50,030 --> 01:01:52,790
And above 40% is pretty,
pretty excellent.

1589
01:01:52,790 --> 01:01:54,412
And that's basically
state of the art.

1590
01:01:54,412 --> 01:01:55,870
And here's some
numbers that we can

1591
01:01:55,870 --> 01:01:57,160
pull from a couple of papers.

1592
01:01:57,160 --> 01:02:00,580
In particular, this is
that Llama3-405B paper

1593
01:02:00,580 --> 01:02:02,215
that we talked about.

1594
01:02:02,215 --> 01:02:04,750
In their final training step,
they have a couple different

1595
01:02:04,750 --> 01:02:07,480
variants of their training
phases where they train

1596
01:02:07,480 --> 01:02:11,400
on between 8,000 and
16,000 GPUs simultaneously.

1597
01:02:11,400 --> 01:02:15,250
And across that, they're getting
MFUs roughly in the high 30s,

1598
01:02:15,250 --> 01:02:15,970
low 40s.

1599
01:02:15,970 --> 01:02:18,330
And that's pretty good.

1600
01:02:18,330 --> 01:02:19,890
You're never going
to get really much

1601
01:02:19,890 --> 01:02:22,140
higher than that on an H100.

1602
01:02:22,140 --> 01:02:25,200
And actually, paradoxically,
more recent devices sometimes

1603
01:02:25,200 --> 01:02:26,400
get worse MFUs.

1604
01:02:26,400 --> 01:02:29,350
So on the previous generation
devices, the H100s,

1605
01:02:29,350 --> 01:02:31,690
you could sometimes
get MFUs above 50%.

1606
01:02:31,690 --> 01:02:33,960
And the reason for
that is because GPUs

1607
01:02:33,960 --> 01:02:36,810
are getting faster, faster
than they are getting faster

1608
01:02:36,810 --> 01:02:37,780
at communicating.

1609
01:02:37,780 --> 01:02:40,330
So when we move from
the A100 to the H100,

1610
01:02:40,330 --> 01:02:43,170
we got roughly a 3x improvement
in the theoretical throughput

1611
01:02:43,170 --> 01:02:45,570
of the compute, but we
only got a 2x improvement

1612
01:02:45,570 --> 01:02:47,280
in the theoretical
memory bandwidth.

1613
01:02:47,280 --> 01:02:50,940
So there's this growing
gap between making

1614
01:02:50,940 --> 01:02:52,810
GPUs are getting
faster really fast,

1615
01:02:52,810 --> 01:02:55,780
but it's harder to scale the
communication between the GPUs.

1616
01:02:55,780 --> 01:02:58,350
And as a result, we tend
to sometimes get worse MFAs

1617
01:02:58,350 --> 01:03:00,335
actually on more recent
generations of devices.

1619
01:03:03,015 --> 01:03:05,760
And I intentionally I wanted
to spend most of the time

1620
01:03:05,760 --> 01:03:07,803
on those points, because
those are the ones

1621
01:03:07,803 --> 01:03:09,970
that you guys are probably
going to use in practice.

1622
01:03:09,970 --> 01:03:12,400
I don't think anyone in
this room likely has access

1623
01:03:12,400 --> 01:03:14,060
to a 10,000 GPU cluster.

1624
01:03:14,060 --> 01:03:15,980
If you do, come talk
to me after class.

1625
01:03:15,980 --> 01:03:18,310
I would love to be your friend.

1626
01:03:18,310 --> 01:03:19,750
So those are the
ones that you're

1627
01:03:19,750 --> 01:03:21,730
likely to encounter
in practice, like, up

1628
01:03:21,730 --> 01:03:23,620
to many hundreds of GPUs.

1629
01:03:23,620 --> 01:03:25,700
But there are these other
ones that I just like.

1630
01:03:25,700 --> 01:03:27,980
There are slides here that
I think are pretty nice,

1631
01:03:27,980 --> 01:03:29,710
but it's OK if we don't go
through the full details

1632
01:03:29,710 --> 01:03:30,210
of these.

1633
01:03:30,210 --> 01:03:32,320
You can check it offline.

1634
01:03:32,320 --> 01:03:34,480
So we said context
parallelism is basically

1635
01:03:34,480 --> 01:03:36,080
splitting on the
sequence dimension.

1636
01:03:36,080 --> 01:03:38,930
So we said transformers
are operating on sequences.

1637
01:03:38,930 --> 01:03:42,715
And basically the idea is
you've got a long sequence.

1638
01:03:42,715 --> 01:03:45,925
Make different GPUs handle
different parts of the sequence.

1639
01:03:45,925 --> 01:03:48,587
And if you recall,
your transformer block,

1640
01:03:48,587 --> 01:03:50,920
this is actually easy for
large parts of the transformer

1641
01:03:50,920 --> 01:03:54,640
because the LayerNorm,
the FFN, MLP,

1642
01:03:54,640 --> 01:03:56,230
and the residual
connections, those

1643
01:03:56,230 --> 01:03:59,133
all operate independently
across the sequence anyway.

1644
01:03:59,133 --> 01:04:00,550
So it's relatively
straightforward

1645
01:04:00,550 --> 01:04:03,100
to ask to chunk up that
computation across the sequence

1646
01:04:03,100 --> 01:04:05,050
dimension.

1647
01:04:05,050 --> 01:04:07,320
I mean, it does get a little
bit hairy inside the MLP

1648
01:04:07,320 --> 01:04:08,778
because there are
weights in there.

1649
01:04:08,778 --> 01:04:10,950
So you have to have some
all reduce of the gradients

1650
01:04:10,950 --> 01:04:13,685
like we did in the
data parallelism cases.

1651
01:04:13,685 --> 01:04:15,060
The attention is
where things get

1652
01:04:15,060 --> 01:04:16,690
hairy for sequence parallelism.

1653
01:04:16,690 --> 01:04:18,360
Because if you
remember attention,

1654
01:04:18,360 --> 01:04:20,670
we need to compute these
all pairs interaction

1655
01:04:20,670 --> 01:04:23,310
between every pair of
elements inside the sequence.

1656
01:04:23,310 --> 01:04:25,380
The QKV projection is
easy, because that's

1657
01:04:25,380 --> 01:04:27,490
trivially parallelizable
over the sequence,

1658
01:04:27,490 --> 01:04:30,450
but that core attention matrix,
that actually gets pretty tricky

1659
01:04:30,450 --> 01:04:32,640
to parallelize.

1660
01:04:32,640 --> 01:04:34,713
The first version of this
that people developed

1661
01:04:34,713 --> 01:04:36,630
was called ring attention,
where you basically

1662
01:04:36,630 --> 01:04:39,850
take that full attention
matrix, chunk it up into blocks,

1663
01:04:39,850 --> 01:04:43,470
and then have your GPUs
work on those blocks

1664
01:04:43,470 --> 01:04:45,510
independently in parallel
in the right order

1665
01:04:45,510 --> 01:04:47,133
to make sure
everything works out.

1666
01:04:47,133 --> 01:04:48,550
There's a lot of
details in there.

1667
01:04:48,550 --> 01:04:50,760
You can check out the
paper for more details.

1668
01:04:50,760 --> 01:04:53,320
The second, which is a little
bit conceptually easier,

1669
01:04:53,320 --> 01:04:56,010
is called Ulysses
attention, where you

1670
01:04:56,010 --> 01:04:57,520
do parallelism over the heads.

1671
01:04:57,520 --> 01:04:59,610
So remember in a transformer,
you're almost always

1672
01:04:59,610 --> 01:05:01,380
doing multi-head
attention where you're

1673
01:05:01,380 --> 01:05:04,500
computing attention over
multiple attention matrices

1674
01:05:04,500 --> 01:05:05,800
all in parallel.

1675
01:05:05,800 --> 01:05:07,530
So in Ulysses
attention, we're going

1676
01:05:07,530 --> 01:05:11,110
to parallelize the computation
of that core attention operator.

1677
01:05:11,110 --> 01:05:13,493
Parallelize that overheads,
and then everything else,

1678
01:05:13,493 --> 01:05:14,910
all other parts
of the transformer

1679
01:05:14,910 --> 01:05:18,300
are parallelized over
the sequence dimension.

1680
01:05:18,300 --> 01:05:22,290
And as an example, this
context parallelism

1681
01:05:22,290 --> 01:05:24,780
becomes important once you
scale up your sequence length

1682
01:05:24,780 --> 01:05:25,840
to be quite large.

1683
01:05:25,840 --> 01:05:28,570
So if we go back to this
example of Llama3 pretraining,

1684
01:05:28,570 --> 01:05:30,670
they actually train the
model in two stages.

1685
01:05:30,670 --> 01:05:34,410
The first stage they go sequence
length of 8,000 with no context

1686
01:05:34,410 --> 01:05:35,437
parallelism whatsoever.

1687
01:05:35,437 --> 01:05:37,770
And then they have a second
stage of training where they

1688
01:05:37,770 --> 01:05:40,540
crank the sequence
length up to 130,000.

1689
01:05:40,540 --> 01:05:44,260
And at that point, they do
16 way context parallelism.

1690
01:05:44,260 --> 01:05:48,630
So that means that each of those
131,000 long sequences has 16

1691
01:05:48,630 --> 01:05:51,130
GPUs operating on one
sequence in parallel.

1692
01:05:51,130 --> 01:05:54,670
And that's like saying the
batch size is 1/16, because now,

1693
01:05:54,670 --> 01:06:01,050
each batch, each GPU is working
on less than one element.

1694
01:06:01,050 --> 01:06:02,440
So that's context parallelism.

1695
01:06:02,440 --> 01:06:03,460
Pipeline parallelism.

1696
01:06:03,460 --> 01:06:05,670
We're going to split across
the layers dimension.

1697
01:06:05,670 --> 01:06:08,160
Intuitively what you want
to do, you have a network

1698
01:06:08,160 --> 01:06:09,250
with a bunch of layers.

1699
01:06:09,250 --> 01:06:12,370
And we're going to just divide
the layers across the GPUs.

1700
01:06:12,370 --> 01:06:14,820
That's actually a very
intuitive thing to do.

1701
01:06:14,820 --> 01:06:17,380
The problem is that there's
sequential dependencies?

1702
01:06:17,380 --> 01:06:20,310
Because each GPU, it
needs the activations

1703
01:06:20,310 --> 01:06:22,380
from the previous
GPU to continue

1704
01:06:22,380 --> 01:06:23,447
running the forward pass.

1705
01:06:23,447 --> 01:06:25,530
And during the backward
pass, I need the gradients

1706
01:06:25,530 --> 01:06:27,000
from the upstream
layers in order

1707
01:06:27,000 --> 01:06:28,420
to compute the backward pass.

1708
01:06:28,420 --> 01:06:31,830
So we can draw a diagram like
this, where the vertical axis

1709
01:06:31,830 --> 01:06:33,280
are GPUs 1 to 4.

1710
01:06:33,280 --> 01:06:36,370
The horizontal axis is what
happens over the course of time.

1711
01:06:36,370 --> 01:06:39,810
So you can see that GPU 1
runs forward, then passes

1712
01:06:39,810 --> 01:06:43,080
the activations to GPU 2, which
passes activations to GPU 3,

1713
01:06:43,080 --> 01:06:45,090
which passes
activations to GPU 4.

1714
01:06:45,090 --> 01:06:46,260
GPU 4 is lucky.

1715
01:06:46,260 --> 01:06:48,160
It can do forward
backward all at once,

1716
01:06:48,160 --> 01:06:51,970
then pass gradients back to GPU
3, back to GPU 2, back to GPU 1.

1717
01:06:51,970 --> 01:06:55,630
So from this graph, that's
obviously really, really bad,

1718
01:06:55,630 --> 01:06:57,940
because the GPUs are
mostly sitting idle.

1719
01:06:57,940 --> 01:07:02,320
And in fact, if you have N GPUs,
you're only getting useful work

1720
01:07:02,320 --> 01:07:04,370
out of them 1/N of the time.

1721
01:07:04,370 --> 01:07:07,250
So that means that if we had
eight-way pipeline parallelism,

1722
01:07:07,250 --> 01:07:09,400
your maximum possible
MFU at that point

1723
01:07:09,400 --> 01:07:11,420
is like 12%, which is terrible.

1724
01:07:11,420 --> 01:07:13,120
So that's really bad.

1725
01:07:13,120 --> 01:07:14,747
And by the way,
there's a cute name.

1726
01:07:14,747 --> 01:07:16,330
These are sometimes
called the bubble,

1727
01:07:16,330 --> 01:07:19,490
is like that chunk of where
GPUs are waiting for work.

1728
01:07:19,490 --> 01:07:22,040
And they're like waiting
around for the communication.

1729
01:07:22,040 --> 01:07:23,650
So the trick in
pipeline parallelism

1730
01:07:23,650 --> 01:07:24,890
is to shrink the bubble.

1731
01:07:24,890 --> 01:07:26,270
You want to have less bubble.

1732
01:07:26,270 --> 01:07:29,290
And the way that we do that is
running multiple microbatches

1733
01:07:29,290 --> 01:07:30,530
simultaneously.

1734
01:07:30,530 --> 01:07:33,160
So now rather than
running one batch

1735
01:07:33,160 --> 01:07:35,390
of data through all the
GPUs forward and backward,

1736
01:07:35,390 --> 01:07:37,660
we're going to have multiple
batches of data in play

1737
01:07:37,660 --> 01:07:39,700
simultaneously and
shuttle these things

1738
01:07:39,700 --> 01:07:42,537
across the GPUs in parallel.

1739
01:07:42,537 --> 01:07:44,620
So there's a lot of different
interesting patterns

1740
01:07:44,620 --> 01:07:45,980
you can try to design for this.

1741
01:07:45,980 --> 01:07:47,590
But here's a
relatively simple one,

1742
01:07:47,590 --> 01:07:50,240
where we have four-way
pipeline parallelism.

1743
01:07:50,240 --> 01:07:52,790
So there's four GPUs that
are all working in parallel.

1744
01:07:52,790 --> 01:07:55,360
And then we have four
batches of data that are all

1745
01:07:55,360 --> 01:07:57,250
active at the same time.

1746
01:07:57,250 --> 01:07:58,970
And these batches
are color-coded.

1747
01:07:58,970 --> 01:08:02,610
So then we see that GPU 1 runs
forward on the blue batch,

1748
01:08:02,610 --> 01:08:03,990
then forward on
the yellow batch,

1749
01:08:03,990 --> 01:08:06,615
then forward on the green batch,
then forward on the red batch,

1750
01:08:06,615 --> 01:08:09,320
and while GPU 1 is going
forward on the yellow batch,

1751
01:08:09,320 --> 01:08:12,170
we have passed the activations
of the blue batch to GPU 1

1752
01:08:12,170 --> 01:08:13,260
or to GPU 2.

1753
01:08:13,260 --> 01:08:15,810
And GPU 2 can now do
forward on the blue batch.

1754
01:08:15,810 --> 01:08:19,140
So these things can all cascade
down and happen in parallel.

1755
01:08:19,140 --> 01:08:20,540
And then the same
pattern repeats

1756
01:08:20,540 --> 01:08:21,600
during the backward pass.

1757
01:08:21,600 --> 01:08:25,189
We can interleave these
different microbatches

1758
01:08:25,189 --> 01:08:28,160
as we pipeline them
through the different GPUs.

1759
01:08:28,160 --> 01:08:32,240
And in this case with four-way
pipeline parallelism with four

1760
01:08:32,240 --> 01:08:35,660
microbatches, the
max theoretical MFU

1761
01:08:35,660 --> 01:08:38,580
is just the fraction of this
graph, which is not white.

1762
01:08:38,580 --> 01:08:41,689
And that increases now to
57% which is pretty good.

1763
01:08:41,689 --> 01:08:43,920
So with pipeline
parallelism, in theory,

1764
01:08:43,920 --> 01:08:46,260
if you go to lots and
lots of microbatches,

1765
01:08:46,260 --> 01:08:48,260
then your MFU is
going to be good

1766
01:08:48,260 --> 01:08:50,340
because you can do a
lot of work in parallel.

1767
01:08:50,340 --> 01:08:52,288
But the more
microbatches, you have,

1768
01:08:52,288 --> 01:08:54,330
they need to store all
the activations in memory.

1769
01:08:54,330 --> 01:08:56,310
So now you need to do
activation checkpointing,

1770
01:08:56,310 --> 01:08:57,895
and then you think
like, oh crap.

1771
01:08:57,895 --> 01:08:59,020
How do I tune these things?

1772
01:08:59,020 --> 01:09:01,095
Should I have more
pipeline parallelism?

1773
01:09:01,095 --> 01:09:02,470
Should I have
fewer microbatches?

1774
01:09:02,470 --> 01:09:04,762
Should I have more aggressive
activation checkpointing?

1775
01:09:04,762 --> 01:09:07,498
And then should I also layer
data parallelism on top of that?

1776
01:09:07,498 --> 01:09:08,040
I don't know.

1777
01:09:08,040 --> 01:09:09,082
What are you going to do?

1778
01:09:09,082 --> 01:09:09,810
Maximize MFU.

1779
01:09:09,810 --> 01:09:11,850
You're going to try to
tune all of those knobs

1780
01:09:11,850 --> 01:09:16,109
to maximize the MFU of
your training pipeline.

1781
01:09:16,109 --> 01:09:18,460
Then the last one is
tensor parallelism.

1782
01:09:18,460 --> 01:09:22,870
So this one you're going to
split on that model dimension.

1783
01:09:22,870 --> 01:09:24,390
So basically what
we're going to do

1784
01:09:24,390 --> 01:09:26,830
is we have a lot of weight
matrices in our model.

1785
01:09:26,830 --> 01:09:30,060
All those weight matrices
are like computing XW equals

1786
01:09:30,060 --> 01:09:33,510
Y. That's basically what we're
doing over and over again inside

1787
01:09:33,510 --> 01:09:34,569
of our transformer.

1788
01:09:34,569 --> 01:09:39,240
Now the idea is we'll split
each weight matrix across GPUs.

1789
01:09:39,240 --> 01:09:40,750
And now this is
different from FSDP

1790
01:09:40,750 --> 01:09:42,750
because we're actually
splitting a single weight

1791
01:09:42,750 --> 01:09:44,229
matrix across GPUs.

1792
01:09:44,229 --> 01:09:46,390
And now there's
no communication.

1793
01:09:46,390 --> 01:09:48,340
Now we do a block
matrix multiply.

1794
01:09:48,340 --> 01:09:52,229
So each GPU is now computing a
slice of that matrix multiply

1795
01:09:52,229 --> 01:09:53,200
on the full input.

1796
01:09:53,200 --> 01:09:55,050
So in this case, we
split our weight matrix

1797
01:09:55,050 --> 01:09:57,450
into W1, W2, W3, W4.

1798
01:09:57,450 --> 01:10:00,120
And then each GPU just computes
a slice of that matrix,

1799
01:10:00,120 --> 01:10:02,870
multiply to compute a
slice of the output.

1800
01:10:02,870 --> 01:10:06,180
And then a problem is that, now
after you do that forward pass,

1801
01:10:06,180 --> 01:10:09,960
then you need to gather the
activations across all the GPUs

1802
01:10:09,960 --> 01:10:11,660
to do the next forward pass.

1803
01:10:11,660 --> 01:10:13,220
There's a slight
trick, which is,

1804
01:10:13,220 --> 01:10:15,750
if you have two of these
layers in sequence,

1805
01:10:15,750 --> 01:10:20,510
you can actually get away with
not gathering in between two

1806
01:10:20,510 --> 01:10:21,180
layers.

1807
01:10:21,180 --> 01:10:23,780
So if you have two layers, you
can sit down in a quiet place

1808
01:10:23,780 --> 01:10:24,810
and work through this.

1809
01:10:24,810 --> 01:10:26,185
You split the
first weight matrix

1810
01:10:26,185 --> 01:10:27,450
into column-shaped chunks.

1811
01:10:27,450 --> 01:10:30,120
Then you split the second weight
matrix into row-shaped chunks.

1812
01:10:30,120 --> 01:10:32,390
And if you do all
this, then it all

1813
01:10:32,390 --> 01:10:34,850
works out magically because
of the magic and mystery

1814
01:10:34,850 --> 01:10:36,295
of block matrix multiplication.

1815
01:10:36,295 --> 01:10:37,670
And you see that
the final output

1816
01:10:37,670 --> 01:10:41,840
you can compute as an inner
product like structure

1817
01:10:41,840 --> 01:10:44,790
of these block matrix
multiplies of Y and U.

1818
01:10:44,790 --> 01:10:49,490
So then you basically can have
two layers of matrix multiply

1819
01:10:49,490 --> 01:10:51,627
that are split
across multiple GPUs.

1820
01:10:51,627 --> 01:10:53,210
And then they only
need to communicate

1821
01:10:53,210 --> 01:10:54,713
at the end of every two layers.

1822
01:10:54,713 --> 01:10:56,130
And this actually
works out nicely

1823
01:10:56,130 --> 01:11:00,100
because remember transformers
have a two layer MLP in the FFN.

1824
01:11:00,100 --> 01:11:01,860
So this is a really
nice trick that

1825
01:11:01,860 --> 01:11:03,900
plays really nicely
into the two layer MLPs

1826
01:11:03,900 --> 01:11:05,282
that transformers always have.

1827
01:11:05,282 --> 01:11:06,990
So it's pretty common
in big transformers

1828
01:11:06,990 --> 01:11:11,070
to use tensor parallelism,
this like two layer tensor

1829
01:11:11,070 --> 01:11:16,050
parallelism trick on the
MLP in a transformer.

1830
01:11:16,050 --> 01:11:19,020
So that's basically all of our
mechanisms for splitting up

1831
01:11:19,020 --> 01:11:20,890
computation across GPUs.

1832
01:11:20,890 --> 01:11:22,240
Which one is the best?

1833
01:11:22,240 --> 01:11:23,890
The actual answer
is all of them.

1834
01:11:23,890 --> 01:11:26,500
So in practice, we're going
to use ND parallelism.

1835
01:11:26,500 --> 01:11:28,950
We saw already an example of
two-dimensional parallelism

1836
01:11:28,950 --> 01:11:29,860
with HSRP.

1837
01:11:29,860 --> 01:11:32,160
In practice, the
current state of the art

1838
01:11:32,160 --> 01:11:33,940
is like four
dimensional parallelism.

1839
01:11:33,940 --> 01:11:37,080
If we go back to Lama, we see
that they are training with

1840
01:11:37,080 --> 01:11:40,240
on their biggest training
run with 16,000 GPUs.

1841
01:11:40,240 --> 01:11:43,110
They're using 8 way tensor
parallelism, 16 way context

1842
01:11:43,110 --> 01:11:46,650
parallelism, 16 way pipeline
parallelism, and 8 data way

1843
01:11:46,650 --> 01:11:48,750
parallelism all
at the same time.

1844
01:11:48,750 --> 01:11:53,090
And if you're careful, different
mechanisms of parallelism

1845
01:11:53,090 --> 01:11:54,990
have different
communication requirements.

1846
01:11:54,990 --> 01:11:58,850
So if you're careful in how you
arrange these different axes

1847
01:11:58,850 --> 01:12:00,630
of parallelism
along your cluster,

1848
01:12:00,630 --> 01:12:02,780
you can try to take
advantage of that varying

1849
01:12:02,780 --> 01:12:07,910
speeds of communication
across your whole cluster.

1850
01:12:07,910 --> 01:12:09,380
That's basically
a whirlwind tour

1851
01:12:09,380 --> 01:12:10,890
of large-scale
distributed training.

1852
01:12:10,890 --> 01:12:13,730
So the takeaway for today
is that an individual GPU

1853
01:12:13,730 --> 01:12:16,920
is basically a generalizable
parallel computing machine.

1854
01:12:16,920 --> 01:12:20,485
A GPU cluster is a giant
massively parallel machine

1855
01:12:20,485 --> 01:12:22,610
with tens of thousands,
maybe hundreds of thousands

1856
01:12:22,610 --> 01:12:25,790
of individual GPUs, and we want
to program as one big unit.

1857
01:12:25,790 --> 01:12:28,040
And then we talked about
multiple different mechanisms

1858
01:12:28,040 --> 01:12:30,960
of parallelizing our
computation across big clusters,

1859
01:12:30,960 --> 01:12:34,230
as well as one trick activation
checkpointing for saving memory,

1860
01:12:34,230 --> 01:12:36,740
and then the one guiding light
metric that you're always

1861
01:12:36,740 --> 01:12:39,115
trying to optimize when you
design these pipelines, which

1862
01:12:39,115 --> 01:12:40,260
is model flops utilization.

1863
01:12:40,260 --> 01:12:41,780
So the next time you're
going out and training

1864
01:12:41,780 --> 01:12:44,450
on tens of thousands of GPUs,
hope you keep this in mind.

1865
01:12:44,450 --> 01:12:48,040
And let me know so I can borrow
your tens of thousands of GPUs.