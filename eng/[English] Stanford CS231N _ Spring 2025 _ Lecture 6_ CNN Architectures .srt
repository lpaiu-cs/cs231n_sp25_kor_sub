2
00:00:05,270 --> 00:00:06,410
Hi, everyone.

3
00:00:06,410 --> 00:00:07,415
My name is Zain.

4
00:00:07,415 --> 00:00:09,290
I realized I actually
didn't introduce myself

5
00:00:09,290 --> 00:00:12,058
on the first lecture I gave,
which was lecture three,

6
00:00:12,058 --> 00:00:14,100
but I'm one of the
co-instructors for the course.

7
00:00:14,100 --> 00:00:15,100
My name is Zain Durante.

8
00:00:15,100 --> 00:00:17,040
I'm co-advised by
Ehsan and Fei-Fei.

9
00:00:17,040 --> 00:00:22,430
I'm a fourth year PhD student
at Stanford and in this lecture

10
00:00:22,430 --> 00:00:24,260
today, lecture six,
we'll be talking

11
00:00:24,260 --> 00:00:26,330
about training convolutional
neural networks

12
00:00:26,330 --> 00:00:29,420
and also CNN architectures.

13
00:00:29,420 --> 00:00:33,260
So I would say this
lecture is really broken up

14
00:00:33,260 --> 00:00:34,770
into two different components.

15
00:00:34,770 --> 00:00:36,290
The first one is
just telling you

16
00:00:36,290 --> 00:00:39,590
how to piece together all of
the different building blocks

17
00:00:39,590 --> 00:00:42,030
that we've learned like
convolutional layers,

18
00:00:42,030 --> 00:00:43,970
linear layers or
fully connected layers

19
00:00:43,970 --> 00:00:46,350
together to create
a CNN architecture.

20
00:00:46,350 --> 00:00:48,170
We'll go through some
examples and then

21
00:00:48,170 --> 00:00:50,060
also we'll talk about
how you actually

22
00:00:50,060 --> 00:00:53,070
train these and all the
steps involved there.

23
00:00:53,070 --> 00:00:54,800
So as I mentioned
before, we'll have

24
00:00:54,800 --> 00:00:56,520
basically two different topics.

25
00:00:56,520 --> 00:00:58,080
The first one is
how to build CNNs

26
00:00:58,080 --> 00:00:59,955
and by this I mean how
do you actually define

27
00:00:59,955 --> 00:01:02,630
your CNN architecture to
set it up to be trained.

28
00:01:02,630 --> 00:01:04,090
And then the second
set of topics

29
00:01:04,090 --> 00:01:05,780
today is how do you train CNNs?

30
00:01:05,780 --> 00:01:09,500
So starting with the
first set of topics here,

31
00:01:09,500 --> 00:01:12,710
we'll go through the layers in
convolutional neural networks.

32
00:01:12,710 --> 00:01:14,840
And if you recall
from last lecture,

33
00:01:14,840 --> 00:01:17,300
we learned about the key
layer in these models,

34
00:01:17,300 --> 00:01:18,830
which is the convolution layer.

35
00:01:18,830 --> 00:01:23,020
And the way that these layers
work is they have these filters.

36
00:01:23,020 --> 00:01:26,110
You have a predefined
number of filters per one

37
00:01:26,110 --> 00:01:28,390
of these convolution
layers, in this case six.

38
00:01:28,390 --> 00:01:31,820
They match the depth of
your input data here.

39
00:01:31,820 --> 00:01:34,490
So in this case, we have
a 32 by 32 RGB image

40
00:01:34,490 --> 00:01:35,810
so we three depth channels.

41
00:01:35,810 --> 00:01:38,920
Each of these filters
slides across the image

42
00:01:38,920 --> 00:01:40,600
and calculates a
score at each point.

43
00:01:40,600 --> 00:01:42,360
At that location
in the image, you

44
00:01:42,360 --> 00:01:44,470
take the dot product of
the values in the filter

45
00:01:44,470 --> 00:01:45,893
with the values in the image.

46
00:01:45,893 --> 00:01:48,560
So you multiply all these values
together, you sum them together

47
00:01:48,560 --> 00:01:49,970
and then you add a bias term.

48
00:01:49,970 --> 00:01:54,280
And this is how you calculate
each value in your output

49
00:01:54,280 --> 00:01:56,210
activation map on the right.

50
00:01:56,210 --> 00:01:57,970
So you have these
sliding windows

51
00:01:57,970 --> 00:01:59,090
that go across the image.

52
00:01:59,090 --> 00:02:01,010
They calculate a
score at each position

53
00:02:01,010 --> 00:02:04,360
and that's how you get
these activation maps.

54
00:02:04,360 --> 00:02:06,170
And you have one
per each filter.

55
00:02:06,170 --> 00:02:08,080
Normally, we'll
do a sort of ReLU

56
00:02:08,080 --> 00:02:11,420
or a non-linearity activation
function at the end here.

57
00:02:11,420 --> 00:02:13,010
So this is from last lecture.

58
00:02:13,010 --> 00:02:15,160
I won't spend too
much time on it.

59
00:02:15,160 --> 00:02:17,530
The question is,
for images the depth

60
00:02:17,530 --> 00:02:19,880
is equal to the number
of channels RGB,

61
00:02:19,880 --> 00:02:23,000
but here the depth is
6 for the output here.

62
00:02:23,000 --> 00:02:25,880
So if we had a second
convolution layer afterwards

63
00:02:25,880 --> 00:02:27,580
they would need
to be filters that

64
00:02:27,580 --> 00:02:31,400
go across all six of
these activation maps

65
00:02:31,400 --> 00:02:34,300
so the next layer
would be a depth of 6.

66
00:02:34,300 --> 00:02:36,500
OK, and then the second
layer we talked about,

67
00:02:36,500 --> 00:02:38,770
which is much simpler than
the convolution layer,

68
00:02:38,770 --> 00:02:40,310
is this idea of a pooling layer.

69
00:02:40,310 --> 00:02:43,480
So here it's still
this filter that we're

70
00:02:43,480 --> 00:02:47,330
sliding across the image, 2
by 2 filter with stride 2.

71
00:02:47,330 --> 00:02:48,290
So we're skipping over.

72
00:02:48,290 --> 00:02:49,873
We're not doing every
single location.

73
00:02:49,873 --> 00:02:51,410
And here is a max pooling.

74
00:02:51,410 --> 00:02:53,665
So we're just taking the
max of each of these areas

75
00:02:53,665 --> 00:02:55,040
and that's the
value we get here.

76
00:02:55,040 --> 00:02:57,490
Or you could do an
average pooling.

77
00:02:57,490 --> 00:02:59,600
These are both commonly
used, I would say.

78
00:02:59,600 --> 00:03:01,083
And depending on
the architecture,

79
00:03:01,083 --> 00:03:02,500
you would probably
just, if you're

80
00:03:02,500 --> 00:03:03,970
creating new architecture,
you would try both of them

81
00:03:03,970 --> 00:03:05,690
and see what
performs better here.

82
00:03:05,690 --> 00:03:07,870
But the basic idea
is to consolidate

83
00:03:07,870 --> 00:03:11,710
among the height and width
dimensions for your image.

84
00:03:11,710 --> 00:03:15,340
OK, so we've basically
gone over at this point

85
00:03:15,340 --> 00:03:17,890
in the course all the
top row here, which

86
00:03:17,890 --> 00:03:20,290
is convolution layers,
pooling layers and also

87
00:03:20,290 --> 00:03:21,470
the fully connected layers.

88
00:03:21,470 --> 00:03:22,887
These are the first
layers that we

89
00:03:22,887 --> 00:03:24,940
talked about in the
neural networks lecture

90
00:03:24,940 --> 00:03:27,370
where it's basically one
matrix multiply followed

91
00:03:27,370 --> 00:03:29,000
by an activation function.

92
00:03:29,000 --> 00:03:31,190
And for the rest
of this lecture,

93
00:03:31,190 --> 00:03:32,890
I'll talk about the
remaining layers

94
00:03:32,890 --> 00:03:36,220
that you see in CNNs, at least
the commonly used ones, which

95
00:03:36,220 --> 00:03:39,440
include normalization layers,
which I'll go into those,

96
00:03:39,440 --> 00:03:42,760
and then also dropout,
which is a regularization

97
00:03:42,760 --> 00:03:44,560
technique that's used
actually in the model

98
00:03:44,560 --> 00:03:45,860
architecture itself.

99
00:03:45,860 --> 00:03:48,358
And then finally, we'll revisit
the activation functions

100
00:03:48,358 --> 00:03:50,650
and I'll tell you about here
are the most commonly used

101
00:03:50,650 --> 00:03:55,030
ones both historically and in
the modern era of deep learning.

102
00:03:55,030 --> 00:03:57,410
So starting out with
normalization layers,

103
00:03:57,410 --> 00:03:59,410
the basic idea
here is we're going

104
00:03:59,410 --> 00:04:01,900
to be calculating statistics
like the mean and standard

105
00:04:01,900 --> 00:04:05,080
deviation for our input
data and then using

106
00:04:05,080 --> 00:04:06,710
those to normalize the data.

107
00:04:06,710 --> 00:04:10,390
And then we'll
basically learn what

108
00:04:10,390 --> 00:04:13,960
is the optimal
distribution for the model

109
00:04:13,960 --> 00:04:15,020
to learn at that point.

110
00:04:15,020 --> 00:04:18,130
So very concretely
we learn parameters

111
00:04:18,130 --> 00:04:19,930
that will scale
and shift our input

112
00:04:19,930 --> 00:04:24,290
data by a learned mean and a
learned standard deviation.

113
00:04:24,290 --> 00:04:27,640
So how all of these
normalization layers work is

114
00:04:27,640 --> 00:04:28,700
there's two steps.

115
00:04:28,700 --> 00:04:31,120
The first is to normalize
the data coming in

116
00:04:31,120 --> 00:04:34,790
to be a unit Gaussian, so mean
0, our standard deviation 1.

117
00:04:34,790 --> 00:04:36,920
And then we will
scale and shift it.

118
00:04:36,920 --> 00:04:40,828
So multiply by some value
to increase or decrease

119
00:04:40,828 --> 00:04:42,370
the standard deviation
and then shift

120
00:04:42,370 --> 00:04:43,880
it to change where the mean is.

121
00:04:43,880 --> 00:04:46,280
And all normalization
layers do this technique,

122
00:04:46,280 --> 00:04:47,920
but the way that
they differ is how

123
00:04:47,920 --> 00:04:49,520
they calculate the statistics.

124
00:04:49,520 --> 00:04:52,330
So how are you calculating the
mean and standard deviation

125
00:04:52,330 --> 00:04:53,840
and which values
are you applying

126
00:04:53,840 --> 00:04:56,900
these calculated statistics to,
but all normalization layers are

127
00:04:56,900 --> 00:04:59,690
doing this high level process.

128
00:04:59,690 --> 00:05:03,410
So I'll talk about
layer norm, which

129
00:05:03,410 --> 00:05:06,590
is the most commonly used
normalization layer I would

130
00:05:06,590 --> 00:05:07,920
say today in deep learning.

131
00:05:07,920 --> 00:05:11,120
And it's really commonly used
in transformers specifically.

132
00:05:11,120 --> 00:05:13,970
And so you can imagine you
have some data coming in,

133
00:05:13,970 --> 00:05:16,500
x, which is a batch size of n.

134
00:05:16,500 --> 00:05:19,110
So we have n samples
coming into our model.

135
00:05:19,110 --> 00:05:22,130
And each of these are
vectors of dimension D.

136
00:05:22,130 --> 00:05:25,250
So what LayerNorm
does is we calculate

137
00:05:25,250 --> 00:05:29,180
a mean and standard deviation
for each of our samples

138
00:05:29,180 --> 00:05:29,940
separately.

139
00:05:29,940 --> 00:05:32,270
So we're calculating
what is the mean

140
00:05:32,270 --> 00:05:35,970
along the depth or
the dimension D here

141
00:05:35,970 --> 00:05:37,770
and what is the
standard deviation.

142
00:05:37,770 --> 00:05:40,920
Then we learn parameters.

143
00:05:40,920 --> 00:05:42,530
And so these are
learnable parameters

144
00:05:42,530 --> 00:05:45,950
learned via gradient
descent in our model

145
00:05:45,950 --> 00:05:47,460
to then apply to each sample.

146
00:05:47,460 --> 00:05:49,490
So after we calculate
our statistics

147
00:05:49,490 --> 00:05:51,900
in this way, treating
each sample separately

148
00:05:51,900 --> 00:05:54,490
to calculate the mean
and standard deviation,

149
00:05:54,490 --> 00:05:59,930
we then apply these learned
scale and shift or parameters

150
00:05:59,930 --> 00:06:00,430
here.

151
00:06:00,430 --> 00:06:03,780
So we subtract the mean and
divide by the standard deviation

152
00:06:03,780 --> 00:06:06,370
within our input
data to normalize it

153
00:06:06,370 --> 00:06:09,420
and then we apply the scale
here with multiplication

154
00:06:09,420 --> 00:06:10,870
and the shift.

155
00:06:10,870 --> 00:06:13,870
So this is the idea
behind LayerNorm.

156
00:06:13,870 --> 00:06:19,740
And at a high level idea all of
these different normalization

157
00:06:19,740 --> 00:06:22,990
layers are all computing
very similar things,

158
00:06:22,990 --> 00:06:25,020
but the main difference
is how are they

159
00:06:25,020 --> 00:06:28,600
computing the mean and
standard deviation.

160
00:06:28,600 --> 00:06:30,840
So this is a really
nice visualization

161
00:06:30,840 --> 00:06:33,960
from a paper called Group
Normalization that introduces

162
00:06:33,960 --> 00:06:35,790
a new way to normalize.

163
00:06:35,790 --> 00:06:38,095
Its I would say not so
commonly used these days,

164
00:06:38,095 --> 00:06:39,720
but this is actually
a really great way

165
00:06:39,720 --> 00:06:42,510
to gain intuition about how
these different normalization

166
00:06:42,510 --> 00:06:43,540
layers are different.

167
00:06:43,540 --> 00:06:47,610
So for LayerNorm I described
the really simple case

168
00:06:47,610 --> 00:06:50,170
where we just have vectors
that were normalizing,

169
00:06:50,170 --> 00:06:52,990
but in the case for
convolutional neural networks,

170
00:06:52,990 --> 00:06:56,520
we have a channel
dimension or the depth

171
00:06:56,520 --> 00:06:58,770
and the height and the width
or the spatial dimensions

172
00:06:58,770 --> 00:06:59,650
of the image.

173
00:06:59,650 --> 00:07:02,890
So what LayerNorm does
is for each sample,

174
00:07:02,890 --> 00:07:04,780
we're still processing
it separately

175
00:07:04,780 --> 00:07:07,410
and we're calculating
the mean across all

176
00:07:07,410 --> 00:07:10,150
of the channels, all of the
heights and all the widths.

177
00:07:10,150 --> 00:07:14,970
So if we look back
into this diagram here,

178
00:07:14,970 --> 00:07:18,876
you would basically
be calculating

179
00:07:18,876 --> 00:07:22,260
one mean and one
standard deviation

180
00:07:22,260 --> 00:07:23,770
over all of these values.

181
00:07:23,770 --> 00:07:26,292
So for each of our
input data points

182
00:07:26,292 --> 00:07:28,500
we're calculating one mean
and one standard deviation

183
00:07:28,500 --> 00:07:31,250
across all of the channels,
all of the height and width

184
00:07:31,250 --> 00:07:31,750
dimensions.

185
00:07:31,750 --> 00:07:33,900
So this is what
LayerNorm is doing.

186
00:07:33,900 --> 00:07:36,630
But you could imagine feasibly
that you could calculate

187
00:07:36,630 --> 00:07:38,050
these statistics differently.

188
00:07:38,050 --> 00:07:41,470
BatchNorm, you're
taking each channel

189
00:07:41,470 --> 00:07:44,070
so each channel is being
calculated as one mean and one

190
00:07:44,070 --> 00:07:45,100
standard deviation.

191
00:07:45,100 --> 00:07:46,930
And you're applying it
just to that channel

192
00:07:46,930 --> 00:07:49,510
and so you're averaging across
all the data in your batch.

193
00:07:49,510 --> 00:07:52,310
InstanceNorm is even more
granular and then a GroupNorm.

194
00:07:52,310 --> 00:07:54,400
So I just want to point
out all these layers

195
00:07:54,400 --> 00:07:57,100
are kind of trying to do
the same thing where you're

196
00:07:57,100 --> 00:07:58,868
basically normalizing
your data and then

197
00:07:58,868 --> 00:08:01,160
having these learnable scaling
and shifting parameters,

198
00:08:01,160 --> 00:08:03,285
but the way they do it is
different because they're

199
00:08:03,285 --> 00:08:07,390
calculating the statistics using
different subsets of your input

200
00:08:07,390 --> 00:08:08,770
data.

201
00:08:08,770 --> 00:08:10,490
Yeah, so the question
is for LayerNorm,

202
00:08:10,490 --> 00:08:12,740
are we calculating one mean
and one standard deviation

203
00:08:12,740 --> 00:08:14,390
for each image or input data?

204
00:08:14,390 --> 00:08:17,710
Yes, they're all
calculated separately,

205
00:08:17,710 --> 00:08:19,770
but for BatchNorm
it would not be

206
00:08:19,770 --> 00:08:21,020
the case in this example here.

207
00:08:21,020 --> 00:08:22,060
Yeah.

208
00:08:22,060 --> 00:08:25,590
For BatchNorm it's actually
within the mini batch for when

209
00:08:25,590 --> 00:08:26,840
you're doing gradient descent.

210
00:08:26,840 --> 00:08:28,882
You have a small batch of
data you're looking at.

211
00:08:28,882 --> 00:08:30,580
You feed it into your model.

212
00:08:30,580 --> 00:08:34,330
You're calculating
the per channel

213
00:08:34,330 --> 00:08:37,120
mean and standard
deviation based on all

214
00:08:37,120 --> 00:08:38,340
of the data in your batch.

216
00:08:41,075 --> 00:08:43,450
Yeah, I think this is a good--
if you can understand this

217
00:08:43,450 --> 00:08:46,495
diagram, I think you understand
what all the different

218
00:08:46,495 --> 00:08:49,120
normalization layers are doing,
so it might be worthwhile after

219
00:08:49,120 --> 00:08:51,078
lecture if you still
don't fully understand it,

220
00:08:51,078 --> 00:08:54,070
just to go through and make
sure you understand what--

221
00:08:54,070 --> 00:08:56,080
and shaded in blue,
these are the values

222
00:08:56,080 --> 00:08:58,810
we're both calculating our
statistics over and then

223
00:08:58,810 --> 00:09:00,973
applying the mean and
standard deviations to.

224
00:09:00,973 --> 00:09:02,890
Yeah, one final question
and then we'll go on.

225
00:09:02,890 --> 00:09:05,860
For channel, same
has the layers?

226
00:09:05,860 --> 00:09:10,430
So channel here
is the depth here.

227
00:09:10,430 --> 00:09:21,790
So the number of values you
have at each spatial location.

228
00:09:21,790 --> 00:09:23,260
OK, cool.

229
00:09:23,260 --> 00:09:27,260
So we talked about
normalization layers.

230
00:09:27,260 --> 00:09:29,500
The key idea is you're
calculating the statistics,

231
00:09:29,500 --> 00:09:33,310
applying them to your input
data and then learning

232
00:09:33,310 --> 00:09:37,160
a scale and shift parameter
that you then apply.

233
00:09:37,160 --> 00:09:41,150
So the next type of layer we'll
talk about is called dropout.

234
00:09:41,150 --> 00:09:44,420
And this is a regularization
layer in CNN's.

235
00:09:44,420 --> 00:09:46,280
And this is the final
layer that you'll

236
00:09:46,280 --> 00:09:48,360
need to learn in
order to-- basically,

237
00:09:48,360 --> 00:09:50,443
we can start going through
all these different CNN

238
00:09:50,443 --> 00:09:53,060
architectures that people
have created over the years.

239
00:09:53,060 --> 00:09:56,030
So with dropout,
the basic idea is

240
00:09:56,030 --> 00:09:59,630
to add randomization
during the training process

241
00:09:59,630 --> 00:10:01,572
that we then take
away at test time.

242
00:10:01,572 --> 00:10:03,530
And the goal is to make
it harder for the model

243
00:10:03,530 --> 00:10:06,570
to learn the training data, but
then it will generalize better.

244
00:10:06,570 --> 00:10:09,350
So this is a form
of regularization.

245
00:10:09,350 --> 00:10:12,950
The way we do it concretely
is that in each forward pass

246
00:10:12,950 --> 00:10:15,500
of our layer, we'll
actually randomly zero out

247
00:10:15,500 --> 00:10:19,440
some of the outputs or
activations from that layer.

248
00:10:19,440 --> 00:10:24,000
And the main parameter you
have for this dropout layer,

249
00:10:24,000 --> 00:10:25,800
which is just a
fixed hyperparameter,

250
00:10:25,800 --> 00:10:28,590
is the probability of
dropping out the values.

251
00:10:28,590 --> 00:10:32,390
And 0.5 is probably
the most common or 0.25

252
00:10:32,390 --> 00:10:34,040
is also commonly used here.

253
00:10:34,040 --> 00:10:37,310
So you're just dropping out a
fixed percentage of the values

254
00:10:37,310 --> 00:10:37,980
here.

255
00:10:37,980 --> 00:10:41,790
And so going forward to the
next layer these would be 0.

256
00:10:41,790 --> 00:10:45,950
And so you don't really need
to calculate the values here.

257
00:10:45,950 --> 00:10:50,082
So I mean, basically all the
outputs here are 0 at this point

258
00:10:50,082 --> 00:10:52,540
so there's some tricks you can
do with masking so you don't

259
00:10:52,540 --> 00:10:54,700
even need to calculate
the values because 0

260
00:10:54,700 --> 00:10:57,350
times any value will be 0.

261
00:10:57,350 --> 00:11:01,820
So I think in general you
might ask, why does this work?

262
00:11:01,820 --> 00:11:07,660
And I would say this is more of
an empirical thing than a well

263
00:11:07,660 --> 00:11:11,900
studied from a
theoretical standpoint,

264
00:11:11,900 --> 00:11:13,810
but there are
actually some ways you

265
00:11:13,810 --> 00:11:16,360
can view what dropout is
doing to gain intuition

266
00:11:16,360 --> 00:11:17,780
of why this might be useful.

267
00:11:17,780 --> 00:11:21,430
So it basically forces
your network to--

268
00:11:21,430 --> 00:11:24,530
you can imagine it forces it to
have redundant representations.

269
00:11:24,530 --> 00:11:27,520
So if we have a list of features
that we're learning at a given

270
00:11:27,520 --> 00:11:31,340
layer, say the layer right
before the output of our model,

271
00:11:31,340 --> 00:11:34,960
and we have a CNN that is
basically extracting each

272
00:11:34,960 --> 00:11:37,510
of these features so it
can detect if there's

273
00:11:37,510 --> 00:11:40,070
ears in the image or if
there's a tail, if it's furry,

274
00:11:40,070 --> 00:11:43,310
it has claws and
you want your model

275
00:11:43,310 --> 00:11:45,270
to output the probability
of a cat score.

276
00:11:45,270 --> 00:11:47,690
So one of the things
that's useful about this

277
00:11:47,690 --> 00:11:49,910
is because some of these
values might randomly

278
00:11:49,910 --> 00:11:51,330
be dropped out during training.

279
00:11:51,330 --> 00:11:53,540
Your model can't over
rely on certain features

280
00:11:53,540 --> 00:11:56,430
being present in
some of the classes

281
00:11:56,430 --> 00:11:59,450
and actually needs to
learn a more broad set

282
00:11:59,450 --> 00:12:02,690
of correspondences between
your features and your output

283
00:12:02,690 --> 00:12:03,270
classes.

284
00:12:03,270 --> 00:12:07,560
So the model can't just
hard focus on, OK, well,

285
00:12:07,560 --> 00:12:11,450
if it has an ear and is
furry, it just so happens

286
00:12:11,450 --> 00:12:14,190
that these are always cats
or something like this

287
00:12:14,190 --> 00:12:17,670
or if it has claws
and it has an ear,

288
00:12:17,670 --> 00:12:20,468
it'll almost always be
a cat on your data set.

289
00:12:20,468 --> 00:12:22,010
So it'll actually
help you generalize

290
00:12:22,010 --> 00:12:25,490
better to new features, despite
the fact that in your data

291
00:12:25,490 --> 00:12:28,040
set there might actually be
really strong correlations

292
00:12:28,040 --> 00:12:30,680
between the codependency of
certain features and your output

293
00:12:30,680 --> 00:12:31,200
class.

294
00:12:31,200 --> 00:12:33,920
By having dropout,
you're essentially

295
00:12:33,920 --> 00:12:35,383
making it so the
model can't rely

296
00:12:35,383 --> 00:12:36,800
on these during
the training phase

297
00:12:36,800 --> 00:12:40,410
because it won't always see
the pairs of features together.

298
00:12:40,410 --> 00:12:43,000
So this is an example for cat.

299
00:12:43,000 --> 00:12:46,260
And the question is if we had
something like tree instead

300
00:12:46,260 --> 00:12:48,880
how would you determine
which features to drop out?

301
00:12:48,880 --> 00:12:52,600
So the dropping out part is
actually completely random

302
00:12:52,600 --> 00:12:54,580
so we're not making
any choices about this.

303
00:12:54,580 --> 00:12:59,520
It's just in this case 50% of
your features at any given step

304
00:12:59,520 --> 00:13:01,800
will be dropped
out and set to 0.

305
00:13:01,800 --> 00:13:04,410
So yeah, you don't
have to make choices

306
00:13:04,410 --> 00:13:08,785
about it, which is kind of nice,
but it is completely random.

307
00:13:08,785 --> 00:13:10,410
How would the model
know if you're only

308
00:13:10,410 --> 00:13:13,460
seeing a subset of the features
like tail and claw here?

309
00:13:13,460 --> 00:13:15,960
The point is you will actually
do worse on the training data

310
00:13:15,960 --> 00:13:18,127
because you're only seeing
a subset of the features.

311
00:13:18,127 --> 00:13:20,190
So it does make the
model worse by not having

312
00:13:20,190 --> 00:13:22,680
all the information,
but then it does better

313
00:13:22,680 --> 00:13:23,830
at test time, this idea.

314
00:13:23,830 --> 00:13:25,205
So worst training
time and better

315
00:13:25,205 --> 00:13:27,510
and test time
because at test time

316
00:13:27,510 --> 00:13:30,670
you're basically no longer
having this drop out.

317
00:13:30,670 --> 00:13:32,830
So the final component
here, which maybe I

318
00:13:32,830 --> 00:13:35,080
should have explained first
before fielding questions,

319
00:13:35,080 --> 00:13:38,370
is the idea at test time
you're no longer dropping out

320
00:13:38,370 --> 00:13:39,230
any of the values.

321
00:13:39,230 --> 00:13:41,210
So this is randomness that we're
adding during the training phase

322
00:13:41,210 --> 00:13:41,710
only.

323
00:13:41,710 --> 00:13:45,100
Then at test time
we're never masking

324
00:13:45,100 --> 00:13:49,000
any of the output activations
and we're removing this dropout

325
00:13:49,000 --> 00:13:50,480
idea altogether.

326
00:13:50,480 --> 00:13:54,340
Now one thing we need to note is
that because if we were dropping

327
00:13:54,340 --> 00:13:57,388
out 50% of the activations
during training time,

328
00:13:57,388 --> 00:13:58,930
at test time, you're
basically having

329
00:13:58,930 --> 00:14:04,520
50% more values that are being
input to each of your layers.

330
00:14:04,520 --> 00:14:06,920
And so this can cause issues
if you don't scale it.

331
00:14:06,920 --> 00:14:09,370
So what you need to do is
multiply by the probability

332
00:14:09,370 --> 00:14:14,440
of dropout so that the
magnitude of the values coming

333
00:14:14,440 --> 00:14:17,330
into each layer is preserved
during training and test time.

334
00:14:17,330 --> 00:14:19,333
Otherwise, if you're
dropping 50% of the values

335
00:14:19,333 --> 00:14:21,500
and then at test time, you
just include all of them,

336
00:14:21,500 --> 00:14:24,580
you'll get really weird
behavior because you'll

337
00:14:24,580 --> 00:14:27,970
be seeing much larger magnitude
of inputs than before.

338
00:14:27,970 --> 00:14:30,910
Yeah, so what about
for backward prop?

339
00:14:30,910 --> 00:14:34,700
So for back prop when you
have these zeroed values,

340
00:14:34,700 --> 00:14:40,620
yeah, it's like you don't
need to traverse that path

341
00:14:40,620 --> 00:14:41,920
of your directed graph anymore.

342
00:14:41,920 --> 00:14:43,003
It's very similar to ReLU.

343
00:14:43,003 --> 00:14:46,110
If you have a zeroed
value at that point

344
00:14:46,110 --> 00:14:47,380
the gradient becomes 0.

345
00:14:47,380 --> 00:14:54,120
So anything further back in
your computational graph,

346
00:14:54,120 --> 00:14:56,790
there's no gradients
calculated at that point.

347
00:14:56,790 --> 00:15:01,410
If you're dropping out
certain values or activations,

348
00:15:01,410 --> 00:15:03,660
the weights associated with
those specific activations

349
00:15:03,660 --> 00:15:06,780
will not be updated
during gradient descent

350
00:15:06,780 --> 00:15:08,820
if you're dropping them out.

351
00:15:08,820 --> 00:15:10,560
So the question is--

352
00:15:10,560 --> 00:15:11,590
maybe I'll reframe it.

353
00:15:11,590 --> 00:15:13,030
What are we doing at test time?

354
00:15:13,030 --> 00:15:17,380
So at test time, we are using
all of the output activations.

355
00:15:17,380 --> 00:15:18,880
We're not dropping
them out anymore,

356
00:15:18,880 --> 00:15:21,430
but we need to scale by the
probability of drop out.

357
00:15:21,430 --> 00:15:23,340
So we multiply each of
our output activations

358
00:15:23,340 --> 00:15:26,200
by this p value because now
we're using all of them.

359
00:15:26,200 --> 00:15:29,640
So otherwise you
can imagine you have

360
00:15:29,640 --> 00:15:32,880
each node is seeing a
significantly higher number

361
00:15:32,880 --> 00:15:35,290
of inputs than it did during
training at test time.

362
00:15:35,290 --> 00:15:37,780
So you need to multiply
by this p value

363
00:15:37,780 --> 00:15:41,360
to maintain the same magnitude
of your inputs coming in.

364
00:15:41,360 --> 00:15:43,505
And the variance
stays the same in all

365
00:15:43,505 --> 00:15:44,630
these different properties.

366
00:15:44,630 --> 00:15:47,500
It works very nicely
if you do it like this.

367
00:15:47,500 --> 00:15:50,300
So the question is, can you just
add noise to the image instead?

368
00:15:50,300 --> 00:15:51,925
The answer is yes
and we'll go over how

369
00:15:51,925 --> 00:15:53,420
to do that in future slides.

370
00:15:53,420 --> 00:15:57,400
Yes, that's a great idea
to add noise to your image

371
00:15:57,400 --> 00:15:59,438
OK, some specific code here.

372
00:15:59,438 --> 00:16:01,730
I won't go over this because
we already mentioned this,

373
00:16:01,730 --> 00:16:06,790
but you're dropping a p
percentage of your activations

374
00:16:06,790 --> 00:16:11,220
here and then you multiply
here at test time.

375
00:16:11,220 --> 00:16:13,558
The next topic I'll talk
about is activation functions.

376
00:16:13,558 --> 00:16:15,850
So you all have basically
learned all of the key layers

377
00:16:15,850 --> 00:16:17,057
now in CNNs.

378
00:16:17,057 --> 00:16:19,390
And now we're going to be
talking about these activation

379
00:16:19,390 --> 00:16:20,120
functions.

380
00:16:20,120 --> 00:16:23,020
If you remember, the whole point
of these activation functions

381
00:16:23,020 --> 00:16:25,910
is to introduce
nonlinearities to our model.

382
00:16:25,910 --> 00:16:29,452
So right now with these
convolution operators,

383
00:16:29,452 --> 00:16:30,910
the kernel sliding
across the image

384
00:16:30,910 --> 00:16:35,893
and the fully connected
layers without activations,

385
00:16:35,893 --> 00:16:37,310
they're all just
linear operations

386
00:16:37,310 --> 00:16:39,623
because they're
multiplications and additions.

387
00:16:39,623 --> 00:16:41,540
And the whole point of
the activation function

388
00:16:41,540 --> 00:16:43,230
is to add non-linearity.

389
00:16:43,230 --> 00:16:49,100
So historically, sigmoid was a
really commonly used activation

390
00:16:49,100 --> 00:16:52,410
function, but there's actually
a key problem with sigmoid

391
00:16:52,410 --> 00:16:55,110
that is the reason why
it's no longer used today.

392
00:16:55,110 --> 00:16:57,390
And so sigmoid, if you
graph it, looks like this.

393
00:16:57,390 --> 00:17:01,320
You can see the equation in the
top right of the slide here.

394
00:17:01,320 --> 00:17:06,470
And the main issue is that
empirically what happened

395
00:17:06,470 --> 00:17:08,819
was after many
layers of sigmoids,

396
00:17:08,819 --> 00:17:10,670
you would get smaller
and smaller gradients

397
00:17:10,670 --> 00:17:12,105
as you're computing backprop.

398
00:17:12,105 --> 00:17:13,730
So starting from the
end, the gradients

399
00:17:13,730 --> 00:17:15,329
are fairly large in magnitude.

400
00:17:15,329 --> 00:17:18,030
And as you undergo multiple
layers of backpropagation,

401
00:17:18,030 --> 00:17:20,488
go to the initial early
layers of your model,

402
00:17:20,488 --> 00:17:22,280
you would get smaller
and smaller gradients

403
00:17:22,280 --> 00:17:23,490
as you do this process.

404
00:17:23,490 --> 00:17:27,109
So I'll actually open this
question up to the class.

405
00:17:27,109 --> 00:17:30,480
This isn't a phenomenon we
see that occurs with sigmoid.

406
00:17:30,480 --> 00:17:33,970
And so in what
regions in our graph

407
00:17:33,970 --> 00:17:36,870
does sigmoid have a
really small gradient?

408
00:17:36,870 --> 00:17:39,573
Yeah, so very negative and very
positive values is correct.

409
00:17:39,573 --> 00:17:40,990
And this is actually
a huge issue.

410
00:17:40,990 --> 00:17:42,365
I mean, you can
visually see here

411
00:17:42,365 --> 00:17:44,070
in the graph the
gradient is very flat.

412
00:17:44,070 --> 00:17:45,790
You're taking the
derivative here.

413
00:17:45,790 --> 00:17:46,920
It's very small.

414
00:17:46,920 --> 00:17:49,050
And so basically for
almost all of our input

415
00:17:49,050 --> 00:17:52,160
space from negative infinity
to positive infinity

416
00:17:52,160 --> 00:17:53,410
you have very small gradients.

417
00:17:53,410 --> 00:17:55,285
And it's only this narrow
range in the middle

418
00:17:55,285 --> 00:17:58,540
where you have something
that's non-zero

419
00:17:58,540 --> 00:18:00,750
so it basically
approaches 0 very quickly

420
00:18:00,750 --> 00:18:02,230
on both ends of the extremes.

421
00:18:02,230 --> 00:18:05,400
And so this means that if the
values coming into sigmoid

422
00:18:05,400 --> 00:18:07,140
are very large or
very small, then

423
00:18:07,140 --> 00:18:11,190
your gradient will
be very small.

424
00:18:11,190 --> 00:18:13,290
So this is one of
the main reasons

425
00:18:13,290 --> 00:18:15,930
why ReLU became super
popular because now

426
00:18:15,930 --> 00:18:19,410
in the positive region we don't
have any of this behavior.

427
00:18:19,410 --> 00:18:24,120
It's just derivative of 1 here,
but in practice, you still

428
00:18:24,120 --> 00:18:27,240
have this flat portion
here on the left

429
00:18:27,240 --> 00:18:31,910
where your gradient is 0.

430
00:18:31,910 --> 00:18:36,360
So now we basically have
half of our input domain.

431
00:18:36,360 --> 00:18:38,570
Here we get a gradient
of 1 and the other half

432
00:18:38,570 --> 00:18:41,060
is 0, which is better
than almost all

433
00:18:41,060 --> 00:18:43,940
of it being 0 or very close
to 0 except for a small region

434
00:18:43,940 --> 00:18:44,880
in the middle.

435
00:18:44,880 --> 00:18:47,100
So in practice
these work better.

436
00:18:47,100 --> 00:18:48,830
Also, it's much
cheaper to just compute

437
00:18:48,830 --> 00:18:51,710
a max operation between 0
and your input value then

438
00:18:51,710 --> 00:18:52,830
the sigmoid function.

439
00:18:52,830 --> 00:18:56,360
So for those two reasons,
ReLU became super popular.

440
00:18:56,360 --> 00:18:59,810
But you still have this issue
where for any negative input

441
00:18:59,810 --> 00:19:03,650
you get a 0 gradient and
so more recently there's

442
00:19:03,650 --> 00:19:07,670
been popular activation
functions that avoid this

443
00:19:07,670 --> 00:19:12,710
by basically having a non-flat
section of the activation

444
00:19:12,710 --> 00:19:17,400
function in the nearby
neighborhood to near 0.

445
00:19:17,400 --> 00:19:20,990
So this is GELU, And
there's also SELU,

446
00:19:20,990 --> 00:19:23,490
which I'll show in a slide, but
I won't go over the formula.

447
00:19:23,490 --> 00:19:24,860
They look very similar.

448
00:19:24,860 --> 00:19:31,110
The basic idea is to smoothen
out this non-smooth jump here

449
00:19:31,110 --> 00:19:35,680
in the derivative from
0 to 1 at 0.0 for ReLU.

450
00:19:35,680 --> 00:19:41,680
So this is a very sharp and
non-smooth function with ReLU,

451
00:19:41,680 --> 00:19:44,070
but the nice part about
GELU is we actually

452
00:19:44,070 --> 00:19:46,330
have non-zero gradients here.

453
00:19:46,330 --> 00:19:49,960
And in the limit as x approaches
infinity or negative infinity,

454
00:19:49,960 --> 00:19:52,840
it does converge
to ReLU as well,

455
00:19:52,840 --> 00:19:55,510
but you get more smooth
behavior in the middle here.

456
00:19:55,510 --> 00:19:57,330
And specifically
what GELU calculates

457
00:19:57,330 --> 00:20:01,360
is this Gaussian
error linear unit.

458
00:20:01,360 --> 00:20:05,250
So this is the cumulative
distribution function

459
00:20:05,250 --> 00:20:06,940
of a Gaussian normal.

460
00:20:06,940 --> 00:20:09,610
So if you imagine the area
under the curve of a Gaussian,

461
00:20:09,610 --> 00:20:11,740
that's what this phi
of x is at any point x.

462
00:20:11,740 --> 00:20:14,370
So if you have a
really negative value

463
00:20:14,370 --> 00:20:16,720
here you'll have a
value close to 0,

464
00:20:16,720 --> 00:20:19,570
which is why it
converges to ReLU 0 here.

465
00:20:19,570 --> 00:20:22,230
And at a very high
positive value

466
00:20:22,230 --> 00:20:24,490
it gets very close to 1,
the area under the curve

467
00:20:24,490 --> 00:20:26,530
so it converges to x here.

468
00:20:26,530 --> 00:20:28,162
So this is GELU.

469
00:20:28,162 --> 00:20:30,370
It has these nice properties
and it converges to ReLU

470
00:20:30,370 --> 00:20:31,870
at the extremes too.

471
00:20:31,870 --> 00:20:35,740
And this is the main activation
function used in transformers

472
00:20:35,740 --> 00:20:38,380
today.

473
00:20:38,380 --> 00:20:40,900
If you look at all of them
and you squint, a lot of these

474
00:20:40,900 --> 00:20:41,597
look the same.

475
00:20:41,597 --> 00:20:43,430
The basic idea is
something relatively flat.

476
00:20:43,430 --> 00:20:49,870
And then in the limit, it
approaches f of x equals x

477
00:20:49,870 --> 00:20:53,020
and it becomes a linear line.

478
00:20:53,020 --> 00:20:56,410
So SELU is actually
x times sigmoid

479
00:20:56,410 --> 00:20:59,170
of x, which also
has this property

480
00:20:59,170 --> 00:21:02,450
of for very negative value,
you have something close to 0

481
00:21:02,450 --> 00:21:05,150
and for very positive
value it's close to 1.

482
00:21:05,150 --> 00:21:08,500
So it's actually similar to
the cumulative distribution

483
00:21:08,500 --> 00:21:11,128
function for the unit
Gaussian that is phi here.

484
00:21:11,128 --> 00:21:12,670
So that's why the
shapes are actually

485
00:21:12,670 --> 00:21:13,795
really similar looking too.

487
00:21:17,236 --> 00:21:20,207
So you might ask where are
these activations used in CNNs

488
00:21:20,207 --> 00:21:21,790
and the general
answer is that they're

489
00:21:21,790 --> 00:21:24,080
placed after linear operators.

490
00:21:24,080 --> 00:21:27,695
So almost any time we have a
feedforward or a linear layer

491
00:21:27,695 --> 00:21:29,070
or a fully connected
layer, these

492
00:21:29,070 --> 00:21:31,210
are all words for
the same layer,

493
00:21:31,210 --> 00:21:34,740
so matrix multiply followed
by a activation function.

494
00:21:34,740 --> 00:21:37,380
Or if we have a convolutional
layer, pretty much after these

495
00:21:37,380 --> 00:21:39,305
is where we place the--

496
00:21:39,305 --> 00:21:40,680
so after the
convolutional layer,

497
00:21:40,680 --> 00:21:45,300
after these linear layers, we'll
put the activation function.

498
00:21:45,300 --> 00:21:47,850
OK, so you've learned
everything now

499
00:21:47,850 --> 00:21:49,668
about all the
components of CNNs.

500
00:21:49,668 --> 00:21:51,210
And I'll now go
through some examples

501
00:21:51,210 --> 00:21:53,280
of how we put them
together and how people

502
00:21:53,280 --> 00:21:56,010
have created state of the art
convolutional neural network

503
00:21:56,010 --> 00:21:58,650
architectures.

504
00:21:58,650 --> 00:22:00,930
I think this is a really
neat slide because it

505
00:22:00,930 --> 00:22:02,530
plots two different values.

506
00:22:02,530 --> 00:22:04,570
So on one hand, we
have the error rate,

507
00:22:04,570 --> 00:22:06,105
which is these blue bars.

508
00:22:06,105 --> 00:22:08,730
And this is over time so these
are different models people have

509
00:22:08,730 --> 00:22:09,940
trained on ImageNet.

510
00:22:09,940 --> 00:22:12,060
And then you have
these orange triangles

511
00:22:12,060 --> 00:22:16,330
which represent the number
of layers that models have.

512
00:22:16,330 --> 00:22:17,940
And you can see
at the same point

513
00:22:17,940 --> 00:22:21,420
that we have a significant
drop in error where we actually

514
00:22:21,420 --> 00:22:24,190
surpass human performance
for the first time,

515
00:22:24,190 --> 00:22:27,710
we see a huge increase
in the number of layers.

516
00:22:27,710 --> 00:22:29,753
So we'll go over
in class today how

517
00:22:29,753 --> 00:22:31,420
they were able to
achieve this, and what

518
00:22:31,420 --> 00:22:35,920
were the design challenges and
goals for how they did this.

519
00:22:35,920 --> 00:22:38,590
Historically, AlexNet
was the first CNN

520
00:22:38,590 --> 00:22:41,180
based paper that worked
really well on ImageNet.

521
00:22:41,180 --> 00:22:43,593
And they were able to
train it by using GPUs.

522
00:22:43,593 --> 00:22:45,260
We talked about this
earlier in lecture,

523
00:22:45,260 --> 00:22:48,670
so I won't spend too much
details around AlexNet from

524
00:22:48,670 --> 00:22:51,280
a historical lens, but I do
want to compare it to another

525
00:22:51,280 --> 00:22:55,600
architecture called VGG,
which was a really standard

526
00:22:55,600 --> 00:22:59,300
and commonly used
architecture in the 2010s.

527
00:22:59,300 --> 00:23:05,350
And I think I can plot the
two CNN architectures together

528
00:23:05,350 --> 00:23:06,380
side by side here.

529
00:23:06,380 --> 00:23:09,910
So in general in AI, we like
to plot our model architectures

530
00:23:09,910 --> 00:23:13,090
using block diagrams,
where each block represents

531
00:23:13,090 --> 00:23:15,910
a different layer
or a group of layers

532
00:23:15,910 --> 00:23:17,480
that are stacked together.

533
00:23:17,480 --> 00:23:19,540
And it also helps
you gain intuition

534
00:23:19,540 --> 00:23:22,000
about what are the
general differences just

535
00:23:22,000 --> 00:23:23,700
at an initial glance.

536
00:23:23,700 --> 00:23:26,550
So these orange blocks, which
are the common ones here,

537
00:23:26,550 --> 00:23:28,670
are 3 by 3 convolution layers.

538
00:23:28,670 --> 00:23:30,170
So these are
convolution layers that

539
00:23:30,170 --> 00:23:33,980
have filters that are sliding
across that are size 3 by 3.

540
00:23:33,980 --> 00:23:36,620
Their stride is 1
so they're visiting

541
00:23:36,620 --> 00:23:39,570
every location in the image,
not skipping over anything.

542
00:23:39,570 --> 00:23:42,020
And they add padding
of 1 around the outside

543
00:23:42,020 --> 00:23:47,400
so that we're not shrinking as
we do these convolution layers.

544
00:23:47,400 --> 00:23:52,460
And so they also add these max
pooling layers throughout here

545
00:23:52,460 --> 00:23:52,980
too.

546
00:23:52,980 --> 00:23:57,320
And you'll notice that for all
of these after a pooling layer

547
00:23:57,320 --> 00:24:01,130
they'll start doing two sets
of fully connected layers

548
00:24:01,130 --> 00:24:05,940
of dimension 4,096 followed
by a dimension of 1,000.

549
00:24:05,940 --> 00:24:08,810
And the reason we have 1,000 at
the end is because ImageNet was

550
00:24:08,810 --> 00:24:10,890
1,000 different
image categories.

551
00:24:10,890 --> 00:24:13,530
So we need to have scores
for each of these categories.

552
00:24:13,530 --> 00:24:16,160
So the final layer is
always equal to the number

553
00:24:16,160 --> 00:24:21,170
of classes you have for an
image classification problem.

554
00:24:21,170 --> 00:24:23,500
And so you can see it actually
looks extremely similar.

555
00:24:23,500 --> 00:24:26,130
It's just like a scaled
up version of AlexNet

556
00:24:26,130 --> 00:24:28,120
with more layers here.

557
00:24:28,120 --> 00:24:31,680
And also they're now doing some
three groups of convolutions

558
00:24:31,680 --> 00:24:34,530
at a time, followed by
pooling rather than two layers

559
00:24:34,530 --> 00:24:37,170
in a pooling or even one.

560
00:24:37,170 --> 00:24:39,210
So it's actually
pretty remarkable

561
00:24:39,210 --> 00:24:42,870
that there's only basically
three different types of layers

562
00:24:42,870 --> 00:24:46,320
in these models, but they
perform extremely well

563
00:24:46,320 --> 00:24:49,470
compared to anything people
had tried before at this point.

564
00:24:49,470 --> 00:24:52,638
So these are, I would
say, the simplest models

565
00:24:52,638 --> 00:24:54,180
we're going to
discuss today, but you

566
00:24:54,180 --> 00:24:57,490
might ask why are they
doing 3 by 3 convolutions.

567
00:24:57,490 --> 00:24:59,740
How do they pick this value?

568
00:24:59,740 --> 00:25:02,790
And there is actually
some intuition

569
00:25:02,790 --> 00:25:08,640
behind how they chose 3
by 3 and specifically they

570
00:25:08,640 --> 00:25:10,810
have groups of three
or even four of these.

571
00:25:10,810 --> 00:25:13,470
So I'll ask you all a question.

572
00:25:13,470 --> 00:25:16,060
What is the effective
receptive field?

573
00:25:16,060 --> 00:25:19,480
So we looked at receptive
field last time,

574
00:25:19,480 --> 00:25:22,610
but it's basically the idea of
the parts of your input image

575
00:25:22,610 --> 00:25:26,550
that a particular value in your
activation map has seen before.

576
00:25:26,550 --> 00:25:30,020
So what values have been used
to compute the final activation

577
00:25:30,020 --> 00:25:32,130
map after many
layers of your model?

578
00:25:32,130 --> 00:25:34,130
So you have three
of these layers that

579
00:25:34,130 --> 00:25:36,980
are all 3 by 3 convolutions
with the sliding filter

580
00:25:36,980 --> 00:25:38,280
with stride of 1.

581
00:25:38,280 --> 00:25:39,980
What is the effective
receptive field

582
00:25:39,980 --> 00:25:43,740
of each value in our
activation map A3 here?

583
00:25:43,740 --> 00:25:46,340
So this is after
the third layer.

584
00:25:46,340 --> 00:25:48,990
So I'm showing one
of the layers here.

585
00:25:48,990 --> 00:25:52,640
You can see for each value in A3
it's computed by looking at a 3

586
00:25:52,640 --> 00:25:54,810
by 3 grid of values in A2.

587
00:25:54,810 --> 00:25:57,110
And then conceivably for
each one in A2 it's a 3

588
00:25:57,110 --> 00:25:58,530
by 3 grid in A1.

589
00:25:58,530 --> 00:26:01,680
And for each of those it's
a 3 by 3 grid in our input.

590
00:26:01,680 --> 00:26:05,420
So I'll let you all think
about this for a little bit.

591
00:26:05,420 --> 00:26:09,275
Maybe it will help to
see the next layer here.

592
00:26:09,275 --> 00:26:11,400
So it is actually really
helpful to visualize this.

593
00:26:11,400 --> 00:26:15,410
So at A1 each of
the corners here,

594
00:26:15,410 --> 00:26:21,030
we're calculating from
a new 3 by 3 grid here.

595
00:26:21,030 --> 00:26:26,290
So from our input, how large
is this overall square?

596
00:26:26,290 --> 00:26:27,160
7 by 7.

597
00:26:27,160 --> 00:26:27,940
Yeah, exactly.

598
00:26:27,940 --> 00:26:29,650
So this first one is 3 by 3.

599
00:26:29,650 --> 00:26:31,980
This next one is 5 by 5 And?

600
00:26:31,980 --> 00:26:33,370
Then the next one is 7 by 7.

601
00:26:33,370 --> 00:26:36,400
And we can visualize
it here pretty easily.

602
00:26:36,400 --> 00:26:39,240
So the nice thing about the 3
by 3 convolution with stride one

603
00:26:39,240 --> 00:26:42,270
is that you're basically
always adding 2

604
00:26:42,270 --> 00:26:44,590
to your receptive
field at each layer

605
00:26:44,590 --> 00:26:48,300
because each point here,
you're looking to the left

606
00:26:48,300 --> 00:26:50,050
and to the right and
above and below it.

607
00:26:50,050 --> 00:26:51,745
So after you have
many blocks of those,

608
00:26:51,745 --> 00:26:53,120
you're just adding
two each time.

610
00:26:57,876 --> 00:27:00,780
So we basically just
showed that a stack

611
00:27:00,780 --> 00:27:04,050
of three of these 3 by 3
convolution and stride 1 layers

612
00:27:04,050 --> 00:27:08,192
has the same effective
field as one 7 by 7 layer.

613
00:27:08,192 --> 00:27:09,900
Yeah, so the question
is how much of this

614
00:27:09,900 --> 00:27:12,192
is justification after the
fact versus how much of this

615
00:27:12,192 --> 00:27:16,260
is intuition that they then use
to, say, design the experiments?

616
00:27:16,260 --> 00:27:18,860
I think it actually probably
depends on the architecture.

617
00:27:18,860 --> 00:27:20,990
So for some of them it's
more intuition focused.

618
00:27:20,990 --> 00:27:23,532
And actually the one we're going
to cover next I think really

619
00:27:23,532 --> 00:27:24,340
was--

620
00:27:24,340 --> 00:27:27,070
the whole research direction was
spawned by an empirical finding

621
00:27:27,070 --> 00:27:28,820
that there was a thought
experiment about.

622
00:27:28,820 --> 00:27:30,653
So that's ResNets and
I think for there,

623
00:27:30,653 --> 00:27:32,320
there's actually a
pretty good intuition

624
00:27:32,320 --> 00:27:35,140
that led the whole investigation
into what will work well.

625
00:27:35,140 --> 00:27:40,600
But then for this one, I mean,
I can't speak for the authors

626
00:27:40,600 --> 00:27:43,450
whether it's justification
after the fact

627
00:27:43,450 --> 00:27:49,360
or based on empirical
findings or if it

628
00:27:49,360 --> 00:27:51,513
was involved in
the design choices

629
00:27:51,513 --> 00:27:53,930
because I haven't seen them
speak publicly about that yet.

630
00:27:53,930 --> 00:27:58,030
But for ResNets, I do know
it was the hypothesis that

631
00:27:58,030 --> 00:28:02,270
led to the creation, but this is
actually a really nice property.

632
00:28:02,270 --> 00:28:04,780
So three of these 3 by 3s have
the same effective receptive

633
00:28:04,780 --> 00:28:06,290
field as one 7 by 7 layer.

634
00:28:06,290 --> 00:28:08,330
And it actually has
fewer parameters too.

635
00:28:08,330 --> 00:28:13,570
So if you imagine our channel
dimension is staying the same,

636
00:28:13,570 --> 00:28:17,780
you have these 3 by 3
grids where you have

637
00:28:17,780 --> 00:28:19,470
your input number of channels.

638
00:28:19,470 --> 00:28:21,788
So 3 times 3 times C would
be the number of values

639
00:28:21,788 --> 00:28:22,830
in each of these filters.

640
00:28:22,830 --> 00:28:26,180
And if we have C of these
filters it's 3 times 3 times

641
00:28:26,180 --> 00:28:28,500
C times C or 3
squared C squared.

642
00:28:28,500 --> 00:28:30,150
And we have three layers total.

643
00:28:30,150 --> 00:28:35,120
So if we look at it
through this lens,

644
00:28:35,120 --> 00:28:37,080
it's actually fewer parameters.

645
00:28:37,080 --> 00:28:41,750
And we're having a more
complex and more nonlinear

646
00:28:41,750 --> 00:28:43,650
model that we're building here.

647
00:28:43,650 --> 00:28:47,180
So fewer parameters and it can
model more complex relationships

648
00:28:47,180 --> 00:28:49,460
among your input data.

649
00:28:49,460 --> 00:28:52,610
So this is maybe why
stacking these 3 by 3 layers

650
00:28:52,610 --> 00:28:56,600
together could be better than
having just a larger filter

651
00:28:56,600 --> 00:28:57,750
that you're sliding across.

652
00:28:57,750 --> 00:28:59,630
So fewer parameters
and then also

653
00:28:59,630 --> 00:29:02,990
you can model more
complex relationships.

654
00:29:02,990 --> 00:29:05,690
OK, I'll now talk
about ResNets, which

655
00:29:05,690 --> 00:29:07,700
I guess I'll very much
bring up the thought

656
00:29:07,700 --> 00:29:09,930
experiment that someone
just asked a question about.

657
00:29:09,930 --> 00:29:13,590
So there was actually
an empirical finding

658
00:29:13,590 --> 00:29:16,530
that spawned a lot
of the conversation

659
00:29:16,530 --> 00:29:19,240
and thought around
designing ResNets.

660
00:29:19,240 --> 00:29:23,820
And the idea was actually shown
that if you keep stacking deeper

661
00:29:23,820 --> 00:29:27,497
layers on a plain CNN
network, something like this

662
00:29:27,497 --> 00:29:29,080
and you just keep
adding layers to it,

663
00:29:29,080 --> 00:29:31,540
you build it larger and
larger, what happens?

664
00:29:31,540 --> 00:29:33,870
So what we found
and what they found

665
00:29:33,870 --> 00:29:38,040
is that the 20 layer model
will actually have a lower test

666
00:29:38,040 --> 00:29:40,030
error than a 56 layer model.

667
00:29:40,030 --> 00:29:42,715
And you might think that this
is because of overfitting,

668
00:29:42,715 --> 00:29:45,340
but it's actually not because if
we look at the training error,

669
00:29:45,340 --> 00:29:48,400
the training error of this
20 layer model is also lower.

670
00:29:48,400 --> 00:29:51,930
So it has a lower training and
a lower test error basically

671
00:29:51,930 --> 00:29:55,410
means that the model is
doing better on all accounts.

672
00:29:55,410 --> 00:29:59,640
So why is this 56 layer
model performing worse

673
00:29:59,640 --> 00:30:02,940
than a 20 layer model?

674
00:30:02,940 --> 00:30:04,540
It might be confusing.

675
00:30:04,540 --> 00:30:07,080
And we know as I
mentioned before, it's not

676
00:30:07,080 --> 00:30:09,240
caused by overfitting.

677
00:30:09,240 --> 00:30:12,450
So these deeper models have
more representational power,

678
00:30:12,450 --> 00:30:16,160
and theoretically they
should be able to represent

679
00:30:16,160 --> 00:30:20,610
any model that a more
shallow network can model.

680
00:30:20,610 --> 00:30:25,370
So the set of possible
mappings between your inputs

681
00:30:25,370 --> 00:30:28,370
and different values
for your larger networks

682
00:30:28,370 --> 00:30:31,410
is a superset for
your smaller networks

683
00:30:31,410 --> 00:30:35,318
because theoretically you could
imagine that you basically

684
00:30:35,318 --> 00:30:36,860
are just setting
some of these layers

685
00:30:36,860 --> 00:30:40,310
to be the identity
function where

686
00:30:40,310 --> 00:30:41,880
the layer is doing nothing.

687
00:30:41,880 --> 00:30:44,300
And then if you set half
your layers to do nothing,

688
00:30:44,300 --> 00:30:46,460
you have exactly the
same representation power

689
00:30:46,460 --> 00:30:49,080
as a model one half the size.

690
00:30:49,080 --> 00:30:53,570
So the idea is not
that these models

691
00:30:53,570 --> 00:30:56,760
are worse in terms of the
representational power,

692
00:30:56,760 --> 00:30:58,970
but they're actually
harder to optimize

693
00:30:58,970 --> 00:31:03,080
because the set of possible
models for your deeper networks

694
00:31:03,080 --> 00:31:05,690
is larger and it contains
all of the possible models

695
00:31:05,690 --> 00:31:11,280
that your more shallow
networks could learn.

696
00:31:11,280 --> 00:31:16,290
So I hinted at it before,
but how specifically could

697
00:31:16,290 --> 00:31:17,970
the deeper model learn
to be at least as

698
00:31:17,970 --> 00:31:19,330
good as a shallow model?

699
00:31:19,330 --> 00:31:20,440
It's by setting.

700
00:31:20,440 --> 00:31:23,670
So if we have a two layer model
versus a one layer, so one layer

701
00:31:23,670 --> 00:31:26,190
here and two layer on the right,
if we set one of the layers

702
00:31:26,190 --> 00:31:29,670
to just essentially be an
identity matrix or this

703
00:31:29,670 --> 00:31:32,700
is just an identity
function, The?

704
00:31:32,700 --> 00:31:36,630
Model should be at least as
good as the shallow model.

705
00:31:36,630 --> 00:31:40,290
It should be at least as
good as the shallow model.

706
00:31:40,290 --> 00:31:43,720
So how do we actually build
this intuition into our models?

707
00:31:43,720 --> 00:31:45,810
We want them to be
able to be just as

708
00:31:45,810 --> 00:31:48,000
good as a shallower
model if they

709
00:31:48,000 --> 00:31:50,500
want to be during optimization.

710
00:31:50,500 --> 00:31:53,100
So the way that we
do this is actually

711
00:31:53,100 --> 00:31:56,070
by fitting what's called
a residual mapping instead

712
00:31:56,070 --> 00:32:00,580
of directly trying to fit a
desired underlying mapping.

713
00:32:00,580 --> 00:32:04,170
And what this looks like is
we basically take the value x

714
00:32:04,170 --> 00:32:07,830
and we copy it over past
our convolution layers

715
00:32:07,830 --> 00:32:12,310
so that the value at
this point is already

716
00:32:12,310 --> 00:32:16,810
receiving x, our original input,
as well as the output of our two

717
00:32:16,810 --> 00:32:18,080
convolution stacks.

718
00:32:18,080 --> 00:32:20,530
So basically at
this point this F

719
00:32:20,530 --> 00:32:23,590
of x, which is called
the residual map here,

720
00:32:23,590 --> 00:32:29,870
it could just learn zero values
for all the conv filters.

721
00:32:29,870 --> 00:32:31,670
And the output would be 0 here.

722
00:32:31,670 --> 00:32:35,120
And then we would just add x
along here and we would get x.

723
00:32:35,120 --> 00:32:37,660
So it allows a very
simple way for the model

724
00:32:37,660 --> 00:32:40,690
to bypass these layers if it
doesn't need to learn anything

725
00:32:40,690 --> 00:32:41,480
for the layers.

726
00:32:41,480 --> 00:32:45,190
And what this means is you
can really easily now learn

727
00:32:45,190 --> 00:32:47,920
basically this identity function
that we talked about earlier

728
00:32:47,920 --> 00:32:51,440
by just learning zero filters.

729
00:32:51,440 --> 00:32:53,450
So your filters all are
filled with zero values,

730
00:32:53,450 --> 00:32:57,725
for example or in this
case more practically

731
00:32:57,725 --> 00:32:59,350
what happens is they
just need to learn

732
00:32:59,350 --> 00:33:03,025
very small values because
instead of learning

733
00:33:03,025 --> 00:33:06,310
this entire mapping from
x to h of x, they just

734
00:33:06,310 --> 00:33:08,500
need to learn this
difference, which is F of x.

735
00:33:08,500 --> 00:33:11,850
So you're now just
learning this sort

736
00:33:11,850 --> 00:33:14,370
of difference between
your desired output

737
00:33:14,370 --> 00:33:18,220
here and the copied over block.

738
00:33:18,220 --> 00:33:22,150
This is called a residual
block or residual connection

739
00:33:22,150 --> 00:33:24,360
when you copy over values
from an earlier layer

740
00:33:24,360 --> 00:33:26,910
into a later layer in
your model and then

741
00:33:26,910 --> 00:33:30,720
you just add it to the
values at that point.

742
00:33:30,720 --> 00:33:32,830
So I talked a bit
about the intuition,

743
00:33:32,830 --> 00:33:36,240
which was this observed
phenomenon that these larger

744
00:33:36,240 --> 00:33:39,810
networks were achieving worse
training and worse test error

745
00:33:39,810 --> 00:33:41,560
because they were
harder to optimize.

746
00:33:41,560 --> 00:33:44,760
So the intuition was we need to
build a model that can really

747
00:33:44,760 --> 00:33:47,070
easily model the
shallower networks so it

748
00:33:47,070 --> 00:33:49,390
can be at least as good
as a shallower model.

749
00:33:49,390 --> 00:33:51,990
The way they did this was by
adding a residual connection so

750
00:33:51,990 --> 00:33:54,683
that you can just copy
over the values easily,

751
00:33:54,683 --> 00:33:56,350
build that into the
architecture itself,

752
00:33:56,350 --> 00:33:59,070
rather than trying to
learn some identity mapping

753
00:33:59,070 --> 00:34:00,550
among the convolutional layers.

754
00:34:00,550 --> 00:34:04,470
And empirically, this showed
to work extremely well too.

755
00:34:04,470 --> 00:34:06,500
Yeah, so what is the
residual block carry?

756
00:34:06,500 --> 00:34:07,940
So we have our input x.

757
00:34:07,940 --> 00:34:10,550
We pass it through two
different convolutional layers

758
00:34:10,550 --> 00:34:12,670
and we get our output F of x.

759
00:34:12,670 --> 00:34:14,900
x is just copied over here.

760
00:34:14,900 --> 00:34:17,050
So this is exactly the
same as x and we add it

761
00:34:17,050 --> 00:34:21,820
to the output of these two
blocks which is F of x.

762
00:34:21,820 --> 00:34:24,679
Yeah, x is the output of
one of the previous layers

763
00:34:24,679 --> 00:34:26,739
or if it's the very first
layer of the model it

764
00:34:26,739 --> 00:34:28,659
would be the image.

765
00:34:28,659 --> 00:34:30,460
Yeah, so the question
is if maybe you just

766
00:34:30,460 --> 00:34:32,627
don't have enough data and
if you added enough data,

767
00:34:32,627 --> 00:34:35,440
then maybe you could train a
model without these blocks.

768
00:34:35,440 --> 00:34:38,320
I think these blocks actually
do extremely help you

769
00:34:38,320 --> 00:34:40,840
with learning from more data.

770
00:34:40,840 --> 00:34:43,610
I think the issue, it was
really an optimization problem.

771
00:34:43,610 --> 00:34:45,280
So transformers
used residual blocks

772
00:34:45,280 --> 00:34:47,505
for exactly the same
reason because--

773
00:34:47,505 --> 00:34:48,880
and I think it
actually helps you

774
00:34:48,880 --> 00:34:51,580
model these more complex
models and it actually

775
00:34:51,580 --> 00:34:53,000
enables you to use more data.

776
00:34:53,000 --> 00:34:55,427
So I think it's very good.

777
00:34:55,427 --> 00:34:57,010
Residual blocks help
you use more data

778
00:34:57,010 --> 00:35:00,650
more efficiently because you're
able to model a greater number--

779
00:35:00,650 --> 00:35:04,970
you're able to more easily model
a greater number of functions.

780
00:35:04,970 --> 00:35:09,260
Yeah, so the question
is that maybe if we just

781
00:35:09,260 --> 00:35:13,070
trained for longer, the
performance would eventually

782
00:35:13,070 --> 00:35:15,188
converge to the value
of the smaller network.

783
00:35:15,188 --> 00:35:17,480
And maybe it's just harder
to optimize because it takes

784
00:35:17,480 --> 00:35:20,000
longer to train a larger model.

785
00:35:20,000 --> 00:35:25,760
And I think the answer
is no, that they were not

786
00:35:25,760 --> 00:35:28,507
converging to the performance
of the smaller model,

787
00:35:28,507 --> 00:35:30,090
regardless of how
long you trained it.

788
00:35:30,090 --> 00:35:32,390
And the reason is
because it's being stuck

789
00:35:32,390 --> 00:35:36,110
in essentially local minimum.

790
00:35:36,110 --> 00:35:39,120
And when you add these
residual connections,

791
00:35:39,120 --> 00:35:41,430
you're avoiding these.

792
00:35:41,430 --> 00:35:44,970
This is the actual explanation
why this is the case is still,

793
00:35:44,970 --> 00:35:48,950
I would say, being a more
active area of research.

794
00:35:48,950 --> 00:35:51,350
It's really hard to
understand exactly what causes

795
00:35:51,350 --> 00:35:56,060
these models to avoid
a global or, sorry,

796
00:35:56,060 --> 00:35:58,830
to avoid local minimum and
not get an optimal solution

797
00:35:58,830 --> 00:36:03,730
or what causes them to not
train and find better solutions.

798
00:36:03,730 --> 00:36:06,130
And oftentimes this is
really an empirical finding,

799
00:36:06,130 --> 00:36:07,690
but there's some
intuition behind it.

800
00:36:07,690 --> 00:36:09,150
And in this case,
the intuition was

801
00:36:09,150 --> 00:36:11,612
that we want to enable our
models to do at least as

802
00:36:11,612 --> 00:36:14,070
well as the shallower models,
which we know were performing

803
00:36:14,070 --> 00:36:15,300
better at the time.

804
00:36:15,300 --> 00:36:17,520
So it's not that you could
just train it for longer

805
00:36:17,520 --> 00:36:19,920
and it would do better.

806
00:36:19,920 --> 00:36:23,940
It was actually a
limitation it was just

807
00:36:23,940 --> 00:36:26,780
completely unable to achieve as
good as the shallower models.

809
00:36:31,590 --> 00:36:38,100
OK, so here's the overall
ResNet architecture.

810
00:36:38,100 --> 00:36:41,050
We have these stacks
of residual blocks now.

811
00:36:41,050 --> 00:36:44,700
So that's what these two
blocks here together mean.

812
00:36:44,700 --> 00:36:47,500
It's a residual block.

813
00:36:47,500 --> 00:36:49,860
So we have a 3 by 3 convolution
with a ReLU followed

814
00:36:49,860 --> 00:36:51,730
by another 3 by 3 convolution.

815
00:36:51,730 --> 00:36:54,172
And we're copying over
this x value here.

816
00:36:54,172 --> 00:36:55,630
We're adding it to
the outputs here

817
00:36:55,630 --> 00:36:57,297
and then we're having
a ReLU afterwards.

818
00:36:57,297 --> 00:37:00,975
So each of these pairs of
blocks is one of these residual.

819
00:37:00,975 --> 00:37:03,100
And that's why you see this
line skipping over here

820
00:37:03,100 --> 00:37:06,370
because the value is
getting added forward.

821
00:37:06,370 --> 00:37:08,620
The cool thing
about ResNets also

822
00:37:08,620 --> 00:37:14,150
is that they basically had a
lot of these different depths

823
00:37:14,150 --> 00:37:14,900
that they created.

824
00:37:14,900 --> 00:37:16,608
So they created a
whole family of models,

825
00:37:16,608 --> 00:37:17,980
some smaller and some larger.

826
00:37:17,980 --> 00:37:20,480
And they showed that as they
increased the number of layers,

827
00:37:20,480 --> 00:37:22,100
their performance
was increasing,

828
00:37:22,100 --> 00:37:25,390
albeit the difference
in performance

829
00:37:25,390 --> 00:37:28,010
as you got to the larger and
larger models became smaller.

830
00:37:28,010 --> 00:37:31,700
So it was reaching a point
of, given the data set,

831
00:37:31,700 --> 00:37:35,530
they weren't able to scale any
significant amount by adding

832
00:37:35,530 --> 00:37:38,110
more layers further
than that, but they

833
00:37:38,110 --> 00:37:40,658
saw significant improvements
in performance among especially

834
00:37:40,658 --> 00:37:41,450
the earlier models.

835
00:37:41,450 --> 00:37:43,877
And then 101 to 152 is
where the performance,

836
00:37:43,877 --> 00:37:44,960
it wasn't really changing.

837
00:37:44,960 --> 00:37:47,860
It was marginally better,
but performance changes maybe

838
00:37:47,860 --> 00:37:49,990
only 1% at that point.

839
00:37:49,990 --> 00:37:51,800
Yeah, how did they
get the number of 152?

840
00:37:51,800 --> 00:37:54,520
I actually don't know how
they got the number of 152.

841
00:37:54,520 --> 00:37:58,190
I think they wanted to
try different values here.

842
00:37:58,190 --> 00:37:59,170
And you can see that--

843
00:37:59,170 --> 00:38:00,760
I mean, they're not
exactly doubling,

844
00:38:00,760 --> 00:38:05,350
but there's a significant
increase in each time.

845
00:38:05,350 --> 00:38:07,400
I don't know how
they picked 152.

846
00:38:07,400 --> 00:38:08,620
That's a good question.

847
00:38:08,620 --> 00:38:11,700
Maybe they showed it somehow
worked better than other.

848
00:38:11,700 --> 00:38:13,650
I actually don't though.

849
00:38:13,650 --> 00:38:16,830
So generally when you're trying
multiple different number

850
00:38:16,830 --> 00:38:18,970
of layers for your
model, given that

851
00:38:18,970 --> 00:38:21,720
say these are the number
of layers you want to try,

852
00:38:21,720 --> 00:38:24,940
what you'll do is you'll first
train the smallest model,

853
00:38:24,940 --> 00:38:26,800
see performance and
then add more layers,

854
00:38:26,800 --> 00:38:29,230
see if your performance
increases and go on so forth.

855
00:38:29,230 --> 00:38:31,170
So that's probably why
they stopped at 152

856
00:38:31,170 --> 00:38:34,410
is because performance wasn't
increased as much anymore.

857
00:38:34,410 --> 00:38:36,670
And also there's GPU
memory limitations

858
00:38:36,670 --> 00:38:38,470
so as you get larger
and larger models,

859
00:38:38,470 --> 00:38:40,720
it becomes harder to train
from a hardware perspective

860
00:38:40,720 --> 00:38:43,450
because you need to fit more
parameters into your GPU memory.

861
00:38:43,450 --> 00:38:47,040
So there is a limit for, given
your compute setup, how large

862
00:38:47,040 --> 00:38:49,080
of a model you can train.

863
00:38:49,080 --> 00:38:51,940
I think you need to train
these models separately though,

864
00:38:51,940 --> 00:38:55,710
so you have one model
run for 18 layers and one

865
00:38:55,710 --> 00:38:57,790
for 34, et cetera.

866
00:38:57,790 --> 00:39:03,100
So the question is how to think
of intuition of CNN blocks,

867
00:39:03,100 --> 00:39:07,270
given we're using these
residual connections because you

868
00:39:07,270 --> 00:39:09,568
can still think of it as
higher levels of abstraction.

869
00:39:09,568 --> 00:39:11,360
And this is shown to
be true in the layers.

870
00:39:11,360 --> 00:39:16,020
So instead of learning--

871
00:39:16,020 --> 00:39:18,520
within the block itself, instead
of just learning the higher

872
00:39:18,520 --> 00:39:20,228
level features, you're
learning the delta

873
00:39:20,228 --> 00:39:23,180
from the original image to
get the higher level features.

874
00:39:23,180 --> 00:39:24,950
That's what you're
learning in the block.

875
00:39:24,950 --> 00:39:27,280
So you're learning the
delta, but you're still

876
00:39:27,280 --> 00:39:29,900
achieving these higher level
representations at each step.

877
00:39:29,900 --> 00:39:32,500
So that part is the same,
but the actual functional way

878
00:39:32,500 --> 00:39:38,560
of doing it is you learn
this F of x that you

879
00:39:38,560 --> 00:39:41,120
add your previous input to.

880
00:39:41,120 --> 00:39:43,270
So it's like you're
learning the delta.

881
00:39:43,270 --> 00:39:44,785
The question is,
if you do addition,

882
00:39:44,785 --> 00:39:46,910
does that require you to
have the same tensor size?

883
00:39:46,910 --> 00:39:47,690
The answer is yes.

884
00:39:47,690 --> 00:39:51,203
And it's part of the
reason why it's really

885
00:39:51,203 --> 00:39:52,870
the nice property
that all of these ones

886
00:39:52,870 --> 00:39:57,590
have this 3 by 3 convolution
with stride 1 and 1 padding

887
00:39:57,590 --> 00:40:01,370
so that you maintain the same
size at every layer going

888
00:40:01,370 --> 00:40:01,890
forward.

889
00:40:01,890 --> 00:40:05,443
So after you had, say,
a pooling layer, I mean,

890
00:40:05,443 --> 00:40:06,860
you could maybe
come up with a way

891
00:40:06,860 --> 00:40:12,710
to do it where you
unspool the values,

892
00:40:12,710 --> 00:40:14,343
but after pooling
layer, for example,

893
00:40:14,343 --> 00:40:16,010
you couldn't do a
naive addition anymore

894
00:40:16,010 --> 00:40:18,570
because the size of your
tensors are different.

895
00:40:18,570 --> 00:40:23,925
So these are done before a
pool, at least the regular one.

896
00:40:23,925 --> 00:40:25,550
I mean, you could
get around it by just

897
00:40:25,550 --> 00:40:28,800
having each value be spread
out into multiple values,

898
00:40:28,800 --> 00:40:30,710
for example.

899
00:40:30,710 --> 00:40:35,390
OK, so these are basically the
main takeaways for ResNets.

900
00:40:35,390 --> 00:40:38,180
One other neat trick
they do is they basically

901
00:40:38,180 --> 00:40:40,620
periodically after a certain
number of these blocks,

902
00:40:40,620 --> 00:40:42,770
they'll double the
number of filters

903
00:40:42,770 --> 00:40:46,040
and downsample the
spatial dimension.

904
00:40:46,040 --> 00:40:48,170
So basically you
can imagine if you

905
00:40:48,170 --> 00:40:53,525
start with a really flat
image, as the activations get

906
00:40:53,525 --> 00:40:55,900
pushed through the network,
it becomes smaller spatially,

907
00:40:55,900 --> 00:40:57,880
but then the depth is larger.

908
00:40:57,880 --> 00:40:59,195
So this is how to think of it.

909
00:40:59,195 --> 00:41:00,570
And then at the
very end, it just

910
00:41:00,570 --> 00:41:03,243
becomes a vector that you
then use for classification.

911
00:41:03,243 --> 00:41:05,160
So that's how you should
be visualizing what's

912
00:41:05,160 --> 00:41:07,650
happening to the values
in the network itself

913
00:41:07,650 --> 00:41:08,740
and the shape of them.

914
00:41:08,740 --> 00:41:11,280
And then one other
sort of thing that's

915
00:41:11,280 --> 00:41:14,110
somewhat unique to ResNets,
other architectures do this too,

916
00:41:14,110 --> 00:41:17,740
but before all these layers
with the residual blocks,

917
00:41:17,740 --> 00:41:21,520
they have this relatively
larger convolutional layer.

918
00:41:21,520 --> 00:41:23,040
And here is just
empirically shown

919
00:41:23,040 --> 00:41:25,240
that it did better if
they added this here.

920
00:41:25,240 --> 00:41:29,610
So this one is purely
empirical finding.

921
00:41:29,610 --> 00:41:34,890
OK, I think basically
to highlight,

922
00:41:34,890 --> 00:41:37,230
it did extremely well,
these larger models.

923
00:41:37,230 --> 00:41:39,870
It was the first time they were
able to train 100 plus layer

924
00:41:39,870 --> 00:41:42,280
models successfully so
it was a really big deal.

925
00:41:42,280 --> 00:41:45,810
And basically ResNets were used
in a huge variety of computer

926
00:41:45,810 --> 00:41:47,320
vision tasks afterwards.

927
00:41:47,320 --> 00:41:48,848
Almost every task
in computer vision

928
00:41:48,848 --> 00:41:50,640
was using a ResNet at
the time because they

929
00:41:50,640 --> 00:41:54,890
performed so well because of
these residual connections.

930
00:41:54,890 --> 00:41:59,370
So we talked about some CNN
architectures, the main one

931
00:41:59,370 --> 00:42:02,710
being ResNet and then
also VGG historically.

932
00:42:02,710 --> 00:42:05,800
So we talked about why the
smaller filter size is useful

933
00:42:05,800 --> 00:42:07,780
and having many layers
of these is useful.

934
00:42:07,780 --> 00:42:09,780
So the final thing I'll
talk about in terms

935
00:42:09,780 --> 00:42:11,910
of how we actually
construct the CNNs

936
00:42:11,910 --> 00:42:15,120
and prime them to be ready for
training is how do you actually

937
00:42:15,120 --> 00:42:19,170
initialize the weight values
of the individual layers?

938
00:42:19,170 --> 00:42:24,010
So depending on what
values you choose,

939
00:42:24,010 --> 00:42:27,370
you could either put values
that are too small or too large,

940
00:42:27,370 --> 00:42:29,880
which would cause significant
issues for your model

941
00:42:29,880 --> 00:42:30,820
during training.

942
00:42:30,820 --> 00:42:36,360
So here it's basically a six
layer network where we have

943
00:42:36,360 --> 00:42:40,000
4,096 dimensional features.

944
00:42:40,000 --> 00:42:43,600
And this is just six layers
of fully connected model

945
00:42:43,600 --> 00:42:45,330
and we initialize them.

946
00:42:45,330 --> 00:42:48,400
Here this is getting
a unit Gaussian random

947
00:42:48,400 --> 00:42:50,610
and then we
multiplying it by 0.01

948
00:42:50,610 --> 00:42:53,560
to get very small
values close to 0

949
00:42:53,560 --> 00:42:55,720
and we have ReLU
at each layer too.

950
00:42:55,720 --> 00:43:00,340
So if you plot the forward
pass of this model,

951
00:43:00,340 --> 00:43:02,400
you would actually see
that at the beginning

952
00:43:02,400 --> 00:43:06,280
you get a relatively
high-- because there's ReLU

953
00:43:06,280 --> 00:43:08,260
so all the means are
going to be positive,

954
00:43:08,260 --> 00:43:10,890
but you'll have a mean
and standard deviation

955
00:43:10,890 --> 00:43:12,130
that is relatively high.

956
00:43:12,130 --> 00:43:14,790
But as each layer
progresses, because we

957
00:43:14,790 --> 00:43:17,160
had a really small
weight initialization,

958
00:43:17,160 --> 00:43:20,120
it becomes smaller and smaller
mean and standard deviation.

959
00:43:20,120 --> 00:43:21,870
And really ideally we
would want basically

960
00:43:21,870 --> 00:43:24,360
all of these to be the
same for each layer

961
00:43:24,360 --> 00:43:26,460
because it makes our
optimization problem much

962
00:43:26,460 --> 00:43:28,860
nicer to solve.

963
00:43:28,860 --> 00:43:33,690
So if we say use
0.05 instead of 0.01,

964
00:43:33,690 --> 00:43:36,780
can anyone imagine what might
be the issue here if we set it

965
00:43:36,780 --> 00:43:38,925
to too large of a value?

966
00:43:38,925 --> 00:43:42,540
So when it's too small,
it goes to 0, basically.

967
00:43:42,540 --> 00:43:44,970
What happens if it's too large?

968
00:43:44,970 --> 00:43:47,580
Yeah, basically the activations
get larger and larger

969
00:43:47,580 --> 00:43:48,400
at each layer.

970
00:43:48,400 --> 00:43:51,730
So if you plot it here,
you can see that by the end

971
00:43:51,730 --> 00:43:54,050
there's some massive mean
and standard deviation.

972
00:43:54,050 --> 00:43:57,910
And if you're training
152 layer ResNet and you

973
00:43:57,910 --> 00:44:00,610
can imagine that this becomes
quite an issue very quickly.

974
00:44:00,610 --> 00:44:03,980
So how do you actually do this?

975
00:44:03,980 --> 00:44:07,240
So in this case maybe
the optimal value I think

976
00:44:07,240 --> 00:44:10,600
is 0.022 or something, but how
would you actually know that

977
00:44:10,600 --> 00:44:13,480
and how would you do this more
generally across any layer?

978
00:44:13,480 --> 00:44:16,610
There are a few different ways
you can initialize weights.

979
00:44:16,610 --> 00:44:18,430
And I'll go over the
most commonly used one

980
00:44:18,430 --> 00:44:20,510
today in class, but no,
there are other ones.

981
00:44:20,510 --> 00:44:22,930
And generally what
they're a function of

982
00:44:22,930 --> 00:44:27,380
is the dimension of
your values here.

983
00:44:27,380 --> 00:44:33,100
So you'll have a different value
for a 4,096 dimension fully

984
00:44:33,100 --> 00:44:34,880
connected layer versus a 2048.

985
00:44:34,880 --> 00:44:37,990
And the specific
formula we'll go through

986
00:44:37,990 --> 00:44:40,190
is called Kaiming
Initialization.

987
00:44:40,190 --> 00:44:42,980
It's actually the same
person who created ResNets.

988
00:44:42,980 --> 00:44:45,818
So Kaiming, he was very famous.

989
00:44:45,818 --> 00:44:47,860
I mean, he still is a very
famous computer vision

990
00:44:47,860 --> 00:44:48,440
researcher.

991
00:44:48,440 --> 00:44:50,523
I think he's one of the
most widely cited computer

992
00:44:50,523 --> 00:44:53,560
scientists of the last 10
or 15 years, maybe the most.

993
00:44:53,560 --> 00:44:57,730
So he's extremely well known in
the computer vision community.

994
00:44:57,730 --> 00:45:00,600
And he also came
up with this idea

995
00:45:00,600 --> 00:45:05,190
of initializing the values
to the square root of 2

996
00:45:05,190 --> 00:45:07,390
over your input dimension size.

997
00:45:07,390 --> 00:45:09,450
And I won't go over
all the details

998
00:45:09,450 --> 00:45:12,977
for how they derived this
and showed that with a ReLU

999
00:45:12,977 --> 00:45:15,060
activation, this will cause
the standard deviation

1000
00:45:15,060 --> 00:45:17,680
and mean to be relatively
constant throughout the layers.

1001
00:45:17,680 --> 00:45:20,297
But if you do plot it, you
see this does have the effect

1002
00:45:20,297 --> 00:45:22,380
so you can almost think
of this as a magic formula

1003
00:45:22,380 --> 00:45:24,505
where if you plug it in
get the desired properties.

1004
00:45:24,505 --> 00:45:26,590
And if you want to
know the derivation,

1005
00:45:26,590 --> 00:45:28,800
you can actually-- we
link the paper here so

1006
00:45:28,800 --> 00:45:31,690
feel free to look into that,
but you can just take our word.

1007
00:45:31,690 --> 00:45:33,190
I won't go through
the details here,

1008
00:45:33,190 --> 00:45:36,600
but it does this desired effect
where the mean and standard

1009
00:45:36,600 --> 00:45:37,990
deviation is unchanging.

1010
00:45:37,990 --> 00:45:40,180
And you can also imagine
that for any given setup,

1011
00:45:40,180 --> 00:45:42,340
you could also, just
through testing,

1012
00:45:42,340 --> 00:45:46,630
try to find what
is the value here.

1013
00:45:46,630 --> 00:45:50,140
OK, so these are
how you we discuss

1014
00:45:50,140 --> 00:45:52,270
how you initialize
weights, how you

1015
00:45:52,270 --> 00:45:53,980
combine these different
layers together

1016
00:45:53,980 --> 00:45:58,570
to form a CNN architecture,
which activation functions

1017
00:45:58,570 --> 00:46:01,010
people use, and then all the
different layers in CNNs.

1018
00:46:01,010 --> 00:46:03,260
So I think already we
covered quite a few topics.

1019
00:46:03,260 --> 00:46:04,990
So I think I'll
pause very briefly

1020
00:46:04,990 --> 00:46:06,920
to see if there's any
questions about these.

1021
00:46:06,920 --> 00:46:08,980
And the second part
of the lecture, it's

1022
00:46:08,980 --> 00:46:11,210
actually much less dense
than the first part.

1023
00:46:11,210 --> 00:46:13,120
So we'll be mainly
going over a lot

1024
00:46:13,120 --> 00:46:17,158
of nice practical tips for when
you're training these models.

1025
00:46:17,158 --> 00:46:19,450
So the question is, how do
you do weight initialization

1026
00:46:19,450 --> 00:46:20,180
for CNNs?

1027
00:46:20,180 --> 00:46:23,120
So you still use the
same initialization,

1028
00:46:23,120 --> 00:46:26,120
but your dimension n here
is the size of your kernel.

1029
00:46:26,120 --> 00:46:29,320
So if you have a 3 by
3 kernel with channels

1030
00:46:29,320 --> 00:46:33,830
say 6 would be 3 times 3 times
6, but it's the same idea.

1031
00:46:33,830 --> 00:46:36,460
It's just you calculate
your dimensions differently

1032
00:46:36,460 --> 00:46:40,120
depending on the layer,
the type of layer.

1033
00:46:40,120 --> 00:46:43,150
You can think of it as the
number of values roughly

1034
00:46:43,150 --> 00:46:46,220
in each operation, but it
does depend on the layer.

1035
00:46:46,220 --> 00:46:49,590
And some layers use different
weight initializations,

1036
00:46:49,590 --> 00:46:51,260
but this is
specifically for CNNs

1037
00:46:51,260 --> 00:46:54,170
how this initialization
applies to it.

1038
00:46:54,170 --> 00:46:56,600
Yeah, so the question is,
why do your activations

1039
00:46:56,600 --> 00:46:59,370
explode if you have too large
of an initialization value?

1040
00:46:59,370 --> 00:47:04,220
So you imagine at each layer
of your initialized network

1041
00:47:04,220 --> 00:47:08,070
you have a set of randomly
initialized values.

1042
00:47:08,070 --> 00:47:13,328
And if they're very large,
then when you do your--

1043
00:47:13,328 --> 00:47:15,630
you have a ReLU
activation afterwards.

1044
00:47:15,630 --> 00:47:21,330
And that doesn't actually cap
the outputs of your layer.

1045
00:47:21,330 --> 00:47:23,400
You can go to
infinity with ReLU.

1046
00:47:23,400 --> 00:47:27,050
So if you have too
large of a set of values

1047
00:47:27,050 --> 00:47:29,458
that you're essentially
repeating the same operation on

1048
00:47:29,458 --> 00:47:31,250
because you're initializing
all the weights

1049
00:47:31,250 --> 00:47:35,370
to the same set of random
values, then at each layer,

1050
00:47:35,370 --> 00:47:38,120
you'll be multiplying
one set of large values

1051
00:47:38,120 --> 00:47:40,730
by a set that's been
initialized to large so

1052
00:47:40,730 --> 00:47:44,620
that it becomes larger at
each iteration afterwards.

1053
00:47:44,620 --> 00:47:47,850
I mean, you could think of it as
it's like a recurrence relation

1054
00:47:47,850 --> 00:47:50,250
where because they're
all initialized randomly

1055
00:47:50,250 --> 00:47:53,650
at the start where it's
being multiplied by a value

1056
00:47:53,650 --> 00:47:55,860
and you would want in a
simple recurrence relation,

1057
00:47:55,860 --> 00:47:56,860
you'd want it to be one.

1058
00:47:56,860 --> 00:47:59,100
But because we have
a vector of values

1059
00:47:59,100 --> 00:48:01,912
that are being
multiplied by our matrix,

1060
00:48:01,912 --> 00:48:03,870
it depends on the dimension
of the vector, what

1061
00:48:03,870 --> 00:48:10,530
your average output would be
and after the ReLU because you

1062
00:48:10,530 --> 00:48:12,930
have basically a standard
deviation for what

1063
00:48:12,930 --> 00:48:14,440
is your activations.

1064
00:48:14,440 --> 00:48:16,110
Then you remove all
the negative ones

1065
00:48:16,110 --> 00:48:18,857
and you're left with what are
your outputs at that point

1066
00:48:18,857 --> 00:48:20,440
and if you have a
really large values,

1067
00:48:20,440 --> 00:48:22,232
you have a really large
standard deviation.

1068
00:48:22,232 --> 00:48:26,520
So when you remove
the bottom half of it,

1069
00:48:26,520 --> 00:48:29,910
your mean starts moving more
positive and more positive.

1070
00:48:29,910 --> 00:48:31,380
Did that make sense to--

1071
00:48:31,380 --> 00:48:33,147
I didn't have slides
to show it, but OK.

1072
00:48:33,147 --> 00:48:33,730
Mostly, sorry.

1073
00:48:33,730 --> 00:48:35,290
Yeah, you can see more
details in the paper.

1074
00:48:35,290 --> 00:48:37,040
It's actually not too
bad to read I think.

1076
00:48:40,950 --> 00:48:43,690
So the conclusion
of the discussion

1077
00:48:43,690 --> 00:48:47,297
here is that normalization would
solve this activation issue

1078
00:48:47,297 --> 00:48:48,880
where they're blowing
up, but it still

1079
00:48:48,880 --> 00:48:50,470
might be harder to optimize.

1080
00:48:50,470 --> 00:48:54,160
Maybe we should
probably do a follow up

1081
00:48:54,160 --> 00:48:55,948
post explaining
this in more detail,

1082
00:48:55,948 --> 00:48:58,240
but I think it's a really
good question actually, yeah.

1083
00:48:58,240 --> 00:49:00,198
It would solve, I think,
this particular issue,

1084
00:49:00,198 --> 00:49:03,790
but maybe it's still hard to
optimize this in the discussion.

1085
00:49:03,790 --> 00:49:06,125
Yeah, it's a good question.

1086
00:49:06,125 --> 00:49:06,625
OK.

1088
00:49:09,280 --> 00:49:10,150
Cool.

1089
00:49:10,150 --> 00:49:12,573
So I'll talk about
these steps now.

1090
00:49:12,573 --> 00:49:14,240
So how do you actually
train your model?

1091
00:49:14,240 --> 00:49:16,510
And the nice thing for
data pre-processing

1092
00:49:16,510 --> 00:49:17,930
is it's really easy for images.

1093
00:49:17,930 --> 00:49:19,970
So if you have your
giant image data set,

1094
00:49:19,970 --> 00:49:23,480
the standard way to do it is
you calculate the average red,

1095
00:49:23,480 --> 00:49:25,450
the average green, and
the average blue pixel

1096
00:49:25,450 --> 00:49:27,110
along with the
standard deviations.

1097
00:49:27,110 --> 00:49:30,100
And you take your input
image, you subtract the mean

1098
00:49:30,100 --> 00:49:32,300
and you divide by the
standard deviation.

1099
00:49:32,300 --> 00:49:36,220
And this is how you do data
normalization for images.

1100
00:49:36,220 --> 00:49:38,500
Is actually very
straightforward.

1101
00:49:38,500 --> 00:49:40,270
So it does require
you to pre-compute

1102
00:49:40,270 --> 00:49:44,100
the means and standard deviation
for each pixel channel.

1103
00:49:44,100 --> 00:49:45,750
So sometimes what
people will do is

1104
00:49:45,750 --> 00:49:48,490
they'll use means that have
already been calculated.

1105
00:49:48,490 --> 00:49:51,450
A very common one is
to use the ImageNet

1106
00:49:51,450 --> 00:49:54,715
means and standard
deviations and apply those

1107
00:49:54,715 --> 00:49:57,090
to your input images, even if
you're training a model not

1108
00:49:57,090 --> 00:49:58,410
on ImageNet.

1109
00:49:58,410 --> 00:50:05,080
So it is very data set dependent
is the way to think of this.

1110
00:50:05,080 --> 00:50:07,622
And different models will
use different values here

1111
00:50:07,622 --> 00:50:10,080
depending on their data set,
but the most commonly used one

1112
00:50:10,080 --> 00:50:11,940
is just use the mean
and standard deviation

1113
00:50:11,940 --> 00:50:14,040
from ImageNet.

1114
00:50:14,040 --> 00:50:16,980
Yeah, so any input image,
you apply this operation

1115
00:50:16,980 --> 00:50:18,815
before the model sees it.

1117
00:50:23,580 --> 00:50:25,650
So yeah, that one
was really quick.

1118
00:50:25,650 --> 00:50:27,460
And then in terms of
data augmentation,

1119
00:50:27,460 --> 00:50:31,170
so someone had a suggestion
earlier in the class

1120
00:50:31,170 --> 00:50:34,350
why don't we just add
noise to our image.

1121
00:50:34,350 --> 00:50:35,350
And that's a great idea.

1122
00:50:35,350 --> 00:50:37,020
And we'll talk about the
different ways you can add noise

1123
00:50:37,020 --> 00:50:37,962
to your image here.

1124
00:50:37,962 --> 00:50:40,420
This helps with regularization
and helps prevent your model

1125
00:50:40,420 --> 00:50:42,730
from overfitting.

1126
00:50:42,730 --> 00:50:47,110
So we talked about it before,
but this is a common pattern

1127
00:50:47,110 --> 00:50:49,270
with regularization,
where during training time

1128
00:50:49,270 --> 00:50:50,810
you add some kind of randomness.

1129
00:50:50,810 --> 00:50:53,990
And then at testing time you
then average out the randomness.

1130
00:50:53,990 --> 00:50:56,690
So sometimes this
is approximate,

1131
00:50:56,690 --> 00:51:00,070
but for example, for dropout,
we saw that during training time

1132
00:51:00,070 --> 00:51:02,930
we'll randomly drop, say,
50% of the activations.

1133
00:51:02,930 --> 00:51:05,470
And then at testing time
we'll use all the activations,

1134
00:51:05,470 --> 00:51:06,970
but then we'll need
to scale it down

1135
00:51:06,970 --> 00:51:09,490
by this probability
of dropout p.

1136
00:51:09,490 --> 00:51:11,540
So this is a really
common pattern.

1137
00:51:11,540 --> 00:51:13,760
And it's also used
for data augmentation.

1138
00:51:13,760 --> 00:51:21,010
So you can imagine this
being this cylinder here

1139
00:51:21,010 --> 00:51:22,010
is like your data set.

1140
00:51:22,010 --> 00:51:24,140
You load an image and a label.

1141
00:51:24,140 --> 00:51:26,800
So we have a cat label and
we have our original image

1142
00:51:26,800 --> 00:51:28,210
from our data set.

1143
00:51:28,210 --> 00:51:31,790
Before we actually pass it into
our model, it's extremely common

1144
00:51:31,790 --> 00:51:35,390
and basically always in
modern deep learning,

1145
00:51:35,390 --> 00:51:39,420
people will always use data
augmentation for training

1146
00:51:39,420 --> 00:51:40,600
computer vision models.

1147
00:51:40,600 --> 00:51:42,960
But the basic idea is to
apply some transformations

1148
00:51:42,960 --> 00:51:45,660
to the image to make it
look different but still

1149
00:51:45,660 --> 00:51:47,980
recognizable for
the category class

1150
00:51:47,980 --> 00:51:50,370
and then pass that to
your model and you're

1151
00:51:50,370 --> 00:51:51,610
computing the loss here.

1152
00:51:51,610 --> 00:51:53,100
So one of the nice
benefits of this

1153
00:51:53,100 --> 00:51:55,660
is you can effectively increase
the size of your data set

1154
00:51:55,660 --> 00:51:58,228
because instead of seeing
each image multiple times,

1155
00:51:58,228 --> 00:52:00,270
it will be seeing different
versions of the image

1156
00:52:00,270 --> 00:52:02,820
with different
transformations that all still

1157
00:52:02,820 --> 00:52:04,300
are the same category label.

1158
00:52:04,300 --> 00:52:06,450
So you can basically
get more data

1159
00:52:06,450 --> 00:52:09,310
and therefore it increases your
generalization capabilities,

1160
00:52:09,310 --> 00:52:12,937
but your training loss will be
higher because you're not just

1161
00:52:12,937 --> 00:52:14,770
seeing the same example
over and over again.

1162
00:52:14,770 --> 00:52:18,060
So it makes it harder for
the model to just memorize.

1163
00:52:18,060 --> 00:52:20,770
So how do we know the weight
initialization is just right?

1164
00:52:20,770 --> 00:52:23,320
So we know it's
right in this case,

1165
00:52:23,320 --> 00:52:25,470
because the means and
the standard deviations

1166
00:52:25,470 --> 00:52:28,290
are relatively constant
throughout the layers

1167
00:52:28,290 --> 00:52:28,990
of the network.

1168
00:52:28,990 --> 00:52:30,330
And we're not seeing--

1169
00:52:30,330 --> 00:52:34,570
in this case, we saw
mode collapse to 0.

1170
00:52:34,570 --> 00:52:38,470
In this case, it was
blowing up to infinity as we

1171
00:52:38,470 --> 00:52:40,660
increased the number of layers.

1172
00:52:40,660 --> 00:52:43,510
So the way you can
ensure it always happens

1173
00:52:43,510 --> 00:52:44,720
is by using the formula.

1174
00:52:44,720 --> 00:52:46,870
This will always
initialize them well.

1175
00:52:46,870 --> 00:52:49,100
So in practice that's
how people do it.

1176
00:52:49,100 --> 00:52:52,690
If you were creating
a new layer that

1177
00:52:52,690 --> 00:52:54,910
maybe does some different
kind of operation

1178
00:52:54,910 --> 00:52:56,890
that no one's done
before, then you probably

1179
00:52:56,890 --> 00:52:59,390
would need to try a bunch of
different weight initialization

1180
00:52:59,390 --> 00:53:01,390
schemes and see what works best.

1181
00:53:01,390 --> 00:53:04,480
But generally for
these linear layers

1182
00:53:04,480 --> 00:53:09,530
or the convolutional layers,
you can use this formula here,

1183
00:53:09,530 --> 00:53:14,110
which is called the
timing initialization.

1184
00:53:14,110 --> 00:53:14,980
OK.

1185
00:53:14,980 --> 00:53:17,967
So back to data
augmentation, so what

1186
00:53:17,967 --> 00:53:20,300
are the different augmentations
you can do specifically?

1187
00:53:20,300 --> 00:53:21,890
So one of them is
horizontal flipping.

1188
00:53:21,890 --> 00:53:23,710
This depends on the problem.

1189
00:53:23,710 --> 00:53:25,780
So if you want to
have a model that

1190
00:53:25,780 --> 00:53:28,450
reads text this would be
a very bad augmentation

1191
00:53:28,450 --> 00:53:30,250
to use because the
text is now-- it's

1192
00:53:30,250 --> 00:53:31,750
like you're looking
through a mirror

1193
00:53:31,750 --> 00:53:33,380
and you can't read it properly.

1194
00:53:33,380 --> 00:53:37,560
So this is sometimes useful
for everyday objects.

1195
00:53:37,560 --> 00:53:40,100
It's usually pretty good
because most objects

1196
00:53:40,100 --> 00:53:44,330
are symmetrical so this property
actually works pretty well.

1197
00:53:44,330 --> 00:53:46,640
And then you could also
imagine if you're looking maybe

1198
00:53:46,640 --> 00:53:48,907
at images from a
microscope or overhead

1199
00:53:48,907 --> 00:53:50,490
that you could also
do a vertical flip

1200
00:53:50,490 --> 00:53:51,573
and that would make sense.

1201
00:53:51,573 --> 00:53:53,990
But for everyday objects,
vertical flipping actually

1202
00:53:53,990 --> 00:53:56,240
doesn't really make sense
because a cat is almost

1203
00:53:56,240 --> 00:53:58,160
always seen in this
position, but maybe

1204
00:53:58,160 --> 00:53:59,330
if you had a data
set where cats were

1205
00:53:59,330 --> 00:54:00,440
in all different
orientations, you

1206
00:54:00,440 --> 00:54:02,990
could imagine that flipping or
rotating or all these things

1207
00:54:02,990 --> 00:54:06,860
would make sense
for your data set.

1208
00:54:06,860 --> 00:54:11,310
Another type of augmentation is
this resizing and cropping idea.

1209
00:54:11,310 --> 00:54:16,790
So what ResNets and
many different image

1210
00:54:16,790 --> 00:54:21,140
models in deep learning
do is they basically take

1211
00:54:21,140 --> 00:54:25,400
a random crop of the
image and then resize

1212
00:54:25,400 --> 00:54:27,105
that to be your image size.

1213
00:54:27,105 --> 00:54:28,980
They might even take
another crop afterwards.

1214
00:54:28,980 --> 00:54:30,740
So the most common
strategy is you

1215
00:54:30,740 --> 00:54:33,540
pick the length of
what is basically

1216
00:54:33,540 --> 00:54:35,490
the short side of your image.

1217
00:54:35,490 --> 00:54:40,230
So if you have a input image
size to your model of 224

1218
00:54:40,230 --> 00:54:43,980
by 224 pixels, you would pick
a value larger than this first

1219
00:54:43,980 --> 00:54:51,150
and find some crop of your image
that contains this larger scale

1220
00:54:51,150 --> 00:54:53,950
L and these are
commonly used values.

1221
00:54:53,950 --> 00:54:55,063
You crop the image.

1223
00:54:58,163 --> 00:54:59,080
Sorry, you don't crop.

1224
00:54:59,080 --> 00:55:01,960
You resize the image
to be that scale.

1225
00:55:01,960 --> 00:55:04,870
So instead say this
is a 800 by 600 image,

1226
00:55:04,870 --> 00:55:10,350
if we use 256 here we resize
the short side, so 600 to be 256

1227
00:55:10,350 --> 00:55:12,730
and then 800 would be
scaled correspondingly.

1228
00:55:12,730 --> 00:55:15,990
So we scale it to this L.
We scale the short side to L

1229
00:55:15,990 --> 00:55:19,200
and then we crop a
random patch of 224

1230
00:55:19,200 --> 00:55:21,190
by 224 pixels from that image.

1231
00:55:21,190 --> 00:55:24,070
So you're scaling the
image by first preserving--

1232
00:55:24,070 --> 00:55:26,350
you've preserved the
relative resolution,

1233
00:55:26,350 --> 00:55:29,620
but you make it smaller
or larger to fit this L

1234
00:55:29,620 --> 00:55:31,340
and then you take a
random crop of that.

1235
00:55:31,340 --> 00:55:35,073
And this is by far the
most commonly used--

1236
00:55:35,073 --> 00:55:37,490
random resized crop is what
it's called in most libraries.

1237
00:55:37,490 --> 00:55:40,032
So this is used in most problems
because it works pretty well

1238
00:55:40,032 --> 00:55:44,902
and it reserves the relative
resolution of your images.

1239
00:55:44,902 --> 00:55:46,360
And then there's
another neat trick

1240
00:55:46,360 --> 00:55:49,150
you can do with augmentation
called test time augmentation.

1241
00:55:49,150 --> 00:55:51,400
So if you really just want
to get the best performance

1242
00:55:51,400 --> 00:55:54,520
possible, you can basically get
a bunch of these different crops

1243
00:55:54,520 --> 00:55:56,860
and resizes and run them
all through your model

1244
00:55:56,860 --> 00:55:58,820
and then average your
predictions at the end.

1245
00:55:58,820 --> 00:56:01,000
And for ResNets,
people will often

1246
00:56:01,000 --> 00:56:04,240
try a bunch of different scales,
a bunch of different crop

1247
00:56:04,240 --> 00:56:07,910
locations and
maybe even flip it.

1248
00:56:07,910 --> 00:56:10,970
And usually you'll start
getting diminishing returns,

1249
00:56:10,970 --> 00:56:13,210
but you can get actually
pretty good 1% to 2%

1250
00:56:13,210 --> 00:56:15,890
performance boost by using this
sort of test time augmentation.

1251
00:56:15,890 --> 00:56:18,110
So if you're in a setting
where it really matters,

1252
00:56:18,110 --> 00:56:20,780
you're trying to eke out every
last bit of percentage points,

1253
00:56:20,780 --> 00:56:22,530
then this is actually
a really great trick

1254
00:56:22,530 --> 00:56:24,885
that you can use for almost
any computer vision problem.

1256
00:56:27,820 --> 00:56:31,200
OK, so a final
few augmentations.

1257
00:56:31,200 --> 00:56:34,310
One is color jitter, so
here we're specifically

1258
00:56:34,310 --> 00:56:36,320
randomizing the
contrast and brightness

1259
00:56:36,320 --> 00:56:37,920
and scaling the image
correspondingly.

1260
00:56:37,920 --> 00:56:40,850
So maybe images
look more muted or I

1261
00:56:40,850 --> 00:56:43,830
say the colors look more
muted or more brighter,

1262
00:56:43,830 --> 00:56:47,250
but these are very traditional
image processing techniques.

1263
00:56:47,250 --> 00:56:49,860
And usually with all these
different augmentations,

1264
00:56:49,860 --> 00:56:52,190
you'll try different
values and see

1265
00:56:52,190 --> 00:56:55,133
which ones make your images
still look in distribution

1266
00:56:55,133 --> 00:56:56,550
and look normal
to you as a human.

1267
00:56:56,550 --> 00:56:58,302
And that's a pretty
good way to judge

1268
00:56:58,302 --> 00:57:00,260
what values you should
pick for how much jitter

1269
00:57:00,260 --> 00:57:02,910
you should have, how much
brightness, variance, et cetera.

1270
00:57:02,910 --> 00:57:04,680
So normally when I'm
starting a problem,

1271
00:57:04,680 --> 00:57:06,390
I'll try a bunch of these
different augmentations.

1272
00:57:06,390 --> 00:57:08,090
I'll see what is
making the data look

1273
00:57:08,090 --> 00:57:09,840
different from
the original data,

1274
00:57:09,840 --> 00:57:13,130
but then still recognizable
to me and still very easy

1275
00:57:13,130 --> 00:57:13,800
to recognize.

1276
00:57:13,800 --> 00:57:18,080
And that's generally a good
set of augmentations to use.

1277
00:57:18,080 --> 00:57:19,970
Final one is you
can imagine just

1278
00:57:19,970 --> 00:57:21,470
like cropping out
parts of the image

1279
00:57:21,470 --> 00:57:22,970
where you just are
basically putting

1280
00:57:22,970 --> 00:57:25,130
a black or a gray box over it.

1281
00:57:25,130 --> 00:57:27,550
And I think this one's
maybe less commonly used,

1282
00:57:27,550 --> 00:57:29,050
but it kind of shows
you how you can

1283
00:57:29,050 --> 00:57:30,520
get creative with
the augmentations

1284
00:57:30,520 --> 00:57:32,090
depending on your problem.

1285
00:57:32,090 --> 00:57:35,000
Say you're in a setting
where things will get covered

1286
00:57:35,000 --> 00:57:38,403
like the camera will be occluded
so it won't be able to see

1287
00:57:38,403 --> 00:57:40,570
the objects fully, this
could be a really neat trick

1288
00:57:40,570 --> 00:57:42,612
you do to make your model
more resilient to stuff

1289
00:57:42,612 --> 00:57:44,230
blocking parts of your objects.

1290
00:57:44,230 --> 00:57:46,880
So you could almost imagine
for your given setting,

1291
00:57:46,880 --> 00:57:48,170
what augmentations make sense.

1292
00:57:48,170 --> 00:57:51,280
What ways can you
transform your input data

1293
00:57:51,280 --> 00:57:53,508
so that it's still
recognizable to you as a human,

1294
00:57:53,508 --> 00:57:56,050
but it makes it harder for the
model to memorize the training

1295
00:57:56,050 --> 00:57:58,960
examples?

1296
00:57:58,960 --> 00:58:02,500
OK, so the final
set of topics here

1297
00:58:02,500 --> 00:58:04,940
are basically
extremely practical.

1298
00:58:04,940 --> 00:58:06,940
So when you're,
say, doing a project

1299
00:58:06,940 --> 00:58:11,470
or training a model, so say
for your course project,

1300
00:58:11,470 --> 00:58:14,230
I think you should basically
do the exact things we're

1301
00:58:14,230 --> 00:58:16,463
going to be describing
in the coming slides,

1302
00:58:16,463 --> 00:58:18,130
but this also applies
outside the course

1303
00:58:18,130 --> 00:58:21,440
to any computer vision domain
you could be practicing in.

1304
00:58:21,440 --> 00:58:25,720
So in practice in many
times we don't actually

1305
00:58:25,720 --> 00:58:26,890
have so much data.

1306
00:58:26,890 --> 00:58:29,728
ImageNet, the original
version had a million images.

1307
00:58:29,728 --> 00:58:32,020
Maybe you don't have a million
images for your problem,

1308
00:58:32,020 --> 00:58:33,780
which almost none of
us do unless you've

1309
00:58:33,780 --> 00:58:36,970
been collecting vast amount
of data with a huge team.

1310
00:58:36,970 --> 00:58:40,380
So if you don't have a lot of
data, can you still train CNNs?

1311
00:58:40,380 --> 00:58:42,370
And the short answer
is yes, you can,

1312
00:58:42,370 --> 00:58:45,870
but you need to be a little
bit smart with how you do it.

1313
00:58:45,870 --> 00:58:48,183
So I think in one of the--

1314
00:58:48,183 --> 00:58:49,600
I think maybe it
was last lecture,

1315
00:58:49,600 --> 00:58:52,620
we showed how the different
filters in your CNN

1316
00:58:52,620 --> 00:58:56,093
are extracting different
types of features.

1317
00:58:56,093 --> 00:58:57,510
So this goes back
to someone asked

1318
00:58:57,510 --> 00:59:00,240
about the hierarchy of features
in convolutional neural

1319
00:59:00,240 --> 00:59:00,940
networks.

1320
00:59:00,940 --> 00:59:03,690
So at the beginning, it's
more of just edges or patterns

1321
00:59:03,690 --> 00:59:05,740
or really small shapes.

1322
00:59:05,740 --> 00:59:08,115
And then at the
highest level, you

1323
00:59:08,115 --> 00:59:11,980
can imagine if we put
an image into our CNN

1324
00:59:11,980 --> 00:59:16,380
and we get this final vector
right before we get our class

1325
00:59:16,380 --> 00:59:21,570
scores, and we compare that to
other images in our data set,

1326
00:59:21,570 --> 00:59:25,530
you'll actually see
that these values

1327
00:59:25,530 --> 00:59:27,910
of the vector of our image
are actually really close.

1328
00:59:27,910 --> 00:59:31,890
So you can think of this as the
nearest neighbors thing we did

1329
00:59:31,890 --> 00:59:34,770
before, but instead of it being
the pixels of the image where

1330
00:59:34,770 --> 00:59:38,370
we're looking at the vector at
the very end of your CNN right

1331
00:59:38,370 --> 00:59:41,190
before you have the
classification layer,

1332
00:59:41,190 --> 00:59:45,060
so this would be like the
4,096 or the 2048 layer.

1333
00:59:45,060 --> 00:59:49,590
And if we look at the difference
here, it's the L2 distance.

1334
00:59:49,590 --> 00:59:52,330
You'll find that
for a given image,

1335
00:59:52,330 --> 00:59:53,880
if you put it into
your model and you

1336
00:59:53,880 --> 00:59:55,890
look at the other
images that are

1337
00:59:55,890 --> 00:59:57,990
close to the model
in this vector space

1338
00:59:57,990 --> 01:00:00,690
right here after you go
through all the layers

1339
01:00:00,690 --> 01:00:03,600
except the last one, that you'll
find the images are extremely

1340
01:00:03,600 --> 01:00:06,700
close to each other when the
items are in the same category.

1341
01:00:06,700 --> 01:00:08,760
So intuitively, what
this basically means

1342
01:00:08,760 --> 01:00:14,400
is that these features here
are actually really good at--

1343
01:00:14,400 --> 01:00:17,730
you could build a linear
classifier on top of them

1344
01:00:17,730 --> 01:00:19,440
or a K nearest
neighbor classifier

1345
01:00:19,440 --> 01:00:23,050
and be able to classify
objects extremely well.

1346
01:00:23,050 --> 01:00:25,700
So how could you use
this in practice?

1347
01:00:25,700 --> 01:00:28,480
So what you would do
is you would first

1348
01:00:28,480 --> 01:00:30,760
train your model on
ImageNet or you would just

1349
01:00:30,760 --> 01:00:33,890
grab a model someone else
is trained on ImageNet

1350
01:00:33,890 --> 01:00:37,370
or one of these really large
web internet scale data sets.

1351
01:00:37,370 --> 01:00:40,930
And you can just freeze
all of these layers

1352
01:00:40,930 --> 01:00:42,350
so you don't train any of them.

1353
01:00:42,350 --> 01:00:45,760
You keep them exactly the same
as before and you replace this

1354
01:00:45,760 --> 01:00:48,822
final layer instead of it being
in the case of ImageNet 1,000

1355
01:00:48,822 --> 01:00:51,280
classes, you replace it with
the number of classes you have

1356
01:00:51,280 --> 01:00:52,520
in your data set.

1357
01:00:52,520 --> 01:00:54,320
And then when you're
training the model,

1358
01:00:54,320 --> 01:00:56,150
you only train this layer here.

1359
01:00:56,150 --> 01:01:00,220
So if we think about
we talked about how

1360
01:01:00,220 --> 01:01:02,385
in the old paradigm
of computer vision

1361
01:01:02,385 --> 01:01:03,760
you had feature
extractors, which

1362
01:01:03,760 --> 01:01:05,830
was a predefined
set of operations

1363
01:01:05,830 --> 01:01:07,720
to get stuff like
color histograms

1364
01:01:07,720 --> 01:01:10,570
and other predefined features.

1365
01:01:10,570 --> 01:01:13,820
You could almost think of the
frozen model as doing this.

1366
01:01:13,820 --> 01:01:15,700
It's a predefined
feature extractor

1367
01:01:15,700 --> 01:01:17,390
that we're not
changing in any way,

1368
01:01:17,390 --> 01:01:19,780
but we're using it to
calculate features that we then

1369
01:01:19,780 --> 01:01:20,975
train a model on top of.

1370
01:01:20,975 --> 01:01:23,100
It's actually extremely
similar under that paradigm

1371
01:01:23,100 --> 01:01:25,150
because you're not
training it here.

1372
01:01:25,150 --> 01:01:27,330
And if you have a
larger data set, what

1373
01:01:27,330 --> 01:01:29,550
tends to work best
in practice is

1374
01:01:29,550 --> 01:01:32,290
to actually train
the whole model,

1375
01:01:32,290 --> 01:01:35,400
but you're initializing
it from these values that

1376
01:01:35,400 --> 01:01:38,310
were pre-trained on ImageNet or
some other really large internet

1377
01:01:38,310 --> 01:01:39,400
scale data set.

1378
01:01:39,400 --> 01:01:43,110
So I think pretty much for
all of the problems I ever

1379
01:01:43,110 --> 01:01:48,150
work on, I'm doing this step
three here because I have maybe

1380
01:01:48,150 --> 01:01:51,400
a million or 10 million
training examples.

1381
01:01:51,400 --> 01:01:54,180
So I'll start it with a model
that was trained on billions

1382
01:01:54,180 --> 01:01:55,870
that I don't have
the compute for

1383
01:01:55,870 --> 01:01:59,400
and then I'll fine tune
the model on my relatively

1384
01:01:59,400 --> 01:02:00,340
smaller data set.

1385
01:02:00,340 --> 01:02:02,257
And I'll get better
performance than if I just

1386
01:02:02,257 --> 01:02:03,640
tried to train a model myself.

1387
01:02:03,640 --> 01:02:05,710
Because the model is
basically seen more data.

1388
01:02:05,710 --> 01:02:08,400
It's created a better
feature extractor.

1389
01:02:08,400 --> 01:02:10,150
And then when I fine
tune the whole thing,

1390
01:02:10,150 --> 01:02:12,370
it can still be specific
enough to my problem.

1391
01:02:12,370 --> 01:02:14,520
You're basically
taking-- say let's

1392
01:02:14,520 --> 01:02:16,020
use a very concrete
case where we're

1393
01:02:16,020 --> 01:02:18,060
training a model on ImageNet.

1394
01:02:18,060 --> 01:02:21,370
We're taking this model and
we're replacing the final layer

1395
01:02:21,370 --> 01:02:25,000
so that it's no longer
outputting 1,000 classes.

1396
01:02:25,000 --> 01:02:28,670
It's outputting the number
of classes in your data set.

1397
01:02:28,670 --> 01:02:31,870
And we're initializing this
randomly using the Kaiming

1398
01:02:31,870 --> 01:02:33,530
Initialization we
talked about before,

1399
01:02:33,530 --> 01:02:36,640
but the rest of these layers
are maintaining their values

1400
01:02:36,640 --> 01:02:38,420
that they had before.

1401
01:02:38,420 --> 01:02:40,190
So we're not
changing these values

1402
01:02:40,190 --> 01:02:41,590
and during gradient
descent we're

1403
01:02:41,590 --> 01:02:42,950
never changing these values.

1404
01:02:42,950 --> 01:02:45,110
So these values are unchanged.

1405
01:02:45,110 --> 01:02:46,490
We basically take our image.

1406
01:02:46,490 --> 01:02:47,905
We pass it through our model.

1407
01:02:47,905 --> 01:02:50,530
And now it's just like we have
a-- it's almost like you're just

1408
01:02:50,530 --> 01:02:54,640
training a linear classifier
where your input are these 4,096

1409
01:02:54,640 --> 01:02:57,550
vectors for each image that we
calculate by passing it through

1410
01:02:57,550 --> 01:02:58,550
the whole model.

1411
01:02:58,550 --> 01:03:00,782
Then we have our
vector of 4,096.

1412
01:03:00,782 --> 01:03:02,990
And we're just mapping that
to the number of classes.

1413
01:03:02,990 --> 01:03:05,680
And we're only training
this mapping at the end.

1414
01:03:05,680 --> 01:03:09,280
Yeah, so the question is, will
you have some bias in your model

1415
01:03:09,280 --> 01:03:10,880
because it was
trained on ImageNet?

1416
01:03:10,880 --> 01:03:12,140
The answer is definitely.

1417
01:03:12,140 --> 01:03:17,740
So the model if you
do this number two,

1418
01:03:17,740 --> 01:03:22,490
this way of training, then
it will do best on data sets

1419
01:03:22,490 --> 01:03:24,270
that look very
similar to ImageNet.

1420
01:03:24,270 --> 01:03:27,230
So these would be pictures of
everyday things like laptops

1421
01:03:27,230 --> 01:03:31,340
or maybe a classroom or a
person, things like this where

1422
01:03:31,340 --> 01:03:32,640
ImageNet is everyday objects.

1423
01:03:32,640 --> 01:03:35,850
But if it was, say, photos of
Mars, it would do a lot worse.

1424
01:03:35,850 --> 01:03:37,970
So there's definitely
bias based on the training

1425
01:03:37,970 --> 01:03:39,283
data of the pre-trained model.

1426
01:03:39,283 --> 01:03:40,700
And you want to
get something that

1427
01:03:40,700 --> 01:03:42,860
is in the same type
of distribution

1428
01:03:42,860 --> 01:03:45,830
where you're seeing the same
kinds of objects or locations

1429
01:03:45,830 --> 01:03:47,600
or things like that.

1430
01:03:47,600 --> 01:03:50,390
So the question is, what do you
do when your data set is out

1431
01:03:50,390 --> 01:03:51,650
of distribution?

1432
01:03:51,650 --> 01:03:55,590
I actually have a slide
here to cover some of that.

1433
01:03:55,590 --> 01:03:57,050
So it's a great question.

1434
01:03:57,050 --> 01:04:00,685
So if you have a very similar
data set but very little data,

1435
01:04:00,685 --> 01:04:02,060
you can use the
linear classifier

1436
01:04:02,060 --> 01:04:03,313
strategy we just mentioned.

1437
01:04:03,313 --> 01:04:05,480
If you have a similar data
set, quite a lot of data,

1438
01:04:05,480 --> 01:04:07,320
you'll get best performance
by fine tuning all the layers.

1439
01:04:07,320 --> 01:04:08,737
These are strategies
two and three

1440
01:04:08,737 --> 01:04:10,640
on this slide that
I mentioned earlier.

1441
01:04:10,640 --> 01:04:12,932
But what about when you have
a very different data set?

1442
01:04:12,932 --> 01:04:16,280
So if you have a lot
of data you might just

1443
01:04:16,280 --> 01:04:20,810
want to start from scratch or
you might get better performance

1444
01:04:20,810 --> 01:04:21,810
if you still initialize.

1445
01:04:21,810 --> 01:04:23,893
Here you would test, but
there's no guaranteed way

1446
01:04:23,893 --> 01:04:26,190
to know that performance
would be better or worse.

1447
01:04:26,190 --> 01:04:29,430
And then, yeah, if you have very
little data or a very different

1448
01:04:29,430 --> 01:04:31,020
data set, you
probably want to try

1449
01:04:31,020 --> 01:04:33,130
to find a model that's
trained on something close.

1450
01:04:33,130 --> 01:04:35,460
There are specific
techniques that researchers

1451
01:04:35,460 --> 01:04:38,410
have looked into for
out-of-domain generalization

1452
01:04:38,410 --> 01:04:41,420
and this basic idea of
you have one domain,

1453
01:04:41,420 --> 01:04:43,170
you train a model on
one domain and you're

1454
01:04:43,170 --> 01:04:45,430
trying to learn a new domain,
that's different in some ways.

1455
01:04:45,430 --> 01:04:47,140
So this is an active
area of research,

1456
01:04:47,140 --> 01:04:50,100
but I wouldn't say there's a
general technique that always

1457
01:04:50,100 --> 01:04:52,690
works and it's a bit problem
dependent in that setting,

1458
01:04:52,690 --> 01:04:56,150
whereas this is for everything
except the upper right quadrant

1459
01:04:56,150 --> 01:04:56,650
here.

1460
01:04:56,650 --> 01:04:58,060
It works pretty
well in practice.

1461
01:04:58,060 --> 01:04:59,950
So there are actually
techniques for this.

1462
01:04:59,950 --> 01:05:01,510
And it's a pretty
active area of research.

1463
01:05:01,510 --> 01:05:02,890
And certain models
generalize better.

1464
01:05:02,890 --> 01:05:04,307
I think language
models are pretty

1465
01:05:04,307 --> 01:05:07,020
good at learning a lot of
different domains, for example.

1466
01:05:07,020 --> 01:05:10,470
But yeah, it's definitely
the worst scenario

1467
01:05:10,470 --> 01:05:12,892
to be in where you have a
completely different problem

1468
01:05:12,892 --> 01:05:14,350
that anyone's ever
worked on before

1469
01:05:14,350 --> 01:05:15,340
and you don't have
a lot of data.

1470
01:05:15,340 --> 01:05:17,890
It's by far the hardest to
train a model in that setting.

1471
01:05:17,890 --> 01:05:19,860
So the question is, do you ever
do anything between training

1472
01:05:19,860 --> 01:05:20,950
one final layer and all layers?

1473
01:05:20,950 --> 01:05:22,492
Yeah, people have
actually done a lot

1474
01:05:22,492 --> 01:05:25,770
of work looking into training
a subset of the layers.

1475
01:05:25,770 --> 01:05:27,825
There's also a
technique called LoRA,

1476
01:05:27,825 --> 01:05:29,950
which we might go into in
the transformers lecture.

1477
01:05:29,950 --> 01:05:31,930
I'm not sure if they'll
make it this year,

1478
01:05:31,930 --> 01:05:34,152
but the basic idea
is to fine tune

1479
01:05:34,152 --> 01:05:36,360
all the layers in a way
where you're not changing all

1480
01:05:36,360 --> 01:05:41,820
the values exactly, but you're
learning basically low rank

1481
01:05:41,820 --> 01:05:44,340
differences between
the different layers

1482
01:05:44,340 --> 01:05:47,160
where you're fine
tuning the differences

1483
01:05:47,160 --> 01:05:49,830
between the original
layers rather than fine

1484
01:05:49,830 --> 01:05:51,040
tuning the layers themselves.

1485
01:05:51,040 --> 01:05:55,410
So yeah, there's techniques
you could use LoRA.

1486
01:05:55,410 --> 01:05:57,835
And it would need
more explanation,

1487
01:05:57,835 --> 01:06:00,460
but the basic idea is instead of
fine tuning the actual values,

1488
01:06:00,460 --> 01:06:03,480
you're fine tuning these
differences between the value

1489
01:06:03,480 --> 01:06:06,143
layers like how a ResNet,
you're learning the difference.

1490
01:06:06,143 --> 01:06:07,560
LoRAs are like
that, but you do it

1491
01:06:07,560 --> 01:06:09,690
with a very small
number of parameters.

1492
01:06:09,690 --> 01:06:12,000
I think the question is,
how do they basically

1493
01:06:12,000 --> 01:06:13,750
decide how many layers to pick?

1494
01:06:13,750 --> 01:06:16,180
Why did they pick a
large number of layers?

1495
01:06:16,180 --> 01:06:18,720
Specifically why are there
two convolution layers

1496
01:06:18,720 --> 01:06:20,940
of each size instead of one?

1497
01:06:20,940 --> 01:06:23,190
So it's actually really
similar to the example

1498
01:06:23,190 --> 01:06:26,130
we showed earlier with VGG where
if you have three of these 3

1499
01:06:26,130 --> 01:06:30,600
by 3 convolutions, you're able
to have the same receptive field

1500
01:06:30,600 --> 01:06:32,640
as a 7 by 7
convolution, but you're

1501
01:06:32,640 --> 01:06:35,880
able to model more non-linear
relationships because you have

1502
01:06:35,880 --> 01:06:38,910
these three activation functions
rather than just one activation

1503
01:06:38,910 --> 01:06:40,390
on the 7 by 7 filter.

1504
01:06:40,390 --> 01:06:42,820
So basically 3 by 3
is more expressive,

1505
01:06:42,820 --> 01:06:44,970
but you're still looking
at the same set of values

1506
01:06:44,970 --> 01:06:46,510
as long as you have
enough of them.

1507
01:06:46,510 --> 01:06:48,960
So a larger set
of smaller filters

1508
01:06:48,960 --> 01:06:51,455
is more expressive than a
smaller set of larger filters.

1510
01:06:55,320 --> 01:06:57,523
OK, so we'll go on.

1511
01:06:57,523 --> 01:06:59,190
Yeah, basically try
to find a large data

1512
01:06:59,190 --> 01:07:00,810
set that has similar data.

1513
01:07:00,810 --> 01:07:02,310
Get a model that
was trained on that

1514
01:07:02,310 --> 01:07:04,800
and then fine tune
it on your own data.

1515
01:07:04,800 --> 01:07:06,180
Some good links.

1516
01:07:06,180 --> 01:07:08,010
PyTorch image models
has a bunch of models

1517
01:07:08,010 --> 01:07:10,380
that are trained on
ImageNet and other data

1518
01:07:10,380 --> 01:07:14,270
sets and then also just in the
PyTorch version, a GitHub repo.

1519
01:07:14,270 --> 01:07:15,260
You'll find some too.

1520
01:07:15,260 --> 01:07:17,680
OK, I'll talk over
very briefly at the end

1521
01:07:17,680 --> 01:07:19,190
for hyperparameter selection.

1522
01:07:19,190 --> 01:07:22,630
So if you're having
difficulty training your model

1523
01:07:22,630 --> 01:07:24,060
and it's not working
right away, I

1524
01:07:24,060 --> 01:07:27,140
think the best thing you can do
is to overfit on a small sample.

1525
01:07:27,140 --> 01:07:29,650
So this is the default
debugging strategy

1526
01:07:29,650 --> 01:07:31,960
in deep learning where you
just have one data point

1527
01:07:31,960 --> 01:07:34,503
and you want to see your
training loss basically go to 0.

1528
01:07:34,503 --> 01:07:36,920
Your model should be able to
memorize one training example

1529
01:07:36,920 --> 01:07:38,560
and if it's not able to do
that have a bug somewhere

1530
01:07:38,560 --> 01:07:41,380
in your code or you're not
picking the right kind of model

1531
01:07:41,380 --> 01:07:42,910
to model your problem.

1532
01:07:42,910 --> 01:07:45,980
So this is a really
good training problem.

1533
01:07:45,980 --> 01:07:47,980
And it'll also tell you
what learning rates work

1534
01:07:47,980 --> 01:07:49,250
or what ones don't work.

1535
01:07:49,250 --> 01:07:50,890
And you'll get a rough idea of
the neighborhood of learning

1536
01:07:50,890 --> 01:07:51,975
rates you should explore.

1537
01:07:51,975 --> 01:07:54,350
So this is a good way to make
sure your model is correct,

1538
01:07:54,350 --> 01:07:57,197
make sure your learning
rate is reasonable

1539
01:07:57,197 --> 01:07:59,530
and also to make sure you
don't have any other bugs that

1540
01:07:59,530 --> 01:08:00,950
could be impacting your code.

1541
01:08:00,950 --> 01:08:02,860
So this is always step one.

1542
01:08:02,860 --> 01:08:06,350
If you're having issues
just running some code,

1543
01:08:06,350 --> 01:08:08,140
this is how you debug.

1544
01:08:08,140 --> 01:08:09,910
The second thing
you would want to do

1545
01:08:09,910 --> 01:08:12,178
after you get this is maybe
you try a very coarse grid

1546
01:08:12,178 --> 01:08:12,970
of hyperparameters.

1547
01:08:12,970 --> 01:08:15,540
So I would first try with
different learning rates

1548
01:08:15,540 --> 01:08:18,347
and see how if you trained model
on different learning rates,

1549
01:08:18,347 --> 01:08:19,930
what do the training
losses look like.

1550
01:08:19,930 --> 01:08:23,279
You want one that has the
most sustained decreasing

1551
01:08:23,279 --> 01:08:25,689
in the training loss
over maybe one epoch.

1552
01:08:25,689 --> 01:08:29,069
That's a pretty good estimation,
but you can train for longer.

1553
01:08:29,069 --> 01:08:30,939
Once you get a good
set of learning rates,

1554
01:08:30,939 --> 01:08:33,279
you could then look into
other hyperparameters too.

1555
01:08:33,279 --> 01:08:35,380
And specifically,
besides the loss,

1556
01:08:35,380 --> 01:08:38,189
you'll also want to look
at the accuracy curves.

1557
01:08:38,189 --> 01:08:39,660
So you have your
training accuracy

1558
01:08:39,660 --> 01:08:41,460
and your validation accuracy.

1559
01:08:41,460 --> 01:08:43,380
If they're still
going up, it means

1560
01:08:43,380 --> 01:08:45,819
you want to keep training
pretty reasonable,

1561
01:08:45,819 --> 01:08:48,270
but you might have a
scenario where the training

1562
01:08:48,270 --> 01:08:50,979
loss is going up, but your
validation loss is going down.

1563
01:08:50,979 --> 01:08:52,899
This is overfitting.

1564
01:08:52,899 --> 01:08:56,437
So we need to either
increase the regularization

1565
01:08:56,437 --> 01:08:58,479
or if we can get more
data, that could also work,

1566
01:08:58,479 --> 01:09:00,776
but you need to
do one of the two

1567
01:09:00,776 --> 01:09:02,609
in order to improve
your performance further

1568
01:09:02,609 --> 01:09:04,680
beyond the peak
right here I guess

1569
01:09:04,680 --> 01:09:07,229
would be the best
model you have so far.

1570
01:09:07,229 --> 01:09:09,870
If you're seeing very
little of a gap here,

1571
01:09:09,870 --> 01:09:14,450
then you can probably train for
longer because generally you

1572
01:09:14,450 --> 01:09:18,680
do want to just get to the point
where your validation loss has

1573
01:09:18,680 --> 01:09:19,470
been maximized.

1574
01:09:19,470 --> 01:09:21,649
So if you could
just keep training,

1575
01:09:21,649 --> 01:09:22,649
you could keep training.

1576
01:09:22,649 --> 01:09:25,399
So even if there's not
a significant gap here

1577
01:09:25,399 --> 01:09:31,085
or anything, if you see
the validation accuracy is

1578
01:09:31,085 --> 01:09:32,460
similar to the
training accuracy,

1579
01:09:32,460 --> 01:09:35,029
you can probably keep training
until your training accuracy

1580
01:09:35,029 --> 01:09:39,050
starts diverging from
your validation accuracy.

1581
01:09:39,050 --> 01:09:41,600
And you basically can repeat
this process over and over

1582
01:09:41,600 --> 01:09:42,410
again.

1583
01:09:42,410 --> 01:09:46,529
One final note is in terms
of for hyperparameter search,

1584
01:09:46,529 --> 01:09:49,160
normally people think you have
two hyperparameters or more

1585
01:09:49,160 --> 01:09:50,930
that you're searching
over, should you

1586
01:09:50,930 --> 01:09:53,015
try every combination
of the hyperparameters

1587
01:09:53,015 --> 01:09:54,390
or what is the
best way to do it?

1588
01:09:54,390 --> 01:09:57,620
I think in practice, random
search over the hyperparameter

1589
01:09:57,620 --> 01:10:00,470
space works a lot better
than a grid search

1590
01:10:00,470 --> 01:10:03,810
where you're trying every
set of a predefined set.

1591
01:10:03,810 --> 01:10:05,780
And the reason is
mainly because you

1592
01:10:05,780 --> 01:10:08,060
can imagine you have one
axis, which is an unimportant

1593
01:10:08,060 --> 01:10:09,893
hyperparameter where
depending on the value,

1594
01:10:09,893 --> 01:10:11,602
your performance will
be roughly the same

1595
01:10:11,602 --> 01:10:12,740
versus an important one.

1596
01:10:12,740 --> 01:10:14,700
If you do random values
across all of these,

1597
01:10:14,700 --> 01:10:16,910
you'll actually search
the hyperparameter space

1598
01:10:16,910 --> 01:10:19,280
of your important
parameter much more

1599
01:10:19,280 --> 01:10:23,000
thoroughly, whereas you're
wasting time doing on the grid

1600
01:10:23,000 --> 01:10:25,010
search rechecking
multiple values

1601
01:10:25,010 --> 01:10:26,520
for an unimportant
hyperparameter.

1602
01:10:26,520 --> 01:10:28,040
So in practice,
you should define

1603
01:10:28,040 --> 01:10:30,950
the ranges you want to
try and then just randomly

1604
01:10:30,950 --> 01:10:34,040
collect hyperparameters with
values from those ranges.

1605
01:10:34,040 --> 01:10:35,790
And that's probably
the best way to do it.

1606
01:10:35,790 --> 01:10:38,082
And you just keep running
until you get the best model.

1607
01:10:38,082 --> 01:10:39,390
OK, that's it.

1608
01:10:39,390 --> 01:10:42,210
So we talked about layers in
CNNs activation functions,

1609
01:10:42,210 --> 01:10:44,160
CNN architectures, and
weight initialization,

1610
01:10:44,160 --> 01:10:46,710
how do you actually predefine
and build these models

1611
01:10:46,710 --> 01:10:48,960
and then we talked about how
do you actually train it.

1612
01:10:48,960 --> 01:10:51,210
How do you change your data
to be input to the model?

1613
01:10:51,210 --> 01:10:52,640
How do you augment it?

1614
01:10:52,640 --> 01:10:54,320
Transfer learning is
a really neat trick

1615
01:10:54,320 --> 01:10:56,778
for improving performance and
then how do you pick the best

1616
01:10:56,778 --> 01:10:57,450
hyperparameters.

1617
01:10:57,450 --> 01:10:59,610
So yeah, we covered a
lot in lecture today.

1618
01:10:59,610 --> 01:11:00,710
Thank you all so much.

1619
01:11:00,710 --> 01:11:02,260
Yeah.