1
00:00:05,600 --> 00:00:09,040
I think at the beginning of the 
course, we announced that we would have  

2
00:00:09,760 --> 00:00:15,120
a few guest lecturers, people who previously 
taught the course, to come and give a single  

3
00:00:15,120 --> 00:00:18,960
guest lecture about a topic that they're very 
familiar with. And I'm very happy to announce  

4
00:00:18,960 --> 00:00:24,880
we have the first one of those lectures 
today. So I'll introduce Dr. Ruohan Gao.

5
00:00:24,880 --> 00:00:29,440
He is an Assistant Professor in the Department of 
Computer Science at the University of Maryland,  

6
00:00:30,080 --> 00:00:35,280
College Park. And he leads the Multisensory 
Machine Intelligence Lab there. He was previously  

7
00:00:35,280 --> 00:00:44,160
an instructor for CS231N from 2022 to 2023. 
And this is while he completed his postdoc with  

8
00:00:45,280 --> 00:00:49,520
Fei-Fei Li, Jiajun Wu, and Silvio 
Savarese. So without further ado,  

9
00:00:49,520 --> 00:00:53,680
I'll leave it to Ruohan to 
give the presentation today.

10
00:00:53,680 --> 00:01:00,400
OK, thanks. Hello, everyone. So it's 
really exciting to be back to the class  

11
00:01:00,400 --> 00:01:05,840
of CS231N. And I'm Ruohan, just like [? 
Zen ?] introduced. So as you can tell,  

12
00:01:05,840 --> 00:01:10,560
I'm very interested in multimodal stuff. So 
not only in vision, but also how we can make  

13
00:01:10,560 --> 00:01:16,160
use of other sensory modalities like audio, 
tactile, or other modalities, just like us,  

14
00:01:16,160 --> 00:01:21,280
humans, to perceive, understand, and 
interact with this multisensory world.

15
00:01:21,280 --> 00:01:25,120
But of course, vision is the most important 
modalities. That's why we have this course,  

16
00:01:25,120 --> 00:01:30,560
deep learning for computer vision. And I'm sure up 
to this point that you guys are very familiar with  

17
00:01:30,560 --> 00:01:38,880
image classification. Given a 2D image like this, 
how to give a label to see whether it's a dog,  

18
00:01:38,880 --> 00:01:43,520
it's a cat, or it's a truck, a plane. 
That's a 2D-based image classification.

19
00:01:43,520 --> 00:01:47,920
And from the last lecture, I'm sure you have 
also learned some other tasks that you can  

20
00:01:47,920 --> 00:01:53,120
do on images. Not only you can just assign a 
single label to say it's a cat or not. Also,  

21
00:01:53,120 --> 00:01:59,120
you can do semantic segmentation to segment the 
picture into different portions, components,  

22
00:01:59,120 --> 00:02:02,880
and also have some semantic meaning like 
where is grass, where is cat, where is tree.

23
00:02:02,880 --> 00:02:08,880
And you can also put a bounding box on 
top of the objects you detect in the image  

24
00:02:08,880 --> 00:02:13,040
to see where the dog is, where the cat is, 
and also do instant segmentation that you not  

25
00:02:13,040 --> 00:02:18,720
only want to know the categories, but also 
for each category, if there are two dogs,  

26
00:02:18,720 --> 00:02:22,560
I want to have a segmentation mask for each 
category. That's instance segmentation.  

27
00:02:22,560 --> 00:02:28,880
So a lot of tasks, classification, recognition 
tasks you can do based on 2D images.

28
00:02:28,880 --> 00:02:34,480
But that's not the only thing that we can use 
for computer vision system to do. And also,  

29
00:02:34,480 --> 00:02:41,440
our world is not just static like this. So if we 
look at this image, hopefully up to this point,  

30
00:02:41,440 --> 00:02:49,840
you have learned a lot of tools that you can 
train some models to this classify this is a  

31
00:02:49,840 --> 00:02:55,760
living room. You also have tools. You have 
learned that to put a bounding box to see  

32
00:02:55,760 --> 00:03:00,800
that this is a dog, and this is a baby. And 
also, you can even have a segmentation mask  

33
00:03:00,800 --> 00:03:05,920
to send them out to see where those 
objects you detect are in the image.

34
00:03:05,920 --> 00:03:10,400
So today, we're going to focus on video 
understanding. So more formally, what is video?  

35
00:03:10,400 --> 00:03:16,480
Basically, video is just like this 2D image plus 
time. There is an extra time dimension. So now,  

36
00:03:16,480 --> 00:03:23,520
we are tackling things not only in this 
3D image but also now in 4D. We have this  

37
00:03:25,600 --> 00:03:30,080
three times T. T is temporal dimension. 
And H and W are the spatial dimension. Now,  

38
00:03:30,080 --> 00:03:37,840
we are considering this kind of image and the 
videos as a volume of images of video frames.

39
00:03:37,840 --> 00:03:43,120
So an example task is video classification, 
just like image classification. So we are  

40
00:03:43,120 --> 00:03:50,320
given a video like this. Some person is 
running. We want to take this videos as  

41
00:03:50,320 --> 00:03:54,160
input. And also we want to train some 
model, a deep learning model. We want  

42
00:03:54,160 --> 00:03:58,320
to classify whether this person is doing 
swimming or running or jumping or what  

43
00:03:58,320 --> 00:04:04,560
actions that he is doing just based on 
this temporal streams of video frames.

44
00:04:04,560 --> 00:04:09,440
So also, we have from the previous lectures, 
I'm sure you have already learned some loss  

45
00:04:09,440 --> 00:04:13,920
functions like cross-entropy loss. And you 
can train an image classifier. Similarly,  

46
00:04:14,720 --> 00:04:18,640
you can use the similar tools. You just train 
a video classifier. You just get some features.  

47
00:04:18,640 --> 00:04:23,120
And you use the same loss functions 
and train a video classifier. So now,  

48
00:04:23,120 --> 00:04:28,480
the problem on video understanding is 
that how can we get features of videos  

49
00:04:28,480 --> 00:04:32,240
that you can apply the loss functions you 
have learned from the previous lectures?

50
00:04:34,080 --> 00:04:38,720
And also another kind of difference between image 
classification and the video classification,  

51
00:04:38,720 --> 00:04:42,400
video understanding is that now the things, 
the tasks you want to do might be a little bit  

52
00:04:42,400 --> 00:04:47,760
different just from the previous example. For 
image classification, usually, you care more  

53
00:04:47,760 --> 00:04:53,440
about the scenes, the objects. You want to just 
do a classification. What is the object category?

54
00:04:53,440 --> 00:04:57,120
For videos, usually, just like 
this example I'm showing here,  

55
00:04:57,120 --> 00:05:00,000
you only want to classify actions. 
It's often actions where the person,  

56
00:05:00,000 --> 00:05:06,160
what activities the person or some animals are 
doing in the videos. That's what we care about  

57
00:05:06,160 --> 00:05:12,400
usually in video understanding. So the nature of 
things to recognize can be a little bit different.

58
00:05:12,400 --> 00:05:17,520
And another problem that we want to be 
careful about for video understanding is  

59
00:05:17,520 --> 00:05:24,000
that videos are usually very big. 
Well, when we talk about images,  

60
00:05:24,000 --> 00:05:30,640
it's just like three times H times W. 
It's a single matrix of some RGB numbers.

61
00:05:30,640 --> 00:05:34,880
But now, if we consider videos, it's a 
sequence of frames. It can be like 30  

62
00:05:34,880 --> 00:05:43,280
frames per second. So in movies, sometimes, 
we can have even higher resolution and also  

63
00:05:43,280 --> 00:05:50,240
temporal resolution video frames. And 
so you consider a space to store videos.

64
00:05:50,240 --> 00:05:56,320
For example, if we consider standard 
definition videos, it can take about 1.5  

65
00:05:56,320 --> 00:06:01,600
gigabytes per minute if we store this video. 
If we consider even higher resolution,  

66
00:06:01,600 --> 00:06:08,960
like 1920 times 1080. And now, 
it takes like 10 gigabytes per  

67
00:06:08,960 --> 00:06:14,800
minute. So it takes the gigantic space in 
order to store this kind of video data.

68
00:06:14,800 --> 00:06:18,560
And also, there is no way for us to 
just fit this kind of data directly  

69
00:06:18,560 --> 00:06:24,240
to GPUs. If we just have the input, then 
we have a lot of storage to store them,  

70
00:06:24,240 --> 00:06:28,560
to store this kind of data. And also, there are 
other things you have to store, like the weights,  

71
00:06:28,560 --> 00:06:36,560
the activations in your convolution neural 
networks. So then, your model will be very huge.

72
00:06:36,560 --> 00:06:44,960
And the solution-- what solutions we can have to 
make videos smaller to make them processable? So  

73
00:06:44,960 --> 00:06:51,760
one simple solution is that we just make videos 
smaller. So although the high definition videos  

74
00:06:51,760 --> 00:06:58,720
and also the original videos are long, we can 
shrink things both temporally and spatially.

75
00:06:59,920 --> 00:07:05,040
For example, for 3.2-second 
videos like this, for example,  

76
00:07:05,040 --> 00:07:09,600
we can maybe-- for each second, 
maybe we don't need all the frames.  

77
00:07:09,600 --> 00:07:13,360
Let's just take five frames because there are 
a lot of redundancies in the video frames.

78
00:07:13,360 --> 00:07:19,680
If we take frames per second, and 
also we just have a smaller spatial  

79
00:07:19,680 --> 00:07:24,000
resolution like 112 times 112. 
And now, we can make the videos  

80
00:07:24,000 --> 00:07:32,240
slightly smaller. For example, it's five 588 
KB for this simple video. But definitely,  

81
00:07:32,240 --> 00:07:36,960
we can also do larger resolution if 
you have the compute, just like images.

82
00:07:36,960 --> 00:07:45,120
And also, how to train a model on these long 
videos? Previous slide, I showed that we are  

83
00:07:45,120 --> 00:07:49,520
training this video classifier on 3.2 
seconds. But videos can be very long,  

84
00:07:49,520 --> 00:07:54,400
can be minutes, can be hours. So one way 
that people do is that we train on clips,  

85
00:07:54,400 --> 00:08:00,320
just like we train on chunks of this 
video frames using a video classifier.

86
00:08:00,320 --> 00:08:04,960
And what we do is that we train models 
to classify short clips with some low  

87
00:08:06,240 --> 00:08:12,320
fps, frame per second. And we just use a sliding 
window. We sample a lot of different clips and use  

88
00:08:12,320 --> 00:08:16,720
them as training data. And we train a classifier. 
And then during testing, during inference time,  

89
00:08:16,720 --> 00:08:22,400
we just run the model on different clips. 
We sample a few clips. We made 10 clips. And  

90
00:08:22,400 --> 00:08:29,520
then we average the prediction results. And 
that is our prediction for this long video.

91
00:08:29,520 --> 00:08:37,440
And then what is a same post, like, video 
classification model we can use? So I have  

92
00:08:37,440 --> 00:08:45,760
mentioned basically video is just like a sequence 
of images, a sequence of video image frames. So  

93
00:08:45,760 --> 00:08:50,800
one simple thing that we just treat them as 
images. That's the simplest tool we already  

94
00:08:50,800 --> 00:08:56,640
have. We just run single frame convolution neural 
networks because we already have all the tools.

95
00:08:56,640 --> 00:09:00,720
We have learned that we can train an image 
classifier. If we just take our image  

96
00:09:00,720 --> 00:09:05,200
classifier to just run on top of these 
video frames and treat them as images,  

97
00:09:05,200 --> 00:09:09,200
we can indeed get decent predictions. 
Especially like a video like this. You  

98
00:09:09,200 --> 00:09:13,120
can see that there are not many changes across 
videos. The person is running. Maybe there are  

99
00:09:13,120 --> 00:09:17,760
some different movements on the body. 
But generally, it looks pretty similar.

100
00:09:17,760 --> 00:09:25,040
Maybe you run an image action classifier. And 
on every frame, maybe all of the frames will  

101
00:09:25,040 --> 00:09:29,440
tell you it's running. And if you average 
the prediction results from each image,  

102
00:09:29,440 --> 00:09:34,480
each video frame, then you will predict 
running for this particular video.

103
00:09:34,480 --> 00:09:40,960
So actually, also, it's usually a very, very 
strong baseline for this simple image classifier,  

104
00:09:41,920 --> 00:09:45,600
especially for video like this because 
there are not too many changes across  

105
00:09:45,600 --> 00:09:51,200
videos. So if you are trying to design some 
video classifier, you should always run this  

106
00:09:51,200 --> 00:09:56,560
first because that's simple things to try. And 
maybe you can already get pretty decent results.

107
00:09:56,560 --> 00:10:02,400
So the question is, whether we just run single 
frame or we run a chunk of frames? So for this, a  

108
00:10:02,400 --> 00:10:08,480
simple single frame, say. And basically, we have a 
video of 30 frames. Maybe you just maybe sample a  

109
00:10:08,480 --> 00:10:13,360
few frames. And you just use the image classifier 
to run on those sampled 10 frames and just treat  

110
00:10:13,360 --> 00:10:18,720
them as images. And you just directly average 
the results. That's basically the per frame CNN.

111
00:10:18,720 --> 00:10:22,000
So I think you have some very important 
question is how to sample the frame?  

112
00:10:22,000 --> 00:10:26,480
That's the very key question. Because 
we give you a-- I'm talking about we  

113
00:10:26,480 --> 00:10:30,000
want to sample some frames and we want 
to run a CNN on the frame. So how to get  

114
00:10:30,000 --> 00:10:33,680
those frames? So that is actually 
also an active area of research.

115
00:10:33,680 --> 00:10:38,800
One simple way is that you do random sampling. 
You have a one-hour video. I don't know where the  

116
00:10:38,800 --> 00:10:42,720
interesting part, where the important parts 
are. We just sample maybe every one minute,  

117
00:10:42,720 --> 00:10:46,880
I sample one frame. And then I run an image 
classifier. I average the results. But obviously,  

118
00:10:46,880 --> 00:10:51,200
maybe this gives us some good results. But maybe 
this is not the smartest way to do sampling.

119
00:10:51,200 --> 00:10:55,040
So there are other methods trying to 
propose smarter sampling strategy.  

120
00:10:55,040 --> 00:10:59,600
Maybe you can sample one frame. You can 
then use that decision to decide where  

121
00:10:59,600 --> 00:11:05,600
else to sample. I actually also have some 
examples later in the later lecture slides.

122
00:11:05,600 --> 00:11:10,560
So this is a very, very simple video 
classifier. Just like we just adopt  

123
00:11:10,560 --> 00:11:17,200
image classifier, a single frame CNN. And 
similarly, just maybe we take one step  

124
00:11:17,200 --> 00:11:23,840
further. Instead of directly just run single 
frame CNN and average the prediction results,  

125
00:11:23,840 --> 00:11:32,400
maybe we can do some fusion across the 
features on using the single frame CNNs.

126
00:11:32,400 --> 00:11:39,200
So this is often called late fusion. Basically, 
the idea is that we still take some 2D CNNs.  

127
00:11:39,200 --> 00:11:48,080
And we have some input, maybe key frames. And for 
each frame, we use a 2D CNN. And then we extract  

128
00:11:48,080 --> 00:11:56,080
some feature vector. And then we get maybe a 
feature map of D times H prime times W prime.

129
00:11:56,080 --> 00:11:59,360
And then because we have T frames. Then basically,  

130
00:11:59,360 --> 00:12:06,800
we have T feature maps. And then the simple 
thing that we just flatten all the feature  

131
00:12:06,800 --> 00:12:12,720
maps to vectors and then concatenate them. 
Then we have a giant feature vector that  

132
00:12:12,720 --> 00:12:16,480
basically contains all the information, 
all the features across all the frames.

133
00:12:16,480 --> 00:12:19,120
And then what we can do, we can use 
tools that we have learned like fully  

134
00:12:19,120 --> 00:12:25,360
connected networks. We train an MLP 
that maps this vector to some neural  

135
00:12:25,360 --> 00:12:30,400
dimension. And then we train a classifier 
on top of it to map it, to class score C.

136
00:12:30,400 --> 00:12:37,040
So this is called late fusion. Because basically, 
you can see that we extract the feature maps. And  

137
00:12:37,040 --> 00:12:41,600
we process them very independently. And then at 
the very late stage, we concatenate the feature  

138
00:12:41,600 --> 00:12:45,280
vectors and run some fully connected 
layers to doing the classification.

139
00:12:45,280 --> 00:12:54,800
So this is useful. But one drawback that 
you can already probably tell from this  

140
00:12:54,800 --> 00:13:00,160
example from my description is that this fully 
connected layer, it's going to introduce a lot  

141
00:13:00,160 --> 00:13:06,240
of parameters. Because if we concatenate 
a lot of-- we flatten them across time.

142
00:13:06,240 --> 00:13:10,720
And these feature vectors, depending 
on how long how large T is, then you  

143
00:13:10,720 --> 00:13:14,480
can have a giant feature vector. And then 
you use this giant feature vector. You want  

144
00:13:14,480 --> 00:13:19,760
to map them into some lower dimension. And 
then you have a very large, fully connected  

145
00:13:19,760 --> 00:13:23,440
layers. And that will introduce a lot of 
parameters. So it's not very efficient.

146
00:13:23,440 --> 00:13:28,400
So another way to do this is that 
instead of concatenating them,  

147
00:13:29,040 --> 00:13:34,240
we don't do concatenation. We don't 
just use the giant feature vector,  

148
00:13:34,240 --> 00:13:39,920
and then have a fully connected layer to 
map them to scores. We can actually just  

149
00:13:39,920 --> 00:13:45,440
do a simple pooling. Doing pooling, you don't 
increase the length of the feature vector.

150
00:13:45,440 --> 00:13:51,440
Basically, if you have some feature dimension 
for a single frame and you pool across time,  

151
00:13:51,440 --> 00:13:54,160
for these T frames, you're just 
doing a pooling to do a temporal  

152
00:13:54,160 --> 00:14:01,360
aggregation. And then based on this clip 
feature D, and then instead of D times T,  

153
00:14:01,360 --> 00:14:04,480
you just still have a feature 
vector of time D if you do pooling.

154
00:14:04,480 --> 00:14:10,640
And then you have a linear layer to map D to some 
dimension C that matches the class score. And  

155
00:14:10,640 --> 00:14:16,640
then you train the cross-entropy loss on top 
of it. And that's also late fusion. But now,  

156
00:14:16,640 --> 00:14:22,080
we are using pooling. And the 
good side here is that now,  

157
00:14:22,080 --> 00:14:26,320
you don't have to have a very large fully 
connected layers. But the pooling can also  

158
00:14:28,320 --> 00:14:34,400
get rid of information that may be important. 
So that's the downside of this operation.

159
00:14:34,400 --> 00:14:41,280
So the reason I'm calling late fusion-- the 
important part is late. And when it's late,  

160
00:14:41,280 --> 00:14:46,560
maybe there's some information that has already 
been lost. So you have using this 2D convolution  

161
00:14:46,560 --> 00:14:52,800
networks to process images. For example, as 
shown in these red circles here. So what is  

162
00:14:52,800 --> 00:14:59,440
very important to recognize this video is 
actually the motion of this man's feet,  

163
00:14:59,440 --> 00:15:04,480
right. It's moving up and down, up and down. 
And you can maybe tell that he's running.

164
00:15:04,480 --> 00:15:10,240
So if we just use a single 2D CNN 
to process them independently as a  

165
00:15:11,040 --> 00:15:16,320
2D image and extract some feature map and maybe 
up to very late stage of the feature maps,  

166
00:15:18,720 --> 00:15:25,200
it doesn't contain the information of this 
movement of the feet of this man anymore at  

167
00:15:25,200 --> 00:15:31,040
this very late stage. So some information of this 
feet up and down is showing these red circles,  

168
00:15:31,040 --> 00:15:35,520
which should be useful cues. But now, 
it's not there in the feature maps.

169
00:15:35,520 --> 00:15:40,880
The intuition is that if you extract 
features from the earlier layers,  

170
00:15:41,440 --> 00:15:48,400
it's very close to the original video 
frames. So there is a larger chance it  

171
00:15:48,400 --> 00:15:54,480
will contain this low level information, this 
movement from the video frames. And also,  

172
00:15:54,480 --> 00:16:00,880
if you concatenate them or pool them, across 
time, it will analyze the motion across time.

173
00:16:00,880 --> 00:16:05,840
But because we are processing a lot of 
convolution pooling up to a late stage,  

174
00:16:05,840 --> 00:16:09,440
you need-- even at the very late stage, it 
contains more high level information like  

175
00:16:09,440 --> 00:16:13,520
semantic information instead of this low 
level motion information. So that's why  

176
00:16:13,520 --> 00:16:19,520
it's most likely it's not there. So 
that's the downside of late fusion.

177
00:16:19,520 --> 00:16:24,160
So instead of doing late fusion, we actually 
we can do early fusion. So to do early fusion,  

178
00:16:24,160 --> 00:16:28,240
if we want to make use of the feature vectors 
more closer to the actual video frames,  

179
00:16:28,240 --> 00:16:38,960
we can just take this input and then we directly 
reshape them to 3T times H times W. We just  

180
00:16:38,960 --> 00:16:42,640
directly aggregate the information 
temporally from the very beginning.

181
00:16:42,640 --> 00:16:47,760
And then we use some 2D convolution, the first 
2D convolution layer, just directly map them  

182
00:16:47,760 --> 00:16:55,200
to from channel dimensions 3T to D. Basically, we 
use the 2D convolution to process this temporal  

183
00:16:55,200 --> 00:17:03,200
information in the first layer to map the channel 
dimension from 3T to D to process the video frames  

184
00:17:03,200 --> 00:17:08,960
of all the information from the frames in the 
very beginning of the convolution neural networks.

185
00:17:08,960 --> 00:17:11,680
And the rest of the network 
is then in standard 2 CNN. And  

186
00:17:13,040 --> 00:17:16,320
the only difference is that now, we destroy 
and collapse all the temporal information  

187
00:17:18,240 --> 00:17:21,920
using a single layer. And then the rest 
is just like image classification. And  

188
00:17:21,920 --> 00:17:26,080
then you're doing this classification 
using standard cross-entropy loss.

189
00:17:26,080 --> 00:17:32,320
For each frame, we get features like D. And each 
single frame will give you a feature D. So you  

190
00:17:32,320 --> 00:17:36,400
have T. This feature vectors D. So for pooling, 
we are pooling over the features. Basically,  

191
00:17:36,400 --> 00:17:41,280
we can do mean pooling to average the features 
or be max pooling with max over the features.  

192
00:17:41,280 --> 00:17:48,640
Then after that, we still get a feature that's D. 
So it's pooling over the features not the frame.

193
00:17:49,200 --> 00:17:54,880
So that's early fusion. So then, the 
downside of the early fusion is that  

194
00:17:56,240 --> 00:18:00,800
although we explicitly trying to handle 
the motion from the early layer. But then,  

195
00:18:05,200 --> 00:18:08,640
we are too ambitious. We're trying to 
capture everything in a single layer.  

196
00:18:08,640 --> 00:18:12,960
We just concatenate all the frames and then 
collapse all the [INAUDIBLE] information using a  

197
00:18:12,960 --> 00:18:18,000
single convolution network. Maybe it's not 
going to achieve what we want it to achieve.

198
00:18:18,000 --> 00:18:23,920
So then, another solution is that instead of doing 
late fusion or early fusion, maybe we should do  

199
00:18:23,920 --> 00:18:30,240
something in between. That's slow fusion. That's 
exactly what this 3D convolution neural network  

200
00:18:30,240 --> 00:18:35,440
is doing. So the intuition is that we want to 
use this 3D version of convolution and pooling.

201
00:18:35,440 --> 00:18:40,400
We want to slowly fuse the information over the 
course of the network. Instead of doing it at a  

202
00:18:40,400 --> 00:18:45,760
very late stage or at a very early stage, we 
gradually shrink over temporal dimension and  

203
00:18:45,760 --> 00:18:52,640
spatial dimension to get this 3D feature maps. So 
that's the idea of 3D convolution neural network.

204
00:18:52,640 --> 00:18:57,040
We just use 3D convolution and a 3D pooling 
operation. So what is 3D convolution, 3D  

205
00:18:57,040 --> 00:19:02,400
pooling? So you have learned the 2D convolution. 
Right. For 2D convolution layer. Basically,  

206
00:19:02,400 --> 00:19:10,960
you take an image like this-- 32 times 32 times 3 
image. And if you use 2D convolution, basically,  

207
00:19:10,960 --> 00:19:15,520
you have learned that you can-- for 
each kernel, you have this filter.

208
00:19:16,400 --> 00:19:23,440
You can have this convolution kernel that 
is maybe 5 times 5 times 3 that runs like  

209
00:19:23,440 --> 00:19:34,240
a sliding window approach, just slides across 
space and goes all the way in the depth dimension.  

210
00:19:34,240 --> 00:19:42,560
So then for each computation, you map that to 
a single value in that final activation maps.

211
00:19:42,560 --> 00:19:48,640
And then finally, you obtain this 
activation map of 28 times 28 times 1,  

212
00:19:48,640 --> 00:19:53,520
in this case. You convolve over all spatial 
locations and map this channel dimension,  

213
00:19:53,520 --> 00:20:01,840
the depths that-- go all the way over the channel 
dimension, and then map from 3 to 1, in this case.

214
00:20:01,840 --> 00:20:06,400
So that's 2D convolution. So the 
difference is that for 3D convolution,  

215
00:20:06,400 --> 00:20:14,960
now, we just have one extra dimension. So 
here, you can think of that-- here, the input  

216
00:20:14,960 --> 00:20:21,520
is C times T times H times W. The extra thing is 
this T dimension. That is a temporal dimension.

217
00:20:21,520 --> 00:20:26,720
But what I'm showing here, because we can only 
show things in 3D, we cannot show things in 4D.  

218
00:20:26,720 --> 00:20:30,400
So there's actually one dimension that is not 
shown here. That is a C dimension. The channel  

219
00:20:30,400 --> 00:20:35,200
dimension is not shown here. So you can think 
of that for each grid point in this feature map,  

220
00:20:35,200 --> 00:20:42,880
there are many features. There 
are C features in that grid point.

221
00:20:42,880 --> 00:20:49,200
And then for this 3D convolution, basically, 
if we are talking about a 66 times 6 times  

222
00:20:49,200 --> 00:20:52,800
6 convolution, because we have 
one extra dimension. Instead of  

223
00:20:52,800 --> 00:20:58,000
slide over the spatial dimension just in 
the H and W dimension over the images,  

224
00:20:58,000 --> 00:21:06,720
now we are sliding over this cube. We are sliding 
over this cube of dimension T times H times W.

225
00:21:06,720 --> 00:21:13,200
So it includes both the spatial dimension and the 
temporal dimension. And also, it goes all the way  

226
00:21:13,200 --> 00:21:20,000
along the channel dimension. So then, gradually, 
you can-- just like 2D calculus. The other part  

227
00:21:20,000 --> 00:21:24,480
is just like the 2D convolution. It's 
just we have this extra dimension.

228
00:21:24,480 --> 00:21:29,520
And then you get this 6 times 6 times 
6 3D convolution. And maybe another  

229
00:21:29,520 --> 00:21:35,120
layer of 5 times 5. And finally, after you 
processed doing these 3D convolution operations,  

230
00:21:35,120 --> 00:21:39,520
and you flatten the feature vectors, and 
then you use a fully connected layers to  

231
00:21:39,520 --> 00:21:45,920
map them to the class scores. So that 
is basically the idea of 3D convolution.

232
00:21:46,800 --> 00:21:54,320
So let's walk maybe through some toy examples to 
better understand it, to compare the early, late,  

233
00:21:54,320 --> 00:22:03,680
and the 3D convolution neural networks. Just to 
give you a flavor of how it works. In practice,  

234
00:22:03,680 --> 00:22:07,520
actually, definitely, it's better-- can be 
much larger and more complicated. But here,  

235
00:22:07,520 --> 00:22:13,040
I'm just trying to show that a toy 
example to walk through the size of  

236
00:22:13,040 --> 00:22:16,320
the feature maps and also the receptive 
field to give you a sense about what's  

237
00:22:16,320 --> 00:22:20,000
the difference between early fusion and late 
fusion and 3D convolution neural networks.

238
00:22:20,000 --> 00:22:25,920
So for late fusion, you can think 
of that, for example, in this case,  

239
00:22:25,920 --> 00:22:32,280
maybe originally the input is 3 times 
20. 20, the temporal dimension. And 64,  

240
00:22:32,280 --> 00:22:40,720
64 is a spatial dimension. And you use a 
2D convolution. Because we are doing late  

241
00:22:40,720 --> 00:22:45,280
fusion. We don't do anything over the temporal 
dimension initially. We just keep the 20,  

242
00:22:45,280 --> 00:22:49,040
the temporal dimension. We just build 
up the receptive field spatially.

243
00:22:49,040 --> 00:22:53,520
Now, we have a convolution 2D layer to 
map the channel dimension from 3 to 12,  

244
00:22:53,520 --> 00:22:58,000
but just keep the temporal dimension 20. 
And then gradually, maybe we use some  

245
00:22:58,000 --> 00:23:02,640
pooling layers. Still, we didn't do anything 
with the temporal dimension. So it's still 20.  

246
00:23:02,640 --> 00:23:08,960
But because of the pooling operation, we build 
up the receptive field in the spatial dimension.

247
00:23:08,960 --> 00:23:14,160
And then gradually, we maybe use another 
2D layer. And now, the feature map is 24  

248
00:23:14,160 --> 00:23:19,680
times 20 times 16 times 16. And we also gradually 
increase the spatial receptive field, but it will  

249
00:23:19,680 --> 00:23:23,920
still keep the temporal dimension 20. So we 
didn't do anything over the temporal dimension.

250
00:23:23,920 --> 00:23:28,480
And finally, just using a single global 
average pooling, we pull across the feature  

251
00:23:28,480 --> 00:23:34,400
maps 20 times 16 times 16. So we pull over 
both time in the spatial dimension. And now,  

252
00:23:34,400 --> 00:23:39,760
we get from this 20 times 16 times 16. We get a 
1 times 1 times 1 feature point. So basically,  

253
00:23:39,760 --> 00:23:42,640
we collapse everything in the 
final single layer. And we build  

254
00:23:42,640 --> 00:23:47,760
up the temporal receptive field in a 
single layer. So that's a late fusion.

255
00:23:47,760 --> 00:23:50,400
So then for early fusion, 
what's the difference? So now,  

256
00:23:50,400 --> 00:23:54,320
instead of building slowly in 
space or at once in time at N,  

257
00:23:54,320 --> 00:24:00,960
now, we are building slowly in space and all 
at once in time at the very beginning. So the  

258
00:24:00,960 --> 00:24:07,360
input is still 3 times 20 times 64 times 64. But 
now, we're just using a single conv 2D layer.

259
00:24:07,360 --> 00:24:11,520
Now, we just treat this 3 times 20 as a 
single-- the channel dimension. We just  

260
00:24:11,520 --> 00:24:16,640
map everything. There's 3 times 30. Just 
treat all of them as channel dimension,  

261
00:24:16,640 --> 00:24:23,200
and then map them to 12. So basically, we use a 
single convolution layer, 2D convolution layer,  

262
00:24:24,160 --> 00:24:26,800
to collapse all the temporal 
information from the very beginning.

263
00:24:26,800 --> 00:24:31,200
So we build the temporal receptive field 
in the first layer. So now, the temporal  

264
00:24:31,200 --> 00:24:36,320
receptive field becomes from 1 to 20. And then 
the spatial receptive field gradually builds  

265
00:24:36,320 --> 00:24:42,320
up. And we use pooling and conv 2D to build 
up the spatial dimension just as late fusion.

266
00:24:42,320 --> 00:24:46,080
And finally, we use the global average 
pooling. Now, with global average pooling  

267
00:24:46,080 --> 00:24:53,360
is only just trying to doing the averaging, 
doing the pooling across space. So we build  

268
00:24:53,360 --> 00:24:58,000
slowly in space, but all at once at the 
very beginning. So that's early fusion.

269
00:24:58,000 --> 00:25:03,360
So then, what is the 3D convolution neural 
networks? So for 3D convolution layer, basically  

270
00:25:03,360 --> 00:25:08,800
we build slowly both in space and time. So 
that's why we call it the slow fusion. So the  

271
00:25:08,800 --> 00:25:14,960
input can still be the same 3-- times 20 times 64 
times 64. But now, we are using 3D convolutions.

272
00:25:14,960 --> 00:25:27,840
So in the first layer, we just-- to map things 
from 3 to 12. But we also just keep the temporal  

273
00:25:27,840 --> 00:25:32,160
dimension in this case. And then we build 
up a little bit temporal receptive field  

274
00:25:32,160 --> 00:25:36,880
and a spatial receptive field. And then 
we use a pooling layer. And then 4 times  

275
00:25:36,880 --> 00:25:41,760
4 times 4 for pooling layer. And then we pull 
a little bit of this temporal feature and also  

276
00:25:41,760 --> 00:25:46,240
spatial features. And then we further build up 
both the spatial and temporal receptive field.

277
00:25:46,240 --> 00:25:51,600
And we have another conv 3D layer, and 
then to further build up the spatial and  

278
00:25:51,600 --> 00:25:55,920
temporal receptive field. And finally, we're 
using a global average pooling. But now,  

279
00:25:55,920 --> 00:26:01,520
we're pooling over these 4 times 16 times 16 
feature map, and then to further increase the  

280
00:26:01,520 --> 00:26:06,000
temporal and spatial receptive field. So we are 
building up gradually in both space and time.  

281
00:26:06,000 --> 00:26:12,000
So that's the difference between early fusion, 
late fusion, and 3D convolution neural networks.

282
00:26:12,000 --> 00:26:18,880
So you can see that for the early fusion and 
3D convolution neural networks. So both of them  

283
00:26:18,880 --> 00:26:24,400
builds receptive fields over time. But what's 
the actual difference? So let's look at it  

284
00:26:25,200 --> 00:26:36,160
more closely here. So if we think of it as a 
feature vector for each spatial grid point,  

285
00:26:36,160 --> 00:26:44,560
so the common filter, if it's a 2D 
convolution, for this grid point,  

286
00:26:45,200 --> 00:26:54,400
it will consider all the temporal 
dimensions. T is equal to 16.

287
00:26:55,120 --> 00:27:04,000
So it is local in space but extends fully 
in time. That's the filter in 2D convolution  

288
00:27:04,000 --> 00:27:10,400
neural network. But what is the problem? 
Think about it. So if we directly just go  

289
00:27:10,400 --> 00:27:15,120
all the way through time dimension in this 2D 
convolution, what problem is going to happen?

290
00:27:15,120 --> 00:27:22,480
So the shortcoming of that is that there will 
be no temporal shift invariance. It's because  

291
00:27:22,480 --> 00:27:29,280
the 2D, the filter, now extends fully in 
time. So if we want to learn some global  

292
00:27:29,280 --> 00:27:32,240
transition in color of different 
time, for example, if we want to--  

293
00:27:33,680 --> 00:27:39,600
because it's a video. It's something that 
we want to recognize temporal information.

294
00:27:39,600 --> 00:27:43,360
If there are some changes from blue 
to orange at different time step,  

295
00:27:43,360 --> 00:27:49,280
maybe there is some change that is happening 
at time step 4. There's another same change  

296
00:27:49,280 --> 00:27:55,600
that is happening at time step 15. But 
it's the same change from blue to orange.

297
00:27:56,640 --> 00:28:04,080
If we go all the way through time, the filter 
extends fully in time, then if we want to learn  

298
00:28:04,080 --> 00:28:09,840
the global transition at different times, then we 
have to have a whole separate filter in order to  

299
00:28:09,840 --> 00:28:15,200
learn this. So we have to learn a different-- 
this kernel in order to learn these different  

300
00:28:15,200 --> 00:28:21,920
transitions across at different time steps. 
So there is no temporal shift invariance.

301
00:28:22,800 --> 00:28:30,080
So how to recognize this kind of blue to orange 
transition just anywhere in space and time? Just  

302
00:28:30,080 --> 00:28:35,840
like when we are doing image classification, we 
want to have some spatial invariance. We want to  

303
00:28:35,840 --> 00:28:41,120
be able to recognize the image contains a cat. No 
matter where the cat is on the right corner left  

304
00:28:41,120 --> 00:28:49,520
corner, we want to share the kernels to be able to 
recognize things at a different spatial location.

305
00:28:49,520 --> 00:28:53,040
Here, we want to be able to learn 
these different types of motion,  

306
00:28:53,040 --> 00:28:59,200
different types of temporal patterns at different 
temporal time steps. So that's similar idea. So  

307
00:28:59,200 --> 00:29:04,880
then, that's exactly the benefit of 3D 
convolution neural networks. Right now,  

308
00:29:04,880 --> 00:29:10,160
instead of extends fully in time, in 
this T dimension, originally for this  

309
00:29:10,160 --> 00:29:14,400
early version T, extends all the way in 
the temporal dimension T is equal to 16.

310
00:29:14,400 --> 00:29:19,520
But now if, T is equal to 3. And we can 
slide over the temporal dimension. Just  

311
00:29:19,520 --> 00:29:24,000
like we learned spatial invariance 
using filter and local regions. Now,  

312
00:29:24,000 --> 00:29:29,680
this conv filter only spans a local window 
in time and slide over in the time dimension.

313
00:29:29,680 --> 00:29:36,720
So then, the benefit is that now, we can have some 
temporal shift invariance. Because each filter  

314
00:29:36,720 --> 00:29:42,480
slides over time. So we can reuse this filter to 
recognize different motion patterns across these  

315
00:29:42,480 --> 00:29:48,160
dimensions. So the transition from blue to orange 
can now be recognized at every moment in time.

316
00:29:49,360 --> 00:29:55,600
And then, the benefit of this is that we don't 
have to have separate filters. Now, we are more  

317
00:29:55,600 --> 00:30:01,120
representation efficient. We don't need to learn 
separate filters anymore. So that's basically  

318
00:30:01,120 --> 00:30:08,560
the main difference between 2D conv early 
fusion and the 3D convolution neural network.

319
00:30:08,560 --> 00:30:13,920
And also, in the last lecture, 
I think, you have already also  

320
00:30:13,920 --> 00:30:20,480
seen some examples of some tools that we can 
use to visualize what we have learned in a 2D  

321
00:30:20,480 --> 00:30:27,200
convolution neural networks. Similarly, we 
can also visualize filters just for this 3D  

322
00:30:27,200 --> 00:30:35,280
convolution networks as these video clips. You 
can see that-- I'm not sure if you can see it.

323
00:30:35,280 --> 00:30:42,400
The learned filters from the 3D convolution neural 
networks, because now the filter extends both in  

324
00:30:42,400 --> 00:30:51,520
space and time, so we can see that as a video 
clip. And you can see that for these filters,  

325
00:30:52,480 --> 00:30:58,560
some of them are just like those filters 
you have seen for image classifier.

326
00:30:58,560 --> 00:31:03,520
You can have this color patterns and also 
these different edges. But you can also  

327
00:31:03,520 --> 00:31:09,600
see that there are some other filters. There 
are some temporal transition from one corner  

328
00:31:09,600 --> 00:31:16,960
to another or from some one edge pattern 
to another. And some doesn't learn motion,  

329
00:31:16,960 --> 00:31:23,440
and some maybe focus on just the color patterns. 
But some learns motion in different directions.

330
00:31:23,440 --> 00:31:29,040
So we can just visualize these kernels 
like this to interpret them. Basically,  

331
00:31:30,080 --> 00:31:36,400
two difference-- one is this slow fusion. 
In terms of convolution operation, yeah,  

332
00:31:36,400 --> 00:31:39,520
indeed, basically 3D convolution. 
3D convolution, definitely.

333
00:31:39,520 --> 00:31:44,960
And 2D convolution, they are totally different. 
You have another dimension of convolution. You  

334
00:31:44,960 --> 00:31:47,120
have this temporal dimension. So the 
difference is you have this temporal  

335
00:31:47,120 --> 00:31:52,560
dimension in the convolution operation. But 
practically, you use 3D convolution neural  

336
00:31:52,560 --> 00:31:57,280
networks. It gradually builds this 
receptive field over space and time.

337
00:32:01,040 --> 00:32:08,800
So we have talked about this tools, 3D 
convolution networks or architectures.  

338
00:32:08,800 --> 00:32:13,360
But what data we can use just like 
ImageNet? Or what data we can use  

339
00:32:13,360 --> 00:32:18,960
to do video to train a video classifier? So 
one example dataset, challenge dataset that  

340
00:32:19,600 --> 00:32:27,680
people have been tackling is this dataset called 
Sports 1 million, which was introduced in 2014.

341
00:32:27,680 --> 00:32:33,120
So for this dataset, you can see what tasks 
we can do. We can do very, very fine-grained  

342
00:32:33,120 --> 00:32:37,120
sports category classification. 
You can see here the blue shows,  

343
00:32:37,120 --> 00:32:46,000
the ground shoes. And yellow, it shows 
the top five predictions. And the green  

344
00:32:46,000 --> 00:32:49,200
shows the correct prediction. And the 
red shows the incorrect prediction.

345
00:32:49,200 --> 00:32:52,560
You can see that the action 
categories in the dataset is very,  

346
00:32:52,560 --> 00:32:59,600
very fine-grain. There are 487 different 
types of sports. There can be marathon,  

347
00:32:59,600 --> 00:33:02,960
ultra marathon. Actually, I don't 
know the difference between them,  

348
00:33:02,960 --> 00:33:09,680
but it's very [INAUDIBLE] to different 
types of sports categories in this dataset.

349
00:33:09,680 --> 00:33:14,960
And here are some results if we train these 
different types of classifiers we have talked  

350
00:33:14,960 --> 00:33:20,720
about on this Sports 1 million dataset. So 
one very shocking results you can probably  

351
00:33:20,720 --> 00:33:27,520
see here is that for this single frame model 
that I asked you to try, if you want to develop  

352
00:33:27,520 --> 00:33:33,440
some video classification model, is that 
it actually has a very good performance.

353
00:33:33,440 --> 00:33:38,880
You can see that the single frame model, if you 
just treat it as an image classifier, it actually  

354
00:33:38,880 --> 00:33:45,760
gives you 77.7 classification, top five accuracy. 
And for the early fusion we talked about,  

355
00:33:45,760 --> 00:33:51,040
it actually has a slightly worse performance. And 
for late fusion is slightly better. And if we use  

356
00:33:51,040 --> 00:33:59,120
3D convolutional neural networks, in this case, 
on this dataset, it gets like a 2% to 3% boost.

357
00:33:59,840 --> 00:34:04,800
So the takeaway message here is that, 
definitely, you should try the single  

358
00:34:04,800 --> 00:34:12,800
frame model. [INAUDIBLE] actually works pretty 
well. But the 3D convolution neural networks--  

359
00:34:12,800 --> 00:34:18,720
I showed here is the 3D convolution neural 
network used in 2014. But over the past 10 years,  

360
00:34:18,720 --> 00:34:23,600
we have seen a lot of advancements. 
So the numbers are also getting much,  

361
00:34:23,600 --> 00:34:26,640
much better, as I'm going to 
talk about in the later slides.

362
00:34:26,640 --> 00:34:30,400
For both training and testing, it's 
just treating videos as images and  

363
00:34:30,400 --> 00:34:34,880
train an image classifier. That's exactly 
what a single frame is doing. Basically,  

364
00:34:34,880 --> 00:34:38,240
if I understand question correctly, 
it's using image classifier. But it's  

365
00:34:38,240 --> 00:34:41,520
training on a lot of frames on videos. 
It's not a single frame in each video.

366
00:34:43,520 --> 00:34:47,600
And also, because this dataset is a 
huge dataset. Because like I mentioned,  

367
00:34:47,600 --> 00:34:55,600
videos are very huge. When people sharing 
video datasets, we cannot just share them  

368
00:34:55,600 --> 00:35:02,640
just like ImageNet. People can download from, 
some database because videos are really huge. Like  

369
00:35:02,640 --> 00:35:08,800
this dataset has maybe 1 million videos. It's not 
very doable to download all of them, to share it.

370
00:35:08,800 --> 00:35:14,160
Actually, this video actually, originally, when 
it was released, is shared as a list of URLs,  

371
00:35:14,160 --> 00:35:19,280
YouTube URLs. But one thing you can expect 
from this YouTube video URLs is that people  

372
00:35:20,160 --> 00:35:24,880
modify their videos and delete their 
videos. And so that original list maybe  

373
00:35:24,880 --> 00:35:30,560
have 1 million videos. But now, I guess, 
maybe half of the videos are already gone  

374
00:35:30,560 --> 00:35:36,960
or maybe not there. So this dataset is 
not very stable because of this reason.

375
00:35:37,600 --> 00:35:49,920
So like I mentioned, 3D convolution neural 
networks have been improving gradually since  

376
00:35:49,920 --> 00:35:56,000
May 2014. So one early popular version of 
this 3D convolution network is this model  

377
00:35:56,000 --> 00:36:01,760
called [? C3N ?] network. So basically, 
it's actually very simple. Basically,  

378
00:36:03,440 --> 00:36:08,640
it's very similar to the VGG architecture 
we use for 2D image classification.

379
00:36:09,200 --> 00:36:19,120
But now, we just convert things to three-dimension 
convolution neural network. And for example,  

380
00:36:19,120 --> 00:36:25,760
for the 3D CNN, you use 3 times 3 times 
3 conv and 2 times 2 times 2 pooling.  

381
00:36:25,760 --> 00:36:31,280
And except maybe for the first layer, it has 
some changes. So the overall architecture is  

382
00:36:31,280 --> 00:36:36,640
very similar to VGG architecture. It's 
now just we have this extra dimension.

383
00:36:36,640 --> 00:36:43,600
And so that's why it's called the 
VGG of 3D CNNs. And the model,  

384
00:36:44,160 --> 00:36:50,400
it was trained on this Sports 1 million dataset 
said that I just mentioned, and because it  

385
00:36:50,960 --> 00:36:58,080
was introduced like in 2014. And at that time, 
imagine that you want to train such a model.  

386
00:36:58,080 --> 00:37:07,120
It needs a lot of compute because not too many 
people have access to a lot of GPUs at that time.

387
00:37:07,120 --> 00:37:13,040
So actually, this model was trained at Facebook. 
And they released this model, the pre-trained  

388
00:37:13,040 --> 00:37:19,920
weights, they train the 3D model on Sports 1 
million, and then they released the feature  

389
00:37:19,920 --> 00:37:23,760
pre-trained model as a feature extractor. 
So many people actually who cannot afford  

390
00:37:23,760 --> 00:37:28,400
to train a video model themselves, they actually 
started to use this model as a feature extractor.

391
00:37:28,400 --> 00:37:31,600
So they can just take a video 
and extract features from this  

392
00:37:33,600 --> 00:37:37,360
using this pre-trained model. So a 
3D model, and then maybe train some  

393
00:37:37,360 --> 00:37:44,880
other linear classifier. So people start 
to use it. That's why it got popular.

394
00:37:44,880 --> 00:37:48,720
So the question basically is about what we're 
talking about, [INAUDIBLE] classification about  

395
00:37:48,720 --> 00:37:52,480
how many frames we should take as input in 
terms to extract the features? So basically,  

396
00:37:52,480 --> 00:37:56,720
for all these models we are talking about, 
we assume that we are just passing a clip,  

397
00:37:56,720 --> 00:38:03,520
a predefined length, like 16 frames or 32 frames 
to train a single model that always takes 16  

398
00:38:03,520 --> 00:38:09,280
frames or 32 frames as input. And there are other 
techniques we can talk about, how we're going to  

399
00:38:09,280 --> 00:38:16,080
aggregate this clip level prediction. But for now, 
we're just doing clip level feature extraction.

400
00:38:16,080 --> 00:38:21,360
So the downside of this 3D CNN is that it's 
very computationally expensive. Basically,  

401
00:38:21,360 --> 00:38:27,760
we just directly, in a brute force way, 
we just make this VGG style from 2D to  

402
00:38:27,760 --> 00:38:35,440
3D. And you can see that for AlexNet. For this 
GFLOP, basically, what it means is that it's  

403
00:38:35,440 --> 00:38:40,800
like gigaflops. It's trying to measure how 
many floating point operations you need for  

404
00:38:40,800 --> 00:38:45,680
a single forward pass, basically just trying to 
measure whether the network is efficient or not.

405
00:38:45,680 --> 00:38:54,320
So for AlexNet, it takes 0.7 GFLOPS. For 
VGG-16, it takes like 13.6 GFLOPS. But for C3D,  

406
00:38:54,320 --> 00:39:01,040
you are actually doing this kind of mapping from 
2D to 3D. And now, it takes like 39.5 GFLOPS. So  

407
00:39:01,040 --> 00:39:10,080
it's 2.9 times VGG, so it's not very efficient. 
So that's the downside of this kind of network.

408
00:39:10,640 --> 00:39:16,640
And if we look at the performance on 
Sports 1 million, this is just 360 now,  

409
00:39:17,280 --> 00:39:26,960
gets about 4% gain in terms of top five 
accuracy. So this is just one example  

410
00:39:26,960 --> 00:39:31,520
of the 3D convolution network we can do. 
But there definitely can be other things  

411
00:39:31,520 --> 00:39:36,240
right. We are talking about a lot of tricks 
that we can do for 2D image classification.

412
00:39:36,240 --> 00:39:41,200
We can have this residue connections like 
you have seen in ResNet. But definitely,  

413
00:39:41,200 --> 00:39:45,600
we can also do that just to improve, say, 3D 
to adding some residual connections or other  

414
00:39:45,600 --> 00:39:50,320
techniques we talked about in 2D convolutions. 
And indeed, there are also a lot of work on  

415
00:39:50,320 --> 00:39:59,280
trying to improve this, different types of 
video architectures, and also papers on that.

416
00:39:59,280 --> 00:40:05,280
But apart from that, let's think maybe a little 
bit more on whether we should treat space and  

417
00:40:05,280 --> 00:40:10,800
time in a separate way. Because that, indeed, 
very different things-- spatial information,  

418
00:40:10,800 --> 00:40:14,320
temporal information. So maybe 
we should actually explicit,  

419
00:40:14,320 --> 00:40:19,600
try to model things that exists 
there temporally. That is motion.

420
00:40:20,240 --> 00:40:26,720
So humans actually can do incredible 
job processing motion. So maybe  

421
00:40:26,720 --> 00:40:30,000
take a guess what actions the 
humans are doing here in this  

422
00:40:30,800 --> 00:40:45,840
simple video/ You can say it out 
if you want. What's this? Sitting.

423
00:40:45,840 --> 00:40:51,600
Yeah, just from these very- few points, you 
can actually do a good job just to recognize  

424
00:40:51,600 --> 00:41:03,280
what actions that this person is 
doing or maybe two persons. Now,  

425
00:41:03,280 --> 00:41:08,800
there are not any appearance information. 
Just a few points. Just motion.

426
00:41:08,800 --> 00:41:11,440
We can actually have a very good 
understanding about some activities  

427
00:41:11,440 --> 00:41:17,600
that is going on in these videos. So that's 
why how we process appearance and motion  

428
00:41:17,600 --> 00:41:22,960
might be very different. Maybe we should have 
separate networks to process them. So indeed,  

429
00:41:22,960 --> 00:41:28,160
that's kind of motivation for this 
work that was introduced in 2014.

430
00:41:30,240 --> 00:41:37,200
And they are trying to propose a two-stream 
network to process appearance information and  

431
00:41:37,200 --> 00:41:43,120
the motion information separately. So basically, 
one way to explicitly measure of motion is to use  

432
00:41:43,120 --> 00:41:47,840
this concept called optical flow. So for 
optical flow, basically, the idea is that  

433
00:41:48,720 --> 00:41:55,920
we want to measure the motion, the changes of 
the motion, of the pixels in adjacent frames.

434
00:41:55,920 --> 00:41:58,000
Basically, for the first frame, for every pixel,  

435
00:41:58,000 --> 00:42:02,320
how it's going to move in the second 
frame. So it calculates the velocity  

436
00:42:02,320 --> 00:42:06,960
for points within the frames and provides 
an estimation of where the points could be  

437
00:42:06,960 --> 00:42:12,400
in the next frame, a sequence. For example, 
in this case, like for frame T and p plus 1.

438
00:42:12,400 --> 00:42:20,000
Basically, this flow field, here are two 
dimensions, and tells where each pixel will  

439
00:42:20,000 --> 00:42:28,400
move in the next frame. So F(x, y) is equal to 
(dx, dy). And then the I t plus 1 (x plus dx),  

440
00:42:28,400 --> 00:42:35,840
that's where the pixel, E, in the next frame 
and is equal to I t (x, y) in the current frame.

441
00:42:35,840 --> 00:42:42,080
So it's a way to measure, explicitly measure 
motion of the pixels. So there are many papers  

442
00:42:42,080 --> 00:42:46,560
actually on doing research and also how to 
actually compute optic flow given a pair of  

443
00:42:46,560 --> 00:42:53,600
frames. There are ways to make different types 
of assumptions. Some work assume the optic flow,  

444
00:42:53,600 --> 00:42:57,200
just assumes brightly stays constant as things  

445
00:42:57,200 --> 00:43:00,800
move. And then trying to propose some 
techniques to compute this optic flow.

446
00:43:00,800 --> 00:43:03,680
But once you get it, it basically 
captures the motion information  

447
00:43:03,680 --> 00:43:09,440
for two adjacent frames. And also, these 
are two dimensions because it's trying to  

448
00:43:09,440 --> 00:43:14,560
capture how pixels move horizontally 
and vertically. So you can actually also  

449
00:43:14,560 --> 00:43:19,200
visualize it separately. You can visualize 
it the horizontal motion, horizontal flow,  

450
00:43:19,200 --> 00:43:23,600
dx. And also you can visualize the vertical 
flow dy. You can see that it captures some  

451
00:43:23,600 --> 00:43:28,640
horizontal motion, the vertical motion. So we 
capture this kind of low level motion cues.

452
00:43:28,640 --> 00:43:32,720
So once you have a way to capture this 
kind of motion cues as optical flow,  

453
00:43:32,720 --> 00:43:38,880
then people trying to propose two-stream networks 
to train a motion classifier and appearance  

454
00:43:38,880 --> 00:43:43,760
classifier. So this is a famous two-stream network 
for action recognition. So basically, it has a one  

455
00:43:43,760 --> 00:43:48,880
single frame model that's doing appearance 
classification to tell what action it is.

456
00:43:48,880 --> 00:43:55,920
And then you have a separate stream. That's the 
temporal stream ConvNet that takes this multiframe  

457
00:43:55,920 --> 00:44:01,440
optic flow. For every two adjacent frames, 
it computes the optical flow map. And also,  

458
00:44:01,440 --> 00:44:06,720
it separately treats the horizontal motion 
optical flow and vertical flow. And then stack  

459
00:44:06,720 --> 00:44:11,440
them together. And then they process them using 
a temporal stream convolution neural network.

460
00:44:11,440 --> 00:44:14,800
And then you make a prediction. And 
then they aggregate the prediction  

461
00:44:14,800 --> 00:44:18,640
results for both the motion stream 
and the appearance stream to get the  

462
00:44:18,640 --> 00:44:22,400
final prediction. So that's the 
idea of this two-stream network.

463
00:44:22,400 --> 00:44:27,840
And it actually works pretty good. It's on 
another dataset called UCF-101. There are  

464
00:44:29,440 --> 00:44:33,840
101 action categories in this dataset. 
So you can see that-- one surprising  

465
00:44:33,840 --> 00:44:38,800
thing you can see that is using only motion 
actually works very well, surprisingly well.

466
00:44:40,160 --> 00:44:45,280
You see the performance of 3D convolution network 
and spatial only. And that's only the appearance  

467
00:44:45,280 --> 00:44:48,560
string. And the temporal only, that's the 
motion stream. You can see that the motion  

468
00:44:48,560 --> 00:44:55,840
stream actually works much better compared to 
the spatial only stream, the appearance stream.

469
00:44:55,840 --> 00:45:03,840
So my hypothesis is that it's less easier 
to overfit. Because for the motion,  

470
00:45:04,800 --> 00:45:09,440
there are a lot of background information, 
which may be not important for the background,  

471
00:45:09,440 --> 00:45:13,040
for the action classification. 
But for the motion stream,  

472
00:45:13,040 --> 00:45:17,760
it actually contains the key 
information, the movements,  

473
00:45:17,760 --> 00:45:22,240
which are less easier to overfit. Actually, 
you can get better results on this dataset.

474
00:45:22,240 --> 00:45:32,080
So, so far, we have been talking about short 
term structures in videos. And also earlier,  

475
00:45:32,080 --> 00:45:39,280
I think folks asking about how many 
frames we should use actually to do  

476
00:45:39,280 --> 00:45:42,960
the classification? So definitely, 
it's very important to modeling the  

477
00:45:42,960 --> 00:45:50,080
long-term temporal structure to 
recognize more distant in time.

478
00:45:50,640 --> 00:45:57,200
So we already know actually. I already 
have the tools to handle sequences  

479
00:45:58,240 --> 00:46:07,280
to use recurrent networks, to process a 
sequence of words to doing some caption  

480
00:46:07,280 --> 00:46:13,920
tasks and some prediction tasks. So we can also 
use similar tools, just recurrent neural networks.

481
00:46:13,920 --> 00:46:20,320
We just have a convolutional neural networks. No 
matter whether it's a single frame convolutional  

482
00:46:20,320 --> 00:46:26,160
networks to get a 2D feature vector or use a 3D 
convolution network to get a feature vector from  

483
00:46:26,160 --> 00:46:32,880
a clip. But you have a much longer video. We 
can get a feature vector, and then we just use  

484
00:46:34,960 --> 00:46:40,320
the RNs or LSTMs we have talked about, to 
model the long-term temporal structure.

485
00:46:40,320 --> 00:46:48,880
We just process the local features using this 
recurrent networks and make a final prediction  

486
00:46:48,880 --> 00:46:53,520
at the last time step if we want to do 
a single video level classification,  

487
00:46:53,520 --> 00:46:59,440
where we're just doing a many to one output 
at the end of the video. And we can also do  

488
00:47:01,680 --> 00:47:03,760
one to one mapping like we talked about.

489
00:47:04,400 --> 00:47:09,680
So for each frame, we can make a prediction. 
Maybe there are some predictions we want to make  

490
00:47:09,680 --> 00:47:16,960
for each video frame. And we can also get this 
output from LSTM or recurrent neural network.

491
00:47:16,960 --> 00:47:27,280
And actually, this kind of idea has already been 
explored in 2011. Actually, that's way ahead of  

492
00:47:27,280 --> 00:47:35,280
its time. Because AlexNet was introduced in 2012. 
So, but it's more popularized by this 2015 paper.

493
00:47:35,280 --> 00:47:41,760
So if you want to train this kind of 
recurrent architectures for modeling  

494
00:47:41,760 --> 00:47:48,080
long-term temporal structure, you can often 
only backpropagate through this [INAUDIBLE]. Or  

495
00:47:48,080 --> 00:47:51,840
you can fuse the CNNs. You can 
pre-train them on some clips,  

496
00:47:51,840 --> 00:47:57,040
on image classification. And otherwise, you 
have a huge network with recurrent part,  

497
00:47:57,040 --> 00:48:02,480
this convolution part. It's very hard to 
train them end to end. So you can just use  

498
00:48:02,480 --> 00:48:07,440
it [INAUDIBLE] 3D as a feature extractor 
and train this recurrent neural networks.

499
00:48:07,440 --> 00:48:14,240
And we have already seen two approaches to model 
the temporal structure. How about we can combine  

500
00:48:14,240 --> 00:48:19,760
these two approaches, This convolution neural 
networks and this recurrent neural network.  

501
00:48:19,760 --> 00:48:24,880
Both of them has some advantages. So we 
can maybe just combine them in a single  

502
00:48:25,680 --> 00:48:28,480
architecture to process this video data.

503
00:48:28,480 --> 00:48:32,640
So indeed, we can take some inspiration from 
this multi-layer recurrent neural networks we  

504
00:48:32,640 --> 00:48:38,000
have talked about. So each timestep can take 
these previous hidden timestep from the same  

505
00:48:38,000 --> 00:48:42,000
layer, and also the output from 
the same timestep from the previous  

506
00:48:42,000 --> 00:48:44,880
layer. That's basically the 
idea of this multi-layer RNN.

507
00:48:44,880 --> 00:48:50,640
But similarly, we can just do it for videos. 
Now, we introduce this recurrent convolution  

508
00:48:50,640 --> 00:48:59,360
neural networks. It's very similar. It's just now, 
we build this grid of features, where each one is  

509
00:48:59,360 --> 00:49:05,200
a three-dimensional vector. Like two spatial 
dimension, and the one is a channel dimension.

510
00:49:07,040 --> 00:49:12,560
So each feature vector, it's like the 
fourth dimension C times H times W.  

511
00:49:12,560 --> 00:49:16,080
So each depends on two inputs for 
each vector. For each feature map,  

512
00:49:16,080 --> 00:49:20,480
it depends on the feature map from the 
same layer by the previous timestep. But  

513
00:49:20,480 --> 00:49:24,560
it also depends on the feature map from 
the previous layer but the same timestep.

514
00:49:26,800 --> 00:49:33,680
So you record in 2D convolution network, 
where we just map this feature map from  

515
00:49:33,680 --> 00:49:39,360
some input feature to an output feature. But 
here for this recurrent convolution network,  

516
00:49:39,360 --> 00:49:48,240
we can just use as input these two 3D tensors. 
One from the same layer and previous timestep,  

517
00:49:48,240 --> 00:49:50,960
and one from the previous 
layer at the same timestep.

518
00:49:50,960 --> 00:50:00,800
So you recall that the recurrent network, you have 
this form of it has some hidden layer feature map,  

519
00:50:00,800 --> 00:50:06,880
ht minus 1. It takes the input of this current 
timestep, have some function with some parameter  

520
00:50:06,880 --> 00:50:13,920
W. And then process the new state feature 
vector ht. That's basically the key of RNN.

521
00:50:13,920 --> 00:50:19,040
So now, instead, we just change these 
vectors forms of RNN. We just replace  

522
00:50:19,040 --> 00:50:25,440
all this matrix multiplication in recurrent 
neural networks with 2D convolutions. Now,  

523
00:50:25,440 --> 00:50:28,800
we get this recurrent convolution neural 
networks. So we have the feature map.

524
00:50:28,800 --> 00:50:32,880
You do 2D convolution instead have this 
matrix multiplication. We get another  

525
00:50:32,880 --> 00:50:37,600
feature map. And also for features from 
the previous layer, the same timestep,  

526
00:50:37,600 --> 00:50:43,440
also do this. And then after doing this 
2D convolution, we add them together,  

527
00:50:43,440 --> 00:50:50,720
use another tanh layer, and then we get the 
feature map for the current hidden layer.

528
00:50:50,720 --> 00:50:54,640
So that's basically the idea of recurrent 
convolution neural network. Basically, we  

529
00:50:54,640 --> 00:50:57,520
can combine convolution operations and recurrent  

530
00:50:57,520 --> 00:51:02,320
operations. And we can also actually do this 
for any kind of recurrent neural network  

531
00:51:02,320 --> 00:51:08,080
variants like GRUs and LSTMs, maybe you 
have already learned from previous class.

532
00:51:08,080 --> 00:51:14,160
And so now, we can successfully combine 
the benefits of the two. We have this both  

533
00:51:14,160 --> 00:51:22,240
spatial and temporal fusion inside this recurrent 
convolution neural network. But this model was not  

534
00:51:22,240 --> 00:51:29,680
used too much because there's one large downside 
of recurrent networks, which you may have already  

535
00:51:29,680 --> 00:51:35,440
learned that RNNs are very slow for processing 
non-sequence. And videos are usually very, very,  

536
00:51:35,440 --> 00:51:42,720
long, and you have to be process them in parallel. 
But RNNs are very hard to be parallelized.

537
00:51:42,720 --> 00:51:48,560
But there's another important model we have 
learned, I think in the previous lectures,  

538
00:51:48,560 --> 00:51:56,400
what we can do. We can also use operations 
like the self-attention to process videos. For  

539
00:51:56,400 --> 00:52:03,920
self-attention, you have this queries, keys, 
and values. And you can use self-attention  

540
00:52:03,920 --> 00:52:08,320
layer as a standalone operation to process 
images. Here, we can also do it for videos.

541
00:52:08,320 --> 00:52:15,360
And one very large advantage of self-attention 
is highly parallelizable. And all the alignment  

542
00:52:15,360 --> 00:52:21,840
and these attention scores for all the inputs can 
be done completely in parallel. So indeed, people  

543
00:52:21,840 --> 00:52:29,040
are trying to use self-attention also in videos. 
So they just pause self-attention directly to 3D.

544
00:52:29,040 --> 00:52:31,440
Maybe you have some 3D convolution 
neural network. You get some feature  

545
00:52:31,440 --> 00:52:36,720
map like C times T times H times W. And 
then similarly, you can get some query  

546
00:52:36,720 --> 00:52:41,760
feature maps. You can use some 1 times 
1 times 1 3D convolutions to change the  

547
00:52:41,760 --> 00:52:46,560
channel dimension to map them to query feature 
map that is C prime times T times H times W.

548
00:52:46,560 --> 00:52:50,960
Similarly for keys, you get this feature map 
for values, get this feature maps. And then you  

549
00:52:50,960 --> 00:52:56,160
want to get some attention weights. Basically, 
you're doing some transpose of this feature map  

550
00:52:56,160 --> 00:53:07,280
from queries. And the vectorized multiplication 
get a attention score for each query and key  

551
00:53:07,280 --> 00:53:13,360
feature pair. And then you can get this attention 
map and then use it to condition the values.

552
00:53:13,360 --> 00:53:21,200
And you can get another value, a feature map. 
And then you can map them. Use another 1 times  

553
00:53:21,200 --> 00:53:25,520
1 times 1 convolution to map them back 
to the same dimension C so that it can  

554
00:53:25,520 --> 00:53:31,280
be concatenated with the original feature input. 
So that is a residual connection. So in total,  

555
00:53:31,280 --> 00:53:36,160
you can see that it's very similar 
to the self-attention operations.

556
00:53:36,160 --> 00:53:41,280
But now, we move things to 3D. And this 
is one block that is very independent.  

557
00:53:41,280 --> 00:53:46,320
It can stand on its own. So you see in 
this paper, it's called nonlocal neural  

558
00:53:46,320 --> 00:53:51,120
network. It introduces block, and call 
it a nonlocal block. You can use it as  

559
00:53:51,680 --> 00:53:55,680
a building block for processing 
videos, to do video understanding.

560
00:53:55,680 --> 00:53:59,840
For example, you can just add this 
on nonlocal blocks into existing  

561
00:53:59,840 --> 00:54:06,560
3D convolution neural network architectures. 
And to have some 3D CNN on a nonlocal block,  

562
00:54:06,560 --> 00:54:12,480
and another block of 3D CNN add on a nonlocal 
block. And each nonlocal block, basically,  

563
00:54:12,480 --> 00:54:18,640
is very powerful to fuse across both space and 
time, and finally, into this classification.

564
00:54:18,640 --> 00:54:23,040
So the one thing we haven't talked about is 
what is this 3D convolution neural network?  

565
00:54:23,040 --> 00:54:30,640
So what we should use here? So a 
very interesting idea that people  

566
00:54:30,640 --> 00:54:35,520
have explored in the past that can is, 
can we reuse the 2D convolution neural  

567
00:54:35,520 --> 00:54:41,280
Network many successful architectures we have 
talked about or have learned, directly to 3D.

568
00:54:41,840 --> 00:54:48,560
We just do some inflation of this 2D networks. So 
then, we can get a 3D convolution neural networks.  

569
00:54:48,560 --> 00:54:55,120
So for this work, it's called I3D architecture. 
The idea is that they just take a 2D CNN  

570
00:54:55,120 --> 00:55:05,760
architecture. They replace each 2D conv pool, the 
layer that originates of dimension Kh times Kw.

571
00:55:05,760 --> 00:55:15,520
But now, we replace with a 3D version, that is a 
Kt times Kh times Kw, just inflated basically. And  

572
00:55:15,520 --> 00:55:28,320
they use it on top of the inception block. And 
then after they're doing this inflation, then  

573
00:55:28,320 --> 00:55:35,200
you have an architecture for processing videos, 
directly just reuse the existing architectures.

574
00:55:35,200 --> 00:55:42,240
And now, we can transfer the architecture 
that works pretty well in 2D to work  

575
00:55:42,240 --> 00:55:47,040
also in 3D. But taking one step 
further, people also have been  

576
00:55:49,600 --> 00:55:54,320
trying things that not only we can transfer the 
architectures. But actually, we also can transfer  

577
00:55:54,320 --> 00:56:00,800
the weights. Because we have already pre-trained 
a lot of architectures models on image datasets.

578
00:56:00,800 --> 00:56:03,840
Maybe we can actually reuse the weights 
we have learned there. There are maybe  

579
00:56:03,840 --> 00:56:08,240
some good prior information. So one thing 
you can do is that you can just initialize  

580
00:56:09,360 --> 00:56:12,240
the inflated CNN with weights trained on images.

581
00:56:12,240 --> 00:56:22,000
For example, you have maybe for one-- originally, 
you have this 2D conv kernel. You just copy the  

582
00:56:22,000 --> 00:56:29,680
kernel by Kt times. And you divide it by Kt. 
And originally takes one single image as input.

583
00:56:29,680 --> 00:56:34,880
Now, you take this video of 3 times Kt times H 
times w as input because we have divided them  

584
00:56:34,880 --> 00:56:42,912
by Kt. And you just use this inflated version 
and copy the weights by Kt times. And then you  

585
00:56:42,912 --> 00:56:51,440
will get the same output if you just input a 
single frame or a video of constant frames.

586
00:56:51,440 --> 00:56:59,520
So now, we have a way to recycle this existing 2D 
image based on this architecture and weights from  

587
00:56:59,520 --> 00:57:05,760
2D image understanding. And actually, it works 
pretty well. So if you look at the performance,  

588
00:57:05,760 --> 00:57:10,400
you inflate them. Compared to this two-stream 
convolutional network, it actually has better  

589
00:57:10,400 --> 00:57:14,080
performance. And you can also inflate 
actually not only the appearance frame.  

590
00:57:14,080 --> 00:57:20,880
You can also inflate a motion stream. So 
it actually gets some further improvements.

591
00:57:20,880 --> 00:57:25,840
Basically, this is just like a technique you 
can do to reuse this kind of independent from  

592
00:57:25,840 --> 00:57:33,600
the 3D convolutional networks. You can build 
this non-local blocks, this part. And what I'm  

593
00:57:33,600 --> 00:57:40,800
trying to say is that we have a lot of 2D 
convolution neural networks, the weights.

594
00:57:40,800 --> 00:57:45,840
Successful people have shown that they are 
very successful. And if we want to reuse them,  

595
00:57:45,840 --> 00:57:52,080
people have shown that they can actually 
copy the weights and reuse their weights,  

596
00:57:53,120 --> 00:57:58,480
directly use them to operate on videos. So 
that basically, that's the high level idea.

597
00:57:58,480 --> 00:58:05,040
And after doing this initialization, you can 
still fine tune on the video data. But you have  

598
00:58:05,040 --> 00:58:08,400
the pre-trained weights from images. So then,  

599
00:58:08,400 --> 00:58:12,160
we can give you some good initialization 
for training the video models. So this  

600
00:58:13,120 --> 00:58:18,960
idea of this I3D network basically is trying 
to copy the weights and doing the inflation.

601
00:58:20,560 --> 00:58:26,400
So this is also just one example of this 
video understanding net model. And there  

602
00:58:26,400 --> 00:58:33,440
are also many other video transfer model 
proposed for video understanding. That is,  

603
00:58:34,560 --> 00:58:37,840
for example, this work, space-time  

604
00:58:37,840 --> 00:58:42,640
attention is trying to do more factorized 
attention to attend both space and time.

605
00:58:42,640 --> 00:58:46,720
And also, there are some other methods trying to 
be more efficient in terms of this transformer  

606
00:58:46,720 --> 00:58:54,000
architecture. Or this mask autoencoder you have 
heard about doing more efficient, scalable video,  

607
00:58:54,000 --> 00:58:59,200
video level pre-training to doing video 
understanding. So I'm not going to talk them  

608
00:58:59,200 --> 00:59:02,720
here in the class. But if you are interested, 
you can check out their papers. Because there  

609
00:59:02,720 --> 00:59:07,760
are also many progress has been made to 
have better media understanding models.

610
00:59:07,760 --> 00:59:10,800
And if you look at the performance of 
progress that way, I think we started  

611
00:59:10,800 --> 00:59:16,720
from a single-frame model 62.2 on this-- 
this is another dataset, Kinetics-400.  

612
00:59:16,720 --> 00:59:22,240
It's a large video dataset. And then, you can 
see that for this video model encoder, now,  

613
00:59:22,240 --> 00:59:30,880
it already gets to 90% accuracy. So there are some 
other-- new transformer model has been proposed.

614
00:59:30,880 --> 00:59:39,440
So we are doing very well on classifying the 
videos. And similar to the image classification  

615
00:59:39,440 --> 00:59:45,200
in the last class, we can also use similar 
tricks for visualizing video models. So we can  

616
00:59:45,200 --> 00:59:51,440
take in this two-stream network as an example. 
We can randomly initialize a appearance image  

617
00:59:51,440 --> 00:59:57,680
and the flow image doing-- we are doing a 
forward pass, and then compute the score.

618
00:59:57,680 --> 01:00:01,840
And then we can back propagate with 
respect to the score of a particular  

619
01:00:01,840 --> 01:00:08,880
class and using gradient ascent to maximize the 
classification score. Just like what we're doing  

620
01:00:08,880 --> 01:00:15,760
the visualization for the image-based 
model. So then, it's this way that we  

621
01:00:15,760 --> 01:00:20,240
can visualize doing some visualization, 
interpretation of what has been learned.

622
01:00:20,240 --> 01:00:26,320
So the left is the optimized image for appearance 
string. Maybe it's hard to guess what is maybe  

623
01:00:26,320 --> 01:00:34,240
happening in the visual stream. On the right, 
it's optimized image for the flow stream. One  

624
01:00:34,240 --> 01:00:41,280
has some temporal constraints to prevent the 
temporal stream to change too fast. So you  

625
01:00:41,280 --> 01:00:46,800
can capture slow motion. And the other captures 
right motion. So you can guess what the action is.

626
01:00:46,800 --> 01:00:55,040
And maybe in this case, it's pretty clear. So what 
action is this. So this is a weightlifting. You  

627
01:00:55,040 --> 01:01:01,920
can see that-- the middle one is doing some 
bar shaking. And the right one is doing some  

628
01:01:02,560 --> 01:01:08,640
pushing overhead, the motion. So it's 
indeed actually-- you can see that this  

629
01:01:08,640 --> 01:01:13,120
video model is actually-- which models 
are learning something about this motion,

630
01:01:16,480 --> 01:01:23,360
So so far, I have been talking about how we 
can classify these short clips-- the swimming,  

631
01:01:23,360 --> 01:01:30,560
running. But another very important thing 
is that how we can-- other task is that  

632
01:01:31,600 --> 01:01:37,200
this is called temporal action localization 
is that not only we want to just doing clip  

633
01:01:37,200 --> 01:01:42,240
level or a classification. Sometimes, we want to 
localize, just we want to do object detection.

634
01:01:42,240 --> 01:01:46,800
Now, we want to localize where in the video 
the action is happening. Maybe sometimes the  

635
01:01:46,800 --> 01:01:51,440
person is running. Sometimes, it's jumping. 
So there is another task. This is another  

636
01:01:51,440 --> 01:01:57,840
class called temporal action localization. 
You can also use similar ideas from faster  

637
01:01:57,840 --> 01:02:03,600
R-CNN. You can just generate some temporal 
proposals and then doing the classification.

638
01:02:03,600 --> 01:02:08,400
And also, you can also do both. This is 
a spatial temporal detection. Basically,  

639
01:02:09,680 --> 01:02:14,000
you want to localize not only in space but also 
in time. Where the action is happening in space.  

640
01:02:14,000 --> 01:02:20,640
Where the action is happening temporally. So this 
is another task called spatial temporal detection.

641
01:02:21,920 --> 01:02:27,520
So so far, I have been talking 
about the temporal stream and  

642
01:02:27,520 --> 01:02:33,440
the architectures we can use to do 
3D CNN, two-stream neural networks,  

643
01:02:33,440 --> 01:02:39,520
spatial-temporal self-attention. And we have 
already talked about some tools to do that.

644
01:02:39,520 --> 01:02:46,080
But yeah, maybe in the final 10 minutes, 
let's just revisit. I hope to finish  

645
01:02:46,080 --> 01:02:50,160
in time. Let's revisit the 
example that we started today,  

646
01:02:50,160 --> 01:02:56,698
where I showed you a video. But that's 
still maybe not the full picture.

647
01:02:56,698 --> 01:02:56,714
[VIDEO PLAYBACK]

648
01:02:56,714 --> 01:03:10,533
[BABY LAUGHING]

649
01:03:10,533 --> 01:03:10,546
[DOG BARKING]

650
01:03:10,546 --> 01:03:10,560
[END PLAYBACK]

651
01:03:10,560 --> 01:03:13,120
So we are looking at video. 
I think video understanding,  

652
01:03:13,120 --> 01:03:18,240
there is another very important dimension that 
we have never covered until now. That is this,  

653
01:03:18,240 --> 01:03:22,000
there is sound. There is audio. There 
is other modalities in videos. If we  

654
01:03:22,000 --> 01:03:26,400
miss that ingredient, you lose a lot of 
fun. There's emotions you can perceive.  

655
01:03:26,400 --> 01:03:31,360
There's another interactions you can do 
if you combine this visual and motion.

656
01:03:31,360 --> 01:03:34,800
So if we have this audio in mind 
and we have this vision stream,  

657
01:03:34,800 --> 01:03:38,400
then people also have proposed many 
other interesting tasks. And also,  

658
01:03:38,400 --> 01:03:43,360
we have explored other tasks to doing video 
understanding. Here's another example that  

659
01:03:44,800 --> 01:03:48,800
in videos that maybe there are some 
multiple objects, multiple speakers.

660
01:03:48,800 --> 01:03:53,680
And you can actually-- one task, example 
task that I also personally have explored  

661
01:03:53,680 --> 01:03:57,280
in the past with a visually-guided audio 
source separation. And you can actually  

662
01:03:57,280 --> 01:04:01,600
understand trying to process things visually and 
acoustically. You can use the visual information  

663
01:04:01,600 --> 01:04:06,080
to guide the source separation. You want to 
separate the sound components. Originally,  

664
01:04:06,080 --> 01:04:08,560
maybe there's a mixture. You want to 
use the visual information to separate  

665
01:04:08,560 --> 01:04:12,480
into some sound components. This is called 
visually-guided audio source separation.

666
01:04:12,480 --> 01:04:17,440
So just to give you an example for this task. 
For example, here is a speech mixture. Maybe  

667
01:04:17,440 --> 01:04:21,600
sometimes, you want to hear the sounds for 
each person individually. And then we can use  

668
01:04:21,600 --> 01:04:25,200
their visual information, audio information 
to process them together, to separate their  

669
01:04:25,200 --> 01:04:29,280
sounds. So here is what we can do. We can 
separate the voice for the left speakers.

670
01:04:29,280 --> 01:04:33,920
So only we can do this for people, for 
speech. When we have to process audio and  

671
01:04:33,920 --> 01:04:38,000
speech and the visual stream. But we can 
also do this for other types of sounds,  

672
01:04:38,000 --> 01:04:41,680
like music instruments. Here's another 
example. We can even do musical instruments  

673
01:04:41,680 --> 01:04:46,160
separation by analyzing the motion, 
the object-centric information with  

674
01:04:46,160 --> 01:04:51,920
the audio stream and doing the separation. 
So this is another example for this task.

675
01:04:51,920 --> 01:04:56,880
And also, once we introduce this 
new modality of audio, we just want  

676
01:04:56,880 --> 01:05:01,280
to do video understanding classification. 
Audio can also be useful cues. So indeed,  

677
01:05:01,280 --> 01:05:04,720
there are other work, audio-visual 
video understanding work proposed  

678
01:05:04,720 --> 01:05:10,480
from transformer attention-based models, not 
only we want to map images, videos to patches.

679
01:05:10,480 --> 01:05:15,040
But also, map those audio spectrum to patches 
and use some transformer architectures to  

680
01:05:15,040 --> 01:05:21,040
doing the classification. Or even, we 
can do some mask autoencoder style. We  

681
01:05:21,040 --> 01:05:26,240
want to predict the patches for the images and 
also spectrograms doing video understanding.

682
01:05:28,000 --> 01:05:33,440
And also, another aspect people have 
been exploring is how to do efficient  

683
01:05:33,440 --> 01:05:39,920
video understanding. So I will just quickly give 
some examples. So here, throughout this class,  

684
01:05:39,920 --> 01:05:45,360
I think I'm mainly focusing on clip level 
classification. Just giving a clip, how to doing  

685
01:05:45,360 --> 01:05:50,400
this classification. And after we classify a lot 
of clips and we want to aggregate the information  

686
01:05:50,400 --> 01:05:54,560
to get a video level predictions. That's 
action recognition in long videos.

687
01:05:54,560 --> 01:05:58,480
So for efficient video understanding, 
why we would want to do efficient video  

688
01:05:58,480 --> 01:06:02,400
understanding? Because videos are very 
long. We cannot afford to process every  

689
01:06:02,400 --> 01:06:09,360
clip one by one. So that we're trying to 
increase the efficiency for a single clip  

690
01:06:09,360 --> 01:06:14,640
just to building like this X3D is trying to 
build better 3D convolution neural network.

691
01:06:14,640 --> 01:06:22,960
But also, that we're trying to-- like this SD 
sampler, trying to predict which clips are the  

692
01:06:23,600 --> 01:06:29,360
most useful. And then you can only combine the 
predictions. Only run your clip classifier on  

693
01:06:29,360 --> 01:06:35,600
those important clips. And also, they are doing 
policy learning, trying to predict which modality  

694
01:06:35,600 --> 01:06:39,440
we should use in order to doing this action 
classification we can select, or whether we want  

695
01:06:39,440 --> 01:06:45,520
to use video, or how many video clips, or whether 
we want to use audio or other sensory data.

696
01:06:46,160 --> 01:06:51,360
So here's one example that we can also 
use audio, as a preview mechanism,  

697
01:06:52,880 --> 01:07:00,480
to predict where are the important 
moments. And then we use that  

698
01:07:00,480 --> 01:07:05,840
as a guiding clue to process the clips and 
to average the results. So that's about the  

699
01:07:05,840 --> 01:07:10,400
efficient video understanding. So 
that's also one area of research.

700
01:07:10,400 --> 01:07:16,640
And also, nowadays, there are people moving 
to VR and AR, the smart glasses. And also now,  

701
01:07:16,640 --> 01:07:20,560
in the future, I'm guessing there are a 
lot of egocentric video streams. That's  

702
01:07:20,560 --> 01:07:24,640
another aspect of video understanding. So 
not only you have this egocentric videos,  

703
01:07:24,640 --> 01:07:29,680
but you also have this multi-microphone, 
microphone array, multi-channel audios.

704
01:07:30,880 --> 01:07:33,120
So how to doing better under video understanding  

705
01:07:33,120 --> 01:07:38,000
from this egocentric multimodal egocentric 
video streams is also a hot topic. Maybe  

706
01:07:39,360 --> 01:07:45,360
we have explored that we can use processes 
video streams, the audio, multichannel audio,  

707
01:07:45,360 --> 01:07:50,720
and visual information to predict who is speaking 
to whom and who is listening to whom. Imagine,  

708
01:07:50,720 --> 01:07:55,360
in the future, you wear these smart glasses. 
You want to use it to help you to understand  

709
01:07:55,360 --> 01:08:00,640
these different types of social interactions. 
So that's an egocentric video understanding.

710
01:08:00,640 --> 01:08:04,880
So my final slide, definitely, LLMs right now,  

711
01:08:06,240 --> 01:08:09,520
there are also a lot of ongoing work 
trying to build video level foundation  

712
01:08:09,520 --> 01:08:14,320
models. How to collect the video understanding to 
LLMs? So indeed, there are works trying to just  

713
01:08:14,320 --> 01:08:23,280
map the videos to tokenize them and map them to 
the LLM embedding space and maybe prompt the video  

714
01:08:23,280 --> 01:08:27,600
foundation model-- where the person is-- 
what the person is doing in the video.

715
01:08:27,600 --> 01:08:35,680
And then you output some text to describe 
the videos. So there are many work trying  

716
01:08:35,680 --> 01:08:41,520
to collect video understanding, LLMs. 
So that's also a hot topic right now.