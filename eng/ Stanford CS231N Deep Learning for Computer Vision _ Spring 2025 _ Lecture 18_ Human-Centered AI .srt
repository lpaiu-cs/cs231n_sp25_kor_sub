2
00:00:05,210 --> 00:00:10,410
Welcome to the last lecture
of the quarter for CS231N,

3
00:00:10,410 --> 00:00:14,150
and it was great to see
you guys at the beginning

4
00:00:14,150 --> 00:00:15,630
and now at the end.

5
00:00:15,630 --> 00:00:18,360
And this lecture is a
little bit of a departure.

6
00:00:18,360 --> 00:00:21,230
We're not going to
teach any new materials

7
00:00:21,230 --> 00:00:23,100
in terms of algorithms.

8
00:00:23,100 --> 00:00:27,590
It's more a talk that I'd like
to give to students to put

9
00:00:27,590 --> 00:00:33,830
a perspective both on a
more longer term research

10
00:00:33,830 --> 00:00:39,140
evolution, but as well
as another dimension that

11
00:00:39,140 --> 00:00:42,140
is important to today's
AI, which we would call it

12
00:00:42,140 --> 00:00:43,560
the human perspective.

13
00:00:43,560 --> 00:00:47,400
So for the completeness
of the material,

14
00:00:47,400 --> 00:00:49,520
there's a little
bit of an overlap

15
00:00:49,520 --> 00:00:53,040
that you might see from
other parts of this course,

16
00:00:53,040 --> 00:00:58,980
but hopefully, it makes
sense in a fuller way.

17
00:00:58,980 --> 00:01:05,200
So the title of this slide or
this lecture is what we see

18
00:01:05,200 --> 00:01:08,960
and what we value, AI with
the human perspective.

19
00:01:08,960 --> 00:01:12,040
And I know that some of you
have already heard about this.

20
00:01:12,040 --> 00:01:15,380
Is really the beginning,
the origin of vision,

21
00:01:15,380 --> 00:01:18,430
both in terms of
evolution as well as

22
00:01:18,430 --> 00:01:20,270
in terms of our technology.

23
00:01:20,270 --> 00:01:23,830
And we did talk
about the first slide

24
00:01:23,830 --> 00:01:30,260
that came to the animal world
back 540 million years ago.

25
00:01:30,260 --> 00:01:35,360
And that was when animals or
trilobites, to be specific,

26
00:01:35,360 --> 00:01:41,410
developed photosensitive
cells to glean

27
00:01:41,410 --> 00:01:43,670
what the outer world is about.

28
00:01:43,670 --> 00:01:49,510
And according to zoologists
like Andrew Parker, what

29
00:01:49,510 --> 00:01:56,960
happened is that because
of the onset of vision,

30
00:01:56,960 --> 00:02:02,370
it set off an evolutionary
arms race, where animals

31
00:02:02,370 --> 00:02:04,600
either evolved or died.

32
00:02:04,600 --> 00:02:09,210
And that arms race gave
rise to the speciation

33
00:02:09,210 --> 00:02:12,280
or explosive
speciation of animals,

34
00:02:12,280 --> 00:02:16,470
which now zoologists call
the Cambrian explosion

35
00:02:16,470 --> 00:02:18,580
or the big bang of evolution.

36
00:02:18,580 --> 00:02:22,470
And of course, you
wouldn't be surprised

37
00:02:22,470 --> 00:02:25,120
that vision is
still to this day,

38
00:02:25,120 --> 00:02:31,420
a primary sensory intelligent
system in many, many animals.

39
00:02:31,420 --> 00:02:35,980
Not all animals use vision,
admittedly, but many do.

40
00:02:35,980 --> 00:02:39,300
And that's also one of the
primary sensory systems

41
00:02:39,300 --> 00:02:41,170
for humans.

42
00:02:41,170 --> 00:02:48,060
And we use vision to do
everything from survival

43
00:02:48,060 --> 00:02:51,180
to work to entertainment
to socialization

44
00:02:51,180 --> 00:02:54,730
to learning, development,
and many other things.

45
00:02:54,730 --> 00:03:01,390
So that's the recapture
or summary of evolution.

46
00:03:01,390 --> 00:03:07,060
And we also briefly talked about
computer vision being a summer

47
00:03:07,060 --> 00:03:13,260
vision project back in the 1960s
as an attempt to use a couple

48
00:03:13,260 --> 00:03:18,240
of undergrads to construct
the significant portion

49
00:03:18,240 --> 00:03:19,570
of the visual system.

50
00:03:19,570 --> 00:03:23,770
And that was very in line
with the history of AI,

51
00:03:23,770 --> 00:03:28,660
where we tend to have
clarity of the North Star,

52
00:03:28,660 --> 00:03:32,230
but underestimate how
long it would take.

53
00:03:32,230 --> 00:03:34,990
We are still probably
experiencing that today,

54
00:03:34,990 --> 00:03:37,874
but a lot has happened.

55
00:03:37,874 --> 00:03:42,270
You don't need me to tell
you that from empowering

56
00:03:42,270 --> 00:03:45,600
self-driving car to
understanding images

57
00:03:45,600 --> 00:03:49,110
to the generative
AI revolution, we're

58
00:03:49,110 --> 00:03:54,770
seeing vision is playing a huge
role, and also in many parts,

59
00:03:54,770 --> 00:03:56,630
leading the wave.

60
00:03:56,630 --> 00:04:02,580
So maybe it's time to just
take a different look at this

61
00:04:02,580 --> 00:04:05,240
both historically and
going towards the future.

62
00:04:05,240 --> 00:04:09,090
Is where have we come from,
and where are we going?

63
00:04:09,090 --> 00:04:13,650
And this is an important
topic to discuss,

64
00:04:13,650 --> 00:04:16,310
because a lot of
what has happened

65
00:04:16,310 --> 00:04:18,950
will inform what will happen.

66
00:04:18,950 --> 00:04:23,390
So I'm organizing this
talk in three chunks.

67
00:04:23,390 --> 00:04:28,980
Is first of all, building
AI to see what humans see.

68
00:04:28,980 --> 00:04:30,660
And that's where we came from.

69
00:04:30,660 --> 00:04:33,980
That we were so inspired
by human capability

70
00:04:33,980 --> 00:04:37,560
that we want to make
machines that do the same.

71
00:04:37,560 --> 00:04:39,380
And then we'll talk
about building AI

72
00:04:39,380 --> 00:04:41,640
to see what humans don't see.

73
00:04:41,640 --> 00:04:44,390
And then we'll finish
with building AI to see

74
00:04:44,390 --> 00:04:46,970
what humans would like to see.

75
00:04:46,970 --> 00:04:48,840
Let's start with the first one.

76
00:04:48,840 --> 00:04:50,820
Building AI to see
what humans see.

77
00:04:50,820 --> 00:04:53,440
Again, just a little
bit of a review.

78
00:04:53,440 --> 00:04:55,930
Humans are so good at seeing.

79
00:04:55,930 --> 00:04:57,050
We know this.

80
00:04:57,050 --> 00:05:02,830
This is a very half a century
old experiment showing us

81
00:05:02,830 --> 00:05:07,810
that even watching a
video you've never watched

82
00:05:07,810 --> 00:05:11,890
played at 10 hertz, which
means every frame is only about

83
00:05:11,890 --> 00:05:14,750
on the screen for
100 milliseconds,

84
00:05:14,750 --> 00:05:18,220
you've never seen that, it's
still no problem for human eyes

85
00:05:18,220 --> 00:05:22,900
to detect a target, in
this case, a person.

86
00:05:22,900 --> 00:05:26,950
In this complex scene where you
have no idea prior knowledge

87
00:05:26,950 --> 00:05:29,290
about what this
person is, it really

88
00:05:29,290 --> 00:05:36,530
underscores the superb ability
of human visual understanding,

89
00:05:36,530 --> 00:05:39,560
especially object-focused
understanding.

90
00:05:39,560 --> 00:05:43,900
We also have briefly
mentioned that around the turn

91
00:05:43,900 --> 00:05:47,920
of the century,
neurophysiologists

92
00:05:47,920 --> 00:05:53,820
are measuring the speed of
vision in terms of humans

93
00:05:53,820 --> 00:06:01,290
seeing complex objects in the
form of brain signals, brain

94
00:06:01,290 --> 00:06:05,740
electrical signals
measured from EEG caps.

95
00:06:05,740 --> 00:06:10,020
And we see that
differentiating or categorizing

96
00:06:10,020 --> 00:06:13,480
animals versus animals
is a very complex task,

97
00:06:13,480 --> 00:06:18,930
yet humans are capable of
doing that at 150 millisecond

98
00:06:18,930 --> 00:06:21,160
after the onset of the stimuli.

99
00:06:21,160 --> 00:06:25,230
And this is remarkable
speed given the wetware

100
00:06:25,230 --> 00:06:28,300
we have under our skulls.

101
00:06:28,300 --> 00:06:31,830
Also neurophysiologists
have taught us

102
00:06:31,830 --> 00:06:38,070
that objects is a very
important functionality

103
00:06:38,070 --> 00:06:41,050
in our visual
intelligence in humans.

104
00:06:41,050 --> 00:06:45,390
So important that there are
neural correlates in our brain.

105
00:06:45,390 --> 00:06:49,240
Areas that are dedicated
to object understanding,

106
00:06:49,240 --> 00:06:54,330
such as phase areas or place
areas or body parts areas.

107
00:06:54,330 --> 00:06:57,680
This shows that evolution
has really spent time

108
00:06:57,680 --> 00:07:01,250
to hone in our visual
intelligence skills

109
00:07:01,250 --> 00:07:03,720
when it comes to
object recognition.

110
00:07:03,720 --> 00:07:10,040
So all this built up the history
for the field of computer vision

111
00:07:10,040 --> 00:07:13,850
that a few decades
ago, object recognition

112
00:07:13,850 --> 00:07:17,820
became a fundamental building
block for visual intelligence.

113
00:07:17,820 --> 00:07:20,640
And we want to empower
machines with that.

114
00:07:20,640 --> 00:07:25,280
And in order to do that, we
define the problem, or at least

115
00:07:25,280 --> 00:07:29,490
the original problem
as given an image,

116
00:07:29,490 --> 00:07:35,870
how do we empower, enable a
computer to call out the objects

117
00:07:35,870 --> 00:07:40,370
or what the object
is in the image?

118
00:07:40,370 --> 00:07:43,410
That's such an effortless
task for humans.

119
00:07:43,410 --> 00:07:45,230
But if you think about
it, now that you've

120
00:07:45,230 --> 00:07:48,950
learned enough computer vision
to know that mathematically,

121
00:07:48,950 --> 00:07:54,200
there's infinite possibilities
to actually recognize

122
00:07:54,200 --> 00:07:59,000
any object because of different
lighting, texture, background,

123
00:07:59,000 --> 00:08:04,610
occlusion, viewing angle,
scaling, and whatever,

124
00:08:04,610 --> 00:08:05,460
you name it.

125
00:08:05,460 --> 00:08:12,500
So this is actually
fundamentally a difficult task.

126
00:08:12,500 --> 00:08:16,920
The history pre-deep learning
is also very interesting.

127
00:08:16,920 --> 00:08:20,120
There were some
pretty heroic attempts

128
00:08:20,120 --> 00:08:25,040
at solving the problem of
generic generalizable object

129
00:08:25,040 --> 00:08:26,010
recognition.

130
00:08:26,010 --> 00:08:30,920
And the first wave of attempt
was actually very inspired

131
00:08:30,920 --> 00:08:32,730
by psychology itself.

132
00:08:32,730 --> 00:08:36,470
We self-introspect
sometimes even

133
00:08:36,470 --> 00:08:39,140
to the detriment of
over self-introspection.

134
00:08:39,140 --> 00:08:43,415
We think that humans
compose parts.

135
00:08:43,415 --> 00:08:47,060
We look at objects, we
can see geometric parts,

136
00:08:47,060 --> 00:08:50,360
and then we can compose
them into different objects.

137
00:08:50,360 --> 00:08:57,850
And that idea of using
predesignated parts or shapes

138
00:08:57,850 --> 00:09:03,340
and to compose them
in specific ways

139
00:09:03,340 --> 00:09:07,340
was the first wave of
object recognition.

140
00:09:07,340 --> 00:09:11,260
So these are different
work or models coming from

141
00:09:11,260 --> 00:09:15,430
the '70s, '80s, or even going
all the way to '90s of using

142
00:09:15,430 --> 00:09:18,220
different parts and
configurations to recognize

143
00:09:18,220 --> 00:09:19,040
objects.

144
00:09:19,040 --> 00:09:21,260
Of course, it
didn't really work.

145
00:09:21,260 --> 00:09:24,590
It's mathematically beautiful
and simple, but it didn't work.

146
00:09:24,590 --> 00:09:30,070
So a second wave of object
recognition pre-deep learning

147
00:09:30,070 --> 00:09:36,970
was actually a really important
era in the field of AI.

148
00:09:36,970 --> 00:09:39,380
Is really the beginning
of machine learning.

149
00:09:39,380 --> 00:09:41,330
Statistical machine learning.

150
00:09:41,330 --> 00:09:44,140
It was the marriage between
computer programming

151
00:09:44,140 --> 00:09:46,480
and statistical modeling.

152
00:09:46,480 --> 00:09:49,320
And with that marriage,
we start to realize

153
00:09:49,320 --> 00:09:53,550
the world is so complex, these
problems, these intelligence

154
00:09:53,550 --> 00:09:57,000
problems, whether it's visual
intelligence or language

155
00:09:57,000 --> 00:10:00,150
intelligence or other
kinds of intelligence,

156
00:10:00,150 --> 00:10:05,710
in order to generalize, we
need to learn the parameters.

157
00:10:05,710 --> 00:10:15,190
It's very hard to use hand-tuned
models to get good learning.

158
00:10:15,190 --> 00:10:19,140
We now know we need data, even
though we didn't at that time

159
00:10:19,140 --> 00:10:20,350
know how much data.

160
00:10:20,350 --> 00:10:25,200
But we also know that we
need to design or architect

161
00:10:25,200 --> 00:10:28,320
statistical models
so that they have

162
00:10:28,320 --> 00:10:31,950
the capability of learning
through different learning

163
00:10:31,950 --> 00:10:32,710
rules.

164
00:10:32,710 --> 00:10:37,650
And because of that, we saw a
blossoming of models in that era

165
00:10:37,650 --> 00:10:41,250
where we're learning
random fields or Bayes nets

166
00:10:41,250 --> 00:10:45,110
or support vector
machines and all that.

167
00:10:45,110 --> 00:10:50,720
And in fact, a lot of progress
was made by the time we are

168
00:10:50,720 --> 00:10:55,230
in the first decade of the 21st
century in object recognition,

169
00:10:55,230 --> 00:10:59,390
that we even have international
benchmarks of a small number

170
00:10:59,390 --> 00:11:04,970
of object classes to
encourage everybody to compare

171
00:11:04,970 --> 00:11:06,210
their algorithms.

172
00:11:06,210 --> 00:11:08,160
So we're inching together.

173
00:11:08,160 --> 00:11:12,920
The last unlock for object
recognition, as we have learned,

174
00:11:12,920 --> 00:11:15,900
again goes back to
cognitive science.

175
00:11:15,900 --> 00:11:19,170
So this particular
psychologist, Irv Biederman,

176
00:11:19,170 --> 00:11:23,600
had long conjectured
that humans can recognize

177
00:11:23,600 --> 00:11:26,190
a huge number of objects.

178
00:11:26,190 --> 00:11:28,980
And this is intuitive
for our common knowledge.

179
00:11:28,980 --> 00:11:32,000
But he actually
put a number on it.

180
00:11:32,000 --> 00:11:34,620
I personally call it
the Biederman number,

181
00:11:34,620 --> 00:11:40,530
which is that by age six or
seven, children, he conjectured,

182
00:11:40,530 --> 00:11:46,070
were able to recognize about
30,000 to 100,000 different

183
00:11:46,070 --> 00:11:47,580
visual categories.

184
00:11:47,580 --> 00:11:50,040
And he used this-- where
did he come up with?

185
00:11:50,040 --> 00:11:54,000
This number is a combination
of looking at dictionary,

186
00:11:54,000 --> 00:11:58,610
the number of nouns, as well
as visual studies of how kids

187
00:11:58,610 --> 00:12:01,590
recognize different objects.

188
00:12:01,590 --> 00:12:06,020
But it's a number that's pretty
daunting and pretty sobering

189
00:12:06,020 --> 00:12:10,320
for the field of computer
vision, because up till now,

190
00:12:10,320 --> 00:12:13,440
this is middle of the first
decade of 21st century,

191
00:12:13,440 --> 00:12:18,770
we were working with tiny number
of object categories and a tiny

192
00:12:18,770 --> 00:12:23,150
number of images to work
with compared to what humans

193
00:12:23,150 --> 00:12:24,120
experience.

194
00:12:24,120 --> 00:12:29,120
And this was, as you
know, the motivation

195
00:12:29,120 --> 00:12:36,380
for ImageNet project, which took
this number really seriously

196
00:12:36,380 --> 00:12:38,630
and constructed--

197
00:12:38,630 --> 00:12:42,590
we constructed this data
set that is on par with what

198
00:12:42,590 --> 00:12:45,300
the psychologist
Biederman conjectured,

199
00:12:45,300 --> 00:12:51,480
which is around 22,000 object
classes, over 15 million images.

200
00:12:51,480 --> 00:12:53,720
And of course,
that's the beginning

201
00:12:53,720 --> 00:12:57,330
that you start to
come into this class.

202
00:12:57,330 --> 00:13:01,890
Is that because of the large
data provided by ImageNet,

203
00:13:01,890 --> 00:13:07,250
we start to see that powerful
algorithms like neural network

204
00:13:07,250 --> 00:13:09,630
at the beginning with
convolutional neural network

205
00:13:09,630 --> 00:13:12,500
of course, now we use
transformers and all that,

206
00:13:12,500 --> 00:13:17,750
start to really show their
power through big data.

207
00:13:17,750 --> 00:13:23,360
And this is a generic
slide for those people

208
00:13:23,360 --> 00:13:24,630
who didn't learn about this.

209
00:13:24,630 --> 00:13:26,790
I'm going to skip this
because you all know this.

210
00:13:26,790 --> 00:13:31,740
So the quick history is, as
soon as we have ImageNet,

211
00:13:31,740 --> 00:13:35,360
as soon as we use convolutional
neural network, a few years

212
00:13:35,360 --> 00:13:38,270
after the beginning
of ImageNet, we

213
00:13:38,270 --> 00:13:41,460
saw this door
blasted open in terms

214
00:13:41,460 --> 00:13:45,610
of solving the problem
of object recognition.

215
00:13:45,610 --> 00:13:48,120
Now, we have
algorithms that we can

216
00:13:48,120 --> 00:13:51,420
take to look at any
picture in the world

217
00:13:51,420 --> 00:13:55,440
and be able to recognize
objects in the big or small

218
00:13:55,440 --> 00:13:59,250
and in any kind of orientation.

219
00:13:59,250 --> 00:14:01,030
Is it 100% soft?

220
00:14:01,030 --> 00:14:01,530
No.

221
00:14:01,530 --> 00:14:04,840
There is always a long
tail problems we can solve.

222
00:14:04,840 --> 00:14:07,720
But as far as industrial
application goes,

223
00:14:07,720 --> 00:14:10,590
this has come a long
way, and really has

224
00:14:10,590 --> 00:14:13,630
been a matured problem.

225
00:14:13,630 --> 00:14:16,860
And of course, all of
you know, and all this

226
00:14:16,860 --> 00:14:21,660
came at a convergence point,
which is the year 2012, where

227
00:14:21,660 --> 00:14:24,240
the ImageNet challenge
provided the data

228
00:14:24,240 --> 00:14:26,770
for the convolutional
neural network,

229
00:14:26,770 --> 00:14:30,340
and they used two
GPUs at that time.

230
00:14:30,340 --> 00:14:34,590
And the three ingredients
come together and brought

231
00:14:34,590 --> 00:14:37,580
the moment of deep learning--

232
00:14:37,580 --> 00:14:39,300
the birth of deep learning.

233
00:14:39,300 --> 00:14:42,980
And in this class, we
also talked a little bit

234
00:14:42,980 --> 00:14:46,220
about different
various architectures

235
00:14:46,220 --> 00:14:49,880
that ImageNet
challenge engendered

236
00:14:49,880 --> 00:14:53,510
throughout the past
decade or so in terms

237
00:14:53,510 --> 00:14:57,990
of convolutional neural
network or ResNet and so on.

238
00:14:57,990 --> 00:15:05,610
So that's the beginning, really,
about deep learning revolution.

239
00:15:05,610 --> 00:15:09,510
And of course, in terms of the
quest for visual intelligence,

240
00:15:09,510 --> 00:15:13,220
we're not going to stop at just
being able to label objects

241
00:15:13,220 --> 00:15:14,070
in a scene.

242
00:15:14,070 --> 00:15:18,420
For example, in this two scenes,
if you just label objects,

243
00:15:18,420 --> 00:15:21,150
you'll think it's just
a llama and a person.

244
00:15:21,150 --> 00:15:23,450
But if I show you
the second scene

245
00:15:23,450 --> 00:15:25,670
with the llama and
the person, the story

246
00:15:25,670 --> 00:15:27,420
is completely different.

247
00:15:27,420 --> 00:15:29,730
Even though you have
the same object,

248
00:15:29,730 --> 00:15:31,920
you have very
different relationship.

249
00:15:31,920 --> 00:15:35,290
So cognitive
scientists, once again,

250
00:15:35,290 --> 00:15:37,780
was the head of
computer scientist

251
00:15:37,780 --> 00:15:43,270
and inspired us to think
about visual intelligence

252
00:15:43,270 --> 00:15:47,330
beyond just naming objects
or categorizing objects.

253
00:15:47,330 --> 00:15:49,790
In this particular
paper, Jeremy Wolfe,

254
00:15:49,790 --> 00:15:53,530
who is a pretty
prominent psychologist,

255
00:15:53,530 --> 00:15:56,830
wrote this beautiful
paper that called out

256
00:15:56,830 --> 00:15:59,590
that relationships
between objects

257
00:15:59,590 --> 00:16:02,890
must be coded as part
of our understanding

258
00:16:02,890 --> 00:16:05,420
of complex natural scenes.

259
00:16:05,420 --> 00:16:09,160
And inspired by that work,
the field of computer vision

260
00:16:09,160 --> 00:16:13,220
started to look at how do
we understand relationships.

261
00:16:13,220 --> 00:16:15,140
And this is early work.

262
00:16:15,140 --> 00:16:20,730
You guys got a lecture
from Ranjay last week or--

263
00:16:20,730 --> 00:16:21,230
yeah.

264
00:16:21,230 --> 00:16:21,910
Last week.

265
00:16:21,910 --> 00:16:27,850
This was his PhD thesis, looking
at learning object relationships

266
00:16:27,850 --> 00:16:31,820
using scene graph
as a representation.

267
00:16:31,820 --> 00:16:34,390
In this case, scene
graph is defined

268
00:16:34,390 --> 00:16:37,880
by these entity nodes
that are objects,

269
00:16:37,880 --> 00:16:40,510
and their relationships
are defined

270
00:16:40,510 --> 00:16:42,710
by the connectivity
between the nodes,

271
00:16:42,710 --> 00:16:45,760
or sometimes they have
attribute relationships that

272
00:16:45,760 --> 00:16:48,380
defines the particular objects.

273
00:16:48,380 --> 00:16:51,250
And even a thing
as simple as this

274
00:16:51,250 --> 00:16:56,590
with mostly just two people,
one feeding a cake to the other,

275
00:16:56,590 --> 00:17:01,510
you can form a very dense scene
graph because of the richness

276
00:17:01,510 --> 00:17:02,900
of the visual scene.

277
00:17:02,900 --> 00:17:10,630
And this was Ranjay's thesis
after the object recognition

278
00:17:10,630 --> 00:17:16,480
error, where we build a data
set called visual genome, where

279
00:17:16,480 --> 00:17:22,810
we try to put together
object relationships, also

280
00:17:22,810 --> 00:17:24,380
story descriptions.

281
00:17:24,380 --> 00:17:27,440
And one of the work
that Ranjay did

282
00:17:27,440 --> 00:17:30,820
I thought that was really
fun was zero shot learning

283
00:17:30,820 --> 00:17:33,790
of unusual object relationships.

284
00:17:33,790 --> 00:17:38,560
For example, it's not unusual
to see person riding a horse.

285
00:17:38,560 --> 00:17:42,070
It's not unusual to see
person wearing a hat,

286
00:17:42,070 --> 00:17:45,910
but it's unusual in general,
to see horse wearing hat.

287
00:17:45,910 --> 00:17:49,390
And in the era of
big data training,

288
00:17:49,390 --> 00:17:51,540
it's hard to get
this kind of data

289
00:17:51,540 --> 00:17:54,580
repeatedly because you just
don't have too many of that.

290
00:17:54,580 --> 00:17:58,750
But using this compositional
scene graph representation,

291
00:17:58,750 --> 00:18:02,280
we're able to learn the more
common relationships, and then

292
00:18:02,280 --> 00:18:06,880
derive uncommon relationships
in that representation.

293
00:18:06,880 --> 00:18:09,630
And again, this
is another example

294
00:18:09,630 --> 00:18:13,920
of zero shot learning, where
person sitting on chair and fire

295
00:18:13,920 --> 00:18:16,080
hydrant on the lawn
or on the field

296
00:18:16,080 --> 00:18:18,840
are all common
relationships, but person

297
00:18:18,840 --> 00:18:22,880
sitting on fire hydrant is
the one that would not--

298
00:18:22,880 --> 00:18:24,370
it's hard to get data.

299
00:18:24,370 --> 00:18:27,790
And we're able to do
that to make that happen.

300
00:18:27,790 --> 00:18:31,190
And this is just a figure
from the paper that shows that

301
00:18:31,190 --> 00:18:35,060
Ranjay's work at that time
achieved state-of-the-art

302
00:18:35,060 --> 00:18:39,530
recognition rate compared
to many other methods.

303
00:18:39,530 --> 00:18:42,530
But relationship is not enough.

304
00:18:42,530 --> 00:18:46,220
The ability to actually
tell a story that

305
00:18:46,220 --> 00:18:50,630
is a lot more richer, or
also using natural language

306
00:18:50,630 --> 00:18:55,560
is actually the next big goal.

307
00:18:55,560 --> 00:19:00,920
So around the year 2014--

308
00:19:00,920 --> 00:19:04,710
around 2014, we start working on
that problem and think about it.

309
00:19:04,710 --> 00:19:09,360
That's just two years after the
image that Alex, that moment.

310
00:19:09,360 --> 00:19:12,660
But the field was starting
to evolve so fast.

311
00:19:12,660 --> 00:19:16,850
We're so inspired
by what we can do

312
00:19:16,850 --> 00:19:21,950
using a combination of
convolutional neural network

313
00:19:21,950 --> 00:19:26,880
as well as a language
model called LSTM.

314
00:19:26,880 --> 00:19:29,960
And this is the thesis
by Andrej Karpathy

315
00:19:29,960 --> 00:19:34,480
that we were one of
the first teams that

316
00:19:34,480 --> 00:19:38,800
showed how to do image
captioning or storytelling, as

317
00:19:38,800 --> 00:19:42,340
well as dense captioning,
which is also part of the work

318
00:19:42,340 --> 00:19:43,820
that Justin Johnson did.

319
00:19:43,820 --> 00:19:48,380
And I know he's one of the
co-instructors of this course.

320
00:19:48,380 --> 00:19:53,840
And that was around the
time between 2015 to 2018.

321
00:19:53,840 --> 00:19:58,130
A lot of work has happened
to solve the problem.

322
00:19:58,130 --> 00:20:01,940
Of course today, using
a multimodal LLMs,

323
00:20:01,940 --> 00:20:04,420
we have taken the
solution of this problem

324
00:20:04,420 --> 00:20:07,790
even to another notch.

325
00:20:07,790 --> 00:20:10,940
But this is the beginning
of that line of work.

326
00:20:10,940 --> 00:20:15,730
And frankly, I myself, as
a computer vision scientist

327
00:20:15,730 --> 00:20:18,860
who entered the field at the
beginning of the century,

328
00:20:18,860 --> 00:20:24,130
was very surprised by how fast
our field was able to solve

329
00:20:24,130 --> 00:20:27,130
this problem, as soon as
we've got data, as well

330
00:20:27,130 --> 00:20:29,970
as neural network algorithms.

331
00:20:29,970 --> 00:20:34,660
But a much harder problem is
actually in dynamic scenes.

332
00:20:34,660 --> 00:20:38,400
In dynamic scenes, we
tend to have much more

333
00:20:38,400 --> 00:20:40,030
complex relationships.

334
00:20:40,030 --> 00:20:42,760
Much more complex movements.

335
00:20:42,760 --> 00:20:50,350
And also the camera
movement or the entity,

336
00:20:50,350 --> 00:20:55,000
the actors within the scene can
do a lot of different things.

337
00:20:55,000 --> 00:20:57,720
So in this work that a
collaboration with Esan

338
00:20:57,720 --> 00:21:01,390
and a bunch of
students in our lab,

339
00:21:01,390 --> 00:21:05,040
we call it multi-object
multi-actor activity

340
00:21:05,040 --> 00:21:05,970
understanding.

341
00:21:05,970 --> 00:21:07,960
This is a much newer work.

342
00:21:07,960 --> 00:21:11,040
We only published this
a couple of years ago.

343
00:21:11,040 --> 00:21:15,570
To capture the relationship
between these actors

344
00:21:15,570 --> 00:21:19,630
and their activities in
dynamic scene is still,

345
00:21:19,630 --> 00:21:22,180
I would say, an
unsolved problem.

346
00:21:22,180 --> 00:21:24,970
And this will have
profound implications.

347
00:21:24,970 --> 00:21:27,370
You know that you're
in Silicon Valley.

348
00:21:27,370 --> 00:21:34,510
So you're hearing so much
excitement of robots,

349
00:21:34,510 --> 00:21:35,410
for example.

350
00:21:35,410 --> 00:21:38,580
If we ever dream to have
everyday robots that

351
00:21:38,580 --> 00:21:42,370
work amongst us, robots
need to solve this problem.

352
00:21:42,370 --> 00:21:46,180
Understand how complex the
scene is, what people are doing,

353
00:21:46,180 --> 00:21:48,370
who is doing what, what is next.

354
00:21:48,370 --> 00:21:49,985
And this is an unsolved problem.

356
00:21:53,730 --> 00:21:57,680
Also in addition to
what I have shown you,

357
00:21:57,680 --> 00:22:02,340
you have learned a little bit in
this class and related computer

358
00:22:02,340 --> 00:22:06,670
vision problem, but we didn't
have time to elaborate.

359
00:22:06,670 --> 00:22:11,010
For example, 3D computer vision
or human pose understanding.

360
00:22:11,010 --> 00:22:15,580
And of course, generative
AI and generative models.

361
00:22:15,580 --> 00:22:18,390
So this is just to
show you that the field

362
00:22:18,390 --> 00:22:22,800
of computer vision since
the rebirth of modern AI

363
00:22:22,800 --> 00:22:27,410
has been just moving
extraordinarily fast.

364
00:22:27,410 --> 00:22:31,220
But the take home message
in this section for me

365
00:22:31,220 --> 00:22:34,110
is that two things.

366
00:22:34,110 --> 00:22:38,240
One is that data, compute and
neural network algorithm truly

367
00:22:38,240 --> 00:22:42,030
have converged about 10
years ago or 13 years ago.

368
00:22:42,030 --> 00:22:45,770
And that was the moment that
modern AI or deep learning

369
00:22:45,770 --> 00:22:47,340
revolution has happened.

370
00:22:47,340 --> 00:22:50,060
But the history of
that, and so much

371
00:22:50,060 --> 00:22:52,320
of the problem that we
have been working on,

372
00:22:52,320 --> 00:22:57,110
is truly inspired by cognitive
science and psychology

373
00:22:57,110 --> 00:22:58,580
and neuroscience.

374
00:22:58,580 --> 00:23:05,930
And that, to me, is going
to continue to happen.

375
00:23:05,930 --> 00:23:10,640
Is that we will continue to be
inspired by what the brain can

376
00:23:10,640 --> 00:23:14,300
do or how the brain
does things, and also

377
00:23:14,300 --> 00:23:18,270
will continue to use AI to
help our brain research.

378
00:23:18,270 --> 00:23:22,450
So there is a very intimate
relationship between today's AI

379
00:23:22,450 --> 00:23:25,510
and cognitive science,
neuroscience, brain science,

380
00:23:25,510 --> 00:23:26,810
and all that.

381
00:23:26,810 --> 00:23:29,540
So that's the first section.

382
00:23:29,540 --> 00:23:33,640
And of course, a lot of people,
students and collaborators

383
00:23:33,640 --> 00:23:37,840
have contributed to what
I have just presented.

384
00:23:37,840 --> 00:23:42,580
Now, let's talk about
going beyond just building

385
00:23:42,580 --> 00:23:45,320
AI to see what humans don't see.

386
00:23:45,320 --> 00:23:49,600
This is where pushing AI beyond
the capability of humans or you

387
00:23:49,600 --> 00:23:51,320
can call it super humans.

388
00:23:51,320 --> 00:23:57,380
For example, most people don't
recognize a ton of dinosaurs.

389
00:23:57,380 --> 00:23:59,450
You can probably name a few.

390
00:23:59,450 --> 00:24:01,910
Some kids really can name a lot.

391
00:24:01,910 --> 00:24:05,920
Well, let alone
thousands and tens

392
00:24:05,920 --> 00:24:11,440
of thousands of bird
species or tens of thousands

393
00:24:11,440 --> 00:24:14,480
of car categories.

394
00:24:14,480 --> 00:24:19,600
So this is the line of work
that I call fine-grained object

395
00:24:19,600 --> 00:24:20,870
categorization.

396
00:24:20,870 --> 00:24:24,010
Humans are just not
that good at it.

397
00:24:24,010 --> 00:24:27,780
And this is still
a problem that I

398
00:24:27,780 --> 00:24:31,170
don't think we're fully
solved yet, to be honest.

399
00:24:31,170 --> 00:24:34,170
In this generative AI
era, especially, we're

400
00:24:34,170 --> 00:24:37,600
talking a lot about
multimodal LLMs.

401
00:24:37,600 --> 00:24:42,160
This problem has
somewhat been neglected,

402
00:24:42,160 --> 00:24:44,950
or it just is not a
mainstream problem.

403
00:24:44,950 --> 00:24:46,650
But it really will--

404
00:24:46,650 --> 00:24:50,650
still will come and
play an important role.

405
00:24:50,650 --> 00:24:53,910
So in this early work of
fine-grained bird species

406
00:24:53,910 --> 00:24:58,320
recognition, we put
together, a data set--

407
00:24:58,320 --> 00:25:02,230
actually, we used a
data set of 4,000 birds.

408
00:25:02,230 --> 00:25:04,770
And as you can
see, as we go down

409
00:25:04,770 --> 00:25:13,570
the tree of the species,
the errors, actually,

410
00:25:13,570 --> 00:25:20,490
as we go up the species, as
we have more general names,

411
00:25:20,490 --> 00:25:24,620
the error decreases, which means
it's a convoluted way of saying,

412
00:25:24,620 --> 00:25:30,120
by the time you're reading
the fine-grained level,

413
00:25:30,120 --> 00:25:32,490
we still make a lot of errors.

414
00:25:32,490 --> 00:25:39,710
The algorithm are still
not totally ready.

415
00:25:39,710 --> 00:25:42,980
Another work that
I find fascinating

416
00:25:42,980 --> 00:25:50,660
is that a few years ago, a
group of students in my lab

417
00:25:50,660 --> 00:25:54,110
trained a fine-grained
car classifier

418
00:25:54,110 --> 00:25:58,680
in terms of make,
model, and year.

419
00:25:58,680 --> 00:26:07,010
It turns out after 1970s, there
are thousands of car models that

420
00:26:07,010 --> 00:26:11,460
are defined by different
make, model, and year.

421
00:26:11,460 --> 00:26:13,410
And then you can take--

422
00:26:13,410 --> 00:26:20,910
we took Google Street View
images from 200 or 100, I think,

423
00:26:20,910 --> 00:26:23,970
major cities across the country.

424
00:26:23,970 --> 00:26:28,250
And then we use the
fine-grained car detectors

425
00:26:28,250 --> 00:26:32,610
to detect what are the cars
on the street of these cities.

426
00:26:32,610 --> 00:26:36,930
And we use it as a lens
to study social patterns.

427
00:26:36,930 --> 00:26:40,620
For example, what
is the pattern here,

428
00:26:40,620 --> 00:26:43,465
I showed education patterns.

429
00:26:43,465 --> 00:26:46,220
Car models and
education patterns

430
00:26:46,220 --> 00:26:53,850
are highly correlated or income
patterns, highly correlated.

431
00:26:53,850 --> 00:26:59,060
In that paper, we show voting
patterns, highly correlated.

432
00:26:59,060 --> 00:27:03,270
Or even environmental
patterns, highly correlated.

433
00:27:03,270 --> 00:27:07,010
So it's a really interesting
way of using computer vision

434
00:27:07,010 --> 00:27:09,720
as a lens to study our society.

435
00:27:09,720 --> 00:27:12,000
And no human, no
individual human,

436
00:27:12,000 --> 00:27:17,380
not even a collection of humans
can do this easily at all.

437
00:27:17,380 --> 00:27:24,070
So AI is really pushing the
boundary of what humans can see.

438
00:27:24,070 --> 00:27:27,980
To drive home this idea,
let's do a couple of tests.

439
00:27:27,980 --> 00:27:31,840
Humans actually have
our limitations.

440
00:27:31,840 --> 00:27:35,120
I just talked about celebrating
human's ability of seeing.

441
00:27:35,120 --> 00:27:36,680
But we also have
our limitations.

442
00:27:36,680 --> 00:27:39,490
This is a very famous
visual illusion

443
00:27:39,490 --> 00:27:41,240
test called Stroop test.

444
00:27:41,240 --> 00:27:44,390
And the idea is that you
all can read the words.

445
00:27:44,390 --> 00:27:49,810
But if I ask you to read the
color of the word as fast

446
00:27:49,810 --> 00:27:54,080
as possible, going from left
to right and top to down,

447
00:27:54,080 --> 00:27:56,660
you find that it's
not that easy.

448
00:27:56,660 --> 00:28:04,910
Try to read it, like red,
yellow, green, purple, blue,

449
00:28:04,910 --> 00:28:07,095
black, orange.

450
00:28:07,095 --> 00:28:09,680
It's fighting with you.

451
00:28:09,680 --> 00:28:13,520
This is the fight between
visual attention and all that.

452
00:28:13,520 --> 00:28:15,810
Here's another example.

453
00:28:15,810 --> 00:28:20,440
There are two alternating
images of the picture.

454
00:28:20,440 --> 00:28:22,960
And there's one change.

455
00:28:22,960 --> 00:28:26,250
A pretty big change that's
happening between the two

456
00:28:26,250 --> 00:28:27,640
alternating pictures.

457
00:28:27,640 --> 00:28:29,950
I don't know if you
spot the change.

458
00:28:29,950 --> 00:28:31,616
Do you spot it?

459
00:28:31,616 --> 00:28:32,490
The enine--

460
00:28:32,490 --> 00:28:34,320
Yes, it's the engine.

461
00:28:34,320 --> 00:28:37,240
So it takes a while to spot it.

462
00:28:37,240 --> 00:28:41,640
So this is a very famous
psychology experiment

463
00:28:41,640 --> 00:28:43,150
called change blindness.

464
00:28:43,150 --> 00:28:44,820
Now, all this is fun.

465
00:28:44,820 --> 00:28:46,870
Stroop test is fun, this is fun.

466
00:28:46,870 --> 00:28:48,810
But this is not fun.

467
00:28:48,810 --> 00:28:51,850
That human attention is limited.

468
00:28:51,850 --> 00:28:56,260
And in some situations
in our working life,

469
00:28:56,260 --> 00:29:00,280
that kind of attention
limit can be dire.

470
00:29:00,280 --> 00:29:05,940
For example, medical errors
are the third leading cause

471
00:29:05,940 --> 00:29:10,210
of death in America's
health care system.

472
00:29:10,210 --> 00:29:13,370
And of course, leaving
this pair of scissors

473
00:29:13,370 --> 00:29:16,700
in the body of the patient
is the iconic image

474
00:29:16,700 --> 00:29:18,000
of medical errors.

475
00:29:18,000 --> 00:29:20,130
But there are so
many medical errors.

476
00:29:20,130 --> 00:29:25,350
Pharmaceutical errors,
there's procedure errors,

477
00:29:25,350 --> 00:29:28,590
clerical errors,
diagnostic errors.

478
00:29:28,590 --> 00:29:31,260
So one has to be very careful.

479
00:29:31,260 --> 00:29:35,630
For example, in surgery
rooms, honestly, scissors

480
00:29:35,630 --> 00:29:40,130
don't get left in the bodies,
typically, but much smaller

481
00:29:40,130 --> 00:29:45,930
things like suture, needles,
or piece of gauze and all that.

482
00:29:45,930 --> 00:29:52,615
So today, most of this is
still just tracked by hands.

483
00:29:52,615 --> 00:30:00,180
We have these checklists to
track in the surgery rooms.

484
00:30:00,180 --> 00:30:04,170
If something is missing, the
surgery has to be paused.

485
00:30:04,170 --> 00:30:07,920
On average, that pause
is close to an hour.

486
00:30:07,920 --> 00:30:11,040
And think about the
danger for the patient.

487
00:30:11,040 --> 00:30:14,650
The exposure to bacteria
and the bleeding

488
00:30:14,650 --> 00:30:18,470
and all that, just because we
have to search for that item.

489
00:30:18,470 --> 00:30:25,810
So if there is a way to
use AI to help our doctors,

490
00:30:25,810 --> 00:30:29,510
surgeons, to track items,
that would be so powerful.

491
00:30:29,510 --> 00:30:30,860
And this is just a demo.

492
00:30:30,860 --> 00:30:33,200
This is not a deploy system.

493
00:30:33,200 --> 00:30:35,690
We're not there in
terms of fidelity.

494
00:30:35,690 --> 00:30:37,570
But this is a demo
to show that we

495
00:30:37,570 --> 00:30:42,560
can use AI to count, in this
case, gauze and all that.

496
00:30:42,560 --> 00:30:49,090
And this is just an
example of pushing AI

497
00:30:49,090 --> 00:30:51,740
to see what humans don't see.

498
00:30:51,740 --> 00:30:56,030
Here's another example
that is really fun.

499
00:30:56,030 --> 00:30:57,800
I don't know if I
showed this before.

500
00:30:57,800 --> 00:31:01,060
But this is one of my
favorite visual illusions,

501
00:31:01,060 --> 00:31:03,620
where I'm just giving
you the answer.

502
00:31:03,620 --> 00:31:07,030
If you look at the
two squares, A and B,

503
00:31:07,030 --> 00:31:11,170
on a checkerboard at
the top, it is so hard

504
00:31:11,170 --> 00:31:14,838
to believe they have the
same grayscale or luminance.

505
00:31:14,838 --> 00:31:16,880
And then you look at the
bottom, you're like, oh.

506
00:31:16,880 --> 00:31:18,310
Of course, they do.

507
00:31:18,310 --> 00:31:19,400
But why?

508
00:31:19,400 --> 00:31:24,500
Even though you have the
bottom picture in front of you,

509
00:31:24,500 --> 00:31:28,820
seeing the top still
gives you the illusion.

510
00:31:28,820 --> 00:31:29,680
Why?

511
00:31:29,680 --> 00:31:33,676
Because evolution
has pre-wired us

512
00:31:33,676 --> 00:31:39,800
in conjecturing or understanding
our world in its common way,

513
00:31:39,800 --> 00:31:42,430
with the common
physics of the shape

514
00:31:42,430 --> 00:31:48,380
of objects, lighting source, how
shadows are made, and all that,

515
00:31:48,380 --> 00:31:54,460
this is so deep in our
visual development.

516
00:31:54,460 --> 00:31:59,480
That it's hard for us
to see it another way.

517
00:31:59,480 --> 00:32:04,280
So what I'm trying to
get at is there's bias

518
00:32:04,280 --> 00:32:06,410
in our human visual system.

519
00:32:06,410 --> 00:32:10,360
The bias might come from
evolutionary construct,

520
00:32:10,360 --> 00:32:14,860
the bias can come from
our social experience,

521
00:32:14,860 --> 00:32:19,420
the bias can come from
the data we're exposed to.

522
00:32:19,420 --> 00:32:22,980
But some of these
biases can be harmful.

523
00:32:22,980 --> 00:32:26,280
When the bias
happens, that became

524
00:32:26,280 --> 00:32:29,350
unfair to a group of people.

525
00:32:29,350 --> 00:32:30,580
A community.

526
00:32:30,580 --> 00:32:32,530
And we have to be aware of this.

527
00:32:32,530 --> 00:32:35,520
A few years ago, face
recognition algorithm

528
00:32:35,520 --> 00:32:39,540
was not good, and it
tends to recognize

529
00:32:39,540 --> 00:32:43,750
certain skin color and even
gender better than others.

530
00:32:43,750 --> 00:32:45,400
And it has consequences.

531
00:32:45,400 --> 00:32:47,230
Think about self-driving car.

532
00:32:47,230 --> 00:32:53,070
So think about many
other medical use cases.

533
00:32:53,070 --> 00:32:56,100
So we have to be
vigilant about this.

534
00:32:56,100 --> 00:33:03,210
I do believe that AI bias has
been a problem that people now

535
00:33:03,210 --> 00:33:04,460
are carrying.

536
00:33:04,460 --> 00:33:07,490
A few years ago, this
problem was so new

537
00:33:07,490 --> 00:33:10,440
that many people are not
even paying attention.

538
00:33:10,440 --> 00:33:13,070
But fast forward
to 2025, I'm not

539
00:33:13,070 --> 00:33:14,880
saying we have
solved this problem,

540
00:33:14,880 --> 00:33:18,230
but I'm personally
a lot happier to see

541
00:33:18,230 --> 00:33:21,450
that so many people are
paying attention to this,

542
00:33:21,450 --> 00:33:25,730
not only just in academia,
but also in industry.

543
00:33:25,730 --> 00:33:28,860
And then there's another
kind of not seen.

544
00:33:28,860 --> 00:33:30,810
And this is interesting.

545
00:33:30,810 --> 00:33:34,280
Sometimes not seen
is exactly what

546
00:33:34,280 --> 00:33:38,700
we want because you
want to respect privacy.

547
00:33:38,700 --> 00:33:45,350
So how do you create AI
that helps people to see yet

548
00:33:45,350 --> 00:33:46,680
you still want it.

549
00:33:46,680 --> 00:33:49,200
Not to see what people
don't want you to see.

550
00:33:49,200 --> 00:33:50,960
This is a very deep--

551
00:33:50,960 --> 00:33:54,180
it's a technical problem
as well as a human problem.

552
00:33:54,180 --> 00:33:57,120
So from a technical
point of view,

553
00:33:57,120 --> 00:34:01,560
there are many ways to consider
ML, machine learning privacy.

554
00:34:01,560 --> 00:34:05,990
I'm just listing here from a
visual approach point of view,

555
00:34:05,990 --> 00:34:08,949
a few years ago, our
lab wrote this paper

556
00:34:08,949 --> 00:34:14,679
about using smart cameras in
patient rooms or patient homes

557
00:34:14,679 --> 00:34:17,409
to help doctors to see better.

558
00:34:17,409 --> 00:34:20,920
But even there, we have
to recognize issues

559
00:34:20,920 --> 00:34:27,050
like faces or just full body
information and even homes.

560
00:34:27,050 --> 00:34:30,230
And this is a list of
potential solutions.

561
00:34:30,230 --> 00:34:34,460
For example, you can do
blurring, or you can do masking,

562
00:34:34,460 --> 00:34:37,130
you can do
dimensionality reduction,

563
00:34:37,130 --> 00:34:41,750
but you can also, try to
do different approaches,

564
00:34:41,750 --> 00:34:44,080
for example, federated
learning so that you don't

565
00:34:44,080 --> 00:34:49,550
send all the data to the server,
or encryption and other things.

566
00:34:49,550 --> 00:34:51,620
So I'm not going
to belabor this,

567
00:34:51,620 --> 00:34:53,420
but there's one work
I want to show you.

568
00:34:53,420 --> 00:34:56,330
It's not even my work, but
I really like this work.

569
00:34:56,330 --> 00:35:02,560
And it's a work about
taking videos of people

570
00:35:02,560 --> 00:35:06,460
and try to recognize
the action of people,

571
00:35:06,460 --> 00:35:09,740
but yet respecting
the privacy of people.

572
00:35:09,740 --> 00:35:10,760
How do you do that?

573
00:35:10,760 --> 00:35:15,310
For example, in
this case, you want

574
00:35:15,310 --> 00:35:20,200
to take a video of this
kid moving in the scene.

575
00:35:20,200 --> 00:35:22,610
There are ways to do this.

576
00:35:22,610 --> 00:35:30,615
If you blur this or defocus
this or do some of these, yeah,

577
00:35:30,615 --> 00:35:31,810
you can provide.

578
00:35:31,810 --> 00:35:35,020
You can protect privacy, but
you also lose enough information

579
00:35:35,020 --> 00:35:37,700
that you might not even know
what this person is doing.

580
00:35:37,700 --> 00:35:40,030
And for many applications,
the whole goal

581
00:35:40,030 --> 00:35:42,410
is to know what this
person is doing.

582
00:35:42,410 --> 00:35:48,680
So in this particular work,
led by Carl's students,

583
00:35:48,680 --> 00:35:52,690
they actually did a combination
of hardware and software

584
00:35:52,690 --> 00:35:58,360
approach, where they
had-crafted a lens that

585
00:35:58,360 --> 00:36:06,940
is that actually filters visual
data in a particular way.

586
00:36:06,940 --> 00:36:12,100
So particular that if
you look at the top row,

587
00:36:12,100 --> 00:36:14,910
what the lens captures
into the camera

588
00:36:14,910 --> 00:36:17,080
protects the privacy a lot.

589
00:36:17,080 --> 00:36:18,960
You don't see the
person's face, you

590
00:36:18,960 --> 00:36:21,460
don't see the body and so on.

591
00:36:21,460 --> 00:36:24,750
But because it's a lens
that's particularly

592
00:36:24,750 --> 00:36:28,360
designed in connection
with a piece of software,

593
00:36:28,360 --> 00:36:32,460
it can help to block out
the movement information

594
00:36:32,460 --> 00:36:35,880
or the human
activity information

595
00:36:35,880 --> 00:36:39,730
without backing out
phase information.

596
00:36:39,730 --> 00:36:42,100
So that's a really
interesting approach.

597
00:36:42,100 --> 00:36:45,400
That's a hybrid between
hardware and software.

598
00:36:45,400 --> 00:36:48,570
Aiming towards
important applications

599
00:36:48,570 --> 00:36:51,790
that you want to see
people to protect them,

600
00:36:51,790 --> 00:36:54,180
but you don't want to
see too much, because you

601
00:36:54,180 --> 00:36:55,570
want to respect privacy.

602
00:36:55,570 --> 00:36:57,910
So that's a work I really like.

603
00:36:57,910 --> 00:37:00,847
I really like the
spirit of that work.

604
00:37:00,847 --> 00:37:02,040
OK.

605
00:37:02,040 --> 00:37:06,110
So in this part of the
lecture, I shared with you

606
00:37:06,110 --> 00:37:10,460
a number of things, just
considerations of building AI

607
00:37:10,460 --> 00:37:12,510
to see what humans don't see.

608
00:37:12,510 --> 00:37:16,580
Sometimes we're pushing AI
like fine grained recognition

609
00:37:16,580 --> 00:37:19,350
of birds to go
beyond human ability.

610
00:37:19,350 --> 00:37:21,030
Those are superhuman ability.

611
00:37:21,030 --> 00:37:23,580
Sometimes we know
humans are not good.

612
00:37:23,580 --> 00:37:26,820
We have bias or we
have attention issues.

613
00:37:26,820 --> 00:37:29,250
And then we want to
use AI to help us.

614
00:37:29,250 --> 00:37:33,140
And then sometimes we
genuinely have situations

615
00:37:33,140 --> 00:37:35,070
we don't want anyone to see.

616
00:37:35,070 --> 00:37:37,790
And then how do you
use AI to continue

617
00:37:37,790 --> 00:37:41,580
to help without violating
those privacy concerns?

618
00:37:41,580 --> 00:37:45,840
So you can see that AI is a
very interesting, powerful tool.

619
00:37:45,840 --> 00:37:49,890
It can both help but amplify us.

620
00:37:49,890 --> 00:37:54,870
And if we have bias, if we have
issues, AI can amplify us too.

621
00:37:54,870 --> 00:37:58,570
So when we build AI,
it is so important

622
00:37:58,570 --> 00:38:02,380
not only to take that
technology perspective, but also

623
00:38:02,380 --> 00:38:05,200
to take the human
perspective, to commit

624
00:38:05,200 --> 00:38:09,430
to study forecasts and
guide the AI to understand

625
00:38:09,430 --> 00:38:12,980
its human impact and
respect human values.

626
00:38:12,980 --> 00:38:17,600
So that's the second
take home message.

627
00:38:17,600 --> 00:38:21,100
And again, a number of
collaborators and students

628
00:38:21,100 --> 00:38:23,800
participated in this work.

629
00:38:23,800 --> 00:38:24,670
OK.

630
00:38:24,670 --> 00:38:27,400
Now, let's talk
about building AI

631
00:38:27,400 --> 00:38:29,480
to see what humans want to see.

632
00:38:29,480 --> 00:38:32,660
And in fact, we're going
to go beyond seeing.

633
00:38:32,660 --> 00:38:35,630
We're going to connect
seeing and doing together.

634
00:38:35,630 --> 00:38:42,260
So if you think about today's
societal anxiety about AI,

635
00:38:42,260 --> 00:38:46,450
one of the biggest
anxiety is labor.

636
00:38:46,450 --> 00:38:51,170
A lot of headline news will
say, labor is under threat.

637
00:38:51,170 --> 00:38:54,370
Robots taking over jobs.

638
00:38:54,370 --> 00:38:58,080
The truth is, the
picture is complex.

639
00:38:58,080 --> 00:39:01,690
Denying job change is wrong.

640
00:39:01,690 --> 00:39:05,040
Every technological
shift in human history

641
00:39:05,040 --> 00:39:08,700
has caused labor market
change, and some of them

642
00:39:08,700 --> 00:39:10,030
are very painful.

643
00:39:10,030 --> 00:39:15,780
Some of them can lead to
even civil wars and wars.

644
00:39:15,780 --> 00:39:20,290
But also, that change
sometimes is inevitable.

645
00:39:20,290 --> 00:39:24,610
And a tiny digression.

646
00:39:24,610 --> 00:39:27,690
A lot of the labor
threat rhetoric

647
00:39:27,690 --> 00:39:31,680
that we have been hearing,
think about physical labors.

648
00:39:31,680 --> 00:39:34,360
But today, in the
past two years,

649
00:39:34,360 --> 00:39:39,330
if you look at GenAI's
impact is white collar jobs

650
00:39:39,330 --> 00:39:42,730
that are drastically
being impacted,

651
00:39:42,730 --> 00:39:49,240
especially software engineering
and analytical work in offices.

652
00:39:49,240 --> 00:39:53,020
So there's just
definitely a labor change.

653
00:39:53,020 --> 00:39:55,020
But in the meantime,
we also need

654
00:39:55,020 --> 00:39:59,320
to recognize that AI
I also can be helpful.

655
00:39:59,320 --> 00:40:03,360
We actually fundamentally
have human labor shortages

656
00:40:03,360 --> 00:40:08,100
in many situations, especially
in elderly care, as well as

657
00:40:08,100 --> 00:40:08,920
health.

658
00:40:08,920 --> 00:40:12,690
First of all, as modern
medicine improves,

659
00:40:12,690 --> 00:40:15,670
human life expectancy increases.

660
00:40:15,670 --> 00:40:21,790
And that is inevitably pushes a
society towards longer living.

661
00:40:21,790 --> 00:40:23,230
And that's a good thing.

662
00:40:23,230 --> 00:40:28,090
But in the meantime, we
have shortages of laborers.

663
00:40:28,090 --> 00:40:29,760
Young people need to work.

664
00:40:29,760 --> 00:40:33,490
That's how to make
this society vibrant.

665
00:40:33,490 --> 00:40:34,750
Economy vibrant.

666
00:40:34,750 --> 00:40:37,170
But who is taking
care of our elderlies?

667
00:40:37,170 --> 00:40:40,230
Who are taking care of
our chronically ill?

668
00:40:40,230 --> 00:40:45,090
Even in America's hospitals,
we have such a attrition

669
00:40:45,090 --> 00:40:49,110
of health care workers,
especially nurses,

670
00:40:49,110 --> 00:40:55,080
that we don't have enough hands,
ears, eyes to help our patients.

671
00:40:55,080 --> 00:40:59,280
So instead of thinking
about this word replace,

672
00:40:59,280 --> 00:41:03,420
we actually can think
about AI augmenting.

673
00:41:03,420 --> 00:41:07,140
And you got a glimpse of that
in my surgery room example.

674
00:41:07,140 --> 00:41:12,560
Indeed, in health, there's
so many spaces in health

675
00:41:12,560 --> 00:41:15,390
that we don't have
enough pairs of eyes.

676
00:41:15,390 --> 00:41:19,410
And that's what I call
the dark spaces of health.

677
00:41:19,410 --> 00:41:24,920
From surgery room to patient
room to pharmaceutical to homes

678
00:41:24,920 --> 00:41:26,250
and so on.

679
00:41:26,250 --> 00:41:29,010
So how do we make AI help?

680
00:41:29,010 --> 00:41:31,520
And this is something
that Ihsan has been

681
00:41:31,520 --> 00:41:33,750
leading a ton of this work.

682
00:41:33,750 --> 00:41:37,070
Also with zing is that
we have been looking

683
00:41:37,070 --> 00:41:40,830
at this problem of ambient
intelligence for health,

684
00:41:40,830 --> 00:41:45,620
where we combine smart sensors
with machine learning algorithms

685
00:41:45,620 --> 00:41:52,780
to glean health critical
insights from these situations

686
00:41:52,780 --> 00:41:57,790
in health set up so that we can
alert the patients or family

687
00:41:57,790 --> 00:42:01,970
members or doctors in
time to help patients.

688
00:42:01,970 --> 00:42:05,620
And again, the fuller paper
is in this particular paper

689
00:42:05,620 --> 00:42:07,550
we published a
couple of years ago.

690
00:42:07,550 --> 00:42:10,340
Let me just give you
a couple of examples.

691
00:42:10,340 --> 00:42:13,360
One example is this hand
hygiene project, which actually

692
00:42:13,360 --> 00:42:16,360
started way before COVID.

693
00:42:16,360 --> 00:42:21,730
And hand hygiene turns out to
be really important for keeping

694
00:42:21,730 --> 00:42:25,030
hospital infection low.

695
00:42:25,030 --> 00:42:28,810
Hospital acquired
infection is actually

696
00:42:28,810 --> 00:42:32,110
one of the leading causes
of American patients

697
00:42:32,110 --> 00:42:34,490
fatality in our hospitals.

698
00:42:34,490 --> 00:42:39,520
It kills three times more people
per year than car accidents

699
00:42:39,520 --> 00:42:43,850
nationwide, and it is
really hard to control.

700
00:42:43,850 --> 00:42:46,510
Most of these germs are
passed from patient room

701
00:42:46,510 --> 00:42:47,560
to patient room.

702
00:42:47,560 --> 00:42:50,630
And then they just
brew together.

703
00:42:50,630 --> 00:42:52,690
So what do we do?

704
00:42:52,690 --> 00:42:55,390
The hospitals try to
use human auditors,

705
00:42:55,390 --> 00:42:58,420
but we just talked about, we
don't even have enough nurses,

706
00:42:58,420 --> 00:43:00,610
let alone hiring auditors.

707
00:43:00,610 --> 00:43:04,030
And also you cannot
hire enough of them.

708
00:43:04,030 --> 00:43:05,620
There are human fatigue.

709
00:43:05,620 --> 00:43:07,930
We just talk about
human attention problem.

710
00:43:07,930 --> 00:43:12,160
So this is not a pretty
prohibitive solution.

711
00:43:12,160 --> 00:43:16,150
There were some technological
solutions like RFID,

712
00:43:16,150 --> 00:43:17,170
put the badge.

713
00:43:17,170 --> 00:43:19,470
If the badge or the
person wearing the badge

714
00:43:19,470 --> 00:43:25,170
is close to the sink or the
hand hygiene hand sanitizer

715
00:43:25,170 --> 00:43:28,210
dispenser, it gives you
the hint that the person,

716
00:43:28,210 --> 00:43:31,600
or most likely the doctor or the
nurses is washing their hands.

717
00:43:31,600 --> 00:43:33,340
But that's very nonspecific.

718
00:43:33,340 --> 00:43:34,440
You cannot guarantee.

719
00:43:34,440 --> 00:43:36,790
And the hospital rooms
are pretty small,

720
00:43:36,790 --> 00:43:40,650
corridors are small, and just
standing next to something

721
00:43:40,650 --> 00:43:42,130
doesn't mean you're doing it.

722
00:43:42,130 --> 00:43:45,150
So a few years ago,
we did this project

723
00:43:45,150 --> 00:43:50,150
where we put smart sensors
that protects privacy

724
00:43:50,150 --> 00:43:54,590
by just only gleaning depth
information like the blue screen

725
00:43:54,590 --> 00:43:56,880
or the blue video there.

726
00:43:56,880 --> 00:44:01,890
And then we use computer vision
algorithm to classify actions.

727
00:44:01,890 --> 00:44:04,770
Is the person washing
hand or not washing hand?

728
00:44:04,770 --> 00:44:08,510
And the result is that if
you compare ground truth

729
00:44:08,510 --> 00:44:17,240
with the algorithm output versus
human outputs or human detection

730
00:44:17,240 --> 00:44:21,110
results, you can see algorithm
is so much better and more

731
00:44:21,110 --> 00:44:23,780
consistent than humans.

732
00:44:23,780 --> 00:44:29,720
You have to almost show the
same video to four humans

733
00:44:29,720 --> 00:44:35,520
to get almost as good as AI,
and this is just not plausible.

734
00:44:35,520 --> 00:44:40,050
If it's one person, you can see
how sparse the detection is,

735
00:44:40,050 --> 00:44:41,070
and that's not good.

736
00:44:41,070 --> 00:44:43,110
So this is one application.

737
00:44:43,110 --> 00:44:48,620
Another application we
worked on is his ICUs.

738
00:44:48,620 --> 00:44:52,110
ICU is where patients
fight life and death.

739
00:44:52,110 --> 00:44:59,070
ICU is also where 1%
of US GDP is spent.

740
00:44:59,070 --> 00:45:04,700
So making ICU as effective
as safely as possible

741
00:45:04,700 --> 00:45:06,300
is a top priority.

742
00:45:06,300 --> 00:45:10,250
One of the goals
or the goal of ICU

743
00:45:10,250 --> 00:45:14,390
is to get our patients
safely out of ICU

744
00:45:14,390 --> 00:45:17,490
and go into step down
units or even go home.

745
00:45:17,490 --> 00:45:23,360
So one of the most important
thing people have learned in ICU

746
00:45:23,360 --> 00:45:26,900
is to help patients to move.

747
00:45:26,900 --> 00:45:30,180
Proper movement, which
we call mobilization,

748
00:45:30,180 --> 00:45:32,400
is actually important
for recovery.

749
00:45:32,400 --> 00:45:34,770
But this is a very
dicey situation.

750
00:45:34,770 --> 00:45:36,690
You have to get nurses to help.

751
00:45:36,690 --> 00:45:40,980
Doctors have to give orders,
and you have to move properly,

752
00:45:40,980 --> 00:45:45,390
and it has to be in different
time, like designated time.

753
00:45:45,390 --> 00:45:47,560
And you have to
assess the movement

754
00:45:47,560 --> 00:45:49,790
and all this is not easy, right?

755
00:45:49,790 --> 00:45:54,220
So we collaborated with Stanford
as well as Utah's Intermountain

756
00:45:54,220 --> 00:45:57,730
Hospital to put these
smart sensors in ICU units

757
00:45:57,730 --> 00:46:02,710
and help doctors to
monitor patient movement,

758
00:46:02,710 --> 00:46:05,925
in this particular case, four
different kind of movements,

759
00:46:05,925 --> 00:46:08,300
getting out of bed, getting
in bed, getting out of chair,

760
00:46:08,300 --> 00:46:09,320
getting in chair.

761
00:46:09,320 --> 00:46:12,350
These things are so
important for ICU patients.

762
00:46:12,350 --> 00:46:15,080
I know that for us
it's a no brainer,

763
00:46:15,080 --> 00:46:18,440
but this really is critical.

764
00:46:18,440 --> 00:46:23,560
And you can see that I can help
to do the kind of detection

765
00:46:23,560 --> 00:46:28,510
and prediction that is so
helpful for doctors, especially

766
00:46:28,510 --> 00:46:30,610
when there's a labor shortage.

767
00:46:30,610 --> 00:46:34,250
Last, but not the least
example is aging in place.

768
00:46:34,250 --> 00:46:38,510
And this is just so important
for many, many reasons.

769
00:46:38,510 --> 00:46:42,220
People are seniors, want to
live at home independently

770
00:46:42,220 --> 00:46:43,600
and healthily.

771
00:46:43,600 --> 00:46:47,820
And I remember during
the beginning of COVID

772
00:46:47,820 --> 00:46:54,250
when we had so much fatality
among the aging seniors.

773
00:46:54,250 --> 00:46:59,700
A lot has to do with hospital
overrun and overtaxed hospital

774
00:46:59,700 --> 00:47:04,500
system, so putting and
keeping seniors safe and well

775
00:47:04,500 --> 00:47:06,730
in their homes is
really critical.

776
00:47:06,730 --> 00:47:10,410
And using smart
sensors, we can help

777
00:47:10,410 --> 00:47:12,900
early detection of
infection, especially

778
00:47:12,900 --> 00:47:16,390
using thermal
cameras or mobility,

779
00:47:16,390 --> 00:47:19,080
we just talked about
in ICU, similar here

780
00:47:19,080 --> 00:47:22,050
or understanding sleep
patterns or understanding

781
00:47:22,050 --> 00:47:23,320
dietary patterns.

782
00:47:23,320 --> 00:47:26,370
All these are realms
of possibilities

783
00:47:26,370 --> 00:47:29,700
by AI and smart sensors.

784
00:47:29,700 --> 00:47:33,990
And then last but not
least, what if there's still

785
00:47:33,990 --> 00:47:36,700
labor shortage
after smart sensors.

786
00:47:36,700 --> 00:47:40,740
The thing about smart sensors
is that they are information

787
00:47:40,740 --> 00:47:46,010
gathering system, but they
cannot go there and help to turn

788
00:47:46,010 --> 00:47:51,450
a patient or bring water and
medicine pills to the elderly.

789
00:47:51,450 --> 00:47:55,730
So this brings us to the last
technical topic, which is

790
00:47:55,730 --> 00:47:59,540
embodied AI, or we would call--

791
00:47:59,540 --> 00:48:03,960
large part of embodied
AI is robotics.

792
00:48:03,960 --> 00:48:07,790
And this is where I find
it extremely exciting

793
00:48:07,790 --> 00:48:11,850
because it closes the loop
between perception and action.

794
00:48:11,850 --> 00:48:18,350
And if you think about the
Cambrian explosion of evolution,

795
00:48:18,350 --> 00:48:22,680
when there's an onset of
eyes animals start to move.

796
00:48:22,680 --> 00:48:26,420
So the area of
robotics is where we

797
00:48:26,420 --> 00:48:29,310
can close the loop
between seeing and doing.

798
00:48:29,310 --> 00:48:31,140
But it's not easy, right?

799
00:48:31,140 --> 00:48:34,730
Robots, as much as we're
very excited by them, still

800
00:48:34,730 --> 00:48:36,360
are very, very slow.

801
00:48:36,360 --> 00:48:38,850
They are very, very clumsy.

802
00:48:38,850 --> 00:48:45,730
It's very hard for them to adapt
to a generalizable situation.

803
00:48:45,730 --> 00:48:50,170
And in today's robotic
research, we as a field

804
00:48:50,170 --> 00:48:55,840
have made a ton of progress,
and Stanford is definitely

805
00:48:55,840 --> 00:48:58,520
one of the centers
of robotic learning.

806
00:48:58,520 --> 00:49:03,860
But still, most of these work
are constrained in their setup,

807
00:49:03,860 --> 00:49:07,240
our short horizon
tasks pick and place,

808
00:49:07,240 --> 00:49:15,070
and it has anecdotal setup
and lack of clinical--

809
00:49:15,070 --> 00:49:18,260
sorry, lack of
standard benchmark.

810
00:49:18,260 --> 00:49:22,870
So let me just share with you
a couple of work in our lab.

811
00:49:22,870 --> 00:49:25,540
One work is, a few
years ago, we're

812
00:49:25,540 --> 00:49:29,120
looking at how to bring
robots to the wild.

813
00:49:29,120 --> 00:49:35,270
If we have to predesignate the
set of tasks, it's unsatisfying.

814
00:49:35,270 --> 00:49:37,970
On the other hand, if
you look at today's LLM,

815
00:49:37,970 --> 00:49:39,560
it's totally in the wild.

816
00:49:39,560 --> 00:49:41,090
You can talk about anything.

817
00:49:41,090 --> 00:49:44,320
So my student want to
learn and a few students

818
00:49:44,320 --> 00:49:45,950
want to close this gap.

819
00:49:45,950 --> 00:49:50,410
So this idea is
that how do we give

820
00:49:50,410 --> 00:49:55,120
an open instruction to
a robot, any instruction

821
00:49:55,120 --> 00:50:02,200
without pretraining everything
in a closed world and the robot

822
00:50:02,200 --> 00:50:03,680
can do some tasks?

823
00:50:03,680 --> 00:50:08,650
So let's say your training
set is open a door like that.

824
00:50:08,650 --> 00:50:11,720
In the wild, you
have doors like that.

825
00:50:11,720 --> 00:50:17,830
So how do you make some
progress in that problem?

826
00:50:17,830 --> 00:50:22,760
So the goal is, in the
wild, generalization.

827
00:50:22,760 --> 00:50:31,130
And here is a result
or overall algorithm.

828
00:50:31,130 --> 00:50:34,330
I don't know if this is
so glitchy, but whatever.

829
00:50:34,330 --> 00:50:38,970
What you're saying is we
want to tell this robot

830
00:50:38,970 --> 00:50:45,240
arm to open a drawer by planning
a motion path that avoids

831
00:50:45,240 --> 00:50:46,990
knocking down that flower.

832
00:50:46,990 --> 00:50:51,520
And all these instructions
were not pretrained.

833
00:50:51,520 --> 00:50:57,840
So what we do is actually we
borrow the latest advances

834
00:50:57,840 --> 00:51:02,320
in LLM as well as in
visual language model.

835
00:51:02,320 --> 00:51:10,020
And the idea is that we
use LLM and VLM to give us

836
00:51:10,020 --> 00:51:11,980
an instruction set.

837
00:51:11,980 --> 00:51:14,250
And then we use
visual language model

838
00:51:14,250 --> 00:51:17,680
to help us to recognize or
understand the environment.

839
00:51:17,680 --> 00:51:21,480
And then we turn that
into a motion planning map

840
00:51:21,480 --> 00:51:24,790
so that the robotic
arm can execute.

841
00:51:24,790 --> 00:51:29,290
And because we're using
LLMs as well as VLMs,

842
00:51:29,290 --> 00:51:36,110
we get rid of the problem of
training robot in a closed world

843
00:51:36,110 --> 00:51:39,890
and bring them to a more
generalizable or in the wild.

844
00:51:39,890 --> 00:51:44,510
And the details is the
instruction of open top drawer

845
00:51:44,510 --> 00:51:45,690
comes in.

846
00:51:45,690 --> 00:51:51,240
LLM turns this into
like literally codes.

847
00:51:51,240 --> 00:51:55,220
And then because of
these instructions

848
00:51:55,220 --> 00:52:02,700
like draw or handle, we send
this information to a VLM model.

849
00:52:02,700 --> 00:52:07,260
And that model detects draw
and handle in the scene.

850
00:52:07,260 --> 00:52:13,700
And then because of that,
it updates its information,

851
00:52:13,700 --> 00:52:16,740
and then it updates
a motion map.

852
00:52:16,740 --> 00:52:19,730
This is presented by
a heat map to show you

853
00:52:19,730 --> 00:52:22,020
where the robot
arm should focus,

854
00:52:22,020 --> 00:52:25,970
where it should not focus,
and with that, then you

855
00:52:25,970 --> 00:52:30,030
give it another instruction,
but watch out for the vase.

856
00:52:30,030 --> 00:52:34,000
Again, it goes through the same
thing with LLM, writes the code

857
00:52:34,000 --> 00:52:39,650
or generate the code, send
it through a VLM model.

858
00:52:39,650 --> 00:52:45,130
VLM model detects the object and
then updates the motion planning

859
00:52:45,130 --> 00:52:45,920
map.

860
00:52:45,920 --> 00:52:49,040
In this case, it's the
negative, not the positive,

861
00:52:49,040 --> 00:52:51,050
because you want to avoid that.

862
00:52:51,050 --> 00:52:53,840
And then combining
with the previous map,

863
00:52:53,840 --> 00:52:57,950
you get a heat map of knowing
where to avoid and where to go.

864
00:52:57,950 --> 00:53:04,140
And eventually what we do is we
do this for the motion planning

865
00:53:04,140 --> 00:53:04,640
map.

866
00:53:04,640 --> 00:53:08,230
We do it for rotation
to gripper, velocity,

867
00:53:08,230 --> 00:53:11,650
and then this is the
result. Actually,

868
00:53:11,650 --> 00:53:13,360
let me just show you this.

869
00:53:13,360 --> 00:53:18,560
This is the actual
result of the robot.

870
00:53:18,560 --> 00:53:21,675
And then we do this for
many different tasks, right?

871
00:53:21,675 --> 00:53:26,120
We can do it for articulated
object manipulation.

872
00:53:26,120 --> 00:53:27,345
We can do it--

874
00:53:30,670 --> 00:53:33,640
here just many
different examples.

875
00:53:33,640 --> 00:53:37,470
Napkins or sweeping the floor.

876
00:53:37,470 --> 00:53:38,530
What is this?

877
00:53:38,530 --> 00:53:44,670
Getting toast, setting
up table, and also

878
00:53:44,670 --> 00:53:48,190
dealing with online
disturbances and so on.

879
00:53:48,190 --> 00:53:50,710
So this is one work.

880
00:53:50,710 --> 00:53:54,810
Another work I want to
just show you quickly

881
00:53:54,810 --> 00:54:03,420
is that overall robotics
research is still lacking

882
00:54:03,420 --> 00:54:10,240
good benchmark, and while we're
still experimenting in the labs,

883
00:54:10,240 --> 00:54:14,400
we know real world is so much
more complex, so much more

884
00:54:14,400 --> 00:54:17,140
uncertain, have
large variability,

885
00:54:17,140 --> 00:54:20,400
is so interactive
and social, and it

886
00:54:20,400 --> 00:54:23,020
has a lot of multitasking task.

887
00:54:23,020 --> 00:54:26,850
And then we know that both
natural language and computer

888
00:54:26,850 --> 00:54:33,060
vision has benefited a lot from
setting up important large scale

889
00:54:33,060 --> 00:54:36,460
data sets for both
training and benchmark.

890
00:54:36,460 --> 00:54:40,680
So in our lab, we have been
working on this project that

891
00:54:40,680 --> 00:54:47,250
is towards an building an
ecological robotic learning

892
00:54:47,250 --> 00:54:52,590
environment and try to
encourage researchers

893
00:54:52,590 --> 00:54:57,190
to benchmark against a large
and diverse set of activities.

894
00:54:57,190 --> 00:55:00,280
And that's the
behavior of benchmark,

895
00:55:00,280 --> 00:55:03,330
which is benchmark
for everyday household

896
00:55:03,330 --> 00:55:06,330
activities in virtual
interactive and ecological

897
00:55:06,330 --> 00:55:08,130
environments.

898
00:55:08,130 --> 00:55:11,160
Now, here's a question
because this lecture has a lot

899
00:55:11,160 --> 00:55:15,240
to do with human values
is, who is to say

900
00:55:15,240 --> 00:55:17,830
which tasks robots should do?

901
00:55:17,830 --> 00:55:19,830
I know that every
graduate students

902
00:55:19,830 --> 00:55:22,600
who are working on robotics
just want two tasks.

903
00:55:22,600 --> 00:55:25,440
One is laundry, the
other one is dishwasher.

904
00:55:25,440 --> 00:55:31,790
That's great, but moving beyond
grad school, what are the tasks

905
00:55:31,790 --> 00:55:34,200
we should get
robots to do for us?

906
00:55:34,200 --> 00:55:38,760
So instead of us coming
up with this task list,

907
00:55:38,760 --> 00:55:44,390
we actually did a human-centered
survey to ask robots--

908
00:55:44,390 --> 00:55:46,000
or sorry to ask humans--

909
00:55:46,000 --> 00:55:47,660
[LAUGHTER]

910
00:55:47,660 --> 00:55:49,920
--that what would you
like robots to help you?

911
00:55:49,920 --> 00:55:52,100
Let me test this.

912
00:55:52,100 --> 00:55:57,320
Would you like a robot to help
you to clean the kitchen floor?

913
00:55:57,320 --> 00:55:58,700
Say yes or no.

914
00:55:58,700 --> 00:56:00,320
OK, good.

915
00:56:00,320 --> 00:56:03,440
Normal people would say yes.

916
00:56:03,440 --> 00:56:05,600
Shoveling snow?

917
00:56:05,600 --> 00:56:06,440
OK.

918
00:56:06,440 --> 00:56:07,555
Folding laundry?

919
00:56:07,555 --> 00:56:08,360
Yes.

920
00:56:08,360 --> 00:56:09,950
OK, good.

921
00:56:09,950 --> 00:56:11,460
Cooking breakfast?

922
00:56:11,460 --> 00:56:11,960
No.

923
00:56:11,960 --> 00:56:12,710
Yes.

924
00:56:12,710 --> 00:56:16,490
See, we're getting
mixture answers, right?

925
00:56:16,490 --> 00:56:18,790
What about opening
Christmas gift?

926
00:56:18,790 --> 00:56:19,490
No.

927
00:56:19,490 --> 00:56:21,150
Exactly.

928
00:56:21,150 --> 00:56:22,600
People are different.

929
00:56:22,600 --> 00:56:25,320
I actually think robot
can do this pretty well,

930
00:56:25,320 --> 00:56:28,970
but we don't want it.

931
00:56:28,970 --> 00:56:31,400
One of the tasks we ask
is buying wedding rings.

932
00:56:31,400 --> 00:56:33,220
Can you imagine that?

933
00:56:33,220 --> 00:56:37,330
So what we did is
actually-- we want

934
00:56:37,330 --> 00:56:39,380
to respect human preference.

935
00:56:39,380 --> 00:56:42,970
So we took a bunch
of government surveys

936
00:56:42,970 --> 00:56:48,410
from a labor office in the
US and Europe and so on,

937
00:56:48,410 --> 00:56:55,190
and put together thousands
of everyday activity tasks.

938
00:56:55,190 --> 00:56:59,270
And then we went
online to find people.

939
00:56:59,270 --> 00:57:02,030
We want to be as
diverse as possible,

940
00:57:02,030 --> 00:57:05,690
but I think we have
room to improve.

941
00:57:05,690 --> 00:57:08,450
But we found 1,400 people.

942
00:57:08,450 --> 00:57:12,700
And to answer these
tasks and tell us

943
00:57:12,700 --> 00:57:16,790
which tasks they want robots
to help, and then we rank that.

944
00:57:16,790 --> 00:57:20,830
And you can see that
just like grad students,

945
00:57:20,830 --> 00:57:25,610
people want robots to help with
cleaning, a lot of cleaning,

946
00:57:25,610 --> 00:57:30,150
toilet cleaning, floor cleaning,
but people don't want robots

947
00:57:30,150 --> 00:57:34,390
to play squash for you,
or to buy a wedding ring,

948
00:57:34,390 --> 00:57:36,550
or to even mix baby cereals.

949
00:57:36,550 --> 00:57:38,430
There is a lot of
tasks that matters

950
00:57:38,430 --> 00:57:42,370
to us as humans, emotionally
or socially or whatever.

951
00:57:42,370 --> 00:57:51,810
So our goal is first we have a
principled way to decide which

952
00:57:51,810 --> 00:57:57,030
are the 1,000 tasks that we want
to train robots for, and those

953
00:57:57,030 --> 00:57:59,950
are the tasks that humans
prefer to get help.

954
00:57:59,950 --> 00:58:05,010
And with that in mind,
we have to actually build

955
00:58:05,010 --> 00:58:07,060
a virtual environments.

956
00:58:07,060 --> 00:58:15,900
So we scanned or acquired 3D
scene from 50 different real

957
00:58:15,900 --> 00:58:19,410
world environments from
restaurants to apartments

958
00:58:19,410 --> 00:58:24,820
to grocery stores to
offices and so on.

959
00:58:24,820 --> 00:58:29,980
And then we acquired, this
number is actually outdated,

960
00:58:29,980 --> 00:58:34,000
we acquire more than
10,000 object assets,

961
00:58:34,000 --> 00:58:37,930
3D assets that has
a lot of properties,

962
00:58:37,930 --> 00:58:41,160
whether it's articulation,
deformability,

963
00:58:41,160 --> 00:58:43,030
and all those properties.

964
00:58:43,030 --> 00:58:47,620
And then we have to build
a simulation environment.

965
00:58:47,620 --> 00:58:50,350
A lot of people have built
simulation environment.

966
00:58:50,350 --> 00:58:51,730
Let me just fast forward.

967
00:58:51,730 --> 00:58:54,810
But our particular
simulation environment

968
00:58:54,810 --> 00:58:58,090
was a collaboration with
NVIDIA's Omniverse group.

969
00:58:58,090 --> 00:59:03,010
And we were going for
building a physically,

970
00:59:03,010 --> 00:59:06,390
perceptually, and
also interactively

971
00:59:06,390 --> 00:59:11,700
high-quality simulation
environment, and this especially

972
00:59:11,700 --> 00:59:14,340
taking account, for
example, physical effects

973
00:59:14,340 --> 00:59:18,550
like thermal transparency,
deformability and so on.

974
00:59:18,550 --> 00:59:22,830
We also tested our
behavior environment

975
00:59:22,830 --> 00:59:27,740
against other environments in
terms of perceptual realism

976
00:59:27,740 --> 00:59:29,730
from human user study.

977
00:59:29,730 --> 00:59:32,840
And here are some
of the examples

978
00:59:32,840 --> 00:59:39,270
of physical interaction, such
as cloth or liquids and so on.

979
00:59:39,270 --> 00:59:43,350
So there's a lot of nuance
that has gone into this work.

980
00:59:43,350 --> 00:59:48,530
And let me just fast forward,
and these are some benchmarks

981
00:59:48,530 --> 00:59:51,030
we did compared to other work.

982
00:59:51,030 --> 00:59:51,530
OK.

983
00:59:51,530 --> 00:59:54,410
Let me just fast forward.

984
00:59:54,410 --> 00:59:58,320
So this is an ongoing
work actually in our lab,

985
00:59:58,320 --> 01:00:04,790
and because of this,
we are using behavior

986
01:00:04,790 --> 01:00:08,360
to help us to learn
robotics, to help us,

987
01:00:08,360 --> 01:00:11,960
actually, to push us to
gather more interesting data

988
01:00:11,960 --> 01:00:17,700
and also to use that for
even cognitive studies.

989
01:00:17,700 --> 01:00:19,253
Let me just fast forward.

991
01:00:23,410 --> 01:00:28,150
One thing I want to
share with you is that--

992
01:00:28,150 --> 01:00:33,670
let me just share these numbers,
is today's algorithm still

993
01:00:33,670 --> 01:00:35,990
cannot do behavior tasks.

994
01:00:35,990 --> 01:00:41,320
And of all these
roles, the top role

995
01:00:41,320 --> 01:00:45,110
is what we wish robots can do.

996
01:00:45,110 --> 01:00:47,210
Give them no
privileged information.

997
01:00:47,210 --> 01:00:50,750
They have to be dropped in an
environment and do these tasks.

998
01:00:50,750 --> 01:00:53,710
And we benchmarked
three behavior tasks

999
01:00:53,710 --> 01:00:57,620
using today's robotic algorithm.

1000
01:00:57,620 --> 01:01:00,260
And the performance
is just zero.

1001
01:01:00,260 --> 01:01:06,280
And once you start to give
more privileged information

1002
01:01:06,280 --> 01:01:08,530
or make assumptions
that make the task

1003
01:01:08,530 --> 01:01:15,290
simpler, like magic motion or
a perfect memory and all that,

1004
01:01:15,290 --> 01:01:18,130
things start to get better.

1005
01:01:18,130 --> 01:01:23,110
If you look at it, only
look at the top row,

1006
01:01:23,110 --> 01:01:25,780
you get pretty depressed
by today's robots.

1007
01:01:25,780 --> 01:01:27,720
But as a grad
student, I hope you're

1008
01:01:27,720 --> 01:01:33,750
inspired because that means
we have a lot of room to grow.

1009
01:01:33,750 --> 01:01:35,070
OK.

1010
01:01:35,070 --> 01:01:37,360
These are just different
papers from our lab.

1011
01:01:37,360 --> 01:01:39,780
I'm going to actually fast
forward because I think

1012
01:01:39,780 --> 01:01:42,035
we've talked enough about this.

1014
01:01:44,910 --> 01:01:50,040
Well, by the way, we're also
doing digital twin of behavior

1015
01:01:50,040 --> 01:01:52,830
in the digital
environment as well as

1016
01:01:52,830 --> 01:01:56,340
in the real-world
environment, and that's

1017
01:01:56,340 --> 01:02:00,880
a great way of testing
real to sim transfer.

1018
01:02:00,880 --> 01:02:03,730
Again, this is an
unsolved problem.

1019
01:02:03,730 --> 01:02:07,720
And there's a long way to go.

1020
01:02:07,720 --> 01:02:09,930
And in this
particular case, we're

1021
01:02:09,930 --> 01:02:13,590
showing you that this
robot, without speeding up,

1022
01:02:13,590 --> 01:02:15,580
you can see how slow it is.

1023
01:02:15,580 --> 01:02:17,515
It's trying to
clean up this room.

1025
01:02:21,200 --> 01:02:21,710
OK.

1026
01:02:21,710 --> 01:02:23,420
Hooray.

1027
01:02:23,420 --> 01:02:27,590
This is actually
some of the mistakes

1028
01:02:27,590 --> 01:02:30,890
that this robot makes in--

1029
01:02:30,890 --> 01:02:35,120
for example, it cannot
pick up the bottle,

1030
01:02:35,120 --> 01:02:41,720
or earlier it just went
the wrong way and placed it

1031
01:02:41,720 --> 01:02:42,690
in the wrong place.

1032
01:02:42,690 --> 01:02:45,420
So there's still
a lot of mistakes.

1033
01:02:45,420 --> 01:02:50,150
OK, let me fast forward.

1034
01:02:50,150 --> 01:02:53,330
We're actually we're also
using this environment

1035
01:02:53,330 --> 01:02:56,040
to study visually
impaired patients,

1036
01:02:56,040 --> 01:02:59,630
and this is a great
way of putting patients

1037
01:02:59,630 --> 01:03:03,950
in a safe environment to study.

1038
01:03:03,950 --> 01:03:07,950
One last thing I want to show
you is really super cool,

1039
01:03:07,950 --> 01:03:12,740
and this is the last
technical work I want to show

1040
01:03:12,740 --> 01:03:20,060
is that we also now are
collaborating with psychologists

1041
01:03:20,060 --> 01:03:25,100
and doctors to study how we
can use brain waves to control

1042
01:03:25,100 --> 01:03:26,040
robots.

1043
01:03:26,040 --> 01:03:28,790
So what you're
seeing here is a demo

1044
01:03:28,790 --> 01:03:34,380
where a grad student, I think,
is wearing this EEG cap,

1045
01:03:34,380 --> 01:03:38,130
and that just is
sending instructions,

1046
01:03:38,130 --> 01:03:42,440
and the robotic arm is
cooking a Japanese meal purely

1047
01:03:42,440 --> 01:03:43,740
from thoughts.

1048
01:03:43,740 --> 01:03:47,250
There's no invasive
brain control.

1049
01:03:47,250 --> 01:03:49,530
This is from electrical signals.

1050
01:03:49,530 --> 01:03:53,580
What we have to do is to
pretrain these thoughts.

1051
01:03:53,580 --> 01:03:59,030
You have to pretrain the robotic
arm with, say, lift or place

1052
01:03:59,030 --> 01:04:01,680
or drop or whatever.

1053
01:04:01,680 --> 01:04:04,640
And once you do that,
this is an entire meal

1054
01:04:04,640 --> 01:04:06,750
cooked based on the wave.

1055
01:04:06,750 --> 01:04:08,820
This is really sci-fi.

1056
01:04:08,820 --> 01:04:11,640
And this has happened last year.

1057
01:04:11,640 --> 01:04:16,840
So I'm pretty excited by where
all this is going, combining

1058
01:04:16,840 --> 01:04:19,630
vision and perception
and robotics

1059
01:04:19,630 --> 01:04:23,180
and also helping people
in clinical settings.

1060
01:04:23,180 --> 01:04:25,450
This is really
the future of this

1061
01:04:25,450 --> 01:04:29,530
is helping severely
paralyzed patients.

1062
01:04:29,530 --> 01:04:34,000
OK, so behavior
project is really

1063
01:04:34,000 --> 01:04:37,150
aimed at augmenting people.

1064
01:04:37,150 --> 01:04:41,690
It's a large scale
diverse benchmark.

1065
01:04:41,690 --> 01:04:45,670
And it has realistic
and ecological physics

1066
01:04:45,670 --> 01:04:46,880
and perception.

1067
01:04:46,880 --> 01:04:52,510
And the last take
home message is

1068
01:04:52,510 --> 01:04:57,280
that we not only want to build
AI to just do things or see

1069
01:04:57,280 --> 01:05:00,400
things, we really want to
build it to help people.

1070
01:05:00,400 --> 01:05:05,920
AI being an augmentation tool
or enhancing tool for humanity

1071
01:05:05,920 --> 01:05:10,050
is very important instead
of a tool that replaces.