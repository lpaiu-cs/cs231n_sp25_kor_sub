2
00:00:05,600 --> 00:00:12,000
Last time, on Tuesday this
week, we had a lecture on GPUs

3
00:00:12,000 --> 00:00:14,380
and how to train,
how to use them,

4
00:00:14,380 --> 00:00:18,700
and how to use multiple GPUs
for training, larger scaling,

5
00:00:18,700 --> 00:00:20,920
your trainings, and so on.

6
00:00:20,920 --> 00:00:23,440
An exciting, a new
topic that we've

7
00:00:23,440 --> 00:00:26,400
added to this class
this year, which

8
00:00:26,400 --> 00:00:29,360
is, I think, timely
and very important

9
00:00:29,360 --> 00:00:34,960
with the increase of model
sizes and applications

10
00:00:34,960 --> 00:00:39,840
that you see the AI
models have these days.

11
00:00:39,840 --> 00:00:47,800
And before that, we talked about
we covered all of the key tasks

12
00:00:47,800 --> 00:00:50,660
in computer vision,
from classification,

13
00:00:50,660 --> 00:00:53,400
semantic segmentation,
object detection, instance

14
00:00:53,400 --> 00:00:56,880
segmentation, and so on.

15
00:00:56,880 --> 00:00:59,810
And we're going to revisit
some of these topics, some

16
00:00:59,810 --> 00:01:02,790
of the results of the
models we talk about today.

17
00:01:02,790 --> 00:01:09,410
So those tasks are
still quite important.

18
00:01:09,410 --> 00:01:12,650
And then we talked about
visualizing and understanding

19
00:01:12,650 --> 00:01:19,490
the models and seeing what
the models are learning.

20
00:01:19,490 --> 00:01:21,130
One of the things
that we've discussed

21
00:01:21,130 --> 00:01:27,830
was, for example, using
just in the early sessions,

22
00:01:27,830 --> 00:01:31,130
we talked about nearest
neighbor and in the pixel space

23
00:01:31,130 --> 00:01:37,690
and how we can actually do
find the class of images

24
00:01:37,690 --> 00:01:40,550
based on only
pixels-based distances,

25
00:01:40,550 --> 00:01:45,330
and we discussed how it's
actually not efficient.

26
00:01:45,330 --> 00:01:47,250
And one of the things
that we talked about

27
00:01:47,250 --> 00:01:53,130
was if we use the embedding
layers or the feature

28
00:01:53,130 --> 00:01:56,730
space, feature layers, one of
those fully connected layers

29
00:01:56,730 --> 00:02:03,060
in the feature maps there, from
a convolutional neural network

30
00:02:03,060 --> 00:02:05,780
or any other network
architecture that we

31
00:02:05,780 --> 00:02:10,020
use there, that could actually
be a good representation

32
00:02:10,020 --> 00:02:11,280
of images.

33
00:02:11,280 --> 00:02:14,520
And we talked about
the L2 distance,

34
00:02:14,520 --> 00:02:19,260
if we use that as the
metric for nearest neighbor

35
00:02:19,260 --> 00:02:23,540
in the feature space, feature
of these models, right?

36
00:02:23,540 --> 00:02:25,300
So this means that
these features

37
00:02:25,300 --> 00:02:28,380
are quite meaningful
for the specific task

38
00:02:28,380 --> 00:02:30,260
that we had at hand.

39
00:02:30,260 --> 00:02:34,460
And this means that
specifically, OK,

40
00:02:34,460 --> 00:02:39,660
if we run a neural network,
a CNN, a ResNet, or even

41
00:02:39,660 --> 00:02:46,800
the transformer models, and
look at the representations,

42
00:02:46,800 --> 00:02:49,200
the learned representations,
in different contexts,

43
00:02:49,200 --> 00:02:54,740
you may see these as referred
to with different names.

44
00:02:54,740 --> 00:02:56,890
Learned large
representations, features,

45
00:02:56,890 --> 00:02:59,810
embeddings, latent
space, and so on.

46
00:02:59,810 --> 00:03:04,230
But these learned
representations or features

47
00:03:04,230 --> 00:03:07,590
are very good representatives
of the images.

48
00:03:07,590 --> 00:03:12,970
And if we have a way
to extract those,

49
00:03:12,970 --> 00:03:17,470
we can always get the class
labels out of those features

50
00:03:17,470 --> 00:03:21,350
as well by a simple linear
model, as you can see at the end

51
00:03:21,350 --> 00:03:22,430
here.

52
00:03:22,430 --> 00:03:24,910
But the major
challenge that exists

53
00:03:24,910 --> 00:03:30,190
is training or
building these networks

54
00:03:30,190 --> 00:03:33,530
at larger scale is
always challenging.

55
00:03:33,530 --> 00:03:37,350
And can you tell me why
there is a challenge here?

56
00:03:37,350 --> 00:03:40,690
So the thing is that
at larger scale,

57
00:03:40,690 --> 00:03:45,110
we need a lot of labeled data
because this network is trained

58
00:03:45,110 --> 00:03:49,270
starting from an image, and at
the end, we have class labels.

59
00:03:49,270 --> 00:03:51,550
If we train this network,
yes, these features

60
00:03:51,550 --> 00:03:56,190
are going to be very useful for
getting those class labels out,

61
00:03:56,190 --> 00:03:57,370
right?

62
00:03:57,370 --> 00:04:02,410
But at scale, we need a lot
of manual labeling efforts

63
00:04:02,410 --> 00:04:05,148
to sit down and label
the images one by one.

64
00:04:05,148 --> 00:04:06,690
If the task is
segmentation, you have

65
00:04:06,690 --> 00:04:10,850
to label the pixels one
by one in every image,

66
00:04:10,850 --> 00:04:14,650
and that is going to
be very challenging.

67
00:04:14,650 --> 00:04:18,130
So the question
is is there a way

68
00:04:18,130 --> 00:04:22,490
we can train neural networks
without the need for huge

69
00:04:22,490 --> 00:04:25,190
manually labeled data sets?

70
00:04:25,190 --> 00:04:29,470
So these manual labels
are the challenge,

71
00:04:29,470 --> 00:04:32,970
and we want to see
if we can bypass them

72
00:04:32,970 --> 00:04:36,290
in a way to train a
neural network that

73
00:04:36,290 --> 00:04:39,410
gets us very good features.

74
00:04:39,410 --> 00:04:44,330
And with that, the topic
of self-supervised learning

75
00:04:44,330 --> 00:04:45,270
comes in light.

76
00:04:45,270 --> 00:04:48,570
And that's what we are
going to cover today.

77
00:04:48,570 --> 00:04:58,700
So having a large data set of,
say, images without any labels,

78
00:04:58,700 --> 00:05:02,020
our hypothesis is that we
can train a neural network

79
00:05:02,020 --> 00:05:08,460
using an objective function, a
pretext task that gets us good

80
00:05:08,460 --> 00:05:12,600
features from images.

81
00:05:12,600 --> 00:05:17,860
And then when it comes to
learning on a specific data set

82
00:05:17,860 --> 00:05:23,860
with a smaller set of data
points, which have labels,

83
00:05:23,860 --> 00:05:28,740
we can basically train, we can
transfer this trained encoder

84
00:05:28,740 --> 00:05:32,260
and use that to extract
features for a downstream task

85
00:05:32,260 --> 00:05:34,500
or downstream objective.

86
00:05:34,500 --> 00:05:38,780
So here, we want to define
a pretext task, a task that

87
00:05:38,780 --> 00:05:43,540
is general enough
to be able to learn

88
00:05:43,540 --> 00:05:47,500
some good features
from the images

89
00:05:47,500 --> 00:05:53,750
and then use that encoder,
let's call it encoder,

90
00:05:53,750 --> 00:05:56,870
to solve another
problem, what we call

91
00:05:56,870 --> 00:05:59,990
downstream task,
downstream objective, which

92
00:05:59,990 --> 00:06:02,330
is the application
that you care about.

93
00:06:02,330 --> 00:06:06,630
Like for example, we have a lot
of natural images downloaded

94
00:06:06,630 --> 00:06:08,130
from the internet.

95
00:06:08,130 --> 00:06:10,330
We can train
something out of it.

96
00:06:10,330 --> 00:06:13,290
And then we have a small
data set of, for example,

97
00:06:13,290 --> 00:06:14,910
one of these
industrial applications

98
00:06:14,910 --> 00:06:18,870
or medical applications that
we have few labeled images on.

99
00:06:18,870 --> 00:06:21,990
And we can now use that
transferred knowledge

100
00:06:21,990 --> 00:06:26,270
to extract features and then
classify or perform the task

101
00:06:26,270 --> 00:06:29,350
that we are interested in.

102
00:06:29,350 --> 00:06:32,390
So we want to delve
into this topic

103
00:06:32,390 --> 00:06:35,430
and go a little bit
deeper in understanding

104
00:06:35,430 --> 00:06:37,590
the different components.

105
00:06:37,590 --> 00:06:40,810
In a nutshell, what
self-supervised learning is,

106
00:06:40,810 --> 00:06:45,590
as I said, defining this
pretext task on the data

107
00:06:45,590 --> 00:06:47,110
set with no labels.

108
00:06:47,110 --> 00:06:55,140
Encoder often gets us some
learned representations.

109
00:06:55,140 --> 00:06:59,640
And then another module
of the same neural network

110
00:06:59,640 --> 00:07:04,080
generates or does transfer
the learned representations

111
00:07:04,080 --> 00:07:06,240
into the output
space, which could

112
00:07:06,240 --> 00:07:10,440
be labels or outputs
that automatically

113
00:07:10,440 --> 00:07:11,980
are generated from the data.

114
00:07:11,980 --> 00:07:14,880
They are not manual annotations.

115
00:07:14,880 --> 00:07:22,720
So if we can do this, then we
have an objective function,

116
00:07:22,720 --> 00:07:25,400
a loss function,
and a neural network

117
00:07:25,400 --> 00:07:29,280
to be trained with
loss function.

118
00:07:29,280 --> 00:07:33,680
As you can see here, we call the
second part sometimes decoder,

119
00:07:33,680 --> 00:07:36,600
a classifier, a regressor,
depending on how

120
00:07:36,600 --> 00:07:38,420
we define our pretext task.

121
00:07:38,420 --> 00:07:40,920
I will give you some
examples, but these

122
00:07:40,920 --> 00:07:44,120
could be any form of framework.

123
00:07:44,120 --> 00:07:47,230
But when it's encoder
and then a decoder,

124
00:07:47,230 --> 00:07:49,810
this is more of an
autoencoding framework

125
00:07:49,810 --> 00:07:52,570
that I'll talk briefly about.

126
00:07:52,570 --> 00:07:59,810
So after we do this training
with the pretext task,

127
00:07:59,810 --> 00:08:03,890
now we can use the encoder
and the learn representations

128
00:08:03,890 --> 00:08:07,050
for a downstream
task, which we just

129
00:08:07,050 --> 00:08:12,830
need to add one layer or even a
fully connected neural network,

130
00:08:12,830 --> 00:08:16,010
a linear function, or a fully
connected neural network that

131
00:08:16,010 --> 00:08:18,410
predicts the labels, and
these labels are coming now

132
00:08:18,410 --> 00:08:20,570
from the data set.

133
00:08:20,570 --> 00:08:24,450
So that's the major,
that's the main concept

134
00:08:24,450 --> 00:08:31,230
of self-supervised learning,
that the pretext version of it,

135
00:08:31,230 --> 00:08:35,929
a portion of it doesn't
require any labeled data to do

136
00:08:35,929 --> 00:08:37,289
the training.

137
00:08:37,289 --> 00:08:39,210
But how to define
the pretext task

138
00:08:39,210 --> 00:08:43,169
itself is not that
straightforward.

139
00:08:43,169 --> 00:08:46,560
There are many different
ways of defining those.

140
00:08:46,560 --> 00:08:50,740
For example, just
keep in mind that we

141
00:08:50,740 --> 00:08:56,500
want to define the
pretext task in a way

142
00:08:56,500 --> 00:09:02,020
that it's, first, general enough
that can get us good features

143
00:09:02,020 --> 00:09:04,120
and doesn't require
manual labeling.

144
00:09:04,120 --> 00:09:07,492
So the labels should come
from the data itself, right?

145
00:09:07,492 --> 00:09:14,540
So one example would be image
completion, where we half

146
00:09:14,540 --> 00:09:16,360
of the image or
parts of the image.

147
00:09:16,360 --> 00:09:21,460
And we define a task to given
the parts that are unmasked,

148
00:09:21,460 --> 00:09:23,960
predict the parts
that are masked.

149
00:09:23,960 --> 00:09:28,740
Or for example, we rotate the
image with a specific angle,

150
00:09:28,740 --> 00:09:32,660
and the task is to
take the image as input

151
00:09:32,660 --> 00:09:37,340
and predict what's the rotation
angle that it's gone through.

152
00:09:37,340 --> 00:09:41,740
And the other one could
be a jigsaw puzzle,

153
00:09:41,740 --> 00:09:44,850
where we have patches of the
image that are not ordered,

154
00:09:44,850 --> 00:09:50,590
but the task is to output the
correct order of these patches.

155
00:09:50,590 --> 00:09:53,550
And colorization is
one of the popular ones

156
00:09:53,550 --> 00:09:55,070
that these are the
four that we'll

157
00:09:55,070 --> 00:09:58,350
be covering very quickly today.

158
00:09:58,350 --> 00:10:04,850
But given the black and
white version of the image,

159
00:10:04,850 --> 00:10:08,350
predict the colors for
each of the pixels.

160
00:10:08,350 --> 00:10:12,350
So solving the pretext
task allows the model

161
00:10:12,350 --> 00:10:13,810
to learn good features.

162
00:10:13,810 --> 00:10:15,730
That's what we wanted.

163
00:10:15,730 --> 00:10:18,070
And we can
automatically generate

164
00:10:18,070 --> 00:10:20,210
labels for the pretext tasks.

165
00:10:20,210 --> 00:10:22,430
So the two points
that I mentioned

166
00:10:22,430 --> 00:10:27,350
we need for a task that could
be qualified as a good pretext

167
00:10:27,350 --> 00:10:30,270
task for self-supervised
learning.

168
00:10:30,270 --> 00:10:34,470
Some other quick
considerations to always

169
00:10:34,470 --> 00:10:38,510
keep in mind, how to evaluate
a self-supervised learning

170
00:10:38,510 --> 00:10:39,510
framework.

171
00:10:39,510 --> 00:10:44,560
There are many different pieces
and areas that you can actually

172
00:10:44,560 --> 00:10:47,240
look into the
pretext task itself

173
00:10:47,240 --> 00:10:50,160
because we are generating
the labels and so on.

174
00:10:50,160 --> 00:10:53,880
It gives us the power
to do some evaluation

175
00:10:53,880 --> 00:10:59,080
of how good the model is able
to solve that pretext task.

176
00:10:59,080 --> 00:11:03,700
So that's one of the factors.

177
00:11:03,700 --> 00:11:06,120
Then representation
quality itself

178
00:11:06,120 --> 00:11:08,840
is sometimes very important.

179
00:11:08,840 --> 00:11:14,200
Looking at, for example,
only representations

180
00:11:14,200 --> 00:11:15,960
without any fine
tuning or anything

181
00:11:15,960 --> 00:11:17,760
that I'll be talking
about or even

182
00:11:17,760 --> 00:11:20,680
clustering the
representations to see,

183
00:11:20,680 --> 00:11:23,160
do we see a pattern in
the representations?

184
00:11:23,160 --> 00:11:27,000
And sometimes, there are some
good dimensionality reduction

185
00:11:27,000 --> 00:11:28,060
algorithms.

186
00:11:28,060 --> 00:11:33,120
I'm referring to t-SNE here,
which we didn't very much talk

187
00:11:33,120 --> 00:11:35,240
about, but this is a
dimensionality reduction

188
00:11:35,240 --> 00:11:37,880
framework that you can
reduce the dimensionality

189
00:11:37,880 --> 00:11:41,940
of the learned representations
and then visualize it in 2D

190
00:11:41,940 --> 00:11:45,060
or 3D and see if there is
a pattern that you can find

191
00:11:45,060 --> 00:11:48,140
in your representations.

192
00:11:48,140 --> 00:11:52,440
So robustness, generalization,
and computational efficiency,

193
00:11:52,440 --> 00:11:54,540
these are all quite important.

194
00:11:54,540 --> 00:11:59,660
But the most important thing,
the most important aspect

195
00:11:59,660 --> 00:12:04,780
that we are after
is the performance

196
00:12:04,780 --> 00:12:06,880
on the downstream task.

197
00:12:06,880 --> 00:12:11,380
Because we are doing the
entire self-supervised learning

198
00:12:11,380 --> 00:12:14,020
and define the
tasks, pretext tasks,

199
00:12:14,020 --> 00:12:17,180
and so on to be able
to improve results

200
00:12:17,180 --> 00:12:21,100
for a task of interest or
something that we care about.

201
00:12:21,100 --> 00:12:25,700
Let's see some quick examples
of how this could be done.

202
00:12:25,700 --> 00:12:31,260
This is an example of
let's rotate the images

203
00:12:31,260 --> 00:12:36,280
and then predict the degree
of rotation as the output.

204
00:12:36,280 --> 00:12:39,270
So we can train this in
a self-supervised manner

205
00:12:39,270 --> 00:12:43,110
without the need for labels
of the objects in the image.

206
00:12:43,110 --> 00:12:45,350
And we have a bunch of
convolutional layers

207
00:12:45,350 --> 00:12:48,510
in this example and then a
fully connected neural network

208
00:12:48,510 --> 00:12:52,550
at the end to do this either
regression or classification

209
00:12:52,550 --> 00:12:53,590
task.

210
00:12:53,590 --> 00:12:56,430
And this means that
this is giving us

211
00:12:56,430 --> 00:13:02,270
a set of good features, a
good feature extractor, which

212
00:13:02,270 --> 00:13:07,990
we can at the end remove
these tasks, pretext

213
00:13:07,990 --> 00:13:13,630
task, specific parts, the FC
layers, and put one layer,

214
00:13:13,630 --> 00:13:17,990
or in some cases,
multiple layers

215
00:13:17,990 --> 00:13:21,490
to classify the features
into the object label.

216
00:13:21,490 --> 00:13:26,070
So this time, we use the object
labels to do the prediction

217
00:13:26,070 --> 00:13:30,370
and train this linear
function itself.

218
00:13:30,370 --> 00:13:33,770
So we often look for
a shallow network

219
00:13:33,770 --> 00:13:35,960
because if the features
are good enough, then

220
00:13:35,960 --> 00:13:42,760
we don't need to do
a lot of training

221
00:13:42,760 --> 00:13:47,000
on getting the class labels out.

222
00:13:47,000 --> 00:13:52,360
So this is self-supervised
learning in general.

223
00:13:52,360 --> 00:13:56,200
And although we are talking
about the computer vision

224
00:13:56,200 --> 00:13:57,480
applications.

225
00:13:57,480 --> 00:14:01,360
But this paradigm of
self-supervised learning

226
00:14:01,360 --> 00:14:07,080
was actually what enabled all
of these large language models.

227
00:14:07,080 --> 00:14:10,000
GPT-4 and all of
these frameworks

228
00:14:10,000 --> 00:14:18,440
are trained with mostly raw data
without any manual labeling.

229
00:14:18,440 --> 00:14:21,940
And not just in language
models, in speech,

230
00:14:21,940 --> 00:14:25,840
and these days,
quite a lot in robot

231
00:14:25,840 --> 00:14:29,640
and robotics and
reinforcement learning.

232
00:14:29,640 --> 00:14:34,390
Because when we don't
need any me labeling data,

233
00:14:34,390 --> 00:14:41,410
we can start capturing data,
raw data without any manual

234
00:14:41,410 --> 00:14:43,310
labeling and use
those for training.

235
00:14:43,310 --> 00:14:47,410
And that's why you see so many
self-driving cars in the Bay

236
00:14:47,410 --> 00:14:51,550
Area collecting data because
that's getting them the data,

237
00:14:51,550 --> 00:14:53,790
and they don't have to
really annotate the data,

238
00:14:53,790 --> 00:14:57,690
but they can still
train models based on.

239
00:14:57,690 --> 00:15:03,410
So with that, today's agenda
will cover some of these pretext

240
00:15:03,410 --> 00:15:06,990
tasks from image
transformations,

241
00:15:06,990 --> 00:15:08,570
and then I will
talk a little bit

242
00:15:08,570 --> 00:15:13,490
about a set of
algorithms that are

243
00:15:13,490 --> 00:15:19,330
around contrastive
representation learning that

244
00:15:19,330 --> 00:15:22,730
are slightly different from
these image transformation based

245
00:15:22,730 --> 00:15:26,450
pretext tasks but
have shown promise.

246
00:15:26,450 --> 00:15:30,450
So let's start with
the first part.

247
00:15:30,450 --> 00:15:36,460
And there we will cover
the tasks one by one.

248
00:15:36,460 --> 00:15:41,720
So I talked quite a lot about
rotation, predicting rotations.

249
00:15:41,720 --> 00:15:45,820
Let's see if we can
actually rotate the images

250
00:15:45,820 --> 00:15:54,300
with random or arbitrary
degrees and predict the rotation

251
00:15:54,300 --> 00:15:56,840
angle with a model.

252
00:15:56,840 --> 00:16:00,860
And our hypothesis here
is that the model, a model

253
00:16:00,860 --> 00:16:05,380
could recognize the correct
rotation of an object

254
00:16:05,380 --> 00:16:09,940
only if it has the common
sense visual common

255
00:16:09,940 --> 00:16:14,660
sense of what the object
should look like unperturbed.

256
00:16:14,660 --> 00:16:19,940
So these models mostly are
designed around this concept

257
00:16:19,940 --> 00:16:23,420
of visual common sense.

258
00:16:23,420 --> 00:16:28,700
And then if the model
is able to capture

259
00:16:28,700 --> 00:16:32,870
that it means that it is
also able to summarize

260
00:16:32,870 --> 00:16:40,050
the image, the entire image,
into a useful set of features,

261
00:16:40,050 --> 00:16:42,150
good features.

262
00:16:42,150 --> 00:16:48,590
This paper published in
2018 implemented this

263
00:16:48,590 --> 00:16:53,350
with just exploring four
different angles of 0,

264
00:16:53,350 --> 00:17:03,750
19, 90, 180, and 270, these
rotating images with one

265
00:17:03,750 --> 00:17:08,790
of these angles and then using
a convolutional neural network

266
00:17:08,790 --> 00:17:12,150
to basically predict
what each of these

267
00:17:12,150 --> 00:17:14,190
rotations the output is.

268
00:17:14,190 --> 00:17:17,210
And because they only created
four different outputs,

269
00:17:17,210 --> 00:17:19,510
this is a classification
task because it only

270
00:17:19,510 --> 00:17:21,230
has four different cases.

271
00:17:21,230 --> 00:17:23,990
It doesn't have to
predict the exact value

272
00:17:23,990 --> 00:17:26,089
of the angle, the degrees.

273
00:17:26,089 --> 00:17:31,280
But it's actually
just predicting

274
00:17:31,280 --> 00:17:38,016
one of these four
classes of 0, 1, 2, or 3.

275
00:17:38,016 --> 00:17:42,000
y0, 1, 2, or 3.

276
00:17:42,000 --> 00:17:49,040
So with that, the authors
were able to learn

277
00:17:49,040 --> 00:17:50,920
good representations.

278
00:17:50,920 --> 00:17:55,840
And with those
representations, they

279
00:17:55,840 --> 00:17:57,880
started training
the neural network

280
00:17:57,880 --> 00:18:00,640
at a downstream
application, basically fine

281
00:18:00,640 --> 00:18:07,520
tuning the neural network based
on fine tuning the encoders

282
00:18:07,520 --> 00:18:12,740
and also the classifier.

283
00:18:12,740 --> 00:18:19,560
Actually, in this case, they
froze first and second layers,

284
00:18:19,560 --> 00:18:23,560
and then they fine tuned
the last convolution

285
00:18:23,560 --> 00:18:25,060
layer and the linear layer.

286
00:18:25,060 --> 00:18:32,010
So it's not entirely fine
tuning the entire network,

287
00:18:32,010 --> 00:18:35,910
but they were able to
get very good results.

288
00:18:35,910 --> 00:18:39,890
This is on CIFAR10 data
set, one of the data

289
00:18:39,890 --> 00:18:42,330
sets that we've
talked about earlier.

290
00:18:42,330 --> 00:18:45,350
You see that when the
model is pre-trained,

291
00:18:45,350 --> 00:18:48,430
it starts with a good
accuracy to start with.

292
00:18:48,430 --> 00:18:52,650
So it means that it
already is in a good shape,

293
00:18:52,650 --> 00:18:57,330
and it's having a good
understanding of the objects,

294
00:18:57,330 --> 00:19:01,810
even in the very
first iterations.

295
00:19:01,810 --> 00:19:04,150
But if the task
is simple enough,

296
00:19:04,150 --> 00:19:09,250
CIFAR10 is actually not too
hard to train a model for.

297
00:19:09,250 --> 00:19:12,470
If the task is simple enough,
the supervised version,

298
00:19:12,470 --> 00:19:14,810
the fully supervised
version, and the one that

299
00:19:14,810 --> 00:19:17,510
starts with pre-training, often
converge to the same number,

300
00:19:17,510 --> 00:19:19,450
same accuracy.

301
00:19:19,450 --> 00:19:22,130
But again, if the
task is simple enough,

302
00:19:22,130 --> 00:19:25,130
in very hard application, in
much harder to applications,

303
00:19:25,130 --> 00:19:27,390
often the supervised learning
frameworks, if we don't

304
00:19:27,390 --> 00:19:29,590
do any pre-training,
large scale pre-training,

305
00:19:29,590 --> 00:19:31,270
we don't get as good results.

306
00:19:31,270 --> 00:19:34,270
OK.

307
00:19:34,270 --> 00:19:42,590
So they've done also
some applications,

308
00:19:42,590 --> 00:19:51,110
some experiments on this
PASCAL VOC 2017 data

309
00:19:51,110 --> 00:19:53,890
sets which involves
a number of tasks,

310
00:19:53,890 --> 00:19:59,510
including classification,
detection, and segmentation.

311
00:19:59,510 --> 00:20:03,910
And of these three
sets of tasks,

312
00:20:03,910 --> 00:20:08,470
they've used different
setups of just training

313
00:20:08,470 --> 00:20:10,870
a few fully connected
layers or all

314
00:20:10,870 --> 00:20:13,870
of the layers for the
classification, detection,

315
00:20:13,870 --> 00:20:16,550
and segmentation tasks.

316
00:20:16,550 --> 00:20:19,390
If you look at the
ImageNet labels,

317
00:20:19,390 --> 00:20:23,360
like if we have a
huge labeled data set,

318
00:20:23,360 --> 00:20:27,160
and we pre-train
on that data set.

319
00:20:27,160 --> 00:20:29,340
We already get a
very high accuracy.

320
00:20:29,340 --> 00:20:31,440
But again, have this
in mind that this

321
00:20:31,440 --> 00:20:34,080
is ImageNet with all
of the labels involved

322
00:20:34,080 --> 00:20:36,360
for the pre-training.

323
00:20:36,360 --> 00:20:40,880
But if we don't do any
supervised pre-training,

324
00:20:40,880 --> 00:20:44,140
and the pre-training is all
based on self-supervised,

325
00:20:44,140 --> 00:20:49,760
it's showing that this
rotation framework is

326
00:20:49,760 --> 00:20:54,780
doing a much better job than
many of the other counterparts,

327
00:20:54,780 --> 00:21:01,560
any of the other methods
that we don't actually

328
00:21:01,560 --> 00:21:04,540
go into many of the
detail of many of those,

329
00:21:04,540 --> 00:21:09,720
but it's showing efficacy for
this rotation pretext task.

330
00:21:09,720 --> 00:21:12,800
And see how it's different,
how much better it

331
00:21:12,800 --> 00:21:17,960
is if you start with a random
initialization of the weights.

332
00:21:17,960 --> 00:21:21,530
So the random initialization
versus this pretraining

333
00:21:21,530 --> 00:21:24,130
with rotation, pretext task.

334
00:21:24,130 --> 00:21:29,970
The difference is huge, and this
rotation pretext task is not

335
00:21:29,970 --> 00:21:33,610
equal, but it's
close to pre-training

336
00:21:33,610 --> 00:21:35,730
on the entire ImageNet.

337
00:21:35,730 --> 00:21:40,610
So one other thing
that they've looked

338
00:21:40,610 --> 00:21:43,410
into in this paper was
looking at the features

339
00:21:43,410 --> 00:21:48,850
and how the learned
features are meaningful.

340
00:21:48,850 --> 00:21:54,330
I mentioned earlier that one
of the ways of evaluating

341
00:21:54,330 --> 00:21:56,930
the pretext tasks,
generally self-supervised

342
00:21:56,930 --> 00:21:59,070
learning frameworks, is
to look at the features.

343
00:21:59,070 --> 00:22:01,730
And you can always
go from the features

344
00:22:01,730 --> 00:22:03,730
from the fully connected layers.

345
00:22:03,730 --> 00:22:07,770
We talked about grad cam and all
of those other attention-based

346
00:22:07,770 --> 00:22:10,690
frameworks, how we can
go from the features

347
00:22:10,690 --> 00:22:12,330
back to the image space.

348
00:22:12,330 --> 00:22:15,730
So this evaluation
involves projecting those

349
00:22:15,730 --> 00:22:19,380
into the image space and seeing
what the model is looking at.

350
00:22:19,380 --> 00:22:22,760
If you look at the attention
maps for the supervised model,

351
00:22:22,760 --> 00:22:25,180
often the supervised
model has more

352
00:22:25,180 --> 00:22:29,900
focused maps because
it's only trying

353
00:22:29,900 --> 00:22:33,080
to solve one single
task of classification.

354
00:22:33,080 --> 00:22:36,460
So if it captures the eye
and the shape around it,

355
00:22:36,460 --> 00:22:39,460
it doesn't care about the
other parts very much.

356
00:22:39,460 --> 00:22:42,640
But in cases of
self-supervised learning,

357
00:22:42,640 --> 00:22:45,940
often, more features,
more areas are

358
00:22:45,940 --> 00:22:50,580
covered because it has to have
a more holistic understanding

359
00:22:50,580 --> 00:22:52,340
of the image because
we don't know what

360
00:22:52,340 --> 00:22:55,460
the downstream task
is, but the goal

361
00:22:55,460 --> 00:23:00,060
is to perform equally
well in many of those.

362
00:23:00,060 --> 00:23:02,940
So that's one of the tasks.

363
00:23:02,940 --> 00:23:05,240
If you do have any
questions, keep it.

364
00:23:05,240 --> 00:23:08,540
I'll stop after going
over some of the tasks.

365
00:23:08,540 --> 00:23:10,980
And if your question
was not answered,

366
00:23:10,980 --> 00:23:13,940
then I would be
happy to answer that.

367
00:23:13,940 --> 00:23:16,710
OK.

368
00:23:16,710 --> 00:23:21,910
Another one, another
popular pretext task

369
00:23:21,910 --> 00:23:26,910
was to basically
create this 3 by 3 grid

370
00:23:26,910 --> 00:23:33,030
and then use networks to
predict the location of each

371
00:23:33,030 --> 00:23:36,310
of the given patches with
respect to the sensor patch.

372
00:23:36,310 --> 00:23:40,790
So for this patch,
which is from here,

373
00:23:40,790 --> 00:23:46,950
the output should be 3
because we only have 8--

374
00:23:46,950 --> 00:23:50,450
This is 3 by 3, and the
center patch is the reference.

375
00:23:50,450 --> 00:23:56,270
So this also turns out to be an
eight-way classification task.

376
00:23:56,270 --> 00:23:59,270
It's getting any
of these patches,

377
00:23:59,270 --> 00:24:04,630
and it's trying to output what
the location of that given patch

378
00:24:04,630 --> 00:24:11,300
is with respect to
the center patch.

380
00:24:16,200 --> 00:24:18,780
So this was another example.

381
00:24:18,780 --> 00:24:25,040
But this other follow-up
paper, paper publication,

382
00:24:25,040 --> 00:24:29,520
which turned this into a
jigsaw puzzle framework,

383
00:24:29,520 --> 00:24:34,480
was instead of asking
the model to just predict

384
00:24:34,480 --> 00:24:37,240
which of these
eight patches it is,

385
00:24:37,240 --> 00:24:41,060
it tried to predict
the exact permutation,

386
00:24:41,060 --> 00:24:42,020
the right permutation.

387
00:24:42,020 --> 00:24:46,160
So what they've done was they
used the same three by three

388
00:24:46,160 --> 00:24:50,960
grid, took all of the patches,
shuffled them randomly, and then

389
00:24:50,960 --> 00:24:59,080
ask the neural network
to say which one should

390
00:24:59,080 --> 00:25:01,480
be the correct permutation.

391
00:25:01,480 --> 00:25:04,277
So they basically predict
the correct permutations.

392
00:25:04,277 --> 00:25:06,360
Can you tell me what is
the number of permutations

393
00:25:06,360 --> 00:25:11,840
you can have for this setup?

394
00:25:11,840 --> 00:25:13,250
Say again?

395
00:25:13,250 --> 00:25:14,110
9 factorial.

396
00:25:14,110 --> 00:25:14,750
Yes, exactly.

397
00:25:14,750 --> 00:25:16,570
So it's a huge number, right?

398
00:25:16,570 --> 00:25:19,930
300,000 something, I think.

399
00:25:19,930 --> 00:25:22,410
But what they've
done was they've

400
00:25:22,410 --> 00:25:32,130
created this lookup table with
only 64 plausible, possible

401
00:25:32,130 --> 00:25:33,410
permutations.

402
00:25:33,410 --> 00:25:38,030
And then only they
consider 64 permutations.

403
00:25:38,030 --> 00:25:41,450
And then they're shuffling
that, they do the shuffling

404
00:25:41,450 --> 00:25:43,230
based on one of these 64.

405
00:25:43,230 --> 00:25:48,330
And then the output will also
be just the 60, 64 sized vector.

406
00:25:48,330 --> 00:25:52,050
So again, this turns out to be
just a simple classification

407
00:25:52,050 --> 00:25:56,090
task with 64 output classes.

408
00:25:56,090 --> 00:25:59,170
And they've shown this
is also a great idea

409
00:25:59,170 --> 00:26:04,470
for solving to define
as a pretext task,

410
00:26:04,470 --> 00:26:09,130
and on the same data set
with similar type of tasks

411
00:26:09,130 --> 00:26:14,110
that I talked about and how
the supervision is done.

412
00:26:14,110 --> 00:26:17,550
They've shown their
method was outperforming

413
00:26:17,550 --> 00:26:25,190
some of the more previous
models, previous frameworks.

414
00:26:25,190 --> 00:26:30,870
And again, remember that
this was published in 2016.

415
00:26:30,870 --> 00:26:36,590
So next pretext task
is just inpainting

416
00:26:36,590 --> 00:26:38,710
predicting what is missing.

417
00:26:38,710 --> 00:26:44,310
So what they have done here
was a simple masking strategy,

418
00:26:44,310 --> 00:26:47,590
you mask parts of the
image and then you

419
00:26:47,590 --> 00:26:51,230
ask the model to inpaint
those parts that are masked.

420
00:26:51,230 --> 00:26:57,010
So how it was done, simple
masking on the input image.

421
00:26:57,010 --> 00:27:00,170
But because we have
all of the images,

422
00:27:00,170 --> 00:27:02,790
we actually have
the desired output.

423
00:27:02,790 --> 00:27:06,610
So an encoder turns this
into a feature space,

424
00:27:06,610 --> 00:27:08,816
and then that feature
space is also,

425
00:27:08,816 --> 00:27:12,300
there are some fully connected
layers in the middle,

426
00:27:12,300 --> 00:27:14,840
and then there is a
decoder that decodes

427
00:27:14,840 --> 00:27:16,920
the parts that is missing.

428
00:27:16,920 --> 00:27:22,760
And the loss function
is comparing the output

429
00:27:22,760 --> 00:27:27,720
with what the ground truth was.

430
00:27:27,720 --> 00:27:32,680
And this is basically learning
to reconstruct the missing

431
00:27:32,680 --> 00:27:34,560
pixels.

432
00:27:34,560 --> 00:27:38,800
Again, we've talked about
autoencoders a couple of times

433
00:27:38,800 --> 00:27:43,360
before, and this is also some
form of an autoencoder that

434
00:27:43,360 --> 00:27:47,720
encodes the input image
into a representation

435
00:27:47,720 --> 00:27:51,860
that you want to
decode the output.

436
00:27:51,860 --> 00:27:57,600
But this autoencoder is trained
with a masking strategy, masking

437
00:27:57,600 --> 00:27:58,760
objective.

438
00:27:58,760 --> 00:28:03,080
So just to show
you some examples,

439
00:28:03,080 --> 00:28:07,930
the inpainting evaluations
are a little bit

440
00:28:07,930 --> 00:28:10,710
interesting and tricky because
when you want to inpaint,

441
00:28:10,710 --> 00:28:13,630
there are so many different ways
of inpainting this image, right?

442
00:28:13,630 --> 00:28:26,570
And we can say there is just one
output to do in this case here.

443
00:28:26,570 --> 00:28:28,770
Reconstruction-based
frameworks, earlier

444
00:28:28,770 --> 00:28:31,370
reconstruction-based
frameworks were actually

445
00:28:31,370 --> 00:28:42,410
creating a lot of fuzzy
and very smooth outputs.

446
00:28:42,410 --> 00:28:49,530
And that's why this paper
that I'm referring to here

447
00:28:49,530 --> 00:28:54,250
was actually using an
additional adversarial objective

448
00:28:54,250 --> 00:28:56,890
function, which I'm not going
to go into details because this

449
00:28:56,890 --> 00:29:00,950
is a topic of discussion for the
next lecture, generative models.

450
00:29:00,950 --> 00:29:05,730
But generally, how
these frameworks work,

451
00:29:05,730 --> 00:29:07,680
we have a reconstruction loss.

452
00:29:07,680 --> 00:29:09,580
And reconstruction
loss is basically

453
00:29:09,580 --> 00:29:13,620
calculating the difference
between the patch,

454
00:29:13,620 --> 00:29:18,660
between the image x and
the image after it's

455
00:29:18,660 --> 00:29:24,340
passed through the encoder.

456
00:29:24,340 --> 00:29:33,340
And then this is element
wise multiplication.

457
00:29:33,340 --> 00:29:35,740
And we also have this
mask here because we

458
00:29:35,740 --> 00:29:38,300
want to only calculate
the loss function,

459
00:29:38,300 --> 00:29:41,520
the objective function,
only on the masked area.

460
00:29:41,520 --> 00:29:45,180
So we do element wise
multiplication with the mask

461
00:29:45,180 --> 00:29:46,020
as well.

462
00:29:46,020 --> 00:29:51,460
And this basically gives
us the reconstruction loss

463
00:29:51,460 --> 00:29:55,800
for that part of the
mask that we had.

464
00:29:55,800 --> 00:29:59,260
So as I said, it's
also supplemented

465
00:29:59,260 --> 00:30:02,820
with an adversarial objective
adversarial learning loss

466
00:30:02,820 --> 00:30:09,070
function, which ensures the
images that are generated

467
00:30:09,070 --> 00:30:10,910
are real looking.

468
00:30:10,910 --> 00:30:17,350
So with that, they have
been able to improve

469
00:30:17,350 --> 00:30:24,830
the parts that are reconstructed
to look a little bit better.

470
00:30:24,830 --> 00:30:31,630
But again, details will be
discussed in the next lecture.

471
00:30:31,630 --> 00:30:35,990
So this reconstruction
framework was actually

472
00:30:35,990 --> 00:30:39,550
able to again
provide, and this ours

473
00:30:39,550 --> 00:30:43,190
is the same, provide
additional benefit when

474
00:30:43,190 --> 00:30:46,830
it's run on the same
classification, detection,

475
00:30:46,830 --> 00:30:52,710
and segmentation task on
same set of data sets.

476
00:30:52,710 --> 00:30:56,710
I will come back to this
reconstruction-based frameworks

477
00:30:56,710 --> 00:30:59,110
and masking in a
bit because it's

478
00:30:59,110 --> 00:31:05,440
one of the most used models or
pretext tasks that these days

479
00:31:05,440 --> 00:31:07,800
are used for pre-training.

480
00:31:07,800 --> 00:31:10,140
So I'll come back to this.

481
00:31:10,140 --> 00:31:15,520
But before that, let me
introduce this other pretext

482
00:31:15,520 --> 00:31:19,200
task of image coloring.

483
00:31:19,200 --> 00:31:24,400
And this is another very
simple framework setup

484
00:31:24,400 --> 00:31:30,023
that we turn a colored image
because our data set is mostly

485
00:31:30,023 --> 00:31:30,940
colored images, right?

486
00:31:30,940 --> 00:31:36,680
We turn that colored image
into its components or channels

487
00:31:36,680 --> 00:31:41,880
that separate the lightness,
the illumination from the color

488
00:31:41,880 --> 00:31:42,500
itself.

489
00:31:42,500 --> 00:31:44,560
There are several
color spaces if you've

490
00:31:44,560 --> 00:31:47,160
taken courses like
computer graphics

491
00:31:47,160 --> 00:31:51,060
or CS-131, other
computer vision class.

492
00:31:51,060 --> 00:31:54,900
You know that there are so
many different color spaces.

493
00:31:54,900 --> 00:31:57,960
Mostly, in computer
vision, we use RGB.

494
00:31:57,960 --> 00:32:02,130
But if you want to separate
lightness, illumination

495
00:32:02,130 --> 00:32:04,830
from color, there are
some other color spaces.

496
00:32:04,830 --> 00:32:08,330
For example, LAB
color space, LAB,

497
00:32:08,330 --> 00:32:12,930
is one of those color spaces
that separates lightness

498
00:32:12,930 --> 00:32:14,410
from color.

499
00:32:14,410 --> 00:32:18,170
So we have one channel for
lightness and two channels

500
00:32:18,170 --> 00:32:21,970
for defining the actual color.

501
00:32:21,970 --> 00:32:27,897
And if we add these two
all together L, A, and B,

502
00:32:27,897 --> 00:32:29,730
all three channels
together, we can actually

503
00:32:29,730 --> 00:32:31,850
get the colored image.

504
00:32:31,850 --> 00:32:34,770
So the pretext task
here is simple.

505
00:32:34,770 --> 00:32:40,690
Given the L channel, predict
the A and B channels, right?

506
00:32:40,690 --> 00:32:44,310
So again, we don't need to
do any manual annotation.

507
00:32:44,310 --> 00:32:47,530
It's already in the data.

508
00:32:47,530 --> 00:32:52,150
And this was extended
into other frameworks.

509
00:32:52,150 --> 00:32:56,730
Why should we only look at
like given L, predict A and B.

510
00:32:56,730 --> 00:32:58,860
We can also do the
reverse, right?

511
00:32:58,860 --> 00:33:03,180
And that led us to
split, something

512
00:33:03,180 --> 00:33:06,660
that we call a split
brain autoencoder, where

513
00:33:06,660 --> 00:33:12,300
the input image is split, is
basically turned into one.

514
00:33:12,300 --> 00:33:16,140
The L channel, lightness
channel, and the color channels.

515
00:33:16,140 --> 00:33:18,480
These two images,
this is one channel.

516
00:33:18,480 --> 00:33:20,340
This is two channels of color.

517
00:33:20,340 --> 00:33:24,120
And we train two functions,
two neural networks,

518
00:33:24,120 --> 00:33:26,780
sets of layers to
predict the other one.

519
00:33:26,780 --> 00:33:30,780
And then at the end, in order
to calculate the loss function

520
00:33:30,780 --> 00:33:34,300
and backprop, we
just merge these two

521
00:33:34,300 --> 00:33:39,600
to generate the actual
image and a loss an L2 loss.

522
00:33:39,600 --> 00:33:42,900
Any distance function
can help with training

523
00:33:42,900 --> 00:33:45,180
this neural network.

524
00:33:45,180 --> 00:33:49,560
In a more generic
framework or formulation,

525
00:33:49,560 --> 00:33:55,280
the idea is given one
channel or a set of channels,

526
00:33:55,280 --> 00:34:00,920
predict the others,
and do the same for X2.

527
00:34:00,920 --> 00:34:04,760
So sets of channels X1,
sets of channels X2.

528
00:34:04,760 --> 00:34:07,440
So given one we can predict
the other one, and these

529
00:34:07,440 --> 00:34:09,360
are the neural
networks for those.

530
00:34:09,360 --> 00:34:16,159
Merging them we'll get
the value the image

531
00:34:16,159 --> 00:34:18,800
and then loss function
would be simple.

532
00:34:18,800 --> 00:34:21,120
So if we have such
a framework we

533
00:34:21,120 --> 00:34:25,470
can run it on everything, not
just color and illumination.

534
00:34:25,470 --> 00:34:32,040
We can have data from some
of these RGBD sensors, those

535
00:34:32,040 --> 00:34:35,980
that have RGB channels
and depth channels,

536
00:34:35,980 --> 00:34:38,880
like for example,
connect and other sensors

537
00:34:38,880 --> 00:34:40,719
that they use in robotics.

538
00:34:40,719 --> 00:34:46,400
And given the RGB channel,
predict the depth and vice

539
00:34:46,400 --> 00:34:47,320
versa.

540
00:34:47,320 --> 00:34:51,000
And this was a very
successful downstream task

541
00:34:51,000 --> 00:34:54,800
that was used for
different applications.

542
00:34:54,800 --> 00:34:59,770
And as you can see this
model and this paper

543
00:34:59,770 --> 00:35:06,330
that I just the split brain and
the papers that the model that

544
00:35:06,330 --> 00:35:12,150
predicts colorizes the images
is those features themselves.

545
00:35:12,150 --> 00:35:14,330
They do have actually
a very good level

546
00:35:14,330 --> 00:35:19,110
of accuracy for predicting
the class labels.

547
00:35:19,110 --> 00:35:22,650
And you can see, there are
many different other frameworks

548
00:35:22,650 --> 00:35:29,250
that are used also in
terms of comparisons.

549
00:35:29,250 --> 00:35:32,290
Again, this is not as good
as supervised learning

550
00:35:32,290 --> 00:35:36,170
because there is no
label involved here

551
00:35:36,170 --> 00:35:40,450
and it's just based on
the learned features

552
00:35:40,450 --> 00:35:43,690
with concatenated
features out of F1 and F2.

553
00:35:43,690 --> 00:35:49,090
OK, so the image
coloring pretext task

554
00:35:49,090 --> 00:35:55,100
was actually very interesting
because now we could not only

555
00:35:55,100 --> 00:35:57,520
use it for pre-training
neural networks,

556
00:35:57,520 --> 00:35:59,600
but it was also
itself useful somehow,

557
00:35:59,600 --> 00:36:03,460
because now we could
colorize images that we

558
00:36:03,460 --> 00:36:05,560
don't have a colored version.

559
00:36:05,560 --> 00:36:09,900
So we could colorize
images and videos

560
00:36:09,900 --> 00:36:15,100
that we don't have a
colored version of those.

561
00:36:15,100 --> 00:36:18,420
And not only that, one of
the other interesting results

562
00:36:18,420 --> 00:36:21,220
that they've shown
in the paper was

563
00:36:21,220 --> 00:36:27,900
this image of Yosemite and the
Half Dome that they colorized.

564
00:36:27,900 --> 00:36:32,540
The interesting thing
that is seen in this image

565
00:36:32,540 --> 00:36:36,720
is the consistency
between the actual object,

566
00:36:36,720 --> 00:36:40,100
the Half Dome or trees, or
the bridge and its reflection

567
00:36:40,100 --> 00:36:41,420
in the water.

568
00:36:41,420 --> 00:36:44,700
So the model was also
able to understand

569
00:36:44,700 --> 00:36:47,380
that this reflection
should also somehow

570
00:36:47,380 --> 00:36:51,430
preserve the color based
on how it was trained

571
00:36:51,430 --> 00:36:53,430
on vast amounts of data.

572
00:36:53,430 --> 00:36:57,510
Again, keep in mind
that these models

573
00:36:57,510 --> 00:37:01,690
are all pre-large language,
large vision models.

574
00:37:01,690 --> 00:37:04,390
And they have been
trained on specific tasks.

575
00:37:04,390 --> 00:37:09,830
So they're not trained
for solving everything.

576
00:37:09,830 --> 00:37:14,250
So this could be actually
extended into video settings.

577
00:37:14,250 --> 00:37:17,590
Because now if we have a video,
we can have a reference frame

578
00:37:17,590 --> 00:37:21,270
that has the color and do the
coloring for the follow-up

579
00:37:21,270 --> 00:37:23,330
frames and how this is done.

580
00:37:23,330 --> 00:37:26,750
This is very simple
because with--

581
00:37:26,750 --> 00:37:32,350
this is also very useful because
with colorizing future frames

582
00:37:32,350 --> 00:37:34,510
in the video, what
we are doing is

583
00:37:34,510 --> 00:37:41,870
we basically try to track
pixels and objects in the video,

584
00:37:41,870 --> 00:37:44,390
and the model
implicitly learns how

585
00:37:44,390 --> 00:37:47,570
these tracks should be formed.

586
00:37:47,570 --> 00:37:51,040
So the hypothesis is learning
to color video frames

587
00:37:51,040 --> 00:37:54,200
should allow a model to
learn to track regions

588
00:37:54,200 --> 00:37:56,610
or objects without labels.

590
00:37:59,720 --> 00:38:02,600
And learning to color
videos because there

591
00:38:02,600 --> 00:38:06,740
are a lot of correspondences is
an interesting task by itself.

592
00:38:06,740 --> 00:38:10,660
I would suggest taking
a look at the details.

593
00:38:10,660 --> 00:38:13,760
I'll very briefly
to talk about them.

594
00:38:13,760 --> 00:38:16,960
So if you have a
reference frame, what

595
00:38:16,960 --> 00:38:20,180
we need to do is for
coloring the input frame,

596
00:38:20,180 --> 00:38:23,360
we need to find the
pointers of where that's

597
00:38:23,360 --> 00:38:26,520
a specific object or pixel is.

598
00:38:26,520 --> 00:38:28,680
And then based on that
see what the color

599
00:38:28,680 --> 00:38:31,760
is and copy what the
color is as the color

600
00:38:31,760 --> 00:38:36,280
for that pixel as the
output, as the target color.

601
00:38:36,280 --> 00:38:39,600
And how this is done
is very much similar

602
00:38:39,600 --> 00:38:43,680
to the same topic of attention
that we talked about.

603
00:38:43,680 --> 00:38:48,130
So it's about forming
attention for each

604
00:38:48,130 --> 00:38:51,730
of the pixels for each
input frame, sorry reference

605
00:38:51,730 --> 00:38:53,610
frame and target frame.

606
00:38:53,610 --> 00:38:58,450
We often run a CNN to see what
features around those pixels

607
00:38:58,450 --> 00:38:59,550
should be used.

608
00:38:59,550 --> 00:39:02,970
And using those features,
now we can calculate,

609
00:39:02,970 --> 00:39:05,330
for each of the
target pixels, we

610
00:39:05,330 --> 00:39:09,370
can calculate the
attention or the distance

611
00:39:09,370 --> 00:39:13,970
to all of the frames, all of the
pixels in the reference frame.

612
00:39:13,970 --> 00:39:18,530
And then after defining
this attention with respect

613
00:39:18,530 --> 00:39:25,930
to the pixel of
interest in the target

614
00:39:25,930 --> 00:39:31,510
frame with all of the pixels
in the reference frame.

615
00:39:31,510 --> 00:39:37,050
Now we can do an average
color of all of based

616
00:39:37,050 --> 00:39:38,830
on those attention modules.

617
00:39:38,830 --> 00:39:42,170
So attention is basically just
similarity between the two.

618
00:39:42,170 --> 00:39:45,130
So anyways with
that, what we can do

619
00:39:45,130 --> 00:39:48,180
is we can just get
the output color

620
00:39:48,180 --> 00:39:55,300
as an average with that tension
and then ultimately calculate

621
00:39:55,300 --> 00:40:00,360
the loss function because we
have the values of the colors,

622
00:40:00,360 --> 00:40:04,580
the right colors of
those pixels in our data.

623
00:40:04,580 --> 00:40:08,220
And this was able to,
with this reference frame,

624
00:40:08,220 --> 00:40:10,280
colorize the images.

625
00:40:10,280 --> 00:40:15,480
You see how consistent this
becomes in terms of coloring.

626
00:40:15,480 --> 00:40:18,620
If we color them separately
without this consistency

627
00:40:18,620 --> 00:40:25,100
over time, you often see, for
example, the person's shirt

628
00:40:25,100 --> 00:40:32,180
or clothing changes color
because there is no constraint

629
00:40:32,180 --> 00:40:34,220
to keep it consistent.

630
00:40:34,220 --> 00:40:38,780
And then there has been
also some very interesting

631
00:40:38,780 --> 00:40:42,500
applications, because now that
you are calculating attention

632
00:40:42,500 --> 00:40:44,750
to a reference frame,
you're actually

633
00:40:44,750 --> 00:40:48,670
able to track objects,
track segments in videos,

634
00:40:48,670 --> 00:40:54,710
and even identify key
points in the videos.

635
00:40:54,710 --> 00:40:56,010
That's a good question.

636
00:40:56,010 --> 00:41:03,030
You're asking your question
is about this slide basically,

637
00:41:03,030 --> 00:41:09,270
and how the encoder knows
about the data to begin with

638
00:41:09,270 --> 00:41:13,450
and gets us good
learned representations.

639
00:41:13,450 --> 00:41:18,870
So all of these tasks that
I presented and defined

640
00:41:18,870 --> 00:41:26,030
are trying to do something here
at either decoding, classifying,

641
00:41:26,030 --> 00:41:30,950
or using regression to
generate some outputs to be

642
00:41:30,950 --> 00:41:32,730
able to train this encoder.

643
00:41:32,730 --> 00:41:34,830
So if your original
images, if these

644
00:41:34,830 --> 00:41:37,990
are all natural images taken
off of internet or ImageNet

645
00:41:37,990 --> 00:41:41,350
or whatever, then you
are learning an encoder

646
00:41:41,350 --> 00:41:45,730
that can extract features
from those types of images

647
00:41:45,730 --> 00:41:47,330
with the pretext task.

648
00:41:47,330 --> 00:41:50,410
And then when you
remove the decoder

649
00:41:50,410 --> 00:41:52,990
and add this
classifier to the end,

650
00:41:52,990 --> 00:41:56,310
you only need to
train this part,

651
00:41:56,310 --> 00:41:58,930
because this encoder was
already trained with all

652
00:41:58,930 --> 00:42:02,467
of these pre-training tasks
that I just talked about.

653
00:42:02,467 --> 00:42:04,050
You're asking if the
labels are coming

654
00:42:04,050 --> 00:42:07,290
from the decoder for
pre-training the encoder,

655
00:42:07,290 --> 00:42:09,950
and the answer to that is, yes.

656
00:42:09,950 --> 00:42:12,550
That's why we define
the pretext tasks,

657
00:42:12,550 --> 00:42:16,030
because we want to have
some labels, some outputs.

658
00:42:16,030 --> 00:42:20,130
And then based on
those outputs we try

659
00:42:20,130 --> 00:42:23,270
to train this entire network.

660
00:42:23,270 --> 00:42:28,490
And on the way of
predicting the right labels,

661
00:42:28,490 --> 00:42:31,290
this encoder is also trained.

662
00:42:31,290 --> 00:42:32,070
Good question.

663
00:42:32,070 --> 00:42:34,690
You're asking if encoder and
decoder are one big neural

664
00:42:34,690 --> 00:42:37,610
network or there is differences.

665
00:42:37,610 --> 00:42:41,120
In different papers,
in different works,

666
00:42:41,120 --> 00:42:44,380
it has been
completely different.

667
00:42:44,380 --> 00:42:47,800
In some cases, your encoder
and not just decoder,

668
00:42:47,800 --> 00:42:49,700
that's why I'm
calling it classifier.

669
00:42:49,700 --> 00:42:54,940
In the example that I showed
you about predicting the degree,

670
00:42:54,940 --> 00:42:57,060
this is just a simple
neural network.

671
00:42:57,060 --> 00:42:59,680
The decoder is these FC layers.

672
00:42:59,680 --> 00:43:02,700
So this could be
one entire network.

673
00:43:02,700 --> 00:43:04,980
And then you're replacing
it with something

674
00:43:04,980 --> 00:43:06,580
for your downstream task.

675
00:43:06,580 --> 00:43:09,580
But in some cases, for example,
when I talked about auto

676
00:43:09,580 --> 00:43:14,500
encoding an image and then
decoding another image,

677
00:43:14,500 --> 00:43:16,620
you often have two
neural networks

678
00:43:16,620 --> 00:43:19,580
that are trained
end-to-end because you

679
00:43:19,580 --> 00:43:21,500
want to make use of that
representation space

680
00:43:21,500 --> 00:43:22,660
in the middle.

681
00:43:22,660 --> 00:43:26,200
And in the next thing
that I want to talk about,

682
00:43:26,200 --> 00:43:31,260
masked autoencoders, even there
is no symmetry between encoders

683
00:43:31,260 --> 00:43:32,320
and decoders.

684
00:43:32,320 --> 00:43:35,660
They can be just two
different frameworks,

685
00:43:35,660 --> 00:43:39,590
two different neural networks,
even without any symmetry

686
00:43:39,590 --> 00:43:43,330
to train the task.

687
00:43:43,330 --> 00:43:45,490
So this is very
much task dependent,

688
00:43:45,490 --> 00:43:47,150
pretext task dependent.

689
00:43:47,150 --> 00:43:51,470
But there could be belonging
to the same architecture

690
00:43:51,470 --> 00:43:55,230
that we know about,
say, CNN or ResNet,

691
00:43:55,230 --> 00:43:58,310
or there could be two
different architectures, even

692
00:43:58,310 --> 00:44:00,390
without any symmetry.

693
00:44:00,390 --> 00:44:05,990
Remember that these are
the very first methods

694
00:44:05,990 --> 00:44:08,270
for self-supervised learning.

695
00:44:08,270 --> 00:44:11,070
So they're not supposed
to be solving everything.

696
00:44:11,070 --> 00:44:15,070
That's just a quick
disclaimer, but the idea

697
00:44:15,070 --> 00:44:17,230
is, the hypothesis here
is, if the model is

698
00:44:17,230 --> 00:44:20,250
able to say this is
90 degree rotated,

699
00:44:20,250 --> 00:44:22,790
it means that implicitly,
it's understanding

700
00:44:22,790 --> 00:44:28,970
the right rotation, sorry,
right orientation and direction.

701
00:44:28,970 --> 00:44:33,870
And then it will
be able to if given

702
00:44:33,870 --> 00:44:39,360
a right an unrotated image,
it's able to recognize

703
00:44:39,360 --> 00:44:42,260
what it is in it.

704
00:44:42,260 --> 00:44:46,440
But this is a limited task
by itself, I agree with that.

705
00:44:46,440 --> 00:44:49,240
The question that you have
is why here they use 64.

707
00:44:52,680 --> 00:44:54,480
That's a good question.

708
00:44:54,480 --> 00:44:59,000
But that's also an arbitrary
choice, almost arbitrary choice.

709
00:44:59,000 --> 00:45:01,520
As I said, there are
many different types

710
00:45:01,520 --> 00:45:05,920
of different number of
permutations here 9 factorial.

711
00:45:05,920 --> 00:45:07,520
So it's a very big number.

712
00:45:07,520 --> 00:45:10,620
It doesn't make sense for us
to be predicting all of those.

713
00:45:10,620 --> 00:45:12,800
What the authors did
here, they decided

714
00:45:12,800 --> 00:45:16,400
to select a few of
those perturbations

715
00:45:16,400 --> 00:45:19,080
that there is enough
variation because many

716
00:45:19,080 --> 00:45:24,420
of those perturbations is just
like one patch switched only.

717
00:45:24,420 --> 00:45:30,200
So they selected 64 of those
that they have the largest

718
00:45:30,200 --> 00:45:31,660
difference distance
between them,

719
00:45:31,660 --> 00:45:33,840
and they just selected
64 because they wanted

720
00:45:33,840 --> 00:45:36,290
to solve the classification
problem instead

721
00:45:36,290 --> 00:45:38,960
of other types of tasks.

723
00:45:41,570 --> 00:45:42,410
OK.

724
00:45:42,410 --> 00:45:47,730
So I've been talking about
these frameworks that

725
00:45:47,730 --> 00:45:54,130
often do some transformation
on the image or the videos

726
00:45:54,130 --> 00:45:55,070
and so on.

727
00:45:55,070 --> 00:45:58,630
And this brings us to
this newer framework,

728
00:45:58,630 --> 00:46:00,950
which is published in 2021.

729
00:46:00,950 --> 00:46:05,250
And then there has been so
many follow-ups on this,

730
00:46:05,250 --> 00:46:11,670
and it's been a great framework
for pre-training for many tasks,

731
00:46:11,670 --> 00:46:14,250
even if these days when
we want to pre-train

732
00:46:14,250 --> 00:46:21,850
on a data set on raw data sets,
we often use this MAE framework.

733
00:46:21,850 --> 00:46:24,410
It's called Masked
Auto Encoders.

734
00:46:24,410 --> 00:46:27,610
It is also reconstruction
based framework

735
00:46:27,610 --> 00:46:30,670
similar to that masking strategy
in painting that I mentioned,

736
00:46:30,670 --> 00:46:34,900
but this is far more detailed.

737
00:46:34,900 --> 00:46:39,100
And as you can
see, this framework

738
00:46:39,100 --> 00:46:41,680
is not just selecting one mask.

739
00:46:41,680 --> 00:46:44,460
There are so many different
patches and places

740
00:46:44,460 --> 00:46:48,940
that they do masking with
even more aggressive sampling

741
00:46:48,940 --> 00:46:54,900
rates, 50% masking or 75
masking rates and ratios.

742
00:46:54,900 --> 00:46:58,960
And through training
in larger scale,

743
00:46:58,960 --> 00:47:00,900
they have shown
that not only they

744
00:47:00,900 --> 00:47:05,140
are able to reconstruct
all of those masked areas,

745
00:47:05,140 --> 00:47:11,220
they are also getting very good
encoders that can summarize

746
00:47:11,220 --> 00:47:14,340
the images into good features.

747
00:47:14,340 --> 00:47:20,340
And how this was done
was through defining

748
00:47:20,340 --> 00:47:22,358
a encoder and a decoder.

749
00:47:22,358 --> 00:47:23,900
And that's one of
the examples that I

750
00:47:23,900 --> 00:47:26,700
said these are not symmetric.

751
00:47:26,700 --> 00:47:30,900
And encoders and decoder,
so a large portion

752
00:47:30,900 --> 00:47:32,750
of the input masks.

753
00:47:32,750 --> 00:47:37,110
Input patches are masked and
the ones that are not masked,

754
00:47:37,110 --> 00:47:43,470
they are given to the encoder
to encode in two features that

755
00:47:43,470 --> 00:47:49,430
are then passed through decoder
to generate the complete image.

756
00:47:49,430 --> 00:47:53,890
But let's go a little bit into
details of what this means.

757
00:47:53,890 --> 00:47:56,510
I do have some of
the details on how

758
00:47:56,510 --> 00:47:59,030
these models are trained here.

759
00:47:59,030 --> 00:48:04,590
But I very briefly
just explained to you

760
00:48:04,590 --> 00:48:07,750
how these models often work.

761
00:48:07,750 --> 00:48:13,350
The encoder here is very
much similar to ViTs.

762
00:48:13,350 --> 00:48:16,990
All of them are based on
transformers, the ViTs

763
00:48:16,990 --> 00:48:18,870
that we've talked about.

764
00:48:18,870 --> 00:48:23,090
Similar to the ViTs, the
images are split into patches.

765
00:48:23,090 --> 00:48:29,560
The patches are then
sampled, so uniform sampling

766
00:48:29,560 --> 00:48:32,960
is what they've done
and they've shown 75%

767
00:48:32,960 --> 00:48:40,240
sampling was quite efficient
in the experiments.

768
00:48:40,240 --> 00:48:45,520
And they use a
high masking ratio,

769
00:48:45,520 --> 00:48:50,440
and then this makes the
prediction task very challenging

770
00:48:50,440 --> 00:48:54,040
and challenging in
the set of pretext

771
00:48:54,040 --> 00:48:56,880
for pretext tasks in
self-supervised learning

772
00:48:56,880 --> 00:48:58,700
means the task is meaningful.

773
00:48:58,700 --> 00:49:00,640
The task is very good
because the model

774
00:49:00,640 --> 00:49:05,560
has to learn good features
to be able to reconstruct it.

775
00:49:05,560 --> 00:49:09,960
So with that high
sampling, high ratio

776
00:49:09,960 --> 00:49:14,040
of sampling, what it can
do is they can actually

777
00:49:14,040 --> 00:49:17,280
augment the data by a lot
too, because each time you

778
00:49:17,280 --> 00:49:20,840
mask 75% of the data, so
you can reuse the same image

779
00:49:20,840 --> 00:49:23,220
multiple and multiple
times during training.

780
00:49:23,220 --> 00:49:29,620
So you will have so much of
data to train this encoder with.

781
00:49:29,620 --> 00:49:36,460
And then that's why they use
a huge encoder, a large ViT

782
00:49:36,460 --> 00:49:38,040
in as their encoder.

783
00:49:38,040 --> 00:49:43,320
So this encoder itself only
sees 25% of the samples.

784
00:49:43,320 --> 00:49:49,620
The patches embeds those two
with first linear projection

785
00:49:49,620 --> 00:49:51,320
to some embedding spaces.

786
00:49:51,320 --> 00:49:55,420
Then positional embeddings
are added exactly same

787
00:49:55,420 --> 00:49:59,700
as what we mentioned for ViTs.

788
00:49:59,700 --> 00:50:02,140
And all of these are
transformer blocks.

789
00:50:02,140 --> 00:50:09,600
And the encoder is very large,
that's what I just mentioned.

790
00:50:09,600 --> 00:50:13,500
And then when it comes
to the decoding part,

791
00:50:13,500 --> 00:50:17,060
so we have the embeddings
of all of those patches

792
00:50:17,060 --> 00:50:21,820
that were present for the
patches that were masked

793
00:50:21,820 --> 00:50:23,260
or were missing.

794
00:50:23,260 --> 00:50:26,530
For those, there is a
trainable parameter,

795
00:50:26,530 --> 00:50:29,230
very much similar to that
class token that we had.

796
00:50:29,230 --> 00:50:34,270
A shared masked masked
token that is basically

797
00:50:34,270 --> 00:50:36,990
in some sort of average.

798
00:50:36,990 --> 00:50:39,390
We can consider that
as an average patch,

799
00:50:39,390 --> 00:50:43,750
an average representation
that is put for the ones that

800
00:50:43,750 --> 00:50:48,950
are missing are masked, and then
the decoder has to transfer them

801
00:50:48,950 --> 00:50:54,270
to the image patches
of the entire image,

802
00:50:54,270 --> 00:51:02,190
and then the entire image is
the output target is the output.

803
00:51:02,190 --> 00:51:06,430
So how we train this,
this is a simple MSE

804
00:51:06,430 --> 00:51:16,150
based mean squared error loss
function for between the image

805
00:51:16,150 --> 00:51:18,790
and the reconstructed
image, the loss function

806
00:51:18,790 --> 00:51:21,830
is only computed for
the masked patches

807
00:51:21,830 --> 00:51:25,960
similar to the previous one
that I just talked about.

808
00:51:25,960 --> 00:51:30,960
And then what it has is--

809
00:51:30,960 --> 00:51:33,520
When we do the training,
they've shown in the paper

810
00:51:33,520 --> 00:51:42,192
that you can do either linear
probing or full fine tuning

811
00:51:42,192 --> 00:51:44,900
to use it for your
downstream tasks,

812
00:51:44,900 --> 00:51:47,560
for any of the applications
that you have in mind.

813
00:51:47,560 --> 00:51:50,680
And in linear
probing, what happens

814
00:51:50,680 --> 00:51:55,000
is you often have your
encoder frozen and then use

815
00:51:55,000 --> 00:51:58,800
the learned representations and
only learn a linear function

816
00:51:58,800 --> 00:52:00,740
for the final task.

817
00:52:00,740 --> 00:52:04,240
And this mark, it means
that it's being trained,

818
00:52:04,240 --> 00:52:06,460
but in full fine
tuning, in fine tuning,

819
00:52:06,460 --> 00:52:08,320
the pre-trained
encoders are also

820
00:52:08,320 --> 00:52:14,560
fine tuned, either all of them
or just few transformer blocks.

821
00:52:14,560 --> 00:52:19,420
And that's the fine
tuning framework.

822
00:52:19,420 --> 00:52:22,570
The linear probing
provides a measure

823
00:52:22,570 --> 00:52:25,890
of representation quality,
how those representations

824
00:52:25,890 --> 00:52:31,890
features are, and fine
tuning always exploits models

825
00:52:31,890 --> 00:52:37,250
near to potential to
adapt for new tasks.

826
00:52:37,250 --> 00:52:43,250
OK, so if you're
interested in this topic

827
00:52:43,250 --> 00:52:45,310
and if you're planning
to use this paper,

828
00:52:45,310 --> 00:52:49,030
I highly advise looking at
the paper in its follow-ups.

829
00:52:49,030 --> 00:52:53,210
There are so many discussions
around different aspects

830
00:52:53,210 --> 00:52:56,490
model choices,
hyperparameters, masking ratio

831
00:52:56,490 --> 00:53:00,450
is one thing they've shown with
the masking ratio that the 75%

832
00:53:00,450 --> 00:53:04,150
is actually giving them
a very high accuracy.

833
00:53:04,150 --> 00:53:07,810
So 75% that's the reason
that it was chosen.

834
00:53:07,810 --> 00:53:13,530
Decoder depth, decoder width,
mask tokens, reconstruction

835
00:53:13,530 --> 00:53:18,530
targets, data augmentation
how it's helpful

836
00:53:18,530 --> 00:53:22,160
and mask sampling method I'm
showing the results here again.

837
00:53:22,160 --> 00:53:25,180
Mask sampling method
is mostly around

838
00:53:25,180 --> 00:53:30,500
should they use some random
masking blocks or grid

839
00:53:30,500 --> 00:53:31,520
type of masking.

840
00:53:31,520 --> 00:53:35,660
You see the examples here, and
they've came to the conclusion

841
00:53:35,660 --> 00:53:39,620
that this random masking
was the best choice.

842
00:53:39,620 --> 00:53:43,500
And finally they've
been able to show

843
00:53:43,500 --> 00:53:49,340
that MAE is doing a
much better job compared

844
00:53:49,340 --> 00:53:53,960
to many of the other
methods that were used.

845
00:53:53,960 --> 00:53:56,460
So some of the other
state of the art methods

846
00:53:56,460 --> 00:53:58,760
were actually Dino and Moco V3.

847
00:53:58,760 --> 00:54:00,980
If you have time, I will
briefly go over them.

848
00:54:00,980 --> 00:54:06,540
But this framework was
actually outperforming

849
00:54:06,540 --> 00:54:13,660
those that are more at the
time, advanced frameworks

850
00:54:13,660 --> 00:54:16,220
of contrastive learning.

851
00:54:16,220 --> 00:54:22,850
So I'll stop for a few
questions if you have any.

852
00:54:22,850 --> 00:54:28,050
Let me just summarize
what we've talked about.

853
00:54:28,050 --> 00:54:30,970
Pretext tasks were
actually very important.

854
00:54:30,970 --> 00:54:33,910
And as I said their
focus is on understanding

855
00:54:33,910 --> 00:54:36,230
the visual common sense.

856
00:54:36,230 --> 00:54:40,630
And one of the things
that also related

857
00:54:40,630 --> 00:54:44,150
to some of the questions
that were asked, we can see

858
00:54:44,150 --> 00:54:48,790
is coming up with an individual
pretext task is often

859
00:54:48,790 --> 00:54:52,270
challenging because the
learned representations may not

860
00:54:52,270 --> 00:54:58,190
be general enough because of the
type of tasks that you define.

861
00:54:58,190 --> 00:55:05,170
For example, if you're using
completion, rotation prediction,

862
00:55:05,170 --> 00:55:08,070
or jigsaw puzzle
or colorization,

863
00:55:08,070 --> 00:55:11,670
you learned representations
are good for solving

864
00:55:11,670 --> 00:55:14,030
those specific tasks,
and they may not

865
00:55:14,030 --> 00:55:19,680
be very good for
general pretext tasks.

866
00:55:19,680 --> 00:55:22,420
So the question is in
split brain autoencoder,

867
00:55:22,420 --> 00:55:28,880
how does the model know how to
predict the other channel given

868
00:55:28,880 --> 00:55:33,340
for example L channel,
the illumination channel,

869
00:55:33,340 --> 00:55:34,720
lightness channel.

870
00:55:34,720 --> 00:55:39,040
So let me answer your
question with a question.

871
00:55:39,040 --> 00:55:43,000
When you're training a model
to predict classes of objects

872
00:55:43,000 --> 00:55:45,400
in the image, how
does the encoder

873
00:55:45,400 --> 00:55:51,880
know what features to extract
to predict the class of models?

874
00:55:51,880 --> 00:55:55,960
Labeled data, because what
you're doing is you're

875
00:55:55,960 --> 00:55:59,280
back propagating a
loss value that is

876
00:55:59,280 --> 00:56:02,280
calculated around those labels.

877
00:56:02,280 --> 00:56:03,660
It's the same story here.

878
00:56:03,660 --> 00:56:07,760
We define the network that
takes one of the channels

879
00:56:07,760 --> 00:56:09,480
and outputs the other channel.

880
00:56:09,480 --> 00:56:13,040
How this was trained
was by back propagating

881
00:56:13,040 --> 00:56:15,060
what the output should be.

882
00:56:15,060 --> 00:56:17,400
The output was
the other channel.

883
00:56:17,400 --> 00:56:20,020
We do have the other
channel in the data.

884
00:56:20,020 --> 00:56:23,700
So instead of defining the
task being a classification

885
00:56:23,700 --> 00:56:26,460
of predicting the
class of the objects,

886
00:56:26,460 --> 00:56:28,900
here we define the
task to be predicting

887
00:56:28,900 --> 00:56:32,100
the color of the pixels.

888
00:56:32,100 --> 00:56:33,740
And the color of the
pixels, we already

889
00:56:33,740 --> 00:56:36,980
have them in the data set,
so the loss function still

890
00:56:36,980 --> 00:56:40,220
can be calculated
and back propagated.

891
00:56:40,220 --> 00:56:42,540
So the question is
how these outputs are

892
00:56:42,540 --> 00:56:44,540
used as input to the decoder.

893
00:56:44,540 --> 00:56:51,020
So this is, again, a VIT
transformer style framework

894
00:56:51,020 --> 00:56:58,660
encoder that turns every input
to a token as the output that

895
00:56:58,660 --> 00:57:02,700
is representation of that
specific input patch.

896
00:57:02,700 --> 00:57:04,700
So we've talked about this.

897
00:57:04,700 --> 00:57:10,040
But then we know that this is
not the list of all patches,

898
00:57:10,040 --> 00:57:12,740
there are some of the
patches that are masked.

899
00:57:12,740 --> 00:57:15,830
For those that are
masked, we also

900
00:57:15,830 --> 00:57:20,350
train this encoder
outputs a shared mask

901
00:57:20,350 --> 00:57:25,490
token, a token that is basically
an average token, for example.

902
00:57:25,490 --> 00:57:27,110
It's a learnable parameter.

903
00:57:27,110 --> 00:57:29,290
We can't necessarily
interpret it,

904
00:57:29,290 --> 00:57:31,510
but we can say it's
probably something

905
00:57:31,510 --> 00:57:35,670
that is similar to a mask
like an average token.

906
00:57:35,670 --> 00:57:40,990
So that's shared mask token
is put in the place of those

907
00:57:40,990 --> 00:57:42,130
that are missing.

908
00:57:42,130 --> 00:57:46,230
And then this long vector,
long sequence is created.

909
00:57:46,230 --> 00:57:49,550
Decoder, another
transformer framework

910
00:57:49,550 --> 00:57:55,990
takes this long set of
tokens and outputs those

911
00:57:55,990 --> 00:57:58,540
that are projected as
the output pixel value.

913
00:58:01,790 --> 00:58:12,600
Perfect, so we only have 15
minutes and a lot of things

914
00:58:12,600 --> 00:58:14,880
to cover.

915
00:58:14,880 --> 00:58:21,300
But I think what I wanted
to get out of this session,

916
00:58:21,300 --> 00:58:24,800
was for you to understand
what pretext tasks are,

917
00:58:24,800 --> 00:58:26,020
and how we define them.

918
00:58:26,020 --> 00:58:29,400
And one of the most
used frameworks

919
00:58:29,400 --> 00:58:32,720
right now is the masked
autoencoder, which we actually

920
00:58:32,720 --> 00:58:36,120
covered to some extent.

921
00:58:36,120 --> 00:58:42,200
But anyways, I was
here that we did look

922
00:58:42,200 --> 00:58:45,040
at these transformations,
and then we

923
00:58:45,040 --> 00:58:49,280
know that all of
these transformations

924
00:58:49,280 --> 00:58:52,940
are actually the same object
as the original image,

925
00:58:52,940 --> 00:58:54,540
just in a different form.

926
00:58:54,540 --> 00:58:59,360
But then we also have the
knowledge that in the data set,

927
00:58:59,360 --> 00:59:02,660
we have other objects that
look completely different.

928
00:59:02,660 --> 00:59:08,720
So if I define a
task that can say

929
00:59:08,720 --> 00:59:11,990
these that belong to the
same object, same pixel,

930
00:59:11,990 --> 00:59:15,510
try to bring them close in
the representation space,

931
00:59:15,510 --> 00:59:18,210
basically attract them to each
other in the representation

932
00:59:18,210 --> 00:59:21,890
space, and those that are not--

933
00:59:21,890 --> 00:59:27,930
that they do not belong
to the same object,

934
00:59:27,930 --> 00:59:30,610
try to maximize the
distance between them

935
00:59:30,610 --> 00:59:34,650
in the latent space, basically
repel the representation

936
00:59:34,650 --> 00:59:37,130
of these two images.

937
00:59:37,130 --> 00:59:40,530
Then this is another
task that is often

938
00:59:40,530 --> 00:59:44,330
referred to as contrastive
learning, contrastive

939
00:59:44,330 --> 00:59:46,290
representation
learning, and there

940
00:59:46,290 --> 00:59:52,530
are quite a number of very
interesting methods to look at.

941
00:59:52,530 --> 00:59:56,830
We have sampled there are so
many papers in this space,

942
00:59:56,830 --> 01:00:04,530
especially around
2019's to 2020's, and so

943
01:00:04,530 --> 01:00:07,010
on around those years.

944
01:00:07,010 --> 01:00:10,840
Sinclair McCaw, CBC, and
then ultimately dyno,

945
01:00:10,840 --> 01:00:13,900
which actually borrows concepts
from contrastive learning

946
01:00:13,900 --> 01:00:16,440
but is not necessarily
contrastive learning framework.

948
01:00:19,060 --> 01:00:26,940
And what we do there is in order
to define, attract and repel

949
01:00:26,940 --> 01:00:33,260
functions, characteristics
or regularize the model based

950
01:00:33,260 --> 01:00:38,740
on those, we define the
reference image as x, and then

951
01:00:38,740 --> 01:00:40,500
all of those that
are transformations

952
01:00:40,500 --> 01:00:44,140
of the same as positive
samples and all

953
01:00:44,140 --> 01:00:46,540
of the other objects
in the data set,

954
01:00:46,540 --> 01:00:49,940
or in the batch as
negative samples.

955
01:00:49,940 --> 01:00:52,300
And these positive
and negative samples

956
01:00:52,300 --> 01:00:55,260
will basically
define a way for us

957
01:00:55,260 --> 01:00:58,500
to calculate the loss function.

958
01:00:58,500 --> 01:01:00,740
How can we do that?

959
01:01:00,740 --> 01:01:02,720
Assume we have a
scoring function.

960
01:01:02,720 --> 01:01:04,420
We want to get a
scoring function that

961
01:01:04,420 --> 01:01:09,290
says the score for the
reference image encoded version,

962
01:01:09,290 --> 01:01:11,550
features of the reference
image and the features

963
01:01:11,550 --> 01:01:15,810
of a positive sample should be
larger than the score that is,

964
01:01:15,810 --> 01:01:24,150
comparing the reference image
and the negative samples.

965
01:01:24,150 --> 01:01:30,910
So with this type of
scoring function, if we--

966
01:01:30,910 --> 01:01:35,110
so basically to define this
type of scoring function,

967
01:01:35,110 --> 01:01:38,370
we define a loss
function based on that.

968
01:01:38,370 --> 01:01:43,750
After training the
scoring function S,

969
01:01:43,750 --> 01:01:46,150
you can see this S is
the same as the score

970
01:01:46,150 --> 01:01:48,510
in the previous slide.

971
01:01:48,510 --> 01:01:50,590
So if we have that
scoring function,

972
01:01:50,590 --> 01:01:54,350
now in order to
attract and repel,

973
01:01:54,350 --> 01:01:58,830
we can use this
framework of turn

974
01:01:58,830 --> 01:02:05,400
them a softmax setup with
exp that turn those scores

975
01:02:05,400 --> 01:02:07,260
into probability values.

976
01:02:07,260 --> 01:02:09,920
And then in the
denominator, you see

977
01:02:09,920 --> 01:02:15,040
all of the other negative
samples that are used

978
01:02:15,040 --> 01:02:17,480
are considered.

979
01:02:17,480 --> 01:02:18,920
In order to
implement actually we

980
01:02:18,920 --> 01:02:21,780
use this we use the
batch learning framework.

981
01:02:21,780 --> 01:02:23,680
All of the other
negative samples

982
01:02:23,680 --> 01:02:27,800
that belong to other
objects in the batch

983
01:02:27,800 --> 01:02:31,120
are taken as negative samples,
and one of the transform

984
01:02:31,120 --> 01:02:33,540
is as the positive sample.

985
01:02:33,540 --> 01:02:36,600
So we define the loss
function like this.

986
01:02:36,600 --> 01:02:39,440
And a score for
the positive pair,

987
01:02:39,440 --> 01:02:43,880
score for all of the
other negative pairs.

988
01:02:43,880 --> 01:02:48,080
And this function is very
similar to what we've actually

989
01:02:48,080 --> 01:02:50,160
discussed before.

990
01:02:50,160 --> 01:02:51,840
Any ideas?

991
01:02:51,840 --> 01:02:54,520
This is the cross entropy
for multi classes.

992
01:02:54,520 --> 01:02:57,960
So if we have n samples--

993
01:02:57,960 --> 01:03:04,990
sorry n samples, yes, then in
this case, we have n samples.

994
01:03:04,990 --> 01:03:09,190
So the softmax is if you
have multiple classes,

995
01:03:09,190 --> 01:03:11,430
if you have 10
classes as the output,

996
01:03:11,430 --> 01:03:15,210
it wants to maximize the
score for one of those 10,

997
01:03:15,210 --> 01:03:17,070
and minimize for the rest of it.

998
01:03:17,070 --> 01:03:19,070
It's the same story here.

999
01:03:19,070 --> 01:03:23,170
We want to maximize
this score, but minimize

1000
01:03:23,170 --> 01:03:26,050
the score between a
negative and reference.

1001
01:03:26,050 --> 01:03:29,530
So it's the same concept
that we discussed

1002
01:03:29,530 --> 01:03:31,910
about multi-class
classification,

1003
01:03:31,910 --> 01:03:34,690
but put it in the
form of formulation

1004
01:03:34,690 --> 01:03:37,450
of contrastive for
this loss function

1005
01:03:37,450 --> 01:03:39,610
as contrastive learning.

1006
01:03:39,610 --> 01:03:44,850
This function is called info
NCE or the information Noise

1007
01:03:44,850 --> 01:03:47,930
Contrastive
Estimation loss, which

1008
01:03:47,930 --> 01:03:51,170
was proposed in this paper.

1009
01:03:51,170 --> 01:03:55,370
And there are a lot of
theoretical discussions

1010
01:03:55,370 --> 01:03:58,890
in the paper that this
objective, this loss function

1011
01:03:58,890 --> 01:04:04,630
measures the dependencies,
sorry that it's a lower bound

1012
01:04:04,630 --> 01:04:07,790
on mutual information.

1013
01:04:07,790 --> 01:04:12,070
And what mutual information
is, when you have two images

1014
01:04:12,070 --> 01:04:14,790
and you calculate the mutual
information between them,

1015
01:04:14,790 --> 01:04:18,270
it's basically measuring
the dependencies

1016
01:04:18,270 --> 01:04:21,090
or shared information
between these two images.

1017
01:04:21,090 --> 01:04:23,790
So what we want
to do is, we want

1018
01:04:23,790 --> 01:04:27,430
to maximize shared information
between x and x plus,

1019
01:04:27,430 --> 01:04:32,330
but minimize the shared
information for x and x minuses.

1020
01:04:32,330 --> 01:04:38,230
So the paper says and
again, this will itself

1021
01:04:38,230 --> 01:04:40,130
take half an hour to go into.

1022
01:04:40,130 --> 01:04:42,750
So you should definitely
take a look at the paper

1023
01:04:42,750 --> 01:04:47,830
if you're interested, that the
negative of this loss function

1024
01:04:47,830 --> 01:04:52,750
info NCE, is a lower bound on
mutual information between x

1025
01:04:52,750 --> 01:04:54,070
and x plus.

1026
01:04:54,070 --> 01:04:57,390
So a lower bound on
mutual information

1027
01:04:57,390 --> 01:05:03,400
if the negative of it is a lower
bound on mutual information.

1028
01:05:03,400 --> 01:05:07,240
If I minimize the info
NCE, I'm basically

1029
01:05:07,240 --> 01:05:10,280
maximizing the
mutual information

1030
01:05:10,280 --> 01:05:11,940
between x and x plus.

1031
01:05:11,940 --> 01:05:14,000
So this is what I really want.

1032
01:05:14,000 --> 01:05:17,920
So that's why we take
this as the loss function

1033
01:05:17,920 --> 01:05:23,480
and start minimizing that value.

1034
01:05:23,480 --> 01:05:25,800
There is also another
theoretical aspect

1035
01:05:25,800 --> 01:05:29,920
in the info NCE paper
that says the larger

1036
01:05:29,920 --> 01:05:33,300
the number of negative
samples the tighter the bound.

1037
01:05:33,300 --> 01:05:36,600
So it tightens the bound
based on the number

1038
01:05:36,600 --> 01:05:38,520
of negative samples.

1039
01:05:38,520 --> 01:05:43,680
So that's why for training and
loss function a neural network

1040
01:05:43,680 --> 01:05:47,880
with this type of loss function,
we need a huge batch size.

1041
01:05:47,880 --> 01:05:51,040
With a larger number
of negative samples,

1042
01:05:51,040 --> 01:05:59,770
we'll get better and much
faster training convergence.

1043
01:05:59,770 --> 01:06:03,570
And then this loss function
was used in a number

1044
01:06:03,570 --> 01:06:06,230
of different frameworks.

1045
01:06:06,230 --> 01:06:09,210
And in the next few
minutes, I'm just

1046
01:06:09,210 --> 01:06:12,190
going to tell you what
those frameworks are.

1047
01:06:12,190 --> 01:06:16,570
For example, Sinclair
is a simple framework

1048
01:06:16,570 --> 01:06:22,210
for contrastive learning, is
basically taking each image,

1049
01:06:22,210 --> 01:06:24,550
do the two transformations
of the same image,

1050
01:06:24,550 --> 01:06:27,110
transfer it into the
representation space.

1051
01:06:27,110 --> 01:06:29,650
And what it does
is it calculates

1052
01:06:29,650 --> 01:06:36,390
the cosine similarity between
embeddings representations.

1053
01:06:36,390 --> 01:06:40,530
But before doing that, it does a
linear or non-linear projection

1054
01:06:40,530 --> 01:06:47,690
into a set of features z,
that calculates the distance

1055
01:06:47,690 --> 01:06:49,970
between those in this space.

1056
01:06:49,970 --> 01:06:55,280
And this is the way
that they do this,

1057
01:06:55,280 --> 01:06:57,580
generating this
positive samples.

1058
01:06:57,580 --> 01:07:00,920
And for generating
positive samples,

1059
01:07:00,920 --> 01:07:05,020
all sorts of transformations
would make sense.

1060
01:07:05,020 --> 01:07:09,700
The details are
basically covered here.

1061
01:07:09,700 --> 01:07:14,500
Generate a positive pair by
sampling data augmentation

1062
01:07:14,500 --> 01:07:15,580
functions.

1063
01:07:15,580 --> 01:07:20,060
So we sample a few of those
then we calculate the inverse

1064
01:07:20,060 --> 01:07:22,660
and see loss on the pairs.

1065
01:07:22,660 --> 01:07:29,580
And this is what we iterate
because each sample has 2n

1066
01:07:29,580 --> 01:07:32,940
multiplied by n samples
that we have created.

1067
01:07:32,940 --> 01:07:39,980
So what happens is we
take a list of images

1068
01:07:39,980 --> 01:07:43,140
in the mini batch, in
the batch that we have,

1069
01:07:43,140 --> 01:07:48,620
and pass them through
encoder for both variations

1070
01:07:48,620 --> 01:07:49,820
of the same image.

1071
01:07:49,820 --> 01:07:55,310
So each of the images basically
will have a transformed version

1072
01:07:55,310 --> 01:07:56,110
of it.

1073
01:07:56,110 --> 01:08:02,830
And next, so we have sub 2n
samples in the batch now.

1074
01:08:02,830 --> 01:08:05,950
And then this
means that for each

1075
01:08:05,950 --> 01:08:10,550
of the samples the next one,
these two are positive samples,

1076
01:08:10,550 --> 01:08:12,050
and everything else is negative.

1077
01:08:12,050 --> 01:08:16,010
So for the first one, the
second image is positive.

1078
01:08:16,010 --> 01:08:17,569
Everything else is negative.

1079
01:08:17,569 --> 01:08:19,710
For the second one
the first is positive

1080
01:08:19,710 --> 01:08:21,310
and everything else is negative.

1081
01:08:21,310 --> 01:08:24,990
And this repeats for all
of the samples there.

1082
01:08:24,990 --> 01:08:31,710
So this is a high level
definition of SimCLR.

1083
01:08:31,710 --> 01:08:36,870
Please note that we have
in assignment 3, a question

1084
01:08:36,870 --> 01:08:42,510
related to SimCLR that you will
be exploring this framework

1085
01:08:42,510 --> 01:08:44,069
a little bit more.

1086
01:08:44,069 --> 01:08:47,350
But be careful that the
definition is slightly

1087
01:08:47,350 --> 01:08:49,790
different from what the
standard definition that I

1088
01:08:49,790 --> 01:08:51,120
presented here.

1089
01:08:51,120 --> 01:08:57,080
And make sure you follow the
instructions in the assignment.

1090
01:08:57,080 --> 01:08:59,620
So SimCLR was actually
very successful.

1091
01:08:59,620 --> 01:09:05,160
It was able to without
the use of labels or--

1092
01:09:05,160 --> 01:09:07,600
and then a training
a linear classifier

1093
01:09:07,600 --> 01:09:13,279
on top of the features,
it was able to surpass

1094
01:09:13,279 --> 01:09:19,240
all of the previous works
and even basically generate

1095
01:09:19,240 --> 01:09:22,939
results comparable to
the supervised learning,

1096
01:09:22,939 --> 01:09:25,140
fully supervised
learning frameworks.

1097
01:09:25,140 --> 01:09:28,080
Although we need a larger
neural network because now

1098
01:09:28,080 --> 01:09:30,420
we are learning more
generic features.

1099
01:09:30,420 --> 01:09:32,120
But in terms of
accuracy, this was

1100
01:09:32,120 --> 01:09:38,760
not as it was comparable to what
we had for supervised learning.

1101
01:09:38,760 --> 01:09:44,040
So the interesting
thing with SimCLR

1102
01:09:44,040 --> 01:09:47,479
and some of the
results around it

1103
01:09:47,479 --> 01:09:53,630
was that there
are a few choices.

1104
01:09:53,630 --> 01:09:58,530
Actually, let me spend
time on the main choices.

1105
01:09:58,530 --> 01:10:02,850
You may have this question of
why did we project the features

1106
01:10:02,850 --> 01:10:06,850
into a new variable instead of
using the same representations?

1107
01:10:06,850 --> 01:10:10,910
So this was a design choice
that they made in SimCLR,

1108
01:10:10,910 --> 01:10:16,050
because they rightly
so assume that when

1109
01:10:16,050 --> 01:10:17,810
we have an objective
function that

1110
01:10:17,810 --> 01:10:20,490
does this contrast
between samples,

1111
01:10:20,490 --> 01:10:24,110
you often lose some
more information,

1112
01:10:24,110 --> 01:10:26,050
some extra information
that do not

1113
01:10:26,050 --> 01:10:28,070
help with the contrastive
learning framework.

1114
01:10:28,070 --> 01:10:34,490
So in order to preserve all
of those extra features,

1115
01:10:34,490 --> 01:10:36,830
representations
are defined as h,

1116
01:10:36,830 --> 01:10:40,010
but then there is this linear
or non-linear projection--

1117
01:10:40,010 --> 01:10:42,770
in their paper they use
non-linear projection,

1118
01:10:42,770 --> 01:10:49,540
to get the z values that they
can calculate influencer.

1119
01:10:49,540 --> 01:10:52,280
So that's one important
design choice.

1120
01:10:52,280 --> 01:10:56,680
And the other one is I mentioned
earlier large batch sizes.

1121
01:10:56,680 --> 01:10:59,300
You need huge batch
sizes, larger batch sizes

1122
01:10:59,300 --> 01:11:04,040
to be able to get better
SimCLR performance.

1123
01:11:04,040 --> 01:11:08,100
And we talked about it, how
and why this is the case.

1124
01:11:08,100 --> 01:11:11,340
But we can't always
do large batch sizes

1125
01:11:11,340 --> 01:11:13,580
for many of the
tasks that we have

1126
01:11:13,580 --> 01:11:20,060
at hand because of constraints
in memory and so on.

1127
01:11:20,060 --> 01:11:23,640
And that was why a
number of follow ups,

1128
01:11:23,640 --> 01:11:29,920
for example, mocha was proposed
momentum contrastive learning.

1129
01:11:29,920 --> 01:11:34,980
Instead of using the samples,
all of the negative samples

1130
01:11:34,980 --> 01:11:39,260
in the batch, what it
does is it creates a queue

1131
01:11:39,260 --> 01:11:46,440
and keeps a history of
the negative samples

1132
01:11:46,440 --> 01:11:49,260
across batches over
time in the model.

1133
01:11:49,260 --> 01:11:53,840
So it doesn't only depend on the
negative samples in the batch,

1134
01:11:53,840 --> 01:11:57,360
it has a separate
queue that is defined

1135
01:11:57,360 --> 01:12:01,640
and keeps a number of negative
samples and changes it,

1136
01:12:01,640 --> 01:12:11,560
updates it over time to train,
to do the inference loss,

1137
01:12:11,560 --> 01:12:13,360
the contrastive loss here.

1138
01:12:13,360 --> 01:12:17,240
But because we have this
queue, we cannot back propagate

1139
01:12:17,240 --> 01:12:19,540
because those samples are
not in the batch anymore.

1140
01:12:19,540 --> 01:12:25,260
So we cannot back propagate
for the negative samples,

1141
01:12:25,260 --> 01:12:27,560
and that's why it had
to separate the encoder

1142
01:12:27,560 --> 01:12:30,800
for positive samples,
which are now called query.

1143
01:12:30,800 --> 01:12:32,600
And the negative
samples which are now

1144
01:12:32,600 --> 01:12:38,240
called key in this architecture.

1145
01:12:38,240 --> 01:12:42,760
So the training only impacts
encoder and over time

1146
01:12:42,760 --> 01:12:51,590
the Q encoder, using a momentum
m, it updates the key encoder,

1147
01:12:51,590 --> 01:12:53,130
the momentum encoder.

1148
01:12:53,130 --> 01:12:57,370
So this is a framework
that has actually

1149
01:12:57,370 --> 01:13:01,490
been very successful in
terms of implementation,

1150
01:13:01,490 --> 01:13:03,670
and follow up versions of it.

1151
01:13:03,670 --> 01:13:07,930
There is a lot of
interesting results,

1152
01:13:07,930 --> 01:13:13,170
but then what they've
done was basically

1153
01:13:13,170 --> 01:13:19,530
tried hybrid versions of using
some non-linear projection heads

1154
01:13:19,530 --> 01:13:22,210
and data augmentation
from SimCLR

1155
01:13:22,210 --> 01:13:26,010
and using this mini batch style,
the decoupling of the mini batch

1156
01:13:26,010 --> 01:13:29,170
and negative samples
from MoCo, and they've

1157
01:13:29,170 --> 01:13:34,490
shown that actually if you do
this together, MoCo version 2,

1158
01:13:34,490 --> 01:13:39,950
it improves the
performance by a lot.

1159
01:13:39,950 --> 01:13:45,740
So I will stop here, but
there was some notions

1160
01:13:45,740 --> 01:13:50,140
of CPC, the Contrastive
Predictive Coding,

1161
01:13:50,140 --> 01:13:55,020
as another example, that you
can look at in the slides.

1162
01:13:55,020 --> 01:14:00,580
And then a better version of
MoCo, MoCo version 3 and DINO

1163
01:14:00,580 --> 01:14:04,580
is also one of the widely
used frameworks which actually

1164
01:14:04,580 --> 01:14:11,220
has a similar type of
architecture as MoCo,

1165
01:14:11,220 --> 01:14:14,020
but it's not
necessarily contrastive

1166
01:14:14,020 --> 01:14:17,640
learning because now we have
students and teacher networks.

1167
01:14:17,640 --> 01:14:20,280
So I'll leave that for
a separate discussion,

1168
01:14:20,280 --> 01:14:24,220
and if you're interested,
we can discuss maybe

1169
01:14:24,220 --> 01:14:27,520
in the future slides,
in future lectures.

1170
01:14:27,520 --> 01:14:32,380
But anyways, this is also one
of the widely used frameworks

1171
01:14:32,380 --> 01:14:35,140
for extracting features
from images and also videos

1172
01:14:35,140 --> 01:14:36,990
sometimes.