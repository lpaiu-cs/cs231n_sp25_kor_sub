2
00:00:05,270 --> 00:00:07,100
As you can see on
this slide, today, we

3
00:00:07,100 --> 00:00:14,310
are going to talk about neural
networks and backpropagation.

4
00:00:14,310 --> 00:00:17,000
Which is actually the process--

5
00:00:17,000 --> 00:00:19,770
early years, I
was studying this,

6
00:00:19,770 --> 00:00:26,180
I was often referring to it
as the magical process that

7
00:00:26,180 --> 00:00:30,590
lets the neural networks
learn from their own mistakes,

8
00:00:30,590 --> 00:00:35,910
pretty much like humans but
in a more organized fashion,

9
00:00:35,910 --> 00:00:39,720
and also using a
little bit more math.

10
00:00:39,720 --> 00:00:43,380
So let's dive into the topic.

11
00:00:43,380 --> 00:00:46,440
I'm sure this is
going to be exciting.

12
00:00:46,440 --> 00:00:49,460
And this is laying
the foundation

13
00:00:49,460 --> 00:00:51,180
for the rest of the quarter.

14
00:00:51,180 --> 00:00:54,650
Every single algorithm that we
will be discussing in the future

15
00:00:54,650 --> 00:00:59,600
without even mentioning is
using a form of backpropagation.

16
00:00:59,600 --> 00:01:06,850
And so that's why understanding
this lecture and the topics

17
00:01:06,850 --> 00:01:08,270
is very important.

18
00:01:08,270 --> 00:01:12,470
OK, in keeping us
with the tradition,

19
00:01:12,470 --> 00:01:19,180
let's cover what we've
talked about so far.

20
00:01:19,180 --> 00:01:27,310
So I'm sure you now remember
what we talked about last time.

21
00:01:27,310 --> 00:01:34,360
We said, how we can form the
objective functions, or loss

22
00:01:34,360 --> 00:01:36,050
functions, what we call here.

23
00:01:36,050 --> 00:01:40,040
And then we talked
about regularization.

24
00:01:40,040 --> 00:01:45,520
But to do that, we
formulated everything

25
00:01:45,520 --> 00:01:53,180
through the x, y, defining the
pairs and a scoring function.

26
00:01:53,180 --> 00:01:57,260
Which, in this case, we are
using a linear scoring function,

27
00:01:57,260 --> 00:02:02,610
as you can see,
and also defining,

28
00:02:02,610 --> 00:02:04,900
ultimately, this loss function.

29
00:02:04,900 --> 00:02:07,110
So this graph that
you see on the right

30
00:02:07,110 --> 00:02:14,610
is what we drew, showing all
the process, the entire process

31
00:02:14,610 --> 00:02:16,240
of learning.

32
00:02:16,240 --> 00:02:20,610
There has been some questions.

33
00:02:20,610 --> 00:02:26,160
Questions in the last lecture
and also even before that,

34
00:02:26,160 --> 00:02:29,980
that why we are only using
the softmax function.

35
00:02:29,980 --> 00:02:34,230
I wanted to reiterate that
it's not the only loss function

36
00:02:34,230 --> 00:02:36,490
that we have and we use.

37
00:02:36,490 --> 00:02:42,070
It's one of the most widely used
in deep learning and building,

38
00:02:42,070 --> 00:02:44,320
especially for the
task of classification.

39
00:02:44,320 --> 00:02:47,490
But there are so
many other options

40
00:02:47,490 --> 00:02:50,590
that we use for other
tasks, for different tasks.

41
00:02:50,590 --> 00:02:53,290
Even for the task
of classification,

42
00:02:53,290 --> 00:02:55,440
if you've looked at
the slides that we've

43
00:02:55,440 --> 00:02:59,640
shared on the website, I
included this hinge loss,

44
00:02:59,640 --> 00:03:05,630
or used to be called
SVM loss in the reading

45
00:03:05,630 --> 00:03:07,800
assignments in lecture two.

46
00:03:07,800 --> 00:03:11,690
So in the slides, we had
examples and everything

47
00:03:11,690 --> 00:03:16,770
around the topic of hinge loss.

48
00:03:16,770 --> 00:03:20,960
It is also one of
those widely used loss

49
00:03:20,960 --> 00:03:26,700
functions, especially in the
early years of neural networks.

50
00:03:26,700 --> 00:03:31,460
And just to give you a high
level understanding of what

51
00:03:31,460 --> 00:03:38,900
it is, this is a loss function,
that unlike softmax, does not

52
00:03:38,900 --> 00:03:41,250
turn the scores
into probabilities.

53
00:03:41,250 --> 00:03:42,980
So turning them
into probabilities

54
00:03:42,980 --> 00:03:44,490
is not the only option.

55
00:03:44,490 --> 00:03:48,720
So we can use other forms.

56
00:03:48,720 --> 00:03:53,090
This function encourages
the score of--

57
00:03:53,090 --> 00:03:54,820
let me highlight here--

58
00:03:54,820 --> 00:04:01,360
the score of the
correct items, which

59
00:04:01,360 --> 00:04:06,430
is defined by Syi to be
higher than the scores

60
00:04:06,430 --> 00:04:08,900
of all other items Sj.

61
00:04:08,900 --> 00:04:16,329
You can see the condition
here, creating a value of 0

62
00:04:16,329 --> 00:04:18,519
if the condition is true.

63
00:04:18,519 --> 00:04:22,990
And otherwise, what it does is--

64
00:04:22,990 --> 00:04:28,720
so as I said, it encourages
the score of the correct item

65
00:04:28,720 --> 00:04:32,170
to be higher than the
scores of all other items

66
00:04:32,170 --> 00:04:34,220
by at least a margin.

67
00:04:34,220 --> 00:04:35,890
That number one
that you see there

68
00:04:35,890 --> 00:04:38,510
is the margin that it creates.

69
00:04:38,510 --> 00:04:41,690
And then if the
condition is violated,

70
00:04:41,690 --> 00:04:47,000
the loss increases
proportionally from the margin.

71
00:04:47,000 --> 00:04:48,520
And this is the
visualization-- so

72
00:04:48,520 --> 00:04:56,440
creating this visualization
of the function.

73
00:04:56,440 --> 00:05:02,130
So this promotes correct
scores by penalizing cases,

74
00:05:02,130 --> 00:05:05,920
where irrelevant items
are scored too highly.

75
00:05:05,920 --> 00:05:10,620
So again, refer to reading
assignment in lecture two

76
00:05:10,620 --> 00:05:15,630
for examples and to get
better understanding of that.

77
00:05:15,630 --> 00:05:20,580
Next, we have talked about
general optimization,

78
00:05:20,580 --> 00:05:28,750
how to find the best parameters,
W, for the neural network.

79
00:05:28,750 --> 00:05:32,160
And in doing so, we
talked a little bit

80
00:05:32,160 --> 00:05:39,390
about this loss landscape being
as a large valley as shown

81
00:05:39,390 --> 00:05:42,190
in this image.

82
00:05:42,190 --> 00:05:46,470
And every point on that
valley is a different set

83
00:05:46,470 --> 00:05:48,990
of weight parameters.

84
00:05:48,990 --> 00:05:52,850
And we wanted to find the
set of parameters, W, that

85
00:05:52,850 --> 00:05:56,210
minimizes that loss landscape.

86
00:05:56,210 --> 00:05:58,790
We talked about the
fact that the key

87
00:05:58,790 --> 00:06:03,080
is being able to take
the gradient of the loss

88
00:06:03,080 --> 00:06:07,340
function, L, with
respect to W and use

89
00:06:07,340 --> 00:06:12,480
the gradient for optimization
in a step-by-step manner.

90
00:06:12,480 --> 00:06:16,590
Which gave us the gradient
descent algorithm.

91
00:06:16,590 --> 00:06:22,020
So the weights are
basically updated.

92
00:06:22,020 --> 00:06:25,730
Although it's very hard for
me to see from this distance,

93
00:06:25,730 --> 00:06:32,210
what I'm pointing
to, but I can guess.

94
00:06:32,210 --> 00:06:38,030
So anyways, in
order to walk down

95
00:06:38,030 --> 00:06:42,980
the loss landscape
towards the minimum value,

96
00:06:42,980 --> 00:06:47,120
a step size is
defined and we often

97
00:06:47,120 --> 00:06:54,280
get to take one step with
respect to a step size

98
00:06:54,280 --> 00:06:58,100
in the negative direction
of the gradient.

99
00:06:58,100 --> 00:07:01,910
So this was the gradient
descent algorithm.

100
00:07:01,910 --> 00:07:04,030
And in order to
optimize, we talked

101
00:07:04,030 --> 00:07:09,610
about two different approaches
of numerical gradient

102
00:07:09,610 --> 00:07:15,290
and analytical gradient, both
of which having pros and cons.

103
00:07:15,290 --> 00:07:21,820
And we discussed in practice to
derive analytical gradients--

104
00:07:21,820 --> 00:07:24,680
in practice, we derive
analytical gradients.

105
00:07:24,680 --> 00:07:28,930
And often, if it's hard to do
the implementation and the math

106
00:07:28,930 --> 00:07:31,750
and everything, we check
our implementations

107
00:07:31,750 --> 00:07:37,030
with numerical gradients.

108
00:07:37,030 --> 00:07:39,970
And one of the other
challenges we talked about

109
00:07:39,970 --> 00:07:46,270
was the use of incorporating the
loss function and its gradients

110
00:07:46,270 --> 00:07:47,740
on the entire data set.

111
00:07:47,740 --> 00:07:51,480
So if you have a large data
set, it's very expensive

112
00:07:51,480 --> 00:07:58,400
to run the loss function and the
derivative on the entire data

113
00:07:58,400 --> 00:07:58,900
set.

114
00:07:58,900 --> 00:08:02,520
And that's why we talked about
the idea of mini batches,

115
00:08:02,520 --> 00:08:12,030
using a number of examples
sampled from the data set,

116
00:08:12,030 --> 00:08:19,120
often maybe 32,
64, or 128, or 256.

117
00:08:19,120 --> 00:08:27,240
And that subsampled data is used
for identifying the gradients

118
00:08:27,240 --> 00:08:33,990
and then taking the steps
towards the minimum.

119
00:08:33,990 --> 00:08:39,340
And beyond SGD and
stochastic gradient descent,

120
00:08:39,340 --> 00:08:43,350
we talked about some
optimizations of SGD

121
00:08:43,350 --> 00:08:51,570
with Momentum, RMSProp,
and Adam optimizer.

122
00:08:51,570 --> 00:08:56,270
And there were a lot of
details that I would refer you

123
00:08:56,270 --> 00:08:59,790
to the lecture,
the third lecture,

124
00:08:59,790 --> 00:09:06,490
if you have any specific
questions about this.

125
00:09:06,490 --> 00:09:09,230
And then one of the
other things that we

126
00:09:09,230 --> 00:09:12,380
talked about was the
importance of the learning rate

127
00:09:12,380 --> 00:09:14,700
and scheduling
the learning rate.

128
00:09:14,700 --> 00:09:16,910
And in some of
the optimizers, we

129
00:09:16,910 --> 00:09:21,450
often try to start with a larger
value of the learning rate

130
00:09:21,450 --> 00:09:27,800
and then start using different
types of decaying the learning

131
00:09:27,800 --> 00:09:33,250
rate, or reducing its
value by a factor.

132
00:09:33,250 --> 00:09:38,850
And this is normally
needed in many optimizers.

133
00:09:38,850 --> 00:09:43,500
But in some of the more recent
ones, Adam and its variants,

134
00:09:43,500 --> 00:09:47,940
we often do not need to
manually, or explicitly decrease

135
00:09:47,940 --> 00:09:53,280
that because that is encoded
into the optimizer itself.

136
00:09:53,280 --> 00:10:01,410
So with that, I want us to get
to the topic of neural networks

137
00:10:01,410 --> 00:10:06,330
and see how we can actually
build neural networks

138
00:10:06,330 --> 00:10:13,770
and solve more exciting
and harder problems.

139
00:10:13,770 --> 00:10:21,870
So we've so far talked about
this function, linear function,

140
00:10:21,870 --> 00:10:24,400
of W multiplied by x.

141
00:10:24,400 --> 00:10:30,270
And that is the most
basic neural network

142
00:10:30,270 --> 00:10:33,910
that could be defined
as just one layer.

143
00:10:33,910 --> 00:10:36,150
We will be talking
about the layers.

144
00:10:36,150 --> 00:10:42,410
And what I want you to
pay attention to here

145
00:10:42,410 --> 00:10:46,130
are these dimensions
D and C, which

146
00:10:46,130 --> 00:10:50,030
are the dimensionality
of the input data,

147
00:10:50,030 --> 00:10:54,360
input x, or the
number of features.

148
00:10:54,360 --> 00:10:57,590
And C is the number of
classes, basically, the number

149
00:10:57,590 --> 00:11:02,300
of outputs, nodes, or neurons,
whatever number of outputs

150
00:11:02,300 --> 00:11:04,730
we need.

151
00:11:04,730 --> 00:11:12,390
And in order to create a neural
network at a second layer,

152
00:11:12,390 --> 00:11:19,320
we can define a new set of
weights referred to as W2 here.

153
00:11:19,320 --> 00:11:24,410
And we apply those
to the previous layer

154
00:11:24,410 --> 00:11:28,880
of W1 multiplied by x.

155
00:11:28,880 --> 00:11:31,910
Again, pay attention
to the dimensionalities

156
00:11:31,910 --> 00:11:35,360
here that we have the C
number of outputs and D

157
00:11:35,360 --> 00:11:37,380
as the number of input features.

158
00:11:37,380 --> 00:11:42,220
But then you also
define H, and that

159
00:11:42,220 --> 00:11:49,210
defines the number of neurons,
the number of hidden layer

160
00:11:49,210 --> 00:11:52,720
nodes, or neurons.

161
00:11:52,720 --> 00:11:53,810
That's one point.

162
00:11:53,810 --> 00:11:56,620
The second point is
this max function

163
00:11:56,620 --> 00:12:00,770
that we'll be coming back to.

164
00:12:00,770 --> 00:12:04,540
And we'll explain what
it is and what it means.

165
00:12:04,540 --> 00:12:07,630
What the max operation
is doing here

166
00:12:07,630 --> 00:12:10,960
is to create a
non-linearity between

167
00:12:10,960 --> 00:12:15,980
the linear transformations
done by W1 and W2.

168
00:12:15,980 --> 00:12:20,830
And this is actually a
very, very important part

169
00:12:20,830 --> 00:12:22,850
of the process.

170
00:12:22,850 --> 00:12:25,160
I will talk a little bit
about the non-linearity

171
00:12:25,160 --> 00:12:30,430
but also look at this
last part before I forget.

172
00:12:30,430 --> 00:12:34,400
In practice, that's right that
we are only including W and x,

173
00:12:34,400 --> 00:12:39,010
as we talked about this in
the first and second lecture.

174
00:12:39,010 --> 00:12:45,850
We also incorporate a bias just
to have a complete framework.

175
00:12:45,850 --> 00:12:47,560
So in practice,
we also have bias,

176
00:12:47,560 --> 00:12:51,160
but we don't write it here
for the sake of simplicity.

177
00:12:51,160 --> 00:12:56,410
Anyways, the max operation
is creating the nonlinearity.

178
00:12:56,410 --> 00:12:59,460
And it's actually very
important because, as we talked

179
00:12:59,460 --> 00:13:06,820
about the linear classifiers
in the last few lectures,

180
00:13:06,820 --> 00:13:11,460
we said, we mentioned that there
are so many different problems

181
00:13:11,460 --> 00:13:15,390
that we can't separate
the samples with just

182
00:13:15,390 --> 00:13:16,480
one single line.

183
00:13:16,480 --> 00:13:20,370
This was one of the
examples that in order

184
00:13:20,370 --> 00:13:23,770
to be able to solve this
problem with neural networks,

185
00:13:23,770 --> 00:13:27,510
with linear functions, we
need some sort of nonlinear

186
00:13:27,510 --> 00:13:31,780
transformation from the
original space to a new space.

187
00:13:31,780 --> 00:13:35,550
And now, in the new space, you
see that they are separable,

188
00:13:35,550 --> 00:13:37,080
using a line.

189
00:13:37,080 --> 00:13:44,960
So in this case, it's a
nonlinear transformation

190
00:13:44,960 --> 00:13:47,790
between the input and
then the second space,

191
00:13:47,790 --> 00:13:52,760
which is mapping the x and y
to their polar coordinates, r

192
00:13:52,760 --> 00:13:53,580
and theta.

193
00:13:53,580 --> 00:13:57,000
But again, this is
just one example.

194
00:13:57,000 --> 00:14:01,130
There are so many
other examples, too.

195
00:14:01,130 --> 00:14:07,790
So with this example,
let's go back to-- oops,

196
00:14:07,790 --> 00:14:14,360
let's go back to our definition
of the two layer neural network.

197
00:14:14,360 --> 00:14:17,630
As you've probably
seen in the literature

198
00:14:17,630 --> 00:14:20,660
and outside this
class, these types

199
00:14:20,660 --> 00:14:26,570
of networks which only depend
on weights and inputs and layers

200
00:14:26,570 --> 00:14:30,110
and so on, there are no other
operations than multiplication,

201
00:14:30,110 --> 00:14:33,830
are often referred to as
fully connected networks,

202
00:14:33,830 --> 00:14:36,880
or multi-layer
perceptrons, MLPs.

203
00:14:36,880 --> 00:14:40,250
So that's one thing.

204
00:14:40,250 --> 00:14:42,610
And we can actually stack
more and more layers

205
00:14:42,610 --> 00:14:49,030
to create better,
more larger networks.

206
00:14:49,030 --> 00:14:52,540
And in this case,
again, pay attention

207
00:14:52,540 --> 00:14:54,970
to the dimensionalities
and the hidden layers

208
00:14:54,970 --> 00:15:00,220
that we have in the middle
and the dimensionalities that

209
00:15:00,220 --> 00:15:04,150
do match one after the other.

210
00:15:04,150 --> 00:15:11,470
So back to this visual
representation of what

211
00:15:11,470 --> 00:15:14,003
the neural network is doing.

213
00:15:16,540 --> 00:15:21,460
We talked about this when we
had the linear representations,

214
00:15:21,460 --> 00:15:28,600
that often what happens is the
network, through the weights,

215
00:15:28,600 --> 00:15:31,780
is learning some
sort of templates.

216
00:15:31,780 --> 00:15:34,500
If you remember
last week, we were

217
00:15:34,500 --> 00:15:37,120
talking about these templates
that are being learned.

218
00:15:37,120 --> 00:15:40,050
So again, I'm saying,
templates, they're not.

219
00:15:40,050 --> 00:15:44,920
I mean, they are some
representatives of the images

220
00:15:44,920 --> 00:15:49,780
but from the data, depending
on what data it was trained on.

221
00:15:49,780 --> 00:15:54,990
So these templates in what
we discussed last week

222
00:15:54,990 --> 00:15:59,820
were generated by
these 10 outputs

223
00:15:59,820 --> 00:16:05,200
by applying the W's on
top of the input neurons.

224
00:16:05,200 --> 00:16:11,830
So with that, now that we have
multiple layers, more layers,

225
00:16:11,830 --> 00:16:15,370
now we can actually create
some more templates.

226
00:16:15,370 --> 00:16:18,150
Now, we have a layer in the
middle that can actually

227
00:16:18,150 --> 00:16:22,950
create 100 templates,
as opposed to just 10

228
00:16:22,950 --> 00:16:24,250
for a linear classifier.

229
00:16:24,250 --> 00:16:26,350
Although we still
have those 10 as well.

230
00:16:26,350 --> 00:16:31,340
And this, again, in a very high
level-- from a very high level

231
00:16:31,340 --> 00:16:35,340
understanding point of view,
I'm telling you what this means.

232
00:16:35,340 --> 00:16:38,910
When we have these 100
neurons in the middle,

233
00:16:38,910 --> 00:16:40,700
we are giving the
network the power

234
00:16:40,700 --> 00:16:44,580
to create templates
for not entire objects

235
00:16:44,580 --> 00:16:47,160
but maybe parts of the object.

236
00:16:47,160 --> 00:16:49,200
For example, the classes
that you see here,

237
00:16:49,200 --> 00:16:56,460
we had bird, cat, deer, dog,
frog, horse, they all have eyes.

238
00:16:56,460 --> 00:16:59,390
So one of those 10
templates, 100 templates

239
00:16:59,390 --> 00:17:03,230
could probably be a part of
the object that could be shared

240
00:17:03,230 --> 00:17:05,310
between all of the classes.

241
00:17:05,310 --> 00:17:08,150
So from a high level point
of view and understanding

242
00:17:08,150 --> 00:17:12,240
these can form templates.

243
00:17:12,240 --> 00:17:15,859
And when we come back to
the topics of visualization

244
00:17:15,859 --> 00:17:19,640
and what we learned from the
neural networks, this topic,

245
00:17:19,640 --> 00:17:22,550
we'll uncover more
details about what

246
00:17:22,550 --> 00:17:24,500
I'm talking about right now.

247
00:17:24,500 --> 00:17:29,430
So back to the function max.

248
00:17:29,430 --> 00:17:32,230
We talked about max
function, the nonlinearity

249
00:17:32,230 --> 00:17:34,040
that is created here.

250
00:17:34,040 --> 00:17:37,780
And in neural
network terminology,

251
00:17:37,780 --> 00:17:41,270
we call that an
activation function.

252
00:17:41,270 --> 00:17:45,920
And it's actually playing a
very, very important role,

253
00:17:45,920 --> 00:17:49,840
a pivotal role, in
building the model,

254
00:17:49,840 --> 00:17:51,740
building a neural network.

255
00:17:51,740 --> 00:17:54,260
Let's answer this question
that we have on the slide.

256
00:17:54,260 --> 00:17:59,350
What happens if we try to build
a neural network without one

257
00:17:59,350 --> 00:18:03,170
of these activation functions,
let's say, the max function?

258
00:18:03,170 --> 00:18:06,350
This will be our function
if I remove the max.

259
00:18:06,350 --> 00:18:10,400
So it would be W2
multiply by W1 by x.

260
00:18:10,400 --> 00:18:12,220
What would happen here?

261
00:18:12,220 --> 00:18:13,160
Yes, exactly.

262
00:18:13,160 --> 00:18:17,740
So as you can guess and
correctly, you mentioned,

263
00:18:17,740 --> 00:18:23,710
the multiplication of
W2 by W1 could actually

264
00:18:23,710 --> 00:18:27,050
be replaced easily with
another matrix, W3.

265
00:18:27,050 --> 00:18:30,070
And then your function becomes
just a linear function.

266
00:18:30,070 --> 00:18:32,350
So everything could
be lumped together.

267
00:18:32,350 --> 00:18:34,950
So we need some sort
of non-linearity

268
00:18:34,950 --> 00:18:40,710
in the middle to be able
to give us the power

269
00:18:40,710 --> 00:18:44,410
to solve non-linear problems.

270
00:18:44,410 --> 00:18:49,420
The function that we just
talked about is a ReLU.

271
00:18:49,420 --> 00:18:53,710
It's the rectified linear unit.

272
00:18:53,710 --> 00:18:56,700
It's a very popular function,
activation function,

273
00:18:56,700 --> 00:18:59,920
used in neural networks.

274
00:18:59,920 --> 00:19:02,790
While there are so many
other variants that

275
00:19:02,790 --> 00:19:07,630
have been tested in
many other architectures

276
00:19:07,630 --> 00:19:11,220
and even in the more
modern architectures, one

277
00:19:11,220 --> 00:19:13,380
of the problems
that ReLU has is,

278
00:19:13,380 --> 00:19:16,470
it sometimes
creates dead neurons

279
00:19:16,470 --> 00:19:20,700
because it's making
everything equal to 0,

280
00:19:20,700 --> 00:19:22,240
if it's not positive.

281
00:19:22,240 --> 00:19:27,540
So in order to avoid the
dead neurons, leaky ReLU,

282
00:19:27,540 --> 00:19:32,840
with this type of
modeling, or ELU,

283
00:19:32,840 --> 00:19:38,960
that the exponential linear
unit are other options.

284
00:19:38,960 --> 00:19:41,180
ELU is a little bit better
because it has a better

285
00:19:41,180 --> 00:19:44,250
zero-centered function.

286
00:19:44,250 --> 00:19:52,010
And then there are some newer
variations, GeLU, Gaussian,

287
00:19:52,010 --> 00:19:55,250
or linear unit.

288
00:19:55,250 --> 00:19:59,100
Or I don't know, I've heard
both variations, ELU and GeLU--

289
00:19:59,100 --> 00:20:02,130
so could be used.

290
00:20:02,130 --> 00:20:05,600
They are often used more
often in neural architecture

291
00:20:05,600 --> 00:20:07,110
in transformers.

292
00:20:07,110 --> 00:20:14,840
And we also have
SiLU, or switch.

293
00:20:14,840 --> 00:20:20,720
It's the sigmoid
linear unit that, one,

294
00:20:20,720 --> 00:20:25,180
is also used in some of the
modern CNNR architectures.

295
00:20:25,180 --> 00:20:29,440
Google was using this for
some of the variations

296
00:20:29,440 --> 00:20:33,910
of their models and
also in EfficientNet.

297
00:20:33,910 --> 00:20:40,030
Other than these, there are
functions like sigmoid and Tanh,

298
00:20:40,030 --> 00:20:48,020
or Tanh that are often also
used as activation functions.

299
00:20:48,020 --> 00:20:52,750
Although they do
have a few problems

300
00:20:52,750 --> 00:20:57,020
because they do squash
values in a narrow range.

301
00:20:57,020 --> 00:21:01,820
And that sometimes results
in vanishing gradients.

302
00:21:01,820 --> 00:21:05,530
So we often do not
use sigmoid or Tanh

303
00:21:05,530 --> 00:21:08,420
in the middle of
the neural networks.

304
00:21:08,420 --> 00:21:13,390
They are often used in
the later layers, where

305
00:21:13,390 --> 00:21:15,970
we want to, for example,
binarize the outputs

306
00:21:15,970 --> 00:21:19,370
and things like that.

307
00:21:19,370 --> 00:21:22,850
So as I said, ReLU is often
a good default choice.

308
00:21:22,850 --> 00:21:26,860
It's very much used
in many architectures.

309
00:21:26,860 --> 00:21:30,120
And there are so many
variations of the same function

310
00:21:30,120 --> 00:21:33,390
that we talked about.

311
00:21:33,390 --> 00:21:36,810
I want to summarize
what we've talked about

312
00:21:36,810 --> 00:21:38,650
and then answer some questions.

313
00:21:38,650 --> 00:21:45,280
So we did talk about different
adding layers and so on.

314
00:21:45,280 --> 00:21:49,080
But I want to highlight that
activation functions are often

315
00:21:49,080 --> 00:21:54,040
functions that are
operating in the layers.

316
00:21:54,040 --> 00:21:58,170
And we also have
W's, which define

317
00:21:58,170 --> 00:22:01,170
the weights mapping
between the previous layer

318
00:22:01,170 --> 00:22:04,030
and the next layer.

319
00:22:04,030 --> 00:22:07,350
Again, these are fully
connected neural networks

320
00:22:07,350 --> 00:22:12,400
with very simple
implementations.

321
00:22:12,400 --> 00:22:17,310
What we only need is to be
able to define an activation

322
00:22:17,310 --> 00:22:17,830
function.

323
00:22:17,830 --> 00:22:20,530
And in this example, if
you look at the example,

324
00:22:20,530 --> 00:22:26,180
we have the sigmoid function
defined as the activation

325
00:22:26,180 --> 00:22:30,180
function and very easily
using that activation.

326
00:22:30,180 --> 00:22:35,190
The first and the second
layers of the hidden values,

327
00:22:35,190 --> 00:22:42,800
hidden neurons, are calculated
by applying W1 by x and then

328
00:22:42,800 --> 00:22:46,910
also the bias and then
applying the function,

329
00:22:46,910 --> 00:22:48,060
the activation function.

330
00:22:48,060 --> 00:22:50,510
And then same for
H2, and the output

331
00:22:50,510 --> 00:22:57,770
will be very simply the
dot product between the W3

332
00:22:57,770 --> 00:23:01,830
and the last layer
of hidden values,

333
00:23:01,830 --> 00:23:04,290
creating the output layer.

334
00:23:04,290 --> 00:23:07,140
I'll stop here to answer some
questions if there are any.

335
00:23:07,140 --> 00:23:10,280
And then, we'll
love to continue it.

336
00:23:10,280 --> 00:23:11,370
That is a great question.

337
00:23:11,370 --> 00:23:13,640
And the question
is, how would we

338
00:23:13,640 --> 00:23:16,520
choose for a new problem which
of these activation functions

339
00:23:16,520 --> 00:23:18,920
to use?

340
00:23:18,920 --> 00:23:21,160
The short answer
to your question

341
00:23:21,160 --> 00:23:26,140
is yes, it's empirical
in most cases.

342
00:23:26,140 --> 00:23:29,140
But we often start
with value, or we

343
00:23:29,140 --> 00:23:32,290
go with standard
activation functions

344
00:23:32,290 --> 00:23:35,890
being used for those
specific architectures.

345
00:23:35,890 --> 00:23:40,150
As I mentioned, there
are activation functions

346
00:23:40,150 --> 00:23:45,160
that are often commonly used
in CNNs, or in transformers

347
00:23:45,160 --> 00:23:47,600
and different architectures.

348
00:23:47,600 --> 00:23:53,330
So we often go with the
ones that are tested before.

349
00:23:53,330 --> 00:23:55,430
But yes, it's mostly empirical.

350
00:23:55,430 --> 00:23:59,090
If you're designing a new
network for a new problem,

351
00:23:59,090 --> 00:24:02,690
then that's one of your
choices that you have to make,

352
00:24:02,690 --> 00:24:05,830
very much similar to
other hyperparameters.

353
00:24:05,830 --> 00:24:10,060
So the question here is,
what is the attribute

354
00:24:10,060 --> 00:24:13,570
that is basically common
between all of these activation

355
00:24:13,570 --> 00:24:17,090
functions and what
it really does?

356
00:24:17,090 --> 00:24:19,570
I will give you some examples.

357
00:24:19,570 --> 00:24:23,580
And I'll go into some of the
details of what these activation

358
00:24:23,580 --> 00:24:26,640
functions are doing.

359
00:24:26,640 --> 00:24:34,770
Basically, the main and the most
important common characteristic

360
00:24:34,770 --> 00:24:37,260
here is to create nonlinearity.

361
00:24:37,260 --> 00:24:40,420
We're not using a linear
function as the activation.

362
00:24:40,420 --> 00:24:43,110
So creating some
sort of nonlinearity

363
00:24:43,110 --> 00:24:46,330
is something that makes
it very important.

364
00:24:46,330 --> 00:24:48,820
And why do we have
so many variations?

365
00:24:48,820 --> 00:24:51,300
I told you a little
bit about the problems

366
00:24:51,300 --> 00:24:53,320
with vanishing gradients.

367
00:24:53,320 --> 00:24:58,050
I told you a little bit
about differentiability

368
00:24:58,050 --> 00:24:59,440
of the functions.

369
00:24:59,440 --> 00:25:02,040
They should be differentiable
because we are using them

370
00:25:02,040 --> 00:25:03,300
in neural network.

371
00:25:03,300 --> 00:25:10,980
And sometimes, having a
proper zero-centered value

372
00:25:10,980 --> 00:25:14,040
and a smooth function
makes it much faster

373
00:25:14,040 --> 00:25:17,100
to get converging networks.

374
00:25:17,100 --> 00:25:19,650
So there are so many
different factors.

375
00:25:19,650 --> 00:25:23,310
These are the main ones that
I told you and talked about,

376
00:25:23,310 --> 00:25:25,880
which play an important
role for defining,

377
00:25:25,880 --> 00:25:30,650
or designing these functions.

378
00:25:30,650 --> 00:25:32,150
I'll talk a little
bit more about it

379
00:25:32,150 --> 00:25:36,710
when I go into details
of the functions.

380
00:25:36,710 --> 00:25:42,180
In all of the layers, we often
use same activation functions.

381
00:25:42,180 --> 00:25:45,500
But as I said, sometimes, in
the later layers, or the output

382
00:25:45,500 --> 00:25:49,610
layer, we use a activation
sigmoid function

383
00:25:49,610 --> 00:25:55,290
and/or tangent function,
but commonly yes.

384
00:25:55,290 --> 00:26:04,340
And the question was, if we use
the same across the networks,

385
00:26:04,340 --> 00:26:09,020
the entire network, same
function for all of the neurons?

386
00:26:09,020 --> 00:26:16,240
OK, continuing to
what we were talking

387
00:26:16,240 --> 00:26:23,450
about, which is the
implementation of these models,

388
00:26:23,450 --> 00:26:25,580
these neural network.

389
00:26:25,580 --> 00:26:28,840
So there is a very simple way.

390
00:26:28,840 --> 00:26:31,040
I mean, building
a neural network,

391
00:26:31,040 --> 00:26:32,950
a two layer neural
network, in Python

392
00:26:32,950 --> 00:26:36,740
is just less than
20 lines of code.

393
00:26:36,740 --> 00:26:40,990
Very simple, define
our network as I talked

394
00:26:40,990 --> 00:26:42,280
about the dimensionalities.

395
00:26:42,280 --> 00:26:45,640
N is the number of samples.

396
00:26:45,640 --> 00:26:48,490
D_in is the dimensionality
of the input.

397
00:26:48,490 --> 00:26:51,110
And D_out is the
dimensionality of the output.

398
00:26:51,110 --> 00:26:56,570
And h, the number of
neurons in the hidden layer.

399
00:26:56,570 --> 00:27:00,700
And we talked about-- this
is just creating x and y

400
00:27:00,700 --> 00:27:05,630
and randomly initializing W's.

401
00:27:05,630 --> 00:27:09,550
Then we have the
forward pass, which

402
00:27:09,550 --> 00:27:16,000
means applying W's to the
inputs, layer by layer,

403
00:27:16,000 --> 00:27:24,870
and ultimately creating the
output, the predicted wise,

404
00:27:24,870 --> 00:27:29,610
and finally calculating
the loss function

405
00:27:29,610 --> 00:27:33,360
and outputting that loss value.

406
00:27:33,360 --> 00:27:39,240
After the forward pass, we need
an optimization process, a way

407
00:27:39,240 --> 00:27:43,260
to calculate the
analytical gradients

408
00:27:43,260 --> 00:27:48,420
and use those gradients
that are created

409
00:27:48,420 --> 00:27:53,920
to run gradient descent
to optimize W1 and W2,

410
00:27:53,920 --> 00:27:57,090
basically taking one step
towards the optimal value

411
00:27:57,090 --> 00:27:59,890
of the network.

412
00:27:59,890 --> 00:28:04,590
But this part, calculating
the analytical gradient,

413
00:28:04,590 --> 00:28:08,910
is the most important
part in here

414
00:28:08,910 --> 00:28:11,790
that we haven't
very much gone into.

415
00:28:11,790 --> 00:28:13,571
So this is the--

416
00:28:13,571 --> 00:28:15,410
almost the rest
of this lecture is

417
00:28:15,410 --> 00:28:21,600
about making this work and
scale in different settings.

418
00:28:21,600 --> 00:28:29,490
So after training and building
such a neural network,

419
00:28:29,490 --> 00:28:34,890
depending on how many nodes
we use in the hidden layer,

420
00:28:34,890 --> 00:28:36,920
you see that we can
identify, we can

421
00:28:36,920 --> 00:28:41,510
get different patterns of
separation between the two

422
00:28:41,510 --> 00:28:42,600
classes.

423
00:28:42,600 --> 00:28:45,920
And more neurons often
means more capacity

424
00:28:45,920 --> 00:28:51,470
to learn more complex
functions and better separation

425
00:28:51,470 --> 00:28:57,903
of the nodes, the points.

427
00:29:00,440 --> 00:29:05,100
If you take a look at this, this
is very much similar to this.

428
00:29:05,100 --> 00:29:07,820
This pattern I'm showing
here is similar to the one

429
00:29:07,820 --> 00:29:11,400
that I showed in the second
lecture, where we are talking

430
00:29:11,400 --> 00:29:13,140
about k nearest neighbor.

431
00:29:13,140 --> 00:29:20,790
And when we had only k equal
to 1, the one nearest neighbor

432
00:29:20,790 --> 00:29:25,600
framework, it was very much
similar to using more neurons.

433
00:29:25,600 --> 00:29:28,650
So same type of
arguments happen here

434
00:29:28,650 --> 00:29:33,510
that if we give a lot of
capacity to the network, then

435
00:29:33,510 --> 00:29:36,600
we will have some
overfitting problems.

436
00:29:36,600 --> 00:29:40,570
We won't be able to
generalize to unseen data.

437
00:29:40,570 --> 00:29:45,940
But there are many different
solutions for this as well.

438
00:29:45,940 --> 00:29:49,810
And one thing that--
as a rule of thumb,

439
00:29:49,810 --> 00:29:51,990
what I want to
highlight here for you

440
00:29:51,990 --> 00:29:56,800
is to not use the size of the
neural network as a regularizer.

441
00:29:56,800 --> 00:29:59,970
We don't often use that
as a hyperparameter

442
00:29:59,970 --> 00:30:02,500
to finetune this network size.

443
00:30:02,500 --> 00:30:06,720
Although, we experiment with
different values of the network

444
00:30:06,720 --> 00:30:11,040
size and related
hyperparameters.

445
00:30:11,040 --> 00:30:17,900
But what we often do is we go
with a little bit of a bigger

446
00:30:17,900 --> 00:30:19,350
network that we need.

447
00:30:19,350 --> 00:30:22,400
And then we use
the regularization

448
00:30:22,400 --> 00:30:25,310
and this regularizer,
and specifically,

449
00:30:25,310 --> 00:30:33,710
this regularization
hyperparameter

450
00:30:33,710 --> 00:30:35,250
to check the different setups.

451
00:30:35,250 --> 00:30:39,080
So what we often tune
is the regularization

452
00:30:39,080 --> 00:30:41,030
and regularization
hyperparameter and not

453
00:30:41,030 --> 00:30:45,380
necessarily the
network size itself.

454
00:30:45,380 --> 00:30:53,310
OK, this is the concept of
neural networks in a nutshell.

455
00:30:53,310 --> 00:30:58,160
But we have heard
about neural networks

456
00:30:58,160 --> 00:31:02,700
and how they could be
related to the biological--

457
00:31:02,700 --> 00:31:04,980
there are some
biological inspirations.

458
00:31:04,980 --> 00:31:07,990
So I'll talk a little bit about
it, but there is a question.

459
00:31:07,990 --> 00:31:13,930
Basically, your question is, why
is the model more underfitting

460
00:31:13,930 --> 00:31:16,550
when we increase the
value of lambda here?

461
00:31:16,550 --> 00:31:22,340
Yes, so just to quickly
answer that question,

462
00:31:22,340 --> 00:31:24,940
the value of lambda
is controlling

463
00:31:24,940 --> 00:31:27,250
how much contribution
the regularizer

464
00:31:27,250 --> 00:31:30,382
should have in the overall loss.

465
00:31:30,382 --> 00:31:35,870
And the larger contribution that
you have on the regularizer--

466
00:31:35,870 --> 00:31:39,140
and remember that regularizer
was defined on W's.

467
00:31:39,140 --> 00:31:41,360
So it's constraining the W's.

468
00:31:41,360 --> 00:31:45,530
It's giving you less freedom
to the values on W's.

469
00:31:45,530 --> 00:31:54,440
So less freedom equals a little
bit more generic boundaries,

470
00:31:54,440 --> 00:31:58,570
not necessarily giving
you those detailed values,

471
00:31:58,570 --> 00:32:01,670
or detailed parts
of the boundaries.

472
00:32:01,670 --> 00:32:06,860
So if you constrain the model
too much, even with regularizer,

473
00:32:06,860 --> 00:32:11,680
you're also going to
get values like that,

474
00:32:11,680 --> 00:32:13,810
decision boundaries like that.

475
00:32:13,810 --> 00:32:17,320
Yes, the right regularizer
always overfits--

476
00:32:17,320 --> 00:32:19,540
it prevents overfitting.

477
00:32:19,540 --> 00:32:21,830
Again, you are
creating a compromise,

478
00:32:21,830 --> 00:32:27,470
a balance between the loss, like
predicting the right output.

479
00:32:27,470 --> 00:32:30,530
So the first part of the loss
is predicting the right output.

480
00:32:30,530 --> 00:32:33,620
The second part is only playing
with the values of the weights,

481
00:32:33,620 --> 00:32:35,780
doesn't care about
the outputs anymore.

482
00:32:35,780 --> 00:32:37,870
If you overweight
this, you're not

483
00:32:37,870 --> 00:32:40,750
going to get very
good classifiers.

484
00:32:40,750 --> 00:32:44,000
So creating a balance,
regularizer is always good.

485
00:32:44,000 --> 00:32:46,870
But nothing is good if
you use too much of it.

487
00:32:53,440 --> 00:32:55,450
Can you go over
again why we would

488
00:32:55,450 --> 00:32:57,790
want to choose the
regularization rather

489
00:32:57,790 --> 00:32:59,980
than the size [INAUDIBLE]?

490
00:32:59,980 --> 00:33:03,000
So there are many
different reasons.

491
00:33:03,000 --> 00:33:05,010
One of them is size
of the network.

492
00:33:05,010 --> 00:33:06,300
You're building the networks.

493
00:33:06,300 --> 00:33:08,060
You're going to
build networks that,

494
00:33:08,060 --> 00:33:12,470
sometimes, that you have
to run them for few days

495
00:33:12,470 --> 00:33:14,840
to get some results.

496
00:33:14,840 --> 00:33:20,990
So networks, what
we often do is,

497
00:33:20,990 --> 00:33:24,710
we start increasing the
number of parameters

498
00:33:24,710 --> 00:33:29,390
in networks, until we see
some levels of overfitting.

499
00:33:29,390 --> 00:33:33,200
So that's the time that we know
that the network is actually

500
00:33:33,200 --> 00:33:35,360
understanding the
patterns in the data

501
00:33:35,360 --> 00:33:39,080
and is now able to
memorize the data.

502
00:33:39,080 --> 00:33:43,070
And that's the time that we
try to minimize the overfitting

503
00:33:43,070 --> 00:33:45,750
by regularizing the network.

504
00:33:45,750 --> 00:33:48,840
So regularization plays
an important factor there.

505
00:33:48,840 --> 00:33:54,080
So if we go too high on the
number of parameters, number

506
00:33:54,080 --> 00:33:56,900
of complexity of the
network, then that's

507
00:33:56,900 --> 00:33:58,530
going to be causing a problem.

508
00:33:58,530 --> 00:34:00,600
We never often do that.

509
00:34:00,600 --> 00:34:05,020
We often, for a new problem,
start with smaller networks

510
00:34:05,020 --> 00:34:09,250
and increase that after with--

511
00:34:09,250 --> 00:34:11,530
correct that with
the regularizer.

512
00:34:11,530 --> 00:34:14,710
For a given problem,
how do we know

513
00:34:14,710 --> 00:34:19,190
how many neurons we need
to solve the problem?

514
00:34:19,190 --> 00:34:25,929
That's based on
empirical research work

515
00:34:25,929 --> 00:34:29,590
and looking at other
similar type of-- there

516
00:34:29,590 --> 00:34:31,880
is no one prescription for all.

517
00:34:31,880 --> 00:34:36,530
You have to look at
other counterparts,

518
00:34:36,530 --> 00:34:40,600
other types of networks that
were trained on similar data,

519
00:34:40,600 --> 00:34:42,500
start from that range.

520
00:34:42,500 --> 00:34:46,960
And then often, you do
a number of experiments

521
00:34:46,960 --> 00:34:49,480
yourself to balance and
increase or decrease

522
00:34:49,480 --> 00:34:50,870
the complexity of the network.

523
00:34:50,870 --> 00:34:58,300
So it's often and always pretty
much bound to exploration.

524
00:34:58,300 --> 00:35:02,370
So your question is, are there
any theoretical and foundational

525
00:35:02,370 --> 00:35:07,090
work done to see
which ones to use,

526
00:35:07,090 --> 00:35:10,120
which activation functions to
use, and how many layers to use?

527
00:35:10,120 --> 00:35:13,710
There are so many
research and papers

528
00:35:13,710 --> 00:35:16,860
out analyzing these
and also some methods

529
00:35:16,860 --> 00:35:22,980
for optimizing all of these
meta or hyperparameters

530
00:35:22,980 --> 00:35:24,700
of the networks.

531
00:35:24,700 --> 00:35:27,690
We are not going to
get into them in detail

532
00:35:27,690 --> 00:35:29,940
because, again, a
big part of it is

533
00:35:29,940 --> 00:35:33,720
based on-- very much dependent
on the data set, the problem

534
00:35:33,720 --> 00:35:34,840
you're solving.

535
00:35:34,840 --> 00:35:39,570
And so the best answer
to your question

536
00:35:39,570 --> 00:35:41,710
is, yes, there are
some works out there.

537
00:35:41,710 --> 00:35:45,750
But again, each of
those make assumptions

538
00:35:45,750 --> 00:35:50,130
that may not be necessarily
true for your application,

539
00:35:50,130 --> 00:35:51,600
or problem.

540
00:35:51,600 --> 00:35:58,380
So what happens is, there are
some biological inspirations.

541
00:35:58,380 --> 00:36:01,350
Again, these inspirations
are very much very loose.

542
00:36:01,350 --> 00:36:03,500
If there is a
neuroscientist sitting here,

543
00:36:03,500 --> 00:36:10,280
or is watching online, do
not take all of the examples

544
00:36:10,280 --> 00:36:17,640
that I'm giving you, or talking
about as the ground truth.

545
00:36:17,640 --> 00:36:22,010
But generally, what
happens in neurons--

546
00:36:22,010 --> 00:36:26,970
and this is a
visualization of a neuron--

547
00:36:26,970 --> 00:36:32,450
it does have a cell body
that often aggregates

548
00:36:32,450 --> 00:36:39,440
the impulses carried through
dendrites to the cell body

549
00:36:39,440 --> 00:36:40,980
itself, cell body.

550
00:36:40,980 --> 00:36:44,270
And then using
axons, those impulses

551
00:36:44,270 --> 00:36:47,760
are carried away
to other neurons.

552
00:36:47,760 --> 00:36:50,360
This is very much
similar to what we are

553
00:36:50,360 --> 00:36:52,410
doing in our neural networks.

554
00:36:52,410 --> 00:36:56,290
We often have a
function that captures

555
00:36:56,290 --> 00:37:04,610
the signals, all of
the previous impulses,

556
00:37:04,610 --> 00:37:07,430
activations from
the previous layers.

557
00:37:07,430 --> 00:37:12,040
And in the cell
body, that function

558
00:37:12,040 --> 00:37:20,080
is operated on the inputs
and outputs the activations

559
00:37:20,080 --> 00:37:24,050
and passes them to the
next layer, next neuron.

560
00:37:24,050 --> 00:37:27,130
And that's, basically, why we
need some sort of activation

561
00:37:27,130 --> 00:37:32,170
function here to create
the impulses to increase,

562
00:37:32,170 --> 00:37:36,130
or decrease the values.

563
00:37:36,130 --> 00:37:42,010
So with that, again,
there are many differences

564
00:37:42,010 --> 00:37:45,310
between biological neurons
and how they could actually

565
00:37:45,310 --> 00:37:50,470
be very more complex
than what neural networks

566
00:37:50,470 --> 00:37:52,670
we build look like.

567
00:37:52,670 --> 00:37:58,560
But generally, there
are common concepts.

568
00:37:58,560 --> 00:38:00,450
Often, the neural
networks that we build

569
00:38:00,450 --> 00:38:03,520
are organized into
regular patterns.

570
00:38:03,520 --> 00:38:05,850
And those patterns
are because we

571
00:38:05,850 --> 00:38:09,090
want to have better
computational efficiency when we

572
00:38:09,090 --> 00:38:12,490
implement the neural networks.

573
00:38:12,490 --> 00:38:14,760
Although there has
been research creating

574
00:38:14,760 --> 00:38:18,340
these complex neural networks
and trying to optimize,

575
00:38:18,340 --> 00:38:22,470
but again, in terms
of results, they

576
00:38:22,470 --> 00:38:27,510
are almost comparable with
the regular functions,

577
00:38:27,510 --> 00:38:30,210
regular neural networks
that we often build

578
00:38:30,210 --> 00:38:33,213
and we'll be talking
about in this class.

580
00:38:36,150 --> 00:38:42,480
I can't warn you enough on
being careful with your brain

581
00:38:42,480 --> 00:38:48,310
analogies and how this
could be interpreted.

582
00:38:48,310 --> 00:38:49,860
So there are so
many differences.

583
00:38:49,860 --> 00:38:54,710
And I'll just stop
here and would

584
00:38:54,710 --> 00:38:56,780
be happy to discuss
if anybody was

585
00:38:56,780 --> 00:39:01,170
interested in the neuroscience
aspect of things as well.

586
00:39:01,170 --> 00:39:08,660
So plugging everything in, we
did have a scoring function.

587
00:39:08,660 --> 00:39:12,410
This scoring function
turns the inputs

588
00:39:12,410 --> 00:39:19,230
through some W's weight vectors,
or weight matrices into scores.

589
00:39:19,230 --> 00:39:27,200
And what we often use as the
loss function for the network

590
00:39:27,200 --> 00:39:31,070
is using those scores either
through hinge loss, or softmax,

591
00:39:31,070 --> 00:39:33,570
or other variations.

592
00:39:33,570 --> 00:39:39,470
And in addition to that,
we defined regularizers,

593
00:39:39,470 --> 00:39:44,360
which ultimately give us
the total loss of the data

594
00:39:44,360 --> 00:39:47,610
loss plus regularizer.

595
00:39:47,610 --> 00:39:51,610
And we talked about
this fact that in order

596
00:39:51,610 --> 00:39:56,210
to be able to
optimize W1 and W2,

597
00:39:56,210 --> 00:40:00,070
what we need is
to be able to take

598
00:40:00,070 --> 00:40:03,430
the derivative of, the partial
derivative of L with respect

599
00:40:03,430 --> 00:40:11,000
to W1 and W2, partial
L by W1 and W2.

600
00:40:11,000 --> 00:40:13,930
There are so many
different details

601
00:40:13,930 --> 00:40:18,830
that we have to be aware of.

602
00:40:18,830 --> 00:40:24,370
First, building these functions
and then taking the derivatives

603
00:40:24,370 --> 00:40:27,890
and writing them down
is often tedious.

604
00:40:27,890 --> 00:40:30,250
There are lots of
matrix calculations

605
00:40:30,250 --> 00:40:33,520
and need a lot of
work on the paper

606
00:40:33,520 --> 00:40:36,230
before you can actually
implement a neural network.

607
00:40:36,230 --> 00:40:38,020
The other challenge,
the other problem

608
00:40:38,020 --> 00:40:42,370
is, what if you want to
change the loss slightly

609
00:40:42,370 --> 00:40:46,180
different from what we
have done in the paper, all

610
00:40:46,180 --> 00:40:48,910
of the calculations over?

611
00:40:48,910 --> 00:40:55,840
So in that case, again, we
have to redo the entire thing.

612
00:40:55,840 --> 00:41:02,520
And finally, this becomes
intractable and, sometimes,

613
00:41:02,520 --> 00:41:06,310
infeasible if the loss
function is complex.

614
00:41:06,310 --> 00:41:12,270
So with complex functions,
that's going to be even harder.

615
00:41:12,270 --> 00:41:15,390
But there is a better idea,
something that is often

616
00:41:15,390 --> 00:41:18,970
used in our implementations.

617
00:41:18,970 --> 00:41:22,200
And I'm going to go
into a few examples

618
00:41:22,200 --> 00:41:25,650
today just to make sure
everybody is on the same page

619
00:41:25,650 --> 00:41:28,680
and understands these topics.

620
00:41:28,680 --> 00:41:35,670
And that is computational graphs
and the idea of backpropagation.

621
00:41:35,670 --> 00:41:40,650
Computational graphs
are putting together

622
00:41:40,650 --> 00:41:43,470
all of the operations
in the neural network

623
00:41:43,470 --> 00:41:48,150
and creating that
step-by-step thing

624
00:41:48,150 --> 00:41:52,050
and start from the inputs
and all of the parameters

625
00:41:52,050 --> 00:41:55,410
that are basically
needed and get the loss

626
00:41:55,410 --> 00:41:59,080
as the final
output, final layer.

627
00:41:59,080 --> 00:42:02,200
So in this case, we
had a loss function,

628
00:42:02,200 --> 00:42:04,140
which could be a
softmax function,

629
00:42:04,140 --> 00:42:05,740
or a hinge loss function.

630
00:42:05,740 --> 00:42:07,870
Whatever it is, it's
the loss function,

631
00:42:07,870 --> 00:42:14,430
which is added to the
regularizer, the function RW.

632
00:42:14,430 --> 00:42:18,370
And R has W as its input.

633
00:42:18,370 --> 00:42:23,490
So these two added together
calculate, or create the loss.

634
00:42:23,490 --> 00:42:28,780
And before doing the, or
having the loss calculated,

635
00:42:28,780 --> 00:42:34,630
we often also need to aggregate
x and W and create the score.

636
00:42:34,630 --> 00:42:37,620
This is a
multiplication function.

637
00:42:37,620 --> 00:42:41,040
This is actually very
useful because most

638
00:42:41,040 --> 00:42:43,500
of the neural networks
that we build are also

639
00:42:43,500 --> 00:42:45,760
they have graphical
card representations.

640
00:42:45,760 --> 00:42:48,310
And all of these
complex functions

641
00:42:48,310 --> 00:42:52,430
could be shown with
the same framework.

642
00:42:52,430 --> 00:42:58,090
And then we can use this and
build their computation graph,

643
00:42:58,090 --> 00:43:01,160
starting from input
image or input data.

644
00:43:01,160 --> 00:43:03,920
There are a bunch of weights
throughout the network.

645
00:43:03,920 --> 00:43:06,103
And finally, there
is the loss function.

647
00:43:09,730 --> 00:43:13,360
And again, this is
useful because there

648
00:43:13,360 --> 00:43:18,130
are some complex neural
networks, like this neural

649
00:43:18,130 --> 00:43:19,220
Turing machine.

650
00:43:19,220 --> 00:43:23,960
And that is actually used for
temporal and sequential data.

651
00:43:23,960 --> 00:43:27,230
So there is a lot of
unrolling of this machine.

652
00:43:27,230 --> 00:43:30,980
And if we have to do all of
the work manually by hand,

653
00:43:30,980 --> 00:43:39,382
this is going to be
intractable and not feasible.

654
00:43:39,382 --> 00:43:44,620
And that's why when we build
this computational graph,

655
00:43:44,620 --> 00:43:49,830
the solution to that
is backpropagation.

656
00:43:49,830 --> 00:43:53,320
And I want to start with
a very simple example.

657
00:43:53,320 --> 00:43:59,460
So we start with a
function f of x, y, and z,

658
00:43:59,460 --> 00:44:03,220
which is x plus y
multiplied by z.

659
00:44:03,220 --> 00:44:09,180
And if I draw the computational
graph for this function,

660
00:44:09,180 --> 00:44:15,210
you see we have an operation,
which is the addition

661
00:44:15,210 --> 00:44:17,440
operation between x and y.

662
00:44:17,440 --> 00:44:20,970
And then we have
a multiplication

663
00:44:20,970 --> 00:44:29,100
between that addition of
x and y multiplied by z.

664
00:44:29,100 --> 00:44:35,560
So given an input setup of x
equal to minus 2, y equal to 5,

665
00:44:35,560 --> 00:44:39,300
and z equal to minus
4, now actually,

666
00:44:39,300 --> 00:44:41,550
we can make all of
the calculations

667
00:44:41,550 --> 00:44:47,310
for and do this,
the forward pass,

668
00:44:47,310 --> 00:44:50,350
stepping forward in
the neural network.

669
00:44:50,350 --> 00:44:56,670
The first step is adding
x and y, which gives us 3.

670
00:44:56,670 --> 00:44:59,970
And in order to be able
to understand the steps

671
00:44:59,970 --> 00:45:02,950
and step-by-step, I'm
giving a name to it.

672
00:45:02,950 --> 00:45:06,850
So q equals x plus y.

673
00:45:06,850 --> 00:45:12,150
And if I want to calculate
the partial derivatives of q

674
00:45:12,150 --> 00:45:14,550
with respect to
both x and y, it's

675
00:45:14,550 --> 00:45:20,730
very simple because we have
the formulation here between q

676
00:45:20,730 --> 00:45:22,330
and x and y.

677
00:45:22,330 --> 00:45:24,220
The formulation is there.

678
00:45:24,220 --> 00:45:30,270
The derivatives, the partial q
by x equals 1 and partial q by y

679
00:45:30,270 --> 00:45:33,160
equals 1 as well.

680
00:45:33,160 --> 00:45:35,830
So this is a simple setup.

681
00:45:35,830 --> 00:45:37,480
We know it exists.

682
00:45:37,480 --> 00:45:40,000
So just keep it in
the back of our minds.

683
00:45:40,000 --> 00:45:46,820
Then the second operation is
f equals q multiplied by z.

684
00:45:46,820 --> 00:45:48,980
Again, since we
have this function,

685
00:45:48,980 --> 00:45:54,490
it's very easy to write
the partial derivatives.

686
00:45:54,490 --> 00:45:56,710
Partial f by q equals z.

687
00:45:56,710 --> 00:46:01,490
And f by z equals q.

688
00:46:01,490 --> 00:46:04,305
So it's swap between z and q.

690
00:46:07,900 --> 00:46:09,640
I'm hoping that
everybody knows all

691
00:46:09,640 --> 00:46:11,600
of these from linear algebra.

692
00:46:11,600 --> 00:46:14,830
So if you don't, you
should definitely

693
00:46:14,830 --> 00:46:18,490
check it out and remind yourself
because these are actually

694
00:46:18,490 --> 00:46:23,860
very, very important
algebra in general

695
00:46:23,860 --> 00:46:26,930
for the rest of the quarter.

696
00:46:26,930 --> 00:46:30,860
What we want and what
we need in this setup,

697
00:46:30,860 --> 00:46:34,070
and to complete this
example of backpropagation,

698
00:46:34,070 --> 00:46:37,930
we need the partial
derivative of f with respect

699
00:46:37,930 --> 00:46:41,753
to x, y, and z.

701
00:46:44,400 --> 00:46:47,730
How we start and how our
backpropagation implements

702
00:46:47,730 --> 00:46:50,580
this is to start at
the front of the net--

703
00:46:50,580 --> 00:46:52,480
at the end of the network.

704
00:46:52,480 --> 00:46:58,290
And we start going
back, backpropagating

705
00:46:58,290 --> 00:47:00,520
all of the gradients.

706
00:47:00,520 --> 00:47:04,920
And this is basically
a recursive process

707
00:47:04,920 --> 00:47:07,200
that will be running.

708
00:47:07,200 --> 00:47:15,450
So derivative of f with
respect to f is what?

709
00:47:15,450 --> 00:47:17,650
It's the thing with
respect to itself.

710
00:47:17,650 --> 00:47:23,100
So it's always the last
part, the derivative

711
00:47:23,100 --> 00:47:28,270
of loss function with respect
to itself is always 1.

712
00:47:28,270 --> 00:47:36,070
If I want to backprop, the
most immediate one is z.

713
00:47:36,070 --> 00:47:39,720
You can see here that we have z.

714
00:47:39,720 --> 00:47:44,510
And for this one, if I calculate
the derivative of f with respect

715
00:47:44,510 --> 00:47:47,540
to z, we already have it.

716
00:47:47,540 --> 00:47:50,730
F with respect to
z is equal to q.

717
00:47:50,730 --> 00:48:00,020
So whatever the value of q is
goes to this as the gradient

718
00:48:00,020 --> 00:48:01,400
as well.

719
00:48:01,400 --> 00:48:03,880
Next, we have q.

720
00:48:03,880 --> 00:48:08,700
Q is the next one-- our next one
that is directly connected to f.

721
00:48:08,700 --> 00:48:12,980
So this is also easy to compute
because we have derivative

722
00:48:12,980 --> 00:48:14,310
of f with respect to q.

723
00:48:14,310 --> 00:48:18,620
We have also already calculated
that it's equal to z.

724
00:48:18,620 --> 00:48:23,870
Whatever z is, that's the value
of derivative here minus 4.

725
00:48:23,870 --> 00:48:29,570
Next, we have y, which
is directly before q.

726
00:48:29,570 --> 00:48:33,650
And we know that y and f,
although we need derivative

727
00:48:33,650 --> 00:48:37,170
of f with respect
to y, but y and f

728
00:48:37,170 --> 00:48:39,220
are not directly connected.

729
00:48:39,220 --> 00:48:44,640
And that's where we use
the chain rule, where

730
00:48:44,640 --> 00:48:51,840
we split the calculation
of derivatives with respect

731
00:48:51,840 --> 00:48:53,950
to the variable in the middle.

732
00:48:53,950 --> 00:49:01,510
So partial f by y equals
to partial f by q, q by y.

733
00:49:01,510 --> 00:49:09,580
So this is how the chain rule
could be written in this case.

734
00:49:09,580 --> 00:49:13,890
And now, I want to introduce
you to two important new terms--

735
00:49:13,890 --> 00:49:16,660
local gradient and
upstream gradient.

736
00:49:16,660 --> 00:49:18,930
Upstream gradient is
often the gradient

737
00:49:18,930 --> 00:49:27,930
that comes from the end of the
network to this current node

738
00:49:27,930 --> 00:49:29,140
that we are in.

739
00:49:29,140 --> 00:49:35,270
And then the local
gradient is the gradient

740
00:49:35,270 --> 00:49:39,380
of the nodes, what
the input of the node

741
00:49:39,380 --> 00:49:45,110
is, with a gradient of
its output with respect

742
00:49:45,110 --> 00:49:46,200
to its input.

743
00:49:46,200 --> 00:49:48,240
So it's the local gradient.

744
00:49:48,240 --> 00:49:53,420
So defining these is actually
not too hard because f by q,

745
00:49:53,420 --> 00:49:55,010
you already have the value.

746
00:49:55,010 --> 00:49:59,310
Q by y, we also
already have the value.

747
00:49:59,310 --> 00:50:06,900
So it's 1 multiplied by z, and
the value will become minus 4.

748
00:50:06,900 --> 00:50:13,860
Same story, and it's for
the other variable, x.

749
00:50:13,860 --> 00:50:18,500
Here, the local
gradient upstream

750
00:50:18,500 --> 00:50:22,830
could, again, be written
down with this chain rule.

751
00:50:22,830 --> 00:50:29,810
And it also results in
minus 4 and gives us--

752
00:50:29,810 --> 00:50:36,400
because in both cases, the
gradient with respect to x or y

753
00:50:36,400 --> 00:50:38,390
was already 1.

754
00:50:38,390 --> 00:50:41,050
So both of them
get the same value.

755
00:50:41,050 --> 00:50:45,380
So with this computational setup
and the computational graph,

756
00:50:45,380 --> 00:50:50,020
it becomes very easy to
modularize what we want to do.

757
00:50:50,020 --> 00:50:54,790
For every single node
in the neural network,

758
00:50:54,790 --> 00:50:58,670
having x and y as
input, or whatever else,

759
00:50:58,670 --> 00:51:06,070
and z as the output,
what we need are, first,

760
00:51:06,070 --> 00:51:08,410
the local gradients.

761
00:51:08,410 --> 00:51:11,180
Which we can always--
we have the function f.

762
00:51:11,180 --> 00:51:13,520
It's a function of x and y.

763
00:51:13,520 --> 00:51:19,520
So the gradient of output with
respect to each of the inputs,

764
00:51:19,520 --> 00:51:22,730
it's easy to calculate
for every single node.

765
00:51:22,730 --> 00:51:27,220
And what we need to be
able to backpropagate

766
00:51:27,220 --> 00:51:30,280
is the upstream gradient.

767
00:51:30,280 --> 00:51:33,450
And the backpropagation
process is giving us

768
00:51:33,450 --> 00:51:37,210
the power to get this upstream
gradient as step-by-step.

769
00:51:37,210 --> 00:51:39,750
So when we are at
this node, we also

770
00:51:39,750 --> 00:51:41,880
have the upstream
gradient already

771
00:51:41,880 --> 00:51:45,700
calculated from
the future nodes.

772
00:51:45,700 --> 00:51:51,630
And that's what we need.

773
00:51:51,630 --> 00:51:54,330
What we can do
after this is just

774
00:51:54,330 --> 00:51:58,650
to multiply the upstream
gradient with the local gradient

775
00:51:58,650 --> 00:52:03,520
and create what now we
call downstream gradients.

776
00:52:03,520 --> 00:52:05,250
So the downstream
gradients are going

777
00:52:05,250 --> 00:52:08,530
to be upstream gradients
for the previous layers.

778
00:52:08,530 --> 00:52:11,650
So that's how we
calculate that for x,

779
00:52:11,650 --> 00:52:15,660
same story when it comes to y.

780
00:52:15,660 --> 00:52:19,710
So this whole process
gives us the power

781
00:52:19,710 --> 00:52:23,250
to create and calculate
all of these completely

782
00:52:23,250 --> 00:52:26,340
locally and a
step-by-step go backwards

783
00:52:26,340 --> 00:52:28,460
and give it to
the previous nodes

784
00:52:28,460 --> 00:52:30,840
so they can continue
their process.

785
00:52:30,840 --> 00:52:36,770
So again, this is one of the
most fundamental operations

786
00:52:36,770 --> 00:52:41,570
in all of neural networks
and many optimization

787
00:52:41,570 --> 00:52:47,000
processes involving multiple
layers of information.

788
00:52:47,000 --> 00:52:50,040
If I understand the question
correctly, you're saying,

789
00:52:50,040 --> 00:52:51,740
how can we understand
this, intuitively,

790
00:52:51,740 --> 00:52:53,070
what the gradients are doing?

791
00:52:53,070 --> 00:52:58,010
So let's take one
step back and see

792
00:52:58,010 --> 00:53:00,270
why we are here to begin with.

793
00:53:00,270 --> 00:53:03,650
What we needed was to
identify to calculate

794
00:53:03,650 --> 00:53:06,050
the gradients of
the loss function

795
00:53:06,050 --> 00:53:09,690
with respect to W1 and
W2 and W's in general,

796
00:53:09,690 --> 00:53:15,920
to be able to take a step
in the negative direction

797
00:53:15,920 --> 00:53:17,960
of, in the opposite
direction of the gradients

798
00:53:17,960 --> 00:53:21,750
to be able to find the
optimal value, optimal loss.

799
00:53:21,750 --> 00:53:26,980
So in order to do that,
we need gradient of L loss

800
00:53:26,980 --> 00:53:28,850
with respect to everything.

801
00:53:28,850 --> 00:53:33,220
So what we are doing is, we
are just moving gradient of L

802
00:53:33,220 --> 00:53:35,720
with respect to all
variables in the network,

803
00:53:35,720 --> 00:53:39,380
back to every single
value of the network,

804
00:53:39,380 --> 00:53:42,010
without sitting down
and writing the function

805
00:53:42,010 --> 00:53:43,190
for the entire network.

806
00:53:43,190 --> 00:53:46,698
If the network has
100 layers, we're

807
00:53:46,698 --> 00:53:49,240
not going to be sitting down
and writing the function for all

808
00:53:49,240 --> 00:53:51,020
of the 100 layers separately.

809
00:53:51,020 --> 00:53:53,590
This is how we
backpropagate step-by-step

810
00:53:53,590 --> 00:53:57,130
to get the values that we
need for that optimization

811
00:53:57,130 --> 00:54:00,190
process of every single weight
that is going to be incorporated

812
00:54:00,190 --> 00:54:02,380
in the network.

813
00:54:02,380 --> 00:54:11,320
OK, another example,
so this is a little bit

814
00:54:11,320 --> 00:54:15,860
more complex function, a
function of weights and x.

815
00:54:15,860 --> 00:54:20,440
And we have 1 over 1 plus e to
the power of linear combination

816
00:54:20,440 --> 00:54:23,850
of x and w.

817
00:54:23,850 --> 00:54:30,460
So there are a bunch of
multiplications, additions,

818
00:54:30,460 --> 00:54:36,180
negation, and the exp
function, and ultimately,

819
00:54:36,180 --> 00:54:40,510
the 1 over whatever we
calculated function.

820
00:54:40,510 --> 00:54:47,070
So with all of those, let's look
at this example that we have,

821
00:54:47,070 --> 00:54:52,840
specific values for
W0, x0, W1, x1, and W2.

822
00:54:52,840 --> 00:54:56,980
With these given values,
we can do the forward pass,

823
00:54:56,980 --> 00:55:02,050
calculate every single value
that we have in this process.

824
00:55:02,050 --> 00:55:05,940
And just to remind
you, we do have some

825
00:55:05,940 --> 00:55:08,220
of the details, some of the--

826
00:55:08,220 --> 00:55:13,840
we know for an exp function,
e to the power of x,

827
00:55:13,840 --> 00:55:16,710
what its derivative
is with respect to x

828
00:55:16,710 --> 00:55:19,570
constant multiplication.

829
00:55:19,570 --> 00:55:21,990
Always, the derivative is
the constant value itself,

830
00:55:21,990 --> 00:55:27,500
1 over x, as a derivative
of minus 1 over x2.

831
00:55:27,500 --> 00:55:30,726
These are, again, what
we know from algebra.

832
00:55:30,726 --> 00:55:34,130
And if it's a constant
addition, it's always,

833
00:55:34,130 --> 00:55:36,900
the derivative is equal to 1.

834
00:55:36,900 --> 00:55:41,330
So as I said, always
in the very beginning

835
00:55:41,330 --> 00:55:45,320
at the end of the
network, the derivative

836
00:55:45,320 --> 00:55:48,960
of L with respect to L
is always equal to 1.

837
00:55:48,960 --> 00:55:54,050
So that's where we
start using this rule,

838
00:55:54,050 --> 00:55:56,850
the derivative of function 1/x.

839
00:55:56,850 --> 00:55:59,630
Now, we can
calculate-- upstream,

840
00:55:59,630 --> 00:56:02,330
I said, it's 1
always at the end.

841
00:56:02,330 --> 00:56:07,470
The local gradient could
be minus 1 over x2.

842
00:56:07,470 --> 00:56:09,170
What is the value of x?

843
00:56:09,170 --> 00:56:10,440
Whatever the input is.

844
00:56:10,440 --> 00:56:16,650
So this calculation
results in minus 0.53.

845
00:56:16,650 --> 00:56:20,370
So minus 0.53 is the
downstream gradient,

846
00:56:20,370 --> 00:56:22,960
which defines the upstream
gradient for the next one.

847
00:56:22,960 --> 00:56:25,570
And in the next,
again, the function

848
00:56:25,570 --> 00:56:28,160
here is just the
constant addition,

849
00:56:28,160 --> 00:56:32,780
where we know that the
local gradient equals to 1.

850
00:56:32,780 --> 00:56:39,010
So 1 multiplied by upstream
gradient, same value, goes back.

851
00:56:39,010 --> 00:56:43,820
And next step is
the exp function.

852
00:56:43,820 --> 00:56:48,230
So for that, again,
the upstream,

853
00:56:48,230 --> 00:56:49,750
we already have the value.

854
00:56:49,750 --> 00:56:54,770
For the local gradient,
it's e to the power of x.

855
00:56:54,770 --> 00:56:55,390
What is x?

856
00:56:55,390 --> 00:56:59,390
The input of this step minus 1.

857
00:56:59,390 --> 00:57:03,860
So calculating this
will give us minus 0.2.

858
00:57:03,860 --> 00:57:08,180
And this goes back
to the next step.

859
00:57:08,180 --> 00:57:11,950
Here, we, again,
have a multiplication

860
00:57:11,950 --> 00:57:16,030
with a constant number, which
defines the local gradient

861
00:57:16,030 --> 00:57:21,220
equal to that number,
or that constant value,

862
00:57:21,220 --> 00:57:26,290
and defining the new
gradient, downstream gradient.

863
00:57:26,290 --> 00:57:30,510
And going back, now here, we
have an addition function,

864
00:57:30,510 --> 00:57:37,350
where we are getting some
data, some information-- sorry,

865
00:57:37,350 --> 00:57:42,610
two inputs of the
different values here.

866
00:57:42,610 --> 00:57:45,660
And again, if you want to
calculate the upstream gradient,

867
00:57:45,660 --> 00:57:46,320
it's 0.2.

868
00:57:46,320 --> 00:57:47,490
Already, we have it.

869
00:57:47,490 --> 00:57:54,720
The downstream, the local
gradients will be equal to 1

870
00:57:54,720 --> 00:57:57,240
because it's just an
addition between two values.

871
00:57:57,240 --> 00:58:00,180
And an addition,
the derivative of x

872
00:58:00,180 --> 00:58:04,660
plus y with respect to
both x and y is always 1.

873
00:58:04,660 --> 00:58:06,960
So both inputs will be the same.

874
00:58:06,960 --> 00:58:07,920
Then

875
00:58:07,920 --> 00:58:11,070
We have multiplication
operations

876
00:58:11,070 --> 00:58:14,470
with multiplication
upstream gradient.

877
00:58:14,470 --> 00:58:15,930
Again, we have the values.

878
00:58:15,930 --> 00:58:21,500
And the local gradients with
respect to a multiplication

879
00:58:21,500 --> 00:58:24,870
is, if we always have,
say, for example,

880
00:58:24,870 --> 00:58:28,880
a multiplied by x, the
derivative of this with respect

881
00:58:28,880 --> 00:58:31,850
to x is always the
other variable.

882
00:58:31,850 --> 00:58:37,880
So here, for the first
one, it's minus 1,

883
00:58:37,880 --> 00:58:39,240
which is the value of x.

884
00:58:39,240 --> 00:58:42,720
And for the second one, it's
2, which is the value of w.

885
00:58:42,720 --> 00:58:46,140
So the other variable,
whatever the value it has.

886
00:58:46,140 --> 00:58:50,000
So with that, we can
calculate everything and then

887
00:58:50,000 --> 00:58:54,060
also calculate the ones
with respect to W1 and x1.

888
00:58:54,060 --> 00:58:56,820
Again, we made all of
these calculations,

889
00:58:56,820 --> 00:59:01,970
so we can identify
how much W should

890
00:59:01,970 --> 00:59:08,810
be changed in order to step
towards the optimal point

891
00:59:08,810 --> 00:59:10,790
in the network.

892
00:59:10,790 --> 00:59:13,350
So this was another example.

893
00:59:13,350 --> 00:59:19,490
There are so many different ways
to draw a computational graph.

894
00:59:19,490 --> 00:59:21,590
This was not the only
one that I explained.

895
00:59:21,590 --> 00:59:24,400
So we can actually lump
all of the functions

896
00:59:24,400 --> 00:59:28,510
together and define a sigmoid
because this is basically

897
00:59:28,510 --> 00:59:31,310
a sigmoid of a linear function.

898
00:59:31,310 --> 00:59:33,770
So the linear function
could be here.

899
00:59:33,770 --> 00:59:38,150
And then all of these operations
could be defined as sigmoid.

900
00:59:38,150 --> 00:59:41,080
And actually, sigmoid is
interesting and very useful

901
00:59:41,080 --> 00:59:46,330
to use because the local
gradient using sigmoid

902
00:59:46,330 --> 00:59:48,530
is dependent on sigmoid itself.

903
00:59:48,530 --> 00:59:53,750
So the local gradient of sigmoid
with respect to the variable x,

904
00:59:53,750 --> 00:59:56,600
if we do the calculations
and simplify,

905
00:59:56,600 --> 01:00:01,900
it's 1 minus sigmoid multiplied
by the sigmoid of the same x.

906
01:00:01,900 --> 01:00:10,030
So it's actually a very useful
framework, useful function,

907
01:00:10,030 --> 01:00:10,910
and easy.

908
01:00:10,910 --> 01:00:18,040
And in order to calculate the
downstream gradient, again,

909
01:00:18,040 --> 01:00:21,100
what the upstream
gradient was, was value 1.

910
01:00:21,100 --> 01:00:24,270
And if I calculate the
local gradient, which

911
01:00:24,270 --> 01:00:28,240
is this function, replacing
x with what the input was,

912
01:00:28,240 --> 01:00:34,050
which is 1, it's how this will
be calculated multiplied by 1

913
01:00:34,050 --> 01:00:35,290
will be 0.2.

914
01:00:35,290 --> 01:00:37,170
Which is actually
the exact same value

915
01:00:37,170 --> 01:00:42,150
that we had from before
doing it separately.

916
01:00:42,150 --> 01:00:47,490
I want to summarize
and say that there

917
01:00:47,490 --> 01:00:55,980
are few patterns in the data,
often very much in for the nodes

918
01:00:55,980 --> 01:00:59,970
that we can actually memorize.

919
01:00:59,970 --> 01:01:02,590
There is add gate
for the add gate.

920
01:01:02,590 --> 01:01:05,850
It's always a
gradient distributor.

921
01:01:05,850 --> 01:01:10,770
Because of the properties of
addition that I explained,

922
01:01:10,770 --> 01:01:18,710
the gradients will remain the
same as whatever its input is.

923
01:01:18,710 --> 01:01:23,850
For the multiplication
gate, it's a swap function.

924
01:01:23,850 --> 01:01:27,890
Again, I told you, gradient
of xy with respect to x

925
01:01:27,890 --> 01:01:30,090
is y, with respect to y is x.

926
01:01:30,090 --> 01:01:31,710
So it's a swap.

927
01:01:31,710 --> 01:01:37,040
And then there is copy gate.

928
01:01:37,040 --> 01:01:41,000
And the copy gate, the
operation that happens

929
01:01:41,000 --> 01:01:47,390
is just an addition of what
is coming to the network--

930
01:01:47,390 --> 01:01:49,980
to the node, or the gate.

931
01:01:49,980 --> 01:01:53,600
And then ultimately, there is
a max gate, which is actually

932
01:01:53,600 --> 01:01:55,910
something that we use
quite often, very much

933
01:01:55,910 --> 01:01:58,010
similar to the ReLU function.

934
01:01:58,010 --> 01:02:02,030
That max gate has
the gradient of--

935
01:02:02,030 --> 01:02:06,390
because it's taking a
max between its inputs.

936
01:02:06,390 --> 01:02:11,080
So whichever the max
value was, you just

937
01:02:11,080 --> 01:02:15,880
route the gradients
towards that direction.

938
01:02:15,880 --> 01:02:21,520
So with that, it's very
simple to now implement

939
01:02:21,520 --> 01:02:25,880
a neural network, forward
pass, compute all of the steps.

940
01:02:25,880 --> 01:02:27,610
And then in the
backward pass, we

941
01:02:27,610 --> 01:02:29,480
start computing the gradients.

942
01:02:29,480 --> 01:02:32,110
In a step-by-step, I
explained that the gradient

943
01:02:32,110 --> 01:02:38,120
of the loss function with
respect to itself is always 1.

944
01:02:38,120 --> 01:02:43,190
And then we start from the
end of the network and go up.

945
01:02:43,190 --> 01:02:45,410
You can see here,
that we are going up.

946
01:02:45,410 --> 01:02:49,360
So this is the sigmoid function
calculating the gradients,

947
01:02:49,360 --> 01:02:50,900
then going up.

948
01:02:50,900 --> 01:02:52,760
That was the add gate.

949
01:02:52,760 --> 01:02:54,500
We had another add gate.

950
01:02:54,500 --> 01:02:58,190
And then we had
two multiply gates,

951
01:02:58,190 --> 01:03:03,710
which basically, very simply,
gives us the implementations.

952
01:03:03,710 --> 01:03:07,390
And with this type
of formulation,

953
01:03:07,390 --> 01:03:12,960
what we can do is just do some--

954
01:03:12,960 --> 01:03:20,400
modularize every function
in neural network

955
01:03:20,400 --> 01:03:23,280
and create the
forward and backward

956
01:03:23,280 --> 01:03:25,350
APIs for every single
function that we

957
01:03:25,350 --> 01:03:27,640
need in the neural network.

958
01:03:27,640 --> 01:03:31,710
So in this case, this is a
multiplication gate that--

959
01:03:31,710 --> 01:03:33,810
because for
multiplication, we need

960
01:03:33,810 --> 01:03:39,190
to access the inputs for
use in the backward pass.

961
01:03:39,190 --> 01:03:42,750
We often save them,
memorize them, but then

962
01:03:42,750 --> 01:03:45,480
calculate the forward
pass values and then

963
01:03:45,480 --> 01:03:48,600
the backward pass,
calculate the gradients.

964
01:03:48,600 --> 01:03:53,850
So this means we can write our
functions and put the forward

965
01:03:53,850 --> 01:03:56,350
and backward passes all in.

966
01:03:56,350 --> 01:04:00,580
And this is how PyTorch
operators right now look like.

967
01:04:00,580 --> 01:04:02,260
If you look at
the sigmoid layer,

968
01:04:02,260 --> 01:04:04,120
for example, it's
just the forward pass.

969
01:04:04,120 --> 01:04:08,070
Although in this very
function it's not implemented.

970
01:04:08,070 --> 01:04:11,630
It's somewhere else in the
C++ code, in the C code,

971
01:04:11,630 --> 01:04:14,520
that it's actually
implemented in PyTorch.

972
01:04:14,520 --> 01:04:16,940
But then the backward
pass of sigmoid

973
01:04:16,940 --> 01:04:18,950
is also calculating
the same function

974
01:04:18,950 --> 01:04:20,995
that we just talked about.

976
01:04:27,500 --> 01:04:31,140
So far, what we've said--

977
01:04:31,140 --> 01:04:33,530
and I actually covered
most of the examples

978
01:04:33,530 --> 01:04:40,260
that I wanted to cover in
using the scalar values.

979
01:04:40,260 --> 01:04:43,560
All of the examples
were just scalar values.

980
01:04:43,560 --> 01:04:48,410
But we know that all
of these operations

981
01:04:48,410 --> 01:04:52,500
could actually be implemented
in vector or matrix forms,

982
01:04:52,500 --> 01:04:58,700
just expanding on
that piece here.

983
01:04:58,700 --> 01:05:03,080
We talked about this, that with
the scalar to scalar setting,

984
01:05:03,080 --> 01:05:07,090
so far, what we've talked
about for any input x and y

985
01:05:07,090 --> 01:05:08,240
being scalars.

986
01:05:08,240 --> 01:05:13,070
So the derivative
will also be a scalar.

987
01:05:13,070 --> 01:05:17,830
Which means if we change x
by a small amount, how much

988
01:05:17,830 --> 01:05:20,890
the value of y will change?

989
01:05:20,890 --> 01:05:24,070
If it's now vectorized
and there are vectors,

990
01:05:24,070 --> 01:05:28,360
if x is a vector
of n elements and y

991
01:05:28,360 --> 01:05:33,770
is a scalar, a vector
to a scalar derivative,

992
01:05:33,770 --> 01:05:37,060
then in this case,
the derivative

993
01:05:37,060 --> 01:05:38,270
will also be a vector.

994
01:05:38,270 --> 01:05:42,760
And every single
element in that vector

995
01:05:42,760 --> 01:05:48,440
means if we change x by a
small value, that value,

996
01:05:48,440 --> 01:05:51,740
then how much the
amount of y changes,

997
01:05:51,740 --> 01:05:53,770
then the entire amount
of y because it's just

998
01:05:53,770 --> 01:05:55,160
one single value.

999
01:05:55,160 --> 01:05:59,380
And then there are also
vector to vector frameworks,

1000
01:05:59,380 --> 01:06:05,430
where x and y, both of them are
vectors tours of arbitrary size.

1001
01:06:05,430 --> 01:06:09,300
And M, in those
cases, the derivatives

1002
01:06:09,300 --> 01:06:14,130
will form a matrix, or
what we call Jacobians.

1003
01:06:14,130 --> 01:06:20,250
And for each of
the elements in x,

1004
01:06:20,250 --> 01:06:23,520
if it changes by a
small amount, then

1005
01:06:23,520 --> 01:06:27,720
this derivative tells us
how much each element of y

1006
01:06:27,720 --> 01:06:29,830
will be changed.

1007
01:06:29,830 --> 01:06:36,365
Again, look at the scripts
here, they are not completely--

1008
01:06:36,365 --> 01:06:37,240
they're not the same.

1009
01:06:37,240 --> 01:06:38,770
They could be different.

1010
01:06:38,770 --> 01:06:42,360
For every single element
in this Jacobian,

1011
01:06:42,360 --> 01:06:44,950
there is a clear meaning.

1012
01:06:44,950 --> 01:06:51,750
And then how we can do, or see
it and visualize it in here

1013
01:06:51,750 --> 01:06:57,190
is that if you want to
backprop with vectors--

1014
01:06:57,190 --> 01:07:04,790
say, x, y, and z are
vectors of size dx, dy, dz.

1015
01:07:04,790 --> 01:07:11,090
Again, the loss
derivative, L, is--

1016
01:07:11,090 --> 01:07:13,040
the loss itself
is always a scalar

1017
01:07:13,040 --> 01:07:16,650
because that's always one
value we want to minimize.

1018
01:07:16,650 --> 01:07:21,080
But then calculating
the upstream gradient

1019
01:07:21,080 --> 01:07:29,360
will result in also a vector
dz, same size as its variable z.

1020
01:07:29,360 --> 01:07:34,940
And same story happens when it
comes to downstream gradients.

1021
01:07:34,940 --> 01:07:38,700
In downstream
gradients-- actually,

1022
01:07:38,700 --> 01:07:41,000
before going to
downstream, let me tell you

1023
01:07:41,000 --> 01:07:43,700
a little bit about
the local gradients,

1024
01:07:43,700 --> 01:07:52,730
where we have gradient of
z with respect to x and y.

1025
01:07:52,730 --> 01:07:55,160
And in this case
that's the part that I

1026
01:07:55,160 --> 01:07:58,910
said there will be
Jacobians because now,

1027
01:07:58,910 --> 01:08:02,440
the gradients will
turn into matrices.

1028
01:08:02,440 --> 01:08:06,240
So we have two
Jacobian matrices here

1029
01:08:06,240 --> 01:08:10,260
defined by the size of
their input multiplied

1030
01:08:10,260 --> 01:08:14,220
by the size of the output.

1031
01:08:14,220 --> 01:08:21,540
And then this results
in downstream gradients

1032
01:08:21,540 --> 01:08:27,189
that are multiplication of
upstream and the local gradient.

1033
01:08:27,189 --> 01:08:33,220
And there, we get same size
as the inputs x itself.

1034
01:08:33,220 --> 01:08:37,080
So we will have a
vector again here

1035
01:08:37,080 --> 01:08:40,020
because the input was
a vector, same size,

1036
01:08:40,020 --> 01:08:45,300
in terms of the gradients.

1037
01:08:45,300 --> 01:08:48,660
I just mentioned that gradients
of variables with respect

1038
01:08:48,660 --> 01:08:51,990
to loss always have
the same dimensionality

1039
01:08:51,990 --> 01:08:57,180
as the original variable itself,
as also shown in this slide.

1040
01:08:57,180 --> 01:09:07,830
So backprop with vectors was
this, just one example here.

1041
01:09:07,830 --> 01:09:11,550
Let's say we have a function,
which is the max of 0 and x.

1042
01:09:11,550 --> 01:09:14,430
That's the ReLU function.

1043
01:09:14,430 --> 01:09:18,660
So this is an element wise
function that takes the inputs,

1044
01:09:18,660 --> 01:09:19,760
takes the max between 0.

1045
01:09:19,760 --> 01:09:21,090
And if it's nonzero--

1046
01:09:21,090 --> 01:09:24,590
if it's non-negative, it
passes through, otherwise

1047
01:09:24,590 --> 01:09:26,729
replaces it with a 0.

1048
01:09:26,729 --> 01:09:29,819
Assume you get some
upstream gradients

1049
01:09:29,819 --> 01:09:34,010
and now, we need to build
a Jacobian matrix here.

1050
01:09:34,010 --> 01:09:36,479
And this Jacobian
matrix, in this case,

1051
01:09:36,479 --> 01:09:40,140
because this is an
element wise operation,

1052
01:09:40,140 --> 01:09:45,890
it doesn't have any dependence
on any of the other inputs, only

1053
01:09:45,890 --> 01:09:48,990
the dependencies on
the value itself.

1054
01:09:48,990 --> 01:09:52,700
This is a very sparse
matrix, only has value

1055
01:09:52,700 --> 01:09:55,050
on the main diagonal.

1056
01:09:55,050 --> 01:09:59,900
And those values are
actually either 0 or 1,

1057
01:09:59,900 --> 01:10:04,400
depending on if the
max was actually taken,

1058
01:10:04,400 --> 01:10:07,940
or a 0 was if the value
was passed through,

1059
01:10:07,940 --> 01:10:13,930
or just the 0 was replaced
with it in its place.

1060
01:10:13,930 --> 01:10:16,900
Multiplying it by
the upstream gradient

1061
01:10:16,900 --> 01:10:19,700
gives us the
downstream gradient.

1062
01:10:19,700 --> 01:10:25,700
And this is how the
calculations are done.

1063
01:10:25,700 --> 01:10:31,100
As I said, Jacobian here is
sparse because in this case,

1064
01:10:31,100 --> 01:10:34,180
the operation is element wise.

1065
01:10:34,180 --> 01:10:37,390
And that could actually
be instead of--

1066
01:10:37,390 --> 01:10:40,870
in the backward pass,
instead of calculating

1067
01:10:40,870 --> 01:10:44,590
that huge, sparse
Jacobian matrix, what

1068
01:10:44,590 --> 01:10:48,100
we do is just use these
rule-based calculation

1069
01:10:48,100 --> 01:10:50,960
of the gradient for
this max function.

1070
01:10:50,960 --> 01:10:53,800
So we don't really
store that matrix

1071
01:10:53,800 --> 01:10:56,910
and do not calculate
that because we

1072
01:10:56,910 --> 01:11:01,020
know how the function operates.

1073
01:11:01,020 --> 01:11:03,930
And then this could
also be extended

1074
01:11:03,930 --> 01:11:07,870
to matrices and even tensors.

1075
01:11:07,870 --> 01:11:09,840
If the inputs are
not vectors, they

1076
01:11:09,840 --> 01:11:13,840
are high dimensionality data.

1077
01:11:13,840 --> 01:11:18,750
So in those cases,
again, the gradients

1078
01:11:18,750 --> 01:11:21,660
with respect to
the variables would

1079
01:11:21,660 --> 01:11:26,340
be of the same size as
that specific variable.

1080
01:11:26,340 --> 01:11:34,260
And calculating the upstream
and downstream matrices

1081
01:11:34,260 --> 01:11:39,810
and derivatives is going to be
done same as how we discussed

1082
01:11:39,810 --> 01:11:44,350
and showed earlier for vectors.

1083
01:11:44,350 --> 01:11:49,890
And then when it comes to the
local gradients, although--

1084
01:11:49,890 --> 01:11:53,930
however, it's going to be a
huge matrix, a huge Jacobian

1085
01:11:53,930 --> 01:11:56,840
because we have a
matrix as the output

1086
01:11:56,840 --> 01:11:58,320
and the matrix as the input.

1087
01:11:58,320 --> 01:12:00,080
And then the local
gradients will

1088
01:12:00,080 --> 01:12:06,170
be the same size as the
multiplication of its input

1089
01:12:06,170 --> 01:12:08,250
size and the output size.

1090
01:12:08,250 --> 01:12:11,880
So it's going to be a
huge matrix by itself.

1091
01:12:11,880 --> 01:12:13,500
Let me give you an example.

1092
01:12:13,500 --> 01:12:17,810
If this is input
x and w as input

1093
01:12:17,810 --> 01:12:24,980
for a node for a gate
with matrix multiplication

1094
01:12:24,980 --> 01:12:28,050
and generates this
y as the output,

1095
01:12:28,050 --> 01:12:32,930
calculating derivative
of L with respect to y

1096
01:12:32,930 --> 01:12:36,050
gives us these
Jacobian matrices.

1097
01:12:36,050 --> 01:12:43,250
And say, we have a mini batch
size of 64 and dimensionality

1098
01:12:43,250 --> 01:12:47,645
of those matrices is 4,096,
then this means that,

1099
01:12:47,645 --> 01:12:52,930
that Jacobian matrix, that huge
Jacobian matrix will be over 256

1100
01:12:52,930 --> 01:13:00,100
gigabytes for just one
single matrix multiplication.

1101
01:13:00,100 --> 01:13:07,720
So in order to
simplify this, what we

1102
01:13:07,720 --> 01:13:12,160
do is, we try to
look at the values

1103
01:13:12,160 --> 01:13:14,240
and how they impact each other.

1104
01:13:14,240 --> 01:13:17,740
For example, what
parts of y will

1105
01:13:17,740 --> 01:13:24,770
be affected if one element
of x gets impacted?

1106
01:13:24,770 --> 01:13:30,700
So there, xn and d,
this specific one,

1107
01:13:30,700 --> 01:13:35,720
often affects just
one row in the output.

1108
01:13:35,720 --> 01:13:41,560
And this basically helps
us identify for calculating

1109
01:13:41,560 --> 01:13:43,400
each of these nodes.

1110
01:13:43,400 --> 01:13:46,340
We don't need to create
the huge Jacobian.

1111
01:13:46,340 --> 01:13:50,160
We can actually write the
backward pass functions

1112
01:13:50,160 --> 01:13:53,610
specifically for
matrix multiplication

1113
01:13:53,610 --> 01:13:56,170
in a more efficient way.

1114
01:13:56,170 --> 01:13:59,530
And I'm almost done.

1115
01:13:59,530 --> 01:14:01,320
So you answer this question.

1116
01:14:01,320 --> 01:14:08,170
How much does xn, d affect
the value of y and m.

1117
01:14:08,170 --> 01:14:15,480
So this is yn, m that is
getting impacts from xn, d.

1118
01:14:15,480 --> 01:14:17,400
How much does it get impact?

1119
01:14:17,400 --> 01:14:19,680
It means that what
should I place,

1120
01:14:19,680 --> 01:14:22,680
or put as its
gradient with respect

1121
01:14:22,680 --> 01:14:30,720
to the specific value, xn, d?

1122
01:14:30,720 --> 01:14:34,150
Just to remind you, this is
a multiplication operation.

1124
01:14:37,410 --> 01:14:41,010
In multiply gates,
it should be a swap.

1125
01:14:41,010 --> 01:14:47,820
So whatever the answer to this
question, is something a value

1126
01:14:47,820 --> 01:14:49,500
in W?

1127
01:14:49,500 --> 01:14:51,800
Remember that we had this
multiplication gate, which

1128
01:14:51,800 --> 01:14:53,520
was a swap multiplier.

1129
01:14:53,520 --> 01:14:55,950
So there is a swap
happening here.

1130
01:14:55,950 --> 01:15:07,730
So the value of x affecting
y, one of the elements in y,

1131
01:15:07,730 --> 01:15:13,040
is going to be
dependent on the W that

1132
01:15:13,040 --> 01:15:17,510
is in row D defined by
the x matrix and column

1133
01:15:17,510 --> 01:15:19,800
M defined by the y matrix.

1134
01:15:19,800 --> 01:15:22,470
So it's swapping the values.

1135
01:15:22,470 --> 01:15:23,910
It's the same swap.

1136
01:15:23,910 --> 01:15:27,650
But here now, we have to
look at the giant matrices

1137
01:15:27,650 --> 01:15:31,950
and find which specific
element it should be.

1138
01:15:31,950 --> 01:15:34,580
And then based on
that, we can actually

1139
01:15:34,580 --> 01:15:40,940
replace the entire thing with
matrix multiplication and matrix

1140
01:15:40,940 --> 01:15:41,760
operations.

1141
01:15:41,760 --> 01:15:43,700
The gradient of L
with respect to x

1142
01:15:43,700 --> 01:15:47,690
will be defined as this
simple matrix operation.

1143
01:15:47,690 --> 01:15:51,130
And then the gradient
of L with respect to W

1144
01:15:51,130 --> 01:15:54,080
will be defined as this
very simple multiplication.

1145
01:15:54,080 --> 01:15:58,940
Again, swap here, for x,
we include the entire w.

1146
01:15:58,940 --> 01:16:05,420
For w, we include the entire
x and do the multiplications.

1147
01:16:05,420 --> 01:16:09,220
These formulas makes
it easy to implement

1148
01:16:09,220 --> 01:16:14,860
larger and harder operations
and get them implemented

1149
01:16:14,860 --> 01:16:17,470
in the backward passes.

1150
01:16:17,470 --> 01:16:20,020
All right, we're done.

1151
01:16:20,020 --> 01:16:22,690
Just to summarize,
we talked today

1152
01:16:22,690 --> 01:16:24,860
about fully connected
neural networks.

1153
01:16:24,860 --> 01:16:30,320
We went through all the steps
needed for backpropagation,

1154
01:16:30,320 --> 01:16:32,390
the forward passes,
backward passes.

1155
01:16:32,390 --> 01:16:36,160
And next session,
we will be getting

1156
01:16:36,160 --> 01:16:39,820
into the topic of
convolutional neural networks.

1157
01:16:39,820 --> 01:16:41,820
Thank you.