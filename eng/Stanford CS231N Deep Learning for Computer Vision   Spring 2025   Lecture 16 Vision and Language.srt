2
00:00:05,440 --> 00:00:08,000
Thank you everyone for coming.

3
00:00:08,000 --> 00:00:10,000
We have another guest lecture.

4
00:00:10,000 --> 00:00:12,160
And today we have
Ranjay Krishna.

5
00:00:12,160 --> 00:00:14,160
Ranjay Krishna is an
assistant professor

6
00:00:14,160 --> 00:00:16,600
at the School of Computer
Science and Engineering

7
00:00:16,600 --> 00:00:21,040
at the University of Washington,
and he co-directs the RAIVN lab.

8
00:00:21,040 --> 00:00:24,200
He has taught previous
iterations of CS231N

9
00:00:24,200 --> 00:00:27,160
in 2020 and 2021,
and his research

10
00:00:27,160 --> 00:00:30,220
lies at the intersection
of computer vision,

11
00:00:30,220 --> 00:00:33,280
natural language processing,
robotics, and human computer

12
00:00:33,280 --> 00:00:34,240
interaction.

13
00:00:34,240 --> 00:00:37,000
In today's lecture, he will
discuss multimodal foundation

14
00:00:37,000 --> 00:00:37,500
models.

15
00:00:37,500 --> 00:00:38,000
And.

16
00:00:38,000 --> 00:00:39,200
Ranjay, the floor is yours.

17
00:00:39,200 --> 00:00:40,103
Thank you.

18
00:00:40,103 --> 00:00:41,020
It's great to be back.

19
00:00:41,020 --> 00:00:43,080
The first time I ever
taught this course here

20
00:00:43,080 --> 00:00:47,520
at Stanford it was 2020 and
we had about three weeks

21
00:00:47,520 --> 00:00:51,560
where we had to take all the
material and move it online.

22
00:00:51,560 --> 00:00:55,320
Yeah, every year after that
has been much easier to teach.

23
00:00:55,320 --> 00:00:56,300
It's great to be back.

24
00:00:56,300 --> 00:00:58,675
So today, we're going to talk
about multimodal foundation

25
00:00:58,675 --> 00:00:59,530
models.

26
00:00:59,530 --> 00:01:01,610
So a lot of the
lectures in this class

27
00:01:01,610 --> 00:01:05,930
so far has really been focused
on building individual models

28
00:01:05,930 --> 00:01:07,410
for individual tasks.

29
00:01:07,410 --> 00:01:09,968
So these usually
follow a few steps

30
00:01:09,968 --> 00:01:12,010
that you've seen over and
over again in lectures.

31
00:01:12,010 --> 00:01:13,930
You collect a data
set, usually a training

32
00:01:13,930 --> 00:01:16,650
set as well as a test set, then
you train a very specialized

33
00:01:16,650 --> 00:01:18,118
model for that purpose.

34
00:01:18,118 --> 00:01:20,410
So that could be an image
classification model or image

35
00:01:20,410 --> 00:01:22,810
captioning model, like the ones
you've seen in your assignments

36
00:01:22,810 --> 00:01:23,510
as well.

37
00:01:23,510 --> 00:01:27,490
And then you finally evaluate
those models on your test set.

38
00:01:27,490 --> 00:01:29,850
Now, what's been
different in the field

39
00:01:29,850 --> 00:01:32,170
so far in the last
couple of years

40
00:01:32,170 --> 00:01:34,770
is this shift away from
these individual models

41
00:01:34,770 --> 00:01:38,370
into building these
more foundation models.

42
00:01:38,370 --> 00:01:40,210
And the way to think
about foundation models

43
00:01:40,210 --> 00:01:43,290
is that it really is
trying to pre-train models

44
00:01:43,290 --> 00:01:46,790
on a wide variety of skills, a
wide variety of different tasks,

45
00:01:46,790 --> 00:01:50,090
and then later on adapt those
things for individual tasks

46
00:01:50,090 --> 00:01:51,570
depending on your needs.

47
00:01:51,570 --> 00:01:54,690
So, for example, one very
common foundation model

48
00:01:54,690 --> 00:01:58,580
that you all probably use in
some form or the other is GPT,

49
00:01:58,580 --> 00:02:01,660
and GPT was trained on a
lot of Common Crawl data

50
00:02:01,660 --> 00:02:02,600
from the internet.

51
00:02:02,600 --> 00:02:04,438
And then you take that
model that you get,

52
00:02:04,438 --> 00:02:06,480
and then you fine tune it
for different purposes.

53
00:02:06,480 --> 00:02:09,020
So you fine tune that
model for math problems

54
00:02:09,020 --> 00:02:11,520
or symbolic reasoning
or trivia questions.

55
00:02:11,520 --> 00:02:13,140
And all of these
are individual tasks

56
00:02:13,140 --> 00:02:14,845
that this model can
quickly adapt to.

57
00:02:14,845 --> 00:02:16,220
Now, what's nice
about foundation

58
00:02:16,220 --> 00:02:17,980
models is that it
allows you to do

59
00:02:17,980 --> 00:02:20,940
that update step, that
adaptation to new tasks

60
00:02:20,940 --> 00:02:22,590
with very minimal
data, meaning you

61
00:02:22,590 --> 00:02:24,840
don't need to collect a large
amount of training data.

62
00:02:24,840 --> 00:02:27,085
You can usually get away
with very, very little.

63
00:02:27,085 --> 00:02:28,460
Oftentimes, you
can even get away

64
00:02:28,460 --> 00:02:31,780
with collecting no
training data at all.

65
00:02:31,780 --> 00:02:34,120
And so when you think
about foundation models,

66
00:02:34,120 --> 00:02:36,580
there's many different
classes of foundation models

67
00:02:36,580 --> 00:02:38,260
that you might care about.

68
00:02:38,260 --> 00:02:40,980
In language, you've got
ELMo and BERT that really

69
00:02:40,980 --> 00:02:42,820
started this entire revolution.

70
00:02:42,820 --> 00:02:46,833
And then we now have GPT and T5
and variants of these models.

71
00:02:46,833 --> 00:02:48,500
These are things we're
not going to talk

72
00:02:48,500 --> 00:02:50,958
about in this class, since
we're mostly going to be talking

73
00:02:50,958 --> 00:02:52,400
about multimodal models.

74
00:02:52,400 --> 00:02:54,140
What we will talk
about is how do you

75
00:02:54,140 --> 00:02:57,570
build these same foundation
models for image classification.

76
00:02:57,570 --> 00:03:02,110
And we'll go into examples
like CLIP and CoCa today.

77
00:03:02,110 --> 00:03:05,390
We'll also talk about how do you
combine language models that you

78
00:03:05,390 --> 00:03:08,830
might have seen already in class
with these vision foundation

79
00:03:08,830 --> 00:03:12,590
models to enable all new
models, multimodal foundation

80
00:03:12,590 --> 00:03:15,665
models that can solve a
wide variety of tasks.

81
00:03:15,665 --> 00:03:17,790
And then, of course, we
can do a lot more than just

82
00:03:17,790 --> 00:03:19,710
solve tasks in language.

83
00:03:19,710 --> 00:03:21,590
We'll talk about how
you can build models

84
00:03:21,590 --> 00:03:25,270
that can output not just
text, but also masks

85
00:03:25,270 --> 00:03:28,735
or images that you
might want to generate.

86
00:03:28,735 --> 00:03:31,110
And then finally, we'll talk
about this idea of chaining,

87
00:03:31,110 --> 00:03:33,150
where you take a bunch
of foundation models

88
00:03:33,150 --> 00:03:37,030
and then combine them to
do all new things together.

89
00:03:37,030 --> 00:03:38,970
Now, when we talk about
foundation models,

90
00:03:38,970 --> 00:03:41,950
there's many different
ways to classify them.

91
00:03:41,950 --> 00:03:43,910
And it's hard because
the definition is often

92
00:03:43,910 --> 00:03:46,130
disagreed upon.

93
00:03:46,130 --> 00:03:48,590
But what you typically will
see in a foundation model

94
00:03:48,590 --> 00:03:52,450
is that it's robust and general
to many different tasks.

95
00:03:52,450 --> 00:03:55,220
So you can apply that same model
for all different use cases,

96
00:03:55,220 --> 00:03:57,608
and I'll show you a
ton of use cases today.

97
00:03:57,608 --> 00:03:59,400
Also, something else
that's common in a lot

98
00:03:59,400 --> 00:04:01,025
of these foundation
models is that they

99
00:04:01,025 --> 00:04:02,700
have a lot of parameters.

100
00:04:02,700 --> 00:04:05,340
They have large numbers
of amounts of parameters,

101
00:04:05,340 --> 00:04:07,800
large amounts of training
data, and usually they're

102
00:04:07,800 --> 00:04:10,800
trained with some
self-supervised objective.

103
00:04:10,800 --> 00:04:13,500
So of course, we're not going to
talk about the language stuff,

104
00:04:13,500 --> 00:04:16,120
but we will talk about are
the ones in green today.

105
00:04:16,120 --> 00:04:18,800
And so let's get started
with image classification.

106
00:04:18,800 --> 00:04:22,520
So how do we actually go about
building a foundation model that

107
00:04:22,520 --> 00:04:25,200
can solve image
classification for any data

108
00:04:25,200 --> 00:04:26,960
set you might care about.

109
00:04:26,960 --> 00:04:29,300
Now, if you remember
from a few lectures ago,

110
00:04:29,300 --> 00:04:31,840
we were talking about
self-supervised learning.

111
00:04:31,840 --> 00:04:33,560
And in self-supervised
learning, one

112
00:04:33,560 --> 00:04:35,400
of those methods
that you saw was

113
00:04:35,400 --> 00:04:38,720
SimCLR, where you have this
contrastive objective that

114
00:04:38,720 --> 00:04:42,200
contrasts against
dissimilar images

115
00:04:42,200 --> 00:04:45,360
and pulls closer representations
of the same image that

116
00:04:45,360 --> 00:04:47,760
has been transformed in
some way or the other.

117
00:04:47,760 --> 00:04:51,000
Now, this idea you can
think of as pulling together

118
00:04:51,000 --> 00:04:51,780
similar concepts.

119
00:04:51,780 --> 00:04:54,170
So different
augmentations of a cat

120
00:04:54,170 --> 00:04:57,310
should result in representations
that are similar to one another,

121
00:04:57,310 --> 00:04:59,570
but it should push
away representations

122
00:04:59,570 --> 00:05:03,090
for other kinds of categories,
like dogs, for example.

123
00:05:03,090 --> 00:05:06,010
Now, the hope with training with
these self-supervised learning

124
00:05:06,010 --> 00:05:08,850
objectives is that these
representations become general

125
00:05:08,850 --> 00:05:11,330
enough so that when you
see something new, maybe

126
00:05:11,330 --> 00:05:14,490
a sketch of a cat or a
sketch of a dog, it's still

127
00:05:14,490 --> 00:05:17,090
embeds those in the
space so that it's

128
00:05:17,090 --> 00:05:20,250
easy to classify exactly
what those concepts are.

129
00:05:20,250 --> 00:05:21,910
Now, moving on to multimodal.

130
00:05:21,910 --> 00:05:25,090
We can take these same ideas,
the same objective, and then

131
00:05:25,090 --> 00:05:27,690
start thinking about what
would happen if we added text

132
00:05:27,690 --> 00:05:29,690
to that representation space.

133
00:05:29,690 --> 00:05:31,410
So, for example,
if we could also

134
00:05:31,410 --> 00:05:36,090
embed a representation of the
text that says a cute fluffy cat

135
00:05:36,090 --> 00:05:38,792
and have that be close to
the cat representations,

136
00:05:38,792 --> 00:05:40,250
that would be great,
because now we

137
00:05:40,250 --> 00:05:44,370
can query things in both
images as well as text.

138
00:05:44,370 --> 00:05:47,730
And similarly, if we can
also embed the phrase

139
00:05:47,730 --> 00:05:49,710
my favorite dog is
a golden retriever

140
00:05:49,710 --> 00:05:51,380
and ideally that
representation would

141
00:05:51,380 --> 00:05:55,700
lie closer to golden retrievers
than other kinds of dogs.

142
00:05:55,700 --> 00:05:58,185
So that's the general
idea behind adapting

143
00:05:58,185 --> 00:06:00,060
the self-supervised
learning objectives we've

144
00:06:00,060 --> 00:06:03,340
been talking about in class
so far to incorporate text

145
00:06:03,340 --> 00:06:06,140
and other multimodal inputs.

146
00:06:06,140 --> 00:06:08,700
So in SimCLR, if you
remember, the main objective

147
00:06:08,700 --> 00:06:13,180
was that you want to pull
together again, transformations

148
00:06:13,180 --> 00:06:14,240
of the same image.

149
00:06:14,240 --> 00:06:18,060
So the cat should be closest
to its other cat augmentation.

150
00:06:18,060 --> 00:06:20,500
So that green arrow right
there indicates two things

151
00:06:20,500 --> 00:06:21,792
that should be pulled together.

152
00:06:21,792 --> 00:06:23,375
And it should be
further away from all

153
00:06:23,375 --> 00:06:24,440
the other augmentations.

154
00:06:24,440 --> 00:06:27,440
So any other image
of a dog or a monkey,

155
00:06:27,440 --> 00:06:29,980
you want those representations
to be far away.

156
00:06:29,980 --> 00:06:32,500
Now we can use that
same idea and now

157
00:06:32,500 --> 00:06:34,820
think about training
a CLIP model.

158
00:06:34,820 --> 00:06:37,700
In CLIP what they do is they
still have that same image

159
00:06:37,700 --> 00:06:40,080
encoder that you have
on the left-hand side,

160
00:06:40,080 --> 00:06:43,100
but on the right-hand side,
you now have a text encoder.

161
00:06:43,100 --> 00:06:46,020
And this text encoder is
embedding descriptions

162
00:06:46,020 --> 00:06:47,980
of those individual images.

163
00:06:47,980 --> 00:06:51,310
So your dog image will
now hopefully learn

164
00:06:51,310 --> 00:06:54,270
that it should be closer to a
representation of a text that

165
00:06:54,270 --> 00:06:56,270
says, my favorite dog
is a golden retriever

166
00:06:56,270 --> 00:06:59,310
and far away from all the
other representations.

167
00:06:59,310 --> 00:07:01,470
And because this is
the same formulation

168
00:07:01,470 --> 00:07:03,390
that you've seen with
SimCLR, the objective

169
00:07:03,390 --> 00:07:05,470
that you use to train
a model like this

170
00:07:05,470 --> 00:07:08,550
is just by collecting a
lot of image text pairs.

171
00:07:08,550 --> 00:07:10,090
And then once you
have those pairs,

172
00:07:10,090 --> 00:07:12,070
feed them into a
model in a mini-batch,

173
00:07:12,070 --> 00:07:15,390
and then make sure that you
have this contrastive objective

174
00:07:15,390 --> 00:07:16,810
that we use for SimCLR.

175
00:07:16,810 --> 00:07:19,970
But now we're applying them
across images and text.

176
00:07:19,970 --> 00:07:22,270
So we're pulling together,
here in the numerator,

177
00:07:22,270 --> 00:07:24,790
the representations
of similar things

178
00:07:24,790 --> 00:07:26,710
and pulling apart
the representations

179
00:07:26,710 --> 00:07:29,110
in the denominator
for everything else.

180
00:07:29,110 --> 00:07:30,550
Now, of course,
we want that image

181
00:07:30,550 --> 00:07:33,030
to be closest to its
corresponding text

182
00:07:33,030 --> 00:07:35,370
and far away from
all the other text.

183
00:07:35,370 --> 00:07:37,730
But we also want the
inverse to be true as well.

184
00:07:37,730 --> 00:07:39,350
So we have a second
objective as well

185
00:07:39,350 --> 00:07:42,710
that says every text should
be closest to its image

186
00:07:42,710 --> 00:07:45,970
and further away from all
the other image descriptions.

187
00:07:45,970 --> 00:07:48,910
So it's a complimentary
symmetric loss

188
00:07:48,910 --> 00:07:51,550
that you have between the two
different types of modalities

189
00:07:51,550 --> 00:07:55,470
that you're feeding into
this learning objective.

190
00:07:55,470 --> 00:07:58,470
So of course, what's
really nice about a CLIP

191
00:07:58,470 --> 00:08:02,070
like model is that it can be
trained with just associations

192
00:08:02,070 --> 00:08:03,650
of images and text.

193
00:08:03,650 --> 00:08:06,090
And there's a ton of this
data on the internet.

194
00:08:06,090 --> 00:08:08,950
So you have a lot of data of
corresponding images and text

195
00:08:08,950 --> 00:08:10,650
that you can pull up
from the internet.

196
00:08:10,650 --> 00:08:13,030
You can download, and now
you can train this model

197
00:08:13,030 --> 00:08:14,850
at a very, very large scale.

198
00:08:14,850 --> 00:08:16,590
And this is exactly
what OpenAI did

199
00:08:16,590 --> 00:08:18,550
a couple of years
ago in 2021 when

200
00:08:18,550 --> 00:08:20,910
they released their CLIP model.

201
00:08:20,910 --> 00:08:22,650
So they collected
a lot of that data,

202
00:08:22,650 --> 00:08:25,530
and then they trained it using
this contrastive objective,

203
00:08:25,530 --> 00:08:27,230
using all of the
images and text pairs

204
00:08:27,230 --> 00:08:28,650
that they found
from the internet.

205
00:08:28,650 --> 00:08:30,590
And then once they were
done training that,

206
00:08:30,590 --> 00:08:33,309
you follow the same 2-step
pipeline that you saw

207
00:08:33,309 --> 00:08:35,630
in the self-supervised
learning class,

208
00:08:35,630 --> 00:08:38,169
where in step 1 you
do that pre-training,

209
00:08:38,169 --> 00:08:40,890
and then in step 2 you can
take that image encoder,

210
00:08:40,890 --> 00:08:43,470
and now you can adapt
it to a new task.

211
00:08:43,470 --> 00:08:46,140
So once you have this
pre-trained image encoder,

212
00:08:46,140 --> 00:08:47,620
you take it, you
take its weights,

213
00:08:47,620 --> 00:08:50,760
and then you tag on an
additional linear layer on top

214
00:08:50,760 --> 00:08:52,600
to adapt it to an
image classification

215
00:08:52,600 --> 00:08:54,680
task or a detection
task or you can

216
00:08:54,680 --> 00:08:58,000
put in something like a
decoder and decode out

217
00:08:58,000 --> 00:09:00,120
semantic segmentation maps.

218
00:09:00,120 --> 00:09:02,520
So a ton of different
tasks become possible just

219
00:09:02,520 --> 00:09:07,120
by initializing your model from
this pre-trained objective.

220
00:09:07,120 --> 00:09:09,200
What was really exciting
when this paper came out

221
00:09:09,200 --> 00:09:13,400
is that linear addition of
this one linear classifier

222
00:09:13,400 --> 00:09:17,160
on top of this CLIP encoder led
to really large improvements

223
00:09:17,160 --> 00:09:18,160
in performance.

224
00:09:18,160 --> 00:09:20,760
So here in this
graph, I'm showing you

225
00:09:20,760 --> 00:09:23,400
average performance across many
different image classification

226
00:09:23,400 --> 00:09:24,360
data sets.

227
00:09:24,360 --> 00:09:26,780
And the CLIP models,
the ones in red,

228
00:09:26,780 --> 00:09:28,280
they're all the way at the top.

229
00:09:28,280 --> 00:09:32,200
And you can see that as you
train on more and more images,

230
00:09:32,200 --> 00:09:35,000
you end up getting better
and better performance.

231
00:09:35,000 --> 00:09:36,560
So it was very
exciting because it

232
00:09:36,560 --> 00:09:38,440
seemed to indicate that
there's this really

233
00:09:38,440 --> 00:09:41,420
nice pre-training objective
that we've been able to unlock,

234
00:09:41,420 --> 00:09:43,650
and there's an
abundance of image text

235
00:09:43,650 --> 00:09:46,330
data on the internet, which
means that we can train these

236
00:09:46,330 --> 00:09:50,410
to be very, very large
and very, very performant.

237
00:09:50,410 --> 00:09:52,310
Of course, that's not
the end of the story.

238
00:09:52,310 --> 00:09:54,450
What we want to
do, ideally, is not

239
00:09:54,450 --> 00:09:56,837
to have to adapt these
features for something new.

240
00:09:56,837 --> 00:09:59,170
We would ideally want to be
able to use a CLIP model out

241
00:09:59,170 --> 00:10:00,730
of the box.

242
00:10:00,730 --> 00:10:03,610
So in language models, for
example, you train a model

243
00:10:03,610 --> 00:10:05,310
to autocomplete, usually.

244
00:10:05,310 --> 00:10:07,430
And this autocompletion
works like this.

245
00:10:07,430 --> 00:10:09,410
You have a phrase
that says I love

246
00:10:09,410 --> 00:10:13,250
and then your model fills in the
next word, for example, cake.

247
00:10:13,250 --> 00:10:15,810
And then you train with
this pre-training objective.

248
00:10:15,810 --> 00:10:18,170
And what you want to do
during the second stage

249
00:10:18,170 --> 00:10:20,670
is to basically
take that same model

250
00:10:20,670 --> 00:10:22,490
and adapt it to a new task.

251
00:10:22,490 --> 00:10:25,610
For language models, you never
have to retrain that model.

252
00:10:25,610 --> 00:10:28,170
You never have to retrain
it on a new downstream task.

253
00:10:28,170 --> 00:10:30,810
Every task is a language
task, and so every task

254
00:10:30,810 --> 00:10:34,970
can be treated as this
autocomplete process.

255
00:10:34,970 --> 00:10:37,290
But with CLIP, the
problem is there

256
00:10:37,290 --> 00:10:39,410
is no autocomplete process.

257
00:10:39,410 --> 00:10:42,240
So we've trained this model
on this contrastive objective,

258
00:10:42,240 --> 00:10:45,760
but to adapt it to a new task
we still need training data,

259
00:10:45,760 --> 00:10:48,900
and we still need a
linear layer on top

260
00:10:48,900 --> 00:10:51,442
that we need to train to
adapt it to new tasks.

261
00:10:51,442 --> 00:10:52,900
So a lot of people
started thinking

262
00:10:52,900 --> 00:10:59,060
about what we can do to adapt
this model to use it directly

263
00:10:59,060 --> 00:11:00,100
out of the box.

264
00:11:00,100 --> 00:11:02,800
And there's this clever trick
that people came up with.

265
00:11:02,800 --> 00:11:06,900
And this clever trick is
basically using the text encoder

266
00:11:06,900 --> 00:11:11,020
as a way of guiding
the model to generalize

267
00:11:11,020 --> 00:11:13,480
to any downstream
classification task.

268
00:11:13,480 --> 00:11:14,613
And it works like this.

269
00:11:14,613 --> 00:11:16,780
So let's say you want to
classify what this image is

270
00:11:16,780 --> 00:11:20,260
using a CLIP model but you
don't want to retrain this model

271
00:11:20,260 --> 00:11:22,600
or adapt it for any
downstream task.

272
00:11:22,600 --> 00:11:26,660
What you can do is you
can take the text encoder

273
00:11:26,660 --> 00:11:29,380
and pass in a word
through that text encoder

274
00:11:29,380 --> 00:11:33,740
to create a text vector and use
nearest neighbors to figure out

275
00:11:33,740 --> 00:11:35,460
what is the right
classification.

276
00:11:35,460 --> 00:11:38,300
So the way this works is
you take all the categories

277
00:11:38,300 --> 00:11:39,405
in your new data set.

278
00:11:39,405 --> 00:11:41,030
So for example, let's
say your new data

279
00:11:41,030 --> 00:11:44,430
set contains the categories
plain, dog and bird.

280
00:11:44,430 --> 00:11:47,230
You're going to embed all
of them in the text space

281
00:11:47,230 --> 00:11:50,350
to get a vector for
plain, a vector for dog,

282
00:11:50,350 --> 00:11:52,030
and a vector for bird.

283
00:11:52,030 --> 00:11:54,870
And now when a new image
comes in, all you have to do

284
00:11:54,870 --> 00:11:57,870
is embed that image
using the image encoder

285
00:11:57,870 --> 00:11:59,990
and then find the
closest neighbor.

286
00:11:59,990 --> 00:12:02,630
So in this case, you
should find that this image

287
00:12:02,630 --> 00:12:06,870
has the highest similarity
with the correct class.

288
00:12:06,870 --> 00:12:09,325
In this case, it should
be the dog vector.

289
00:12:09,325 --> 00:12:11,950
And you can see the dog vector
does have the highest similarity

290
00:12:11,950 --> 00:12:12,410
score.

291
00:12:12,410 --> 00:12:13,868
And so because of
that, you can now

292
00:12:13,868 --> 00:12:17,780
classify that image as a dog.

293
00:12:17,780 --> 00:12:19,870
Now, you can think of
this entire process

294
00:12:19,870 --> 00:12:23,930
as essentially building
a one neighbor algorithm.

295
00:12:23,930 --> 00:12:27,510
So you have a bunch of centers
that you've or embeddings

296
00:12:27,510 --> 00:12:30,090
that you've generated
in the tech space

297
00:12:30,090 --> 00:12:33,288
and now you can use them as
your class category labels,

298
00:12:33,288 --> 00:12:34,830
and you're doing
one nearest neighbor

299
00:12:34,830 --> 00:12:38,680
to find the optimal
classification for any new image

300
00:12:38,680 --> 00:12:40,640
that comes in.

301
00:12:40,640 --> 00:12:43,960
Now, of course, a
single word might not

302
00:12:43,960 --> 00:12:47,040
be sufficient to get a
really good word vector.

303
00:12:47,040 --> 00:12:49,693
Instead, what you might
want to do is use a phrase.

304
00:12:49,693 --> 00:12:51,360
And the reason you
might want to do this

305
00:12:51,360 --> 00:12:54,040
is because a lot of
the internet data,

306
00:12:54,040 --> 00:12:57,100
it usually doesn't have words
that occur by themselves.

307
00:12:57,100 --> 00:13:00,200
CLIP was trained from
just phrases that were

308
00:13:00,200 --> 00:13:01,573
downloaded from the internet.

309
00:13:01,573 --> 00:13:04,240
And so ideally, you want to pick
the right phrase that gives you

310
00:13:04,240 --> 00:13:06,680
the best representation.

311
00:13:06,680 --> 00:13:10,180
So instead of just having the
categories playing dog and bird,

312
00:13:10,180 --> 00:13:12,320
you might instead want
to embed a vector that

313
00:13:12,320 --> 00:13:15,720
represents a photo of a
plane, a photo of a dog.

314
00:13:15,720 --> 00:13:19,040
And turns out if you do this
one small change, you suddenly

315
00:13:19,040 --> 00:13:21,640
get a large boost on
ImageNet, where you see

316
00:13:21,640 --> 00:13:24,800
an improvement of about 1.3%.

317
00:13:24,800 --> 00:13:26,640
Of course, picking
that right phrase

318
00:13:26,640 --> 00:13:28,820
is also something that's
very difficult to do.

319
00:13:28,820 --> 00:13:30,903
And so what people typically
do is they don't just

320
00:13:30,903 --> 00:13:33,400
pick a single phrase, they
pick many different phrases.

321
00:13:33,400 --> 00:13:35,970
So a photo of a dog,
a drawing of a dog

322
00:13:35,970 --> 00:13:39,290
or a bunch of different ideas
for different phrases, and you

323
00:13:39,290 --> 00:13:41,610
want to create many
different vectors

324
00:13:41,610 --> 00:13:44,730
for all of those different
phrases you might think of.

325
00:13:44,730 --> 00:13:48,130
And at the end, what you do is
you just take the mean vector

326
00:13:48,130 --> 00:13:52,330
representation across all of
your phrases for each category

327
00:13:52,330 --> 00:13:55,970
and use that as your mean
dog vector, your mean plane

328
00:13:55,970 --> 00:13:58,113
vector, and your mean vector.

329
00:13:58,113 --> 00:14:00,030
And then now you're back
to where you started,

330
00:14:00,030 --> 00:14:03,930
and you can do your same one
nearest neighbor algorithm

331
00:14:03,930 --> 00:14:05,090
on this.

332
00:14:05,090 --> 00:14:08,370
It probably has been
trained on ImageNet.

333
00:14:08,370 --> 00:14:10,690
This is, I think, a
point to show that you

334
00:14:10,690 --> 00:14:12,630
can adapt it to a new task.

335
00:14:12,630 --> 00:14:14,680
But I will show you other
examples of data sets

336
00:14:14,680 --> 00:14:16,430
where it's definitely
not been trained on,

337
00:14:16,430 --> 00:14:19,450
and it does adapt
to that as well.

338
00:14:19,450 --> 00:14:21,442
So you get a single
vector out and it

339
00:14:21,442 --> 00:14:23,150
depends on the
architecture you're using.

340
00:14:23,150 --> 00:14:26,530
If you're using ResNet, you take
the final vector representation.

341
00:14:26,530 --> 00:14:29,950
If your text encoder is, let's
say, a VIT or a transformer,

342
00:14:29,950 --> 00:14:35,980
then you usually take the CLS
token of your transformer.

343
00:14:35,980 --> 00:14:38,160
So that's it for CLIP.

344
00:14:38,160 --> 00:14:41,540
You can basically adapt this
for a wide variety of new image

345
00:14:41,540 --> 00:14:42,880
classification tasks.

346
00:14:42,880 --> 00:14:45,480
And to your question
right now, of course,

347
00:14:45,480 --> 00:14:48,180
it's not that big a deal
that it performs just as

348
00:14:48,180 --> 00:14:51,340
well on ImageNet, but it is
still exciting that it does

349
00:14:51,340 --> 00:14:53,540
do well on ImageNet at all.

350
00:14:53,540 --> 00:14:57,420
I think more interesting is
that when you look at other data

351
00:14:57,420 --> 00:15:00,520
sets, data sets that were
collected after CLIP came out.

352
00:15:00,520 --> 00:15:02,900
So data set like ObjectNet
which contains objects

353
00:15:02,900 --> 00:15:06,400
that people took photos
of in very weird places.

354
00:15:06,400 --> 00:15:08,960
So they put a banana on the
ground and took a photo of it,

355
00:15:08,960 --> 00:15:10,940
or they took a banana
that was really rotten

356
00:15:10,940 --> 00:15:12,340
and took a photo of it.

357
00:15:12,340 --> 00:15:14,660
So things that are
just not common.

358
00:15:14,660 --> 00:15:18,440
And so in this data set,
if you train on ImageNet,

359
00:15:18,440 --> 00:15:21,120
you don't do very well
because ImageNet again,

360
00:15:21,120 --> 00:15:22,700
contains most of
these categories

361
00:15:22,700 --> 00:15:24,780
in its most typical form.

362
00:15:24,780 --> 00:15:28,740
But if you take the CLIP model,
it performs just as well.

363
00:15:28,740 --> 00:15:31,420
And that was really, really
exciting for many people

364
00:15:31,420 --> 00:15:33,030
because this ability
to generalize

365
00:15:33,030 --> 00:15:35,030
to a completely new
data set that it hasn't

366
00:15:35,030 --> 00:15:38,150
seen before, that's even out
of domain to some degree,

367
00:15:38,150 --> 00:15:38,858
was really great.

368
00:15:38,858 --> 00:15:40,025
So why do you think this is?

369
00:15:40,025 --> 00:15:42,110
Why do you think CLIP
generalizes a lot better

370
00:15:42,110 --> 00:15:43,870
than training on ImageNet?

371
00:15:43,870 --> 00:15:45,670
To paraphrase your
response, because I

372
00:15:45,670 --> 00:15:49,510
think it's the right
response, is the text

373
00:15:49,510 --> 00:15:51,090
that you download
from the internet,

374
00:15:51,090 --> 00:15:53,090
it contains a lot more
than the category labels.

375
00:15:53,090 --> 00:15:55,050
It contains a lot more
structural information,

376
00:15:55,050 --> 00:15:56,670
contains information
about shape,

377
00:15:56,670 --> 00:15:59,790
about the colors of
things and all of that

378
00:15:59,790 --> 00:16:02,810
adds to the representations.

379
00:16:02,810 --> 00:16:05,790
And so these models are able to
adapt a lot better to something

380
00:16:05,790 --> 00:16:08,790
that maybe is slightly out of
distribution or an object that

381
00:16:08,790 --> 00:16:10,430
looks slightly
different because it

382
00:16:10,430 --> 00:16:13,470
does have all these other
things it's looking for as well.

383
00:16:13,470 --> 00:16:15,670
And so those that additional
supervision really

384
00:16:15,670 --> 00:16:17,150
helps quite a lot.

385
00:16:17,150 --> 00:16:20,290
The other reason it helps quite
a lot is the scale of data.

386
00:16:20,290 --> 00:16:23,610
ImageNet is only about
1.3 million images or so,

387
00:16:23,610 --> 00:16:26,288
whereas the internet contains
millions and billions.

388
00:16:26,288 --> 00:16:27,830
At this point,
billions of image text

389
00:16:27,830 --> 00:16:30,250
pairs that we can
download very easily.

390
00:16:30,250 --> 00:16:33,840
And so these models have
just seen so much more data

391
00:16:33,840 --> 00:16:36,605
that this adaptation
becomes a lot easier.

392
00:16:36,605 --> 00:16:38,480
And so people started
doing these experiments

393
00:16:38,480 --> 00:16:41,115
on a wide variety of
generalization tasks.

394
00:16:41,115 --> 00:16:43,240
So they showed that you
can generalize these models

395
00:16:43,240 --> 00:16:46,800
for not just natural
images but also sketches

396
00:16:46,800 --> 00:16:49,860
that you can also do this on
adversarial data sets as well.

397
00:16:49,860 --> 00:16:51,760
And performance
across the board seem

398
00:16:51,760 --> 00:16:53,860
to indicate that these
models are just really,

399
00:16:53,860 --> 00:16:57,777
really good and robust to
many different applications.

400
00:16:57,777 --> 00:17:00,360
And then here I'm showing you
the difference between zero shot

401
00:17:00,360 --> 00:17:01,275
and linear probe.

402
00:17:01,275 --> 00:17:03,400
And you can see that, of
course, linear probe, when

403
00:17:03,400 --> 00:17:05,800
you add that additional
linear classifier

404
00:17:05,800 --> 00:17:07,760
and train it and
adapt it a little bit,

405
00:17:07,760 --> 00:17:09,960
it does improve
performance in majority

406
00:17:09,960 --> 00:17:13,079
of the data sets,
the ones in green

407
00:17:13,079 --> 00:17:14,460
but it's not always the case.

408
00:17:14,460 --> 00:17:16,880
In some cases,
the CLIP zero shot

409
00:17:16,880 --> 00:17:19,720
just performs really
well out of the box.

410
00:17:19,720 --> 00:17:21,920
And so it just seemed to
indicate that we finally

411
00:17:21,920 --> 00:17:23,880
unlocked this
capability of being

412
00:17:23,880 --> 00:17:27,599
able to adapt image
encoders for a wide variety

413
00:17:27,599 --> 00:17:29,025
of different downstream tasks.

414
00:17:29,025 --> 00:17:30,650
And this is why I
think a lot of people

415
00:17:30,650 --> 00:17:35,050
talk about CLIP as the first
foundation model for images.

416
00:17:35,050 --> 00:17:37,610
So let's talk about what
makes CLIP work so well.

417
00:17:37,610 --> 00:17:40,370
Of course, there's no real
labels as such with CLIP.

418
00:17:40,370 --> 00:17:43,430
We're just downloading any
text associated with images.

419
00:17:43,430 --> 00:17:45,583
What makes CLIP
work so well is what

420
00:17:45,583 --> 00:17:47,750
I was saying it contained
when it was first trained,

421
00:17:47,750 --> 00:17:51,010
it was trained on about
somewhere between--

422
00:17:51,010 --> 00:17:53,270
well, the parameters
were just gigantic.

423
00:17:53,270 --> 00:17:55,090
They sort of scaled
up the model and they

424
00:17:55,090 --> 00:17:58,290
changed the architecture
from ResNet to a VIT.

425
00:17:58,290 --> 00:18:00,370
And so you had this
transformer architecture

426
00:18:00,370 --> 00:18:05,510
with 307 million parameters that
was used to train this model.

427
00:18:05,510 --> 00:18:08,230
And the second thing that
helped was the amount of data.

428
00:18:08,230 --> 00:18:11,230
So instead of just 1.2
million images from ImageNet,

429
00:18:11,230 --> 00:18:14,090
you suddenly had about 400
million image text pairs

430
00:18:14,090 --> 00:18:15,810
from the internet
that they downloaded

431
00:18:15,810 --> 00:18:17,270
and used that to train.

432
00:18:17,270 --> 00:18:19,850
So that scale, both
in terms of model size

433
00:18:19,850 --> 00:18:23,050
as well as the amount of data,
helped improve performance

434
00:18:23,050 --> 00:18:24,210
quite a lot.

435
00:18:24,210 --> 00:18:26,410
So immediately
after CLIP came out,

436
00:18:26,410 --> 00:18:28,680
people started experimenting
with this objective.

437
00:18:28,680 --> 00:18:31,540
And there's many different
variants of a CLIP

438
00:18:31,540 --> 00:18:33,040
that have come out
over the years.

439
00:18:33,040 --> 00:18:35,460
But one in particular that's
really stood out, came out

440
00:18:35,460 --> 00:18:38,220
in 2022, it's called CoCa.

441
00:18:38,220 --> 00:18:40,140
And CoCa took the CLIP model.

442
00:18:40,140 --> 00:18:42,322
Here, you can see it's
the same objective here.

443
00:18:42,322 --> 00:18:44,280
You've got the image
being encoded on one side.

444
00:18:44,280 --> 00:18:46,240
You've got the text being
encoded on one side.

445
00:18:46,240 --> 00:18:47,865
And then you have
that contrastive loss

446
00:18:47,865 --> 00:18:48,820
between the two.

447
00:18:48,820 --> 00:18:50,820
But they added one
additional thing.

448
00:18:50,820 --> 00:18:53,660
They added a
decoder as well that

449
00:18:53,660 --> 00:18:56,540
took the image features
from the image encoder.

450
00:18:56,540 --> 00:18:59,420
And then they fed it in
through cross-attention

451
00:18:59,420 --> 00:19:01,620
and captioned that image.

452
00:19:01,620 --> 00:19:04,180
And it turns out this
captioning process also

453
00:19:04,180 --> 00:19:07,560
helps the model learn quite
a lot of rich information.

454
00:19:07,560 --> 00:19:09,020
So the general
motivation here is

455
00:19:09,020 --> 00:19:11,500
that it's not
sufficient to just be

456
00:19:11,500 --> 00:19:14,360
able to say that this is an
image of a cat versus a dog,

457
00:19:14,360 --> 00:19:16,460
but to describe
that image in text

458
00:19:16,460 --> 00:19:19,760
requires a lot more information
to be learned by the model.

459
00:19:19,760 --> 00:19:22,660
And so it's-- the hypothesis is
that it's a stronger learning

460
00:19:22,660 --> 00:19:23,580
objective.

461
00:19:23,580 --> 00:19:26,830
And so because of that,
it learns better features.

462
00:19:26,830 --> 00:19:29,350
And we found that
to be true overall.

463
00:19:29,350 --> 00:19:32,310
When you compare CoCa
to CLIP, its performance

464
00:19:32,310 --> 00:19:36,230
improves quite a lot across all
the different ImageNet variants.

465
00:19:36,230 --> 00:19:38,790
And overall, there's a
10% boost in performance

466
00:19:38,790 --> 00:19:41,350
across all of the data sets.

467
00:19:41,350 --> 00:19:44,950
And I think this is the first
time where these foundation

468
00:19:44,950 --> 00:19:48,590
models actually beat all the
models that we had trained

469
00:19:48,590 --> 00:19:50,030
from supervised learning.

470
00:19:50,030 --> 00:19:52,070
So at this point, we had
many different models

471
00:19:52,070 --> 00:19:55,190
that people were putting out
onto online leaderboards.

472
00:19:55,190 --> 00:19:58,550
And in those leaderboards
across the years,

473
00:19:58,550 --> 00:20:00,910
you can see the
trend going upwards

474
00:20:00,910 --> 00:20:02,770
as models are performing
better and better.

475
00:20:02,770 --> 00:20:04,395
And this is, I think,
the turning point

476
00:20:04,395 --> 00:20:07,270
where people abandoned
supervised learning objectives

477
00:20:07,270 --> 00:20:09,750
for image encoders and
instead focus solely

478
00:20:09,750 --> 00:20:11,950
on just pre-training
objectives using

479
00:20:11,950 --> 00:20:15,630
these self-supervised learning
methods from the internet data.

480
00:20:15,630 --> 00:20:17,810
So let's talk about
some advantages of CLIP.

481
00:20:17,810 --> 00:20:20,510
CLIP's got a lot of really fun
things that you can do with it.

482
00:20:20,510 --> 00:20:24,000
It's super easy to train because
it's just simple contrastive

483
00:20:24,000 --> 00:20:25,080
learning objective.

484
00:20:25,080 --> 00:20:27,640
It's also really fast
in terms of inference.

485
00:20:27,640 --> 00:20:32,300
You can embed your entire data
set into some representation,

486
00:20:32,300 --> 00:20:34,240
and then all you have
to do to classify

487
00:20:34,240 --> 00:20:38,560
is just do retrieval on
that embedded data set.

488
00:20:38,560 --> 00:20:40,280
So you can retrieve
things very easily

489
00:20:40,280 --> 00:20:41,960
with CLIP's
representations, which

490
00:20:41,960 --> 00:20:44,320
makes it really useful for
not just classification

491
00:20:44,320 --> 00:20:47,918
tasks but also search and
retrieval tasks as well.

492
00:20:47,918 --> 00:20:49,960
Another thing that people
really liked about CLIP

493
00:20:49,960 --> 00:20:51,220
is that it's open vocabulary.

494
00:20:51,220 --> 00:20:53,145
You can feed in any
text description,

495
00:20:53,145 --> 00:20:54,520
and it should be
able to retrieve

496
00:20:54,520 --> 00:20:56,200
the right images for you.

497
00:20:56,200 --> 00:20:58,920
And so that also allows
for its applicability

498
00:20:58,920 --> 00:21:01,213
across many different domains.

499
00:21:01,213 --> 00:21:03,380
And of course, we're going
to talk about this later.

500
00:21:03,380 --> 00:21:07,380
CLIP is really amenable to
being chained with other models.

501
00:21:07,380 --> 00:21:10,025
And this idea of chaining
started becoming really popular.

502
00:21:10,025 --> 00:21:10,900
But hold off on that.

503
00:21:10,900 --> 00:21:13,982
We'll talk about that
in a few minutes.

504
00:21:13,982 --> 00:21:15,940
Of course, I'm telling
you all the good things.

505
00:21:15,940 --> 00:21:18,520
Turns out there's a
lot of bad as well.

506
00:21:18,520 --> 00:21:20,520
CLIP unfortunately
can distinguish

507
00:21:20,520 --> 00:21:22,200
between these two images.

508
00:21:22,200 --> 00:21:25,380
So you have an image
of a mug in grass

509
00:21:25,380 --> 00:21:27,720
and you have some
grass in some mug

510
00:21:27,720 --> 00:21:30,880
and CLIP just does not know the
difference between these two

511
00:21:30,880 --> 00:21:33,070
things.

512
00:21:33,070 --> 00:21:37,200
The reason it doesn't know is
because the CLIP's learning

513
00:21:37,200 --> 00:21:40,720
objective really depends
on its batch size.

514
00:21:40,720 --> 00:21:43,400
If your batch size
is not large enough,

515
00:21:43,400 --> 00:21:45,240
then all of the
other batch elements

516
00:21:45,240 --> 00:21:48,760
are unlikely to provide
any useful supervision

517
00:21:48,760 --> 00:21:49,720
for the model.

518
00:21:49,720 --> 00:21:52,380
If you're always comparing
a cat versus a truck,

519
00:21:52,380 --> 00:21:56,160
you're not really going to learn
a representation for a cat.

520
00:21:56,160 --> 00:21:59,280
Instead, what you get is
some representation that's

521
00:21:59,280 --> 00:22:02,157
OK at some high level, but if
you increase the batch size,

522
00:22:02,157 --> 00:22:04,240
you're more likely to
encounter other animals that

523
00:22:04,240 --> 00:22:05,800
are similar to the
cat, and then you

524
00:22:05,800 --> 00:22:07,393
learn a much better
representation.

525
00:22:07,393 --> 00:22:09,560
And then, of course, if you
increase your batch size

526
00:22:09,560 --> 00:22:15,000
to let's say 32,000 and you
train it across many, many GPUs,

527
00:22:15,000 --> 00:22:17,860
then suddenly you start learning
really good representations.

528
00:22:17,860 --> 00:22:19,770
You can actually start
identifying a Welsh

529
00:22:19,770 --> 00:22:21,530
Corgi versus another Corgi.

530
00:22:21,530 --> 00:22:24,670
And this only is possible when
you have gigantic batch sizes,

531
00:22:24,670 --> 00:22:27,330
because it requires you to
have other negative examples

532
00:22:27,330 --> 00:22:29,490
in your batch that are
close enough, that are

533
00:22:29,490 --> 00:22:33,350
hard negatives that forces
the model to have to learn.

534
00:22:33,350 --> 00:22:34,850
So that's very
important for getting

535
00:22:34,850 --> 00:22:37,250
these models to work well.

536
00:22:37,250 --> 00:22:39,450
But unfortunately,
regardless of how much

537
00:22:39,450 --> 00:22:41,690
people have tried,
increasing the batch size

538
00:22:41,690 --> 00:22:45,090
doesn't guarantee that the model
will learn a good representation

539
00:22:45,090 --> 00:22:46,030
for things.

540
00:22:46,030 --> 00:22:49,730
And so you're at the mercy of
the randomness of your training

541
00:22:49,730 --> 00:22:51,090
data.

542
00:22:51,090 --> 00:22:52,650
So increasing the
batch size does

543
00:22:52,650 --> 00:22:54,790
help with some amount of
fine-grained concepts.

544
00:22:54,790 --> 00:22:56,570
But of course,
it's still limited.

545
00:22:56,570 --> 00:23:01,930
And training on 32,000 amounts
of batch size is just too large

546
00:23:01,930 --> 00:23:06,330
for most labs to
even consider doing.

547
00:23:06,330 --> 00:23:08,410
People have
identified this error

548
00:23:08,410 --> 00:23:11,690
across many different
benchmarks and identify

549
00:23:11,690 --> 00:23:14,810
that CLIP just doesn't have
this notion of compositionality.

550
00:23:14,810 --> 00:23:17,570
So this idea that
the mug and the grass

551
00:23:17,570 --> 00:23:18,960
versus grass in the mug.

552
00:23:18,960 --> 00:23:21,240
It's really about composing
different concepts,

553
00:23:21,240 --> 00:23:24,380
like the mug and the
grass, and the relationship

554
00:23:24,380 --> 00:23:26,900
in all of those
individual components

555
00:23:26,900 --> 00:23:31,303
are not composed well in
your CLIP representations.

556
00:23:31,303 --> 00:23:33,220
And there's been a ton
of different benchmarks

557
00:23:33,220 --> 00:23:36,180
like Winoground or CREPE or ARO.

558
00:23:36,180 --> 00:23:39,620
And a lot of these benchmarks
have actually come from my lab.

559
00:23:39,620 --> 00:23:42,260
They just keep finding over
and over again that CLIP

560
00:23:42,260 --> 00:23:44,900
has a ton of limitations,
and there's a ton of things

561
00:23:44,900 --> 00:23:46,980
that they are just unable to do.

562
00:23:46,980 --> 00:23:49,620
Now, of course, in reaction,
the community immediately

563
00:23:49,620 --> 00:23:53,900
started thinking about how do
I handcraft my batch so that it

564
00:23:53,900 --> 00:23:57,620
contains the hard negatives,
so if I have one type of corgi

565
00:23:57,620 --> 00:24:00,400
that I should hopefully have
another type of corgi in there.

566
00:24:00,400 --> 00:24:03,940
So your model really is forced
to learn good representations.

567
00:24:03,940 --> 00:24:06,660
And so this idea of
training with hard negatives

568
00:24:06,660 --> 00:24:10,100
became really popular in the
community for a whole one

569
00:24:10,100 --> 00:24:12,940
year, until we released
a follow-up paper that

570
00:24:12,940 --> 00:24:14,760
said that if you train
with hard negatives,

571
00:24:14,760 --> 00:24:18,720
you end up actually unlearning
a lot of things about semantics.

572
00:24:18,720 --> 00:24:20,470
For whatever reason,
and this is something

573
00:24:20,470 --> 00:24:22,550
that we still don't
theoretically understand,

574
00:24:22,550 --> 00:24:25,470
we end up actually with
much worse performance

575
00:24:25,470 --> 00:24:28,630
in generalization across
different environments

576
00:24:28,630 --> 00:24:30,510
and different kind of data sets.

577
00:24:30,510 --> 00:24:32,790
So there's a lot of
work to be done still

578
00:24:32,790 --> 00:24:35,855
in terms of figuring out the
right way of constructing

579
00:24:35,855 --> 00:24:38,230
your data set, the right way
of constructing your batches

580
00:24:38,230 --> 00:24:39,590
and training signal.

581
00:24:39,590 --> 00:24:41,370
So we're still really
far away from that.

582
00:24:41,370 --> 00:24:44,510
But regardless, people are
still very excited about CLIP

583
00:24:44,510 --> 00:24:48,190
in general because it does give
you some amount of supervision

584
00:24:48,190 --> 00:24:49,830
regardless.

585
00:24:49,830 --> 00:24:54,630
Of course, again, image level
captions are again, not enough.

586
00:24:54,630 --> 00:24:57,030
Ideally, what we want
is more than just that.

587
00:24:57,030 --> 00:24:59,310
We want to be able to
identify not just that there's

588
00:24:59,310 --> 00:25:01,670
a person crossing the street,
but that the person is

589
00:25:01,670 --> 00:25:04,910
in this location, the car
is here, the street is here.

590
00:25:04,910 --> 00:25:07,110
All of that information,
that grounding information

591
00:25:07,110 --> 00:25:08,990
is completely missing in CLIP.

592
00:25:08,990 --> 00:25:11,990
And so ideally, you'd
want also your data

593
00:25:11,990 --> 00:25:14,453
set to contain this
kind of information

594
00:25:14,453 --> 00:25:16,120
and your model to
also be able to reason

595
00:25:16,120 --> 00:25:18,160
about that kind of information.

596
00:25:18,160 --> 00:25:21,840
Also, the final thing that's
a big disadvantage for CLIP

597
00:25:21,840 --> 00:25:24,300
is that regardless of
how big your data set is,

598
00:25:24,300 --> 00:25:27,900
even if you collect upwards of,
let's say, 5 billion images,

599
00:25:27,900 --> 00:25:29,880
it's still not going
to be enough to capture

600
00:25:29,880 --> 00:25:33,260
all of the important things
that you might care about.

601
00:25:33,260 --> 00:25:34,760
And so there's been
a lot of efforts

602
00:25:34,760 --> 00:25:36,740
that we've been doing
in data filtering.

603
00:25:36,740 --> 00:25:38,200
So how do you
filter the internet

604
00:25:38,200 --> 00:25:41,660
to find the best training data
for training these CLIP models?

605
00:25:41,660 --> 00:25:43,840
I won't go into that
today, but there

606
00:25:43,840 --> 00:25:47,400
are all of these mechanisms that
people are exploring that's now

607
00:25:47,400 --> 00:25:49,680
become the frontier of
what today's research looks

608
00:25:49,680 --> 00:25:51,480
like in this field.

609
00:25:51,480 --> 00:25:54,400
OK, so that's the first
branch of foundation models

610
00:25:54,400 --> 00:25:55,300
we talked about.

611
00:25:55,300 --> 00:25:57,440
It's about generalizing
classification

612
00:25:57,440 --> 00:25:59,260
to a whole host of tasks.

613
00:25:59,260 --> 00:26:02,860
Now, let's talk about
vision and language models.

614
00:26:02,860 --> 00:26:05,780
So there's a new class
of foundation models,

615
00:26:05,780 --> 00:26:08,820
which has become popular in
the last 2, 2 and 1/2 years,

616
00:26:08,820 --> 00:26:12,930
and we often refer to them as
multi-modal language models.

617
00:26:12,930 --> 00:26:15,050
And I'll start off
with this discussion

618
00:26:15,050 --> 00:26:17,690
by focusing on LLaVA,
which is arguably

619
00:26:17,690 --> 00:26:20,250
one of the first of
multi-modal language models

620
00:26:20,250 --> 00:26:22,210
that became very, very popular.

621
00:26:22,210 --> 00:26:24,970
The motivation here is
that language models, which

622
00:26:24,970 --> 00:26:28,830
do this next token prediction,
this autocomplete process,

623
00:26:28,830 --> 00:26:31,930
that process is really
useful for adapting

624
00:26:31,930 --> 00:26:33,350
to a lot of new tasks.

625
00:26:33,350 --> 00:26:37,830
And so can we start thinking
about even image models as well,

626
00:26:37,830 --> 00:26:39,110
doing the same thing?

627
00:26:39,110 --> 00:26:41,010
Can we give it an
image also start

628
00:26:41,010 --> 00:26:43,730
doing different kinds
of reasoning that's

629
00:26:43,730 --> 00:26:46,370
similar to this
autoregressive process?

630
00:26:46,370 --> 00:26:49,210
And that gave rise to this class
of models called vision language

631
00:26:49,210 --> 00:26:51,250
models or multimodal models.

632
00:26:51,250 --> 00:26:54,590
But of course, this just
to be historically correct,

633
00:26:54,590 --> 00:26:58,530
this idea wasn't
completely new in 2022.

634
00:26:58,530 --> 00:27:02,730
In 2019, ViLBERT actually
introduced this idea.

635
00:27:02,730 --> 00:27:05,250
There's a paper called
ViLBERT in 2019 that

636
00:27:05,250 --> 00:27:08,010
took these image models
and language models

637
00:27:08,010 --> 00:27:12,620
and put them all together to
accomplish a generalization

638
00:27:12,620 --> 00:27:15,340
across different tasks,
but they were all

639
00:27:15,340 --> 00:27:20,580
trained pre-transformers and
also mostly use LSTMs instead.

640
00:27:20,580 --> 00:27:23,220
And so the rebirth
of all of this

641
00:27:23,220 --> 00:27:26,780
is what's happening right
now with LLaVA, where

642
00:27:26,780 --> 00:27:30,080
a lot of these models switched
over to a better architecture,

643
00:27:30,080 --> 00:27:33,160
switched over to a
better set of objectives,

644
00:27:33,160 --> 00:27:35,640
and now aren't just training
on individual tasks,

645
00:27:35,640 --> 00:27:39,600
but are training on a foundation
of a variety of different tasks,

646
00:27:39,600 --> 00:27:42,620
using some pre-training
objective from the internet.

647
00:27:42,620 --> 00:27:43,560
So how does this work?

648
00:27:43,560 --> 00:27:45,500
How do you think about LLaVA?

649
00:27:45,500 --> 00:27:49,260
So to talk about LLaVA,
let's take a step back

650
00:27:49,260 --> 00:27:53,340
and think about the transformer
model or self-attention

651
00:27:53,340 --> 00:27:54,540
in particular.

652
00:27:54,540 --> 00:27:58,180
So when we think about language
models, what they're doing

653
00:27:58,180 --> 00:28:00,155
is they're attending
over the past.

654
00:28:00,155 --> 00:28:02,280
So you have a sequence of
words that are coming in.

655
00:28:02,280 --> 00:28:04,980
So for example,
cats are so and then

656
00:28:04,980 --> 00:28:07,060
your model to generate
the next word,

657
00:28:07,060 --> 00:28:09,830
will attend over that
historical context

658
00:28:09,830 --> 00:28:12,470
and then generate what it
thinks the next word should be.

659
00:28:12,470 --> 00:28:14,230
So it might think
that the phrase

660
00:28:14,230 --> 00:28:16,687
should be cats are so cute.

661
00:28:16,687 --> 00:28:18,270
And here's another
way of representing

662
00:28:18,270 --> 00:28:19,770
that same objective.

663
00:28:19,770 --> 00:28:22,090
You've got the input text
coming in at the bottom,

664
00:28:22,090 --> 00:28:25,590
and your model will generate
the next word, which is cute.

665
00:28:25,590 --> 00:28:28,750
So when we think about vision
language models, what people

666
00:28:28,750 --> 00:28:32,710
usually are referring to is
adding in additional context

667
00:28:32,710 --> 00:28:34,550
by grounding that
conversation that we're

668
00:28:34,550 --> 00:28:37,910
having with some image
that we care about.

669
00:28:37,910 --> 00:28:41,310
So we might care about
tokenizing our image somehow

670
00:28:41,310 --> 00:28:44,070
and feeding those tokens
into our language model

671
00:28:44,070 --> 00:28:46,890
along with the historical
context of cats or so,

672
00:28:46,890 --> 00:28:49,350
and then using that
to autocomplete

673
00:28:49,350 --> 00:28:51,670
the rest of the description.

674
00:28:51,670 --> 00:28:54,030
So that's the basic
idea behind LLaVA

675
00:28:54,030 --> 00:28:57,270
is feed in these image tokens,
along with the words that

676
00:28:57,270 --> 00:28:59,470
are being generated to
continuously generate

677
00:28:59,470 --> 00:29:02,070
more words about that image.

678
00:29:02,070 --> 00:29:03,810
So of course, a
question comes in,

679
00:29:03,810 --> 00:29:05,890
which is how do you
define these tokens?

680
00:29:05,890 --> 00:29:08,600
What should these tokens
be in the first place?

681
00:29:08,600 --> 00:29:12,960
And LLaVA, their solution was
to use the CLIP image encoder.

682
00:29:12,960 --> 00:29:16,200
So they took the CLIP model,
they took the image encoder

683
00:29:16,200 --> 00:29:19,440
and then they basically
extracted tokens

684
00:29:19,440 --> 00:29:21,283
from that encoder.

685
00:29:21,283 --> 00:29:23,200
So the first thing you
might think about doing

686
00:29:23,200 --> 00:29:25,080
is just using the CLS token.

687
00:29:25,080 --> 00:29:26,520
So here you've got--

688
00:29:26,520 --> 00:29:28,800
let me see if my
mouse works here.

689
00:29:28,800 --> 00:29:29,300
Oh, it does.

690
00:29:29,300 --> 00:29:31,940
So you've got the image
coming in over here

691
00:29:31,940 --> 00:29:33,760
and then they're
getting patched.

692
00:29:33,760 --> 00:29:35,840
Each patch turns
into a representation

693
00:29:35,840 --> 00:29:39,440
that's fed into your transformer
architecture in CLIP.

694
00:29:39,440 --> 00:29:41,720
It goes through a bunch
of layers of processing.

695
00:29:41,720 --> 00:29:44,080
And then at the end, you get
a bunch of different tokens

696
00:29:44,080 --> 00:29:47,000
for each of the patches,
along with a representation

697
00:29:47,000 --> 00:29:48,560
for the CLS token.

698
00:29:48,560 --> 00:29:51,500
And so far, we've only been
considering the CLS token.

699
00:29:51,500 --> 00:29:53,720
We've only been doing
things with the CLS token

700
00:29:53,720 --> 00:29:56,060
for doing any sort of
classification task.

701
00:29:56,060 --> 00:29:59,400
But there are all of these
other tokens in there as well.

702
00:29:59,400 --> 00:30:01,160
Now, the problem with
these other tokens

703
00:30:01,160 --> 00:30:03,160
is that they're
never supervised.

704
00:30:03,160 --> 00:30:06,090
So the CLS token is supervised
with this contrastive objective

705
00:30:06,090 --> 00:30:10,490
with text, but the other tokens
are never used for any purpose.

706
00:30:10,490 --> 00:30:13,070
So they might not actually
contain any useful information.

707
00:30:13,070 --> 00:30:16,170
And empirically, people have
shown that these features are

708
00:30:16,170 --> 00:30:18,450
not very useful, but
what they have shown

709
00:30:18,450 --> 00:30:22,290
is that if you go one more layer
back, the penultimate layer

710
00:30:22,290 --> 00:30:26,370
in your CLIP encoder,
these features are actually

711
00:30:26,370 --> 00:30:27,770
very useful.

712
00:30:27,770 --> 00:30:30,450
So these features are used
to generate the final CLIP

713
00:30:30,450 --> 00:30:32,610
embedding in the final layer.

714
00:30:32,610 --> 00:30:35,370
And they contain a lot
of spatial information

715
00:30:35,370 --> 00:30:39,050
about where objects are
in your entire image.

716
00:30:39,050 --> 00:30:40,890
And so this is what
people typically

717
00:30:40,890 --> 00:30:49,010
use when combining CLIP encoder
with a transformer LLM-based

718
00:30:49,010 --> 00:30:49,890
model.

719
00:30:49,890 --> 00:30:52,630
So this is what the entire
LLaVA architecture looks like.

720
00:30:52,630 --> 00:30:56,090
You feed an image
through your CLIP--

721
00:30:56,090 --> 00:30:58,650
pretrained CLIP
encoder, you extract

722
00:30:58,650 --> 00:31:01,690
a bunch of features from
it, you take those features

723
00:31:01,690 --> 00:31:05,800
and you pass it through a linear
layer that you need to train.

724
00:31:05,800 --> 00:31:07,740
And what this linear
layer will train to do

725
00:31:07,740 --> 00:31:11,420
is convert your CLIP
representations into something

726
00:31:11,420 --> 00:31:14,460
that the LLM can understand
and make sense of.

727
00:31:14,460 --> 00:31:16,580
And once you have
these tokens, you now

728
00:31:16,580 --> 00:31:20,240
basically pass in all of your
tokens to your language model.

729
00:31:20,240 --> 00:31:22,940
And it can now
generate conversations

730
00:31:22,940 --> 00:31:26,500
about that image itself.

731
00:31:26,500 --> 00:31:28,780
So LLaVA was one of the
very first popular models

732
00:31:28,780 --> 00:31:29,840
that were out there.

733
00:31:29,840 --> 00:31:33,460
And following up, Google
quickly released Flamingo,

734
00:31:33,460 --> 00:31:36,620
and in Flamingo, it followed
very much the entire LLaVA

735
00:31:36,620 --> 00:31:40,300
setup of being able to
combine a vision encoders

736
00:31:40,300 --> 00:31:42,560
features with a
large language model,

737
00:31:42,560 --> 00:31:44,260
but the place where
they innovated

738
00:31:44,260 --> 00:31:48,100
is how do you do that fusing
of these different features.

739
00:31:48,100 --> 00:31:51,460
So in LLaVA, you had
the features coming in

740
00:31:51,460 --> 00:31:54,500
through a linear layer and
fed in as part of the input.

741
00:31:54,500 --> 00:31:56,620
In Flamingo, what
they did instead

742
00:31:56,620 --> 00:31:59,260
was they basically took
all the features coming out

743
00:31:59,260 --> 00:32:01,180
of your vision
encoder and fed them

744
00:32:01,180 --> 00:32:03,750
into every layer of your LLM.

745
00:32:03,750 --> 00:32:04,250
OK.

746
00:32:04,250 --> 00:32:07,510
So they had made some-- they had
to make some changes to the LLM

747
00:32:07,510 --> 00:32:08,790
architecture itself.

748
00:32:08,790 --> 00:32:10,890
And this is how they
made those changes.

749
00:32:10,890 --> 00:32:13,190
So here's an example of
what Flamingo's training

750
00:32:13,190 --> 00:32:14,210
data looks like.

751
00:32:14,210 --> 00:32:15,810
You've got images
that are encoded.

752
00:32:15,810 --> 00:32:18,250
So you got this dog and
you've got this cat,

753
00:32:18,250 --> 00:32:19,630
they both get embedded.

754
00:32:19,630 --> 00:32:22,430
And they're both going to be
fed into every single layer

755
00:32:22,430 --> 00:32:24,190
of your LLM.

756
00:32:24,190 --> 00:32:26,350
And down here you've
got a data that's

757
00:32:26,350 --> 00:32:30,070
of describing every single
starts with the image

758
00:32:30,070 --> 00:32:32,790
and describes that image,
then the next image

759
00:32:32,790 --> 00:32:35,230
describes the next image,
and so on and so forth.

760
00:32:35,230 --> 00:32:38,030
And they're sort of fed
in as input to your LLM.

761
00:32:38,030 --> 00:32:43,030
And your output is going to be
to autocomplete that last image.

762
00:32:43,030 --> 00:32:44,710
So you've got one
image, followed

763
00:32:44,710 --> 00:32:47,037
by a description of
the dog, second image

764
00:32:47,037 --> 00:32:48,870
and you start the
description and your model

765
00:32:48,870 --> 00:32:51,550
will be trained on auto
completing that description

766
00:32:51,550 --> 00:32:53,350
for that second image.

767
00:32:53,350 --> 00:32:54,170
So what do they do?

768
00:32:54,170 --> 00:32:56,330
What do they change
to the model itself?

769
00:32:56,330 --> 00:33:01,000
They added this GATED X
cross-attention attention module

770
00:33:01,000 --> 00:33:03,760
to every single
layer of your LLM.

771
00:33:03,760 --> 00:33:05,580
And they made one other change.

772
00:33:05,580 --> 00:33:08,080
They also added this
perceiver sampler

773
00:33:08,080 --> 00:33:12,720
right here that basically
samples and downsamples

774
00:33:12,720 --> 00:33:14,300
your image representations.

775
00:33:14,300 --> 00:33:17,400
So they're smaller dimensions
and a fixed number of tokens

776
00:33:17,400 --> 00:33:18,680
for every single layer.

777
00:33:18,680 --> 00:33:22,640
So let me go into some details
with what they look like.

778
00:33:22,640 --> 00:33:24,840
So this is the full
architecture, overall.

779
00:33:24,840 --> 00:33:26,500
Most of the
components are frozen,

780
00:33:26,500 --> 00:33:28,660
all the language model
weights are frozen,

781
00:33:28,660 --> 00:33:30,860
all the vision model
parts are frozen.

782
00:33:30,860 --> 00:33:32,400
The only parts that
are trained are

783
00:33:32,400 --> 00:33:34,500
these perceiver
sampler components

784
00:33:34,500 --> 00:33:36,680
and this cross-attention
layer that

785
00:33:36,680 --> 00:33:41,000
added into every single
layer of your LLM.

786
00:33:41,000 --> 00:33:44,040
So let's talk about what this
cross-attention module looks

787
00:33:44,040 --> 00:33:44,840
like.

788
00:33:44,840 --> 00:33:47,900
This is me zooming into
that cross-attention module.

789
00:33:47,900 --> 00:33:51,520
So every single LLM layer,
right before the LLM layer,

790
00:33:51,520 --> 00:33:53,660
you have this
cross-attention component.

791
00:33:53,660 --> 00:33:56,840
And what its purpose is to
look at the image features

792
00:33:56,840 --> 00:33:58,930
and then decide what
parts of the image

793
00:33:58,930 --> 00:34:01,690
features it wants to keep
around, and what it thinks

794
00:34:01,690 --> 00:34:04,810
would be useful for the
language model to know about.

795
00:34:04,810 --> 00:34:08,210
And they designed it
as a set of components

796
00:34:08,210 --> 00:34:10,170
that you've already seen so far.

797
00:34:10,170 --> 00:34:12,530
So you attend over
the image features

798
00:34:12,530 --> 00:34:15,210
using a cross-attention
layer and then following

799
00:34:15,210 --> 00:34:19,268
that cross-attention, they added
this tanh nonlinear activation.

800
00:34:19,268 --> 00:34:21,810
And this is basically deciding
what parts of these components

801
00:34:21,810 --> 00:34:25,130
do I want to keep around, which
ones or which parts of the image

802
00:34:25,130 --> 00:34:26,929
do I want to forget.

803
00:34:26,929 --> 00:34:29,449
And then it goes through
this fully connected layer,

804
00:34:29,449 --> 00:34:32,190
where it adapts those
representations a little bit,

805
00:34:32,190 --> 00:34:35,409
and then again a tanh
nonlinearity to decide again

806
00:34:35,409 --> 00:34:38,062
which parts it should keep
and which parts it shouldn't.

807
00:34:38,062 --> 00:34:39,770
Once it goes through
those two components

808
00:34:39,770 --> 00:34:43,489
and each one has a residual
connection across [INAUDIBLE]

809
00:34:43,489 --> 00:34:46,570
it then goes to your normal
language model processing

810
00:34:46,570 --> 00:34:49,449
and then continues to
generate the word it needs to.

811
00:34:49,449 --> 00:34:52,409
So there's additional
layers are being added just

812
00:34:52,409 --> 00:34:56,409
as a way for the language
to incorporate and attend

813
00:34:56,409 --> 00:34:59,290
over the visual features
at every single layer.

814
00:34:59,290 --> 00:34:59,790
OK.

815
00:34:59,790 --> 00:35:01,382
So the actual
modification itself,

816
00:35:01,382 --> 00:35:03,590
if you're interested in what
this looks like in code,

817
00:35:03,590 --> 00:35:06,730
is just about two or
three lines of code,

818
00:35:06,730 --> 00:35:09,570
where they added this
cross-attention layer and then

819
00:35:09,570 --> 00:35:11,625
this tanh non-linearity
in between.

820
00:35:11,625 --> 00:35:12,750
And that's really about it.

821
00:35:12,750 --> 00:35:15,170
So in terms of code, it's
a very minimal change.

822
00:35:15,170 --> 00:35:17,610
Although for the model,
it's a very gigantic change

823
00:35:17,610 --> 00:35:20,010
because now it can choose
what parts of the image

824
00:35:20,010 --> 00:35:23,470
to attend to at every single
layer of its processing.

825
00:35:23,470 --> 00:35:25,570
So you give the model
a lot of ability

826
00:35:25,570 --> 00:35:30,330
to decide when and how to
attend over the vision features.

827
00:35:30,330 --> 00:35:31,050
OK.

828
00:35:31,050 --> 00:35:33,570
So Flamingo was
very, very exciting,

829
00:35:33,570 --> 00:35:35,890
but training it
was very difficult.

830
00:35:35,890 --> 00:35:38,170
And they had this really
ingenious way of training it

831
00:35:38,170 --> 00:35:41,570
that allowed our models to
adapt to many different tasks.

832
00:35:41,570 --> 00:35:45,970
The way they trained it was
through this concatenation

833
00:35:45,970 --> 00:35:48,750
of a bunch of different
images together.

834
00:35:48,750 --> 00:35:51,430
So you didn't just have one
image and one description.

835
00:35:51,430 --> 00:35:55,140
You had a description at the
beginning that says, here

836
00:35:55,140 --> 00:35:57,180
are some cute
pictures of my pets

837
00:35:57,180 --> 00:35:59,600
and a sentence
beginning of image,

838
00:35:59,600 --> 00:36:01,620
and then a description
of that first image

839
00:36:01,620 --> 00:36:06,420
and of that first component,
and then the second image

840
00:36:06,420 --> 00:36:08,160
and a description
of the second image.

841
00:36:08,160 --> 00:36:10,380
So you had the
training set up so

842
00:36:10,380 --> 00:36:14,180
that it looks like a long
sequence of image text-- image

843
00:36:14,180 --> 00:36:16,500
text interleaved data.

844
00:36:16,500 --> 00:36:19,120
And of course, when
describing any single image,

845
00:36:19,120 --> 00:36:22,480
you don't want the model to
look at the entire context.

846
00:36:22,480 --> 00:36:24,760
You want it to only look at
that one particular image.

847
00:36:24,760 --> 00:36:27,220
And so they created
a masking scheme,

848
00:36:27,220 --> 00:36:31,260
where every single image
when generating only

849
00:36:31,260 --> 00:36:34,100
looks at that particular image
features and not the other ones.

850
00:36:34,100 --> 00:36:35,660
Meaning that when
you're generating

851
00:36:35,660 --> 00:36:38,320
the description for my puppy
is sitting in the grass,

852
00:36:38,320 --> 00:36:40,940
you're only looking
at the features

853
00:36:40,940 --> 00:36:42,820
that correspond to
the puppy, only when

854
00:36:42,820 --> 00:36:44,120
generating those words.

855
00:36:44,120 --> 00:36:47,100
Similarly, when generating
the description for the cat,

856
00:36:47,100 --> 00:36:50,780
you're only looking at the cat
image and not the other image.

857
00:36:50,780 --> 00:36:53,150
So there is this of
distinction where

858
00:36:53,150 --> 00:36:55,790
they created this
handcrafted masking

859
00:36:55,790 --> 00:36:58,750
seam to make sure your
descriptions are always

860
00:36:58,750 --> 00:37:02,030
following and looking at
only that particular image.

861
00:37:02,030 --> 00:37:04,070
But when trained,
the model does get

862
00:37:04,070 --> 00:37:08,750
to see the entire context of
everything that it's generating.

863
00:37:08,750 --> 00:37:09,890
So why is that helpful?

864
00:37:09,890 --> 00:37:11,910
Why is this entire
process helpful of being

865
00:37:11,910 --> 00:37:13,810
able to see all of
this stuff together?

866
00:37:13,810 --> 00:37:15,830
Well, it's helpful because
it allows you to do

867
00:37:15,830 --> 00:37:17,550
these kinds of applications.

868
00:37:17,550 --> 00:37:19,950
So here are three different
applications that Flamingo

869
00:37:19,950 --> 00:37:21,750
was able to showcase.

870
00:37:21,750 --> 00:37:25,390
And they all center around
having multiple conversations

871
00:37:25,390 --> 00:37:27,270
or dealing with multiple images.

872
00:37:27,270 --> 00:37:30,710
So in the first case, you've
got an image that's fed in

873
00:37:30,710 --> 00:37:33,110
and the flamingo model
describes the image

874
00:37:33,110 --> 00:37:37,150
by saying that this is a picture
of two teddy bears on the moon.

875
00:37:37,150 --> 00:37:38,950
And then what it
allows people to do

876
00:37:38,950 --> 00:37:40,950
is then ask another
question so people

877
00:37:40,950 --> 00:37:42,630
can ask, what are they doing.

878
00:37:42,630 --> 00:37:45,430
And because it's already being--

879
00:37:45,430 --> 00:37:48,150
it's training using an
existing large language

880
00:37:48,150 --> 00:37:50,950
model, that large language
models reasoning capabilities

881
00:37:50,950 --> 00:37:53,360
are inherited and now
we can reason and answer

882
00:37:53,360 --> 00:37:54,760
this particular question.

883
00:37:54,760 --> 00:37:56,920
It can now answer and
say the teddy bears

884
00:37:56,920 --> 00:37:58,340
are having a conversation.

885
00:37:58,340 --> 00:38:01,182
And then a user might ask,
what objects are they using.

886
00:38:01,182 --> 00:38:02,640
And again, Flamingo
can say that it

887
00:38:02,640 --> 00:38:04,900
looks like it's a computer,
and so on and so forth.

888
00:38:04,900 --> 00:38:07,480
So you can enable this
multi-turn dialogue

889
00:38:07,480 --> 00:38:10,820
about an image simply
by doing two things.

890
00:38:10,820 --> 00:38:14,000
You train by first
pre-training the language model

891
00:38:14,000 --> 00:38:16,540
and then incorporating the
language model into Flamingo.

892
00:38:16,540 --> 00:38:18,200
And secondly,
allowing your model

893
00:38:18,200 --> 00:38:20,680
to see many different
images and many different

894
00:38:20,680 --> 00:38:23,040
turns throughout
its training data

895
00:38:23,040 --> 00:38:26,600
so it can adapt to
longer sequences of text.

896
00:38:26,600 --> 00:38:28,480
You can also give
it multiple images

897
00:38:28,480 --> 00:38:31,300
and ask, what is a common
thing about these images.

898
00:38:31,300 --> 00:38:32,800
And now the Flamingo
model will look

899
00:38:32,800 --> 00:38:35,200
at each of those different
components and reason

900
00:38:35,200 --> 00:38:37,440
and say that they're
all Flamingos.

901
00:38:37,440 --> 00:38:40,540
So you can start doing a lot of
these really cool applications.

902
00:38:40,540 --> 00:38:43,512
People also showed that you can
start doing in-context learning.

903
00:38:43,512 --> 00:38:45,720
I don't know if this is
something you've seen already

904
00:38:45,720 --> 00:38:48,040
with language models,
but I'm sure you've

905
00:38:48,040 --> 00:38:51,690
used in-context learning with
GPT, where you tell GPT here's

906
00:38:51,690 --> 00:38:55,090
an example of what I want,
give me more things like this.

907
00:38:55,090 --> 00:38:56,970
And you can do the same
thing with Flamingo,

908
00:38:56,970 --> 00:38:59,970
where you can pass in an image
and a description, an image

909
00:38:59,970 --> 00:39:01,030
and a description.

910
00:39:01,030 --> 00:39:02,710
And now when you
pass in a new image,

911
00:39:02,710 --> 00:39:04,410
it'll give you a description.

912
00:39:04,410 --> 00:39:07,990
Or you can say,
here's some image,

913
00:39:07,990 --> 00:39:09,350
here's a question and answer.

914
00:39:09,350 --> 00:39:11,130
Here's an image
question and answer.

915
00:39:11,130 --> 00:39:13,930
And then when you pass in a new
image and just ask the question,

916
00:39:13,930 --> 00:39:15,485
it'll give you the answer.

917
00:39:15,485 --> 00:39:18,110
So you're not training it to do
these different kinds of tasks,

918
00:39:18,110 --> 00:39:21,250
but you're providing it
with examples of behaviors

919
00:39:21,250 --> 00:39:22,890
that it should have,
and it should just

920
00:39:22,890 --> 00:39:25,490
generalize to new
kinds of behaviors

921
00:39:25,490 --> 00:39:26,790
that you might care about.

922
00:39:26,790 --> 00:39:29,570
Similarly, you might care
about just classification.

923
00:39:29,570 --> 00:39:32,250
And you can use Flamingo to
do classification as well.

924
00:39:32,250 --> 00:39:35,390
So you can give it an image
and say this is underground,

925
00:39:35,390 --> 00:39:37,010
this is Congress.

926
00:39:37,010 --> 00:39:39,050
And then you can
ask, what is this.

927
00:39:39,050 --> 00:39:43,330
And you can also even teach
it to do OCR and math,

928
00:39:43,330 --> 00:39:45,930
where you give it an image and
say, oh, this should correspond

929
00:39:45,930 --> 00:39:47,370
to 2 plus 1 equals 3.

930
00:39:47,370 --> 00:39:49,760
And so eventually, when
you give it a new image,

931
00:39:49,760 --> 00:39:51,980
it should be able to
autocomplete and extract out

932
00:39:51,980 --> 00:39:55,740
3 times 6 and then also give
you the output by reasoning

933
00:39:55,740 --> 00:39:57,240
through this entire process.

934
00:39:57,240 --> 00:39:57,740
Yeah.

935
00:39:57,740 --> 00:39:59,740
So this would be an example
of few shot learning

936
00:39:59,740 --> 00:40:02,500
where you give it some examples,
a few examples of things,

937
00:40:02,500 --> 00:40:06,060
and then you ask it what
the new thing should be.

938
00:40:06,060 --> 00:40:09,040
If I were to throw away all
the in-context examples,

939
00:40:09,040 --> 00:40:10,700
that would be zero
shot learning.

940
00:40:10,700 --> 00:40:12,860
So we're not concatenating them.

941
00:40:12,860 --> 00:40:15,340
We're technically
passing the image tokens

942
00:40:15,340 --> 00:40:17,860
through this perceiver sampler
into every single layer

943
00:40:17,860 --> 00:40:19,460
of your LLM instead.

944
00:40:19,460 --> 00:40:22,820
And so only the text is ever
concatenated and fed as input

945
00:40:22,820 --> 00:40:25,060
to your Flamingo
model and it chooses

946
00:40:25,060 --> 00:40:28,100
when to attend to which
parts of the image.

947
00:40:28,100 --> 00:40:29,500
You give it to it once.

948
00:40:29,500 --> 00:40:31,400
But behind the
scenes, of course,

949
00:40:31,400 --> 00:40:33,320
this is the interface--
the web interface.

950
00:40:33,320 --> 00:40:35,300
But behind the scenes,
what they actually do is

951
00:40:35,300 --> 00:40:37,780
they cache the model,
assuming that the user will

952
00:40:37,780 --> 00:40:39,380
want to continue talking.

953
00:40:39,380 --> 00:40:42,740
And so the model is cached and
ready to accept more tokens.

954
00:40:42,740 --> 00:40:44,800
Yeah, but if they did
not cache it then yes,

955
00:40:44,800 --> 00:40:47,890
it would pass in this entire
conversation as input.

956
00:40:47,890 --> 00:40:49,950
Yeah.

957
00:40:49,950 --> 00:40:50,550
OK.

958
00:40:50,550 --> 00:40:52,990
So Flamingo was super
cool because they

959
00:40:52,990 --> 00:40:54,790
have these really big
tables in their paper

960
00:40:54,790 --> 00:40:56,270
that you can go check out.

961
00:40:56,270 --> 00:40:57,870
But what was really
cool about it is

962
00:40:57,870 --> 00:41:01,050
there were all of these tasks
that were very difficult,

963
00:41:01,050 --> 00:41:03,550
and you had to adapt
CLIP to do them.

964
00:41:03,550 --> 00:41:06,990
But Flamingo was just able to do
it with zero shot or few shot.

965
00:41:06,990 --> 00:41:09,310
And you start seeing these
gigantic improvements

966
00:41:09,310 --> 00:41:11,510
across many
different benchmarks.

967
00:41:11,510 --> 00:41:13,070
And this is when,
I think, the field

968
00:41:13,070 --> 00:41:15,950
shifted from reporting on a
few classification benchmarks

969
00:41:15,950 --> 00:41:18,810
to reporting on any
understanding task at all.

970
00:41:18,810 --> 00:41:21,690
As long as you can frame it as
a question answering process,

971
00:41:21,690 --> 00:41:25,810
you can build benchmarks for
a wide variety of skills,

972
00:41:25,810 --> 00:41:28,750
and we started seeing that
become the norm in the last two

973
00:41:28,750 --> 00:41:31,470
years in the computer
vision field.

974
00:41:31,470 --> 00:41:35,270
So this is where we were,
I think, sometime last year

975
00:41:35,270 --> 00:41:38,390
and seeing the success of
LLaVA, a lot of companies

976
00:41:38,390 --> 00:41:41,890
started investing quite
heavily on these models.

977
00:41:41,890 --> 00:41:44,920
And so you start seeing
a lot of API models

978
00:41:44,920 --> 00:41:52,360
like GPT-40 GPT-4V, Gemini
1.5 Pro, Gemini 1.5 Flash.

979
00:41:52,360 --> 00:41:54,980
A lot of these models
started being released,

980
00:41:54,980 --> 00:41:58,760
and even Anthropic came into
the picture with Claude 3 Opus.

981
00:41:58,760 --> 00:42:01,880
And now of course,
Claude 4 Opus is out.

982
00:42:01,880 --> 00:42:03,980
And so you had a lot of
these models come out,

983
00:42:03,980 --> 00:42:05,760
and they were
performing a lot better

984
00:42:05,760 --> 00:42:07,240
on a bunch of these benchmarks.

985
00:42:07,240 --> 00:42:10,120
So here I'm showing you the
average performance across 11

986
00:42:10,120 --> 00:42:13,280
of the more popular visual
understanding benchmarks

987
00:42:13,280 --> 00:42:14,340
in the field.

988
00:42:14,340 --> 00:42:16,360
And there's this
gigantic difference.

989
00:42:16,360 --> 00:42:19,920
So the difference between LLaVA,
the model we talked about that's

990
00:42:19,920 --> 00:42:24,040
open source, that's down here at
about 43% accuracy on average.

991
00:42:24,040 --> 00:42:27,240
Meanwhile GPT and all of these
other models are performing

992
00:42:27,240 --> 00:42:32,780
much, much better at about
somewhere like 80 or high 70s.

993
00:42:32,780 --> 00:42:34,400
So a big difference
in performance

994
00:42:34,400 --> 00:42:37,080
between these two
different kinds of models.

995
00:42:37,080 --> 00:42:39,900
Of course, immediately
seeing this difference,

996
00:42:39,900 --> 00:42:43,370
people started
distilling GPT and Gemini

997
00:42:43,370 --> 00:42:48,130
into distilled variants and
trying to release those models.

998
00:42:48,130 --> 00:42:50,150
So Alibaba, which is
a company in China,

999
00:42:50,150 --> 00:42:52,490
they released this
model called QWEN.

1000
00:42:52,490 --> 00:42:54,970
And then there's
Intern VL, there's Phi.

1001
00:42:54,970 --> 00:42:57,470
There's all of these different
models that start coming out.

1002
00:42:57,470 --> 00:43:00,530
And all of them were
distilled from GPT.

1003
00:43:00,530 --> 00:43:02,810
And if not GPT, then Gemini.

1004
00:43:02,810 --> 00:43:05,890
Now that led to a big
problem in the field,

1005
00:43:05,890 --> 00:43:09,090
a problem that's become a big
part of what my own research

1006
00:43:09,090 --> 00:43:12,170
agenda has been trying
to focus on, which is we

1007
00:43:12,170 --> 00:43:14,210
don't actually know as
a research community

1008
00:43:14,210 --> 00:43:17,730
how to build really performant
vision language models.

1009
00:43:17,730 --> 00:43:20,270
The tricks behind
how to build them,

1010
00:43:20,270 --> 00:43:23,130
they only the people
in OpenAI and Gemini

1011
00:43:23,130 --> 00:43:26,390
in Google, those teams know how
to build these kinds of models.

1012
00:43:26,390 --> 00:43:28,950
But the open source
community, they're down here.

1013
00:43:28,950 --> 00:43:29,870
They're down here.

1014
00:43:29,870 --> 00:43:33,010
This is where the research
community was as of last year.

1015
00:43:33,010 --> 00:43:36,110
Of course, you can argue these
are really nice open models,

1016
00:43:36,110 --> 00:43:37,650
but they're not really open.

1017
00:43:37,650 --> 00:43:39,650
Because they're distilled,
we don't actually

1018
00:43:39,650 --> 00:43:42,980
know how to reproduce
these models.

1019
00:43:42,980 --> 00:43:44,640
We can only produce them.

1020
00:43:44,640 --> 00:43:46,160
But if GPT exists.

1021
00:43:46,160 --> 00:43:48,300
But if GPT doesn't
exist, we don't know how

1022
00:43:48,300 --> 00:43:50,020
to create these other models.

1023
00:43:50,020 --> 00:43:51,717
And so when my own
research agenda

1024
00:43:51,717 --> 00:43:53,800
has been focused on over
the last couple of years,

1025
00:43:53,800 --> 00:43:55,640
is figuring out how
do you close this gap,

1026
00:43:55,640 --> 00:43:58,980
how do you build really good
multi-modal language models

1027
00:43:58,980 --> 00:44:01,340
and disseminate
that understanding

1028
00:44:01,340 --> 00:44:03,060
to the entire community.

1029
00:44:03,060 --> 00:44:08,860
And so what we've done over the
last six months or about a year

1030
00:44:08,860 --> 00:44:12,980
now, is we've created
our own a class of models

1031
00:44:12,980 --> 00:44:14,340
that we called Molmo.

1032
00:44:14,340 --> 00:44:17,540
And I'm showing you Molmo's
performance up at the top.

1033
00:44:17,540 --> 00:44:20,180
And what sets Molmo apart
from all the other models out

1034
00:44:20,180 --> 00:44:23,000
there is that it's
completely open source,

1035
00:44:23,000 --> 00:44:25,160
meaning it's open weights.

1036
00:44:25,160 --> 00:44:26,560
So you can download the model.

1037
00:44:26,560 --> 00:44:29,060
It's open data, meaning you can
download the training set as

1038
00:44:29,060 --> 00:44:30,223
well as the evaluation set.

1039
00:44:30,223 --> 00:44:32,140
It's also open code,
meaning you can basically

1040
00:44:32,140 --> 00:44:34,360
train your own Molmo
in your own home,

1041
00:44:34,360 --> 00:44:37,700
assuming you have enough
GPUs, and you can also

1042
00:44:37,700 --> 00:44:39,810
evaluate, add on
new evaluations,

1043
00:44:39,810 --> 00:44:42,930
adapt this model for all kinds
of new things, and of course,

1044
00:44:42,930 --> 00:44:46,430
start using it for a wide
variety of different contexts.

1045
00:44:46,430 --> 00:44:49,270
Now, of course, academic
benchmarks are not enough.

1046
00:44:49,270 --> 00:44:51,790
Because what we care about
at the end of the day is

1047
00:44:51,790 --> 00:44:53,610
are people going to
use these models?

1048
00:44:53,610 --> 00:44:56,830
Will people want to use
these models over GPT?

1049
00:44:56,830 --> 00:45:00,050
And so to make sure we had
that evaluation properly done,

1050
00:45:00,050 --> 00:45:02,690
we released a
playground with Molmo,

1051
00:45:02,690 --> 00:45:05,030
and we did a
gigantic user study,

1052
00:45:05,030 --> 00:45:07,510
where we actually compared
head to head outputs

1053
00:45:07,510 --> 00:45:10,990
from our models versus outputs
from all the other models.

1054
00:45:10,990 --> 00:45:14,710
And our model has the
same Elo rating as GPT.

1055
00:45:14,710 --> 00:45:17,910
It comes in second with
a difference of 1 in Elo

1056
00:45:17,910 --> 00:45:19,790
rating versus GPT-40.

1057
00:45:19,790 --> 00:45:22,670
This is that same
graph rotated so that I

1058
00:45:22,670 --> 00:45:24,390
can show you some examples.

1059
00:45:24,390 --> 00:45:26,770
This was a gigantic
evaluation by the way.

1060
00:45:26,770 --> 00:45:31,010
This is about 870 users that we
showed these model outputs to.

1061
00:45:31,010 --> 00:45:34,030
And we did about 325,000
pairwise comparisons,

1062
00:45:34,030 --> 00:45:36,800
where we asked people which
models output do you prefer.

1063
00:45:36,800 --> 00:45:39,340
Our model, the Moima
model, ranked again,

1064
00:45:39,340 --> 00:45:41,303
like I said, second,
more or less a coin

1065
00:45:41,303 --> 00:45:42,720
flip between what
people preferred

1066
00:45:42,720 --> 00:45:44,960
between GPT and our model.

1067
00:45:44,960 --> 00:45:49,520
But it already beat out
Gemini 1.5 Pro and Claude 3.5.

1068
00:45:49,520 --> 00:45:52,640
Now, the big difference is
we are a small research lab,

1069
00:45:52,640 --> 00:45:56,080
and we're beating out
Google's billions of dollars

1070
00:45:56,080 --> 00:45:58,160
of investment into
Gemini, as well

1071
00:45:58,160 --> 00:46:00,280
as Anthropic's billions
of dollars of investment

1072
00:46:00,280 --> 00:46:02,080
and already matching GPT.

1073
00:46:02,080 --> 00:46:05,000
And so we were quite excited
by this entire process.

1074
00:46:05,000 --> 00:46:08,240
But we also developed a
7-billion model that comes

1075
00:46:08,240 --> 00:46:10,140
in right after those big models.

1076
00:46:10,140 --> 00:46:12,680
And that 7- billion
model is really exciting

1077
00:46:12,680 --> 00:46:15,240
because you can put
it on a single GPU.

1078
00:46:15,240 --> 00:46:17,360
So you can now have
this model capable

1079
00:46:17,360 --> 00:46:19,680
of doing a wide
variety of vision tasks

1080
00:46:19,680 --> 00:46:23,160
that works on a single GPU,
meaning a lot of people

1081
00:46:23,160 --> 00:46:25,920
can now use it and fine tune
it for all kinds of things.

1082
00:46:25,920 --> 00:46:28,200
We released this
model in September 25

1083
00:46:28,200 --> 00:46:30,720
and the community was
very excited by it.

1084
00:46:30,720 --> 00:46:34,440
This is the first time a very
performant, multimodal vision

1085
00:46:34,440 --> 00:46:36,610
language model was released,
and a ton of people

1086
00:46:36,610 --> 00:46:39,530
started talking and writing
articles about all the ways they

1087
00:46:39,530 --> 00:46:40,358
want to use it.

1088
00:46:40,358 --> 00:46:42,650
One of the use cases that
kept popping up over and over

1089
00:46:42,650 --> 00:46:46,310
again was this idea
of using Molmo finally

1090
00:46:46,310 --> 00:46:48,010
for robotics applications.

1091
00:46:48,010 --> 00:46:50,250
And I won't talk
about robotics today

1092
00:46:50,250 --> 00:46:53,210
because you're going to learn
about it in the next class.

1093
00:46:53,210 --> 00:46:55,330
But I do want to give you
some examples of things

1094
00:46:55,330 --> 00:46:59,050
that people were excited
about with robotics.

1095
00:46:59,050 --> 00:47:02,790
But a ton of people,
even folks at NVIDIA,

1096
00:47:02,790 --> 00:47:05,512
started chatting about
how this should never

1097
00:47:05,512 --> 00:47:06,970
bet against open
source, regardless

1098
00:47:06,970 --> 00:47:09,270
of how much model development
you do in private.

1099
00:47:09,270 --> 00:47:11,750
Eventually, the open source
community will catch up,

1100
00:47:11,750 --> 00:47:14,610
and we were catching
up at that point.

1101
00:47:14,610 --> 00:47:17,490
And so seeing our model
out, Meta quickly released

1102
00:47:17,490 --> 00:47:22,170
in response, their LLaVA 3.2
model and a lot of people

1103
00:47:22,170 --> 00:47:26,110
did evaluations comparing Molmo
versus Meta's LLaVA model.

1104
00:47:26,110 --> 00:47:29,050
And again, I'm very happy that
we came up on top of LLaVA

1105
00:47:29,050 --> 00:47:30,130
as well.

1106
00:47:30,130 --> 00:47:32,730
So let me show you why
Molmo does so well.

1107
00:47:32,730 --> 00:47:36,180
What was the trick to getting
these models to work very well?

1108
00:47:36,180 --> 00:47:38,860
The trick was to
ground its decision

1109
00:47:38,860 --> 00:47:41,260
making in the pixels itself.

1110
00:47:41,260 --> 00:47:44,260
So usually, when you give a
model a question like count

1111
00:47:44,260 --> 00:47:47,000
how many boats there are,
it'll give you some number

1112
00:47:47,000 --> 00:47:48,740
and oftentimes, it hallucinates.

1113
00:47:48,740 --> 00:47:50,380
But what sort of
sets our model apart

1114
00:47:50,380 --> 00:47:52,700
is that it actually
points to all the things

1115
00:47:52,700 --> 00:47:53,740
that it's counting.

1116
00:47:53,740 --> 00:47:56,180
So it generates points
to all the boats

1117
00:47:56,180 --> 00:47:58,620
and then it outputs
a final number.

1118
00:47:58,620 --> 00:48:02,300
So it's decision making is
grounded in the pixels itself.

1119
00:48:02,300 --> 00:48:05,260
And this allowed us to
essentially train a model that

1120
00:48:05,260 --> 00:48:08,260
unlike LLaVA for Meta that was
trained on about six billion

1121
00:48:08,260 --> 00:48:12,380
image taxpayers, our model was
trained on only 700,000 image

1122
00:48:12,380 --> 00:48:13,580
text pairs.

1123
00:48:13,580 --> 00:48:17,580
The big difference was that we
hand-curated the 700,000 image

1124
00:48:17,580 --> 00:48:20,940
text pairs, and that was the
biggest difference between what

1125
00:48:20,940 --> 00:48:23,780
we were able to do versus
what these models that these

1126
00:48:23,780 --> 00:48:26,460
companies were
building were doing.

1127
00:48:26,460 --> 00:48:30,700
So a lot of folks are trying to
currently download these image

1128
00:48:30,700 --> 00:48:32,228
text pairs from the internet.

1129
00:48:32,228 --> 00:48:34,270
That's been the foundation
of how a lot of people

1130
00:48:34,270 --> 00:48:36,010
train these vision
language models.

1131
00:48:36,010 --> 00:48:39,070
You collect a lot of
internet data of images

1132
00:48:39,070 --> 00:48:40,450
with their associated text.

1133
00:48:40,450 --> 00:48:43,110
But the problem with internet
data is that it's incidental.

1134
00:48:43,110 --> 00:48:45,470
The text that's often
associated with an image

1135
00:48:45,470 --> 00:48:47,670
describes something
subjective or something

1136
00:48:47,670 --> 00:48:50,310
that the uploader
felt about the image.

1137
00:48:50,310 --> 00:48:52,550
It rarely actually
talks about the contents

1138
00:48:52,550 --> 00:48:54,070
of the image itself.

1139
00:48:54,070 --> 00:48:57,230
And meanwhile, this is
what our data looks like.

1140
00:48:57,230 --> 00:49:01,110
So for a single image, we
have a dense description

1141
00:49:01,110 --> 00:49:03,650
of the actual contents
of that image.

1142
00:49:03,650 --> 00:49:05,150
And we have things
that people never

1143
00:49:05,150 --> 00:49:06,690
talk about on the internet.

1144
00:49:06,690 --> 00:49:09,550
There's a ton of tasks and
knowledge about the visual world

1145
00:49:09,550 --> 00:49:11,370
that we just never speak about.

1146
00:49:11,370 --> 00:49:12,870
I will never tell
you that something

1147
00:49:12,870 --> 00:49:14,410
is to the left of
something else,

1148
00:49:14,410 --> 00:49:16,930
just because it's unnatural
for us to do that.

1149
00:49:16,930 --> 00:49:18,470
It's just so obvious
that something

1150
00:49:18,470 --> 00:49:19,730
is to the left of something.

1151
00:49:19,730 --> 00:49:21,850
Why would you ever
communicate that information?

1152
00:49:21,850 --> 00:49:23,225
So that's the kind
of information

1153
00:49:23,225 --> 00:49:24,950
we started eliciting
from people.

1154
00:49:24,950 --> 00:49:27,117
We started talking about--
we started getting people

1155
00:49:27,117 --> 00:49:29,470
to talk about how things
have particular size

1156
00:49:29,470 --> 00:49:31,710
like large or shaped
like rectangular.

1157
00:49:31,710 --> 00:49:34,550
We talked about material
like polished and rich

1158
00:49:34,550 --> 00:49:36,630
and its positioning
across the image,

1159
00:49:36,630 --> 00:49:40,350
like it spans the horizontal
plane of the image.

1160
00:49:40,350 --> 00:49:42,350
So all of this
information really

1161
00:49:42,350 --> 00:49:44,650
is what makes these
models more performant.

1162
00:49:44,650 --> 00:49:46,750
Here's another example
from the data set.

1163
00:49:46,750 --> 00:49:50,110
Here's a very simple image of a
phone screen or a tablet screen.

1164
00:49:50,110 --> 00:49:53,750
And we have information here
that again is completely missing

1165
00:49:53,750 --> 00:49:56,990
from the internet, things that
people would find helpful.

1166
00:49:56,990 --> 00:49:58,730
Things like this
is a tablet device.

1167
00:49:58,730 --> 00:50:02,270
The time is this, the amount
of power left in your device

1168
00:50:02,270 --> 00:50:03,000
is this.

1169
00:50:03,000 --> 00:50:04,750
This is the kind of
information that would

1170
00:50:04,750 --> 00:50:06,398
help people use these models.

1171
00:50:06,398 --> 00:50:08,190
But this is, again,
the kind of information

1172
00:50:08,190 --> 00:50:10,630
we never talk about
on the internet.

1173
00:50:10,630 --> 00:50:12,370
And so to get this
kind of information,

1174
00:50:12,370 --> 00:50:14,370
we designed a lot of
different questions.

1175
00:50:14,370 --> 00:50:17,030
We spent two years doing
different kinds of elicitation

1176
00:50:17,030 --> 00:50:19,710
studies to figure out
what are the right things

1177
00:50:19,710 --> 00:50:22,210
or pieces of information that
are missing from the internet,

1178
00:50:22,210 --> 00:50:25,710
and how do we elicit them
as effectively as possible.

1179
00:50:25,710 --> 00:50:27,480
One thing that
was very important

1180
00:50:27,480 --> 00:50:31,860
is we had all of our annotators
not type descriptions,

1181
00:50:31,860 --> 00:50:34,720
but talk about descriptions,
talking automatically

1182
00:50:34,720 --> 00:50:39,760
breaks a lot of stereotypes
around [? Gricean ?] maxims.

1183
00:50:39,760 --> 00:50:41,808
And so by getting
people to talk,

1184
00:50:41,808 --> 00:50:43,600
we got them to speak
about things that they

1185
00:50:43,600 --> 00:50:46,250
would never usually type.

1186
00:50:46,250 --> 00:50:48,500
The model itself didn't look
any different from LLaVA.

1187
00:50:48,500 --> 00:50:51,900
We had the same setup of a
CLIP and coding coming in.

1188
00:50:51,900 --> 00:50:54,320
You had a connector that
was just a linear layer,

1189
00:50:54,320 --> 00:50:56,080
and then you had a
large language model

1190
00:50:56,080 --> 00:50:59,320
that would take in all of these
tokens and then output any thing

1191
00:50:59,320 --> 00:51:00,500
that you care about.

1192
00:51:00,500 --> 00:51:03,660
So the model itself looked very
similar to existing models.

1193
00:51:03,660 --> 00:51:05,480
The biggest difference
was in the data

1194
00:51:05,480 --> 00:51:08,400
and the quality and
density of the data itself.

1195
00:51:08,400 --> 00:51:11,220
And because of
grounding capability,

1196
00:51:11,220 --> 00:51:12,720
where the model
grounds its decision

1197
00:51:12,720 --> 00:51:14,920
making in the image
itself, you could

1198
00:51:14,920 --> 00:51:16,560
get Malmo to do
things that you can't

1199
00:51:16,560 --> 00:51:19,080
do-- you can't use any of
the other models to do;

1200
00:51:19,080 --> 00:51:20,500
things like point to the menu.

1201
00:51:20,500 --> 00:51:22,960
It actually tells you
where that menu item is,

1202
00:51:22,960 --> 00:51:26,370
or you can tell it to point
to where I can set my search

1203
00:51:26,370 --> 00:51:27,970
options and it'll
show you, OK, this

1204
00:51:27,970 --> 00:51:31,490
is where you might want to set
those options or point to where

1205
00:51:31,490 --> 00:51:33,530
the mid-size data sets
are, and it'll tell you

1206
00:51:33,530 --> 00:51:35,670
what option you need to move.

1207
00:51:35,670 --> 00:51:36,170
And

1208
00:51:36,170 --> 00:51:38,550
I already showed you that
you can point to count,

1209
00:51:38,550 --> 00:51:41,870
but you can also point to do
really fine grained things,

1210
00:51:41,870 --> 00:51:45,410
like being able to ask, what is
the route number on this bus?

1211
00:51:45,410 --> 00:51:47,550
Molmo doesn't just simply
give you an answer,

1212
00:51:47,550 --> 00:51:49,810
it actually points to
where in the image.

1213
00:51:49,810 --> 00:51:51,690
In this case, there is
this area right here

1214
00:51:51,690 --> 00:51:54,170
that contains the bus
number and then returns

1215
00:51:54,170 --> 00:51:56,050
the bus number to you.

1216
00:51:56,050 --> 00:51:57,970
You can ask it to reason
about how many cars

1217
00:51:57,970 --> 00:52:00,630
on the left versus how
many cars are on the right.

1218
00:52:00,630 --> 00:52:04,070
You can ask it to reason over
depth images or overhead images,

1219
00:52:04,070 --> 00:52:08,445
or even really crowded
scenes and sports areas.

1220
00:52:08,445 --> 00:52:10,070
What's also really
exciting, and again,

1221
00:52:10,070 --> 00:52:11,695
we'll talk about this
in a few minutes,

1222
00:52:11,695 --> 00:52:13,650
is this idea of
chaining that keeps

1223
00:52:13,650 --> 00:52:16,670
coming up all across
multimodal models today.

1224
00:52:16,670 --> 00:52:20,250
The idea of chaining Molmo to
other models, what you can do

1225
00:52:20,250 --> 00:52:22,930
is chain the output
of Molmo to become

1226
00:52:22,930 --> 00:52:25,500
the input of another
model like SAM 2,

1227
00:52:25,500 --> 00:52:28,280
and so you can tell Molmo
to point to the cricket bat.

1228
00:52:28,280 --> 00:52:31,580
And now you take that point, you
feed it to a model like SAM 2,

1229
00:52:31,580 --> 00:52:33,700
which does segmentation,
and now you

1230
00:52:33,700 --> 00:52:36,700
can do segmentation of that
cricket bat across time.

1231
00:52:36,700 --> 00:52:39,620
And so you can start enabling
all kinds of new applications.

1232
00:52:39,620 --> 00:52:43,318
Here's one that we played around
with in the office, which again,

1233
00:52:43,318 --> 00:52:44,860
you're going to
hopefully learn about

1234
00:52:44,860 --> 00:52:48,700
in the next lecture with
when you hear about robotics.

1235
00:52:48,700 --> 00:52:52,160
But we asked Molmo to point
to where the water bottle is.

1236
00:52:52,160 --> 00:52:55,380
And then we moved the robot
using simple motion planners

1237
00:52:55,380 --> 00:52:56,700
to that water bottle.

1238
00:52:56,700 --> 00:52:59,420
Next, we ask it to go
move that water bottle

1239
00:52:59,420 --> 00:53:01,140
to where the dirty dishes are.

1240
00:53:01,140 --> 00:53:02,460
It points to the dish--

1241
00:53:02,460 --> 00:53:05,460
to the-- sorry-- sink, and
then moves the robot there.

1242
00:53:05,460 --> 00:53:07,300
And then we tell it
to go point to where

1243
00:53:07,300 --> 00:53:09,540
the free space is
in the sink and put

1244
00:53:09,540 --> 00:53:11,460
that bottle in that location.

1245
00:53:11,460 --> 00:53:15,100
So again, you can combine all of
these capabilities together now

1246
00:53:15,100 --> 00:53:19,460
and chain them to even automate
a lot of robotics applications.

1247
00:53:19,460 --> 00:53:21,660
This has been a lot of
focus in my group now

1248
00:53:21,660 --> 00:53:24,070
is adapting a lot of these
visual language models

1249
00:53:24,070 --> 00:53:26,150
and enabling a lot
of generalization

1250
00:53:26,150 --> 00:53:28,750
in the actual physical domain.

1251
00:53:28,750 --> 00:53:31,590
So the question is
around, would these models

1252
00:53:31,590 --> 00:53:34,950
be able to point
if you were always

1253
00:53:34,950 --> 00:53:38,830
changing the resolution of the
image to be a fixed resolution?

1254
00:53:38,830 --> 00:53:41,030
Well, it turns out
that you can actually

1255
00:53:41,030 --> 00:53:43,750
adapt these models to be
any resolution nowadays.

1256
00:53:43,750 --> 00:53:47,910
There are these mechanisms
like [INAUDIBLE] that

1257
00:53:47,910 --> 00:53:52,350
is introduced a way of allowing
for any variable size image

1258
00:53:52,350 --> 00:53:57,230
input and you can adapt them to
point in that new space instead.

1259
00:53:57,230 --> 00:53:59,430
So your models position
embeddings basically

1260
00:53:59,430 --> 00:54:03,710
change depending on how
big your image size is

1261
00:54:03,710 --> 00:54:07,230
and your models-- you typically
tend to generalize well.

1262
00:54:07,230 --> 00:54:10,510
So that was the conversation
around adding vision

1263
00:54:10,510 --> 00:54:12,270
and multimodal models together.

1264
00:54:12,270 --> 00:54:14,467
In the last 20 minutes
that we have left,

1265
00:54:14,467 --> 00:54:16,550
I want to talk about
generalizing these foundation

1266
00:54:16,550 --> 00:54:19,690
models to not just deal with
image classification and text,

1267
00:54:19,690 --> 00:54:22,120
but be able to generalize
to any output space

1268
00:54:22,120 --> 00:54:23,720
you might care about.

1269
00:54:23,720 --> 00:54:25,320
And one of those
models that's become

1270
00:54:25,320 --> 00:54:29,440
really popular in this space
is this Segment Anything Model.

1271
00:54:29,440 --> 00:54:31,480
The Segment Anything
Model or SAM,

1272
00:54:31,480 --> 00:54:34,320
for short, what
it tries to do is

1273
00:54:34,320 --> 00:54:37,000
it tries to build a
segmentation model that's

1274
00:54:37,000 --> 00:54:40,360
a foundation model for all
kinds of segmentation tasks.

1275
00:54:40,360 --> 00:54:41,920
So really what
they're trying to do

1276
00:54:41,920 --> 00:54:45,720
is allow anybody to point to
things that they care about

1277
00:54:45,720 --> 00:54:48,080
in the image, and
then hopefully have

1278
00:54:48,080 --> 00:54:53,360
that thing be something that
the model can output a mask for.

1279
00:54:53,360 --> 00:54:56,000
So for example, you
want a model that

1280
00:54:56,000 --> 00:54:58,560
generalizes beyond just a
fixed number of categories

1281
00:54:58,560 --> 00:55:00,760
to any category you
might care about.

1282
00:55:00,760 --> 00:55:03,080
And you would ideally
want these outputs

1283
00:55:03,080 --> 00:55:07,220
to be masks for any category
that is of interest to the user.

1284
00:55:07,220 --> 00:55:09,080
So those are the
two goals that we

1285
00:55:09,080 --> 00:55:11,720
want to generalize to any
category, a huge number

1286
00:55:11,720 --> 00:55:12,700
of categories.

1287
00:55:12,700 --> 00:55:15,800
And we ideally want to be able
to very specifically output

1288
00:55:15,800 --> 00:55:18,300
something that the user
really cares about.

1289
00:55:18,300 --> 00:55:19,510
So they're both challenges.

1290
00:55:19,510 --> 00:55:20,770
They're both challenges
in figuring out

1291
00:55:20,770 --> 00:55:22,770
how do you collect a
large amount of data that

1292
00:55:22,770 --> 00:55:24,870
spans a wide variety
of categories,

1293
00:55:24,870 --> 00:55:27,330
as well as how do you
design an architecture that

1294
00:55:27,330 --> 00:55:31,050
really pinpoints what the
user really cares about.

1295
00:55:31,050 --> 00:55:33,930
Now, let's start with the
second question first.

1296
00:55:33,930 --> 00:55:38,910
It's really ambiguous when
we want a mask for something.

1297
00:55:38,910 --> 00:55:42,850
So imagine a scenario where
you have two cats in an image

1298
00:55:42,850 --> 00:55:44,570
and a user comes
in and says, hey, I

1299
00:55:44,570 --> 00:55:47,450
want a segmentation for the
cat, but it's really not clear

1300
00:55:47,450 --> 00:55:50,090
which cat they want
a segmentation for.

1301
00:55:50,090 --> 00:55:52,750
Ideally, again, if you had
more mask pointing capability,

1302
00:55:52,750 --> 00:55:54,990
you could actually point to
which cat you care about.

1303
00:55:54,990 --> 00:55:56,490
And then depending
on the point, you

1304
00:55:56,490 --> 00:55:59,690
could create the
masks that matter.

1305
00:55:59,690 --> 00:56:01,755
Now, of course, these
are not very good masks.

1306
00:56:01,755 --> 00:56:03,130
And ideally, you
want these masks

1307
00:56:03,130 --> 00:56:06,290
to be very, very
good at quality that

1308
00:56:06,290 --> 00:56:09,170
can support a wide variety
of downstream applications

1309
00:56:09,170 --> 00:56:11,770
like image editing or
any kinds of other things

1310
00:56:11,770 --> 00:56:13,930
you might think of.

1311
00:56:13,930 --> 00:56:16,650
So to build this architecture
that allows any user

1312
00:56:16,650 --> 00:56:19,280
to be able to specify
exactly what they care about,

1313
00:56:19,280 --> 00:56:22,700
we need it to go beyond
just simply typing

1314
00:56:22,700 --> 00:56:24,620
in text what you care about.

1315
00:56:24,620 --> 00:56:28,538
So what the SAM architecture
has is two components or three,

1316
00:56:28,538 --> 00:56:29,080
specifically.

1317
00:56:29,080 --> 00:56:31,760
It's got the image
encoder, which is again,

1318
00:56:31,760 --> 00:56:33,780
it could be a clip encoder.

1319
00:56:33,780 --> 00:56:36,720
And it's got a prompt encoder
which is something special.

1320
00:56:36,720 --> 00:56:40,580
This prompt encoder really
tries to encode text or points

1321
00:56:40,580 --> 00:56:43,180
or bounding boxes or any
way that a user might

1322
00:56:43,180 --> 00:56:45,300
want to specify what
they care about.

1323
00:56:45,300 --> 00:56:47,220
And then given
these two things, it

1324
00:56:47,220 --> 00:56:49,460
passes it through this
really lightweight decoder

1325
00:56:49,460 --> 00:56:50,640
that outputs a mask.

1326
00:56:50,640 --> 00:56:53,900
And the decoder looks very
similar to the segmentation

1327
00:56:53,900 --> 00:56:56,900
decoders that you've
already seen in this course.

1328
00:56:56,900 --> 00:56:59,540
So overall, this is what
the model looks like.

1329
00:56:59,540 --> 00:57:03,580
Given an image, we encode that
image using an image encoder.

1330
00:57:03,580 --> 00:57:06,100
And then you have a bunch
of different prompts

1331
00:57:06,100 --> 00:57:09,340
that are going
through interacting

1332
00:57:09,340 --> 00:57:12,040
with these image encoding
through a decoder

1333
00:57:12,040 --> 00:57:13,980
and you output a mask.

1334
00:57:13,980 --> 00:57:16,790
So this is the overall
architecture design.

1335
00:57:16,790 --> 00:57:20,330
Now, there's one big thing that
is a problem with segmentation.

1336
00:57:20,330 --> 00:57:23,150
So let's say a user does point
at this particular location

1337
00:57:23,150 --> 00:57:25,270
and says, hey, I want
a segmentation mask

1338
00:57:25,270 --> 00:57:26,477
for this location.

1339
00:57:26,477 --> 00:57:28,310
Now, the problem with
that segmentation mask

1340
00:57:28,310 --> 00:57:30,530
is that it's still
ambiguous even with a point,

1341
00:57:30,530 --> 00:57:32,830
it's still not sufficient
because that point

1342
00:57:32,830 --> 00:57:36,710
might be referring to
the entire scissor.

1343
00:57:36,710 --> 00:57:39,790
It might only be looking
and referring to the parts

1344
00:57:39,790 --> 00:57:42,870
that you can hold, or it can be
referring to one of the parts

1345
00:57:42,870 --> 00:57:43,950
that you can hold.

1346
00:57:43,950 --> 00:57:47,550
So this ambiguity is really
difficult to resolve for.

1347
00:57:47,550 --> 00:57:49,270
And you don't want
to penalize the model

1348
00:57:49,270 --> 00:57:50,830
for picking the wrong one.

1349
00:57:50,830 --> 00:57:52,590
So what the SAM
architecture does

1350
00:57:52,590 --> 00:57:55,050
is instead of outputting
one segmentation mask,

1351
00:57:55,050 --> 00:57:57,150
it actually outputs
three segmentation masks

1352
00:57:57,150 --> 00:57:59,070
at different levels
of granularity.

1353
00:57:59,070 --> 00:58:01,350
And then it picks the
one that is the closest

1354
00:58:01,350 --> 00:58:02,990
matching to ground
truth, and then

1355
00:58:02,990 --> 00:58:06,110
uses that to calculate the loss
and therefore not penalizing

1356
00:58:06,110 --> 00:58:07,190
the other ones.

1357
00:58:07,190 --> 00:58:09,870
So the hope is that overall,
over time, this model

1358
00:58:09,870 --> 00:58:12,130
will learn to output all
different kinds of masks

1359
00:58:12,130 --> 00:58:14,120
and then the user
gets to choose which

1360
00:58:14,120 --> 00:58:17,280
one is the most appropriate
for their use cases.

1361
00:58:17,280 --> 00:58:19,860
And if you put all
of this together,

1362
00:58:19,860 --> 00:58:22,860
the only thing you
need now is data.

1363
00:58:22,860 --> 00:58:25,720
You need a lot of data across
many different categories

1364
00:58:25,720 --> 00:58:28,000
to really make this
model possible.

1365
00:58:28,000 --> 00:58:30,880
Now, the problem with data is
that until this model came out

1366
00:58:30,880 --> 00:58:34,960
in 2023, this was about a year
and a half, maybe two years ago.

1367
00:58:34,960 --> 00:58:38,120
When this model came out, most
of the segmentation data sets

1368
00:58:38,120 --> 00:58:39,840
were extremely small.

1369
00:58:39,840 --> 00:58:43,365
And what the authors of this
paper did is that they grew

1370
00:58:43,365 --> 00:58:45,740
the amount of segmentation
data sets that were out there,

1371
00:58:45,740 --> 00:58:49,000
the amount of images by about 6x
and the number of segmentation

1372
00:58:49,000 --> 00:58:51,040
masks by about 400x.

1373
00:58:51,040 --> 00:58:55,080
So they significantly grew
and collected a lot of masks

1374
00:58:55,080 --> 00:58:57,560
to make this model as
performant as possible.

1375
00:58:57,560 --> 00:58:59,800
So again, the message is
very similar to the message

1376
00:58:59,800 --> 00:59:02,280
we had with Flamingo, the
message we had with Molmo.

1377
00:59:02,280 --> 00:59:04,460
And the message is you
really, really good,

1378
00:59:04,460 --> 00:59:07,240
high-quality data to really
get these models to be

1379
00:59:07,240 --> 00:59:08,860
as performant as possible.

1380
00:59:08,860 --> 00:59:12,050
And for a lot of vision tasks,
the data is completely missing

1381
00:59:12,050 --> 00:59:15,210
from the internet, and you need
to go out and find and collect

1382
00:59:15,210 --> 00:59:18,330
that data to get these
models to work very well.

1383
00:59:18,330 --> 00:59:21,250
And so to make this
data happen, they

1384
00:59:21,250 --> 00:59:23,450
created this in
the loop process,

1385
00:59:23,450 --> 00:59:27,010
where they initially had some
amount of data annotated.

1386
00:59:27,010 --> 00:59:28,490
From that annotation,
they created

1387
00:59:28,490 --> 00:59:30,430
a training data set,
they trained a model,

1388
00:59:30,430 --> 00:59:32,730
and they used that model
to annotate more data.

1389
00:59:32,730 --> 00:59:34,690
And then they
iteratively refined

1390
00:59:34,690 --> 00:59:37,650
that model generated
segments using users

1391
00:59:37,650 --> 00:59:39,450
and continued this process.

1392
00:59:39,450 --> 00:59:41,730
So they had this
human in the loop--

1393
00:59:41,730 --> 00:59:45,210
model in the loop process,
proposing segments and then

1394
00:59:45,210 --> 00:59:47,890
fixing the segments
using human annotators.

1395
00:59:47,890 --> 00:59:50,750
This is what an example image
looks like from their data set.

1396
00:59:50,750 --> 00:59:52,590
You have quite a
lot of categories.

1397
00:59:52,590 --> 00:59:57,130
Each individual vegetable here
is annotated with its own mask.

1398
00:59:57,130 --> 00:59:59,570
So they're quite
expensive to collect.

1399
00:59:59,570 --> 01:00:04,770
And they did this across a lot
of images, millions of images.

1400
01:00:04,770 --> 01:00:07,850
Here's another example where
again all the single umbrellas

1401
01:00:07,850 --> 01:00:09,290
are annotated.

1402
01:00:09,290 --> 01:00:11,560
Again, here's another
one with underwater.

1403
01:00:11,560 --> 01:00:14,080
And of course,
paintings as well.

1404
01:00:14,080 --> 01:00:16,380
They have segmentations
of paintings.

1405
01:00:16,380 --> 01:00:18,140
And so all of this
together is really

1406
01:00:18,140 --> 01:00:20,820
what was foundational to
making this foundation

1407
01:00:20,820 --> 01:00:24,028
model for segmentation, OK.

1408
01:00:24,028 --> 01:00:25,320
So that's for Segment Anything.

1409
01:00:25,320 --> 01:00:27,153
And I want to use the
last couple of minutes

1410
01:00:27,153 --> 01:00:29,380
that I have left
today to really focus

1411
01:00:29,380 --> 01:00:34,100
on chaining, which is the last
part of multimodal language

1412
01:00:34,100 --> 01:00:35,100
models.

1413
01:00:35,100 --> 01:00:38,880
The idea behind chaining is
something you've already seen.

1414
01:00:38,880 --> 01:00:41,370
I've given you hints already
throughout this lecture.

1415
01:00:41,370 --> 01:00:43,620
And the idea is to be able
to combine different models

1416
01:00:43,620 --> 01:00:47,420
together to enable things that
a single model can't do alone.

1417
01:00:47,420 --> 01:00:50,660
Here's a fun little exercise
we can do as a class.

1418
01:00:50,660 --> 01:00:53,740
So I'm giving you four
images and I'm also

1419
01:00:53,740 --> 01:00:55,860
giving you four categories.

1420
01:00:55,860 --> 01:00:59,020
And these are potentially
categories that some of you

1421
01:00:59,020 --> 01:01:00,540
have never seen before.

1422
01:01:00,540 --> 01:01:03,460
And there are also categories
that CLIP hasn't seen.

1423
01:01:03,460 --> 01:01:05,540
And so CLIP actually
fails at these categories

1424
01:01:05,540 --> 01:01:07,123
because it doesn't
have any idea which

1425
01:01:07,123 --> 01:01:09,110
ones are associated with what.

1426
01:01:09,110 --> 01:01:12,190
Does anyone here know
which one is what?

1427
01:01:12,190 --> 01:01:13,930
Marimba, yeah, the
second one is marimba.

1428
01:01:13,930 --> 01:01:14,472
That's right.

1429
01:01:14,472 --> 01:01:16,250
Yeah there's one
that's a little easy.

1430
01:01:16,250 --> 01:01:17,750
The viaduct.

1431
01:01:17,750 --> 01:01:21,630
Yeah, I think a lot of you
know which one that is.

1432
01:01:21,630 --> 01:01:25,390
But, yeah, which one's the
dog and which one's the bird?

1433
01:01:25,390 --> 01:01:28,430
What if I gave
you these instead.

1434
01:01:28,430 --> 01:01:31,322
So now I'm giving you
descriptions of these things,

1435
01:01:31,322 --> 01:01:33,030
and it suddenly becomes
very easy for you

1436
01:01:33,030 --> 01:01:36,276
to associate each one
with the right category.

1437
01:01:36,276 --> 01:01:38,830
And that's the basic
idea behind chaining

1438
01:01:38,830 --> 01:01:41,290
that even if CLIP has
never seen these images,

1439
01:01:41,290 --> 01:01:44,150
chances are these
concepts have been talked

1440
01:01:44,150 --> 01:01:46,030
about on the internet
to some degree,

1441
01:01:46,030 --> 01:01:49,870
and it's likely that GPT
might be able to describe it.

1442
01:01:49,870 --> 01:01:52,350
And if GPT can create
those descriptions,

1443
01:01:52,350 --> 01:01:54,750
now those descriptions
become really good ways

1444
01:01:54,750 --> 01:01:58,150
of classifying exactly
which category is which one.

1445
01:01:58,150 --> 01:01:59,730
And that's the idea
behind chaining,

1446
01:01:59,730 --> 01:02:01,690
is that you take the
strengths of one model,

1447
01:02:01,690 --> 01:02:03,890
and you combine it with the
capabilities of another,

1448
01:02:03,890 --> 01:02:06,230
and suddenly you get all
kinds of new capabilities

1449
01:02:06,230 --> 01:02:07,887
that you didn't have before.

1450
01:02:07,887 --> 01:02:09,720
And so you can take a
ton of categories that

1451
01:02:09,720 --> 01:02:12,240
have no training data in CLIP.

1452
01:02:12,240 --> 01:02:14,080
But if you can
describe all of them,

1453
01:02:14,080 --> 01:02:16,500
because CLIP has seen a lot
of descriptions for things,

1454
01:02:16,500 --> 01:02:19,080
it can now start classifying
all of them very well.

1455
01:02:19,080 --> 01:02:21,600
And you can start getting CLIPS
to generate classifications

1456
01:02:21,600 --> 01:02:24,360
for individual flowers
or individual cars

1457
01:02:24,360 --> 01:02:28,280
or individual spaces or even
different kinds of pets.

1458
01:02:28,280 --> 01:02:30,080
And you start seeing
these improvements

1459
01:02:30,080 --> 01:02:32,000
on a bunch of different
data sets that

1460
01:02:32,000 --> 01:02:35,258
are about more fine-grained,
specialized categories.

1461
01:02:35,258 --> 01:02:36,800
And the only way
it's able to do that

1462
01:02:36,800 --> 01:02:39,680
is because GPT has
ingested some ability

1463
01:02:39,680 --> 01:02:42,600
to describe those things.

1464
01:02:42,600 --> 01:02:45,600
And this idea of being able to
generalize the new capabilities

1465
01:02:45,600 --> 01:02:49,180
is something that was
extremely popular last year,

1466
01:02:49,180 --> 01:02:52,520
and it still remains
very popular this year.

1467
01:02:52,520 --> 01:02:56,280
And it's through
this idea of chaining

1468
01:02:56,280 --> 01:02:58,960
for any question at all.

1469
01:02:58,960 --> 01:03:01,640
So, for example, if I asked
you how many-- are there

1470
01:03:01,640 --> 01:03:04,120
three people in the boat?

1471
01:03:04,120 --> 01:03:07,480
The way you might want
to do this is by, again

1472
01:03:07,480 --> 01:03:09,940
asking a multimodal language
model to answer this question,

1473
01:03:09,940 --> 01:03:13,360
or what you could do is use all
of the hundreds of specialized

1474
01:03:13,360 --> 01:03:15,440
vision models that
we've been developing

1475
01:03:15,440 --> 01:03:17,062
over the last few decades.

1476
01:03:17,062 --> 01:03:19,520
So there are object detection
models that you learned about

1477
01:03:19,520 --> 01:03:20,300
in class.

1478
01:03:20,300 --> 01:03:21,800
If you use an object
detector, you'd

1479
01:03:21,800 --> 01:03:24,133
be able to get a detection
for each of the three people.

1480
01:03:24,133 --> 01:03:25,800
And then you could
just say, oh, they're

1481
01:03:25,800 --> 01:03:27,640
3 people because there
are three detections.

1482
01:03:27,640 --> 01:03:32,800
So that's a general idea, is you
can train other models outputs

1483
01:03:32,800 --> 01:03:35,720
together so that you
can do new capabilities.

1484
01:03:35,720 --> 01:03:37,320
Here's another example.

1485
01:03:37,320 --> 01:03:39,200
If I asked you how
many total people

1486
01:03:39,200 --> 01:03:42,518
there are across these
two boats, is it six?

1487
01:03:42,518 --> 01:03:44,060
And again, you can
do the same thing.

1488
01:03:44,060 --> 01:03:47,120
You can write a program that
does object detection on 1 one

1489
01:03:47,120 --> 01:03:49,000
and then object
detection on image 2,

1490
01:03:49,000 --> 01:03:52,260
and then adds up all of
those components together.

1491
01:03:52,260 --> 01:03:57,933
So this is the basic idea behind
what we now call chaining.

1492
01:03:57,933 --> 01:03:59,600
And this was popularized
by a paper that

1493
01:03:59,600 --> 01:04:03,370
won the Best Paper award
last year called VisProg

1494
01:04:03,370 --> 01:04:06,930
and in this VisProg paper,
the visual programming paper,

1495
01:04:06,930 --> 01:04:10,810
the idea was that you take any
image or any sort of question

1496
01:04:10,810 --> 01:04:13,090
and you generate a program.

1497
01:04:13,090 --> 01:04:16,530
You generate a program that says
answer something about image 1,

1498
01:04:16,530 --> 01:04:19,290
answer something about
image 2, and then combine

1499
01:04:19,290 --> 01:04:22,890
those answers together to
give you the final answer.

1500
01:04:22,890 --> 01:04:25,230
So you write a
function in Python,

1501
01:04:25,230 --> 01:04:27,230
and then in that
Python function,

1502
01:04:27,230 --> 01:04:30,170
you have individual
calls to other models

1503
01:04:30,170 --> 01:04:32,290
that we've already
seen in training.

1504
01:04:32,290 --> 01:04:35,890
So for example, when asking
this particular question,

1505
01:04:35,890 --> 01:04:37,742
deciding if this
statement is true or not,

1506
01:04:37,742 --> 01:04:39,450
the left and right
image contains a total

1507
01:04:39,450 --> 01:04:41,610
of six people and two boats.

1508
01:04:41,610 --> 01:04:44,210
You can ask GPT
to actually create

1509
01:04:44,210 --> 01:04:47,030
a program that tries to
answer this question,

1510
01:04:47,030 --> 01:04:51,610
and then you can take the
answer from its program, OK.

1511
01:04:51,610 --> 01:04:55,530
And you can also get GPT to
do in-context examples, where

1512
01:04:55,530 --> 01:04:57,450
you give it examples of
programs that you can

1513
01:04:57,450 --> 01:04:59,670
generate using other functions.

1514
01:04:59,670 --> 01:05:02,900
And we can see that it
generalizes to new questions

1515
01:05:02,900 --> 01:05:05,460
and starts using all
of the functionality

1516
01:05:05,460 --> 01:05:06,835
that it has available.

1517
01:05:06,835 --> 01:05:08,460
Of course, the one
thing you need to do

1518
01:05:08,460 --> 01:05:10,680
is give it the
functions themselves.

1519
01:05:10,680 --> 01:05:12,260
So you need to tell
it that, hey, you

1520
01:05:12,260 --> 01:05:15,280
have these capabilities from
other models that you can use.

1521
01:05:15,280 --> 01:05:17,600
You can localize things
using an object detector.

1522
01:05:17,600 --> 01:05:21,380
You can localize faces
using a face detector.

1523
01:05:21,380 --> 01:05:24,340
And you can have all of
these different capabilities

1524
01:05:24,340 --> 01:05:27,760
across many different other
models that people have created.

1525
01:05:27,760 --> 01:05:30,800
And you can chain them together
to do different kinds of tasks.

1526
01:05:30,800 --> 01:05:32,800
Yeah, so there's two
different ways of doing it.

1527
01:05:32,800 --> 01:05:34,500
One is a static way
of doing it, where

1528
01:05:34,500 --> 01:05:36,860
you want to give it as
diverse examples as possible

1529
01:05:36,860 --> 01:05:38,680
and then hopes that
it generalizes.

1530
01:05:38,680 --> 01:05:40,840
Another one is to
dynamically choose,

1531
01:05:40,840 --> 01:05:43,860
given this question, what are
the best in-context examples I

1532
01:05:43,860 --> 01:05:44,900
should use?

1533
01:05:44,900 --> 01:05:48,420
And so you can treat that as
another retrieval process, where

1534
01:05:48,420 --> 01:05:50,780
you retrieve the best
examples, and then you

1535
01:05:50,780 --> 01:05:52,540
ask it to generate a program.

1536
01:05:52,540 --> 01:05:54,640
And that tends to
perform a lot better,

1537
01:05:54,640 --> 01:05:58,620
but only if you have a
good retrieval system.

1538
01:05:58,620 --> 01:06:00,690
Yes, it would require
a lot of compute.

1539
01:06:00,690 --> 01:06:04,030
So there's compute in
terms of calling GPT, which

1540
01:06:04,030 --> 01:06:05,335
you have to do through an API.

1541
01:06:05,335 --> 01:06:07,710
And then you have to load each
of these individual models

1542
01:06:07,710 --> 01:06:10,790
into your memory and then run
each of them sequentially.

1543
01:06:10,790 --> 01:06:13,630
So it could actually
be a lot more costly.

1544
01:06:13,630 --> 01:06:15,770
And so what people are
trying to do is figure out,

1545
01:06:15,770 --> 01:06:17,150
can we distill
these capabilities

1546
01:06:17,150 --> 01:06:18,670
into a single model?

1547
01:06:18,670 --> 01:06:20,430
And that's a big
part of what research

1548
01:06:20,430 --> 01:06:22,550
looks like today in 2025.

1549
01:06:22,550 --> 01:06:24,570
But of course, people
are also still trying

1550
01:06:24,570 --> 01:06:27,070
to figure out how do you chain
these things effectively very

1551
01:06:27,070 --> 01:06:28,150
well.

1552
01:06:28,150 --> 01:06:29,810
Yeah, you can think
of it as an agent.

1553
01:06:29,810 --> 01:06:30,310
Yeah.

1554
01:06:30,310 --> 01:06:32,570
So you have an agent that's
basically deciding, hey,

1555
01:06:32,570 --> 01:06:35,030
given this question, what are
the other models I need help

1556
01:06:35,030 --> 01:06:39,282
from and how do I stitch them
together to do new capabilities.

1557
01:06:39,282 --> 01:06:40,490
So that's what it looks like.

1558
01:06:40,490 --> 01:06:41,590
Yeah.

1559
01:06:41,590 --> 01:06:44,550
Here's another example where you
might want to do image editing.

1560
01:06:44,550 --> 01:06:47,030
You might care about
replacing the sand the desert

1561
01:06:47,030 --> 01:06:48,990
with lush green grass.

1562
01:06:48,990 --> 01:06:51,923
Of course, image editing models
are still in its infancy.

1563
01:06:51,923 --> 01:06:53,590
And so what you might
want to do instead

1564
01:06:53,590 --> 01:06:56,850
is call a segmentation
model, identify the desert,

1565
01:06:56,850 --> 01:07:01,520
and then only replace the desert
parts, those pixels, with grass,

1566
01:07:01,520 --> 01:07:05,000
and then you can composite them
together to make a new image.

1567
01:07:05,000 --> 01:07:08,440
That's all of the things I
wanted to talk about in terms

1568
01:07:08,440 --> 01:07:10,200
of different capabilities.

1569
01:07:10,200 --> 01:07:13,520
So you've got here
some capabilities

1570
01:07:13,520 --> 01:07:16,440
around how to think
about foundation models.

1571
01:07:16,440 --> 01:07:18,560
It really is, at
the end of the day,

1572
01:07:18,560 --> 01:07:22,120
an ability to train a
model for a single task,

1573
01:07:22,120 --> 01:07:25,640
and then from that
single task generalize

1574
01:07:25,640 --> 01:07:28,560
to many different
downstream applications.

1575
01:07:28,560 --> 01:07:30,520
And we talked about
in classification

1576
01:07:30,520 --> 01:07:33,360
how you can create these models
by just taking a lot of image

1577
01:07:33,360 --> 01:07:36,040
text pairs from the internet,
training them together

1578
01:07:36,040 --> 01:07:37,920
to do different kinds of tasks.

1579
01:07:37,920 --> 01:07:39,400
And that allows
you to generalize

1580
01:07:39,400 --> 01:07:41,800
to new kinds of data
sets that might not even

1581
01:07:41,800 --> 01:07:44,080
exist in the real world
or have any labels

1582
01:07:44,080 --> 01:07:45,720
for in the real world.

1583
01:07:45,720 --> 01:07:48,040
You can also combine
them with language models

1584
01:07:48,040 --> 01:07:51,640
and train them to do in-context
examples like captioning

1585
01:07:51,640 --> 01:07:53,360
or counting or OCR.

1586
01:07:53,360 --> 01:07:55,280
And these are again
capabilities that enable

1587
01:07:55,280 --> 01:07:57,010
many different applications.

1588
01:07:57,010 --> 01:07:58,890
And then of course, the
outputs don't always

1589
01:07:58,890 --> 01:08:00,510
have to be language
or categories.

1590
01:08:00,510 --> 01:08:02,610
They can also be
segmentation masks,

1591
01:08:02,610 --> 01:08:04,810
where you can take
different kinds of masks

1592
01:08:04,810 --> 01:08:06,850
depending on
different user inputs.

1593
01:08:06,850 --> 01:08:09,650
And you can generalize this
even further by combining many

1594
01:08:09,650 --> 01:08:11,850
of these foundation models
or even smaller models

1595
01:08:11,850 --> 01:08:16,450
together through programs and
do all kinds of new things.

1596
01:08:16,450 --> 01:08:20,490
So hallucinations still
happen all across the board.

1597
01:08:20,490 --> 01:08:22,450
What we're showing
is that it seems

1598
01:08:22,450 --> 01:08:25,090
like pointing does reduce
hallucinations quite a bit

1599
01:08:25,090 --> 01:08:27,569
because it does need
to find some evidence

1600
01:08:27,569 --> 01:08:29,021
for its generations.

1601
01:08:29,021 --> 01:08:30,729
But that being said,
there's no guarantee

1602
01:08:30,729 --> 01:08:33,649
that it's going to point
to the right thing at all.

1603
01:08:33,649 --> 01:08:35,790
So there's many different
ways of fixing for this.

1604
01:08:35,790 --> 01:08:38,850
One is, of course,
collecting more data

1605
01:08:38,850 --> 01:08:41,490
related to the reasoning
that you wanted to do.

1606
01:08:41,490 --> 01:08:45,130
But a better one is to even
have verification methods that

1607
01:08:45,130 --> 01:08:48,410
verify based on the points,
whether the output is something

1608
01:08:48,410 --> 01:08:49,545
you should trust or not.

1609
01:08:49,545 --> 01:08:51,670
So a lot of the bigger
models and bigger companies,

1610
01:08:51,670 --> 01:08:55,660
what they typically do when you
use any of their models is they

1611
01:08:55,660 --> 01:08:58,240
don't have a single model
that's generating an output.

1612
01:08:58,240 --> 01:09:01,540
You usually take that output and
pass it through other verifiers

1613
01:09:01,540 --> 01:09:03,700
before it even gets to the user.

1614
01:09:03,700 --> 01:09:06,060
And that mitigates
some of these problems.

1615
01:09:06,060 --> 01:09:08,680
But it is an active line
of inquiry right now.

1616
01:09:08,680 --> 01:09:11,700
How do you reduce hallucinations
and also improve these models

1617
01:09:11,700 --> 01:09:13,420
actual accuracy?

1618
01:09:13,420 --> 01:09:14,380
Yeah.

1619
01:09:14,380 --> 01:09:17,380
So repeating your question, is
it possible for these models

1620
01:09:17,380 --> 01:09:21,220
to build new tools when a
capability requires a tool

1621
01:09:21,220 --> 01:09:22,859
that it doesn't have?

1622
01:09:22,859 --> 01:09:24,580
Yes, it can.

1623
01:09:24,580 --> 01:09:28,300
We have a few preliminary
experiments in those directions

1624
01:09:28,300 --> 01:09:29,840
as well, where you
can tell a model,

1625
01:09:29,840 --> 01:09:31,843
here's a capability that I want.

1626
01:09:31,843 --> 01:09:33,260
And what you can
build is a system

1627
01:09:33,260 --> 01:09:35,979
that automatically tries
to collect training data

1628
01:09:35,979 --> 01:09:39,260
and builds a tool for
specific use cases.

1629
01:09:39,260 --> 01:09:42,180
But that line of work is
still again, in its infancy.

1630
01:09:42,180 --> 01:09:44,300
It's one that we're
actively working on.

1631
01:09:44,300 --> 01:09:48,829
But a lot of folks are
excited about that direction.