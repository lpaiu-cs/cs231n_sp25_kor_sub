2
00:00:05,270 --> 00:00:06,470
OK.

3
00:00:06,470 --> 00:00:12,920
Today, we'll be talking about
different tasks of core Computer

4
00:00:12,920 --> 00:00:19,110
Vision, algorithms and tasks,
detection and segmentation.

5
00:00:19,110 --> 00:00:22,250
We will also be covering
topics around visualization

6
00:00:22,250 --> 00:00:23,700
and understanding.

7
00:00:23,700 --> 00:00:26,275
I will cover the
most important ones.

9
00:00:28,820 --> 00:00:32,040
Like the previous
lecture last time,

10
00:00:32,040 --> 00:00:37,430
what we discussed was around
the topic of transitioning

11
00:00:37,430 --> 00:00:43,940
from sequence to sequence
models, RNNs to transformers.

12
00:00:43,940 --> 00:00:47,570
And we saw that
transformers were

13
00:00:47,570 --> 00:00:52,520
defined by having some
encoder, a number of layers,

14
00:00:52,520 --> 00:00:57,540
which had multi-headed,
self-attention, and layer norm,

15
00:00:57,540 --> 00:01:01,470
as well as some MLP layers.

16
00:01:01,470 --> 00:01:06,500
And this was, ultimately,
called something

17
00:01:06,500 --> 00:01:10,500
that we now refer to as
encoder encoding the sequence.

18
00:01:10,500 --> 00:01:16,100
And then if we need to decode an
image or a language, a sequence

19
00:01:16,100 --> 00:01:19,970
as the output, then similar
type of architecture

20
00:01:19,970 --> 00:01:27,210
is used for decoder getting
the encoder tokens as input,

21
00:01:27,210 --> 00:01:32,660
taking those as input, and
then generating what is--

22
00:01:32,660 --> 00:01:34,970
I'm hoping that you
can see my cursor too,

23
00:01:34,970 --> 00:01:42,060
what is the desired output.

24
00:01:42,060 --> 00:01:43,560
We did talk--

25
00:01:43,560 --> 00:01:48,620
Justin talked quite extensively
about the differences

26
00:01:48,620 --> 00:01:55,470
of modeling sequences,
recurrent neural networks, RNNs,

27
00:01:55,470 --> 00:01:59,210
and their variations that
we've talked last week,

28
00:01:59,210 --> 00:02:04,940
I think, on Tuesday about and
then using convolution also as

29
00:02:04,940 --> 00:02:06,690
another approach.

30
00:02:06,690 --> 00:02:10,639
But we talked about,
ultimately, that self-attention

31
00:02:10,639 --> 00:02:17,910
is what we work with in many
of the applications these days.

32
00:02:17,910 --> 00:02:24,470
They work much better
than the other two.

33
00:02:24,470 --> 00:02:25,590
They are more expensive.

34
00:02:25,590 --> 00:02:31,775
They do add computation
and memory requirements,

35
00:02:31,775 --> 00:02:36,650
but that comes with much
better modeling of the sequence

36
00:02:36,650 --> 00:02:42,110
and better results in
terms of any of the tasks.

37
00:02:42,110 --> 00:02:48,835
So until here, it was mostly
talking about self-attention.

38
00:02:48,835 --> 00:02:50,210
We also talked a
little bit about

39
00:02:50,210 --> 00:02:55,050
cross-attention
and related topics.

40
00:02:55,050 --> 00:02:59,750
And then we got to the topic
of vision transformers, which

41
00:02:59,750 --> 00:03:02,840
is one of the core
models that is

42
00:03:02,840 --> 00:03:06,290
being used in modern
applications, Computer Vision

43
00:03:06,290 --> 00:03:07,410
applications.

44
00:03:07,410 --> 00:03:17,330
We did go through this in the
last minutes of last lecture,

45
00:03:17,330 --> 00:03:18,300
the previous lecture.

46
00:03:18,300 --> 00:03:21,030
And I want to revisit the topic.

47
00:03:21,030 --> 00:03:26,000
And after that, I'll stop and
hear any questions or comments,

48
00:03:26,000 --> 00:03:28,340
you may have regarding the
assignments and everything

49
00:03:28,340 --> 00:03:30,830
that I talked about so far.

50
00:03:30,830 --> 00:03:33,320
We talked about
the fact that what

51
00:03:33,320 --> 00:03:38,460
we do with transformers, when
we want to process images,

52
00:03:38,460 --> 00:03:48,440
we split the image into patches,
basically creating a sequence.

53
00:03:48,440 --> 00:03:57,590
The image was split to S by
S or in this case, maybe a 3

54
00:03:57,590 --> 00:03:59,760
by 3 patches.

55
00:03:59,760 --> 00:04:02,630
And each of those
patches are then

56
00:04:02,630 --> 00:04:05,280
represented by what
we call tokens.

57
00:04:05,280 --> 00:04:10,610
And tokens are often
a linear projection

58
00:04:10,610 --> 00:04:17,810
of the vector, the
reshaped version

59
00:04:17,810 --> 00:04:20,100
of the image into a vector.

60
00:04:20,100 --> 00:04:24,510
And it's basically a
D-dimensional vector,

61
00:04:24,510 --> 00:04:27,060
as you can see in this slide.

62
00:04:27,060 --> 00:04:29,330
But because we have
turned the image

63
00:04:29,330 --> 00:04:32,210
into patches, what
becomes important-- what

64
00:04:32,210 --> 00:04:33,540
are we losing here?

65
00:04:33,540 --> 00:04:36,390
It's basically we're
losing the position,

66
00:04:36,390 --> 00:04:38,550
the 2D position of the image.

67
00:04:38,550 --> 00:04:43,970
So that's why we often create
or add something that we

68
00:04:43,970 --> 00:04:46,110
call positional embedding.

69
00:04:46,110 --> 00:04:49,080
And there are many different
ways of doing this.

70
00:04:49,080 --> 00:04:53,790
You can create a sequence, just
put numbers of sequence as 1,

71
00:04:53,790 --> 00:04:55,290
2, 3, and so on.

72
00:04:55,290 --> 00:05:02,250
Or you can do a 2D version
of x and y-coordinates,

73
00:05:02,250 --> 00:05:09,080
and adding these two together
creates the new token that goes

74
00:05:09,080 --> 00:05:12,350
to the transformer
layer the same way all

75
00:05:12,350 --> 00:05:15,020
of the self-attention,
layer norm,

76
00:05:15,020 --> 00:05:19,640
and everything that we've
talked about, MLP, everything,

77
00:05:19,640 --> 00:05:24,680
we talked about last week.

78
00:05:24,680 --> 00:05:29,000
And then the output layer will
generate the output vectors

79
00:05:29,000 --> 00:05:29,615
for us.

80
00:05:29,615 --> 00:05:32,280
It could be used
for any application.

81
00:05:32,280 --> 00:05:35,270
One of the major applications
in Computer Vision

82
00:05:35,270 --> 00:05:36,960
has been classification.

83
00:05:36,960 --> 00:05:39,120
We started with
image classification.

84
00:05:39,120 --> 00:05:42,800
So with image classification,
what becomes important

85
00:05:42,800 --> 00:05:51,530
is to somehow be able to
encode or generate something

86
00:05:51,530 --> 00:05:54,960
as the output that is
representative of the class.

87
00:05:54,960 --> 00:06:00,380
So what we do is often, we add
one token, a special extra input

88
00:06:00,380 --> 00:06:05,730
to the transformer, which is
of the same dimensionality,

89
00:06:05,730 --> 00:06:09,440
but it's a learnable
parameter that

90
00:06:09,440 --> 00:06:13,670
in the output space,
whatever that represents

91
00:06:13,670 --> 00:06:19,590
is going to be turned into
the class probability vector.

92
00:06:19,590 --> 00:06:22,680
So a C-dimensional vector, which
is the class probabilities.

93
00:06:22,680 --> 00:06:26,670
And that's what we often
call the class token.

94
00:06:26,670 --> 00:06:31,790
So this is one of the
base and most standard way

95
00:06:31,790 --> 00:06:35,360
of doing using ViTs,
vision transformers,

96
00:06:35,360 --> 00:06:39,830
for image classification.

97
00:06:39,830 --> 00:06:42,810
But transformers are not
only used for classification,

98
00:06:42,810 --> 00:06:45,200
they could be used
for many other tasks

99
00:06:45,200 --> 00:06:49,410
that we will be covering
some of those today as well.

100
00:06:49,410 --> 00:06:52,970
But last week, we also talked
about this other variant

101
00:06:52,970 --> 00:06:54,720
of the transformers.

102
00:06:54,720 --> 00:06:56,250
Again, same tokens.

103
00:06:56,250 --> 00:07:03,090
And from the tokens we go
with the transformer layers.

104
00:07:03,090 --> 00:07:05,600
If you remember
last time, we talked

105
00:07:05,600 --> 00:07:09,780
about these multiple
layers of transformers.

106
00:07:09,780 --> 00:07:13,890
As I said, positional
embeddings are added.

107
00:07:13,890 --> 00:07:20,130
And here, because we see the
entire image all together,

108
00:07:20,130 --> 00:07:22,370
we don't have to
do masking like we

109
00:07:22,370 --> 00:07:25,160
did for language, because
language is really a sequence

110
00:07:25,160 --> 00:07:29,490
that we shouldn't be using
the future information for.

111
00:07:29,490 --> 00:07:33,650
And then, ultimately,
transformers

112
00:07:33,650 --> 00:07:40,680
give an output of a vector
patch for each of the inputs.

113
00:07:40,680 --> 00:07:44,210
And the other option for
training a transformer

114
00:07:44,210 --> 00:07:48,150
is actually to instead of
having a separate class token,

115
00:07:48,150 --> 00:07:51,300
just take the outputs,
run a pooling layer,

116
00:07:51,300 --> 00:07:55,340
and then turn that
into a probability

117
00:07:55,340 --> 00:07:57,360
vector for C different classes.

118
00:07:57,360 --> 00:08:00,810
So I talked about two
versions of transformers.

119
00:08:00,810 --> 00:08:03,540
One of them was we're
using a class token.

120
00:08:03,540 --> 00:08:07,920
And the other one was we take
all of the output tokens.

121
00:08:07,920 --> 00:08:12,110
We apply pooling and projection
into a vector that represents

122
00:08:12,110 --> 00:08:14,010
the class probabilities.

123
00:08:14,010 --> 00:08:15,960
How we supervise this?

124
00:08:15,960 --> 00:08:19,920
It's the exact same thing
that we talked about earlier.

125
00:08:19,920 --> 00:08:23,870
And that was back
propagation, defining

126
00:08:23,870 --> 00:08:27,200
a loss function, binary
cross-entropy, the soft log, max

127
00:08:27,200 --> 00:08:29,450
loss, and so on.

128
00:08:29,450 --> 00:08:33,230
So this was ViTs.

129
00:08:33,230 --> 00:08:35,103
This is ViTs in a nutshell.

131
00:08:37,969 --> 00:08:41,840
And over the years, this
type of architecture

132
00:08:41,840 --> 00:08:46,490
for many different applications
have remained the same.

133
00:08:46,490 --> 00:08:53,300
Many modern architectures right
now use many of these components

134
00:08:53,300 --> 00:08:56,550
very similar to what
we presented here.

135
00:08:56,550 --> 00:09:00,320
But there are some
optimizations that we

136
00:09:00,320 --> 00:09:03,110
had in the slides
last week, but I

137
00:09:03,110 --> 00:09:10,170
think I'll just spend quickly
a couple of minutes on them.

138
00:09:10,170 --> 00:09:12,290
But I want you to
understand that there

139
00:09:12,290 --> 00:09:18,200
are many different tweaks
and optimizations for better

140
00:09:18,200 --> 00:09:21,740
performance, and also making
the transformers, the training

141
00:09:21,740 --> 00:09:23,870
a little bit more stable.

142
00:09:23,870 --> 00:09:28,710
One of them is actually
the residual connections.

143
00:09:28,710 --> 00:09:33,900
This layer norm is basically
outside the residual

144
00:09:33,900 --> 00:09:38,890
So this means that whatever
we get here, we normalize it.

145
00:09:38,890 --> 00:09:42,860
So this doesn't really mean
that we can't replicate

146
00:09:42,860 --> 00:09:46,190
any form of identity function
anymore, that resonates really

147
00:09:46,190 --> 00:09:47,610
wanted to do.

148
00:09:47,610 --> 00:09:50,030
So the solution for
that is to bring

149
00:09:50,030 --> 00:09:53,160
in the layer normalization.

150
00:09:53,160 --> 00:09:56,210
We often put it
before self-attention

151
00:09:56,210 --> 00:09:59,370
and the second one
before the MLP layer.

152
00:09:59,370 --> 00:10:01,320
So normalization is there.

153
00:10:01,320 --> 00:10:05,030
But we also preserve
our identity function.

154
00:10:05,030 --> 00:10:09,180
There are also other
ways of normalizing.

155
00:10:09,180 --> 00:10:13,070
There is this RMSNorm, root
mean square normalization,

156
00:10:13,070 --> 00:10:19,820
which is actually a very
basic type of normalization.

157
00:10:19,820 --> 00:10:21,620
For each of the
features, it doesn't

158
00:10:21,620 --> 00:10:25,590
use the mean value of the
feature for normalization.

159
00:10:25,590 --> 00:10:29,610
But this makes the training
a little bit more stable.

160
00:10:29,610 --> 00:10:38,390
Again, these are all empirically
shown to be better options.

161
00:10:38,390 --> 00:10:40,490
Although there are
some justifications

162
00:10:40,490 --> 00:10:42,040
why while they work well.

163
00:10:42,040 --> 00:10:46,065
But mostly, the reason
for adopting these

164
00:10:46,065 --> 00:10:53,290
is just the fact that they
make the trainings more stable.

165
00:10:53,290 --> 00:10:58,380
The other option is to
instead of using a simple MLP,

166
00:10:58,380 --> 00:11:05,190
we use a SwiGLU MLP, where
we actually do some--

167
00:11:05,190 --> 00:11:08,530
this is what we call
gated nonlinearity.

168
00:11:08,530 --> 00:11:13,720
Instead of having two vectors of
weights, matrices of W1 and W2,

169
00:11:13,720 --> 00:11:16,750
we add a third one, 1, 2, and 3.

170
00:11:16,750 --> 00:11:22,710
But here, we create
some gated nonlinearity.

171
00:11:22,710 --> 00:11:36,390
Basically, what it does
is getting more trainable

172
00:11:36,390 --> 00:11:38,700
parameters and not
just necessarily

173
00:11:38,700 --> 00:11:42,210
trainable parameters, but
creating a better nonlinearity

174
00:11:42,210 --> 00:11:43,750
for a small architecture.

175
00:11:43,750 --> 00:11:50,250
Even if we select the hidden
layer value equal to 8

176
00:11:50,250 --> 00:11:53,790
divided by 3, it
keeps the same size

177
00:11:53,790 --> 00:11:56,200
of the network in terms of
the number of parameters,

178
00:11:56,200 --> 00:12:04,540
but it does learn higher
dimensional nonlinearities

179
00:12:04,540 --> 00:12:05,950
in that layer.

180
00:12:05,950 --> 00:12:09,540
The last piece is
mixture of experts

181
00:12:09,540 --> 00:12:13,590
that is often used in even
the very modern architectures

182
00:12:13,590 --> 00:12:14,530
these days.

183
00:12:14,530 --> 00:12:17,770
Instead of having one
set of MLP layers,

184
00:12:17,770 --> 00:12:20,560
we can have multiple
sets of MLP layers.

185
00:12:20,560 --> 00:12:22,720
Each of those will be an expert.

186
00:12:22,720 --> 00:12:28,930
And what we do is
through a router,

187
00:12:28,930 --> 00:12:34,630
the tokens will be routed
to A of those E experts.

188
00:12:34,630 --> 00:12:40,360
And in this way, we actually
have A active experts.

189
00:12:40,360 --> 00:12:43,110
But then, again,
what it does is it

190
00:12:43,110 --> 00:12:48,220
increases the number
of parameters,

191
00:12:48,220 --> 00:12:55,200
and it helps learning more
robust models without increasing

192
00:12:55,200 --> 00:12:57,850
too much on the compute.

193
00:12:57,850 --> 00:13:01,240
And these, again, are
all parallel MLPs.

194
00:13:01,240 --> 00:13:05,290
So we can have multiple
experts in parallel.

195
00:13:05,290 --> 00:13:08,430
As I said, they are
used in all LLMs

196
00:13:08,430 --> 00:13:10,260
these days, large
language models,

197
00:13:10,260 --> 00:13:15,420
and all of the modern
LLMs up to the level

198
00:13:15,420 --> 00:13:20,860
that we know about are
using these types of tweaks.

199
00:13:20,860 --> 00:13:23,520
And this is the summary
of all of the tweaks

200
00:13:23,520 --> 00:13:25,740
that I just mentioned.

201
00:13:25,740 --> 00:13:28,805
This is similar
to [? bias. ?] No.

202
00:13:28,805 --> 00:13:32,010
This is completely a
trainable parameter by itself

203
00:13:32,010 --> 00:13:37,500
that you train, either a
feedforward network or just

204
00:13:37,500 --> 00:13:42,420
a linear projection to turn that
into the probability vector.

205
00:13:42,420 --> 00:13:45,570
So it's not [? just bias. ?]

206
00:13:45,570 --> 00:13:47,550
And then again,
remember that you

207
00:13:47,550 --> 00:13:54,370
have so many self-attention
networks here, layers here.

208
00:13:54,370 --> 00:13:56,250
And those self-attention
layers are

209
00:13:56,250 --> 00:13:59,050
basically fusing
the information,

210
00:13:59,050 --> 00:14:02,190
creating attention between all
of the tokens and this class

211
00:14:02,190 --> 00:14:02,830
token.

212
00:14:02,830 --> 00:14:05,260
So when you supervise
it from here,

213
00:14:05,260 --> 00:14:06,700
the loss function comes in.

214
00:14:06,700 --> 00:14:13,860
This will represent the
class probabilities vector.

215
00:14:13,860 --> 00:14:16,800
So the question is, if
there are nice intuitions,

216
00:14:16,800 --> 00:14:19,920
what different
experts are doing?

217
00:14:19,920 --> 00:14:21,930
That's a great question.

218
00:14:21,930 --> 00:14:24,930
Because they are
trained in parallel,

219
00:14:24,930 --> 00:14:27,700
and they are
initialized differently,

220
00:14:27,700 --> 00:14:33,630
they often try to learn one
aspect or a related, maybe also

221
00:14:33,630 --> 00:14:36,220
sometimes very much
related aspect,

222
00:14:36,220 --> 00:14:43,510
but it's just adding more
compute and more parameters,

223
00:14:43,510 --> 00:14:47,760
giving the network to learn
different things, if it

224
00:14:47,760 --> 00:14:51,160
does have to learn
multiple concepts.

225
00:14:51,160 --> 00:14:54,660
For example, if you have to
cover multiple probability

226
00:14:54,660 --> 00:14:57,400
distributions, then
with these MLPs,

227
00:14:57,400 --> 00:15:02,260
you often have the power to
separate those modes of data.

228
00:15:02,260 --> 00:15:06,150
So the question is, if
the number of experts

229
00:15:06,150 --> 00:15:09,370
is a hyperparameter or not?

230
00:15:09,370 --> 00:15:11,800
Yes, definitely, it's
a hyperparameter.

231
00:15:11,800 --> 00:15:15,780
From what I know,
it's often predefined.

232
00:15:15,780 --> 00:15:18,910
Don't necessarily like
over fine tune them.

233
00:15:18,910 --> 00:15:21,553
But yes, they are
all hyperparameters.

234
00:15:21,553 --> 00:15:22,470
They are also learned.

235
00:15:22,470 --> 00:15:23,490
Yes.

236
00:15:23,490 --> 00:15:25,050
And they are learned.

237
00:15:25,050 --> 00:15:30,450
So why moving the
layer norm helps us

238
00:15:30,450 --> 00:15:33,160
learn identity transformation?

239
00:15:33,160 --> 00:15:35,110
So look at this architecture.

240
00:15:35,110 --> 00:15:38,470
Will you be able to create
any form of identity?

241
00:15:38,470 --> 00:15:41,680
Because right after that
residual connection,

242
00:15:41,680 --> 00:15:44,490
the feature values are
changed, because you

243
00:15:44,490 --> 00:15:45,900
have a normalization.

244
00:15:45,900 --> 00:15:48,990
You will never have the
identity in the features.

245
00:15:48,990 --> 00:15:53,880
Because right after that, you
see the layer normalization.

246
00:15:53,880 --> 00:15:57,570
And that's why, what we
do is we bring it in.

247
00:15:57,570 --> 00:16:04,750
We have quite a few different
tasks in Computer Vision.

248
00:16:04,750 --> 00:16:09,990
And these were the core and
the most important tasks

249
00:16:09,990 --> 00:16:13,360
over the years for Computer
Vision applications.

250
00:16:13,360 --> 00:16:16,900
Although these days, we are
solving much harder tasks.

251
00:16:16,900 --> 00:16:19,320
And nobody cares about
object detection anymore,

252
00:16:19,320 --> 00:16:22,510
because now, we can just do
it with one line of code.

253
00:16:22,510 --> 00:16:25,470
But over the past
10, 15 years, there

254
00:16:25,470 --> 00:16:27,000
has been a lot of advances.

255
00:16:27,000 --> 00:16:31,230
And we want to cover, I want
to really cover some of those

256
00:16:31,230 --> 00:16:32,290
today.

257
00:16:32,290 --> 00:16:35,640
Just so if you have to design
something new yourself,

258
00:16:35,640 --> 00:16:40,960
you know where to look and
how to design your models.

259
00:16:40,960 --> 00:16:43,230
And then, ultimately,
there is the topic

260
00:16:43,230 --> 00:16:47,310
of visualization and
understanding, which is very

261
00:16:47,310 --> 00:16:49,510
important in many applications.

262
00:16:49,510 --> 00:16:52,360
For example, if you're
working with medical data,

263
00:16:52,360 --> 00:16:54,330
often, the visualization
understanding

264
00:16:54,330 --> 00:16:57,150
is more important than
the classification itself

265
00:16:57,150 --> 00:16:58,895
or detection of
tumor, for example,

266
00:16:58,895 --> 00:17:02,075
if you want to know
where, why, and so on.

268
00:17:06,480 --> 00:17:11,670
The way we started the class,
and this slide is probably

269
00:17:11,670 --> 00:17:18,030
very much familiar to everybody,
we talked about different tasks.

270
00:17:18,030 --> 00:17:23,290
And for object classification,
for the task of classification,

271
00:17:23,290 --> 00:17:24,579
we talked about this.

272
00:17:24,579 --> 00:17:29,610
We spent quite a lot of time
over the first few lectures,

273
00:17:29,610 --> 00:17:33,540
seeing how we can create a
classifier that classifies

274
00:17:33,540 --> 00:17:37,990
images from pixels into labels.

275
00:17:37,990 --> 00:17:45,750
But then one of the other
tasks important similarly

276
00:17:45,750 --> 00:17:48,130
is semantic segmentation.

277
00:17:48,130 --> 00:17:50,110
And within semantic
segmentation,

278
00:17:50,110 --> 00:17:56,520
what we care about is to assign
a label to every single pixel

279
00:17:56,520 --> 00:18:01,110
inside the image, turn
each of the pixels

280
00:18:01,110 --> 00:18:08,235
into a label that is the
label for that object

281
00:18:08,235 --> 00:18:12,630
or anything in the scene.

282
00:18:12,630 --> 00:18:15,600
So basically, when
we train a model that

283
00:18:15,600 --> 00:18:19,890
does this at the test time,
we want to take an image

284
00:18:19,890 --> 00:18:23,130
and generate the same
map as the output.

285
00:18:23,130 --> 00:18:24,910
How do we do that?

286
00:18:24,910 --> 00:18:26,650
There are many
different options.

287
00:18:26,650 --> 00:18:31,140
So let's say, what
I can do is just

288
00:18:31,140 --> 00:18:36,330
look at each pixel, every single
pixel, and say what the value

289
00:18:36,330 --> 00:18:40,360
or what the label for
that pixel should be.

290
00:18:40,360 --> 00:18:43,270
In the very basic form,
as you can see here,

291
00:18:43,270 --> 00:18:46,120
it's actually very
much impossible.

292
00:18:46,120 --> 00:18:52,650
It's hard to say what pixel that
represents that specific-- what

293
00:18:52,650 --> 00:18:57,360
object that specific pixel
represents because there

294
00:18:57,360 --> 00:19:00,430
is no context, if you only
look at the pixel itself.

295
00:19:00,430 --> 00:19:04,270
So that's why
context is important.

296
00:19:04,270 --> 00:19:06,900
We look at the
surrounding areas.

298
00:19:09,475 --> 00:19:13,920
And then if I take these
patches and the pixel

299
00:19:13,920 --> 00:19:16,270
in the center and the
surrounding areas,

300
00:19:16,270 --> 00:19:19,920
now, I can train a
convolutional neural network

301
00:19:19,920 --> 00:19:23,300
or any network that generates
the output label for us.

302
00:19:23,300 --> 00:19:25,170
It's the same
architecture that we've

303
00:19:25,170 --> 00:19:28,090
talked about over the quarter.

304
00:19:28,090 --> 00:19:30,660
And you can select
any of those that we

305
00:19:30,660 --> 00:19:32,500
used for image classification.

306
00:19:32,500 --> 00:19:35,140
Because now, you're
classifying the entire image.

307
00:19:35,140 --> 00:19:38,250
It could be a CNN, it could be
a ResNet, it could be a ViT,

308
00:19:38,250 --> 00:19:40,290
or whatever.

309
00:19:40,290 --> 00:19:44,880
This is really time
consuming because if you

310
00:19:44,880 --> 00:19:50,200
want to run one full network for
every single pixel in an image,

311
00:19:50,200 --> 00:19:55,050
it will take forever to turn
this into a segmentation map.

312
00:19:55,050 --> 00:19:58,380
The other option
that we can use is

313
00:19:58,380 --> 00:20:05,080
to instead of running one
network for every single pixel,

314
00:20:05,080 --> 00:20:07,860
what if we train a
neural network that

315
00:20:07,860 --> 00:20:13,770
takes the image as the input and
outputs the entire pixel map,

316
00:20:13,770 --> 00:20:17,380
the segmentation map, not
just one single label,

317
00:20:17,380 --> 00:20:20,640
a matrix of labels?

318
00:20:20,640 --> 00:20:28,150
And in that case, we will have
our segmentation task solved.

319
00:20:28,150 --> 00:20:32,070
And in order to do
that, we need to have

320
00:20:32,070 --> 00:20:36,960
a layer in the input that is
the same size as the image.

321
00:20:36,960 --> 00:20:39,720
And also in the
output, we also need

322
00:20:39,720 --> 00:20:42,220
some sort of an inflated layer.

323
00:20:42,220 --> 00:20:47,970
You can't go to fully connected
layers, and so on, because now,

324
00:20:47,970 --> 00:20:50,080
we are generating an image.

325
00:20:50,080 --> 00:20:57,695
And because of that, we need
to keep the network inflated.

326
00:20:57,695 --> 00:21:03,180
And then that's what we call,
often, fully convolutional

327
00:21:03,180 --> 00:21:06,600
neural networks or FCNs.

328
00:21:06,600 --> 00:21:10,725
So with fully convolutional
neural networks,

329
00:21:10,725 --> 00:21:13,780
this is definitely a great idea.

330
00:21:13,780 --> 00:21:15,130
But there is a caveat.

331
00:21:15,130 --> 00:21:16,240
There is a problem.

332
00:21:16,240 --> 00:21:18,100
These images are large.

333
00:21:18,100 --> 00:21:24,210
And these networks, these
layers will become very large.

334
00:21:24,210 --> 00:21:26,550
And there will be
so many parameters

335
00:21:26,550 --> 00:21:28,620
to optimize, especially
in the early years

336
00:21:28,620 --> 00:21:31,300
that we didn't
have powerful GPUs,

337
00:21:31,300 --> 00:21:34,950
this was a bottleneck,
a problem, a challenge

338
00:21:34,950 --> 00:21:37,210
for training algorithms.

339
00:21:37,210 --> 00:21:40,980
And that's why the
algorithms evolved

340
00:21:40,980 --> 00:21:46,530
into starting from
full-size images going down

341
00:21:46,530 --> 00:21:49,680
in terms of the
resolution, making

342
00:21:49,680 --> 00:21:53,790
the convolutions, the spatial
resolutions smaller and smaller

343
00:21:53,790 --> 00:21:56,650
through downsampling operations.

344
00:21:56,650 --> 00:21:59,100
And then somewhere
in the middle,

345
00:21:59,100 --> 00:22:02,490
we will have a low
resolution, but somehow thick

346
00:22:02,490 --> 00:22:05,165
in terms of the
number of channels.

347
00:22:05,165 --> 00:22:08,370
And then from
there, what we do is

348
00:22:08,370 --> 00:22:10,740
we go back up to the
same size of the image

349
00:22:10,740 --> 00:22:13,800
to create the output pixel.

350
00:22:13,800 --> 00:22:19,860
And in order to do that, we
know how to do the downsampling.

351
00:22:19,860 --> 00:22:21,250
Downsampling was easy.

352
00:22:21,250 --> 00:22:22,420
We've talked about it.

353
00:22:22,420 --> 00:22:26,500
We talked about pooling
operation, strided convolution,

354
00:22:26,500 --> 00:22:35,460
and several other steps
or operations that

355
00:22:35,460 --> 00:22:37,980
could be used here.

356
00:22:37,980 --> 00:22:41,880
But on the upsampling
side, we don't really

357
00:22:41,880 --> 00:22:46,770
know how to do the upsampling
because we don't have pooling

358
00:22:46,770 --> 00:22:55,135
or reverse of up pooling or
reverse strided convolutions.

359
00:22:55,135 --> 00:22:57,990
And because of that,
we had to invent

360
00:22:57,990 --> 00:23:04,750
some new operations that
reverses downsampling by itself.

361
00:23:04,750 --> 00:23:09,900
But before I go
to the upsampling,

362
00:23:09,900 --> 00:23:12,870
defining what upsampling
is, I just briefly

363
00:23:12,870 --> 00:23:21,160
want to tell you that, maybe
I can ask you a question.

364
00:23:21,160 --> 00:23:24,100
How do you think this
network is trained?

365
00:23:24,100 --> 00:23:27,540
Because now, we have a network
that starts from an image

366
00:23:27,540 --> 00:23:29,440
and ends with an image.

367
00:23:29,440 --> 00:23:33,570
And then the tools that we
have for training this network

368
00:23:33,570 --> 00:23:37,560
was a loss function.

369
00:23:37,560 --> 00:23:41,400
How do you think is the best
to train or define a loss

370
00:23:41,400 --> 00:23:44,370
function for this network?

371
00:23:44,370 --> 00:23:47,130
We talked about softmax loss.

372
00:23:47,130 --> 00:23:51,300
We talked also a little bit
about some regression losses

373
00:23:51,300 --> 00:23:53,170
and SVM loss.

374
00:23:53,170 --> 00:23:58,650
But assuming that we want to
use softmax loss function,

375
00:23:58,650 --> 00:24:02,730
how could we define this
or train this network?

376
00:24:02,730 --> 00:24:04,680
What would the objective be?

377
00:24:04,680 --> 00:24:09,670
So you said mean classification
loss for each of the pixels.

378
00:24:09,670 --> 00:24:11,950
And that's correct.

379
00:24:11,950 --> 00:24:16,690
You can add the loss function
for every single pixel,

380
00:24:16,690 --> 00:24:20,200
because every single pixel
is doing a classification.

381
00:24:20,200 --> 00:24:24,370
So you will have a sigma
over all pixels of the image.

382
00:24:24,370 --> 00:24:28,720
And the loss function is
just a simple softmax.

383
00:24:28,720 --> 00:24:30,970
And then you can backprop.

384
00:24:30,970 --> 00:24:33,810
That's the entire loss
function that you need.

385
00:24:33,810 --> 00:24:37,680
The question is, do
we need, what we call,

386
00:24:37,680 --> 00:24:39,340
ground truths for training?

387
00:24:39,340 --> 00:24:41,920
So that's actually the ground
truths of segmentation.

388
00:24:41,920 --> 00:24:44,280
And yes, for these
types of algorithms,

389
00:24:44,280 --> 00:24:46,060
because they are
fully supervised,

390
00:24:46,060 --> 00:24:49,290
we do need the ground
truths label maps.

391
00:24:49,290 --> 00:24:52,470
And early years,
there has been a lot

392
00:24:52,470 --> 00:24:57,030
of work doing and sitting
down and manually labeling

393
00:24:57,030 --> 00:24:59,530
the pixels to be able to
train these algorithms.

394
00:24:59,530 --> 00:25:00,720
Yes.

395
00:25:00,720 --> 00:25:04,180
These days, we don't need
that because we have tools.

396
00:25:04,180 --> 00:25:07,540
But early on, in order to
train these algorithms,

397
00:25:07,540 --> 00:25:08,990
we needed the ground truth.

399
00:25:13,530 --> 00:25:17,260
Very briefly, let me tell you
what we do with upsampling.

400
00:25:17,260 --> 00:25:19,660
Upsampling is actually
not that hard.

401
00:25:19,660 --> 00:25:22,720
We can use an
unpooling operation.

402
00:25:22,720 --> 00:25:24,730
There are different
ways of doing it.

403
00:25:24,730 --> 00:25:26,440
One is nearest neighbor.

404
00:25:26,440 --> 00:25:32,080
If I want to go from a 2
by 2 as an example here,

405
00:25:32,080 --> 00:25:34,000
a matrix 2 by 2, 4 by 4.

406
00:25:34,000 --> 00:25:39,070
I just need to copy the
data for each of these,

407
00:25:39,070 --> 00:25:42,360
take the nearest neighbor
in the lower resolution one.

408
00:25:42,360 --> 00:25:45,300
Or bed of nails
is you just select

409
00:25:45,300 --> 00:25:48,810
one of those in the
upsampled version.

410
00:25:48,810 --> 00:25:53,850
You only select one of
those, the one in the corner.

411
00:25:53,850 --> 00:25:57,880
To copy the data, replace
everything else with zero.

412
00:25:57,880 --> 00:26:00,240
And through multiple
layers of convolution,

413
00:26:00,240 --> 00:26:05,250
these values will
start appearing.

414
00:26:05,250 --> 00:26:11,550
If we use max pooling
in our network,

415
00:26:11,550 --> 00:26:15,120
in the encoding
side, what we can do

416
00:26:15,120 --> 00:26:20,220
is we can save the locations
of the max, the ones that

417
00:26:20,220 --> 00:26:25,500
were selected, and then copy
the data in the unpooling max,

418
00:26:25,500 --> 00:26:34,930
unpooling stage right over
there that the max was defined.

419
00:26:34,930 --> 00:26:37,980
So basically, we
save the locations.

420
00:26:37,980 --> 00:26:40,350
In the encoding part
and in decoding part

421
00:26:40,350 --> 00:26:46,950
in the upsampling step, we
reuse those saved coordinates.

422
00:26:46,950 --> 00:26:51,750
The other option is to
do a learned upsampling.

423
00:26:51,750 --> 00:26:55,660
All of these that I showed there
is no parameter to be learned.

424
00:26:55,660 --> 00:26:57,370
It's just an operation.

425
00:26:57,370 --> 00:27:01,440
But learned upsampling
are also possible.

426
00:27:01,440 --> 00:27:05,040
Very simply, let's
revisit the convolution.

427
00:27:05,040 --> 00:27:07,260
In the convolution
layer, what we did

428
00:27:07,260 --> 00:27:10,860
was applying a convolution
filter for a pixel

429
00:27:10,860 --> 00:27:13,860
and generating the output,
and do this repeatedly

430
00:27:13,860 --> 00:27:16,620
for all of the pixels.

431
00:27:16,620 --> 00:27:20,400
And when we wanted to do--

432
00:27:20,400 --> 00:27:23,610
to downsample, what
we did was strided

433
00:27:23,610 --> 00:27:27,220
convolution, where, instead
of taking steps of 1,

434
00:27:27,220 --> 00:27:31,720
we take steps of 2, and generate
the outputs step by step.

435
00:27:31,720 --> 00:27:34,888
If you don't remember this
part, go back to the lecture.

436
00:27:34,888 --> 00:27:35,680
We talked about it.

437
00:27:35,680 --> 00:27:36,638
Third lecture, I think.

439
00:27:39,275 --> 00:27:44,590
And then we can replicate the
same for the upsampling process.

440
00:27:44,590 --> 00:27:51,400
So this one will represent this
area in the upsampled image.

441
00:27:51,400 --> 00:27:53,670
And then we define
some weights here

442
00:27:53,670 --> 00:27:57,550
to map that to the output map.

443
00:27:57,550 --> 00:27:59,830
And then for the
next one, same story,

444
00:27:59,830 --> 00:28:01,480
but there will be overlaps.

445
00:28:01,480 --> 00:28:04,840
And for the overlaps, we
often sum over the values.

446
00:28:04,840 --> 00:28:08,320
Let me give you an example,
sum over the outputs.

447
00:28:08,320 --> 00:28:09,940
Let me give you an example.

448
00:28:09,940 --> 00:28:14,680
And that's with a
simple 1D function.

449
00:28:14,680 --> 00:28:18,810
If the input is just
two values of A and B,

450
00:28:18,810 --> 00:28:24,450
we learn a filter that that
filter maps it to the higher

451
00:28:24,450 --> 00:28:26,206
resolution output.

452
00:28:26,206 --> 00:28:30,000
And for doing that, we
just apply the filter

453
00:28:30,000 --> 00:28:34,980
to each of the values and
write the outputs here.

454
00:28:34,980 --> 00:28:38,550
For the parts that there is an
overlap, it's the summation.

455
00:28:38,550 --> 00:28:46,306
Addition of what is coming
from each of the two locations.

457
00:28:49,380 --> 00:28:58,575
So we did talk about this fully
convolutional neural networks

458
00:28:58,575 --> 00:29:00,160
and how they are being used.

459
00:29:00,160 --> 00:29:07,500
These are actually some of the
most basic and mostly widely

460
00:29:07,500 --> 00:29:11,670
used algorithms
for segmentation.

461
00:29:11,670 --> 00:29:15,300
And I want to
also, very quickly,

462
00:29:15,300 --> 00:29:21,040
highlight one of the
widely used networks units.

463
00:29:21,040 --> 00:29:23,700
As you can see,
the shape U, it's

464
00:29:23,700 --> 00:29:26,020
actually the same
architecture as I showed here.

465
00:29:26,020 --> 00:29:31,900
Just let's draw it as
similar to a U shape.

466
00:29:31,900 --> 00:29:35,520
And the reason that
I'm highlighting this

467
00:29:35,520 --> 00:29:40,770
is that still, today, some
of the medical applications

468
00:29:40,770 --> 00:29:44,560
that work on segmentation,
use segmentation algorithms,

469
00:29:44,560 --> 00:29:53,040
still, this unit or its variants
generate the state-of-the art

470
00:29:53,040 --> 00:29:56,490
results, if you don't want
to use any foundation model.

471
00:29:56,490 --> 00:30:01,080
So what it does is
exactly what we explained.

472
00:30:01,080 --> 00:30:05,340
A downsampling phase that
increases the field of view

473
00:30:05,340 --> 00:30:08,380
and loses some
spatial information,

474
00:30:08,380 --> 00:30:12,570
and then upsampling
phase that goes back

475
00:30:12,570 --> 00:30:14,560
to the image resolution.

476
00:30:14,560 --> 00:30:16,440
The only difference
that U-Net has

477
00:30:16,440 --> 00:30:20,370
is because it's used
for segmentation,

478
00:30:20,370 --> 00:30:24,480
there is this
understanding that we

479
00:30:24,480 --> 00:30:32,230
need to keep the spatial
information in the decoder side,

480
00:30:32,230 --> 00:30:35,500
because when we downsample,
we somehow lose resolution.

481
00:30:35,500 --> 00:30:38,770
And then upsampling, if you
don't have the information,

482
00:30:38,770 --> 00:30:40,930
it's going to be
a little bit hard.

483
00:30:40,930 --> 00:30:45,610
And we often get into
sometimes boundaries are faded.

484
00:30:45,610 --> 00:30:53,220
And in order not to get the
feature maps in the encoder side

485
00:30:53,220 --> 00:30:58,570
are actually copied as
inputs to the decoder layers.

486
00:30:58,570 --> 00:31:03,900
In that way, you're keeping
this structural information

487
00:31:03,900 --> 00:31:06,600
within the image and
generate the outputs

488
00:31:06,600 --> 00:31:09,070
that are much sharper.

489
00:31:09,070 --> 00:31:14,050
So this was the idea
behind U-Net And as I said,

490
00:31:14,050 --> 00:31:17,560
it's actually being
used quite often.

491
00:31:17,560 --> 00:31:21,390
Summary of semantic
segmentation--

492
00:31:21,390 --> 00:31:24,840
what we talked about today,
the fully convolutional

493
00:31:24,840 --> 00:31:26,580
neural networks.

494
00:31:26,580 --> 00:31:32,010
You have same filter
as before that we

495
00:31:32,010 --> 00:31:34,100
had for downsampling here.

497
00:31:42,780 --> 00:31:45,630
To save time, I actually
removed some of the slides

498
00:31:45,630 --> 00:31:46,690
from this part.

499
00:31:46,690 --> 00:31:48,370
And I have it in the
back of the slides.

500
00:31:48,370 --> 00:31:49,510
You should check it out.

501
00:31:49,510 --> 00:31:51,330
This is a reverse.

502
00:31:51,330 --> 00:31:54,130
This is transformed
transposed convolution.

503
00:31:54,130 --> 00:31:57,220
So we do have a 3
by 3 matrix here.

504
00:31:57,220 --> 00:32:03,040
And then instead of convolving
the input image, input data,

505
00:32:03,040 --> 00:32:05,530
we do the convolution on the--

506
00:32:05,530 --> 00:32:09,410
applied convolution on the
transposed version of the input.

507
00:32:09,410 --> 00:32:12,590
And it actually generates
a larger output.

508
00:32:12,590 --> 00:32:16,690
So it's the transposed
convolution.

509
00:32:16,690 --> 00:32:20,510
It's the reverse of the
regular convolution.

510
00:32:20,510 --> 00:32:21,760
But why transposed?

511
00:32:21,760 --> 00:32:25,850
I would refer you to take a
look at the additional slides.

512
00:32:25,850 --> 00:32:28,640
So the question is,
is the filter trained?

513
00:32:28,640 --> 00:32:29,140
Yes.

514
00:32:29,140 --> 00:32:31,338
It's very much similar to
other convolution layers.

515
00:32:31,338 --> 00:32:32,630
All of the filters are trained.

516
00:32:32,630 --> 00:32:33,100
Yes.

517
00:32:33,100 --> 00:32:33,600
Yeah.

519
00:32:38,860 --> 00:32:40,120
Great.

520
00:32:40,120 --> 00:32:45,920
This was the topic of
semantic segmentation.

521
00:32:45,920 --> 00:32:51,860
And as we talked about this, we
only get labels for the pixels.

522
00:32:51,860 --> 00:32:55,570
But if there are two
instances of the same object,

523
00:32:55,570 --> 00:32:58,885
we have no idea
which one is which.

524
00:32:58,885 --> 00:33:07,960
Because this is just generating
or outputting the pixel labels.

525
00:33:07,960 --> 00:33:13,040
And that brings us to the
topic of instance segmentation,

526
00:33:13,040 --> 00:33:18,410
where now, we not only care
about the pixel classes,

527
00:33:18,410 --> 00:33:23,050
but also, I want you to know
that these pixels belong

528
00:33:23,050 --> 00:33:24,800
to one instance of the dog.

529
00:33:24,800 --> 00:33:29,442
And this next one is
actually a different dog.

530
00:33:29,442 --> 00:33:37,430
And for doing that, what
we need is understanding

531
00:33:37,430 --> 00:33:41,200
multiple objects in
the image and brings us

532
00:33:41,200 --> 00:33:43,430
to the topic of
object detection.

533
00:33:43,430 --> 00:33:49,030
Object detection has been
one of the, besides image

534
00:33:49,030 --> 00:33:51,280
classification or after
image classification,

535
00:33:51,280 --> 00:33:58,020
has been one of the core
Computer Vision problems

536
00:33:58,020 --> 00:33:59,350
and tasks.

537
00:33:59,350 --> 00:34:03,240
And for many years, many,
many different algorithms

538
00:34:03,240 --> 00:34:09,520
were proposed for just doing
the task of object detection.

539
00:34:09,520 --> 00:34:11,940
We are going to fly
over some of them

540
00:34:11,940 --> 00:34:15,040
and highlight a couple
of important ones.

541
00:34:15,040 --> 00:34:20,280
But again, there are so many
works in the literature,

542
00:34:20,280 --> 00:34:22,590
even in the literature
of deep learning

543
00:34:22,590 --> 00:34:30,239
that I am not covering here,
so in the past 10, 15 years.

544
00:34:30,239 --> 00:34:35,170
How we can do that and solve
the problem of object detection?

545
00:34:35,170 --> 00:34:40,330
If it's just a single object, it
means that we need to generate--

546
00:34:40,330 --> 00:34:42,810
we need to do the
classification,

547
00:34:42,810 --> 00:34:46,739
generate a label
class scores, as well

548
00:34:46,739 --> 00:34:50,350
as getting a bounding
box, coordinates of a box.

549
00:34:50,350 --> 00:34:52,650
So we need the
coordinates of the box

550
00:34:52,650 --> 00:34:58,360
x, y, and h and w as the output,
as well as what class it is.

551
00:34:58,360 --> 00:35:01,630
So this is exactly the
task of object detection.

552
00:35:01,630 --> 00:35:03,190
How can we solve this?

553
00:35:03,190 --> 00:35:04,440
It's very simple.

554
00:35:04,440 --> 00:35:11,410
We can define a softmax loss
function for the class scores.

555
00:35:11,410 --> 00:35:15,210
And we can define an
L2 loss function, which

556
00:35:15,210 --> 00:35:20,910
is a simple distance metric,
a regression loss for the box

557
00:35:20,910 --> 00:35:21,790
coordinates.

558
00:35:21,790 --> 00:35:27,580
And having these two defined,
we have a multi-task loss.

559
00:35:27,580 --> 00:35:30,640
We are solving two
tasks at the same time.

560
00:35:30,640 --> 00:35:34,110
And for doing that,
we, again, add

561
00:35:34,110 --> 00:35:40,420
the loss values and generate
a compound loss function,

562
00:35:40,420 --> 00:35:43,920
as you can see here.

563
00:35:43,920 --> 00:35:46,480
So this is simple.

564
00:35:46,480 --> 00:35:47,760
It's doable.

565
00:35:47,760 --> 00:35:54,780
If you have one single
object, you can for sure

566
00:35:54,780 --> 00:35:56,880
solve this problem
using this architecture

567
00:35:56,880 --> 00:35:58,060
that I talked about.

568
00:35:58,060 --> 00:36:00,420
But this is not that
easy, if you have

569
00:36:00,420 --> 00:36:02,620
multiple objects in the scene.

570
00:36:02,620 --> 00:36:07,270
So for three objects, you have
to generate 12 output numbers.

571
00:36:07,270 --> 00:36:09,540
And if there are
more, then it's going

572
00:36:09,540 --> 00:36:12,820
to be too many
numbers to generate.

573
00:36:12,820 --> 00:36:15,995
So this algorithm is
not really scalable.

574
00:36:15,995 --> 00:36:17,970
It's just extending
the classification

575
00:36:17,970 --> 00:36:20,560
into some object
detection, which is fine,

576
00:36:20,560 --> 00:36:23,040
but it's not really scalable.

577
00:36:23,040 --> 00:36:28,270
So when there are
multiple objects,

578
00:36:28,270 --> 00:36:35,310
one solution is instead of going
or getting the entire image

579
00:36:35,310 --> 00:36:38,970
as the input, why not to
look at bounding boxes.

580
00:36:38,970 --> 00:36:44,800
For each bounding box, we can
say we only have one label,

581
00:36:44,800 --> 00:36:49,470
and whether it's a cat or
a dog or the background.

582
00:36:49,470 --> 00:36:56,460
And if I have this
way of classifying

583
00:36:56,460 --> 00:36:59,140
each of the bounding boxes,
I can do a sliding window.

584
00:36:59,140 --> 00:37:01,060
I can create this
bounding boxes,

585
00:37:01,060 --> 00:37:06,300
slide it over the image
from coordinate 0, 0

586
00:37:06,300 --> 00:37:10,740
to all combination
of x, y, and h and w,

587
00:37:10,740 --> 00:37:12,670
and see if we can
detect the object.

588
00:37:12,670 --> 00:37:16,080
So step by step, I can create--

589
00:37:16,080 --> 00:37:18,090
I can find the
bounding boxes that I

590
00:37:18,090 --> 00:37:23,070
have the maximum probability
of each of the objects.

591
00:37:23,070 --> 00:37:26,140
But there is a
huge problem here.

592
00:37:26,140 --> 00:37:29,400
Again, there are so many
different combinations

593
00:37:29,400 --> 00:37:31,740
of bounding boxes
that you can use.

594
00:37:31,740 --> 00:37:35,820
And again, this algorithm
is not scalable.

595
00:37:35,820 --> 00:37:39,875
What we've been doing in
the literature, again,

596
00:37:39,875 --> 00:37:43,660
early years, if you
look at the years

597
00:37:43,660 --> 00:37:46,230
these articles were
published, 2014,

598
00:37:46,230 --> 00:37:52,300
and before, there has been a
lot of research around finding

599
00:37:52,300 --> 00:37:58,750
regions that they
have high probability

600
00:37:58,750 --> 00:38:00,560
of having the object in them.

601
00:38:00,560 --> 00:38:02,540
So region proposals.

602
00:38:02,540 --> 00:38:07,190
And if I have a way to
find region proposals,

603
00:38:07,190 --> 00:38:10,940
that's actually going to
be an easy-ish problem.

604
00:38:10,940 --> 00:38:14,740
I can do the same thing
as I explained earlier.

605
00:38:14,740 --> 00:38:19,490
For an image, what we can do
is if I have region proposals,

606
00:38:19,490 --> 00:38:23,800
I can just take that
part, that patch out

607
00:38:23,800 --> 00:38:29,620
and run a CNN on that patch,
a convolutional neural network

608
00:38:29,620 --> 00:38:31,820
on the patch, and
then classify it.

609
00:38:31,820 --> 00:38:33,460
And in order to--

610
00:38:33,460 --> 00:38:37,810
even I can refine
the bounding boxes.

611
00:38:37,810 --> 00:38:44,680
So classify and then
refine the bounding boxes

612
00:38:44,680 --> 00:38:48,620
to have the objects detected.

613
00:38:48,620 --> 00:38:51,610
So we can classify the
boxes and also refine

614
00:38:51,610 --> 00:38:53,470
the bounding boxes,
if I have to change

615
00:38:53,470 --> 00:38:55,840
the coordinates a little bit.

616
00:38:55,840 --> 00:39:01,450
And this is what is
called R-CNN algorithm.

617
00:39:01,450 --> 00:39:09,010
And although it works, and,
again, this is one of the early

618
00:39:09,010 --> 00:39:12,860
algorithms up to 2014.

619
00:39:12,860 --> 00:39:17,120
These are very slow because
for each of these boxes, again,

620
00:39:17,120 --> 00:39:20,920
we are running a full
convolutional neural network.

621
00:39:20,920 --> 00:39:26,500
But there is one catch
that what we can do

622
00:39:26,500 --> 00:39:34,780
is instead of running
convolutional neural network

623
00:39:34,780 --> 00:39:39,500
on each of these boxes,
because convolution operations,

624
00:39:39,500 --> 00:39:41,710
they preserve the
spatial information.

625
00:39:41,710 --> 00:39:45,020
They either downsample
or upsample.

626
00:39:45,020 --> 00:39:50,230
We always have a way to track
them where in the pixel space

627
00:39:50,230 --> 00:39:51,115
they are.

628
00:39:51,115 --> 00:39:56,380
So in that case, what we
do is instead of running

629
00:39:56,380 --> 00:40:02,920
the convolutional neural network
on the patches, let's say,

630
00:40:02,920 --> 00:40:06,680
we run one big convolution
on the entire image.

631
00:40:06,680 --> 00:40:09,460
And then now we have the
regions in that feature

632
00:40:09,460 --> 00:40:11,900
map, which is corresponding
to the entire image.

633
00:40:11,900 --> 00:40:13,990
Let's look at those regions.

634
00:40:13,990 --> 00:40:18,040
And now, run a smaller
CNN on top of those

635
00:40:18,040 --> 00:40:24,490
and generate the outputs for
each of these two outputs

636
00:40:24,490 --> 00:40:25,370
that I want.

637
00:40:25,370 --> 00:40:27,580
First, the box offset.

638
00:40:27,580 --> 00:40:30,010
Should I move the
bounding box a little bit?

639
00:40:30,010 --> 00:40:34,070
Or what's the
object category is?

640
00:40:34,070 --> 00:40:37,700
So this is the fast
version of R-CNN

641
00:40:37,700 --> 00:40:40,600
These are some basic
algorithms that we

642
00:40:40,600 --> 00:40:44,570
can use convolutional neural
networks for detecting objects,

643
00:40:44,570 --> 00:40:46,390
their bounding boxes, and so on.

644
00:40:46,390 --> 00:40:49,480
The question is, if the
number of proposed regions

645
00:40:49,480 --> 00:40:51,760
are predefined?

646
00:40:51,760 --> 00:40:53,720
The short answer to that is yes.

647
00:40:53,720 --> 00:40:57,360
I will talk very briefly about
the region proposal networks do.

649
00:41:00,550 --> 00:41:03,260
So easy algorithms.

650
00:41:03,260 --> 00:41:07,360
One, puts the bounding boxes
of the regions, the proposed

651
00:41:07,360 --> 00:41:08,530
regions.

652
00:41:08,530 --> 00:41:12,650
Under the images, one puts it on
the feature maps of the ConvNet.

653
00:41:12,650 --> 00:41:16,460
And both of those generate
the output class label,

654
00:41:16,460 --> 00:41:21,160
as well as offset of
improving the location

655
00:41:21,160 --> 00:41:22,960
of the detected object.

656
00:41:22,960 --> 00:41:26,620
But this will
mean, this requires

657
00:41:26,620 --> 00:41:35,000
us to do that bounding
box region proposal first,

658
00:41:35,000 --> 00:41:37,720
and have a region proposal
network that tells us

659
00:41:37,720 --> 00:41:40,970
where in the image
we should look for.

660
00:41:40,970 --> 00:41:46,480
And there has been research
on building region proposal

661
00:41:46,480 --> 00:41:47,980
networks, RPNs.

662
00:41:47,980 --> 00:41:53,900
And here, what we do is we
just randomly start with a CNN.

663
00:41:53,900 --> 00:41:59,200
We try to randomly start
in different locations

664
00:41:59,200 --> 00:42:00,130
in the image.

665
00:42:00,130 --> 00:42:03,250
And through layers
of convolution,

666
00:42:03,250 --> 00:42:09,370
we refine those regions where
they have the higher probability

667
00:42:09,370 --> 00:42:12,190
of having an object in them,
because we have the object

668
00:42:12,190 --> 00:42:14,600
labels and locations.

669
00:42:14,600 --> 00:42:17,210
So we can optimize,
we can supervise this.

670
00:42:17,210 --> 00:42:21,500
And then each of those also
refine the box coordinates.

671
00:42:21,500 --> 00:42:26,920
So basically, a region
proposal network, what it does

672
00:42:26,920 --> 00:42:32,770
is it refines the boxes,
each of those boxes that

673
00:42:32,770 --> 00:42:36,010
have a probability, a high
probability of an object

674
00:42:36,010 --> 00:42:42,160
in them, the output boxes.

675
00:42:42,160 --> 00:42:43,340
The box corrections.

676
00:42:43,340 --> 00:42:47,260
Again, I'm leaving all of the
details about the coordinates

677
00:42:47,260 --> 00:42:50,110
and all of these
dimensionalities

678
00:42:50,110 --> 00:42:52,930
for you to check
afterwards, because they

679
00:42:52,930 --> 00:42:54,200
will take too much time.

680
00:42:54,200 --> 00:42:57,140
And we don't want to spend too
much time on this algorithm.

681
00:42:57,140 --> 00:43:01,160
But what's important here
is back to your question,

682
00:43:01,160 --> 00:43:05,560
we often take the
top K, the ones

683
00:43:05,560 --> 00:43:08,380
that have the
highest probability

684
00:43:08,380 --> 00:43:12,320
of having an object in them as
the proposals for this image.

685
00:43:12,320 --> 00:43:18,230
This is a simple image
and only has one object.

686
00:43:18,230 --> 00:43:21,730
So most of the
regions are centered

687
00:43:21,730 --> 00:43:23,480
around that single object.

688
00:43:23,480 --> 00:43:25,280
But, in general,
that's not the case.

689
00:43:25,280 --> 00:43:29,590
So in many setups, we
can have region proposals

690
00:43:29,590 --> 00:43:33,640
used in different setups.

691
00:43:33,640 --> 00:43:37,870
We can get different objects
with higher probabilities.

692
00:43:37,870 --> 00:43:42,040
With that, and after
talking a little bit

693
00:43:42,040 --> 00:43:46,970
about R-CNN and mask R-CNN,
which, again, for you,

694
00:43:46,970 --> 00:43:49,940
it's important to go
through the details.

695
00:43:49,940 --> 00:43:55,640
And if you can spend some time
doing the calculations yourself,

696
00:43:55,640 --> 00:43:57,410
that would be very good.

697
00:43:57,410 --> 00:44:01,110
But those types of algorithms,
R-CNN, mask R-CNN, they

698
00:44:01,110 --> 00:44:04,570
are not being used anymore
these days because they

699
00:44:04,570 --> 00:44:07,300
are very heavy computationally.

700
00:44:07,300 --> 00:44:09,700
Although it's important
to understand how

701
00:44:09,700 --> 00:44:11,960
we got to this point.

702
00:44:11,960 --> 00:44:15,860
But those are for many reasons.

703
00:44:15,860 --> 00:44:19,510
One of those reasons is that we
need two separate networks, one

704
00:44:19,510 --> 00:44:25,180
region proposal network, and
then one classification and box

705
00:44:25,180 --> 00:44:26,480
refinement network.

706
00:44:26,480 --> 00:44:32,176
So it's like at least two
passes for detecting objects

707
00:44:32,176 --> 00:44:35,080
for each image.

708
00:44:35,080 --> 00:44:38,950
That's why there
has been advances

709
00:44:38,950 --> 00:44:45,800
after using single stage,
object detectors, SSDs,

710
00:44:45,800 --> 00:44:50,080
and one of the most popular
ones is called YOLO.

711
00:44:50,080 --> 00:44:55,090
YOLO is probably, if you
work with any Computer Vision

712
00:44:55,090 --> 00:44:58,880
problem, you've heard
about YOLO even to date.

713
00:44:58,880 --> 00:45:04,090
Although it's a convolution
heavy network today, at least

714
00:45:04,090 --> 00:45:07,450
its earlier versions.

715
00:45:07,450 --> 00:45:10,520
In many even industrial
applications,

716
00:45:10,520 --> 00:45:16,240
YOLO is being used as the
base for object detection

717
00:45:16,240 --> 00:45:19,220
because it's a fast
object detector.

718
00:45:19,220 --> 00:45:25,100
And it's very good in terms
of detecting the objects.

719
00:45:25,100 --> 00:45:30,040
What YOLO does I want
to very briefly tell you

720
00:45:30,040 --> 00:45:31,130
a little bit about.

721
00:45:31,130 --> 00:45:34,270
It's basically you look
only once in one single pass

722
00:45:34,270 --> 00:45:35,150
on the image.

723
00:45:35,150 --> 00:45:38,750
You generate all of
the bounding boxes.

724
00:45:38,750 --> 00:45:48,640
How it does it is it maps, it
divides the image into grid of S

725
00:45:48,640 --> 00:45:52,430
by S And in this
example, it's 7 by 7.

726
00:45:52,430 --> 00:45:56,930
What happens is that for
each single box in that grid,

727
00:45:56,930 --> 00:46:02,260
it tries to output, it creates
a fully convolutional network

728
00:46:02,260 --> 00:46:07,660
that outputs the probability
of an object being

729
00:46:07,660 --> 00:46:12,770
in that location, refinements
of the bounding boxes.

730
00:46:12,770 --> 00:46:16,510
So it generates B bounding
boxes, a new hyperparameters,

731
00:46:16,510 --> 00:46:20,740
bounding B boxes, that is the
refinement of the object that

732
00:46:20,740 --> 00:46:22,370
is present in that box.

733
00:46:22,370 --> 00:46:27,640
And also, it generates the
class probability, object class

734
00:46:27,640 --> 00:46:28,760
probabilities.

735
00:46:28,760 --> 00:46:33,560
And in this case, for
example, if it's B equal to 2,

736
00:46:33,560 --> 00:46:35,860
it generates just
two bounding boxes

737
00:46:35,860 --> 00:46:38,380
with different probabilities.

738
00:46:38,380 --> 00:46:43,580
It does this for all of
the boxes at the same time.

739
00:46:43,580 --> 00:46:45,910
So basically, it's
the same network

740
00:46:45,910 --> 00:46:49,900
that is generating something
as the output for each

741
00:46:49,900 --> 00:46:52,790
of these bounding boxes.

742
00:46:52,790 --> 00:46:59,662
And it does generate a
number of different options

743
00:46:59,662 --> 00:47:00,950
for the object.

744
00:47:00,950 --> 00:47:03,490
And as I said,
each of those boxes

745
00:47:03,490 --> 00:47:05,660
are associated
with a probability.

746
00:47:05,660 --> 00:47:07,600
And in this example,
the probability

747
00:47:07,600 --> 00:47:13,640
is shown with the weights of the
edges in each of those boxes.

748
00:47:13,640 --> 00:47:17,470
And for these many different
bounding boxes and object

749
00:47:17,470 --> 00:47:21,050
probabilities, now we
can do thresholding.

750
00:47:21,050 --> 00:47:29,300
And also, there is an algorithm
that they use in the paper.

751
00:47:29,300 --> 00:47:31,750
Again, I don't want to
go into the details--

752
00:47:31,750 --> 00:47:37,570
non-maximum suppression,
and some algorithms

753
00:47:37,570 --> 00:47:41,800
with thresholding involved
that identifies the ones that

754
00:47:41,800 --> 00:47:44,450
have the highest probabilities.

755
00:47:44,450 --> 00:47:48,460
So this is a simple
implementation or use

756
00:47:48,460 --> 00:47:55,550
of the object detection.

757
00:47:55,550 --> 00:47:58,930
Again, this is
something very useful.

758
00:47:58,930 --> 00:48:02,360
If you have time, to spend
with the repositories of YOLO.

759
00:48:02,360 --> 00:48:05,800
There are so many different
newer versions of YOLO

760
00:48:05,800 --> 00:48:10,730
that is being used for many
applications in medicine,

761
00:48:10,730 --> 00:48:13,490
robotics, and also in many
industrial applications.

762
00:48:13,490 --> 00:48:16,900
So the question is, how do
we get this second image?

763
00:48:16,900 --> 00:48:18,870
And what's the
intuition behind it?

764
00:48:18,870 --> 00:48:20,680
As I said, for
each of the grids,

765
00:48:20,680 --> 00:48:23,560
we generate bounding boxes.

766
00:48:23,560 --> 00:48:27,310
For this one, we generated
two, for all others,

767
00:48:27,310 --> 00:48:28,900
we also generate two.

768
00:48:28,900 --> 00:48:32,980
This bee is, again, a
probability vector/ and each

769
00:48:32,980 --> 00:48:36,670
of these boxes are associated
with probability of existing

770
00:48:36,670 --> 00:48:37,970
an object in them.

771
00:48:37,970 --> 00:48:41,500
And then if I put all of them
together for all of the patches,

772
00:48:41,500 --> 00:48:43,390
I have so many boxes.

773
00:48:43,390 --> 00:48:47,300
And now each of those are
associated with a probability.

775
00:48:51,654 --> 00:48:54,765
Perfect

777
00:48:55,630 --> 00:48:56,595
Let's move on.

779
00:49:00,340 --> 00:49:05,980
And one of the more recent
approaches for object detection

780
00:49:05,980 --> 00:49:09,500
is deter a detection
transformer.

781
00:49:09,500 --> 00:49:15,070
This is purely based on
transformers and the topic

782
00:49:15,070 --> 00:49:17,240
that we discussed last week.

783
00:49:17,240 --> 00:49:21,100
And also I started
today, same type

784
00:49:21,100 --> 00:49:25,390
of self-attention and
cross-attention modules

785
00:49:25,390 --> 00:49:30,880
could also generate some
object detections and bounding

786
00:49:30,880 --> 00:49:32,080
boxes for us.

787
00:49:32,080 --> 00:49:33,920
And how this works?

788
00:49:33,920 --> 00:49:36,640
This is actually not
a very old paper,

789
00:49:36,640 --> 00:49:39,230
2020, almost five years ago.

790
00:49:39,230 --> 00:49:41,930
Although it's now deprecated.

791
00:49:41,930 --> 00:49:45,430
Nobody uses this for
real applications.

792
00:49:45,430 --> 00:49:48,340
But it's a very
good example of how

793
00:49:48,340 --> 00:49:51,440
to use transformers
for object detection.

794
00:49:51,440 --> 00:49:55,600
And what we do here is
basically similar to what

795
00:49:55,600 --> 00:49:57,260
we've explained earlier.

796
00:49:57,260 --> 00:50:01,280
We may turn the
image into patches.

797
00:50:01,280 --> 00:50:04,480
And then those patches
are passed through CNNs

798
00:50:04,480 --> 00:50:06,530
creating a token.

799
00:50:06,530 --> 00:50:09,490
Then we add positional
encoding the same way

800
00:50:09,490 --> 00:50:12,710
that I explained to the patches.

801
00:50:12,710 --> 00:50:17,260
And those define our tokens
for the inputs, which

802
00:50:17,260 --> 00:50:22,250
are inputs to the transformer
encoder, a transformer encoder,

803
00:50:22,250 --> 00:50:25,570
again, a bunch of
self-attention layer

804
00:50:25,570 --> 00:50:28,040
normalization or
any normalization,

805
00:50:28,040 --> 00:50:31,720
as well as MLP
layers that generates

806
00:50:31,720 --> 00:50:38,860
the output tokens after multiple
layers of transformer encoder.

807
00:50:38,860 --> 00:50:42,950
Then, in order to generate
the bounding boxes,

808
00:50:42,950 --> 00:50:45,220
this is the smart part
for this algorithm

809
00:50:45,220 --> 00:50:49,900
that it does take the
encoder output tokens

810
00:50:49,900 --> 00:50:52,640
as input in the
transformer decoder,

811
00:50:52,640 --> 00:50:57,250
but we also define
some queries, which

812
00:50:57,250 --> 00:50:59,800
are trainable
parameters themselves

813
00:50:59,800 --> 00:51:05,830
that each of those, for example,
if I add five 5 queries as input

814
00:51:05,830 --> 00:51:08,900
for queries as input, or
10 or 20 queries as input,

815
00:51:08,900 --> 00:51:14,840
I'm seeking up to 20 objects
to be detected in that image.

816
00:51:14,840 --> 00:51:17,500
And then again,
through a combination

817
00:51:17,500 --> 00:51:24,230
of self-attention layers in the
beginning of this transformer

818
00:51:24,230 --> 00:51:30,860
decoder, as well as
cross-attention with the encoder

819
00:51:30,860 --> 00:51:32,780
output.

820
00:51:32,780 --> 00:51:39,235
So cross-attention and
self-attention networks layers,

821
00:51:39,235 --> 00:51:44,600
it generates the output values
for each of these queries, which

822
00:51:44,600 --> 00:51:48,980
are passed through an
FNN, feed forward network,

823
00:51:48,980 --> 00:51:56,670
to generate either class
labels and the bounding boxes,

824
00:51:56,670 --> 00:52:01,760
very similar to what we
discussed earlier, or even

825
00:52:01,760 --> 00:52:05,150
in some cases, it just
says there is no object

826
00:52:05,150 --> 00:52:07,495
to be detected.

827
00:52:07,495 --> 00:52:10,640
And at the end, we
have the bounding boxes

828
00:52:10,640 --> 00:52:16,190
and the classes associated with
bounding boxes as the output.

829
00:52:16,190 --> 00:52:19,428
So the question is, are we
inputting every possible box

830
00:52:19,428 --> 00:52:20,220
to the transformer?

831
00:52:20,220 --> 00:52:20,930
No.

832
00:52:20,930 --> 00:52:23,500
The input here are some
triangular parameters

833
00:52:23,500 --> 00:52:30,370
that are representing--
they are queries asking

834
00:52:30,370 --> 00:52:34,345
the question that I actually
want an object to be outwitted

835
00:52:34,345 --> 00:52:37,280
in place of this input query.

836
00:52:37,280 --> 00:52:40,840
So there is no box or
anything as the input.

837
00:52:40,840 --> 00:52:43,480
It's part of the output
that it generates the class

838
00:52:43,480 --> 00:52:46,780
label and the box coordinates.

839
00:52:46,780 --> 00:52:51,520
So the question is, if the
queries are formed in a way

840
00:52:51,520 --> 00:52:56,740
that it actually represents what
we want to look for and where

841
00:52:56,740 --> 00:52:58,480
in the image.

842
00:52:58,480 --> 00:53:02,410
In this case, what
we are looking for

843
00:53:02,410 --> 00:53:06,670
is defined by class labels,
which are predefined,

844
00:53:06,670 --> 00:53:08,810
and they are as
part of the output.

845
00:53:08,810 --> 00:53:10,970
So our supervision is
based on the class labels.

846
00:53:10,970 --> 00:53:13,140
We have a class
probability vector.

847
00:53:13,140 --> 00:53:15,920
Same way that we defined it
for the other algorithms.

848
00:53:15,920 --> 00:53:20,650
So that's how the algorithm
knows what type of classes

849
00:53:20,650 --> 00:53:21,650
to look for.

850
00:53:21,650 --> 00:53:25,390
And then in terms
of the outputs,

851
00:53:25,390 --> 00:53:28,310
again, these outputs
are supervised.

852
00:53:28,310 --> 00:53:31,090
If you remember
based on the L2 norm,

853
00:53:31,090 --> 00:53:35,110
L2 loss of the
ground truth boxes.

854
00:53:35,110 --> 00:53:41,500
We're not telling anything in
the query part what to or where

855
00:53:41,500 --> 00:53:45,740
to look for any of the objects.

856
00:53:45,740 --> 00:53:49,450
The training, the process
itself is back propagating.

857
00:53:49,450 --> 00:53:52,310
If there are any
losses, any errors,

858
00:53:52,310 --> 00:53:55,640
it back propagates the outputs.

859
00:53:55,640 --> 00:54:00,340
So basically, we are
not determining anything

860
00:54:00,340 --> 00:54:02,920
in the beginning
or in this part.

861
00:54:02,920 --> 00:54:09,040
So the question was the query
is give me up to nine objects.

862
00:54:09,040 --> 00:54:14,510
And yes that's basically
what this means.

863
00:54:14,510 --> 00:54:19,010
And through the self-attention
and cross-attention,

864
00:54:19,010 --> 00:54:22,600
it will try to generate
output tokens that

865
00:54:22,600 --> 00:54:25,960
are turned into class
and box coordinates

866
00:54:25,960 --> 00:54:29,380
through that FNN operation.

867
00:54:29,380 --> 00:54:33,640
Your question is, if
the query is over there,

868
00:54:33,640 --> 00:54:35,900
if they are image
patches or not?

869
00:54:35,900 --> 00:54:38,480
No, they are not image patches.

870
00:54:38,480 --> 00:54:44,270
They are just queries
for trainable parameters.

871
00:54:44,270 --> 00:54:47,200
You put them in to
generate the outputs.

872
00:54:47,200 --> 00:54:51,850
For each of the inputs, you
get the value as the output.

873
00:54:51,850 --> 00:54:55,810
And that value is turned into
class and box coordinates.

874
00:54:55,810 --> 00:54:58,130
Again, the question is,
what is object queries?

875
00:54:58,130 --> 00:55:00,980
They are trainable
learnable parameters.

876
00:55:00,980 --> 00:55:02,980
So you initialize them.

877
00:55:02,980 --> 00:55:06,550
The network finds the
best values for them.

878
00:55:06,550 --> 00:55:09,010
And that's what you
get as the output.

879
00:55:09,010 --> 00:55:12,040
The question is, if
there is any intuition,

880
00:55:12,040 --> 00:55:14,710
which FNN gets which box?

881
00:55:14,710 --> 00:55:19,720
The short answer to that
is, no, there's nothing

882
00:55:19,720 --> 00:55:22,280
that stops the network from--

883
00:55:22,280 --> 00:55:24,940
I mean, we're not
including anything

884
00:55:24,940 --> 00:55:28,430
that stops the network
from generating multiple.

885
00:55:28,430 --> 00:55:32,320
But remember, that there
are so many self-attention

886
00:55:32,320 --> 00:55:34,930
and cross-attention
layers over there

887
00:55:34,930 --> 00:55:38,875
that they are actually
interacting with each other

888
00:55:38,875 --> 00:55:41,380
and makes each of
those queries match

889
00:55:41,380 --> 00:55:43,730
with one of the output layer.

890
00:55:43,730 --> 00:55:46,880
So it's not generating the
exact same thing as the output.

891
00:55:46,880 --> 00:55:52,930
And we also have control over
supervising those FNNs as well.

892
00:55:52,930 --> 00:55:57,580
So your question is if there are
image segmentations, pixel level

893
00:55:57,580 --> 00:56:00,490
segmentations as
part of the training.

894
00:56:00,490 --> 00:56:08,040
This algorithm does not require
the pixel level segmentations.

895
00:56:08,040 --> 00:56:11,930
It's only supervised based on
class labels and bounding boxes.

896
00:56:11,930 --> 00:56:15,410
But if you have the pixel
level segmentations,

897
00:56:15,410 --> 00:56:17,560
you can always turn the
pixel level segmentations

898
00:56:17,560 --> 00:56:20,980
into a bounding box to
train this algorithm,

899
00:56:20,980 --> 00:56:24,130
but it doesn't
necessarily require that.

900
00:56:24,130 --> 00:56:26,860
So the question is, if
it's possible to generalize

901
00:56:26,860 --> 00:56:29,890
unseen objects.

902
00:56:29,890 --> 00:56:32,545
And by unseen you mean
a new class label?

903
00:56:32,545 --> 00:56:34,360
Yes.

904
00:56:34,360 --> 00:56:39,170
For these types of algorithms
that they are fully supervised,

905
00:56:39,170 --> 00:56:41,320
often, there is no way
because you are creating

906
00:56:41,320 --> 00:56:43,040
class probability vector.

907
00:56:43,040 --> 00:56:45,100
There is no way of
adding something

908
00:56:45,100 --> 00:56:52,225
at the end for a new class
without previously knowing

909
00:56:52,225 --> 00:56:53,810
there is some other classes.

910
00:56:53,810 --> 00:56:58,010
So fully supervised networks,
often, there is no new object.

911
00:56:58,010 --> 00:57:00,320
We can have a background
object or no object.

912
00:57:00,320 --> 00:57:03,080
As you can see, we have
the label of no object.

913
00:57:03,080 --> 00:57:07,030
But there are many algorithms
and mixed extensions

914
00:57:07,030 --> 00:57:08,620
of these types of
algorithms that are

915
00:57:08,620 --> 00:57:10,930
used for zero-shot learning.

916
00:57:10,930 --> 00:57:13,990
Zero shot means understanding
finding something new

917
00:57:13,990 --> 00:57:16,940
without having an example of
those in the training data.

918
00:57:16,940 --> 00:57:19,640
But it's beyond this topic.

919
00:57:19,640 --> 00:57:25,150
What happens if you have more
objects in the scene than what

920
00:57:25,150 --> 00:57:27,910
you put in as the query?

921
00:57:27,910 --> 00:57:29,900
So that's a great question.

922
00:57:29,900 --> 00:57:34,300
It often generates the ones
that has the highest confidence

923
00:57:34,300 --> 00:57:36,490
on the objects,
so bounding boxes

924
00:57:36,490 --> 00:57:38,030
with the highest confidence.

925
00:57:38,030 --> 00:57:40,240
And in those cases,
you often want

926
00:57:40,240 --> 00:57:44,220
to add more queries just so
you can get more objects.

928
00:57:47,050 --> 00:57:48,580
I'll be here to
answer questions,

929
00:57:48,580 --> 00:57:50,840
if you have any after the class.

930
00:57:50,840 --> 00:57:54,890
But we have a bunch of
other topics to cover,

931
00:57:54,890 --> 00:57:57,080
and I want to make
sure we go over them.

932
00:57:57,080 --> 00:58:01,250
At least you get
familiar with the topics.

933
00:58:01,250 --> 00:58:04,750
So with the object
detections, now back

934
00:58:04,750 --> 00:58:07,250
to the question I
was asked earlier,

935
00:58:07,250 --> 00:58:10,240
how can we use those
types of algorithms

936
00:58:10,240 --> 00:58:13,130
for instance segmentation?

937
00:58:13,130 --> 00:58:15,140
And that's actually
not too hard.

938
00:58:15,140 --> 00:58:18,640
We talked about
this when we were

939
00:58:18,640 --> 00:58:24,440
talking about R-CNN algorithms,
where we run a CNN on the image.

940
00:58:24,440 --> 00:58:27,430
Then we have a region
proposal network that

941
00:58:27,430 --> 00:58:29,450
gives us the bounding boxes.

942
00:58:29,450 --> 00:58:35,560
And those bounding boxes are
turned into either class labels

943
00:58:35,560 --> 00:58:38,750
and bounding box refinements.

944
00:58:38,750 --> 00:58:43,970
So this is what we've talked
so far with R-CNN and so on.

945
00:58:43,970 --> 00:58:46,840
Now, we can turn this
into a mask R-CNN

946
00:58:46,840 --> 00:58:49,960
that also generates the mask.

947
00:58:49,960 --> 00:58:54,680
So basically, same architecture
that we talked about earlier.

948
00:58:54,680 --> 00:59:01,450
Now, we can take one more
output, make it more multitask

949
00:59:01,450 --> 00:59:04,700
and generate the
mask predictions.

950
00:59:04,700 --> 00:59:07,570
So what we used to
be doing before was,

951
00:59:07,570 --> 00:59:12,880
again, image, region proposals,
CNN, gives us class label

952
00:59:12,880 --> 00:59:14,840
and the box coordinates.

953
00:59:14,840 --> 00:59:18,910
Now, we add another
layer of convolution

954
00:59:18,910 --> 00:59:26,360
that generates the masks for
that object in the pixel level.

955
00:59:26,360 --> 00:59:28,990
And that mask, again,
could be the same size

956
00:59:28,990 --> 00:59:35,450
as the input and the image, and,
basically, on the layer itself.

957
00:59:35,450 --> 00:59:37,580
If we use fully convolutional
neural networks,

958
00:59:37,580 --> 00:59:40,000
that's what we often
get as the output.

959
00:59:40,000 --> 00:59:44,390
For each of the objects,
when we have that tiny box,

960
00:59:44,390 --> 00:59:47,630
we can always get
the mask for that.

961
00:59:47,630 --> 00:59:52,430
The chair in different
settings of the box itself,

962
00:59:52,430 --> 00:59:58,460
if you have different boxes,
the bed and the human,

963
00:59:58,460 --> 01:00:00,740
the baby in the image.

964
01:00:00,740 --> 01:00:04,700
And this is an extension
of the R-CNN algorithm,

965
01:00:04,700 --> 01:00:07,420
which we call mask R-CNN.

966
01:00:07,420 --> 01:00:14,380
And with mask R-CNN, the results
have been very, very good

967
01:00:14,380 --> 01:00:21,850
in detecting different objects,
different known objects that we

968
01:00:21,850 --> 01:00:23,780
could train the algorithms for.

969
01:00:23,780 --> 01:00:29,920
And then there are so many
APIs and open source versions

970
01:00:29,920 --> 01:00:34,670
of object detectors
that you can explore.

971
01:00:34,670 --> 01:00:37,460
There are some links
and resources here,

972
01:00:37,460 --> 01:00:40,870
but this all,
basically, rounds up

973
01:00:40,870 --> 01:00:45,727
and summarizes some of the
tasks that we wanted to cover.

974
01:00:45,727 --> 01:00:47,560
And they are actually
very important for you

975
01:00:47,560 --> 01:00:49,190
to understand these tasks.

976
01:00:49,190 --> 01:00:52,400
They have been core
Computer Vision tasks.

977
01:00:52,400 --> 01:00:56,570
Although these days, Computer
Vision is very more advanced.

978
01:00:56,570 --> 01:00:59,270
They are not bound
to these tasks.

979
01:00:59,270 --> 01:01:03,430
But if you have industrial
applications, for example,

980
01:01:03,430 --> 01:01:09,310
quality control of
separating rotten tomatoes

981
01:01:09,310 --> 01:01:12,380
and good tomatoes in
an industrial pipeline.

982
01:01:12,380 --> 01:01:14,050
Then with Computer
Vision, you need

983
01:01:14,050 --> 01:01:15,880
to be able to detect
objects and then

984
01:01:15,880 --> 01:01:19,780
classify them into good or bad.

985
01:01:19,780 --> 01:01:22,540
That's why it's important
to still understand and know

986
01:01:22,540 --> 01:01:26,510
these steps and pipelines and
how to do them in real time.

987
01:01:26,510 --> 01:01:29,170
But now, there are
larger scale models

988
01:01:29,170 --> 01:01:31,660
that you are all familiar with.

989
01:01:31,660 --> 01:01:36,340
This summarizes the first
part, the Computer Vision tasks

990
01:01:36,340 --> 01:01:37,610
that I wanted to talk about.

991
01:01:37,610 --> 01:01:40,480
And the last piece that I
want to spend 10 minutes on

992
01:01:40,480 --> 01:01:46,330
is around visualization
and understanding.

993
01:01:46,330 --> 01:01:50,320
Again, this has been a
big lecture by itself.

994
01:01:50,320 --> 01:01:57,040
And in 2050, 60, until 2020s
that the topic of computer

995
01:01:57,040 --> 01:02:01,810
and even before that 2014, '13
the topic of visualizing neural

996
01:02:01,810 --> 01:02:06,755
networks has been very
hot and very much--

998
01:02:09,840 --> 01:02:12,560
it helped us gain
understanding into what

999
01:02:12,560 --> 01:02:14,040
the networks are learning.

1000
01:02:14,040 --> 01:02:16,500
And I'm going to
summarize some of those,

1001
01:02:16,500 --> 01:02:20,285
the most important ones
here that you may need

1002
01:02:20,285 --> 01:02:22,470
to use in your applications.

1003
01:02:22,470 --> 01:02:26,660
But before that, let me go
back to the linear classifier

1004
01:02:26,660 --> 01:02:28,760
that we talked about.

1005
01:02:28,760 --> 01:02:33,240
We spent quite a lot of
time on linear classifiers.

1006
01:02:33,240 --> 01:02:37,710
And with the linear classifiers,
what we did was at the end,

1007
01:02:37,710 --> 01:02:43,100
we said, if I look at the linear
function, what the network is

1008
01:02:43,100 --> 01:02:46,472
learning, I can have a template
for each of those classes.

1009
01:02:46,472 --> 01:02:47,930
Like for example,
for this car, you

1010
01:02:47,930 --> 01:02:52,310
can always see a front
facing car as a template.

1011
01:02:52,310 --> 01:02:55,100
We can do the same
with neural networks.

1012
01:02:55,100 --> 01:02:57,630
If I visualize one
of the filters,

1013
01:02:57,630 --> 01:03:01,740
so here, we visualize the
weights of the linear function.

1014
01:03:01,740 --> 01:03:03,300
It was the visual viewpoint.

1015
01:03:03,300 --> 01:03:06,140
I can do the same with linear--

1016
01:03:06,140 --> 01:03:11,700
sorry with visualizing the
filters in the neural networks.

1017
01:03:11,700 --> 01:03:15,080
So for each of the
filters, the network

1018
01:03:15,080 --> 01:03:20,510
is, for example, is learning
something that is basically

1019
01:03:20,510 --> 01:03:25,980
some of the basic shapes,
orientations, or simple shapes,

1020
01:03:25,980 --> 01:03:28,040
as you can see here.

1021
01:03:28,040 --> 01:03:32,720
Although this visualization, we
can only do it for the layers

1022
01:03:32,720 --> 01:03:33,950
that we have few channels.

1023
01:03:33,950 --> 01:03:36,150
Like, for example, if
we have three channels,

1024
01:03:36,150 --> 01:03:39,330
I can put them in an RGB
image and just visualize it.

1025
01:03:39,330 --> 01:03:42,690
But as you remember, in
CNNs, that was not the case.

1026
01:03:42,690 --> 01:03:50,420
In CNNs, we had different
sometimes quite a few channels

1027
01:03:50,420 --> 01:03:53,660
in the middle layer, so it's
not easy to visualize those

1028
01:03:53,660 --> 01:03:55,050
in something that we can see.

1029
01:03:55,050 --> 01:03:57,770
But that's what
you basically see.

1030
01:03:57,770 --> 01:04:00,570
In earlier layers that
we have fewer channels,

1031
01:04:00,570 --> 01:04:03,590
we can visualize them and
see the network is actually

1032
01:04:03,590 --> 01:04:04,800
learning some patterns.

1033
01:04:04,800 --> 01:04:06,920
So it starts learning patterns.

1034
01:04:06,920 --> 01:04:14,960
And then later stages, it
gets more holistic and bigger

1035
01:04:14,960 --> 01:04:21,200
patterns as if we train--

1036
01:04:21,200 --> 01:04:26,130
sorry, if we run something that
we call guided back propagation,

1037
01:04:26,130 --> 01:04:30,560
we can also visualize those,
but not as simple as this.

1038
01:04:30,560 --> 01:04:34,310
I want to highlight
a couple of ways

1039
01:04:34,310 --> 01:04:39,350
of understanding and visualizing
the neural networks, which

1040
01:04:39,350 --> 01:04:41,190
are actually important.

1041
01:04:41,190 --> 01:04:47,060
One is the concept of saliency.

1042
01:04:47,060 --> 01:04:50,330
So in many applications,
it's very important for you

1043
01:04:50,330 --> 01:04:53,070
to know which pixel matters.

1044
01:04:53,070 --> 01:04:55,200
For example, in a
medical application,

1045
01:04:55,200 --> 01:04:58,890
when you do a classification
of tumor versus none,

1046
01:04:58,890 --> 01:05:01,550
you want to see which
parts of that image

1047
01:05:01,550 --> 01:05:03,540
is actually the tumor.

1048
01:05:03,540 --> 01:05:05,850
Because if you want
to automate this,

1049
01:05:05,850 --> 01:05:09,090
nobody cares about knowing
if there is tumor or not.

1050
01:05:09,090 --> 01:05:12,500
Everybody cares about where
in the image the tumor is.

1051
01:05:12,500 --> 01:05:17,210
So in order to do that,
simplest application

1052
01:05:17,210 --> 01:05:21,890
is we train a network, a
feedforward neural network that

1053
01:05:21,890 --> 01:05:26,040
generates the value or
the class label doc.

1054
01:05:26,040 --> 01:05:31,940
But what we can do is we can--

1055
01:05:31,940 --> 01:05:35,670
actually, before I showed
you that, in this case,

1056
01:05:35,670 --> 01:05:38,450
in order to train this
network, what we've done was

1057
01:05:38,450 --> 01:05:45,660
we always took the derivative of
the neural network, the weights,

1058
01:05:45,660 --> 01:05:50,000
sorry, of the loss or of
the class score with respect

1059
01:05:50,000 --> 01:05:53,204
to the weights in order
to update the weights.

1060
01:05:53,204 --> 01:05:56,340
Now, what I need
is for each pixel,

1061
01:05:56,340 --> 01:06:03,380
I want to see how much
changing the pixel value

1062
01:06:03,380 --> 01:06:06,740
would affect the dog score?

1063
01:06:06,740 --> 01:06:07,830
What does this mean?

1064
01:06:07,830 --> 01:06:12,950
What I explained is the
meaning of the variation.

1065
01:06:12,950 --> 01:06:19,080
So this means that the
meaning of basically gradient.

1066
01:06:19,080 --> 01:06:23,000
So if I take the gradient
of the score with respect

1067
01:06:23,000 --> 01:06:26,000
to now the pixel values,
not the network weights

1068
01:06:26,000 --> 01:06:30,680
anymore, with pixel values, I
can visualize those gradients.

1069
01:06:30,680 --> 01:06:33,650
And visualizing those
means that these

1070
01:06:33,650 --> 01:06:36,000
are the pixels that matter.

1071
01:06:36,000 --> 01:06:40,020
In order to classify
dog on this image,

1072
01:06:40,020 --> 01:06:41,700
those are the
pixels that matter.

1073
01:06:41,700 --> 01:06:44,840
So if I change the
values of those pixels,

1074
01:06:44,840 --> 01:06:48,590
the dog score will be changed.

1075
01:06:48,590 --> 01:06:52,280
Again, this is the basic meaning
and definition of gradients

1076
01:06:52,280 --> 01:06:53,970
that we've talked about.

1077
01:06:53,970 --> 01:06:56,160
So this is one way.

1078
01:06:56,160 --> 01:06:59,630
If you run this on
different objects

1079
01:06:59,630 --> 01:07:02,700
that you've trained
in the network,

1080
01:07:02,700 --> 01:07:07,220
then this is what you get.

1081
01:07:07,220 --> 01:07:12,080
So that's one way of
understanding saliency.

1082
01:07:12,080 --> 01:07:14,790
And it's very effective
in many cases.

1083
01:07:14,790 --> 01:07:19,790
But sometimes it's not
just about the pixel values

1084
01:07:19,790 --> 01:07:21,810
all the way to the back.

1085
01:07:21,810 --> 01:07:25,640
You want to see for
each of the classes

1086
01:07:25,640 --> 01:07:28,770
how the activations work.

1087
01:07:28,770 --> 01:07:32,330
And this brings us to
class activation maps

1088
01:07:32,330 --> 01:07:37,010
or CAM algorithm, class
activation mapping, CAM

1089
01:07:37,010 --> 01:07:40,760
or Grad-CAM that I'll
talk about in two minutes,

1090
01:07:40,760 --> 01:07:44,900
are one of the most
and widely used

1091
01:07:44,900 --> 01:07:47,430
algorithms for
understanding CNNs,

1092
01:07:47,430 --> 01:07:49,950
and also could be used for
other architectures too.

1093
01:07:49,950 --> 01:07:53,360
But for transformers, we
have a much better way

1094
01:07:53,360 --> 01:07:56,900
of making sense of
those, which actually we

1095
01:07:56,900 --> 01:07:59,090
talked in the last lecture.

1096
01:07:59,090 --> 01:08:04,100
So what happens is that for
each of the convolution layers,

1097
01:08:04,100 --> 01:08:06,200
we often do pooling.

1098
01:08:06,200 --> 01:08:08,220
And the pooling
generates feature maps.

1099
01:08:08,220 --> 01:08:11,400
The feature maps are
then turned into scores.

1100
01:08:11,400 --> 01:08:20,134
And those scores with those
values of the weights.

1101
01:08:20,134 --> 01:08:24,950
If we extend the math,
basically, we simply

1102
01:08:24,950 --> 01:08:29,430
can highlight the class
scores in a weighted sum form.

1103
01:08:29,430 --> 01:08:35,359
And this means that we can
trace back class predictions

1104
01:08:35,359 --> 01:08:39,500
all the way back
to the feature maps

1105
01:08:39,500 --> 01:08:42,060
and in specific
locations of the space,

1106
01:08:42,060 --> 01:08:44,630
because convolution
layers are always

1107
01:08:44,630 --> 01:08:47,379
mapped to a space in
the image space two.

1108
01:08:47,379 --> 01:08:52,850
We do convolution as the
spatial consistency across all

1109
01:08:52,850 --> 01:08:55,729
of the operations can
help us trace back

1110
01:08:55,729 --> 01:08:58,140
all the way to the image space.

1111
01:08:58,140 --> 01:09:02,180
So anyways, we can look
at the feature maps

1112
01:09:02,180 --> 01:09:04,890
and see how the
class activations,

1113
01:09:04,890 --> 01:09:06,859
each of these
classes are actually

1114
01:09:06,859 --> 01:09:11,040
impacting those
locations in the image.

1115
01:09:11,040 --> 01:09:16,880
And with that now, if I do
this multiplication of weights

1116
01:09:16,880 --> 01:09:21,470
versus the weights
that we've learned

1117
01:09:21,470 --> 01:09:26,160
on top of the feature values, we
create some class activations.

1118
01:09:26,160 --> 01:09:31,100
And this means that I have a
way now to go back to the image

1119
01:09:31,100 --> 01:09:33,810
space, because as long as
I'm in the convolution space,

1120
01:09:33,810 --> 01:09:36,080
I can go all the way
back to the image

1121
01:09:36,080 --> 01:09:41,060
and create these maps of, for
example, for each of the classes

1122
01:09:41,060 --> 01:09:46,850
palace, dome, church,
altar, and monastery,

1123
01:09:46,850 --> 01:09:52,529
we can have different
class activation maps.

1124
01:09:52,529 --> 01:09:54,270
These are the weights.

1125
01:09:54,270 --> 01:09:57,380
These are the pixels or areas
of the convolution layer

1126
01:09:57,380 --> 01:10:01,220
that have been
driving the scores

1127
01:10:01,220 --> 01:10:06,260
for these specific classes.

1128
01:10:06,260 --> 01:10:09,920
It's the same for others,
like class activation maps

1129
01:10:09,920 --> 01:10:14,000
for one single object
in different images.

1130
01:10:14,000 --> 01:10:15,780
But there is a
problem with this.

1131
01:10:15,780 --> 01:10:18,050
And that problem
is that we can only

1132
01:10:18,050 --> 01:10:21,090
apply this to the last
convolution layer,

1133
01:10:21,090 --> 01:10:23,910
because this is the
only way we can do it.

1134
01:10:23,910 --> 01:10:26,550
We can only go to the
last convolution layer.

1135
01:10:26,550 --> 01:10:29,550
The way that we did
the calculations here.

1136
01:10:29,550 --> 01:10:32,390
And in order to
solve that problem,

1137
01:10:32,390 --> 01:10:34,640
there is one variant
of the algorithm

1138
01:10:34,640 --> 01:10:38,990
called Grad-CAM, so
gradient-weighted class

1139
01:10:38,990 --> 01:10:40,170
activation maps.

1140
01:10:40,170 --> 01:10:43,040
It's basically the
same algorithm.

1141
01:10:43,040 --> 01:10:46,850
We need to weight calculate the
weights with respect to the--

1142
01:10:46,850 --> 01:10:52,160
we basically take
one of the layers

1143
01:10:52,160 --> 01:10:54,900
that created some activation.

1144
01:10:54,900 --> 01:10:59,130
In the class level, we
compute the gradients,

1145
01:10:59,130 --> 01:11:03,170
instead of just calculating
the multiplication between W

1146
01:11:03,170 --> 01:11:04,760
and feature.

1147
01:11:04,760 --> 01:11:07,640
We go all the way back
with the gradients

1148
01:11:07,640 --> 01:11:11,180
and create a weight
based on the gradients,

1149
01:11:11,180 --> 01:11:15,900
and then that is used
instead of the weights,

1150
01:11:15,900 --> 01:11:17,810
it's aggregate of
all of the weights

1151
01:11:17,810 --> 01:11:20,700
and gradients up to
that specific layer.

1152
01:11:20,700 --> 01:11:24,960
And then we weight
that with that.

1153
01:11:24,960 --> 01:11:32,850
And then we also use ReLU
to pass the positive ones.

1154
01:11:32,850 --> 01:11:36,170
And then that could
also be all the way

1155
01:11:36,170 --> 01:11:38,910
shown in the image space.

1156
01:11:38,910 --> 01:11:43,820
So I talked about CAM,
which was only applied

1157
01:11:43,820 --> 01:11:45,600
to the last convolution layer.

1158
01:11:45,600 --> 01:11:47,260
If you want to--

1159
01:11:47,260 --> 01:11:51,660
but this is not possible because
in most of the CNN algorithms,

1160
01:11:51,660 --> 01:11:55,990
we don't have just one
convolution layer at the end.

1161
01:11:55,990 --> 01:11:59,860
We always have some operations
fully connected and so on.

1162
01:11:59,860 --> 01:12:03,870
So in order to be able to
carry this class activation

1163
01:12:03,870 --> 01:12:06,390
to the convolution
layer, if there

1164
01:12:06,390 --> 01:12:10,230
is something else in the middle,
we often use the gradients

1165
01:12:10,230 --> 01:12:17,560
and basically weigh the maps
with the gradients aggregates.

1166
01:12:17,560 --> 01:12:20,410
And then we can actually
do the visualization.

1167
01:12:20,410 --> 01:12:24,060
They create these heat maps
for each of the objects.

1168
01:12:24,060 --> 01:12:26,980
So this was about CNNs.

1169
01:12:26,980 --> 01:12:32,310
But we talked about transformers
last week in the last lecture

1170
01:12:32,310 --> 01:12:37,270
that they inherently come
with the activation maps.

1171
01:12:37,270 --> 01:12:41,610
Do you remember
that language matrix

1172
01:12:41,610 --> 01:12:45,660
that Justin showed, that for
each of the output words,

1173
01:12:45,660 --> 01:12:47,820
there is a tension
weight for the input.

1174
01:12:47,820 --> 01:12:50,610
We can do that, the same
thing for the pixels.

1175
01:12:50,610 --> 01:12:54,140
For each of the outputs,
we can create these maps

1176
01:12:54,140 --> 01:13:00,110
in the pixel space and visualize
the features of the ViTs

1177
01:13:00,110 --> 01:13:01,020
in the pixel space.

1178
01:13:01,020 --> 01:13:03,140
So basically, with
ViTs and transformers,

1179
01:13:03,140 --> 01:13:04,410
this is much easier.

1180
01:13:04,410 --> 01:13:09,080
We already have a
way to visualize

1181
01:13:09,080 --> 01:13:11,730
the attentions, the weights.

1182
01:13:11,730 --> 01:13:16,880
But with CNNs, we
often use Grad-CAM

1183
01:13:16,880 --> 01:13:19,830
or these types of algorithms.

1184
01:13:19,830 --> 01:13:24,530
That said, I did
this task that I

1185
01:13:24,530 --> 01:13:29,180
thought I wouldn't be able
to, completing the topics I

1186
01:13:29,180 --> 01:13:31,350
wanted to talk about today.

1187
01:13:31,350 --> 01:13:35,000
And next session,
we'll have the lecture

1188
01:13:35,000 --> 01:13:36,810
around video understanding.

1189
01:13:36,810 --> 01:13:38,740
Thank you.