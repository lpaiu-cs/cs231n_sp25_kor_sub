2
00:00:05,400 --> 00:00:09,260
Welcome back to
CS231N lecture 13.

3
00:00:09,260 --> 00:00:11,480
Today we're going to talk
about generative models.

4
00:00:11,480 --> 00:00:14,132
Last time we were talking about
self-supervised learning, which

5
00:00:14,132 --> 00:00:15,840
is this really
interesting paradigm where

6
00:00:15,840 --> 00:00:18,360
we want to somehow
learn structure directly

7
00:00:18,360 --> 00:00:22,120
from data with no
supervision, with no labels.

8
00:00:22,120 --> 00:00:25,277
And the typical formulation
of self-supervised learning

9
00:00:25,277 --> 00:00:27,360
that we talked about a
bunch of examples last time

10
00:00:27,360 --> 00:00:29,540
is you have your big
data set with no labels.

11
00:00:29,540 --> 00:00:30,582
Ideally it's just images.

12
00:00:30,582 --> 00:00:31,290
It 's this great.

13
00:00:31,290 --> 00:00:32,479
You can get a lot of images.

14
00:00:32,479 --> 00:00:34,729
You're going to feed these
through some encoder that's

15
00:00:34,729 --> 00:00:37,300
going to extract a feature
representation from your images,

16
00:00:37,300 --> 00:00:40,080
then go through some decoder
that will predict something

17
00:00:40,080 --> 00:00:41,642
from that feature
representation.

18
00:00:41,642 --> 00:00:43,600
And the whole trick in
self-supervised learning

19
00:00:43,600 --> 00:00:45,800
is coming up with
some pretext task

20
00:00:45,800 --> 00:00:47,360
that you can train
this whole system

21
00:00:47,360 --> 00:00:51,200
on without requiring any human
annotation or human labels.

22
00:00:51,200 --> 00:00:52,960
And then so we
talked about things

23
00:00:52,960 --> 00:00:55,160
like rotation of
different tasks that we

24
00:00:55,160 --> 00:00:59,560
can use as pretexts to formulate
these self-supervised learning

25
00:00:59,560 --> 00:01:00,560
objectives.

26
00:01:00,560 --> 00:01:03,440
And then typically, this is
usually a two-stage procedure

27
00:01:03,440 --> 00:01:05,200
where first, you're
going to go and learn

28
00:01:05,200 --> 00:01:07,320
this self-supervised
encoder decoder

29
00:01:07,320 --> 00:01:10,340
on your self-supervised task on
all the data that you can find.

30
00:01:10,340 --> 00:01:11,715
And then after
that, you're going

31
00:01:11,715 --> 00:01:15,160
to throw away the decoder, and
then slot in some new, possibly

32
00:01:15,160 --> 00:01:17,400
tiny, fully connected
network and actually

33
00:01:17,400 --> 00:01:18,860
train this thing
maybe end to end,

34
00:01:18,860 --> 00:01:21,520
or maybe just learn the fully
connected network at the end

35
00:01:21,520 --> 00:01:23,120
on some small labeled task.

36
00:01:23,120 --> 00:01:25,920
And the idea here is that
via self-supervised learning,

37
00:01:25,920 --> 00:01:28,940
this pretext task, you can
train on lots and lots of data,

38
00:01:28,940 --> 00:01:31,200
millions, hundreds of
millions, billions of samples

39
00:01:31,200 --> 00:01:33,980
where we don't have access
to high quality human labels.

40
00:01:33,980 --> 00:01:35,940
And in the process of
self-supervised learning,

41
00:01:35,940 --> 00:01:38,315
it's going to learn something
about the general structure

42
00:01:38,315 --> 00:01:39,600
of images or of data.

43
00:01:39,600 --> 00:01:42,400
And then you can transfer that
knowledge to downstream tasks

44
00:01:42,400 --> 00:01:44,310
where you have small
amounts of human labels.

45
00:01:44,310 --> 00:01:46,560
So then the typical setup
you should keep in your mind

46
00:01:46,560 --> 00:01:48,893
that we want to work towards
in self-supervised learning

47
00:01:48,893 --> 00:01:51,762
is that you're going to train
on a billion unlabeled images

48
00:01:51,762 --> 00:01:53,720
that we're getting from
the internet somewhere,

49
00:01:53,720 --> 00:01:56,440
and then we're going to transfer
those features to tasks where

50
00:01:56,440 --> 00:01:59,860
we're willing to sit down and
label maybe tens, hundreds,

51
00:01:59,860 --> 00:02:02,040
maybe thousands of examples
for particular tasks

52
00:02:02,040 --> 00:02:03,440
that we really care about.

53
00:02:03,440 --> 00:02:05,960
But we want those
tasks to be improved

54
00:02:05,960 --> 00:02:08,280
by this generic
knowledge that we've

55
00:02:08,280 --> 00:02:10,853
learned through this
self-supervised pretext task.

56
00:02:10,853 --> 00:02:13,520
And we talked about a couple of
different kinds of pretext tasks

57
00:02:13,520 --> 00:02:17,080
last time, including rotation,
rearrangement, reconstruction.

58
00:02:17,080 --> 00:02:18,640
All of these are
basically having

59
00:02:18,640 --> 00:02:21,860
this sense of that you're making
some geometric perturbation,

60
00:02:21,860 --> 00:02:23,995
geometric disturbance
to the input pixels,

61
00:02:23,995 --> 00:02:26,120
and then you're asking the
model to somehow recover

62
00:02:26,120 --> 00:02:27,950
from that perturbation.

63
00:02:27,950 --> 00:02:30,200
So in the case of rotation,
maybe you rotate the image

64
00:02:30,200 --> 00:02:32,720
and you ask the model to
predict how much it was rotated.

65
00:02:32,720 --> 00:02:35,525
In the sense of rearrangement
or solving jigsaw puzzles,

66
00:02:35,525 --> 00:02:37,400
you're going to cut the
image up into patches

67
00:02:37,400 --> 00:02:38,960
and ask the model to
try to predict what

68
00:02:38,960 --> 00:02:40,880
was the relative
arrangement of those patches

69
00:02:40,880 --> 00:02:42,343
in the original image.

70
00:02:42,343 --> 00:02:43,760
Or in reconstruction,
maybe you're

71
00:02:43,760 --> 00:02:45,677
going to delete some
parts of the input image,

72
00:02:45,677 --> 00:02:48,080
and then ask the model to fill
them in as some inpainting

73
00:02:48,080 --> 00:02:50,360
or reconstruction task.

74
00:02:50,360 --> 00:02:52,000
And these are fairly successful.

75
00:02:52,000 --> 00:02:54,680
We also talked last time
about a different formulation

76
00:02:54,680 --> 00:02:57,180
of self-supervised learning
called contrastive learning,

77
00:02:57,180 --> 00:02:58,800
which has been very successful.

78
00:02:58,800 --> 00:03:01,312
And here I was told that you
ran out of time a little bit

79
00:03:01,312 --> 00:03:03,020
to cover a couple of
these later methods,

80
00:03:03,020 --> 00:03:04,687
so I wanted to just
go over those really

81
00:03:04,687 --> 00:03:07,960
quickly at the beginning
of today's lecture instead.

82
00:03:07,960 --> 00:03:10,760
So the idea of
contrastive learning

83
00:03:10,760 --> 00:03:12,937
is you're going to get
pairs that are similar

84
00:03:12,937 --> 00:03:14,520
and pairs that are
dissimilar, and you

85
00:03:14,520 --> 00:03:16,600
want to pull the similar
pairs together and push

86
00:03:16,600 --> 00:03:18,300
the dissimilar pairs apart.

87
00:03:18,300 --> 00:03:19,920
And the way that
you usually do this

88
00:03:19,920 --> 00:03:22,200
in the context of
self-supervised learning

89
00:03:22,200 --> 00:03:24,380
is you're going to start
with your input images.

90
00:03:24,380 --> 00:03:27,020
And again, these are
unlabeled images.

91
00:03:27,020 --> 00:03:28,420
You don't have labels for them.

92
00:03:28,420 --> 00:03:29,920
And now for each
input image, you're

93
00:03:29,920 --> 00:03:32,580
going to apply two
random transformations.

94
00:03:32,580 --> 00:03:34,720
So in the case of the
cat, we took one crop

95
00:03:34,720 --> 00:03:36,800
around the cat's
face, another crop

96
00:03:36,800 --> 00:03:38,440
around the backside of the cat.

97
00:03:38,440 --> 00:03:41,520
And around the monkey, we took
one around the monkey's face

98
00:03:41,520 --> 00:03:44,600
and also dropped it to black
and white, et cetera, et cetera.

99
00:03:44,600 --> 00:03:46,700
So basically for each
one of your input images,

100
00:03:46,700 --> 00:03:49,860
you're going to apply two
or possibly more than two,

101
00:03:49,860 --> 00:03:53,480
but two is a nice minimal
subset, two random perturbations

102
00:03:53,480 --> 00:03:55,360
to your input image.

103
00:03:55,360 --> 00:03:58,230
Now you're going to feed all
of those randomly perturbed

104
00:03:58,230 --> 00:04:02,110
versions of your input data to
some feature extractor, which

105
00:04:02,110 --> 00:04:05,070
could be a VIT, could be
a CNN, any neural network

106
00:04:05,070 --> 00:04:08,230
that can input an image and
output a feature representation.

107
00:04:08,230 --> 00:04:11,190
Then you want to apply
this notion of contrastive.

108
00:04:11,190 --> 00:04:14,270
So for each of the
two augmentations

109
00:04:14,270 --> 00:04:17,149
that came from the cat, we
want those two feature vectors

110
00:04:17,149 --> 00:04:19,329
to be the same, so
we color them green.

111
00:04:19,329 --> 00:04:22,630
So you basically compute this
big n squared similarity matrix

112
00:04:22,630 --> 00:04:23,230
where if--

113
00:04:23,230 --> 00:04:28,910
Well, I guess it's 2n (2n)
squared, so it's 4n squared.

114
00:04:28,910 --> 00:04:31,710
If you have n images you put
two perturbations on each,

115
00:04:31,710 --> 00:04:38,150
so we have a giant 2n by 2n
matrix for all these perturbed

116
00:04:38,150 --> 00:04:40,590
augmented samples that we got.

117
00:04:40,590 --> 00:04:44,930
And now basically we
want to pull together

118
00:04:44,930 --> 00:04:47,870
the two augmentations that
came from the original image.

119
00:04:47,870 --> 00:04:51,310
And for every pair
of augmentations

120
00:04:51,310 --> 00:04:53,130
that came from different
original images,

121
00:04:53,130 --> 00:04:54,430
we want to push them apart.

122
00:04:54,430 --> 00:04:57,990
So you basically run all
of these things through

123
00:04:57,990 --> 00:05:00,470
your feature extractor,
compute this giant,

124
00:05:00,470 --> 00:05:03,725
4n squared matrix of all of
your scaler similarities between

125
00:05:03,725 --> 00:05:06,350
those feature vectors, and then
pull together the ones that are

126
00:05:06,350 --> 00:05:08,910
similar, push apart the ones
that ought to be different,

127
00:05:08,910 --> 00:05:12,150
and that's the basic idea
of contrastive learning.

128
00:05:12,150 --> 00:05:15,190
And one paper that really
pulled all this together

129
00:05:15,190 --> 00:05:17,410
a couple years ago
was called SimCLR

130
00:05:17,410 --> 00:05:19,030
that applied this
very successfully

131
00:05:19,030 --> 00:05:20,590
to self-supervised
representation

132
00:05:20,590 --> 00:05:22,430
learning on images,
and that's the one

133
00:05:22,430 --> 00:05:24,430
I think you walked
through last time.

134
00:05:24,430 --> 00:05:26,590
But one problem with
the SimCLR setup

135
00:05:26,590 --> 00:05:28,390
is that it requires
a fairly large batch

136
00:05:28,390 --> 00:05:33,327
size to get good
convergence because it's

137
00:05:33,327 --> 00:05:34,910
too easy of a problem
for the network.

138
00:05:34,910 --> 00:05:36,327
If there aren't
that many samples,

139
00:05:36,327 --> 00:05:38,070
it's too easy to
pick out the two cat

140
00:05:38,070 --> 00:05:39,330
ones that looked similar.

141
00:05:39,330 --> 00:05:41,030
So to make the problem
hard enough for the network

142
00:05:41,030 --> 00:05:42,610
to give it good enough
learning signal,

143
00:05:42,610 --> 00:05:44,110
you tend to need
quite a large batch

144
00:05:44,110 --> 00:05:47,052
size in order to get this model
to converge to good features.

145
00:05:47,052 --> 00:05:48,510
And then once you
do that, you need

146
00:05:48,510 --> 00:05:50,750
to reopen all the ideas
around large scale distributed

147
00:05:50,750 --> 00:05:53,047
training that we talked about
a couple of lectures ago,

148
00:05:53,047 --> 00:05:54,130
which is totally feasible.

149
00:05:54,130 --> 00:05:55,130
It totally works.

150
00:05:55,130 --> 00:05:56,630
But you might ask,
is there some way

151
00:05:56,630 --> 00:05:59,550
you can get away without that.

152
00:05:59,550 --> 00:06:01,310
And that leads to a
couple of approaches

153
00:06:01,310 --> 00:06:03,198
that I don't want to go
into too much detail.

154
00:06:03,198 --> 00:06:04,990
I actually don't want
to walk through these

155
00:06:04,990 --> 00:06:06,550
and tell you exactly
how they work.

156
00:06:06,550 --> 00:06:08,550
I just want to make you
aware of their existence

157
00:06:08,550 --> 00:06:10,270
and give you the
general flavor of what

158
00:06:10,270 --> 00:06:11,870
they're trying to achieve.

159
00:06:11,870 --> 00:06:14,190
So in this MoCo or
momentum contrast

160
00:06:14,190 --> 00:06:15,970
approach to
self-supervised learning,

161
00:06:15,970 --> 00:06:18,650
the setup is very similar to
what we just saw in SimCLR.

162
00:06:18,650 --> 00:06:19,530
You're taking data.

163
00:06:19,530 --> 00:06:20,970
You're getting augmented pairs.

164
00:06:20,970 --> 00:06:22,595
You run them through
a feature encoder.

165
00:06:22,595 --> 00:06:24,220
You want to pull
together the ones that

166
00:06:24,220 --> 00:06:26,670
are similar, push apart the
ones that are dissimilar.

167
00:06:26,670 --> 00:06:28,630
But the thing that
differs is that we

168
00:06:28,630 --> 00:06:31,190
want to get away with not
having to have a gigantic batch

169
00:06:31,190 --> 00:06:32,650
size at every iteration.

170
00:06:32,650 --> 00:06:38,070
So to do that, they
keep a q of samples

171
00:06:38,070 --> 00:06:40,950
from previous
iterations of training.

172
00:06:40,950 --> 00:06:42,690
And then at every
training iteration,

173
00:06:42,690 --> 00:06:46,050
I've got my x query is my
current new batch of data.

174
00:06:46,050 --> 00:06:49,810
And I have this
q, x0, x1, x2 key,

175
00:06:49,810 --> 00:06:51,230
which are previous
batches of data

176
00:06:51,230 --> 00:06:54,390
that I've seen on previous
iterations of training.

177
00:06:54,390 --> 00:06:56,045
Now, my current
batch of data, I'm

178
00:06:56,045 --> 00:06:57,670
going to run through
my encoder network

179
00:06:57,670 --> 00:07:01,790
the same as I always did, and
compute the contrastive loss

180
00:07:01,790 --> 00:07:03,750
the same way that
we did with SimCLR.

181
00:07:03,750 --> 00:07:07,690
And then this larger q, these
previous history of batches,

182
00:07:07,690 --> 00:07:09,190
we're going to run
through something

183
00:07:09,190 --> 00:07:11,815
different, the momentum
encoder, and then

184
00:07:11,815 --> 00:07:13,190
still get feature
representations

185
00:07:13,190 --> 00:07:17,710
and compute the same similarity
that we did through the SimCLR

186
00:07:17,710 --> 00:07:18,310
thing.

187
00:07:18,310 --> 00:07:19,852
But the problem is
that we don't want

188
00:07:19,852 --> 00:07:21,790
to backpropagate into
the momentum encoder

189
00:07:21,790 --> 00:07:23,247
because it has too much data.

190
00:07:23,247 --> 00:07:24,330
It has too big of a batch.

191
00:07:24,330 --> 00:07:26,470
We can't afford to fit
that in GPU memory.

192
00:07:26,470 --> 00:07:30,090
So we want to not have to
backpropagate through that part.

193
00:07:30,090 --> 00:07:33,830
So that means that we cannot
update this momentum encoder,

194
00:07:33,830 --> 00:07:36,590
the second encoder,
via gradient descent.

195
00:07:36,590 --> 00:07:38,610
Instead we're going
to do something wacky.

196
00:07:38,610 --> 00:07:40,790
What we're going to do is
have this momentum encoder

197
00:07:40,790 --> 00:07:42,050
have its own set of weights.

198
00:07:42,050 --> 00:07:44,332
We're going to learn them
not via gradient descent.

199
00:07:44,332 --> 00:07:46,790
Instead what we're going to do
is have the momentum encoder

200
00:07:46,790 --> 00:07:49,070
be a exponential moving
average of the weights

201
00:07:49,070 --> 00:07:50,335
of the normal encoder.

202
00:07:50,335 --> 00:07:51,710
So the normal
encoder we're going

203
00:07:51,710 --> 00:07:52,960
to learn via gradient descent.

204
00:07:52,960 --> 00:07:53,990
Everything is normal.

205
00:07:53,990 --> 00:07:55,370
We'll forward prop or back prop.

206
00:07:55,370 --> 00:07:56,130
We'll get gradients.

207
00:07:56,130 --> 00:07:57,505
We'll make a
gradient update step

208
00:07:57,505 --> 00:08:00,670
on the typical encoder,
that's the normal thing.

209
00:08:00,670 --> 00:08:06,130
But then after we do that,
the momentum encoder,

210
00:08:06,130 --> 00:08:08,470
we're going to decay the
current momentum encoder

211
00:08:08,470 --> 00:08:11,110
weights by like
0.99, and then add

212
00:08:11,110 --> 00:08:13,100
in 1% of the encoder weights.

213
00:08:13,100 --> 00:08:15,350
So then the momentum encoder
we have this other update

214
00:08:15,350 --> 00:08:18,110
rule where it's this lagging,
trailing, exponential moving

215
00:08:18,110 --> 00:08:20,830
average of the encoder weights.

216
00:08:20,830 --> 00:08:23,470
And I don't have a great
intuition or explanation for why

217
00:08:23,470 --> 00:08:24,680
this exactly makes sense.

218
00:08:24,680 --> 00:08:26,430
But there's very strong
empirical evidence

219
00:08:26,430 --> 00:08:27,510
that this works.

220
00:08:27,510 --> 00:08:30,410
So that's the state of things.

221
00:08:30,410 --> 00:08:32,750
But it's nice because it means
that you can now get away

222
00:08:32,750 --> 00:08:35,070
with learning these
self-supervised representations

223
00:08:35,070 --> 00:08:37,270
without having to have
this gigantic massive batch

224
00:08:37,270 --> 00:08:39,517
of negatives at every iteration.

225
00:08:39,517 --> 00:08:40,809
And this was fairly successful.

226
00:08:40,809 --> 00:08:42,392
There were a bunch
of follow up papers

227
00:08:42,392 --> 00:08:43,830
that pushed this direction.

228
00:08:43,830 --> 00:08:46,310
Another one that you should
be aware of is called DINO.

229
00:08:46,310 --> 00:08:47,930
Again, the idea is very similar.

230
00:08:47,930 --> 00:08:50,330
It uses this similar
momentum encoder--

231
00:08:50,330 --> 00:08:52,470
this dual normal
encoder that's learned

232
00:08:52,470 --> 00:08:54,110
via gradient descent
and a momentum

233
00:08:54,110 --> 00:08:55,665
encoder just as in MoCo.

234
00:08:55,665 --> 00:08:57,290
But the loss is a
little bit different.

235
00:08:57,290 --> 00:09:00,990
Instead of using a softmax, they
use some KL divergence loss.

236
00:09:00,990 --> 00:09:02,710
And the reason I'm
mentioning this one

237
00:09:02,710 --> 00:09:05,850
is because you should be aware
of the existence of DINO V2

238
00:09:05,850 --> 00:09:08,330
even if we don't talk
about exactly what it does,

239
00:09:08,330 --> 00:09:12,790
because DINO V2 is a
really strong model

240
00:09:12,790 --> 00:09:14,990
for self-supervised
features that's used quite

241
00:09:14,990 --> 00:09:16,590
a lot in practice these days.

242
00:09:16,590 --> 00:09:19,070
So what they basically did
is took this recipe from DINO

243
00:09:19,070 --> 00:09:21,590
V1, which was
similar to MoCo, had

244
00:09:21,590 --> 00:09:23,730
a lot of ideas from
SimCLR as well,

245
00:09:23,730 --> 00:09:26,550
but a lot of unique details
in their approach as well.

246
00:09:26,550 --> 00:09:28,682
But the big
difference in DINO V2

247
00:09:28,682 --> 00:09:30,890
is that they scaled up the
training data quite a lot.

248
00:09:30,890 --> 00:09:33,112
So a lot of these previous
self-supervised approaches

249
00:09:33,112 --> 00:09:35,070
had been trained on the
ImageNet dataset, which

250
00:09:35,070 --> 00:09:37,510
was 1 million
images, and DINO V2

251
00:09:37,510 --> 00:09:39,150
was able to
successfully scale up

252
00:09:39,150 --> 00:09:42,090
this approach to a much larger
training set of about 142

253
00:09:42,090 --> 00:09:43,490
million images.

254
00:09:43,490 --> 00:09:46,210
So in deep learning, we
like bigger networks,

255
00:09:46,210 --> 00:09:49,000
we like bigger data, we like
more GPUs, we like more flops,

256
00:09:49,000 --> 00:09:50,460
we like all of those things.

257
00:09:50,460 --> 00:09:52,500
And Dino V2 was able
to find a recipe

258
00:09:52,500 --> 00:09:54,860
for self-supervised learning
that successfully scaled up

259
00:09:54,860 --> 00:09:56,900
to this much larger
dataset, gives very strong

260
00:09:56,900 --> 00:09:58,200
self-supervised features.

261
00:09:58,200 --> 00:10:00,780
And this tends to be used
quite a lot in practice today

262
00:10:00,780 --> 00:10:03,580
if you want to pick up
features and then fine tune

263
00:10:03,580 --> 00:10:07,150
them or supervise them for some
of your own downstream tasks.

264
00:10:07,150 --> 00:10:08,400
So again, I don't expect you--

265
00:10:08,400 --> 00:10:10,400
I don't want to walk through all
the details of how this works.

266
00:10:10,400 --> 00:10:11,140
I don't expect you to know.

267
00:10:11,140 --> 00:10:13,180
But I want you to know
that it exists in case

268
00:10:13,180 --> 00:10:15,820
you want to pick it up and use
it for some of your own projects

269
00:10:15,820 --> 00:10:18,520
in the future.

270
00:10:18,520 --> 00:10:20,020
So that's basically
all I had to say

271
00:10:20,020 --> 00:10:21,700
about self-supervised learning.

272
00:10:21,700 --> 00:10:23,700
Any questions about
that before we

273
00:10:23,700 --> 00:10:25,590
move on to the meat
of today's lecture?

275
00:10:32,920 --> 00:10:33,420
OK.

276
00:10:33,420 --> 00:10:34,780
Guess not.

277
00:10:34,780 --> 00:10:38,500
So today the main topic
is generative models.

278
00:10:38,500 --> 00:10:39,580
This is really cool.

279
00:10:39,580 --> 00:10:41,780
This is an area of deep
learning that basically

280
00:10:41,780 --> 00:10:45,320
went from not working at
all, 10 years ago, to really,

281
00:10:45,320 --> 00:10:47,580
really working in the
last couple of years.

282
00:10:47,580 --> 00:10:50,520
And this has given rise to
things like language models.

283
00:10:50,520 --> 00:10:52,460
These can be viewed
as generative models

284
00:10:52,460 --> 00:10:55,340
as we'll see, all image
generation models,

285
00:10:55,340 --> 00:10:56,960
of video generation models.

286
00:10:56,960 --> 00:10:59,900
These really went from just
absolutely not working at all

287
00:10:59,900 --> 00:11:01,220
when I was in grad school.

288
00:11:01,220 --> 00:11:02,860
You would look at these
samples and peer into them,

289
00:11:02,860 --> 00:11:04,600
and they just look
like low resolution,

290
00:11:04,600 --> 00:11:06,040
complete blurry garbage.

291
00:11:06,040 --> 00:11:08,545
But somehow you could
view some promise in them.

292
00:11:08,545 --> 00:11:10,420
And I'm glad that people
kept pushing on that

293
00:11:10,420 --> 00:11:11,740
and pushed through
the blurry garbage

294
00:11:11,740 --> 00:11:13,440
and scaled it up
over the past decade

295
00:11:13,440 --> 00:11:15,680
because now a lot of these
techniques really do work,

296
00:11:15,680 --> 00:11:17,180
and that's very exciting.

297
00:11:17,180 --> 00:11:19,780
So this is an area
of deep learning

298
00:11:19,780 --> 00:11:22,100
that basically didn't work
at all with the first time

299
00:11:22,100 --> 00:11:25,980
we taught this class, and that's
really cool that it now does.

300
00:11:25,980 --> 00:11:28,260
But that said, a lot of
the fundamental ideas

301
00:11:28,260 --> 00:11:30,820
around generative modeling
actually have remained the same.

302
00:11:30,820 --> 00:11:34,100
The ideas about how you think
about data, what are approaches

303
00:11:34,100 --> 00:11:36,700
for modeling them, a lot of
those mathematical fundamentals

304
00:11:36,700 --> 00:11:41,340
actually have not changed
that much in the past decade.

305
00:11:41,340 --> 00:11:44,460
But what changed
is more compute,

306
00:11:44,460 --> 00:11:47,340
more stable training
recipes, bigger datasets,

307
00:11:47,340 --> 00:11:48,380
distributed training.

308
00:11:48,380 --> 00:11:51,000
And the ability to scale all
this up into more useful tasks,

309
00:11:51,000 --> 00:11:52,792
I think, was really
what drove the progress

310
00:11:52,792 --> 00:11:53,980
over the past decade.

311
00:11:53,980 --> 00:11:55,980
There were some algorithmic
tweaks, especially

312
00:11:55,980 --> 00:12:00,540
we'll see that next lecture when
we talk about diffusion models.

313
00:12:00,540 --> 00:12:02,880
But first, before we talk
about generative modeling,

314
00:12:02,880 --> 00:12:05,380
I wanted to step back a little
bit and talk about supervised

315
00:12:05,380 --> 00:12:08,060
versus unsupervised learning.

316
00:12:08,060 --> 00:12:10,060
There's a couple
of different tasks

317
00:12:10,060 --> 00:12:12,100
that we try to approach
in deep learning,

318
00:12:12,100 --> 00:12:14,180
and they can sometimes
be sliced along a couple

319
00:12:14,180 --> 00:12:15,587
of different orthogonal axes.

320
00:12:15,587 --> 00:12:17,420
So I wanted to talk
about those a little bit

321
00:12:17,420 --> 00:12:20,540
just so we get our terminology
and our nomenclature clear.

322
00:12:20,540 --> 00:12:22,340
So supervised learning
is what we've mostly

323
00:12:22,340 --> 00:12:25,900
been doing all semester
except for last lecture.

324
00:12:25,900 --> 00:12:29,317
In supervised learning, we have
a dataset of pairs, x and y.

325
00:12:29,317 --> 00:12:30,900
And the goal is to
learn some function

326
00:12:30,900 --> 00:12:34,620
that maps from the input data
x to the target or label y.

327
00:12:34,620 --> 00:12:38,180
And we've seen a lot of examples
of this approach so far.

328
00:12:38,180 --> 00:12:40,380
Something like image
classification, our input x

329
00:12:40,380 --> 00:12:41,240
is an image.

330
00:12:41,240 --> 00:12:45,360
The output y is going to be
a label or image captioning.

331
00:12:45,360 --> 00:12:47,160
The input x is going
to be an image.

332
00:12:47,160 --> 00:12:49,700
The output y is going to be
some piece of text describing

333
00:12:49,700 --> 00:12:50,940
what we see in that image.

334
00:12:50,940 --> 00:12:51,780
Object detection.

335
00:12:51,780 --> 00:12:52,840
Input is an image.

336
00:12:52,840 --> 00:12:54,740
Output is a set of
boxes and category

337
00:12:54,740 --> 00:12:57,260
labels describing the objects
that appear in the image.

338
00:12:57,260 --> 00:12:58,160
Or segmentation.

339
00:12:58,160 --> 00:13:02,550
Maybe you assign a label to
every pixel in the input image.

340
00:13:02,550 --> 00:13:04,300
And these are supervised
learning problems

341
00:13:04,300 --> 00:13:06,298
because the task
you're trying to solve,

342
00:13:06,298 --> 00:13:08,340
the thing you want to
predict is exactly what you

343
00:13:08,340 --> 00:13:09,440
have in your data set.

344
00:13:09,440 --> 00:13:11,920
And all you need to
do, in some sense,

345
00:13:11,920 --> 00:13:13,563
is learn a function
that mimics that x

346
00:13:13,563 --> 00:13:14,980
to y mapping on
your training data

347
00:13:14,980 --> 00:13:17,860
set, and then generalizes
that mapping to new samples

348
00:13:17,860 --> 00:13:20,180
beyond your training dataset.

349
00:13:20,180 --> 00:13:22,540
Now, unsupervised learning
is something a bit

350
00:13:22,540 --> 00:13:26,620
more fishy and mysterious
and hard to describe.

351
00:13:26,620 --> 00:13:28,600
But the idea of
unsupervised learning,

352
00:13:28,600 --> 00:13:30,400
or sometimes
self-supervised learning,

353
00:13:30,400 --> 00:13:32,680
is that you don't have any
labels, you just have data.

354
00:13:32,680 --> 00:13:34,340
You just have samples x.

355
00:13:34,340 --> 00:13:35,620
You just have images.

356
00:13:35,620 --> 00:13:38,620
And you want to learn some
structure from that data.

357
00:13:38,620 --> 00:13:41,200
There's no particular task
you're necessarily targeting.

358
00:13:41,200 --> 00:13:43,480
You're just trying to
uncover good representations,

359
00:13:43,480 --> 00:13:45,820
good structure in
all of that data.

360
00:13:45,820 --> 00:13:46,500
Why?

361
00:13:46,500 --> 00:13:49,020
So that you can-- as we talked
about in self-supervised

362
00:13:49,020 --> 00:13:52,340
learning often so you can apply
it to downstream tasks later on.

363
00:13:52,340 --> 00:13:55,380
But the task itself in
unsupervised learning

364
00:13:55,380 --> 00:13:58,380
is often somewhat unspecified.

365
00:13:58,380 --> 00:14:00,400
Some examples of this
are K-means clustering,

366
00:14:00,400 --> 00:14:02,820
where maybe we're trying
to identify clusters

367
00:14:02,820 --> 00:14:06,060
in the data, which is some
structure that we can examine

368
00:14:06,060 --> 00:14:10,300
from the raw pixels even though
we didn't have labels for.

369
00:14:10,300 --> 00:14:12,318
Or dimensionality
reduction, PCA,

370
00:14:12,318 --> 00:14:14,860
where we're trying to uncover
some lower dimensional subspace

371
00:14:14,860 --> 00:14:17,020
or lower dimensional
manifold that explains

372
00:14:17,020 --> 00:14:18,340
the structure of our data.

373
00:14:18,340 --> 00:14:19,380
And again, this
is something we're

374
00:14:19,380 --> 00:14:21,080
trying to discover
from the data itself.

375
00:14:21,080 --> 00:14:24,100
We don't have annotations
of what this ought to be.

376
00:14:24,100 --> 00:14:25,017
Or density estimation.

377
00:14:25,017 --> 00:14:27,183
Maybe we're trying to fit
a probability distribution

378
00:14:27,183 --> 00:14:27,880
to the data.

379
00:14:27,880 --> 00:14:30,060
We're trying to understand what
is the probabilistic function

380
00:14:30,060 --> 00:14:32,360
that gave rise to the data
samples that we're seeing.

381
00:14:32,360 --> 00:14:34,027
And again, we don't
have explicit labels

382
00:14:34,027 --> 00:14:35,960
for this or explicit
training set for this.

383
00:14:35,960 --> 00:14:37,740
So this is some hidden
or latent structure

384
00:14:37,740 --> 00:14:40,820
that we're trying to uncover
through the process of training.

385
00:14:40,820 --> 00:14:44,140
So this unsupervised dichotomy
is something that you always

386
00:14:44,140 --> 00:14:45,340
should keep in mind.

387
00:14:45,340 --> 00:14:47,560
And you can do
unsupervised learning,

388
00:14:47,560 --> 00:14:50,080
which is not probabilistic or
not generative necessarily.

389
00:14:50,080 --> 00:14:52,480
Something like clustering,
something like PCA,

390
00:14:52,480 --> 00:14:54,582
often they have probabilistic
interpretations.

391
00:14:54,582 --> 00:14:56,540
But these are examples
of unsupervised learning

392
00:14:56,540 --> 00:14:58,940
that don't necessarily have
a generative or probabilistic

393
00:14:58,940 --> 00:15:01,900
interpretation or don't have
to be thought of as such.

394
00:15:01,900 --> 00:15:04,900
So I often like to think about
the unsupervised dichotomy

395
00:15:04,900 --> 00:15:10,220
as one spectrum along which
methods or systems can lie.

396
00:15:10,220 --> 00:15:13,780
A separate spectrum along which
we can classify systems or tasks

397
00:15:13,780 --> 00:15:16,260
is that of generative versus
discriminative models.

398
00:15:16,260 --> 00:15:17,975
And these are inherently
probabilistic.

399
00:15:17,975 --> 00:15:20,100
And when we talk about
generative or discriminative

400
00:15:20,100 --> 00:15:23,140
models, we're always imagining
some probabilistic structure

401
00:15:23,140 --> 00:15:25,820
in our data that we're trying
to uncover or learn from.

402
00:15:25,820 --> 00:15:27,300
And the difference
is exactly what

403
00:15:27,300 --> 00:15:28,780
is the probabilistic
relationship

404
00:15:28,780 --> 00:15:31,300
between the variables that
we're trying to model.

405
00:15:31,300 --> 00:15:32,780
So in discriminative models--

406
00:15:32,780 --> 00:15:35,460
so typically we have
some y and some x.

407
00:15:35,460 --> 00:15:37,860
And usually we think of
the x as something large,

408
00:15:37,860 --> 00:15:40,280
high-dimensional, usually
an image in our case.

409
00:15:40,280 --> 00:15:42,410
And the y is some
label or description

410
00:15:42,410 --> 00:15:44,410
or auxiliary information.

411
00:15:44,410 --> 00:15:47,110
And so that would be like
your text, like your caption,

412
00:15:47,110 --> 00:15:50,050
like a category label,
something like that.

413
00:15:50,050 --> 00:15:52,352
And when you talk about
a discriminative model,

414
00:15:52,352 --> 00:15:53,810
we're trying to
learn a probability

415
00:15:53,810 --> 00:15:56,290
distribution of y given x.

416
00:15:56,290 --> 00:15:57,970
So we're trying to
learn a distribution

417
00:15:57,970 --> 00:16:02,570
over labels conditioned
on our input image x.

418
00:16:02,570 --> 00:16:07,070
And to really appreciate what's
going on probabilistically,

419
00:16:07,070 --> 00:16:09,690
you need to remember one very
important feature of probability

420
00:16:09,690 --> 00:16:12,280
distributions, and that's
that they are normalized.

421
00:16:12,280 --> 00:16:14,530
When you talk about a
probability distribution or more

422
00:16:14,530 --> 00:16:16,890
generally, a density
function p of x,

423
00:16:16,890 --> 00:16:22,250
p of x is basically a function
that assigns a nonzero

424
00:16:22,250 --> 00:16:27,010
number to every possible input
x with the very important

425
00:16:27,010 --> 00:16:29,570
normalization constraint,
that if you integrate over

426
00:16:29,570 --> 00:16:33,590
the entire space of all possible
x's, it integrates to 1.

427
00:16:33,590 --> 00:16:35,130
And this normalization
constraint

428
00:16:35,130 --> 00:16:38,210
really gives rise to the
power of probabilistic models

429
00:16:38,210 --> 00:16:41,050
in some sense because the
normalization constraint

430
00:16:41,050 --> 00:16:44,870
means that all of your x's need
to compete for probability mass.

431
00:16:44,870 --> 00:16:49,110
There's a fixed unit
amount of probability mass,

432
00:16:49,110 --> 00:16:51,890
and choosing a probability
distribution or density function

433
00:16:51,890 --> 00:16:53,610
basically amounts
to apportioning out

434
00:16:53,610 --> 00:16:55,370
that fixed amount
of probability mass

435
00:16:55,370 --> 00:16:57,810
and smearing it across
all possible values of x

436
00:16:57,810 --> 00:16:59,090
that could exist.

437
00:16:59,090 --> 00:17:01,035
And all of those x's
are in competition

438
00:17:01,035 --> 00:17:02,410
because there's
only a fixed unit

439
00:17:02,410 --> 00:17:03,990
amount of mass to go around.

440
00:17:03,990 --> 00:17:06,329
So if you want to push up
the probability of one x,

441
00:17:06,329 --> 00:17:09,050
necessarily, the
probabilities or densities

442
00:17:09,050 --> 00:17:11,050
of other x's have to go down.

443
00:17:11,050 --> 00:17:14,089
And so then in these
different formulations

444
00:17:14,089 --> 00:17:16,353
of probabilistic models,
basically what's changing

445
00:17:16,353 --> 00:17:17,770
is what are the
variables that are

446
00:17:17,770 --> 00:17:19,755
competing for probability mass.

447
00:17:19,755 --> 00:17:22,130
And that means that even though
the symbols that we write

448
00:17:22,130 --> 00:17:26,849
on the page look very similar,
the different competitions

449
00:17:26,849 --> 00:17:28,927
among what is competing
for probability mass

450
00:17:28,927 --> 00:17:31,010
induces very different
structure that the model is

451
00:17:31,010 --> 00:17:32,890
trying to learn or uncover.

452
00:17:32,890 --> 00:17:34,670
So in the case of a
discriminative model,

453
00:17:34,670 --> 00:17:36,170
we're learning a
probabilistic model

454
00:17:36,170 --> 00:17:40,050
of y conditioned on x, which
means that for every x,

455
00:17:40,050 --> 00:17:42,690
our model is predicting a
probability distribution

456
00:17:42,690 --> 00:17:44,210
over all possible labels.

457
00:17:44,210 --> 00:17:47,370
So if our labels are discrete
and categorical, like cat

458
00:17:47,370 --> 00:17:51,060
or dog, then that means we have
a fixed amount of probability 0

459
00:17:51,060 --> 00:17:55,210
to 1, and cat and
dog must sum to 1.

460
00:17:55,210 --> 00:17:57,810
And we have a separate
probability distribution

461
00:17:57,810 --> 00:18:00,508
over the labels
for every input x.

462
00:18:00,508 --> 00:18:02,050
And crucially, notice
here that there

463
00:18:02,050 --> 00:18:05,510
is no competition among
images for probability mass.

464
00:18:05,510 --> 00:18:07,970
Because every image is
inducing its own distribution

465
00:18:07,970 --> 00:18:10,450
over the label space,
there's no competition

466
00:18:10,450 --> 00:18:13,170
for mass across the
different images.

467
00:18:13,170 --> 00:18:14,970
The only things that
are competing for mass

468
00:18:14,970 --> 00:18:16,943
are the different
labels for each image.

469
00:18:16,943 --> 00:18:18,610
And that's very
important when you think

470
00:18:18,610 --> 00:18:20,850
about discriminative modeling.

471
00:18:20,850 --> 00:18:22,730
And one interesting
other thing--

472
00:18:22,730 --> 00:18:25,810
and one interesting other facet
of discriminative modeling

473
00:18:25,810 --> 00:18:29,090
is that they have no real way
to reject unreasonable inputs.

474
00:18:29,090 --> 00:18:31,310
So once we've fixed our
label space of, say,

475
00:18:31,310 --> 00:18:33,970
cat and dog, in this example,
if we feed in something that's

476
00:18:33,970 --> 00:18:36,530
not a cat or a dog at all,
like a monkey or a piece

477
00:18:36,530 --> 00:18:39,590
of abstract art, the
system has no flexibility.

478
00:18:39,590 --> 00:18:41,790
It has no freedom to say
this is unreasonable.

479
00:18:41,790 --> 00:18:43,650
It's forced to
output a distribution

480
00:18:43,650 --> 00:18:46,490
over the fixed vocabulary that
we assigned at the beginning.

481
00:18:46,490 --> 00:18:48,828
So that maybe could be
seen as a shortcoming,

482
00:18:48,828 --> 00:18:50,370
but it's just
important to understand

483
00:18:50,370 --> 00:18:52,177
what exactly is
happening under the hood

484
00:18:52,177 --> 00:18:54,010
when you think about
modeling different data

485
00:18:54,010 --> 00:18:56,103
probabilistically.

486
00:18:56,103 --> 00:18:58,270
Now, a generative model is
something very different.

487
00:18:58,270 --> 00:19:00,395
Now, instead what we're
doing in a generative model

488
00:19:00,395 --> 00:19:02,210
is learning a
distribution p of x.

489
00:19:02,210 --> 00:19:05,770
We want to learn a distribution
over all possible images x.

490
00:19:05,770 --> 00:19:07,590
And now this is
very interesting.

491
00:19:07,590 --> 00:19:09,850
This means that all possible
images that could ever

492
00:19:09,850 --> 00:19:11,650
exist in the
universe are all now

493
00:19:11,650 --> 00:19:14,610
competing with each other
for probability mass.

494
00:19:14,610 --> 00:19:18,010
And this is now
really a hard question

495
00:19:18,010 --> 00:19:21,835
that requires-- it sounds
simple on its face,

496
00:19:21,835 --> 00:19:23,210
but this requires
you to confront

497
00:19:23,210 --> 00:19:25,870
some very deep and philosophical
problems about the world

498
00:19:25,870 --> 00:19:29,070
because now all images are
competing for probability mass.

499
00:19:29,070 --> 00:19:31,410
And in order to
model that, you're

500
00:19:31,410 --> 00:19:34,730
forced to answer
questions like, should

501
00:19:34,730 --> 00:19:36,850
an image of a three-legged
dog-- how should that

502
00:19:36,850 --> 00:19:38,530
get probability
mass in relationship

503
00:19:38,530 --> 00:19:40,187
to an image of a
three-armed monkey.

504
00:19:40,187 --> 00:19:42,770
Probably the three-legged dog
should get more probability mass

505
00:19:42,770 --> 00:19:45,187
because you can have that
happen by a dog losing a leg.

506
00:19:45,187 --> 00:19:47,270
But how are you going to
get a three-armed monkey?

507
00:19:47,270 --> 00:19:49,210
I don't know, that
seems much more rare

508
00:19:49,210 --> 00:19:51,910
unless you're modeling sci-fi
images or something like this.

509
00:19:51,910 --> 00:19:54,890
So once you're in this regime
of all possible images competing

510
00:19:54,890 --> 00:19:56,690
for probability
mass, now your model

511
00:19:56,690 --> 00:19:59,330
really needs to think very
carefully about the structure

512
00:19:59,330 --> 00:20:01,010
that it can exist
in the data, and it

513
00:20:01,010 --> 00:20:04,290
becomes a much, much
harder problem to solve.

514
00:20:04,290 --> 00:20:06,330
And another
interesting thing here

515
00:20:06,330 --> 00:20:07,990
is that now with a
generative model,

516
00:20:07,990 --> 00:20:10,770
the model now does have the
capacity to basically say,

517
00:20:10,770 --> 00:20:12,310
no, this is not a
reasonable image.

518
00:20:12,310 --> 00:20:13,710
This is not a reasonable input.

519
00:20:13,710 --> 00:20:15,127
And the way that
it can do that is

520
00:20:15,127 --> 00:20:19,090
by assigning low or even zero
probability mass to any one

521
00:20:19,090 --> 00:20:20,310
image that it gets.

522
00:20:20,310 --> 00:20:21,910
So maybe in our
generative model,

523
00:20:21,910 --> 00:20:25,015
maybe we only want it to be a
generative model of zoo animals.

524
00:20:25,015 --> 00:20:27,390
And if we want to have a
generative model of zoo animals,

525
00:20:27,390 --> 00:20:30,150
then if we feed in an
image of abstract art,

526
00:20:30,150 --> 00:20:31,910
it should have zero
probability mass.

527
00:20:31,910 --> 00:20:34,410
So now we have a mechanism
for rejecting or saying

528
00:20:34,410 --> 00:20:36,770
that this type of image is
not within the scope of what

529
00:20:36,770 --> 00:20:39,130
we care about.

530
00:20:39,130 --> 00:20:42,233
And now a conditional generative
model is even more interesting.

531
00:20:42,233 --> 00:20:44,650
So this is where we're learning
a conditional distribution

532
00:20:44,650 --> 00:20:49,450
over images x conditioned
on some label signal y.

533
00:20:49,450 --> 00:20:52,810
And now this means that
for every possible label,

534
00:20:52,810 --> 00:20:56,570
we're now inducing a competition
among all possible images.

535
00:20:56,570 --> 00:20:59,010
So in this case, if y is
say, a categorical label

536
00:20:59,010 --> 00:21:03,690
of cat and dog, now, for each
possible categorical label, cat

537
00:21:03,690 --> 00:21:06,330
and dog, the model is separately
inducing a competition

538
00:21:06,330 --> 00:21:07,970
among all possible images.

539
00:21:07,970 --> 00:21:10,370
So now in the top
distribution, maybe this

540
00:21:10,370 --> 00:21:12,890
is the probability of all
these images conditioned

541
00:21:12,890 --> 00:21:14,970
on the cat label, so then
obviously the cat image

542
00:21:14,970 --> 00:21:15,750
should be high.

543
00:21:15,750 --> 00:21:17,610
Maybe the monkey
and the dog image

544
00:21:17,610 --> 00:21:19,570
should be somewhat higher
because they're still

545
00:21:19,570 --> 00:21:20,310
mammals at least.

546
00:21:20,310 --> 00:21:22,850
But the abstract art should
be very low, maybe even 0.

547
00:21:22,850 --> 00:21:25,170
And then a different
distribution among images

548
00:21:25,170 --> 00:21:27,352
if we're conditioning
on the dog label.

549
00:21:27,352 --> 00:21:28,810
And this gets even
more interesting

550
00:21:28,810 --> 00:21:30,643
if you imagine that
your conditioning signal

551
00:21:30,643 --> 00:21:33,285
y is something much richer than
a single categorical label,

552
00:21:33,285 --> 00:21:36,080
that that conditioning
signal y might

553
00:21:36,080 --> 00:21:37,572
have been a text description.

554
00:21:37,572 --> 00:21:39,780
It might have been a whole
paragraph of written text.

555
00:21:39,780 --> 00:21:42,260
It might have been another
image plus a piece of text.

556
00:21:42,260 --> 00:21:44,920
So now once you talk about
modeling these very rich output

557
00:21:44,920 --> 00:21:48,652
spaces x, conditioned on
very rich input spaces y, now

558
00:21:48,652 --> 00:21:50,360
you're actually asking
the model to solve

559
00:21:50,360 --> 00:21:53,000
a very complicated
and quite ill-defined

560
00:21:53,000 --> 00:21:55,120
problem that requires
very deep reasoning

561
00:21:55,120 --> 00:21:57,478
about the objects involved.

562
00:21:57,478 --> 00:21:59,520
So that's why I think that
generative modeling is

563
00:21:59,520 --> 00:22:03,860
such an interesting topic
because it looks simple.

564
00:22:03,860 --> 00:22:05,540
All we did was flop
the x and the y.

565
00:22:05,540 --> 00:22:06,583
How hard could it be?

566
00:22:06,583 --> 00:22:08,000
But all of a sudden
it required us

567
00:22:08,000 --> 00:22:09,375
to think really
hard about what's

568
00:22:09,375 --> 00:22:11,560
going on in the visual world.

569
00:22:11,560 --> 00:22:13,760
And what's also interesting
is that we wrote down

570
00:22:13,760 --> 00:22:16,560
discriminative generative models
and conditional generative

571
00:22:16,560 --> 00:22:19,680
models as three separate
categories of things.

572
00:22:19,680 --> 00:22:21,793
But actually
they're all related.

573
00:22:21,793 --> 00:22:23,460
And they're related
through Bayes' rule,

574
00:22:23,460 --> 00:22:28,880
which is one of the most amazing
relationships in probability.

575
00:22:28,880 --> 00:22:33,720
And in particular, it says
that if we have access

576
00:22:33,720 --> 00:22:36,360
to a discriminative
model p y of x,

577
00:22:36,360 --> 00:22:38,920
and an unconditional generative
model p of x, as well

578
00:22:38,920 --> 00:22:41,440
as some prior distribution
over our labels y,

579
00:22:41,440 --> 00:22:44,240
we can compose those to build
a conditional generative model

580
00:22:44,240 --> 00:22:45,920
p of y given x.

581
00:22:45,920 --> 00:22:48,160
Or in general, you can
always rearrange Bayes' rule

582
00:22:48,160 --> 00:22:50,240
in some way so that if
you have any two of these

583
00:22:50,240 --> 00:22:54,780
you can always get a third
one, which is pretty cool.

584
00:22:54,780 --> 00:22:57,813
So in theory, you
can, in principle,

585
00:22:57,813 --> 00:22:59,480
build a conditional
generative model out

586
00:22:59,480 --> 00:23:00,780
of the other two components.

587
00:23:00,780 --> 00:23:03,620
Although, in practice, this
is not really how you do it.

588
00:23:03,620 --> 00:23:06,040
You tend to learn
conditional generative models

589
00:23:06,040 --> 00:23:08,557
from scratch on their own.

590
00:23:08,557 --> 00:23:10,640
Although, as we'll talk
about in diffusion, you do

591
00:23:10,640 --> 00:23:13,000
end up sometimes learning
conditional and unconditional

592
00:23:13,000 --> 00:23:14,985
models jointly for some reasons.

593
00:23:14,985 --> 00:23:16,360
But this is nice
to keep in mind,

594
00:23:16,360 --> 00:23:17,902
that there's a very
deep relationship

595
00:23:17,902 --> 00:23:20,962
across these different flavors
of probabilistic models.

596
00:23:20,962 --> 00:23:22,420
So then you might
be wondering, OK,

597
00:23:22,420 --> 00:23:24,212
what can we do with
these different flavors

598
00:23:24,212 --> 00:23:25,360
of probabilistic models.

599
00:23:25,360 --> 00:23:26,920
With discriminative
models, this one

600
00:23:26,920 --> 00:23:28,503
shouldn't require a
lot of creativity.

601
00:23:28,503 --> 00:23:31,453
We've seen a lot of these
examples so far this quarter.

602
00:23:31,453 --> 00:23:33,620
So with discriminative
models, after you train them,

603
00:23:33,620 --> 00:23:34,940
you can assign labels to data.

604
00:23:34,940 --> 00:23:36,500
You could also do
feature learning.

605
00:23:36,500 --> 00:23:39,260
In the case of say, supervised
learning on ImageNet,

606
00:23:39,260 --> 00:23:41,000
we've seen that in
the process of trying

607
00:23:41,000 --> 00:23:43,700
to predict categorical
labels of images,

608
00:23:43,700 --> 00:23:46,280
those models tend to learn
useful feature representations

609
00:23:46,280 --> 00:23:47,822
in the middle that
can be transferred

610
00:23:47,822 --> 00:23:48,860
to downstream things.

611
00:23:48,860 --> 00:23:49,598
So this is an--

612
00:23:49,598 --> 00:23:51,640
so you tend to use
discriminative models for just

613
00:23:51,640 --> 00:23:53,580
directly predicting the
y's that you care about,

614
00:23:53,580 --> 00:23:55,760
or for learning feature
representations that are induced

615
00:23:55,760 --> 00:23:57,677
in the process of trying
to predict those y's.

617
00:24:00,380 --> 00:24:02,280
These unconditional
generative models I think

618
00:24:02,280 --> 00:24:04,980
are actually useless in general.

619
00:24:04,980 --> 00:24:08,360
But what they let you do
is maybe detect outliers.

620
00:24:08,360 --> 00:24:11,400
They look at images and say,
are they really-- do they really

621
00:24:11,400 --> 00:24:12,500
have low probability mass?

622
00:24:12,500 --> 00:24:15,260
Are they unreasonable images?

623
00:24:15,260 --> 00:24:18,020
You can use them for feature
learning without data,

624
00:24:18,020 --> 00:24:19,160
without labels.

625
00:24:19,160 --> 00:24:20,880
So hope that in the
process of trying

626
00:24:20,880 --> 00:24:23,180
to fit an unconditional
distribution p of x,

627
00:24:23,180 --> 00:24:25,960
the model maybe learn some
useful feature representations.

628
00:24:25,960 --> 00:24:27,840
Although in general these
have not been super successful

629
00:24:27,840 --> 00:24:29,680
for self-supervised
learning, and typically,

630
00:24:29,680 --> 00:24:30,960
the contrastive
methods that we've

631
00:24:30,960 --> 00:24:32,240
talked about in the
previous lecture

632
00:24:32,240 --> 00:24:34,280
have, in practice, been
much more successful

633
00:24:34,280 --> 00:24:36,280
for self-supervised
learning as compared

634
00:24:36,280 --> 00:24:39,560
to unconditional
density estimation.

635
00:24:39,560 --> 00:24:42,320
Or in principle, you could use
this unconditional generative

636
00:24:42,320 --> 00:24:44,800
model to sample and
produce new samples x.

637
00:24:44,800 --> 00:24:46,960
But I think this
is actually useless

638
00:24:46,960 --> 00:24:49,960
because it gives you no control
over what is being sampled.

639
00:24:49,960 --> 00:24:52,300
If you have an unconditional
generative model of images,

640
00:24:52,300 --> 00:24:54,258
then you can sample from
it to get a new image,

641
00:24:54,258 --> 00:24:57,040
but you have no control
over what's in that image.

642
00:24:57,040 --> 00:25:00,280
So I think it's mathematically
interesting to think about how

643
00:25:00,280 --> 00:25:02,160
to build such
models, but I don't

644
00:25:02,160 --> 00:25:05,200
think they have as much
practical significance.

645
00:25:05,200 --> 00:25:08,160
And conditional generative
models are, I think,

646
00:25:08,160 --> 00:25:10,098
are the most useful and
the most interesting.

647
00:25:10,098 --> 00:25:12,640
And these are the generative
models that get trained and used

648
00:25:12,640 --> 00:25:14,840
in practice by far the most.

649
00:25:14,840 --> 00:25:17,120
So you can, in principle,
use them to assign labels

650
00:25:17,120 --> 00:25:18,660
while rejecting outliers.

651
00:25:18,660 --> 00:25:21,420
You could say, if I
have a piece of data x,

652
00:25:21,420 --> 00:25:25,280
then look at the p of x given y
over all of my possible y's, and

653
00:25:25,280 --> 00:25:28,200
then I could reject if
that's too low among all

654
00:25:28,200 --> 00:25:29,500
the possible whys.

655
00:25:29,500 --> 00:25:32,080
So in principle, you could use
conditional generative models

656
00:25:32,080 --> 00:25:35,000
to do some classification while
also maintaining the ability

657
00:25:35,000 --> 00:25:36,380
to reject outliers.

658
00:25:36,380 --> 00:25:37,880
Although I don't
think that's really

659
00:25:37,880 --> 00:25:39,640
used too much in practice.

660
00:25:39,640 --> 00:25:42,520
What's really useful for
conditional generative models

661
00:25:42,520 --> 00:25:44,540
and what is used in
practice all the time,

662
00:25:44,540 --> 00:25:46,800
everywhere, is sampling
to generate new data

663
00:25:46,800 --> 00:25:48,360
from labels where
you actually get

664
00:25:48,360 --> 00:25:50,200
to control what is generated.

665
00:25:50,200 --> 00:25:52,600
Because if your y is now
maybe a piece of text,

666
00:25:52,600 --> 00:25:54,240
you can write
down, I want to see

667
00:25:54,240 --> 00:25:58,838
a cat wearing a hot dog flavored
t-shirt on the moon or whatever,

668
00:25:58,838 --> 00:26:00,880
and then your favorite
generative model of images

669
00:26:00,880 --> 00:26:03,360
will now generate you a
brand new image x conditioned

670
00:26:03,360 --> 00:26:04,513
on that label y.

671
00:26:04,513 --> 00:26:07,180
So this is where I think all the
juices, where all the magic is,

672
00:26:07,180 --> 00:26:09,440
where all the excitement is.

673
00:26:09,440 --> 00:26:11,700
Although somewhat confusingly,
in the literature,

674
00:26:11,700 --> 00:26:13,920
whenever you see the
term generative model,

675
00:26:13,920 --> 00:26:16,960
they mush up between
unconditional and conditional

676
00:26:16,960 --> 00:26:17,898
generative modeling.

677
00:26:17,898 --> 00:26:19,440
And a lot of the
papers that you read

678
00:26:19,440 --> 00:26:21,718
will even sometimes drop
the conditioning signal y

679
00:26:21,718 --> 00:26:23,760
because it makes the math
look cleaner-- it makes

680
00:26:23,760 --> 00:26:25,360
the equations look cleaner.

681
00:26:25,360 --> 00:26:27,200
But I don't think
unconditional generative

682
00:26:27,200 --> 00:26:28,437
modeling is super useful.

683
00:26:28,437 --> 00:26:30,520
It's almost always conditional
generative modeling

684
00:26:30,520 --> 00:26:33,200
that you really want
to do in most cases.

685
00:26:33,200 --> 00:26:37,140
So just be aware that if you
read papers, see equations,

686
00:26:37,140 --> 00:26:39,880
hear people talk, when they
talk about generative modeling,

687
00:26:39,880 --> 00:26:42,400
they're probably the one they
care about training more are

688
00:26:42,400 --> 00:26:44,200
these conditional generative
models even if the math

689
00:26:44,200 --> 00:26:46,720
doesn't-- even if the equations
or notation doesn't reflect

690
00:26:46,720 --> 00:26:47,920
that.

691
00:26:47,920 --> 00:26:50,880
So for unconditional
generative model,

692
00:26:50,880 --> 00:26:53,920
what are the inputs and outputs?

693
00:26:53,920 --> 00:26:55,680
So I didn't really
tell you that.

694
00:26:55,680 --> 00:26:58,800
And I was sneaky there
because how you parameterize

695
00:26:58,800 --> 00:27:00,060
that actually depends a lot.

696
00:27:00,060 --> 00:27:01,280
There's a lot of
different formulations

697
00:27:01,280 --> 00:27:02,420
for all of these things.

698
00:27:02,420 --> 00:27:04,360
And what exactly are they going
to be the inputs and the outputs

699
00:27:04,360 --> 00:27:06,610
of the network are going to
vary quite a lot depending

700
00:27:06,610 --> 00:27:07,520
on the formulation.

701
00:27:07,520 --> 00:27:09,480
So we're going to talk about
a whole taxonomy of those

702
00:27:09,480 --> 00:27:10,272
in a couple slides.

704
00:27:12,786 --> 00:27:15,160
So why generative models?

705
00:27:15,160 --> 00:27:17,400
The main reason you want
to build generative models

706
00:27:17,400 --> 00:27:19,840
is whenever there's some
ambiguity in the task you're

707
00:27:19,840 --> 00:27:21,440
trying to model.

708
00:27:21,440 --> 00:27:25,400
So the beauty of a probabilistic
model p of x given y

709
00:27:25,400 --> 00:27:27,150
is that it's probabilistic.

710
00:27:27,150 --> 00:27:30,630
There might be a whole space of
possible outputs x conditioned

711
00:27:30,630 --> 00:27:32,150
on that input label y.

712
00:27:32,150 --> 00:27:34,612
So whenever there's not--

713
00:27:34,612 --> 00:27:36,570
sometimes there's just
a deterministic mapping.

714
00:27:36,570 --> 00:27:37,790
I look at an image,
I want to ask

715
00:27:37,790 --> 00:27:39,090
how many cats are in the image.

716
00:27:39,090 --> 00:27:40,090
There's just three cats.

717
00:27:40,090 --> 00:27:41,270
There's just one answer.

718
00:27:41,270 --> 00:27:43,130
But in a lot of cases,
it's more subtle.

719
00:27:43,130 --> 00:27:46,615
If I ask for a picture of a
dog wearing a hot dog hat,

720
00:27:46,615 --> 00:27:47,990
there's a lot of
different images

721
00:27:47,990 --> 00:27:49,870
that could exist
based on that query.

722
00:27:49,870 --> 00:27:51,362
There's uncertainty
in the output.

723
00:27:51,362 --> 00:27:53,070
And that's exactly
what generative models

724
00:27:53,070 --> 00:27:53,903
are trying to model.

725
00:27:53,903 --> 00:27:56,153
They model a whole distribution
of outputs conditioned

726
00:27:56,153 --> 00:27:57,110
on their input signal.

727
00:27:57,110 --> 00:27:59,910
So anytime that there's
ambiguity in the output

728
00:27:59,910 --> 00:28:01,870
that you want the model
to produce conditioned

729
00:28:01,870 --> 00:28:03,328
on the input, that's
where you want

730
00:28:03,328 --> 00:28:04,910
to turn to a generative model.

731
00:28:04,910 --> 00:28:05,690
And this is why--

732
00:28:05,690 --> 00:28:08,230
And this is-- we'll see a
couple examples of where

733
00:28:08,230 --> 00:28:10,870
this has gotten used a lot
in the last couple of years.

734
00:28:10,870 --> 00:28:12,590
So one example is
language modeling.

735
00:28:12,590 --> 00:28:14,498
Someone asked about
ChatGPT a moment ago.

736
00:28:14,498 --> 00:28:16,790
So in language modeling, what
you're often trying to do

737
00:28:16,790 --> 00:28:20,750
is predict output text
x from input text y.

738
00:28:20,750 --> 00:28:23,870
Sorry, the x's and y's ended
up flipped in an awkward way

739
00:28:23,870 --> 00:28:26,950
on this example.

740
00:28:26,950 --> 00:28:29,230
So here's an example
from ChatGPT.

741
00:28:29,230 --> 00:28:30,830
The input is write
me a short rhyming

742
00:28:30,830 --> 00:28:32,370
poem about generative models.

743
00:28:32,370 --> 00:28:34,067
And wow, it actually works.

744
00:28:34,067 --> 00:28:34,650
This is crazy.

745
00:28:34,650 --> 00:28:37,248
This didn't work at all when
we first taught this class.

746
00:28:37,248 --> 00:28:38,290
I'm not going to read it.

747
00:28:38,290 --> 00:28:39,415
That would be embarrassing.

748
00:28:39,415 --> 00:28:41,097
You can read it yourself.

749
00:28:41,097 --> 00:28:42,930
Now, this is a conditional
generative model.

750
00:28:42,930 --> 00:28:45,472
You could imagine there's a lot
of different possible rhyming

751
00:28:45,472 --> 00:28:48,550
poems about generative
models that one might write,

752
00:28:48,550 --> 00:28:49,842
and we had to pick one of them.

753
00:28:49,842 --> 00:28:51,342
And the beauty of
a generative model

754
00:28:51,342 --> 00:28:53,750
is that it, in principle,
models that whole distribution

755
00:28:53,750 --> 00:28:57,070
over possible outputs
conditioned on that input.

756
00:28:57,070 --> 00:28:58,455
Or text to image.

757
00:28:58,455 --> 00:28:59,830
Make me an image
showing a person

758
00:28:59,830 --> 00:29:02,590
teaching a class on generative
models in front of a whiteboard.

759
00:29:02,590 --> 00:29:04,770
You're looking at one
example through your eyes.

760
00:29:04,770 --> 00:29:06,590
ChatGPT gave you a
different example.

761
00:29:06,590 --> 00:29:08,830
There's a whole different
space of possible images

762
00:29:08,830 --> 00:29:11,255
that might match
this input text.

763
00:29:11,255 --> 00:29:12,630
And a generative
model allows you

764
00:29:12,630 --> 00:29:14,797
to model that whole space
and sample from that space

765
00:29:14,797 --> 00:29:17,070
depending on what you want.

766
00:29:17,070 --> 00:29:18,590
Or image to video.

767
00:29:18,590 --> 00:29:21,110
Input an image,
what happens next?

768
00:29:21,110 --> 00:29:24,170
This was me holding my
AirPods over a cardboard box.

769
00:29:24,170 --> 00:29:25,330
Maybe I'm going to drop it.

770
00:29:25,330 --> 00:29:26,730
Maybe I'm going to move my hand.

771
00:29:26,730 --> 00:29:29,310
Maybe I'm going to move
my hand and the AirPods

772
00:29:29,310 --> 00:29:31,228
will morph into a
different AirPods.

773
00:29:31,228 --> 00:29:32,770
There's all things
that could happen.

774
00:29:32,770 --> 00:29:34,010
A generative model,
in principle,

775
00:29:34,010 --> 00:29:36,260
lets you model and sample
from these possible futures.

777
00:29:39,910 --> 00:29:42,392
So this is why we want to care
about generative modeling.

778
00:29:42,392 --> 00:29:44,350
Anytime there's ambiguity
in the output, that's

779
00:29:44,350 --> 00:29:48,430
when you want to try to turn to
a generative model to solve it.

780
00:29:48,430 --> 00:29:50,770
And someone asked about what
are the inputs and outputs.

781
00:29:50,770 --> 00:29:52,790
It turns out this
is a huge field.

782
00:29:52,790 --> 00:29:55,270
And this is surprisingly
one field of deep learning

783
00:29:55,270 --> 00:29:57,630
that is quite mathematical
because it requires

784
00:29:57,630 --> 00:29:59,590
thinking about what are
different ways to model

785
00:29:59,590 --> 00:30:01,130
probability distributions?

786
00:30:01,130 --> 00:30:02,630
How can we write
down loss functions

787
00:30:02,630 --> 00:30:05,010
that cause the right
things to happen?

788
00:30:05,010 --> 00:30:06,590
So this is one
area where when you

789
00:30:06,590 --> 00:30:08,410
read papers there
may be a lot of math,

790
00:30:08,410 --> 00:30:09,830
there may be a lot of equations.

791
00:30:09,830 --> 00:30:12,670
And you actually might need to
think through those equations

792
00:30:12,670 --> 00:30:14,730
pretty carefully to
understand what's going on.

793
00:30:14,730 --> 00:30:17,470
So this is one subfield
that tends to have more math

794
00:30:17,470 --> 00:30:19,910
and more equations,
which I think is fun--

795
00:30:19,910 --> 00:30:20,788
interesting.

796
00:30:20,788 --> 00:30:22,830
So there's this whole
taxonomy of different kinds

797
00:30:22,830 --> 00:30:24,750
of generative models
that people build.

798
00:30:24,750 --> 00:30:26,510
So on the one hand,
you can imagine

799
00:30:26,510 --> 00:30:30,110
one part of the family tree or
what we call explicit density

800
00:30:30,110 --> 00:30:30,930
methods.

801
00:30:30,930 --> 00:30:33,163
These are ones where the
model actually does--

802
00:30:33,163 --> 00:30:34,830
the whole thing is
we're trying to model

803
00:30:34,830 --> 00:30:36,750
p of x or p of x given y.

804
00:30:36,750 --> 00:30:38,590
And with these explicit
density methods,

805
00:30:38,590 --> 00:30:40,910
you can actually compute,
you can get that value out,

806
00:30:40,910 --> 00:30:43,350
p of x, for any sample x.

807
00:30:43,350 --> 00:30:46,565
And the counterpoint are
implicit density methods.

808
00:30:46,565 --> 00:30:48,190
These are ones where
you can't actually

809
00:30:48,190 --> 00:30:50,470
get that probability mass
value-- you can't actually

810
00:30:50,470 --> 00:30:53,290
get that density value p
of x out from the model,

811
00:30:53,290 --> 00:30:56,650
but you can somehow sample from
that probability distribution.

812
00:30:56,650 --> 00:31:00,150
So the difference here is
that in an implicit model,

813
00:31:00,150 --> 00:31:03,418
you can't actually access the
value of the density function,

814
00:31:03,418 --> 00:31:05,710
but you can sample from the
underlying density function

815
00:31:05,710 --> 00:31:06,290
somehow.

816
00:31:06,290 --> 00:31:07,790
So the model has
implicitly learned

817
00:31:07,790 --> 00:31:10,590
to model the density even if
you can't get the value out.

818
00:31:10,590 --> 00:31:12,550
And on the explicit
density side,

819
00:31:12,550 --> 00:31:15,870
it's almost the opposite
where in many cases--

820
00:31:15,870 --> 00:31:19,057
you can get that explicit
density value out,

821
00:31:19,057 --> 00:31:21,390
but then sampling tends to
be more complicated sometimes

822
00:31:21,390 --> 00:31:23,090
with these explicit
density methods.

823
00:31:23,090 --> 00:31:25,270
Not always, but sometimes.

824
00:31:25,270 --> 00:31:27,670
And the reason why you might
turn to implicit models

825
00:31:27,670 --> 00:31:29,830
is because, in many cases,
you may not actually

826
00:31:29,830 --> 00:31:32,510
care about knowing what
exactly was the density

827
00:31:32,510 --> 00:31:33,950
value for any input.

828
00:31:33,950 --> 00:31:35,990
Maybe all you care about
is generating samples

829
00:31:35,990 --> 00:31:39,967
and get generating a good
diversity of samples.

830
00:31:39,967 --> 00:31:42,050
So if the thing you really
care about is sampling,

831
00:31:42,050 --> 00:31:43,467
then maybe you
don't actually need

832
00:31:43,467 --> 00:31:45,230
to explicitly be
able to see the value

833
00:31:45,230 --> 00:31:48,270
of the density for any input.

834
00:31:48,270 --> 00:31:50,270
And then things break
down and cascade

835
00:31:50,270 --> 00:31:52,110
and get more fractal
like from here.

836
00:31:52,110 --> 00:31:54,395
So inside explicit
density methods,

837
00:31:54,395 --> 00:31:55,770
there's ones where
actually yeah,

838
00:31:55,770 --> 00:31:58,630
you can really compute the real
p of x that's being modeled.

839
00:31:58,630 --> 00:32:01,710
And autoregressive models
are one example of that.

840
00:32:01,710 --> 00:32:04,590
Or another version of
explicit density methods

841
00:32:04,590 --> 00:32:07,087
are ones where you can
get a density value out,

842
00:32:07,087 --> 00:32:08,170
but it's not the real one.

843
00:32:08,170 --> 00:32:11,710
It's some approximation to
the true density of the data.

844
00:32:11,710 --> 00:32:13,870
And variational
autoencoders are one example

845
00:32:13,870 --> 00:32:16,630
of an explicit but
approximate generative method

846
00:32:16,630 --> 00:32:18,190
that we'll see.

847
00:32:18,190 --> 00:32:20,830
Now, on the other branch
of the family tree,

848
00:32:20,830 --> 00:32:23,338
we can think about direct
methods for implicit density.

849
00:32:23,338 --> 00:32:25,630
These are ones where maybe
it requires a single network

850
00:32:25,630 --> 00:32:30,110
evaluation to just draw a sample
from the underlying distribution

851
00:32:30,110 --> 00:32:31,090
that's being modeled.

852
00:32:31,090 --> 00:32:32,590
And a generative
adversarial network

853
00:32:32,590 --> 00:32:35,173
is an example of a generative
model in this part of the family

854
00:32:35,173 --> 00:32:35,870
tree.

855
00:32:35,870 --> 00:32:36,910
And the other part is--

856
00:32:36,910 --> 00:32:39,705
I don't know if it has a good
name, I called it indirect.

857
00:32:39,705 --> 00:32:41,330
But this is a name
I made up yesterday,

858
00:32:41,330 --> 00:32:43,390
so please feel free to
correct me if there's

859
00:32:43,390 --> 00:32:44,830
a better term for this.

860
00:32:44,830 --> 00:32:46,950
But these indirect
ones are ones where

861
00:32:46,950 --> 00:32:50,910
you can sample from
the underlying density

862
00:32:50,910 --> 00:32:52,750
p of x that's being
modeled, but it requires

863
00:32:52,750 --> 00:32:54,330
some iterative procedure.

864
00:32:54,330 --> 00:32:57,310
There's no feed forward function
that you can input and get

865
00:32:57,310 --> 00:32:58,310
the sample directly out.

866
00:32:58,310 --> 00:32:59,893
There's some iterative
method that you

867
00:32:59,893 --> 00:33:01,590
need to run in order
to draw a sample

868
00:33:01,590 --> 00:33:03,652
from the underlying density
that's being modeled.

869
00:33:03,652 --> 00:33:05,110
And diffusion models
are an example

870
00:33:05,110 --> 00:33:06,680
of this that we'll
see next time.

871
00:33:06,680 --> 00:33:08,430
I told you a couple
slides ago that people

872
00:33:08,430 --> 00:33:10,207
are sloppy with
notation and drop the y,

873
00:33:10,207 --> 00:33:12,290
and I did that explicitly
on purpose on this slide

874
00:33:12,290 --> 00:33:13,730
so that someone would
ask me that question,

875
00:33:13,730 --> 00:33:15,750
and you would always be
attentive to that fact.

876
00:33:15,750 --> 00:33:16,677
So yes, exactly.

877
00:33:16,677 --> 00:33:18,510
Every time I've written
p of x on this slide

878
00:33:18,510 --> 00:33:21,040
and actually all the rest
of the slides this lecture--

879
00:33:21,040 --> 00:33:22,960
I also have been
lazy and drop the y,

880
00:33:22,960 --> 00:33:25,740
but you should always imagine
an additional conditioned on y

881
00:33:25,740 --> 00:33:28,460
in all of these p of x that you
see for the rest of the lecture.

882
00:33:28,460 --> 00:33:30,300
So thank you for asking that.

883
00:33:30,300 --> 00:33:34,180
So the question was,
can you just treat

884
00:33:34,180 --> 00:33:36,700
that indirect iterative
procedure as a black box

885
00:33:36,700 --> 00:33:39,580
and then treat that as a
direct sampling method?

886
00:33:39,580 --> 00:33:40,800
In principle, yes.

887
00:33:40,800 --> 00:33:43,660
But in practice no,
because your samples

888
00:33:43,660 --> 00:33:47,600
end up approximate depending
on exactly the method.

889
00:33:47,600 --> 00:33:49,713
But with diffusion
models, you would

890
00:33:49,713 --> 00:33:51,380
need to take an
infinite number of steps

891
00:33:51,380 --> 00:33:54,020
in order to draw a true sample,
so instead we approximate that

892
00:33:54,020 --> 00:33:55,520
with a finite number of steps.

893
00:33:55,520 --> 00:33:57,360
And that's true of
other methods as well.

894
00:33:57,360 --> 00:33:59,940
Diffusion models are the
most common for this today.

895
00:33:59,940 --> 00:34:03,340
But some Markov chain method
or MCMC method in years past

896
00:34:03,340 --> 00:34:05,220
might have also had this
property where there

897
00:34:05,220 --> 00:34:06,520
is an iterative procedure.

898
00:34:06,520 --> 00:34:08,699
But if you want to draw an exact
sample from the distribution

899
00:34:08,699 --> 00:34:10,540
that's being modeled, you need
an infinite number of steps

900
00:34:10,540 --> 00:34:11,580
to converge.

901
00:34:11,580 --> 00:34:16,820
So we always approximate that by
taking a finite number of steps.

902
00:34:16,820 --> 00:34:20,380
And I was pretty
proud of this taxonomy

903
00:34:20,380 --> 00:34:22,159
because it's very symmetric.

904
00:34:22,159 --> 00:34:25,360
There's four leaves.

905
00:34:25,360 --> 00:34:26,397
There's two branches.

906
00:34:26,397 --> 00:34:28,980
And we're going to cover half
the tree today and half the tree

907
00:34:28,980 --> 00:34:29,860
next time.

908
00:34:29,860 --> 00:34:32,277
So I thought that was a pretty
nice pretty nice breakdown.

909
00:34:32,277 --> 00:34:33,860
The question is,
what's the difference

910
00:34:33,860 --> 00:34:36,739
between the approximate
density and directly sampling

911
00:34:36,739 --> 00:34:38,659
from an implicit p of x.

912
00:34:38,659 --> 00:34:42,040
The difference is that in an
indirect but implicit method,

913
00:34:42,040 --> 00:34:44,102
there's no density value
anywhere to be found.

914
00:34:44,102 --> 00:34:46,060
You can't compute one at
all, but you can still

915
00:34:46,060 --> 00:34:47,900
iteratively sample in some way.

916
00:34:47,900 --> 00:34:49,677
With an approximate
density method,

917
00:34:49,677 --> 00:34:52,260
you can still get a value out--
you can actually get a density

918
00:34:52,260 --> 00:34:54,552
value out, that's going to
be some approximate or bound

919
00:34:54,552 --> 00:34:55,760
to the true p of x.

921
00:34:59,020 --> 00:35:01,583
So then the first
such generative model

922
00:35:01,583 --> 00:35:03,500
that we'll actually talk
about in a little bit

923
00:35:03,500 --> 00:35:07,632
more concrete specificity
are autoregressive models.

924
00:35:07,632 --> 00:35:09,340
So autoregressive
models-- we're actually

925
00:35:09,340 --> 00:35:10,923
going to take a
slight detour and talk

926
00:35:10,923 --> 00:35:13,203
about a really general
idea behind all

927
00:35:13,203 --> 00:35:14,620
of generative
modeling, and that's

928
00:35:14,620 --> 00:35:17,060
the idea of maximum
likelihood estimation.

929
00:35:17,060 --> 00:35:18,598
And maximum
likelihood estimation

930
00:35:18,598 --> 00:35:20,140
is actually a quite
general procedure

931
00:35:20,140 --> 00:35:22,620
that we can use to fit
probabilistic models given

932
00:35:22,620 --> 00:35:24,153
a finite set of samples.

933
00:35:24,153 --> 00:35:25,820
So the idea is we're
going to write down

934
00:35:25,820 --> 00:35:28,060
some explicit function
for the density.

935
00:35:28,060 --> 00:35:32,060
We said that some methods
are going to explicitly model

936
00:35:32,060 --> 00:35:32,880
the density.

937
00:35:32,880 --> 00:35:34,680
Well, let's do it
with a neural network.

938
00:35:34,680 --> 00:35:36,138
Let's write a neural
network that's

939
00:35:36,138 --> 00:35:39,780
going to input the data
x, input the weights,

940
00:35:39,780 --> 00:35:41,235
W, of the neural
network, and it's

941
00:35:41,235 --> 00:35:42,860
going to spit out a
number that's going

942
00:35:42,860 --> 00:35:45,540
to tell us what is the density.

943
00:35:45,540 --> 00:35:47,820
So then, we're going
to train the data--

944
00:35:47,820 --> 00:35:50,460
given a data set of
samples x1, x2, xN,

945
00:35:50,460 --> 00:35:53,360
we're going to train the model
via this objective function.

946
00:35:53,360 --> 00:35:58,338
We want to find the weights that
make the dataset most likely.

947
00:35:58,338 --> 00:36:00,880
Where we want to [? accept ?]
because as we vary the weights,

948
00:36:00,880 --> 00:36:02,660
it's going to vary
the densities that are

949
00:36:02,660 --> 00:36:04,000
being modeled by the network.

950
00:36:04,000 --> 00:36:07,580
So we want the network to select
the density that maximizes

951
00:36:07,580 --> 00:36:09,420
the likelihood of the data.

952
00:36:09,420 --> 00:36:12,140
Note that we said likelihood
rather than probability,

953
00:36:12,140 --> 00:36:15,820
that's a deep philosophical
rabbit hole you can fall into.

954
00:36:15,820 --> 00:36:17,420
The difference is
what we're varying.

955
00:36:17,420 --> 00:36:18,878
If you think about
probability, you

956
00:36:18,878 --> 00:36:21,020
imagine that the
density is fixed

957
00:36:21,020 --> 00:36:23,160
and we're sliding x
around, and changing

958
00:36:23,160 --> 00:36:25,760
what is the probability of x
under a fixed distribution.

959
00:36:25,760 --> 00:36:27,860
When you talk about
likelihood, instead, you're

960
00:36:27,860 --> 00:36:29,940
often fixing the
samples x and you're

961
00:36:29,940 --> 00:36:34,004
varying the distribution
itself and saying,

962
00:36:34,004 --> 00:36:37,100
how does the probability
density of those samples

963
00:36:37,100 --> 00:36:39,512
change as we vary
different distributions.

964
00:36:39,512 --> 00:36:41,220
So you have to think
about very carefully

965
00:36:41,220 --> 00:36:43,660
in these equations what's
being fixed and what's varying.

966
00:36:43,660 --> 00:36:46,460
So in this process of maximum
likelihood estimation, what

967
00:36:46,460 --> 00:36:49,340
we're doing is varying
the distribution

968
00:36:49,340 --> 00:36:52,580
that the neural network is
modeling to try to maximize

969
00:36:52,580 --> 00:36:54,380
the probability of the
fixed set of samples

970
00:36:54,380 --> 00:36:57,280
we have from that distribution
in our training set.

971
00:36:57,280 --> 00:36:59,380
And I guess the unsaid
thing behind all of this

972
00:36:59,380 --> 00:37:02,660
is that we assume that there is
some underlying true probability

973
00:37:02,660 --> 00:37:05,900
distribution p data, which
was used by the universe

974
00:37:05,900 --> 00:37:08,260
to generate the data
that we are seeing.

975
00:37:08,260 --> 00:37:10,340
And in some sense,
always what we want to do

976
00:37:10,340 --> 00:37:13,500
is try to model that true
underlying unknown distribution

977
00:37:13,500 --> 00:37:14,360
p data.

978
00:37:14,360 --> 00:37:16,820
And we can never access
p data because we

979
00:37:16,820 --> 00:37:19,340
don't have this omniscient view
of exactly how the universe

980
00:37:19,340 --> 00:37:22,940
works, but instead we get
some samples from p data

981
00:37:22,940 --> 00:37:24,880
that the universe
has given to us.

982
00:37:24,880 --> 00:37:27,340
And what we're trying to do
through our learning procedure

983
00:37:27,340 --> 00:37:29,515
is uncover that
unknown distribution p

984
00:37:29,515 --> 00:37:31,900
data given a finite
number of samples

985
00:37:31,900 --> 00:37:33,740
from that unknown distribution.

986
00:37:33,740 --> 00:37:35,900
So one procedure that
you can go about this

987
00:37:35,900 --> 00:37:38,340
is like, well, let's select
the distribution that

988
00:37:38,340 --> 00:37:41,180
makes the data that I
saw actually most likely,

989
00:37:41,180 --> 00:37:45,645
and that's the maximum
likelihood objective function.

990
00:37:45,645 --> 00:37:47,020
And then a standard
trick that we

991
00:37:47,020 --> 00:37:50,140
do here, we assume that the
data was IID, independent

992
00:37:50,140 --> 00:37:51,440
and identically distributed.

993
00:37:51,440 --> 00:37:53,460
So we assume that each
one of those x's was

994
00:37:53,460 --> 00:37:55,417
drawn from that true
p data distribution.

995
00:37:55,417 --> 00:37:57,500
And now we want to maximize
the joint distribution

996
00:37:57,500 --> 00:37:58,800
of all the data that we saw.

997
00:37:58,800 --> 00:38:01,300
But because it's independent,
we can factor it down

998
00:38:01,300 --> 00:38:05,220
into independent likelihood of
each of the independent samples.

999
00:38:05,220 --> 00:38:07,880
And then the common trick that
we always use is the log trick.

1000
00:38:07,880 --> 00:38:10,040
So that log is a
monotonic function.

1001
00:38:10,040 --> 00:38:12,700
So if you maximize
something, its equivalent

1002
00:38:12,700 --> 00:38:15,300
to maximizing the log of
that something because log

1003
00:38:15,300 --> 00:38:16,642
is a monotonic function.

1004
00:38:16,642 --> 00:38:18,100
And the log is also
very convenient

1005
00:38:18,100 --> 00:38:20,240
because it swaps
sums and products.

1006
00:38:20,240 --> 00:38:21,100
So it's common to--

1007
00:38:21,100 --> 00:38:25,695
instead of maximizing the
likelihood of the data,

1008
00:38:25,695 --> 00:38:28,320
instead we're going to maximize
the log likelihood of the data,

1009
00:38:28,320 --> 00:38:30,362
and that's the same as
maximizing the likelihood.

1010
00:38:30,362 --> 00:38:33,880
And once we apply the log, then
that product splits into a sum,

1011
00:38:33,880 --> 00:38:36,340
and sums are easier to handle.

1012
00:38:36,340 --> 00:38:38,660
And now we slot in
our neural network

1013
00:38:38,660 --> 00:38:40,660
because our neural network
is now maybe directly

1014
00:38:40,660 --> 00:38:41,712
outputting the density.

1015
00:38:41,712 --> 00:38:43,420
So this gives a direct
objective function

1016
00:38:43,420 --> 00:38:46,148
that we could use to
train a neural network.

1017
00:38:46,148 --> 00:38:47,940
This gives it a very
concrete loss function

1018
00:38:47,940 --> 00:38:50,023
that we can use to train
a neural network to solve

1019
00:38:50,023 --> 00:38:52,147
this generative
modeling problem.

1020
00:38:52,147 --> 00:38:53,980
But we need a little
bit more structure here

1021
00:38:53,980 --> 00:38:56,900
to actually make progress.

1022
00:38:56,900 --> 00:38:59,140
So this idea of maximum
likelihood estimation

1023
00:38:59,140 --> 00:38:59,960
is very general.

1024
00:38:59,960 --> 00:39:02,135
It doesn't really assume
anything about the data.

1025
00:39:02,135 --> 00:39:04,260
It doesn't really assume
any structure in the data.

1026
00:39:04,260 --> 00:39:06,677
And in general, we need to put
a little bit more structure

1027
00:39:06,677 --> 00:39:08,500
on this to make some progress.

1028
00:39:08,500 --> 00:39:11,060
So autoregressive models
basically make the assumption

1029
00:39:11,060 --> 00:39:14,500
that there is some canonical
way that we can take our data x

1030
00:39:14,500 --> 00:39:19,140
and split each data sample x
into some sequence of subparts,

1031
00:39:19,140 --> 00:39:20,820
x1, x2, xT.

1032
00:39:20,820 --> 00:39:22,600
You got to be careful
with indices here.

1033
00:39:22,600 --> 00:39:24,080
Here I said the subparts.

1034
00:39:24,080 --> 00:39:26,380
These are subparts
of a single sample.

1035
00:39:26,380 --> 00:39:27,680
So I use a subscript.

1036
00:39:27,680 --> 00:39:29,620
In the previous slide,
we had superscript

1037
00:39:29,620 --> 00:39:32,980
to indicate different
samples x1 to xN.

1038
00:39:32,980 --> 00:39:34,580
So be careful with that.

1039
00:39:34,580 --> 00:39:37,340
Superscript on this slide
is different samples x.

1040
00:39:37,340 --> 00:39:39,220
Subscript on this slide
means different parts

1041
00:39:39,220 --> 00:39:40,467
of the same sample.

1042
00:39:40,467 --> 00:39:42,300
So we assume that there's
some canonical way

1043
00:39:42,300 --> 00:39:46,500
to break up our data sample x
into some sequence of subparts.

1044
00:39:46,500 --> 00:39:48,820
And now we can apply the
chain rule of probability.

1045
00:39:48,820 --> 00:39:52,220
So probability of x is
just the joint probability

1046
00:39:52,220 --> 00:39:55,580
of all of the subparts x1 to xT.

1047
00:39:55,580 --> 00:39:57,640
And then given any
probability distribution,

1048
00:39:57,640 --> 00:40:00,180
you can always break it
apart into this chain rule

1049
00:40:00,180 --> 00:40:03,640
that the joint distribution
of all these variables

1050
00:40:03,640 --> 00:40:06,220
is equal to probability
of the first one times

1051
00:40:06,220 --> 00:40:08,480
probability of the first given--

1052
00:40:08,480 --> 00:40:09,970
conditioned on the first--

1053
00:40:09,970 --> 00:40:12,230
probability of the second
condition on the first times

1054
00:40:12,230 --> 00:40:13,730
probability of the
third conditioned

1055
00:40:13,730 --> 00:40:16,910
on the first and the second, et
cetera, et cetera, et cetera.

1056
00:40:16,910 --> 00:40:19,070
And this is the chain
rule of probability.

1057
00:40:19,070 --> 00:40:20,310
This requires no assumptions.

1058
00:40:20,310 --> 00:40:22,570
This is always true of
any joint distribution

1059
00:40:22,570 --> 00:40:24,970
of random variables.

1060
00:40:24,970 --> 00:40:28,510
And then this gives us
our objective function.

1061
00:40:28,510 --> 00:40:31,170
Then you could basically
train a neural network

1062
00:40:31,170 --> 00:40:35,228
that's going to input the
previous part of the sequence

1063
00:40:35,228 --> 00:40:36,770
and then try to give
us a probability

1064
00:40:36,770 --> 00:40:39,290
distribution over the
next part of the sequence.

1065
00:40:39,290 --> 00:40:40,610
Does that sound familiar?

1066
00:40:40,610 --> 00:40:43,170
Does that sound like
something we've done before?

1067
00:40:43,170 --> 00:40:44,310
RNNs, yes.

1068
00:40:44,310 --> 00:40:46,330
So that's exactly
what an RNN is doing.

1069
00:40:46,330 --> 00:40:48,290
An RNN has this very
natural structure

1070
00:40:48,290 --> 00:40:51,930
that by passing hidden states
along forward through time,

1071
00:40:51,930 --> 00:40:54,770
the hidden state always depends
on the beginning of the sequence

1072
00:40:54,770 --> 00:40:56,250
up to the current point.

1073
00:40:56,250 --> 00:40:57,930
So then there's a
very natural way

1074
00:40:57,930 --> 00:41:01,010
to use RNNs for
autoregressive modeling.

1075
00:41:01,010 --> 00:41:02,930
So you have your
sequence of hidden states

1076
00:41:02,930 --> 00:41:05,510
that are basically
summarizing your sequence.

1077
00:41:05,510 --> 00:41:07,010
And then from each
hidden state, you

1078
00:41:07,010 --> 00:41:08,810
predict probability
of the next piece

1079
00:41:08,810 --> 00:41:11,850
of the sequence
conditioned on all earlier

1080
00:41:11,850 --> 00:41:13,850
parts of the sequence,
and that basically

1081
00:41:13,850 --> 00:41:16,810
is an RNN language model that
we saw some lectures ago.

1082
00:41:16,810 --> 00:41:19,370
Have we seen anything
else that can do this?

1083
00:41:19,370 --> 00:41:20,930
Yes, transformers.

1084
00:41:20,930 --> 00:41:23,690
And particularly
masked transformers.

1085
00:41:23,690 --> 00:41:25,850
In the transformers
lecture, transformers

1086
00:41:25,850 --> 00:41:29,050
can also be used to have this
structure where by masking out

1087
00:41:29,050 --> 00:41:30,830
the attention matrix
in the right way,

1088
00:41:30,830 --> 00:41:32,810
we can make each output
of the transformer

1089
00:41:32,810 --> 00:41:35,230
depend on only the
prefix of the sequence.

1090
00:41:35,230 --> 00:41:38,590
So we can also use transformers
for autoregressive modeling.

1091
00:41:38,590 --> 00:41:40,850
And they're very
commonly used for this.

1092
00:41:40,850 --> 00:41:44,250
So this is-- but the problem
with autoregressive modeling

1093
00:41:44,250 --> 00:41:46,750
is that you need to break
your data up into a sequence.

1094
00:41:46,750 --> 00:41:50,490
And this is very, very natural
with text data because text data

1095
00:41:50,490 --> 00:41:52,850
is naturally a 1D sequence.

1096
00:41:52,850 --> 00:41:54,870
And it's even a 1D sequence
of discrete things,

1097
00:41:54,870 --> 00:41:57,370
which is great because it's
very easy to model probabilities

1098
00:41:57,370 --> 00:41:58,530
of discrete things.

1099
00:41:58,530 --> 00:42:00,210
We've been doing
that all semester

1100
00:42:00,210 --> 00:42:02,630
with our favorite
cross-entropy softmax loss.

1101
00:42:02,630 --> 00:42:04,730
The cross entropy
softmax loss is always

1102
00:42:04,730 --> 00:42:09,410
distribution over fixed
discrete number of categories.

1103
00:42:09,410 --> 00:42:11,568
The network predicts a
score for each one of those.

1104
00:42:11,568 --> 00:42:14,110
Normalize it with a softmax,
train with a cross-entropy loss.

1105
00:42:14,110 --> 00:42:15,410
We know how to do that.

1106
00:42:15,410 --> 00:42:17,650
So that's why these
things fit very naturally

1107
00:42:17,650 --> 00:42:21,410
for language models because
language is already discrete.

1108
00:42:21,410 --> 00:42:23,335
Language is already
a 1D sequence.

1109
00:42:23,335 --> 00:42:25,210
So there's a little bit
of fuzziness in how--

1110
00:42:25,210 --> 00:42:26,870
there's a tokenizer in there.

1111
00:42:26,870 --> 00:42:28,290
We're not going
to get into that.

1112
00:42:28,290 --> 00:42:30,870
But these are very naturally
well-suited to language problems

1113
00:42:30,870 --> 00:42:34,170
because language is already
1D, it's already discrete.

1114
00:42:34,170 --> 00:42:38,210
Images are more tricky because
images are not naturally 1D.

1115
00:42:38,210 --> 00:42:40,470
Images are also not
naturally discrete.

1116
00:42:40,470 --> 00:42:43,690
We often think of images as
continuous, real valued things.

1117
00:42:43,690 --> 00:42:46,850
So they don't-- these don't
naturally fit quite as nicely

1118
00:42:46,850 --> 00:42:48,370
onto images.

1119
00:42:48,370 --> 00:42:51,550
But you got a hammer, you're
going to whack some nails.

1120
00:42:51,550 --> 00:42:53,970
So people definitely apply
autoregressive models

1121
00:42:53,970 --> 00:42:58,210
to images in a naive way
at least some years ago.

1122
00:42:58,210 --> 00:43:02,810
So one thing you can do, one
thing you can do to model images

1123
00:43:02,810 --> 00:43:06,410
with autoregressive models
is to treat an image

1124
00:43:06,410 --> 00:43:08,370
as a sequence of pixels.

1125
00:43:08,370 --> 00:43:11,330
And in particular, each pixel
is actually just three numbers.

1126
00:43:11,330 --> 00:43:16,710
And in most displays and in
most representations of images,

1127
00:43:16,710 --> 00:43:18,330
those numbers are
actually discrete.

1128
00:43:18,330 --> 00:43:20,190
So most JPEGs or PNGs--

1129
00:43:20,190 --> 00:43:23,330
most of the file formats we use
to store images are typically

1130
00:43:23,330 --> 00:43:24,470
8-bit per channel.

1131
00:43:24,470 --> 00:43:27,530
So there's actually only
a fixed number of values

1132
00:43:27,530 --> 00:43:29,030
that each pixel can take.

1133
00:43:29,030 --> 00:43:32,390
So a pixel is just three
single byte values.

1134
00:43:32,390 --> 00:43:35,650
A single byte is just like
an integer from 0 to 255.

1135
00:43:35,650 --> 00:43:38,010
So a pixel is just
three integers.

1136
00:43:38,010 --> 00:43:40,490
Each integer can be 0 to 255.

1137
00:43:40,490 --> 00:43:43,370
So what we can do
is take our image

1138
00:43:43,370 --> 00:43:46,610
and then rasterize
it out into a long

1139
00:43:46,610 --> 00:43:49,010
into a long sequence where
each element of the sequence

1140
00:43:49,010 --> 00:43:51,970
is one of the subpixel
values of our image.

1141
00:43:51,970 --> 00:43:56,350
And now we've turned our image
into a one-dimensional sequence

1142
00:43:56,350 --> 00:43:58,750
where each entry in that
sequence is a discrete value.

1143
00:43:58,750 --> 00:44:00,792
So you can apply autoregressive
modeling directly

1144
00:44:00,792 --> 00:44:02,907
to that sequence in exactly
the way that you might

1145
00:44:02,907 --> 00:44:03,990
have for a language model.

1146
00:44:03,990 --> 00:44:07,210
I'm using an RNN
or a transformer.

1147
00:44:07,210 --> 00:44:09,955
Can anyone spot a problem
with this approach?

1148
00:44:09,955 --> 00:44:10,830
[? Too ?] [? much. ?]

1149
00:44:10,830 --> 00:44:11,590
Very expensive.

1150
00:44:11,590 --> 00:44:13,010
Very, very expensive.

1151
00:44:13,010 --> 00:44:15,970
So a reasonable image that
you might want to model

1152
00:44:15,970 --> 00:44:18,850
is maybe 1024 by 1024.

1153
00:44:18,850 --> 00:44:20,870
That's not even that
high resolution, really,

1154
00:44:20,870 --> 00:44:22,510
but that's a pretty
good resolution.

1155
00:44:22,510 --> 00:44:25,350
But if you have
1024 by 1024 image,

1156
00:44:25,350 --> 00:44:28,050
that's going to be a sequence
of three million pixels.

1157
00:44:28,050 --> 00:44:32,190
People actually can model these
days sequences in the millions.

1158
00:44:32,190 --> 00:44:33,690
But it gets very,
very expensive.

1159
00:44:33,690 --> 00:44:35,977
There's got to be a more
efficient way to do this.

1160
00:44:35,977 --> 00:44:37,810
So there were some
papers a couple years ago

1161
00:44:37,810 --> 00:44:40,185
where people applied these
autoregressive models directly

1162
00:44:40,185 --> 00:44:42,210
to pixels of images,
but they were not

1163
00:44:42,210 --> 00:44:44,490
super successful, I
think, because they

1164
00:44:44,490 --> 00:44:46,890
are very difficult to
scale to high resolution.

1165
00:44:46,890 --> 00:44:49,450
So a spoiler alert that
we'll talk a little bit

1166
00:44:49,450 --> 00:44:51,570
more next lecture is
that this actually

1167
00:44:51,570 --> 00:44:53,810
has made a resurgence in
the last couple of years.

1168
00:44:53,810 --> 00:44:56,050
But the trick is to not
model the individual pixels

1169
00:44:56,050 --> 00:44:58,590
in the sequence as
individual pixel values,

1170
00:44:58,590 --> 00:45:01,770
but instead to use some other
process or procedure or model,

1171
00:45:01,770 --> 00:45:04,610
neural network maybe, to break
that image into a sequence

1172
00:45:04,610 --> 00:45:06,130
of one dimensional tokens.

1173
00:45:06,130 --> 00:45:08,770
So that's something we'll talk
about a bit more next lecture.

1174
00:45:08,770 --> 00:45:10,312
But this at least
gives you the sense

1175
00:45:10,312 --> 00:45:11,915
of what is an
autoregressive model.

1176
00:45:11,915 --> 00:45:13,790
What's the probabilistic
formulation of them.

1177
00:45:13,790 --> 00:45:15,207
How do you apply
them to language.

1178
00:45:15,207 --> 00:45:16,770
How do you apply them to images.

1179
00:45:16,770 --> 00:45:18,890
So then from
autoregressive models,

1180
00:45:18,890 --> 00:45:21,770
we next turn to
variational autoencoders.

1181
00:45:21,770 --> 00:45:24,400
And variational
autoencoders are pretty fun.

1183
00:45:28,570 --> 00:45:32,130
In these autoregressive
models, we talked about,

1184
00:45:32,130 --> 00:45:33,750
we're trying to do
maximum likelihood.

1185
00:45:33,750 --> 00:45:35,750
We broke up our data
into a sequence of parts.

1186
00:45:35,750 --> 00:45:38,280
We're trying to maximize
the likelihood of the data.

1187
00:45:38,280 --> 00:45:40,530
And variational autoencoders
are going to do something

1188
00:45:40,530 --> 00:45:41,890
a little bit different.

1189
00:45:41,890 --> 00:45:44,470
Instead, they're still going
to be an explicit method.

1190
00:45:44,470 --> 00:45:47,450
They're still going to be some
density that we can compute,

1191
00:45:47,450 --> 00:45:49,510
but it's going to
be intractable.

1192
00:45:49,510 --> 00:45:51,290
We're going to be able
to approximate it.

1193
00:45:51,290 --> 00:45:52,510
Why are we going to do that?

1194
00:45:52,510 --> 00:45:55,888
We had a perfectly good method
that computed densities exactly.

1195
00:45:55,888 --> 00:45:57,930
And what we're going to
give up for that is we're

1196
00:45:57,930 --> 00:45:59,110
going to gain something.

1197
00:45:59,110 --> 00:46:01,610
We're going to gain the ability
to compute reasonable latent

1198
00:46:01,610 --> 00:46:02,980
vectors over our data.

1199
00:46:02,980 --> 00:46:05,040
We're going to have
vectors that represent

1200
00:46:05,040 --> 00:46:08,240
our data that pop out naturally
from the learning process.

1201
00:46:08,240 --> 00:46:10,740
And those vectors are going to
be useful in their own right.

1202
00:46:10,740 --> 00:46:13,000
And the ability to get access
to those latent vectors

1203
00:46:13,000 --> 00:46:14,920
is going to be
useful enough to us

1204
00:46:14,920 --> 00:46:17,268
that we're willing to give
up computing exact densities,

1205
00:46:17,268 --> 00:46:19,560
and instead settle for these
approximate densities that

1206
00:46:19,560 --> 00:46:22,480
are actually lower bounds
on the true density.

1207
00:46:22,480 --> 00:46:24,400
Oh, the motivation
for breaking stuff up

1208
00:46:24,400 --> 00:46:26,360
in a sequence in
autoregressive models

1209
00:46:26,360 --> 00:46:27,660
because it factors the problem.

1210
00:46:27,660 --> 00:46:30,400
It makes each part
easier to model.

1211
00:46:30,400 --> 00:46:33,520
So imagine you're doing
language modeling,

1212
00:46:33,520 --> 00:46:35,600
And you have a
vocabulary of V words.

1213
00:46:35,600 --> 00:46:38,920
And I want to model the
probability of two words

1214
00:46:38,920 --> 00:46:39,580
jointly.

1215
00:46:39,580 --> 00:46:41,900
How many possible two
word sequences are there?

1216
00:46:41,900 --> 00:46:42,978
There's V squared.

1217
00:46:42,978 --> 00:46:45,020
How many possible three
word sequences are there?

1218
00:46:45,020 --> 00:46:46,240
There's V cubed.

1219
00:46:46,240 --> 00:46:48,080
And in general, if
you have how many

1220
00:46:48,080 --> 00:46:50,620
T word sequences with a
vocabulary V are there?

1221
00:46:50,620 --> 00:46:53,562
It's V to the T. So that's bad.

1222
00:46:53,562 --> 00:46:54,520
It grows exponentially.

1223
00:46:54,520 --> 00:46:56,812
If you want it to directly
model the joint distribution

1224
00:46:56,812 --> 00:46:59,520
of a sequence of T
things, the number

1225
00:46:59,520 --> 00:47:02,360
of entries in that discrete
probability distribution

1226
00:47:02,360 --> 00:47:04,360
you need to model is going
to grow exponentially

1227
00:47:04,360 --> 00:47:05,440
with the sequence length.

1228
00:47:05,440 --> 00:47:07,815
And that's quickly going to
become completely intractable

1229
00:47:07,815 --> 00:47:09,342
if we want to go
to long sequences.

1230
00:47:09,342 --> 00:47:10,800
So then the reason
we break that up

1231
00:47:10,800 --> 00:47:12,900
is so that we don't have
to model it all at once.

1232
00:47:12,900 --> 00:47:15,400
We factor it in this way and
predict only one part condition

1233
00:47:15,400 --> 00:47:17,000
on the previous parts.

1234
00:47:17,000 --> 00:47:17,900
Good question.

1235
00:47:17,900 --> 00:47:19,740
Can we apply the log
trick to mitigate that?

1236
00:47:19,740 --> 00:47:20,460
Yeah, exactly.

1237
00:47:20,460 --> 00:47:23,640
So in practice, you'll never
actually see these probability

1238
00:47:23,640 --> 00:47:24,828
density values modeled.

1239
00:47:24,828 --> 00:47:27,120
Almost always you're going
to work in log probabilities

1240
00:47:27,120 --> 00:47:27,920
instead.

1241
00:47:27,920 --> 00:47:31,020
So the model is going to
output log probabilities.

1242
00:47:31,020 --> 00:47:33,100
You're going to compute
your loss in log space.

1243
00:47:33,100 --> 00:47:34,600
For numeric stability,
you're almost

1244
00:47:34,600 --> 00:47:37,138
going to compute everything
in log space in practice.

1245
00:47:37,138 --> 00:47:38,680
So then the p of x
is being generated

1246
00:47:38,680 --> 00:47:41,560
because at the top
of the transformer,

1247
00:47:41,560 --> 00:47:43,600
it's outputting a
probability distribution

1248
00:47:43,600 --> 00:47:46,178
over the next token conditioned
on all the previous tokens.

1249
00:47:46,178 --> 00:47:48,220
And it does that for every
point in the sequence.

1250
00:47:48,220 --> 00:47:50,720
So you could actually recover
this exact probability density

1251
00:47:50,720 --> 00:47:53,360
value by multiplying
out the values

1252
00:47:53,360 --> 00:47:54,740
that all points in the sequence.

1253
00:47:54,740 --> 00:47:56,160
So if I have an
input sequence, I

1254
00:47:56,160 --> 00:47:58,160
pass it through the
transformer, the transformer

1255
00:47:58,160 --> 00:48:01,400
will have predicted at every
point in the sequence, what

1256
00:48:01,400 --> 00:48:03,600
is the distribution over
all the tokens conditioned

1257
00:48:03,600 --> 00:48:05,480
on the earlier part
of the sequence.

1258
00:48:05,480 --> 00:48:07,780
And I can compute what was
the actual next token, what

1259
00:48:07,780 --> 00:48:09,780
was the predicted probability
of the next token,

1260
00:48:09,780 --> 00:48:12,387
and then multiply all of those
across the entire sequence.

1261
00:48:12,387 --> 00:48:14,720
So that's how we can recover
the exact density value out

1262
00:48:14,720 --> 00:48:16,303
of one of these
autoregressive models.

1263
00:48:16,303 --> 00:48:17,803
And that actually
would apply either

1264
00:48:17,803 --> 00:48:18,980
to an RNN or a transformer.

1266
00:48:22,560 --> 00:48:24,040
Good questions.

1267
00:48:24,040 --> 00:48:27,778
So then in a variational
autoencoder, things get hairy.

1268
00:48:27,778 --> 00:48:29,320
So we're actually
going to drop the V

1269
00:48:29,320 --> 00:48:31,612
and talk about autoencoders
for just a couple of slides

1270
00:48:31,612 --> 00:48:34,280
because I don't think we've
done that yet this course.

1271
00:48:34,280 --> 00:48:36,162
So in a variational
autoencoder, this

1272
00:48:36,162 --> 00:48:38,120
is basically going to be
an unsupervised method

1273
00:48:38,120 --> 00:48:43,080
for learning to extract features
z from inputs x without labels.

1274
00:48:43,080 --> 00:48:47,320
And this actually is in this
vein of self-supervised learning

1275
00:48:47,320 --> 00:48:49,120
that we just talked about.

1276
00:48:49,120 --> 00:48:51,120
And our notion is
that the features

1277
00:48:51,120 --> 00:48:53,580
ought to extract useful
information about the data.

1278
00:48:53,580 --> 00:48:55,680
Maybe they somehow
implicitly encode

1279
00:48:55,680 --> 00:48:59,160
what is the identity of objects
in the image, how many of them

1280
00:48:59,160 --> 00:48:59,660
there are?

1281
00:48:59,660 --> 00:49:00,827
What are the colors of them?

1282
00:49:00,827 --> 00:49:03,480
We want this feature vector z
to contain useful information

1283
00:49:03,480 --> 00:49:05,958
about the input x.

1284
00:49:05,958 --> 00:49:08,000
And this encoder itself
could be a neural network

1285
00:49:08,000 --> 00:49:08,833
of any architecture.

1286
00:49:08,833 --> 00:49:11,920
It could be an MLP, transformer,
CNN, whatever you want.

1287
00:49:11,920 --> 00:49:13,440
But inputs are data x.

1288
00:49:13,440 --> 00:49:16,240
And then it's going to
output some vector z.

1289
00:49:16,240 --> 00:49:18,732
And then the question is, how
do we this without labels.

1290
00:49:18,732 --> 00:49:20,440
We actually saw a lot
of examples of this

1291
00:49:20,440 --> 00:49:21,720
in the previous lecture.

1292
00:49:21,720 --> 00:49:23,320
But there's a very
simple one, which

1293
00:49:23,320 --> 00:49:25,360
is just try to
reconstruct the input.

1294
00:49:25,360 --> 00:49:28,080
So we're going to now have a
second part of the model called

1295
00:49:28,080 --> 00:49:30,920
the decoder, which is going
to input the z and then output

1296
00:49:30,920 --> 00:49:32,320
back an x.

1297
00:49:32,320 --> 00:49:34,960
And we want-- oh,
and I dropped the x.

1298
00:49:34,960 --> 00:49:36,425
And we're going to
train this thing

1299
00:49:36,425 --> 00:49:37,800
so that the output
from the model

1300
00:49:37,800 --> 00:49:40,220
should actually match the input.

1301
00:49:40,220 --> 00:49:42,220
In some sense, the stupidest
loss function ever.

1302
00:49:42,220 --> 00:49:44,790
We're just training the model
to mimic the identity function.

1303
00:49:44,790 --> 00:49:45,540
Why do we do that?

1304
00:49:45,540 --> 00:49:46,860
We already know the
identity function.

1305
00:49:46,860 --> 00:49:48,160
Why are we spending
a lot of flops

1306
00:49:48,160 --> 00:49:49,840
and training a neural
network on a big data set

1307
00:49:49,840 --> 00:49:52,240
to just learn the identity
function that we already know?

1308
00:49:52,240 --> 00:49:55,000
It's because we're going to
bottleneck it in some way.

1309
00:49:55,000 --> 00:49:57,020
If this model had
infinite capacity,

1310
00:49:57,020 --> 00:49:59,440
for example, if that z vector
was very wide, if there were

1311
00:49:59,440 --> 00:50:02,080
no constraints on the learning,
I would expect a neural network

1312
00:50:02,080 --> 00:50:03,520
to just nail this problem.

1313
00:50:03,520 --> 00:50:06,240
But we don't want to do that
because we explicitly don't care

1314
00:50:06,240 --> 00:50:07,717
about learning this objective.

1315
00:50:07,717 --> 00:50:09,300
We already know the
identity function.

1316
00:50:09,300 --> 00:50:11,633
We don't need an expensive
neural network to compute it.

1317
00:50:11,633 --> 00:50:13,520
What we want to do
is force the network

1318
00:50:13,520 --> 00:50:16,265
to try to learn the identity
function under some constraint.

1319
00:50:16,265 --> 00:50:17,640
And the constraint
that you often

1320
00:50:17,640 --> 00:50:20,680
use in a traditional
autoencoder is by bottlenecking

1321
00:50:20,680 --> 00:50:22,120
that representation z.

1322
00:50:22,120 --> 00:50:25,120
In particular, that means that
that vector z in the middle

1323
00:50:25,120 --> 00:50:28,120
is going to be much, much
smaller than the input x.

1324
00:50:28,120 --> 00:50:31,520
So your input x might be a high
resolution image, maybe a 1024

1325
00:50:31,520 --> 00:50:34,040
by 1024 image that
we said is composed

1326
00:50:34,040 --> 00:50:35,380
of three million floats.

1327
00:50:35,380 --> 00:50:39,240
But then that z might be like
128 dimensional latent code.

1328
00:50:39,240 --> 00:50:41,920
So the model is now asked to
solve this problem where I want

1329
00:50:41,920 --> 00:50:43,940
to reconstruct the output--

1330
00:50:43,940 --> 00:50:46,040
reconstruct the
data x but squash it

1331
00:50:46,040 --> 00:50:49,278
through this bottleneck
representation in the middle.

1332
00:50:49,278 --> 00:50:51,320
And we hope that this is
going to force the model

1333
00:50:51,320 --> 00:50:54,960
to learn some nontrivial
structure about the data

1334
00:50:54,960 --> 00:50:57,000
by squashing it through
this representation

1335
00:50:57,000 --> 00:50:59,025
in the middle of the network.

1336
00:50:59,025 --> 00:51:00,400
And then after we
do this, we can

1337
00:51:00,400 --> 00:51:02,400
apply our normal
self-supervised learning trick

1338
00:51:02,400 --> 00:51:04,660
where you could throw
away the decoder,

1339
00:51:04,660 --> 00:51:07,280
and then use this Z to
initialize some supervised model

1340
00:51:07,280 --> 00:51:08,540
for some downstream task.

1341
00:51:08,540 --> 00:51:11,160
The same story as in the
self-supervised story

1342
00:51:11,160 --> 00:51:13,040
that we just saw.

1343
00:51:13,040 --> 00:51:15,320
But what about-- what
if we want actually

1344
00:51:15,320 --> 00:51:17,265
want to use this
to generate data?

1345
00:51:17,265 --> 00:51:18,640
Then, what we'd
really like to do

1346
00:51:18,640 --> 00:51:21,220
is somehow the opposite of
the self-supervised story.

1347
00:51:21,220 --> 00:51:23,860
What we'd really like to do
is throw away the encoder,

1348
00:51:23,860 --> 00:51:27,800
and instead be able to somehow
sample z's that match the z's

1349
00:51:27,800 --> 00:51:30,560
that the model learns
to represent data as.

1350
00:51:30,560 --> 00:51:33,840
And if we had some procedure
for sampling z's that

1351
00:51:33,840 --> 00:51:36,300
matched the data
distribution in some way,

1352
00:51:36,300 --> 00:51:40,060
then we could sample a z, pass
it through our learned decoder,

1353
00:51:40,060 --> 00:51:41,560
and now generate a new sample.

1354
00:51:41,560 --> 00:51:43,640
And now this is an
implicit method.

1355
00:51:43,640 --> 00:51:46,080
You said that there's no
densities floating around

1356
00:51:46,080 --> 00:51:46,820
anywhere.

1357
00:51:46,820 --> 00:51:48,440
But if we had a
way to do this, it

1358
00:51:48,440 --> 00:51:51,240
would be a way to draw
samples from the model

1359
00:51:51,240 --> 00:51:55,070
without explicitly modeling
the density in any way.

1360
00:51:55,070 --> 00:51:58,150
But the problem is that we've
just kicked the can down

1361
00:51:58,150 --> 00:52:01,377
the road here a little bit
because we said if we want

1362
00:52:01,377 --> 00:52:03,210
to generate images, we
want to generate x's.

1363
00:52:03,210 --> 00:52:04,710
We have a dataset of x's.

1364
00:52:04,710 --> 00:52:05,895
How do we do that?

1365
00:52:05,895 --> 00:52:07,270
We said we're
going to solve that

1366
00:52:07,270 --> 00:52:08,523
by training this autoencoder.

1367
00:52:08,523 --> 00:52:10,190
And now we have a
dataset of z's, and we

1368
00:52:10,190 --> 00:52:11,530
need to sample in z space.

1369
00:52:11,530 --> 00:52:12,730
It's not any easier.

1370
00:52:12,730 --> 00:52:14,870
So we're stuck.

1371
00:52:14,870 --> 00:52:18,510
And the idea of variational
autoencoders is,

1372
00:52:18,510 --> 00:52:22,110
what if we could force
some structure on the z's.

1373
00:52:22,110 --> 00:52:24,630
If you have this traditional
autoencoder structure,

1374
00:52:24,630 --> 00:52:27,270
you're not forcing the model
to impose any known structure

1375
00:52:27,270 --> 00:52:27,970
on the z's.

1376
00:52:27,970 --> 00:52:30,590
You're just asking it to
reconstruct the data given

1377
00:52:30,590 --> 00:52:32,050
its latent representation.

1378
00:52:32,050 --> 00:52:33,750
But what if we
had some mechanism

1379
00:52:33,750 --> 00:52:36,490
to force the z's to come
from a Gaussian distribution,

1380
00:52:36,490 --> 00:52:38,050
or some other
known distribution?

1381
00:52:38,050 --> 00:52:41,190
If that were the case, then
we could just draw a sample

1382
00:52:41,190 --> 00:52:42,293
at inference time.

1383
00:52:42,293 --> 00:52:44,710
After this model is trained,
draw a sample from that known

1384
00:52:44,710 --> 00:52:47,890
distribution, pass it
through the decoder,

1385
00:52:47,890 --> 00:52:51,070
and now we would
have our sample.

1386
00:52:51,070 --> 00:52:53,870
So forcing these autoencoders
to be probabilistic

1387
00:52:53,870 --> 00:52:57,170
and to enforce a probabilistic
structure on that latent space

1388
00:52:57,170 --> 00:53:00,510
exactly is what a variational
autoencoder tries to do.

1389
00:53:00,510 --> 00:53:01,790
Why variational?

1390
00:53:01,790 --> 00:53:02,990
It's a long story.

1391
00:53:02,990 --> 00:53:06,470
It says it has a long history
around that terminology

1392
00:53:06,470 --> 00:53:08,070
in the literature.

1393
00:53:08,070 --> 00:53:09,790
But basically,
variational autoencoders

1394
00:53:09,790 --> 00:53:12,950
are a probabilistic spin on
our traditional autoencoder.

1395
00:53:12,950 --> 00:53:16,530
So it's going to learn latent
features z from raw data.

1396
00:53:16,530 --> 00:53:19,310
And then we'll be able to
enforce a structure on that

1397
00:53:19,310 --> 00:53:22,110
learned latent space z
such that we can sample

1398
00:53:22,110 --> 00:53:24,350
from it at inference time
after the model is trained

1399
00:53:24,350 --> 00:53:26,710
and generate new samples.

1400
00:53:26,710 --> 00:53:30,610
So more concretely, we'll assume
that our training data xi--

1401
00:53:30,610 --> 00:53:33,110
again, note here the superscript
i means these are different

1402
00:53:33,110 --> 00:53:35,510
independent samples of x.

1403
00:53:35,510 --> 00:53:39,110
We assume that each
xi was generated

1404
00:53:39,110 --> 00:53:42,510
from some underlying
latent vector z,

1405
00:53:42,510 --> 00:53:45,670
that there's some zi that's
lurking under the surface

1406
00:53:45,670 --> 00:53:47,550
associated with every xi.

1407
00:53:47,550 --> 00:53:50,370
And in the universe's
procedure for generating data,

1408
00:53:50,370 --> 00:53:55,212
first it generated the z, then
it generated the xi from the zi.

1409
00:53:55,212 --> 00:53:56,670
Everything that
the universe needed

1410
00:53:56,670 --> 00:53:59,670
to know in order to generate
the image that we saw

1411
00:53:59,670 --> 00:54:02,110
was contained in
that latent vector z.

1412
00:54:02,110 --> 00:54:04,167
But we can't see
those latent vector z.

1413
00:54:04,167 --> 00:54:05,250
We can never observe them.

1414
00:54:05,250 --> 00:54:06,810
We don't have a
data set of them.

1415
00:54:06,810 --> 00:54:10,590
So the intuition is that x an
image, z is some latent feature

1416
00:54:10,590 --> 00:54:12,910
representation that tells
you everything you would ever

1417
00:54:12,910 --> 00:54:14,910
need to know about that
image, but you can never

1418
00:54:14,910 --> 00:54:16,670
observe that latent vector.

1419
00:54:16,670 --> 00:54:19,480
And then after training, we
could generate a sample by--

1420
00:54:19,480 --> 00:54:21,230
oh, and the other
constraint is that we're

1421
00:54:21,230 --> 00:54:24,152
going to force those z's to
come from a known distribution.

1422
00:54:24,152 --> 00:54:25,610
So then after the
model is trained,

1423
00:54:25,610 --> 00:54:27,318
then we can do exactly
what we just said.

1424
00:54:27,318 --> 00:54:29,210
Draw a z from that
known distribution,

1425
00:54:29,210 --> 00:54:32,630
pass through the decoder, that's
going to give us a sample.

1426
00:54:32,630 --> 00:54:34,910
And then we'll typically
assume a simple prior.

1427
00:54:34,910 --> 00:54:37,430
Almost always a unit
Gaussian distribution

1428
00:54:37,430 --> 00:54:39,590
is by far the most common.

1429
00:54:39,590 --> 00:54:41,590
So then how do we
possibly train this?

1430
00:54:41,590 --> 00:54:43,510
This feels like an
impossible problem.

1431
00:54:43,510 --> 00:54:45,430
We want to basically
train this network that's

1432
00:54:45,430 --> 00:54:49,250
going to get these z's,
find a z for every x.

1433
00:54:49,250 --> 00:54:50,493
We can never observe the z's.

1434
00:54:50,493 --> 00:54:51,410
This seems impossible.

1435
00:54:51,410 --> 00:54:52,510
What are we going to do?

1436
00:54:52,510 --> 00:54:54,910
We're going to go back
to a maximum likelihood.

1437
00:54:54,910 --> 00:54:58,750
If we indeed had a dataset
of x's and z's, then, we

1438
00:54:58,750 --> 00:55:02,030
could use maximum likelihood
to directly use the same log

1439
00:55:02,030 --> 00:55:02,670
trick--

1440
00:55:02,670 --> 00:55:03,970
maximize the log probability.

1441
00:55:03,970 --> 00:55:06,875
We could use the exact same
thing that we previously saw.

1442
00:55:06,875 --> 00:55:08,750
And then train a
conditional generative model

1443
00:55:08,750 --> 00:55:10,750
p of x conditioned on z.

1444
00:55:10,750 --> 00:55:11,690
But we don't know z.

1445
00:55:11,690 --> 00:55:13,232
But let's pretend
we do for a moment.

1447
00:55:15,470 --> 00:55:18,630
Because we don't know z, we
could try to marginalize.

1448
00:55:18,630 --> 00:55:21,190
We know that p of x
is equal to-- maybe

1449
00:55:21,190 --> 00:55:24,670
there's some joint distribution
of x and z that must exist even

1450
00:55:24,670 --> 00:55:26,110
though we can't observe it.

1451
00:55:26,110 --> 00:55:28,110
And then in principle,
you could integrate out

1452
00:55:28,110 --> 00:55:31,150
the z to marginalize
over it to get a p of x.

1453
00:55:31,150 --> 00:55:34,350
And then maybe we could pretend
there's a joint distribution x

1454
00:55:34,350 --> 00:55:36,750
and z, marginalize out the
z somehow, and still do

1455
00:55:36,750 --> 00:55:39,650
maximum likelihood.

1456
00:55:39,650 --> 00:55:40,770
Let's see how this works.

1457
00:55:40,770 --> 00:55:41,977
So this term in--

1458
00:55:41,977 --> 00:55:44,310
and then here we've also used
the chain rule to break up

1459
00:55:44,310 --> 00:55:48,190
that p of x given z that
joint probability p of x and z

1460
00:55:48,190 --> 00:55:51,630
into p of x given
z and just p of z.

1461
00:55:51,630 --> 00:55:54,230
So this p of x
given z, that's OK.

1462
00:55:54,230 --> 00:55:56,110
We could compute
that with our decoder

1463
00:55:56,110 --> 00:55:58,277
here on the left, that's a
neural network that we're

1464
00:55:58,277 --> 00:55:59,030
hoping to train.

1465
00:55:59,030 --> 00:56:00,015
This p of z term is OK.

1466
00:56:00,015 --> 00:56:01,390
We're going to
assume that that's

1467
00:56:01,390 --> 00:56:03,432
a unit Gaussian or some
other simple distribution

1468
00:56:03,432 --> 00:56:05,430
that we can compute
or reason about.

1469
00:56:05,430 --> 00:56:06,910
But this integral kills us.

1470
00:56:06,910 --> 00:56:08,910
In general, we have
no feasible way

1471
00:56:08,910 --> 00:56:12,470
to integrate over the full space
of a neural network's input.

1472
00:56:12,470 --> 00:56:14,088
This p of x given
z is going to be

1473
00:56:14,088 --> 00:56:15,630
some very complicated
function that's

1474
00:56:15,630 --> 00:56:16,890
modeled by a neural network.

1475
00:56:16,890 --> 00:56:19,015
There's going to be no way
that we can analytically

1476
00:56:19,015 --> 00:56:20,877
or exactly integrate this.

1477
00:56:20,877 --> 00:56:23,210
You can train neural networks
for individual parts here.

1478
00:56:23,210 --> 00:56:24,748
So the whole
underlying notion here,

1479
00:56:24,748 --> 00:56:26,790
whenever you're doing this
probabilistic modeling

1480
00:56:26,790 --> 00:56:29,292
is like we're going to write
down some probabilistic terms.

1481
00:56:29,292 --> 00:56:31,750
Hopefully some of them are
going to be simple distributions

1482
00:56:31,750 --> 00:56:33,958
that we can write down
analytically and reason about.

1483
00:56:33,958 --> 00:56:36,083
Some of them are going to
be learned neural network

1484
00:56:36,083 --> 00:56:36,730
components.

1485
00:56:36,730 --> 00:56:38,710
So we're assuming that
probability of x given

1486
00:56:38,710 --> 00:56:40,630
z is going to be some
neural network that we

1487
00:56:40,630 --> 00:56:44,110
could, in principle, learn
via maximum likelihood.

1488
00:56:44,110 --> 00:56:47,550
But we're starting to write
down what objective could we

1489
00:56:47,550 --> 00:56:50,467
use to learn that neural
network via maximum likelihood.

1490
00:56:50,467 --> 00:56:52,550
And we're out of luck here
because you have no way

1491
00:56:52,550 --> 00:56:53,870
to integrate over z.

1492
00:56:53,870 --> 00:56:55,670
You could try to
approximate that integral

1493
00:56:55,670 --> 00:56:58,683
via some finite sampling, but
in general, that's probably not

1494
00:56:58,683 --> 00:57:00,350
going to work very
well because the z is

1495
00:57:00,350 --> 00:57:04,230
a super high dimensional
space and doing

1496
00:57:04,230 --> 00:57:07,070
an approximate numerical
integral in the inner loop

1497
00:57:07,070 --> 00:57:09,910
of your training is not
going to be a very good idea.

1498
00:57:09,910 --> 00:57:11,773
So we could try something else.

1499
00:57:11,773 --> 00:57:13,190
Bayes rule, that's
the other thing

1500
00:57:13,190 --> 00:57:14,490
we always do in probability.

1501
00:57:14,490 --> 00:57:15,830
So let's try Bayes' rule.

1502
00:57:15,830 --> 00:57:18,110
If we have Bayes' rule,
we have another formula

1503
00:57:18,110 --> 00:57:20,890
that we can use to
write down p of x.

1504
00:57:20,890 --> 00:57:24,550
So p of x we can write
down using Bayes' rule

1505
00:57:24,550 --> 00:57:26,430
in this equation on the screen.

1506
00:57:26,430 --> 00:57:28,710
Let's see what we can
do with these terms.

1507
00:57:28,710 --> 00:57:31,070
So this one, p of
x given z, again,

1508
00:57:31,070 --> 00:57:32,870
we can compute that
with our decoder.

1509
00:57:32,870 --> 00:57:35,930
P of z, again OK, this one's--
we assume this is Gaussian,

1510
00:57:35,930 --> 00:57:37,510
so we can compute
something with it.

1511
00:57:37,510 --> 00:57:38,593
There's no integrals here.

1512
00:57:38,593 --> 00:57:39,170
That's good.

1513
00:57:39,170 --> 00:57:40,870
So we're in good shape.

1514
00:57:40,870 --> 00:57:43,950
But now we're out of luck,
this p of z given x term.

1515
00:57:43,950 --> 00:57:47,070
This posterior of z given
x, we have no good way

1516
00:57:47,070 --> 00:57:48,243
to compute this.

1517
00:57:48,243 --> 00:57:49,660
In order to compute
this term, you

1518
00:57:49,660 --> 00:57:52,000
would also need some kind
of integral out of luck.

1519
00:57:52,000 --> 00:57:52,880
We can't compute it.

1520
00:57:52,880 --> 00:57:55,380
What are we going to do?

1521
00:57:55,380 --> 00:57:56,980
Let's use another
neural network.

1522
00:57:56,980 --> 00:57:58,660
So the variational
autoencoder trick

1523
00:57:58,660 --> 00:58:01,340
is like there's use that
probabilistic term on the bottom

1524
00:58:01,340 --> 00:58:04,180
here, a Bayes rule that
we can't compute, let's

1525
00:58:04,180 --> 00:58:05,900
just slot in another
neural network

1526
00:58:05,900 --> 00:58:07,460
to try to compute it for us.

1527
00:58:07,460 --> 00:58:09,335
So we're going to have
another neural network

1528
00:58:09,335 --> 00:58:12,300
q with different
weights phi that's

1529
00:58:12,300 --> 00:58:15,420
going to learn a different
conditional distribution

1530
00:58:15,420 --> 00:58:17,860
probability of z given x.

1531
00:58:17,860 --> 00:58:20,220
And the whole thing is we
want this other neural network

1532
00:58:20,220 --> 00:58:22,940
to try to approximate
the true p of x given

1533
00:58:22,940 --> 00:58:24,900
z of the first neural network.

1534
00:58:24,900 --> 00:58:27,400
And you can't really
enforce this in general,

1535
00:58:27,400 --> 00:58:31,740
but let's put a neural network
there and see what we can do.

1536
00:58:31,740 --> 00:58:34,540
So then if we could somehow have
this other neural network that

1537
00:58:34,540 --> 00:58:36,248
was approximating this
term on the bottom

1538
00:58:36,248 --> 00:58:38,620
that we can't compute, then
we could go and compute

1539
00:58:38,620 --> 00:58:41,220
our likelihood and do
maximum likelihood,

1540
00:58:41,220 --> 00:58:43,820
and we would all be set.

1541
00:58:43,820 --> 00:58:45,420
So that's what we
do when training

1542
00:58:45,420 --> 00:58:46,517
a variational autoencoder.

1543
00:58:46,517 --> 00:58:48,100
We're basically going
to jointly learn

1544
00:58:48,100 --> 00:58:49,520
two different neural networks.

1545
00:58:49,520 --> 00:58:52,220
One is the decoder, which
inputs the latent code z

1546
00:58:52,220 --> 00:58:55,340
and outputs a distribution
over the data x.

1547
00:58:55,340 --> 00:58:57,460
The other is an
encoder, which is

1548
00:58:57,460 --> 00:59:00,620
going to input the data x
and output a distribution

1549
00:59:00,620 --> 00:59:01,755
over the latent codes z.

1550
00:59:01,755 --> 00:59:04,380
And each of these are going to
be separate neural networks that

1551
00:59:04,380 --> 00:59:07,365
are separately trained with
their own independent weights.

1552
00:59:07,365 --> 00:59:08,740
There's a question
you might have

1553
00:59:08,740 --> 00:59:11,420
which is, how can you
possibly output a probability

1554
00:59:11,420 --> 00:59:12,960
distribution from
a neural network?

1555
00:59:12,960 --> 00:59:15,900
That seems confusing
and hard and unclear.

1556
00:59:15,900 --> 00:59:18,540
So the trick here is we're going
to actually force everything

1557
00:59:18,540 --> 00:59:20,070
to be a normal distribution.

1558
00:59:20,070 --> 00:59:21,820
And we're going to
have the neural network

1559
00:59:21,820 --> 00:59:24,358
output the parameters of
the normal distribution.

1560
00:59:24,358 --> 00:59:25,900
So typically for
the decoder network,

1561
00:59:25,900 --> 00:59:28,100
we're going to assume
that the output

1562
00:59:28,100 --> 00:59:30,180
distribution from
the decoder is going

1563
00:59:30,180 --> 00:59:34,020
to be diagonal Gaussian where
the entries in the diagonal

1564
00:59:34,020 --> 00:59:37,020
are the pixels of
the neural network.

1565
00:59:37,020 --> 00:59:38,980
And the model is going
to output the mean

1566
00:59:38,980 --> 00:59:40,762
of that diagonal
Gaussian distribution.

1567
00:59:40,762 --> 00:59:42,220
And typically for
the decoder, we'd

1568
00:59:42,220 --> 00:59:45,260
assume a fixed variance
or standard deviation

1569
00:59:45,260 --> 00:59:46,620
sigma squared.

1570
00:59:46,620 --> 00:59:49,520
Now, for the encoder
network, same idea.

1571
00:59:49,520 --> 00:59:51,723
The model is going to
input the data sample x.

1572
00:59:51,723 --> 00:59:54,140
And then it's going to output
the parameters of a Gaussian

1573
00:59:54,140 --> 00:59:59,340
distribution that model the
distribution q of z given x.

1574
00:59:59,340 --> 01:00:01,740
So in this case,
the encoder network

1575
01:00:01,740 --> 01:00:04,700
will output one vector, which
is the mean of that Gaussian

1576
01:00:04,700 --> 01:00:06,420
distribution, and
another vector, which

1577
01:00:06,420 --> 01:00:09,580
is the diagonal of the
covariance of that Gaussian

1578
01:00:09,580 --> 01:00:10,480
distribution.

1579
01:00:10,480 --> 01:00:11,980
And here it's very
important that we

1580
01:00:11,980 --> 01:00:15,020
assume the diagonal structure,
because otherwise we

1581
01:00:15,020 --> 01:00:19,060
would have to model h squared
entries in that full covariance

1582
01:00:19,060 --> 01:00:19,820
matrix.

1583
01:00:19,820 --> 01:00:24,180
So here you imagine an
image that's h by w pixels,

1584
01:00:24,180 --> 01:00:26,820
so that means that the entries
in your diagonal matrix

1585
01:00:26,820 --> 01:00:29,400
are like the full--

1586
01:00:29,400 --> 01:00:31,460
You could in principle
model the full covariance

1587
01:00:31,460 --> 01:00:33,295
across every pair of
pixels in the image,

1588
01:00:33,295 --> 01:00:35,420
but that would require h
squared w squared entries,

1589
01:00:35,420 --> 01:00:36,700
and that would be too big.

1590
01:00:36,700 --> 01:00:39,380
So instead we'll just ignore
any correlation structure

1591
01:00:39,380 --> 01:00:41,460
among the different values.

1592
01:00:41,460 --> 01:00:44,060
And now that means that the
diagonal covariance is now

1593
01:00:44,060 --> 01:00:46,700
a vector that's the same
size as the data itself.

1594
01:00:46,700 --> 01:00:48,460
So that means this
mu of z given x.

1595
01:00:48,460 --> 01:00:50,620
And this sigma of z
given x are both vectors

1596
01:00:50,620 --> 01:00:52,980
of the same shape as z.

1597
01:00:52,980 --> 01:00:56,900
So we basically have the neural
network output two vectors

1598
01:00:56,900 --> 01:00:58,500
of the same shape,
and then treat them

1599
01:00:58,500 --> 01:01:01,300
as the parameters of this
Gaussian distribution.

1600
01:01:01,300 --> 01:01:03,340
So that's how we can
output a distribution

1601
01:01:03,340 --> 01:01:04,860
from a neural network.

1602
01:01:04,860 --> 01:01:08,460
If you do maximum
likelihood on this thing

1603
01:01:08,460 --> 01:01:10,620
with a fixed standard
deviation, it actually

1604
01:01:10,620 --> 01:01:14,043
becomes equivalent to L2,
and that's a nice trick.

1605
01:01:14,043 --> 01:01:15,460
And the reason you
want to do that

1606
01:01:15,460 --> 01:01:19,260
is because trying to
model the diagonal--

1607
01:01:19,260 --> 01:01:21,780
you could, in principle,
try to model the same thing

1608
01:01:21,780 --> 01:01:25,060
on the decoder and try to
model a separate variance

1609
01:01:25,060 --> 01:01:25,960
of every pixel.

1610
01:01:25,960 --> 01:01:28,622
But that would be useless
because that would be--

1611
01:01:28,622 --> 01:01:30,580
if you're not modeling
any covariance structure

1612
01:01:30,580 --> 01:01:32,163
among the pixels,
that would basically

1613
01:01:32,163 --> 01:01:35,500
be saying that each pixel is
allowed to vary a little bit,

1614
01:01:35,500 --> 01:01:37,820
and the amount that each
pixel is allowed to vary

1615
01:01:37,820 --> 01:01:38,753
depends on the pixel.

1616
01:01:38,753 --> 01:01:40,420
And then sampling
from that distribution

1617
01:01:40,420 --> 01:01:43,500
would basically amount
to fixing the mean

1618
01:01:43,500 --> 01:01:46,420
and then adding per pixel
independent noise that's

1619
01:01:46,420 --> 01:01:48,360
scaled by the per
pixel variances,

1620
01:01:48,360 --> 01:01:50,540
and that would not be
a sensible thing to do.

1621
01:01:50,540 --> 01:01:55,300
So in general, for the decoder,
you cheat a little bit--

1622
01:01:55,300 --> 01:01:58,040
you pretend it's outputting
a probability distribution,

1623
01:01:58,040 --> 01:01:59,220
but in general we're
never going to sample

1624
01:01:59,220 --> 01:02:00,178
from that distribution.

1625
01:02:00,178 --> 01:02:02,380
Instead we're always
going to output the mean.

1626
01:02:02,380 --> 01:02:03,760
Does that make sense?

1627
01:02:03,760 --> 01:02:04,260
Yeah.

1628
01:02:04,260 --> 01:02:06,960
And then it turns out
if you write this down

1629
01:02:06,960 --> 01:02:09,020
that constant sigma
squared just comes off

1630
01:02:09,020 --> 01:02:10,420
as a constant in the front.

1631
01:02:10,420 --> 01:02:14,380
And in practice, all maximizing
the log likelihood of a Gaussian

1632
01:02:14,380 --> 01:02:18,807
distribution with a fixed
variance along the diagonal

1633
01:02:18,807 --> 01:02:21,140
is equivalent to minimizing
L2 distance between the mean

1634
01:02:21,140 --> 01:02:23,163
and the x, which is nice.

1635
01:02:23,163 --> 01:02:24,080
Yeah, a good question.

1636
01:02:24,080 --> 01:02:27,700
Is there some weird invariance
or noninvariace structure

1637
01:02:27,700 --> 01:02:29,818
here of what the pixel shifting?

1638
01:02:29,818 --> 01:02:31,860
That would be more a
property of the architecture

1639
01:02:31,860 --> 01:02:34,180
that you would choose to
build the neural network.

1640
01:02:34,180 --> 01:02:37,452
So you could try to build into
your network architecture that's

1641
01:02:37,452 --> 01:02:38,160
predicting these.

1642
01:02:38,160 --> 01:02:41,540
You could try to build some
invariance or equivariance

1643
01:02:41,540 --> 01:02:43,118
properties into
the architecture.

1644
01:02:43,118 --> 01:02:45,660
But yeah, you're right that, in
general, that's not accounted

1645
01:02:45,660 --> 01:02:47,320
for at the loss level here.

1647
01:02:50,180 --> 01:02:51,800
So now we've got this idea.

1648
01:02:51,800 --> 01:02:53,683
We've got an encoder, a decoder.

1649
01:02:53,683 --> 01:02:55,100
But one is inputting
x, outputting

1650
01:02:55,100 --> 01:02:59,160
a distribution over z, other is
inputting a distribution over x.

1651
01:02:59,160 --> 01:03:00,660
What's our training objective?

1652
01:03:00,660 --> 01:03:03,380
And here's the one slide where
we're going to do some math.

1653
01:03:03,380 --> 01:03:04,820
But we'll see.

1654
01:03:04,820 --> 01:03:06,020
So here we're going to--

1655
01:03:06,020 --> 01:03:08,435
basically the idea is we want
to do maximum likelihood.

1656
01:03:08,435 --> 01:03:10,060
That's usually the
single thing that we

1657
01:03:10,060 --> 01:03:11,580
want-- that's the
guiding principle

1658
01:03:11,580 --> 01:03:14,620
behind a lot of objectives
in generative modeling.

1659
01:03:14,620 --> 01:03:17,185
So we want to
maximize log p of x.

1660
01:03:17,185 --> 01:03:19,060
And then we can use
Bayes' rule to write that

1661
01:03:19,060 --> 01:03:21,980
as log p of this
Bayes rule expression.

1662
01:03:21,980 --> 01:03:24,460
This is an exact equivalence.

1663
01:03:24,460 --> 01:03:26,400
Now we're going to
do something silly.

1664
01:03:26,400 --> 01:03:28,500
We're going to multiply
the top and bottom of this

1665
01:03:28,500 --> 01:03:30,015
by our q of z given x.

1666
01:03:30,015 --> 01:03:32,140
Remember, we just introduced
another neural network

1667
01:03:32,140 --> 01:03:35,660
q out of nowhere that was
modeling this other distribution

1668
01:03:35,660 --> 01:03:36,667
q of z given x.

1669
01:03:36,667 --> 01:03:38,500
And now we're going to
multiply that density

1670
01:03:38,500 --> 01:03:42,410
term on the top and bottom of
this Bayes rule expression.

1671
01:03:42,410 --> 01:03:44,450
Now we're going to do
some logarithm math.

1672
01:03:44,450 --> 01:03:49,650
And if you have some foresight,
you'll for some reason

1673
01:03:49,650 --> 01:03:52,078
decide to rearrange these
terms in this particular order.

1674
01:03:52,078 --> 01:03:54,370
And I've color coded them so
you can later go and track

1675
01:03:54,370 --> 01:03:55,810
which term went where.

1676
01:03:55,810 --> 01:03:58,690
But we do some logarithms
and break this up

1677
01:03:58,690 --> 01:04:01,370
into three separate terms.

1678
01:04:01,370 --> 01:04:04,690
Now, you need to make another
magical observation, which

1679
01:04:04,690 --> 01:04:09,050
is that this p of x actually
does not depend on z.

1680
01:04:09,050 --> 01:04:11,490
So far, this sequence
of three terms, this

1681
01:04:11,490 --> 01:04:12,890
is all in exact equivalence.

1682
01:04:12,890 --> 01:04:14,510
These are all exact equalities.

1683
01:04:14,510 --> 01:04:16,565
So even though there's
a z in this expression--

1684
01:04:16,565 --> 01:04:18,690
we actually doesn't depend
on z because all the z's

1685
01:04:18,690 --> 01:04:20,170
would cancel out.

1686
01:04:20,170 --> 01:04:22,310
And if you have something
that doesn't depend on z,

1687
01:04:22,310 --> 01:04:25,850
you can always wrap
an expectation over z

1688
01:04:25,850 --> 01:04:26,770
of that thing.

1689
01:04:26,770 --> 01:04:29,250
So in this case, we know
that this is our p of x.

1690
01:04:29,250 --> 01:04:33,090
We can always feel free to wrap
an expectation of z sampled

1691
01:04:33,090 --> 01:04:37,410
according to any distribution
that we want of p of x.

1692
01:04:37,410 --> 01:04:39,870
And because that internal
thing does not depend on z,

1693
01:04:39,870 --> 01:04:43,010
this is always true
for any distribution

1694
01:04:43,010 --> 01:04:46,582
that we might choose to
take this expectation over.

1695
01:04:46,582 --> 01:04:50,393
So then because expectation
is a linear thing,

1696
01:04:50,393 --> 01:04:52,810
we can apply that expectation
to each of these three terms

1697
01:04:52,810 --> 01:04:53,850
upstairs.

1698
01:04:53,850 --> 01:04:57,090
And now we have these
three terms, each of which

1699
01:04:57,090 --> 01:04:58,970
looks very mysterious.

1700
01:04:58,970 --> 01:05:02,343
But if you had a lot of
intuition about probability,

1701
01:05:02,343 --> 01:05:04,010
you memorize all these
formulas that you

1702
01:05:04,010 --> 01:05:06,610
may have seen in an earlier
statistics or probability

1703
01:05:06,610 --> 01:05:10,170
course, maybe you could learn
to recognize some of these.

1704
01:05:10,170 --> 01:05:13,490
So this first one we're going
to carry down as it was before.

1705
01:05:13,490 --> 01:05:18,270
And the second two
are actually KL terms.

1706
01:05:18,270 --> 01:05:20,370
So the KL divergence
is a measure

1707
01:05:20,370 --> 01:05:23,350
of dissimilarity between
probability distributions.

1708
01:05:23,350 --> 01:05:25,850
And it just so happens to
have this exact definition

1709
01:05:25,850 --> 01:05:27,730
of these latter two terms.

1710
01:05:27,730 --> 01:05:30,970
So we can rewrite this exactly
as this first term, which

1711
01:05:30,970 --> 01:05:33,610
is this expectation blah, blah,
blah, we'll talk about it,

1712
01:05:33,610 --> 01:05:36,810
And then plus these
two other KL terms.

1713
01:05:36,810 --> 01:05:40,770
So these two KL
terms are basically

1714
01:05:40,770 --> 01:05:42,730
measuring discrepancy
or dissimilarity

1715
01:05:42,730 --> 01:05:44,970
between these different
probability distributions

1716
01:05:44,970 --> 01:05:47,330
that we have floating
around on this slide.

1717
01:05:47,330 --> 01:05:49,610
And now these all look crazy.

1718
01:05:49,610 --> 01:05:51,750
But if we stare at
each of these terms,

1719
01:05:51,750 --> 01:05:55,730
we can actually recover
an interpretable meaning

1720
01:05:55,730 --> 01:05:57,730
for each of these three terms.

1721
01:05:57,730 --> 01:06:00,290
This first one is actually
a data reconstruction term.

1722
01:06:00,290 --> 01:06:02,090
If we walk through
what this is saying,

1723
01:06:02,090 --> 01:06:04,110
this is saying that we're
going to sample a z.

1724
01:06:04,110 --> 01:06:08,970
And the way we're going to
sample the z is by q of z

1725
01:06:08,970 --> 01:06:11,230
given x, which is our encoder.

1726
01:06:11,230 --> 01:06:13,750
So we're going to take our
x, pass it to the encoder.

1727
01:06:13,750 --> 01:06:17,267
The encoder is going to predict
a distribution q of z given x.

1728
01:06:17,267 --> 01:06:18,850
Then from that
predicted distribution,

1729
01:06:18,850 --> 01:06:20,250
we're going to sample a z.

1730
01:06:20,250 --> 01:06:23,130
Then we're going to take an
expectation over all such z

1731
01:06:23,130 --> 01:06:26,170
and maximize the log
probability of x given z.

1732
01:06:26,170 --> 01:06:28,450
So this is basically a
data reconstruction term.

1733
01:06:28,450 --> 01:06:31,550
It's saying that if we
take an x, a data point x,

1734
01:06:31,550 --> 01:06:33,890
run it through the encoder
to get a distribution over z,

1735
01:06:33,890 --> 01:06:38,170
and then pass any sample of that
predicted distribution over z

1736
01:06:38,170 --> 01:06:40,550
into the decoder, we're
going to recover x.

1737
01:06:40,550 --> 01:06:43,490
So this is a data
reconstruction term.

1738
01:06:43,490 --> 01:06:45,270
The middle one is a prior term.

1739
01:06:45,270 --> 01:06:46,850
This is saying we want to--

1740
01:06:46,850 --> 01:06:49,930
this is measuring the KL
divergence between q of z

1741
01:06:49,930 --> 01:06:51,450
given x and p of z.

1742
01:06:51,450 --> 01:06:54,810
So remember, q of z given
x, this is the encoder,

1743
01:06:54,810 --> 01:06:58,410
is inputting the data x and
outputting a distribution

1744
01:06:58,410 --> 01:07:00,410
over the latent space z.

1745
01:07:00,410 --> 01:07:02,690
So this is the predicted
distribution over the latent

1746
01:07:02,690 --> 01:07:04,030
space of the encoder.

1747
01:07:04,030 --> 01:07:06,338
And this other term p
of z, this is the prior.

1748
01:07:06,338 --> 01:07:08,630
This is the prior that we
assumed for the latent space,

1749
01:07:08,630 --> 01:07:10,030
usually diagonal Gaussian.

1750
01:07:10,030 --> 01:07:11,790
So this term is
basically saying,

1751
01:07:11,790 --> 01:07:14,130
well, the model is
separately predicting--

1752
01:07:14,130 --> 01:07:17,070
is predicting
distributions of z given x,

1753
01:07:17,070 --> 01:07:18,850
and we want those
predicted distributions

1754
01:07:18,850 --> 01:07:20,810
to match the simple
Gaussian prior

1755
01:07:20,810 --> 01:07:23,830
that we had previously set,
that we had previously chosen.

1756
01:07:23,830 --> 01:07:25,290
So this is just
measuring how much

1757
01:07:25,290 --> 01:07:27,650
does that latent space
that's learned by our model

1758
01:07:27,650 --> 01:07:29,290
match the prior.

1759
01:07:29,290 --> 01:07:31,770
And this third term
gets us in trouble.

1760
01:07:31,770 --> 01:07:34,330
So this third term
is q of z given

1761
01:07:34,330 --> 01:07:38,050
x, so that's the predicted
distribution over z given

1762
01:07:38,050 --> 01:07:41,970
the input data x to the
encoder, and how much

1763
01:07:41,970 --> 01:07:44,330
does that match p of z given x?

1764
01:07:44,330 --> 01:07:47,530
So that's this flipped
around distribution of what

1765
01:07:47,530 --> 01:07:49,210
the decoder is modeling.

1766
01:07:49,210 --> 01:07:50,990
And this one we're out of luck.

1767
01:07:50,990 --> 01:07:53,490
We cannot compute this term
because remember what got us

1768
01:07:53,490 --> 01:07:56,270
into trouble in the first
place was this p of z given x.

1769
01:07:56,270 --> 01:08:00,330
The whole reason we introduced
q was because we could not

1770
01:08:00,330 --> 01:08:03,530
compute this p of z given x.

1771
01:08:03,530 --> 01:08:06,250
So now what do we do?

1772
01:08:06,250 --> 01:08:07,610
We're going to throw it away.

1773
01:08:07,610 --> 01:08:10,090
Because we know that KL
divergences are always greater

1774
01:08:10,090 --> 01:08:12,770
than or equal to 0, so we
know that this last term,

1775
01:08:12,770 --> 01:08:15,167
because it's KL divergence
of two distributions,

1776
01:08:15,167 --> 01:08:17,750
Even though we cannot compute
those distributions, in general,

1777
01:08:17,750 --> 01:08:19,450
we know that it must
be greater than 0

1778
01:08:19,450 --> 01:08:22,410
because that's a well-known
property of KL divergences.

1779
01:08:22,410 --> 01:08:25,850
So we can throw it away
and get a lower bound

1780
01:08:25,850 --> 01:08:27,290
to the true probability.

1781
01:08:27,290 --> 01:08:29,189
So if we throw away
that last term,

1782
01:08:29,189 --> 01:08:32,370
then we know that log
p of x is greater than

1783
01:08:32,370 --> 01:08:35,370
or equal to those two terms,
are reconstruction term

1784
01:08:35,370 --> 01:08:36,175
and our prior term.

1785
01:08:36,175 --> 01:08:38,050
So this will be the loss
that we use to train

1786
01:08:38,050 --> 01:08:40,090
our variational autoencoder.

1787
01:08:40,090 --> 01:08:43,130
And the idea is that this is an
approximation to the true log

1788
01:08:43,130 --> 01:08:43,830
likelihood.

1789
01:08:43,830 --> 01:08:46,029
This is a lower bound
to the log likelihood.

1790
01:08:46,029 --> 01:08:48,010
So if we maximize
the lower bound,

1791
01:08:48,010 --> 01:08:50,649
hopefully that will also
maximize the true log likelihood

1792
01:08:50,649 --> 01:08:53,090
even though we're
not doing it exactly.

1793
01:08:53,090 --> 01:08:55,870
So that's our training objective
for variational autoencoders.

1794
01:08:55,870 --> 01:08:58,710
So that's the summary.

1795
01:08:58,710 --> 01:09:02,130
You're going to jointly train
an encoder q and a decoder p

1796
01:09:02,130 --> 01:09:05,930
to maximize what's called
a variational lower bound

1797
01:09:05,930 --> 01:09:07,513
on the true data log likelihood.

1798
01:09:07,513 --> 01:09:09,930
And this is also sometimes
called the evidence lower bound

1799
01:09:09,930 --> 01:09:10,793
or ELBo.

1800
01:09:10,793 --> 01:09:11,710
So it's just the ELBo.

1801
01:09:11,710 --> 01:09:13,529
We're going to
maximize the ELBo.

1802
01:09:13,529 --> 01:09:14,950
And it has this particular term.

1803
01:09:14,950 --> 01:09:17,529
We have these encoder
network this decoder network.

1804
01:09:17,529 --> 01:09:18,649
That's what we do.

1805
01:09:18,649 --> 01:09:21,410
So then to walk through what
the training procedure looks

1806
01:09:21,410 --> 01:09:24,609
like more explicitly,
we're going

1807
01:09:24,609 --> 01:09:26,930
to have this neural network
encoder, inputs the x,

1808
01:09:26,930 --> 01:09:29,850
outputs the distribution
over z, then

1809
01:09:29,850 --> 01:09:32,130
we're going to
apply this KL term

1810
01:09:32,130 --> 01:09:34,520
to the predicted distribution.

1811
01:09:34,520 --> 01:09:35,972
And in particular,
because this is

1812
01:09:35,972 --> 01:09:37,680
going to force the
predicted distribution

1813
01:09:37,680 --> 01:09:39,310
to be unit Gaussian,
so it's basically

1814
01:09:39,310 --> 01:09:41,560
going to force-- it's going
to encourage the predicted

1815
01:09:41,560 --> 01:09:47,000
mean to be 0 and the predicted
sigma to be all ones.

1816
01:09:47,000 --> 01:09:50,620
Then once we get those predicted
distribution from the encoder,

1817
01:09:50,620 --> 01:09:53,040
we're going to sample from
that predicted distribution

1818
01:09:53,040 --> 01:09:55,040
using this so-called
reparameterization

1819
01:09:55,040 --> 01:09:57,500
trick that allows you to
backprop through this.

1820
01:09:57,500 --> 01:10:00,400
So then we draw a sample z from
the predicted distribution.

1821
01:10:00,400 --> 01:10:04,840
Once you get this sample z,
you run it through your decoder

1822
01:10:04,840 --> 01:10:08,680
to get your normal distribution
predicted by the decoder,

1823
01:10:08,680 --> 01:10:12,520
and then you apply your
reconstruction term of the loss

1824
01:10:12,520 --> 01:10:14,120
to the output of the decoder.

1825
01:10:14,120 --> 01:10:18,300
So even though this looked like
a large, scary slides of math,

1826
01:10:18,300 --> 01:10:21,000
it actually led to not too
crazy of a training objective

1827
01:10:21,000 --> 01:10:22,833
for this thing.

1828
01:10:22,833 --> 01:10:25,000
And I think this variational
autoencoder is actually

1829
01:10:25,000 --> 01:10:26,417
very interesting
because these two

1830
01:10:26,417 --> 01:10:29,732
losses fight against each other
in a very interesting way.

1831
01:10:29,732 --> 01:10:31,440
Because we're basically
forcing the model

1832
01:10:31,440 --> 01:10:34,000
to bottleneck through
this latent space

1833
01:10:34,000 --> 01:10:36,480
z, and these two terms
want different things

1834
01:10:36,480 --> 01:10:39,720
from the latent space, so
the reconstruction loss

1835
01:10:39,720 --> 01:10:41,840
wants the sigma
to be 0 and the mu

1836
01:10:41,840 --> 01:10:46,000
x to be a different and
unique vector for each data x.

1837
01:10:46,000 --> 01:10:47,680
Because if that
were the case, then

1838
01:10:47,680 --> 01:10:50,460
we could perfectly satisfy
the reconstruction objective.

1839
01:10:50,460 --> 01:10:54,480
We would have a separate unique
vector for every data point.

1840
01:10:54,480 --> 01:10:56,350
And there would be no
probability in there.

1841
01:10:56,350 --> 01:10:58,100
We could perfectly
reconstruct everything.

1842
01:10:58,100 --> 01:11:00,420
So that's what the
reconstruction loss wants.

1843
01:11:00,420 --> 01:11:02,560
But the prior loss
actually wants the sigmas

1844
01:11:02,560 --> 01:11:05,820
to be all one because it wants
it wants it to be unit Gaussian,

1845
01:11:05,820 --> 01:11:07,880
and it wants all
the mu's to be 0

1846
01:11:07,880 --> 01:11:10,340
which is very different from
what the two losses want.

1847
01:11:10,340 --> 01:11:12,200
So in the process
of training a VAE,

1848
01:11:12,200 --> 01:11:14,760
you're asking these two losses
to fight against each other

1849
01:11:14,760 --> 01:11:18,120
to try to find some equilibrium
between reconstructing

1850
01:11:18,120 --> 01:11:20,560
your data well and forcing
your latent space to be

1851
01:11:20,560 --> 01:11:22,092
close to your prior.

1852
01:11:22,092 --> 01:11:23,800
And then once you've
trained it, then you

1853
01:11:23,800 --> 01:11:26,440
can sample z from your prior,
run through the decoder,

1854
01:11:26,440 --> 01:11:27,840
and get a sample.

1855
01:11:27,840 --> 01:11:30,520
Another nice thing is that
because your latent space was

1856
01:11:30,520 --> 01:11:32,560
diagonal Gaussian,
there's also a notion

1857
01:11:32,560 --> 01:11:37,960
of statistical independence
across the different entries

1858
01:11:37,960 --> 01:11:39,300
in your latent space z.

1859
01:11:39,300 --> 01:11:41,040
So you can vary them separately.

1860
01:11:41,040 --> 01:11:42,920
And maybe those
separate dimensions

1861
01:11:42,920 --> 01:11:46,280
often encode something useful
or interpretable or orthogonal

1862
01:11:46,280 --> 01:11:47,360
about your data.

1863
01:11:47,360 --> 01:11:49,120
So in this case, we
took a VAE, trained it

1864
01:11:49,120 --> 01:11:50,860
on a data set of
handwritten digits.

1865
01:11:50,860 --> 01:11:54,240
And you see that as we vary two
dimensions of the latent space,

1866
01:11:54,240 --> 01:11:58,940
the digits smoothly morph from
one category into another.

1867
01:11:58,940 --> 01:12:01,760
And this is a pretty
common property of VAEs.

1868
01:12:01,760 --> 01:12:03,600
So that's basically
it for today.

1869
01:12:03,600 --> 01:12:06,280
To recap what we talked about,
we talked about supervised

1870
01:12:06,280 --> 01:12:07,700
versus unsupervised learning.

1871
01:12:07,700 --> 01:12:09,600
We talked about these
three different flavors

1872
01:12:09,600 --> 01:12:10,880
of generative modeling.

1873
01:12:10,880 --> 01:12:13,400
And then we talked about
one branch of this family

1874
01:12:13,400 --> 01:12:15,000
tree of generative models.

1875
01:12:15,000 --> 01:12:18,623
So then next time, we're
going to come back and talk

1876
01:12:18,623 --> 01:12:20,040
about the other
half of the family

1877
01:12:20,040 --> 01:12:22,120
tree of generative models,
in particular, talking

1878
01:12:22,120 --> 01:12:25,590
about generative adversarial
networks and diffusion models.