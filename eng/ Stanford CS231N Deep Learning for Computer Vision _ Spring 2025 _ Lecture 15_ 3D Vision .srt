2
00:00:05,440 --> 00:00:08,160
I'm really happy to
announce our next guest

3
00:00:08,160 --> 00:00:11,120
speaker for the course,
Professor Jiajun Wu.

4
00:00:11,120 --> 00:00:14,597
So Jiajun is an assistant
professor here at Stanford

5
00:00:14,597 --> 00:00:16,180
in the Department
of Computer Science,

6
00:00:16,180 --> 00:00:22,720
and he's a faculty member of
the Stanford Vision and Learning

7
00:00:22,720 --> 00:00:24,040
Lab.

8
00:00:24,040 --> 00:00:27,240
His research focuses on scene
understanding with an emphasis

9
00:00:27,240 --> 00:00:31,360
on multimodal perception,
robotics and embodied AI,

10
00:00:31,360 --> 00:00:35,740
visual generation and
reasoning, and 3D understanding,

11
00:00:35,740 --> 00:00:38,120
which is the topic
of today's lecture.

12
00:00:38,120 --> 00:00:41,920
And so I'll now turn it over to
Jiajun to begin today's lecture.

13
00:00:41,920 --> 00:00:43,340
OK, yeah, so I'm Jiajun.

14
00:00:43,340 --> 00:00:44,860
I'm an assistant professor here.

15
00:00:44,860 --> 00:00:46,360
And I think, a few
years ago, I used

16
00:00:46,360 --> 00:00:48,840
to teach this class, co-teach.

17
00:00:48,840 --> 00:00:51,940
So I heard this year, it's
the 10th year anniversary.

18
00:00:51,940 --> 00:00:55,360
So you have guest speakers
from different places.

19
00:00:55,360 --> 00:00:58,520
OK, so today, we're going
to talk about 3D vision.

20
00:00:58,520 --> 00:01:01,130
So it might be kind of
different from a lot of things

21
00:01:01,130 --> 00:01:05,938
you learned before because, I
think, in the past few weeks,

22
00:01:05,938 --> 00:01:07,730
we talked about
convolution neural networks

23
00:01:07,730 --> 00:01:09,970
and transformers and maybe
vision language models

24
00:01:09,970 --> 00:01:12,100
and generative models
as well for Justin.

25
00:01:12,100 --> 00:01:13,730
Right, OK.

26
00:01:13,730 --> 00:01:16,690
Yeah, so here, for 3D, I think
I'm going to first introduce

27
00:01:16,690 --> 00:01:19,730
a little bit on, what are
the 3D representations?

28
00:01:19,730 --> 00:01:22,370
So it's more like, it's
pretty distant from all

29
00:01:22,370 --> 00:01:23,430
the deep learning stuff.

30
00:01:23,430 --> 00:01:25,850
But then we're going to talk
about how deep learning or AI

31
00:01:25,850 --> 00:01:29,010
has changed 3D vision and
how they can be integrated

32
00:01:29,010 --> 00:01:30,290
in different ways.

33
00:01:30,290 --> 00:01:32,930
And we're looking into a few
different applications about 3D

34
00:01:32,930 --> 00:01:35,570
generation, reconstruction,
and stuff like that.

35
00:01:35,570 --> 00:01:39,050
OK, so let's begin by looking
into, what are the possible ways

36
00:01:39,050 --> 00:01:41,070
to represent objects in 3D?

37
00:01:41,070 --> 00:01:42,755
Because in 2D, it's
so straightforward.

38
00:01:42,755 --> 00:01:44,130
It looks like I
just have pixels.

39
00:01:44,130 --> 00:01:47,750
I have a loading a file of
a PNG file or a JPEG file.

40
00:01:47,750 --> 00:01:50,130
It's like 200 by 200 pixels.

41
00:01:50,130 --> 00:01:51,950
But how can we
represent 3D objects?

42
00:01:51,950 --> 00:01:54,890
I think that's the first
thing we want to look into.

43
00:01:54,890 --> 00:01:59,260
And 3D objects,
they can be diverse.

44
00:01:59,260 --> 00:02:01,520
They can be at different scales.

45
00:02:01,520 --> 00:02:05,580
It can be huge, large buildings
and trees, complex structures.

46
00:02:05,580 --> 00:02:09,380
And if you zoom in, you can
also see all the fine details.

47
00:02:09,380 --> 00:02:11,820
So what are the best 3D
representations to represent all

48
00:02:11,820 --> 00:02:14,420
these different types of 3D
objects at different scales with

49
00:02:14,420 --> 00:02:16,860
different features?

50
00:02:16,860 --> 00:02:20,220
And unlike images, where
everyone just uses pixels,

51
00:02:20,220 --> 00:02:24,260
so we have 200 by
200, 500 by 500.

52
00:02:24,260 --> 00:02:27,780
The way to represent 3D
objects, objects are also like,

53
00:02:27,780 --> 00:02:28,650
you have geometry.

54
00:02:28,650 --> 00:02:29,400
You have textures.

55
00:02:29,400 --> 00:02:30,320
You have materials.

56
00:02:30,320 --> 00:02:32,640
But let's just maybe start
by looking at geometry.

57
00:02:32,640 --> 00:02:34,495
And even just for
3D object geometry,

58
00:02:34,495 --> 00:02:36,620
there are so many different
ways to represent them.

59
00:02:36,620 --> 00:02:39,680
We can basically categorize them
into two different categories.

60
00:02:39,680 --> 00:02:41,860
One is called explicit
representations.

61
00:02:41,860 --> 00:02:45,140
So where you can, in
some sense, directly, I

62
00:02:45,140 --> 00:02:46,700
would say, explicitly
representing

63
00:02:46,700 --> 00:02:47,680
part of the objects.

64
00:02:47,680 --> 00:02:50,160
This includes things
like point clouds,

65
00:02:50,160 --> 00:02:52,460
if you have a
cloud of 3D points,

66
00:02:52,460 --> 00:02:55,720
or a polygon mesh
or subdivisions,

67
00:02:55,720 --> 00:02:59,063
which we're going to talk
about it, and others.

68
00:02:59,063 --> 00:03:00,480
And there's a
different categories

69
00:03:00,480 --> 00:03:03,550
of object shape representations,
which are often called implicit.

70
00:03:03,550 --> 00:03:05,300
So we're going to talk
about them as well.

71
00:03:05,300 --> 00:03:07,425
So I'm going to explain
them a little bit of detail

72
00:03:07,425 --> 00:03:09,840
later, including level sets,
algebraic surfaces, distance

73
00:03:09,840 --> 00:03:10,660
functions.

74
00:03:10,660 --> 00:03:13,080
So they are basically
representing 3D objects

75
00:03:13,080 --> 00:03:14,680
or their geometries
as functions,

76
00:03:14,680 --> 00:03:17,720
which it is not directly--
it's not as, in some sense,

77
00:03:17,720 --> 00:03:19,985
intuitive as, oh, it's just
a collection of points.

78
00:03:19,985 --> 00:03:21,360
But as we'll see
later, they also

79
00:03:21,360 --> 00:03:24,280
have their own
advantages and weaknesses

80
00:03:24,280 --> 00:03:26,240
using implicit representations.

81
00:03:26,240 --> 00:03:29,240
So every choice, they have
their suitable task and type

82
00:03:29,240 --> 00:03:30,155
of geometry.

83
00:03:30,155 --> 00:03:32,280
And in particular, in the
context of deep learning,

84
00:03:32,280 --> 00:03:34,488
they may also have their
own strengths and weaknesses

85
00:03:34,488 --> 00:03:37,960
when you want to apply deep
learning methods on top of it.

86
00:03:37,960 --> 00:03:41,480
So when do we choose
a representation?

87
00:03:41,480 --> 00:03:43,480
We have to store
them, so pixels are

88
00:03:43,480 --> 00:03:45,640
easy to store because
it's just a matrix.

89
00:03:45,640 --> 00:03:48,620
But then 3D point clouds
are more irregular.

90
00:03:48,620 --> 00:03:51,320
And also, especially if you use
some implicit representations,

91
00:03:51,320 --> 00:03:53,340
like representing
object as a function,

92
00:03:53,340 --> 00:03:56,090
how would you store
that in a computer?

93
00:03:56,090 --> 00:03:59,370
And how does it support
creating new shapes?

94
00:03:59,370 --> 00:04:02,210
And especially now, let's say,
maybe the input is a picture,

95
00:04:02,210 --> 00:04:04,770
or the input is a
language description.

96
00:04:04,770 --> 00:04:06,390
And different type
of operations--

97
00:04:06,390 --> 00:04:07,550
you have 3D objects.

98
00:04:07,550 --> 00:04:10,370
And then how can you edit it,
simplify it, smooth it, filter,

99
00:04:10,370 --> 00:04:11,310
and repair it?

100
00:04:11,310 --> 00:04:12,850
So you can have
to do a lot more.

101
00:04:12,850 --> 00:04:14,917
For images, sometimes
you want to do that, too.

102
00:04:14,917 --> 00:04:15,750
You want to edit it.

103
00:04:15,750 --> 00:04:17,208
You want to edit
it using language.

104
00:04:17,208 --> 00:04:19,010
You want to edit
it using stroke.

105
00:04:19,010 --> 00:04:22,210
And how can you edit or perform
any type of operations on 3D

106
00:04:22,210 --> 00:04:24,070
objects and rendering?

107
00:04:24,070 --> 00:04:28,112
How can you turn that 3D
objects into 2D pixels?

108
00:04:28,112 --> 00:04:29,070
In some sense, you can.

109
00:04:29,070 --> 00:04:31,210
3D vision is to
invert the process.

110
00:04:31,210 --> 00:04:35,432
How can you go from 2D images
to reconstruct the 3D objects?

111
00:04:35,432 --> 00:04:37,390
So how they support all
these different things,

112
00:04:37,390 --> 00:04:40,870
including in animations,
especially if you are modeling,

113
00:04:40,870 --> 00:04:43,790
let's say, 3D humans or animals,
and you want to animate them,

114
00:04:43,790 --> 00:04:46,130
so all these factors
need to be considered.

115
00:04:46,130 --> 00:04:49,170
And something else, of course,
that connects through all these

116
00:04:49,170 --> 00:04:51,780
is also their integration with
different deep learning methods

117
00:04:51,780 --> 00:04:55,180
for, let's say, shape editing,
rendering, inverse rendering,

118
00:04:55,180 --> 00:04:57,260
and animation as well.

119
00:04:57,260 --> 00:04:59,380
So very quickly, I
can go through some

120
00:04:59,380 --> 00:05:01,780
of these representations
like point clouds.

121
00:05:01,780 --> 00:05:05,820
So point cloud is probably
the simplest representations.

122
00:05:05,820 --> 00:05:07,120
I only have 3D points.

123
00:05:07,120 --> 00:05:09,508
It doesn't have connectivity,
so it doesn't capture

124
00:05:09,508 --> 00:05:10,800
how these points are connected.

125
00:05:10,800 --> 00:05:12,383
So you basically
just have a-- instead

126
00:05:12,383 --> 00:05:14,260
of having an n by
[? n ?] matrix,

127
00:05:14,260 --> 00:05:16,660
which is about the pixel
values of all the pixels

128
00:05:16,660 --> 00:05:20,060
in the picture, now you
have a 3 by n matrix,

129
00:05:20,060 --> 00:05:23,960
where 3 is x, y, z coordinates
of these individual points.

130
00:05:23,960 --> 00:05:26,300
And you have a number of points.

131
00:05:26,300 --> 00:05:29,853
So sometimes, you can represent
the surface normals of the point

132
00:05:29,853 --> 00:05:32,020
as well so that you have
not only where the point is

133
00:05:32,020 --> 00:05:35,960
in the 3D space, but also to
which direction it's facing.

134
00:05:35,960 --> 00:05:38,740
So you have the surface
normals, which give you

135
00:05:38,740 --> 00:05:40,240
a bit more information.

136
00:05:40,240 --> 00:05:42,180
And sometimes, people
call them surfels, which

137
00:05:42,180 --> 00:05:46,020
is points with orientations.

138
00:05:46,020 --> 00:05:48,322
And yeah, so why do you
need surface normals?

139
00:05:48,322 --> 00:05:49,780
Because if you want
to render them,

140
00:05:49,780 --> 00:05:52,430
you want to say how
the object look like,

141
00:05:52,430 --> 00:05:55,310
then that means you have to
often specify a lighting source.

142
00:05:55,310 --> 00:05:57,350
Where's the lighting
coming from?

143
00:05:57,350 --> 00:05:59,430
But to make the
rendering look realistic,

144
00:05:59,430 --> 00:06:01,630
you have to consider
how the lighting, coming

145
00:06:01,630 --> 00:06:03,350
from a certain
direction, is going

146
00:06:03,350 --> 00:06:05,090
to interact with the point.

147
00:06:05,090 --> 00:06:07,230
And this is where
the surface normals

148
00:06:07,230 --> 00:06:10,070
is used to help you to make
the rendering look realistic,

149
00:06:10,070 --> 00:06:12,550
like you can see here.

150
00:06:12,550 --> 00:06:14,590
So how can you get points?

151
00:06:14,590 --> 00:06:18,790
A benefit of the point cloud, it
is often a raw format that you

152
00:06:18,790 --> 00:06:21,710
will get from a lot
of the 3D sensors,

153
00:06:21,710 --> 00:06:25,110
including these depth sensors,
including some 3D scanners.

154
00:06:25,110 --> 00:06:27,970
And nowadays, I think if
you can even use an iPhone.

155
00:06:27,970 --> 00:06:30,870
I think they have an AR kit, or
these kind of software allow you

156
00:06:30,870 --> 00:06:32,230
to scan 3D objects.

157
00:06:32,230 --> 00:06:34,770
But the raw output
of those sensors,

158
00:06:34,770 --> 00:06:36,050
they're still 3D point clouds.

159
00:06:36,050 --> 00:06:37,550
Now, of course,
after that, you have

160
00:06:37,550 --> 00:06:40,103
to process them and fuse
them to make it like maybe

161
00:06:40,103 --> 00:06:41,020
objects with textures.

163
00:06:43,590 --> 00:06:45,570
So yeah, they often
results from scanners.

164
00:06:45,570 --> 00:06:47,185
They can potentially
be very noisy.

165
00:06:47,185 --> 00:06:48,810
And there are things
like this, and you

166
00:06:48,810 --> 00:06:52,130
want to fuse them,
merge them, repair them.

167
00:06:52,130 --> 00:06:54,650
And in this part,
you have to consider

168
00:06:54,650 --> 00:06:57,770
how these different pictures
can be registered to give you

169
00:06:57,770 --> 00:07:00,810
the shared point cloud.

170
00:07:00,810 --> 00:07:04,490
And they're very flexible,
so you can-- because you

171
00:07:04,490 --> 00:07:05,870
can move points here and there.

172
00:07:05,870 --> 00:07:07,850
So you can use them
to represent basically

173
00:07:07,850 --> 00:07:09,110
any type of object geometry.

174
00:07:09,110 --> 00:07:11,770
You're not constrained by the
topology or stuff like that.

175
00:07:11,770 --> 00:07:14,290
It's useful for large data
sets because sometimes, you

176
00:07:14,290 --> 00:07:17,330
have to consider a very
diverse set of objects.

177
00:07:17,330 --> 00:07:20,370
But because the points are
already, in some sense--

178
00:07:20,370 --> 00:07:23,390
you consider they
being presampled.

179
00:07:23,390 --> 00:07:26,830
So if you have a lot of points--
if you're representing objects,

180
00:07:26,830 --> 00:07:29,342
but your points are sampled
in an even way in the sense

181
00:07:29,342 --> 00:07:31,050
that you have a lot
of points, let's say,

182
00:07:31,050 --> 00:07:34,090
on the head of the rabbit, but
you have very, very few points

183
00:07:34,090 --> 00:07:35,690
on the tail of the
rabbit, then it

184
00:07:35,690 --> 00:07:37,650
will be actually
hard to draw samples

185
00:07:37,650 --> 00:07:39,470
from these undersampled regions.

186
00:07:39,470 --> 00:07:41,950
So sometimes, when people
consider sampling points,

187
00:07:41,950 --> 00:07:43,490
you have to design
algorithm to make

188
00:07:43,490 --> 00:07:45,770
sure you sample
them roughly evenly,

189
00:07:45,770 --> 00:07:48,020
across different
parts of the objects.

190
00:07:48,020 --> 00:07:50,900
And other limitations,
or it's not obvious

191
00:07:50,900 --> 00:07:52,460
how we can directly
perform sometimes

192
00:07:52,460 --> 00:07:55,340
the very useful operations, like
simplification or subdivisions,

193
00:07:55,340 --> 00:07:57,460
on these objects.

194
00:07:57,460 --> 00:08:00,500
It doesn't directly allow
you to do smooth rendering.

195
00:08:00,500 --> 00:08:02,620
There's no topological
information.

196
00:08:02,620 --> 00:08:06,620
So for example here, if I give
you a collection of points,

197
00:08:06,620 --> 00:08:11,540
then you can't even tell if
this is like a torus or this is

198
00:08:11,540 --> 00:08:14,588
like these ring-like shapes,
because it doesn't tell you

199
00:08:14,588 --> 00:08:15,880
how these points are connected.

200
00:08:15,880 --> 00:08:20,220
So it's a partial information
about what the object is if you

201
00:08:20,220 --> 00:08:21,783
just have the point clouds.

202
00:08:21,783 --> 00:08:23,200
So naturally,
people will say, OK,

203
00:08:23,200 --> 00:08:25,468
how can I actually
capture more information

204
00:08:25,468 --> 00:08:27,260
so that I can distinguish
between these two

205
00:08:27,260 --> 00:08:28,540
different objects?

206
00:08:28,540 --> 00:08:32,440
Then naturally, that goes
to the polygonal meshes.

207
00:08:32,440 --> 00:08:36,080
So it represents the objects
still as a collection of points,

208
00:08:36,080 --> 00:08:38,507
but then also how these
points are connected.

209
00:08:38,507 --> 00:08:40,840
So now you have not only the
points, but also the faces,

210
00:08:40,840 --> 00:08:43,179
the surfaces.

211
00:08:43,179 --> 00:08:45,930
And this is arguably,
I would say,

212
00:08:45,930 --> 00:08:48,310
the most widely used
representation for 3D objects

213
00:08:48,310 --> 00:08:51,350
in all these graphics engines
and in computer games and stuff

214
00:08:51,350 --> 00:08:54,630
like that, basically it is all
represented as polygon meshes.

215
00:08:54,630 --> 00:08:57,010
But you can see that
to represent faces,

216
00:08:57,010 --> 00:09:00,990
it is more complex because
often, you have to consider--

217
00:09:00,990 --> 00:09:03,040
especially if you're
looking at raw meshes,

218
00:09:03,040 --> 00:09:05,290
then every face may have a
different number of points.

219
00:09:05,290 --> 00:09:07,873
You may have three points and
have four points or five points.

220
00:09:07,873 --> 00:09:10,310
And how you can represent
them, especially given

221
00:09:10,310 --> 00:09:12,645
their irregularity, how
you can integrate them

222
00:09:12,645 --> 00:09:15,270
with neural networks, especially
in the early stage when people

223
00:09:15,270 --> 00:09:17,070
start with convolution
neural networks,

224
00:09:17,070 --> 00:09:18,930
they always assume
a fixed resolution.

225
00:09:18,930 --> 00:09:20,650
But here you have not--

226
00:09:20,650 --> 00:09:24,730
I would say a variable dimension
of these raw information.

227
00:09:24,730 --> 00:09:26,610
How does that integrate
with deep learning?

228
00:09:26,610 --> 00:09:27,770
That has been some
big challenge.

229
00:09:27,770 --> 00:09:30,410
That's why deep learning with
3D vision started kind of late,

230
00:09:30,410 --> 00:09:33,270
because people were thinking
about how we can adapt all these

231
00:09:33,270 --> 00:09:36,870
deep learning methods to
deal with all these complex

232
00:09:36,870 --> 00:09:40,070
representations for objects,
which are not as unified

233
00:09:40,070 --> 00:09:42,270
as images.

234
00:09:42,270 --> 00:09:44,140
But meshes are
really widely used,

235
00:09:44,140 --> 00:09:45,880
and they can be
very complex meshes

236
00:09:45,880 --> 00:09:47,680
that capture all the details.

237
00:09:47,680 --> 00:09:49,215
For example, you have scanners.

238
00:09:49,215 --> 00:09:50,840
You get points, and
then you fuse them.

239
00:09:50,840 --> 00:09:52,260
And you apply some algorithm.

240
00:09:52,260 --> 00:09:54,860
You can get very large mesh.

241
00:09:54,860 --> 00:09:58,320
This one has 56
million triangles

242
00:09:58,320 --> 00:10:04,200
and 28 million vertices to
represent the sculpture.

243
00:10:04,200 --> 00:10:06,480
And you can have even
larger ones, let's say,

244
00:10:06,480 --> 00:10:07,420
from Google Earth.

245
00:10:07,420 --> 00:10:08,820
They have trillions
of triangles.

246
00:10:08,820 --> 00:10:13,138
Try to represent basically
all the buildings on Earth.

247
00:10:13,138 --> 00:10:15,680
The nice thing about meshes, it
supports a lot of operations,

248
00:10:15,680 --> 00:10:16,657
like subdivisions.

249
00:10:16,657 --> 00:10:18,240
Or I want to have
more details and how

250
00:10:18,240 --> 00:10:22,837
can I use more meshes to capture
more details of the shape?

251
00:10:22,837 --> 00:10:24,420
And you can do
simplification as well.

252
00:10:24,420 --> 00:10:25,962
Sometimes, you want
to process things

253
00:10:25,962 --> 00:10:28,140
very fast, so I don't
need that many meshes.

254
00:10:28,140 --> 00:10:30,640
I just want to simplify
it, and so there

255
00:10:30,640 --> 00:10:34,520
are existing algorithms that
allow you to do that as well.

256
00:10:34,520 --> 00:10:37,520
And regularization-- if
you get irregular mesh,

257
00:10:37,520 --> 00:10:39,360
and sometimes you want
to regularize them

258
00:10:39,360 --> 00:10:42,140
so that every face
is a triangle,

259
00:10:42,140 --> 00:10:43,840
they always connect
three vertices.

260
00:10:43,840 --> 00:10:47,300
They have roughly the
same size and so that it's

261
00:10:47,300 --> 00:10:48,360
easier for processing.

262
00:10:48,360 --> 00:10:51,100
And they have more
obviously good properties

263
00:10:51,100 --> 00:10:54,660
that supports future processing
of different graphics algorithms

264
00:10:54,660 --> 00:10:55,580
and meshes.

265
00:10:55,580 --> 00:10:58,080
There have been people who
develop these algorithms as well

266
00:10:58,080 --> 00:11:01,600
so that you can
ensure that basically,

267
00:11:01,600 --> 00:11:04,100
points at different regions,
they are roughly evenly sampled

268
00:11:04,100 --> 00:11:05,220
so that it won't
be the case that,

269
00:11:05,220 --> 00:11:07,740
OK, let's say the head of the
tail or the head of the rabbit

270
00:11:07,740 --> 00:11:09,620
is much more densely
sampled than the tail

271
00:11:09,620 --> 00:11:12,060
and these kind of things.

272
00:11:12,060 --> 00:11:14,927
OK, so this is of one type
of shape representations.

273
00:11:14,927 --> 00:11:16,760
And there are other
type of representations,

274
00:11:16,760 --> 00:11:20,740
for example, parametric
representations, because objects

275
00:11:20,740 --> 00:11:22,308
are not just totally irregular.

276
00:11:22,308 --> 00:11:24,100
It's not just often a
collection of points.

277
00:11:24,100 --> 00:11:26,000
In meshes, they're very general.

278
00:11:26,000 --> 00:11:27,920
But sometimes, we lose
a lot of information.

279
00:11:27,920 --> 00:11:30,460
If you look at, let's say,
your chairs or your tables,

280
00:11:30,460 --> 00:11:31,877
you have all these
straight lines.

281
00:11:31,877 --> 00:11:34,127
So how can you represent
these kind of straight lines?

282
00:11:34,127 --> 00:11:35,940
And when people design
them, they often

283
00:11:35,940 --> 00:11:38,700
use some of these
parametric representations.

284
00:11:38,700 --> 00:11:41,390
So you can represent
shapes as a function.

285
00:11:41,390 --> 00:11:42,510
So think about it.

286
00:11:42,510 --> 00:11:45,870
So when I design them, I can--

287
00:11:45,870 --> 00:11:48,690
if I want to represent a
surface or represent a curve,

288
00:11:48,690 --> 00:11:51,670
the underlying degree of
freedom is actually lower.

289
00:11:51,670 --> 00:11:53,870
Often, if I have
a curve, there's

290
00:11:53,870 --> 00:11:55,683
only one underlying
degree of freedom.

291
00:11:55,683 --> 00:11:57,350
That's why I can
represent a curve using

292
00:11:57,350 --> 00:12:01,350
a function f of X. I just
varied x and get a value of y.

293
00:12:01,350 --> 00:12:03,750
So you can use basically
all these different types

294
00:12:03,750 --> 00:12:08,790
of functions in 2D, but
also more often in 3D to map

295
00:12:08,790 --> 00:12:11,390
a certain number of variables,
the underlying intrinsic

296
00:12:11,390 --> 00:12:13,890
dimensionality of the objects,
which are often, let's say,

297
00:12:13,890 --> 00:12:17,370
in two or even one, and then
mapping into the 3D space.

298
00:12:17,370 --> 00:12:19,990
And then this allows you
to represent 3D objects

299
00:12:19,990 --> 00:12:22,830
in a parametric representations
using basically a set

300
00:12:22,830 --> 00:12:23,790
of functions.

301
00:12:23,790 --> 00:12:26,970
You can do that for curves,
let's say, in circles.

302
00:12:26,970 --> 00:12:28,990
Basically, if you want
to represent a circle,

303
00:12:28,990 --> 00:12:30,950
you don't really need--

304
00:12:30,950 --> 00:12:33,190
one way is you sample
a number of points.

305
00:12:33,190 --> 00:12:35,850
Or you can even connect
them, like meshes.

306
00:12:35,850 --> 00:12:37,230
You're using the lines.

307
00:12:37,230 --> 00:12:40,120
Another way is you just
represent the curve

308
00:12:40,120 --> 00:12:41,782
of the circle as this function.

309
00:12:41,782 --> 00:12:43,240
Basically, there's
a sine function,

310
00:12:43,240 --> 00:12:48,500
a cosine function, and you just
vary one variable, which is t.

311
00:12:48,500 --> 00:12:50,700
You can think about it
as the degrees or angles,

312
00:12:50,700 --> 00:12:52,940
and it will map it to all
the points on the circles.

313
00:12:52,940 --> 00:12:56,560
So here now, you can use a
function to represent parametric

314
00:12:56,560 --> 00:12:59,100
representations
for curves in 2D.

315
00:12:59,100 --> 00:13:01,580
And of course, you can
do that in 3D as well.

316
00:13:01,580 --> 00:13:03,440
If you want to
represent a sphere,

317
00:13:03,440 --> 00:13:07,382
all you need is just two
degrees of freedom, u and v.

318
00:13:07,382 --> 00:13:09,840
And then you can go through
these functions so that you can

319
00:13:09,840 --> 00:13:14,200
map them to every point in
3D space for this sphere.

320
00:13:14,200 --> 00:13:15,480
So people have designed--

321
00:13:15,480 --> 00:13:16,900
I'm not going to detail here--

322
00:13:16,900 --> 00:13:18,960
but more complex
parametric representations,

323
00:13:18,960 --> 00:13:21,280
like Bézier curves
and Bézier surfaces,

324
00:13:21,280 --> 00:13:25,760
which allows you to represent
these pretty flexible and smooth

325
00:13:25,760 --> 00:13:30,680
surfaces in 3D using basically
a few control points.

326
00:13:30,680 --> 00:13:33,120
So you basically use
these Bézier functions

327
00:13:33,120 --> 00:13:35,520
to capture the underlying
lower dimensionalities

328
00:13:35,520 --> 00:13:37,370
of these surfaces,
and then you can

329
00:13:37,370 --> 00:13:39,370
map the underlying
low dimensionality

330
00:13:39,370 --> 00:13:43,370
into these flexible shapes.

331
00:13:43,370 --> 00:13:46,130
And they also allow you to
do things like subdivision,

332
00:13:46,130 --> 00:13:49,570
so you can trying to get more
details into the surfaces

333
00:13:49,570 --> 00:13:51,823
and to make it more fine-grained
and stuff like that.

334
00:13:51,823 --> 00:13:53,490
OK, so that would be
the second category

335
00:13:53,490 --> 00:13:54,550
of shape representations.

336
00:13:54,550 --> 00:13:57,630
You can represent 3D objects
in a nonparametric way,

337
00:13:57,630 --> 00:14:00,530
like a collection of unordered
points or their connections

338
00:14:00,530 --> 00:14:01,230
as meshes.

339
00:14:01,230 --> 00:14:04,050
Or you can represent them
in a parametric way, where

340
00:14:04,050 --> 00:14:05,390
you have a function, basically.

341
00:14:05,390 --> 00:14:08,650
And by varying a
few parameters that

342
00:14:08,650 --> 00:14:11,290
are underlying the true degree
of freedoms of the object

343
00:14:11,290 --> 00:14:15,970
geometry, you can map them
into more complex shapes.

344
00:14:15,970 --> 00:14:18,905
So basically, everything
here, as I said, it has been--

345
00:14:18,905 --> 00:14:20,530
if you remember, at
the very beginning,

346
00:14:20,530 --> 00:14:21,810
we said there are
two types of ways

347
00:14:21,810 --> 00:14:23,030
to represent object geometry.

348
00:14:23,030 --> 00:14:24,390
One is explicit.

349
00:14:24,390 --> 00:14:25,590
There's more implicit.

350
00:14:25,590 --> 00:14:27,570
And all of them, they
fall into this category

351
00:14:27,570 --> 00:14:29,070
of being quite explicit.

352
00:14:29,070 --> 00:14:30,790
So it's like, I have points.

353
00:14:30,790 --> 00:14:33,550
And points are just directly
points [INAUDIBLE] on objects.

354
00:14:33,550 --> 00:14:36,510
And for the surfaces, or for
parametric curves as well,

355
00:14:36,510 --> 00:14:41,670
they directly map it to
the points on the objects.

356
00:14:41,670 --> 00:14:43,650
So there are explicit
representations.

357
00:14:43,650 --> 00:14:45,830
They have a lot of benefits.

358
00:14:45,830 --> 00:14:49,950
First, you map all
the points directly,

359
00:14:49,950 --> 00:14:51,990
so you can get all these points.

360
00:14:51,990 --> 00:14:55,970
In general, every point I
have in, let's say, a sample,

361
00:14:55,970 --> 00:14:58,810
I have Bézier surface
representations.

362
00:14:58,810 --> 00:15:02,230
I can basically sample two
points and UV in this underlying

363
00:15:02,230 --> 00:15:04,990
lower dimensional space and then
go into that function and map it

364
00:15:04,990 --> 00:15:06,850
to a point in the 3D space.

365
00:15:06,850 --> 00:15:10,968
So I directly get a
point in a 3D space.

366
00:15:10,968 --> 00:15:13,510
So all points are given, in some
sense, you can say directly.

367
00:15:13,510 --> 00:15:15,430
I can directly get other points.

368
00:15:15,430 --> 00:15:17,810
So it's very easy for
us to sample points.

369
00:15:17,810 --> 00:15:20,910
So let's say I have this
torus, and I have represented

370
00:15:20,910 --> 00:15:22,450
using this f function.

371
00:15:22,450 --> 00:15:24,830
So now my question is, can
you just sample some points

372
00:15:24,830 --> 00:15:26,850
on the surface of
the object for me?

373
00:15:26,850 --> 00:15:29,230
This is so easy because
I would just randomly

374
00:15:29,230 --> 00:15:30,530
put in some u and v values.

375
00:15:30,530 --> 00:15:33,320
I just randomly sample those u
and v values and then let them

376
00:15:33,320 --> 00:15:37,240
go through this function, and
it will just compute and give me

377
00:15:37,240 --> 00:15:40,840
some of the 3D points, which are
guaranteed to be on this surface

378
00:15:40,840 --> 00:15:41,960
of the object.

379
00:15:41,960 --> 00:15:44,640
So sampling is much easier.

380
00:15:44,640 --> 00:15:48,440
So what is hard about these
explicit representations?

381
00:15:48,440 --> 00:15:51,880
The hard thing is it's
very hard, in some sense,

382
00:15:51,880 --> 00:15:56,400
to test whether a point is
inside or outside the objects.

383
00:15:56,400 --> 00:15:59,560
Similarly, if I represent
a sphere as this

384
00:15:59,560 --> 00:16:03,480
function, and it's easy for me
to sample points on the sphere.

385
00:16:03,480 --> 00:16:05,520
But it is hard
for me to say, now

386
00:16:05,520 --> 00:16:07,360
I have a different--
now I have a query.

387
00:16:07,360 --> 00:16:10,520
I say, at this point, 3
over 4, 1 over 2, 1 over 4.

388
00:16:10,520 --> 00:16:13,120
At this point in 3D space,
the is the inside object

389
00:16:13,120 --> 00:16:16,720
or the outside object.

390
00:16:16,720 --> 00:16:18,440
I think we can maybe--

391
00:16:18,440 --> 00:16:21,360
actually, I'm not
even sure about that.

392
00:16:21,360 --> 00:16:24,760
So it is actually hard to test
whether a certain point is

393
00:16:24,760 --> 00:16:27,000
inside or outside object.

394
00:16:27,000 --> 00:16:31,557
So you can see that explicit
representations is--

395
00:16:31,557 --> 00:16:33,890
all these representations,
they have their own strengths

396
00:16:33,890 --> 00:16:34,655
and weaknesses.

397
00:16:34,655 --> 00:16:36,030
And for explicit
representations,

398
00:16:36,030 --> 00:16:38,290
it's actually pretty easy
to sample points, which

399
00:16:38,290 --> 00:16:39,450
are very useful
because sometimes, you

400
00:16:39,450 --> 00:16:41,590
want to convert them into, let's
say, a collection of points.

401
00:16:41,590 --> 00:16:43,310
And then you want to
apply, whatever your point,

402
00:16:43,310 --> 00:16:44,490
neural networks on it.

403
00:16:44,490 --> 00:16:47,410
But it is hard to test
if a certain point is

404
00:16:47,410 --> 00:16:49,850
inside or outside object,
which may have some issues.

405
00:16:49,850 --> 00:16:52,533
For example, let's say if you
want to use a neural rendering

406
00:16:52,533 --> 00:16:54,950
method, and now there's a lot
of neural rendering methods,

407
00:16:54,950 --> 00:16:56,450
requires a lot of
these kind of queries

408
00:16:56,450 --> 00:16:59,070
about whether a point is inside
the object or outside object.

409
00:16:59,070 --> 00:17:01,330
What would be the geometry
or density of object

410
00:17:01,330 --> 00:17:02,430
at that particular point?

411
00:17:02,430 --> 00:17:03,390
What would be the material?

412
00:17:03,390 --> 00:17:05,609
Or what would be the radiance
or color of the object

413
00:17:05,609 --> 00:17:06,930
at that particular point?

414
00:17:06,930 --> 00:17:11,170
So essentially, representations
are not very supportive of--

415
00:17:11,170 --> 00:17:12,849
it's not easy to
run these operations

416
00:17:12,849 --> 00:17:15,740
on these explicit
representations.

417
00:17:15,740 --> 00:17:17,490
So naturally, people
thought, OK, maybe we

418
00:17:17,490 --> 00:17:19,157
can come up with a
different type of way

419
00:17:19,157 --> 00:17:20,329
to represent geometry.

420
00:17:20,329 --> 00:17:23,030
And here I say "implicit"
representations for geometry.

421
00:17:23,030 --> 00:17:25,694
But as you can see later, in a
lot of these neural rendering

422
00:17:25,694 --> 00:17:27,069
methods or deep
learning methods,

423
00:17:27,069 --> 00:17:29,569
they just extend these implicit
representations for not only

424
00:17:29,569 --> 00:17:33,100
geometry, but also for colors
and appearance of objects in 3D.

425
00:17:33,100 --> 00:17:36,180
So the idea of these implicit
representations is now,

426
00:17:36,180 --> 00:17:39,160
I want to classify these points.

427
00:17:39,160 --> 00:17:42,540
So I assume, if the
points are on the objects,

428
00:17:42,540 --> 00:17:44,080
they're on the
surface of objects,

429
00:17:44,080 --> 00:17:46,740
then they satisfy some
certain relationship.

430
00:17:46,740 --> 00:17:48,820
So for example,
for a sphere, what

431
00:17:48,820 --> 00:17:51,980
would be the points on the
sphere, on the unit sphere?

432
00:17:51,980 --> 00:17:54,580
The constraint they
satisfy is the square

433
00:17:54,580 --> 00:17:57,340
of x and the square
of y and square z.

434
00:17:57,340 --> 00:17:59,280
When we sum them
up, they equal to 1.

435
00:17:59,280 --> 00:18:03,540
So this is constraint they
satisfied for all the points

436
00:18:03,540 --> 00:18:05,285
on the sphere.

437
00:18:05,285 --> 00:18:06,660
All right, so more
generally, you

438
00:18:06,660 --> 00:18:09,740
can write it down as the
constraint will be some function

439
00:18:09,740 --> 00:18:12,310
of x and y and z equals 0.

440
00:18:12,310 --> 00:18:14,060
So in this case, f,
x, and y, the function

441
00:18:14,060 --> 00:18:17,680
will be x squared plus y
squared plus z squared minus 1.

442
00:18:17,680 --> 00:18:19,160
So that would be
the function here.

443
00:18:19,160 --> 00:18:20,910
But more generally,
you can think about it

444
00:18:20,910 --> 00:18:23,652
as even for complex shapes,
you can represent-- sometimes,

445
00:18:23,652 --> 00:18:25,860
these functions can be so
complex that you don't even

446
00:18:25,860 --> 00:18:27,320
have a closed form.

447
00:18:27,320 --> 00:18:28,700
So how can I represent an f?

448
00:18:28,700 --> 00:18:30,860
I just write it as
a neural network.

449
00:18:30,860 --> 00:18:33,680
My hope is a neural network
will be able to represent it.

450
00:18:33,680 --> 00:18:35,600
But in general, the
idea is, you have

451
00:18:35,600 --> 00:18:38,480
some function or
some constraints

452
00:18:38,480 --> 00:18:41,360
that points on a certain
object will satisfy.

453
00:18:41,360 --> 00:18:43,460
And this is the way you
will represent an object.

454
00:18:43,460 --> 00:18:45,380
And this is called
implicit representations,

455
00:18:45,380 --> 00:18:47,100
which started with geometry.

456
00:18:47,100 --> 00:18:49,480
But as I said, you're now
using all these different ways

457
00:18:49,480 --> 00:18:51,920
representing textures,
materials, appearance,

458
00:18:51,920 --> 00:18:52,817
and all these things.

459
00:18:52,817 --> 00:18:55,400
So the good thing about implicit
representation-- sorry, let's

460
00:18:55,400 --> 00:18:56,380
start with the bad thing.

461
00:18:56,380 --> 00:18:58,172
The bad thing about
implicit representation

462
00:18:58,172 --> 00:19:00,380
is now it's actually much
harder to sample points.

463
00:19:00,380 --> 00:19:02,540
I tell you, OK, this
is a constraint,

464
00:19:02,540 --> 00:19:04,160
let's say, this torus satisfy.

465
00:19:04,160 --> 00:19:09,160
Right, OK, every x and y and z,
if I put it into this function,

466
00:19:09,160 --> 00:19:11,440
and the output is
0, then, yeah, they

467
00:19:11,440 --> 00:19:13,460
must be on the surface
of this object.

468
00:19:13,460 --> 00:19:17,398
But then how can I get a
couple of these x, y, z tuples?

469
00:19:17,398 --> 00:19:19,440
That would be very hard
because they are required

470
00:19:19,440 --> 00:19:20,540
to solve this function.

471
00:19:20,540 --> 00:19:22,720
And this function is maybe
not too hard to solve.

472
00:19:22,720 --> 00:19:25,330
Maybe you can still solve that
using some high school math.

473
00:19:25,330 --> 00:19:29,650
But when the function gets
really complex for arbitrary

474
00:19:29,650 --> 00:19:32,310
shapes, it becomes much harder
to solve these functions.

475
00:19:32,310 --> 00:19:34,850
So it's not easy to
actually now sample points

476
00:19:34,850 --> 00:19:38,010
on the surface of objects if
you are representing objects

477
00:19:38,010 --> 00:19:39,330
implicitly.

478
00:19:39,330 --> 00:19:40,715
But benefit, the
strength of that

479
00:19:40,715 --> 00:19:42,090
is now, it's
actually pretty easy

480
00:19:42,090 --> 00:19:44,170
to test whether a point
is inside an object,

481
00:19:44,170 --> 00:19:45,290
outside object.

482
00:19:45,290 --> 00:19:48,610
Because if I want to do
testing, I just have a query,

483
00:19:48,610 --> 00:19:50,910
then this is so easy because
it's inside or outside.

484
00:19:50,910 --> 00:19:52,550
I just send it
into that function.

485
00:19:52,550 --> 00:19:55,510
I'll get a value, or the
value is minus 1 over 8.

486
00:19:55,510 --> 00:19:56,810
That's smaller than 0.

487
00:19:56,810 --> 00:20:02,170
So because I assume the object
is represented by this function.

488
00:20:02,170 --> 00:20:03,930
And all the surface
points on object,

489
00:20:03,930 --> 00:20:05,870
they satisfy that
function equals 0.

490
00:20:05,870 --> 00:20:07,870
Anything that's,
[? I don't know, ?] I would say

491
00:20:07,870 --> 00:20:08,950
lower than 0 is negative.

492
00:20:08,950 --> 00:20:11,490
If the output value is
negative, then the point

493
00:20:11,490 --> 00:20:12,910
must be inside the object.

494
00:20:12,910 --> 00:20:14,710
And if the output
value is positive,

495
00:20:14,710 --> 00:20:17,490
then the point must
be outside the object.

496
00:20:17,490 --> 00:20:20,730
So now it becomes much easier to
test whether a certain point is

497
00:20:20,730 --> 00:20:22,850
inside or outside object,
although it becomes much

498
00:20:22,850 --> 00:20:24,740
harder to sample
a number of points

499
00:20:24,740 --> 00:20:26,460
on the surface of an object.

500
00:20:26,460 --> 00:20:28,700
So you can see now, there
is a clear trade-off

501
00:20:28,700 --> 00:20:31,500
between these implicit and
explicit representations.

502
00:20:31,500 --> 00:20:33,500
Here, again, we
talk about geometry.

503
00:20:33,500 --> 00:20:36,020
But this distinction
and the contrast

504
00:20:36,020 --> 00:20:38,620
between explicit and implicit
representations, I think,

505
00:20:38,620 --> 00:20:40,120
is very important
and fundamental.

506
00:20:40,120 --> 00:20:44,060
It is behind development, deep
neural networks when they apply

507
00:20:44,060 --> 00:20:48,060
to 3D data in general,
as we'll see later.

508
00:20:48,060 --> 00:20:50,800
So before we-- let's
[? see, I'm at ?] 25 minutes.

509
00:20:50,800 --> 00:20:52,758
So I promise I spend no
more than five minutes,

510
00:20:52,758 --> 00:20:54,592
and we're going to talk
about deep learning.

511
00:20:54,592 --> 00:20:57,380
So before we talk about how deep
learning can be applied to 3D

512
00:20:57,380 --> 00:21:00,420
representations in general,
a little bit more on implicit

513
00:21:00,420 --> 00:21:02,140
representations is--

514
00:21:02,140 --> 00:21:04,480
some other features of
implicit representations,

515
00:21:04,480 --> 00:21:08,183
the good things about them
is it's easy to compose them.

516
00:21:08,183 --> 00:21:10,100
So sometimes, you feel
like, oh, if I actually

517
00:21:10,100 --> 00:21:14,060
represent everything with a
function, that seems great

518
00:21:14,060 --> 00:21:15,980
if I have a closed
form, but also seem

519
00:21:15,980 --> 00:21:18,800
very constrained because every
closed form, I can write it out.

520
00:21:18,800 --> 00:21:21,400
The geometries look
very, very regular.

521
00:21:21,400 --> 00:21:24,285
So if I want to represent
the shape of a cow,

522
00:21:24,285 --> 00:21:25,410
how would I represent that?

523
00:21:25,410 --> 00:21:27,370
What would be the function I can
write for the shape of a cow?

524
00:21:27,370 --> 00:21:28,430
It's just not obvious.

525
00:21:28,430 --> 00:21:30,430
But the nice thing about
implicit representation

526
00:21:30,430 --> 00:21:32,513
is you don't have to write
everything in one shot,

527
00:21:32,513 --> 00:21:34,190
because it's so easy
to compose them.

528
00:21:34,190 --> 00:21:36,310
You can actually perform
logical operations

529
00:21:36,310 --> 00:21:37,490
on these implicit functions.

530
00:21:37,490 --> 00:21:39,190
Let's say you have
two objects, and you

531
00:21:39,190 --> 00:21:42,070
find unions or intersections
or differences.

532
00:21:42,070 --> 00:21:43,670
Again, they're just values.

533
00:21:43,670 --> 00:21:45,583
So you put x, y, z
onto this function.

534
00:21:45,583 --> 00:21:46,250
You get a value.

535
00:21:46,250 --> 00:21:47,570
You put x, y, z
onto that function.

536
00:21:47,570 --> 00:21:48,290
You get a value.

537
00:21:48,290 --> 00:21:50,645
You can just do
arithmetic operations

538
00:21:50,645 --> 00:21:52,270
on top of these
values, and that allows

539
00:21:52,270 --> 00:21:54,645
you to compute the unions or
intersections of differences

540
00:21:54,645 --> 00:21:56,110
between these objects.

541
00:21:56,110 --> 00:21:59,110
And eventually, you can
compose them to divide

542
00:21:59,110 --> 00:22:00,670
pretty complex shapes.

543
00:22:00,670 --> 00:22:04,055
And this is actually
behind support

544
00:22:04,055 --> 00:22:05,430
a lot of these
industrial designs

545
00:22:05,430 --> 00:22:10,788
when people are designing
complex parts for, I would say,

546
00:22:10,788 --> 00:22:11,330
I don't know.

547
00:22:11,330 --> 00:22:14,550
I mean, when you're
doing manufacturing,

548
00:22:14,550 --> 00:22:17,077
you have to fabricate
some complex shapes.

549
00:22:17,077 --> 00:22:18,910
A lot of these designs
are done by these CAD

550
00:22:18,910 --> 00:22:20,210
models, Computer-Aided Designs.

551
00:22:20,210 --> 00:22:22,970
And they're composing
these implicit functions

552
00:22:22,970 --> 00:22:25,650
using simple logical operations.

553
00:22:25,650 --> 00:22:29,827
And you can also do things
that are beyond just logical.

554
00:22:29,827 --> 00:22:31,410
You can even add
things up, especially

555
00:22:31,410 --> 00:22:34,290
if you have a distance
function, where every point is

556
00:22:34,290 --> 00:22:36,995
like, oh, the positive value
and the negative value--

557
00:22:36,995 --> 00:22:38,370
the values actually
have meanings

558
00:22:38,370 --> 00:22:40,010
because they indicate
how far you are

559
00:22:40,010 --> 00:22:41,590
to the surface of the object.

560
00:22:41,590 --> 00:22:43,650
So you can even add them
up, and this allows you

561
00:22:43,650 --> 00:22:46,756
to smoothly blend the shapes.

562
00:22:46,756 --> 00:22:49,590
So you can see that here, if
I have a distance function,

563
00:22:49,590 --> 00:22:51,810
and here I just want to
represent a vertical line,

564
00:22:51,810 --> 00:22:52,830
OK, this is here.

565
00:22:52,830 --> 00:22:55,710
And then anything that's minus
0 is to the left of the line.

566
00:22:55,710 --> 00:22:58,258
Anything that is positive
is to the right of the line.

567
00:22:58,258 --> 00:23:00,050
And then you have
another line representing

568
00:23:00,050 --> 00:23:01,490
using a different function.

569
00:23:01,490 --> 00:23:03,130
So what happens if
you add them up?

570
00:23:03,130 --> 00:23:05,610
If you add them up,
then it naturally

571
00:23:05,610 --> 00:23:07,750
becomes an interpolation
between these two shapes.

572
00:23:07,750 --> 00:23:10,370
This is an example of
doing things in 1D.

573
00:23:10,370 --> 00:23:13,210
But you can imagine you can even
similarly doing things in 3D

574
00:23:13,210 --> 00:23:15,610
in a sense, OK, now you can
actually even blend these

575
00:23:15,610 --> 00:23:16,770
different shapes.

576
00:23:16,770 --> 00:23:19,620
And these distance functions
can be arbitrarily composed

577
00:23:19,620 --> 00:23:22,060
and allow you to create
actually pretty complex worlds,

578
00:23:22,060 --> 00:23:22,560
like this.

579
00:23:22,560 --> 00:23:25,620
And this is not easy, but
you can even think about it.

580
00:23:25,620 --> 00:23:28,300
You can construct really
complex with all the details

581
00:23:28,300 --> 00:23:31,220
worlds by just simply--

582
00:23:31,220 --> 00:23:34,077
not simply, by
difficulty, but compose

583
00:23:34,077 --> 00:23:35,160
these different functions.

584
00:23:35,160 --> 00:23:37,118
But they are actually
very expressive if you're

585
00:23:37,118 --> 00:23:39,820
very good at it.

586
00:23:39,820 --> 00:23:44,087
OK, so we said, OK, we have
parametric representation that

587
00:23:44,087 --> 00:23:46,420
can be explicit, that directly
give you points on the 3D

588
00:23:46,420 --> 00:23:47,120
surface.

589
00:23:47,120 --> 00:23:48,620
Or we can have parametric
representations,

590
00:23:48,620 --> 00:23:49,540
like these functions.

591
00:23:49,540 --> 00:23:50,660
But they're implicit.

592
00:23:50,660 --> 00:23:52,660
So they're just like,
OK, now you can only

593
00:23:52,660 --> 00:23:55,240
try to verify if the point is
inside or outside an object.

594
00:23:55,240 --> 00:23:56,657
But then you can
also compose them

595
00:23:56,657 --> 00:23:58,260
to build more complex shapes.

596
00:23:58,260 --> 00:24:00,500
And is that possible
for us to also have

597
00:24:00,500 --> 00:24:02,460
implicit representation
and nonparametric,

598
00:24:02,460 --> 00:24:05,207
like that point style, but then
you also querying functions?

599
00:24:05,207 --> 00:24:07,540
Well, sometimes, they actually
do have things like that.

600
00:24:07,540 --> 00:24:10,080
This eventually goes to
methods like level set methods.

601
00:24:10,080 --> 00:24:13,460
So implicit surfaces are
very nice because as we said,

602
00:24:13,460 --> 00:24:14,800
it's easy to merge them.

603
00:24:14,800 --> 00:24:16,100
It's easy to split them.

604
00:24:16,100 --> 00:24:18,690
But sometimes, it's hard
to describe, as we said,

605
00:24:18,690 --> 00:24:20,050
complex shapes in closed forms.

606
00:24:20,050 --> 00:24:21,245
You have a cow.

607
00:24:21,245 --> 00:24:22,370
How would you represent it?

608
00:24:22,370 --> 00:24:23,850
You can compose them.

609
00:24:23,850 --> 00:24:27,390
But if every time I have to
query whether a certain point is

610
00:24:27,390 --> 00:24:30,030
inside a cow, you have to
have hundreds of functions.

611
00:24:30,030 --> 00:24:32,550
And you perform all
these add and or, and/or,

612
00:24:32,550 --> 00:24:33,970
plus/minus operations.

613
00:24:33,970 --> 00:24:35,230
Then it takes a long time.

614
00:24:35,230 --> 00:24:37,410
So what if I just prequery?

615
00:24:37,410 --> 00:24:41,030
So I have a 3D space, and
I just sample, let's say,

616
00:24:41,030 --> 00:24:43,110
a 100 by 100 by 100 grid.

617
00:24:43,110 --> 00:24:45,887
So I have now a
million points sampled.

618
00:24:45,887 --> 00:24:47,470
And for these 1
million points, I just

619
00:24:47,470 --> 00:24:50,190
precompute whether
they're inside the objects

620
00:24:50,190 --> 00:24:52,150
or outside the objects,
what is the distance

621
00:24:52,150 --> 00:24:55,883
of these points to the
surfaces for a complex shapes?

622
00:24:55,883 --> 00:24:57,550
So you can precompute
them, and then you

623
00:24:57,550 --> 00:25:00,090
can store all the
values in a matrix.

624
00:25:00,090 --> 00:25:00,890
This is in 2D.

625
00:25:00,890 --> 00:25:04,070
But this is for visualization.

626
00:25:04,070 --> 00:25:05,890
But in practice, it's in 3D.

627
00:25:05,890 --> 00:25:09,190
So you have a 3D matrix that
store all these precomputed

628
00:25:09,190 --> 00:25:10,950
values of the
distance functions.

629
00:25:10,950 --> 00:25:13,930
So now, in some sense, you still
have an implicit representation.

630
00:25:13,930 --> 00:25:16,260
But because you have
prequeried them,

631
00:25:16,260 --> 00:25:20,280
you turned it into
nonparametric representations.

632
00:25:20,280 --> 00:25:23,740
And even if you just look
at this matrix in 2D,

633
00:25:23,740 --> 00:25:26,373
you can now still find
where the boundaries are.

634
00:25:26,373 --> 00:25:27,540
So where are the boundaries?

635
00:25:27,540 --> 00:25:30,460
They're just basically where
you have two adjacent values.

636
00:25:30,460 --> 00:25:32,480
One is positive,
and one is negative.

637
00:25:32,480 --> 00:25:34,840
So that means there must
be somewhere in between.

638
00:25:34,840 --> 00:25:37,340
That is the point here.

639
00:25:37,340 --> 00:25:39,320
They satisfy the
function f of x equals

640
00:25:39,320 --> 00:25:42,860
0, which means the point
must be on the surface.

641
00:25:42,860 --> 00:25:46,280
So in that sense,
you are turning

642
00:25:46,280 --> 00:25:48,640
on parametric
representation or implicit

643
00:25:48,640 --> 00:25:50,560
into nonparametric
representations

644
00:25:50,560 --> 00:25:54,760
by prequerying a lot of these
points using the functions.

645
00:25:54,760 --> 00:25:57,480
And this allows you to have
actually more explicit controls

646
00:25:57,480 --> 00:26:00,447
because you can
now visualize them.

647
00:26:00,447 --> 00:26:01,780
You can say, I have this matrix.

648
00:26:01,780 --> 00:26:03,900
And I can visualize them
based on their values.

649
00:26:03,900 --> 00:26:08,240
And this is used a lot in
things like CTs and MRIs and all

650
00:26:08,240 --> 00:26:09,920
these medical data.

651
00:26:09,920 --> 00:26:12,052
And a related thing
is people may say, OK,

652
00:26:12,052 --> 00:26:14,260
what if I don't care about
all these distance values?

653
00:26:14,260 --> 00:26:17,203
I can prequery what's going
on at all these points,

654
00:26:17,203 --> 00:26:18,620
but then I compute
all the values.

655
00:26:18,620 --> 00:26:20,560
I say plus 5, minus 5.

656
00:26:20,560 --> 00:26:23,900
But all I care about is whether
this is inside or outside

657
00:26:23,900 --> 00:26:24,578
object.

658
00:26:24,578 --> 00:26:26,620
So if it's positive, I'll
just treat them as one.

659
00:26:26,620 --> 00:26:29,120
If it's negative, which
means they're inside object,

660
00:26:29,120 --> 00:26:30,420
treat them as 0, let's say.

661
00:26:30,420 --> 00:26:32,700
So if you binarize them,
then this gives you

662
00:26:32,700 --> 00:26:34,900
a final representation,
which is arguably

663
00:26:34,900 --> 00:26:39,000
the easiest to understand
is this is called voxels.

664
00:26:39,000 --> 00:26:42,900
So you can prequery where the
implicit functions are, and then

665
00:26:42,900 --> 00:26:45,560
you have all these,
density-sampled grid.

666
00:26:45,560 --> 00:26:47,980
But now, instead of storing
their distance functions,

667
00:26:47,980 --> 00:26:49,940
how far they are
from the surface

668
00:26:49,940 --> 00:26:52,660
by going through the functions
and give you plus 5, minus 5,

669
00:26:52,660 --> 00:26:54,160
you just binarize it.

670
00:26:54,160 --> 00:26:57,580
You only care about whether a
certain point is inside objects

671
00:26:57,580 --> 00:26:59,060
or outside objects.

672
00:26:59,060 --> 00:27:00,880
Then you have a
voxel representation,

673
00:27:00,880 --> 00:27:04,980
which is again like a 3D matrix,
can be 100 by 100 by 100.

674
00:27:04,980 --> 00:27:07,820
But for every point, you have
to go through this function

675
00:27:07,820 --> 00:27:10,320
and query whether it's
inside and outside objects.

676
00:27:10,320 --> 00:27:12,910
You have 1 or 0, and you
can represent objects

677
00:27:12,910 --> 00:27:14,010
in a binarized way.

678
00:27:14,010 --> 00:27:16,510
So this gives you the final
representation I'm going to talk

679
00:27:16,510 --> 00:27:19,670
about for objects in 3D.

680
00:27:19,670 --> 00:27:23,028
So I have introduced voxels
in a kind of complex way.

681
00:27:23,028 --> 00:27:25,070
But from a different
perspective, people may say,

682
00:27:25,070 --> 00:27:27,190
this is actually very easy to
understand because, in some

683
00:27:27,190 --> 00:27:29,530
sense, voxels actually have
a lot of analogy as pixels,

684
00:27:29,530 --> 00:27:31,470
because pixels are 2D matrices.

685
00:27:31,470 --> 00:27:34,630
And now you have 3D matrices,
and voxels is basically just

686
00:27:34,630 --> 00:27:37,002
a 3D matrix.

687
00:27:37,002 --> 00:27:39,630
So although, you
can see that they

688
00:27:39,630 --> 00:27:41,630
have connections with all
the other ways that we

689
00:27:41,630 --> 00:27:43,150
can represent shapes.

690
00:27:43,150 --> 00:27:46,880
And the way I'm introducing
it this way is, actually,

691
00:27:46,880 --> 00:27:49,130
when deep learning comes
in-- so first, deep learning,

692
00:27:49,130 --> 00:27:49,880
when did it start?

693
00:27:49,880 --> 00:27:51,215
2010.

694
00:27:51,215 --> 00:27:53,090
Deep learning has been
there for a long time,

695
00:27:53,090 --> 00:27:56,690
but the modern deep
learning thing is 2010.

696
00:27:56,690 --> 00:27:59,210
Geoff Hinton started doing
that on speech recognition.

697
00:27:59,210 --> 00:28:02,345
And in 2012, they have AlexNet,
which was written on ImageNet.

698
00:28:02,345 --> 00:28:04,470
So you've learned all these,
and they're all in 2D.

699
00:28:04,470 --> 00:28:07,370
OK, now people say, OK,
what if I want to do in 3D?

700
00:28:07,370 --> 00:28:08,750
This is a very natural thought.

701
00:28:08,750 --> 00:28:11,360
So I want to go from 2D
convolution neural networks--

702
00:28:11,360 --> 00:28:14,067
2012, there's no transformer.

703
00:28:14,067 --> 00:28:16,400
So how can I apply a 2D
convolution neural network on 3D

704
00:28:16,400 --> 00:28:17,120
data?

705
00:28:17,120 --> 00:28:19,640
And everyone knows we have
all these different 3D

706
00:28:19,640 --> 00:28:21,280
representations.

707
00:28:21,280 --> 00:28:23,912
But which one do you begin with?

708
00:28:23,912 --> 00:28:26,390
It turns out that people
will say, OK, yeah,

709
00:28:26,390 --> 00:28:28,922
the people who started doing
deep learning on 3D data,

710
00:28:28,922 --> 00:28:30,380
they're the computer
vision people.

711
00:28:30,380 --> 00:28:31,540
They're not the graphics people.

712
00:28:31,540 --> 00:28:33,540
They're like, oh, I've
been working with pixels.

713
00:28:33,540 --> 00:28:36,600
And maybe the easiest thing
I can do is just to scale up.

714
00:28:36,600 --> 00:28:38,260
And instead of working
on 2D matrices,

715
00:28:38,260 --> 00:28:40,300
I just make it work
on 3D matrices.

716
00:28:40,300 --> 00:28:41,540
So that would be the
simplest thing I can do.

717
00:28:41,540 --> 00:28:43,248
Instead of having 2D
convolution network,

718
00:28:43,248 --> 00:28:45,280
I have a volumetric
convolution neural network.

719
00:28:45,280 --> 00:28:47,240
Then, which of these
representations

720
00:28:47,240 --> 00:28:50,023
allow you or support a
volumetric convolution?

721
00:28:50,023 --> 00:28:51,940
It turned out to be this
voxel representation.

722
00:28:51,940 --> 00:28:55,603
This is basically the
easiest you can imagine.

723
00:28:55,603 --> 00:28:57,520
But the graphics people
don't agree with that,

724
00:28:57,520 --> 00:28:58,900
because the graphics
people are like,

725
00:28:58,900 --> 00:29:00,358
oh, this voxel
[? representation ?]

726
00:29:00,358 --> 00:29:02,760
is really bad because
it's very slow to compute.

727
00:29:02,760 --> 00:29:04,920
As we talked about, we have
this [INAUDIBLE] sample

728
00:29:04,920 --> 00:29:05,900
all these values.

729
00:29:05,900 --> 00:29:08,130
And there, you can
look at the quality.

730
00:29:08,130 --> 00:29:10,378
It's so bad compared with
meshes or point clouds.

731
00:29:10,378 --> 00:29:12,670
So people are like, why do
you want to start with that?

732
00:29:12,670 --> 00:29:14,920
But the reason that people
started doing deep learning

733
00:29:14,920 --> 00:29:17,356
on 3D data with voxels,
because I think it's--

734
00:29:17,356 --> 00:29:20,550
it's so easy to draw an analogy
between pixels and voxels,

735
00:29:20,550 --> 00:29:22,470
and you just have to
change one of the code.

736
00:29:22,470 --> 00:29:24,178
That is, instead of
doing 2D convolution,

737
00:29:24,178 --> 00:29:25,730
now do 3D convolution.

738
00:29:25,730 --> 00:29:28,570
So that's, in some sense,
how things get started.

739
00:29:28,570 --> 00:29:32,090
But before I talk about the deep
learning methods for 3D data,

740
00:29:32,090 --> 00:29:34,490
another aspect that's very
important is the data-- sorry,

741
00:29:34,490 --> 00:29:35,370
for 3D data, yeah.

742
00:29:35,370 --> 00:29:38,030
So beyond methods, data sets
are also very important.

743
00:29:38,030 --> 00:29:41,270
ImageNet really prompted
AlexNet and stuff like that.

744
00:29:41,270 --> 00:29:44,650
So for 3D, similarly, we have to
collect a lot of data as well.

745
00:29:44,650 --> 00:29:48,708
So pre-3D, pre-deep learning,
the common data sets or popular

746
00:29:48,708 --> 00:29:51,250
data set people often use is
this thing called Princeton data

747
00:29:51,250 --> 00:29:55,070
Shape Benchmark, which has
1,800 models, 180 categories.

748
00:29:55,070 --> 00:29:57,570
So you can see they actually
have quite a lot of categories,

749
00:29:57,570 --> 00:29:58,910
180 categories.

750
00:29:58,910 --> 00:30:01,770
But there are only 1,800 models,
which means there are basically

751
00:30:01,770 --> 00:30:04,130
10 models per [INAUDIBLE]
year, which is so small.

752
00:30:04,130 --> 00:30:06,522
But back then, it was
considered pretty large.

753
00:30:06,522 --> 00:30:08,230
And people feel like,
oh, this is already

754
00:30:08,230 --> 00:30:10,605
enough to do because we can't
really make anything really

755
00:30:10,605 --> 00:30:12,350
work well on them.

756
00:30:12,350 --> 00:30:15,230
And there was very little
machine learning there.

757
00:30:15,230 --> 00:30:20,430
So prior to 2014, all these data
sets are more or less small.

758
00:30:20,430 --> 00:30:23,630
They may have a certain number
of models, even up to 10,000,

759
00:30:23,630 --> 00:30:25,070
9,000, 10,000.

760
00:30:25,070 --> 00:30:27,810
But they're also divided into
so many different classes.

761
00:30:27,810 --> 00:30:31,250
And so each class, you only have
10 models each or less than 100,

762
00:30:31,250 --> 00:30:32,790
I would say.

763
00:30:32,790 --> 00:30:36,130
So after that, people started by
saying, OK, if we have ImageNet,

764
00:30:36,130 --> 00:30:38,870
can we also have the 3D
data sets for shapes?

765
00:30:38,870 --> 00:30:42,010
So this is behind efforts
of a few concurrent work.

766
00:30:42,010 --> 00:30:43,510
But really, I think
eventually, they

767
00:30:43,510 --> 00:30:44,968
consolidated into
this thing called

768
00:30:44,968 --> 00:30:46,670
ShapeNet, which
is, a lot of them

769
00:30:46,670 --> 00:30:48,030
are actually led by Stanford.

770
00:30:48,030 --> 00:30:52,070
There's Leo Guibas
and Silvio Savarese.

771
00:30:52,070 --> 00:30:55,050
So they led these large
data sets called ShapeNet,

772
00:30:55,050 --> 00:30:57,922
which has 3 million models.

773
00:30:57,922 --> 00:31:00,750
But in practice, just ImageNet,
you have this large image.

774
00:31:00,750 --> 00:31:03,010
And there's a smaller data
set that people often use.

775
00:31:03,010 --> 00:31:05,427
So ShapeNet, similarly, you
have a ShapeNet core data set,

776
00:31:05,427 --> 00:31:09,440
which is what people typically
use as basically 50,000 models

777
00:31:09,440 --> 00:31:10,600
in 55 categories.

778
00:31:10,600 --> 00:31:12,100
Now you can see,
for every category,

779
00:31:12,100 --> 00:31:13,720
you have 1,000
models on average.

780
00:31:13,720 --> 00:31:15,460
But in practice,
it's not a balance.

781
00:31:15,460 --> 00:31:17,780
So for chairs, you have
actually a lot more.

782
00:31:17,780 --> 00:31:19,240
So that's why people
say, oh, now I

783
00:31:19,240 --> 00:31:21,300
have finally I have thousands
of models on chairs.

784
00:31:21,300 --> 00:31:22,760
I can train some
deep networks on it.

785
00:31:22,760 --> 00:31:24,385
Before I just-- you
can have 10 models.

786
00:31:24,385 --> 00:31:25,600
You can't do anything.

787
00:31:25,600 --> 00:31:28,520
So this is how things
started, and so there

788
00:31:28,520 --> 00:31:31,520
has been a few years where a
lot of these advances and all

789
00:31:31,520 --> 00:31:33,620
the results are just
presented on chairs and cars

790
00:31:33,620 --> 00:31:36,115
because these are the largest
categories in ShapeNet

791
00:31:36,115 --> 00:31:37,740
And people feel like,
OK, that's great.

792
00:31:37,740 --> 00:31:38,980
But then that's not enough.

793
00:31:38,980 --> 00:31:41,960
So we should move even bigger.

794
00:31:41,960 --> 00:31:45,140
So in the past few years,
this is work at AI2,

795
00:31:45,140 --> 00:31:47,662
the Allen Institute
from Seattle,

796
00:31:47,662 --> 00:31:50,120
where what they did is they
collected much larger data sets

797
00:31:50,120 --> 00:31:52,880
called Objaverse and Objaverse
Extra Large that you have

798
00:31:52,880 --> 00:31:57,100
roughly 1 million or 10 million
models for different 3D assets.

799
00:31:57,100 --> 00:32:00,240
And you can see they have
much more categories.

800
00:32:00,240 --> 00:32:03,330
These models are on average
also have a higher quality

801
00:32:03,330 --> 00:32:05,730
also with textures.

802
00:32:05,730 --> 00:32:07,190
So these are
synthetic data sets.

803
00:32:07,190 --> 00:32:09,648
But also, there are real data
sets that are being produced,

804
00:32:09,648 --> 00:32:12,130
including some of them
are from 3D scans.

805
00:32:12,130 --> 00:32:15,470
You just take 3D scanners.

806
00:32:15,470 --> 00:32:17,630
Back in 2016, people
have been working on it.

807
00:32:17,630 --> 00:32:19,750
This is a data set
called, I think,

808
00:32:19,750 --> 00:32:21,270
the Redwood data
set or something.

809
00:32:21,270 --> 00:32:24,810
So you have 10,000 scans
of real-world objects.

810
00:32:24,810 --> 00:32:28,890
And more recently, people have
been building larger data sets

811
00:32:28,890 --> 00:32:30,850
where they also
encourage people.

812
00:32:30,850 --> 00:32:35,250
I think this is an effort
co-led by Meta and Oxford.

813
00:32:35,250 --> 00:32:37,598
So they encourage people
to take data for them.

814
00:32:37,598 --> 00:32:39,390
They also pay people
to take data for them.

815
00:32:39,390 --> 00:32:41,630
So people just, using an
iPhone-- you have an object.

816
00:32:41,630 --> 00:32:42,310
You put it on a table.

817
00:32:42,310 --> 00:32:43,060
You use an iPhone.

818
00:32:43,060 --> 00:32:44,890
You take a 360 video
around objects,

819
00:32:44,890 --> 00:32:47,370
and then you get $1 or
something like that.

820
00:32:47,370 --> 00:32:49,987
So they encourage people
to take data for them.

821
00:32:49,987 --> 00:32:51,070
This is the first version.

822
00:32:51,070 --> 00:32:53,050
They have 19,000
videos of objects.

823
00:32:53,050 --> 00:32:55,565
These are real objects
because capturing real objects

824
00:32:55,565 --> 00:32:56,190
is much harder.

825
00:32:56,190 --> 00:32:57,250
And the object version
and all the things

826
00:32:57,250 --> 00:32:59,590
I talked about before,
it was synthetic objects.

827
00:32:59,590 --> 00:33:01,260
But these are real objects.

828
00:33:01,260 --> 00:33:04,780
And then, also because a lot
of the development in 3D vision

829
00:33:04,780 --> 00:33:07,220
algorithms, you can actually
take these 360 videos and try

830
00:33:07,220 --> 00:33:09,680
to reconstruct the 3D objects.

831
00:33:09,680 --> 00:33:13,400
So now you have paired data of
the videos or images of objects,

832
00:33:13,400 --> 00:33:15,510
as well as their 3D
geometries and textures.

834
00:33:27,380 --> 00:33:28,480
This is the first version.

835
00:33:28,480 --> 00:33:31,020
I think they have a more recent
version, V2, or maybe even

836
00:33:31,020 --> 00:33:33,880
V3 right now, which is
supposed to be a little larger.

837
00:33:33,880 --> 00:33:35,535
But still, it is
hard to scale up.

838
00:33:35,535 --> 00:33:36,160
Think about it.

839
00:33:36,160 --> 00:33:39,580
Right now, you have 90,000
videos or basically 90,000

840
00:33:39,580 --> 00:33:40,360
objects.

841
00:33:40,360 --> 00:33:42,200
And I think they're
scaling it up,

842
00:33:42,200 --> 00:33:44,440
but I don't think
it's over 100,000.

843
00:33:44,440 --> 00:33:47,000
So basically, you can think
about it as, for real objects,

844
00:33:47,000 --> 00:33:48,900
you have 100,000 models.

845
00:33:48,900 --> 00:33:52,660
But if you look at, What's the
data set size of the images?

846
00:33:52,660 --> 00:33:54,880
So it's [? in the ?]
lie on 5 B or whatever.

847
00:33:54,880 --> 00:33:56,280
That's like 5 billion images.

848
00:33:56,280 --> 00:33:59,240
And Google and OpenAI must
have much larger data sets.

849
00:33:59,240 --> 00:34:02,160
So there's still a huge gap
between the number of data

850
00:34:02,160 --> 00:34:05,520
points that you can have for 2D
images or videos versus you can

851
00:34:05,520 --> 00:34:06,860
have for 3D objects.

852
00:34:06,860 --> 00:34:08,400
So I think that's
a big challenge,

853
00:34:08,400 --> 00:34:10,500
how we can move
forward with 3D vision.

854
00:34:10,500 --> 00:34:12,360
And people have different ideas.

855
00:34:12,360 --> 00:34:15,060
But still, this is much larger
than what we had before.

856
00:34:15,060 --> 00:34:15,962
At least you can.

857
00:34:15,962 --> 00:34:17,920
It's possible that you
can still, more or less,

858
00:34:17,920 --> 00:34:22,108
train some deep learning
models on these data sets now.

859
00:34:22,108 --> 00:34:24,400
And quickly, there are also
other data sets that people

860
00:34:24,400 --> 00:34:25,760
being built on parts.

861
00:34:25,760 --> 00:34:27,528
This is also from
Stanford, where

862
00:34:27,528 --> 00:34:29,320
they try to annotate
a little bit of object

863
00:34:29,320 --> 00:34:31,760
parts and their correspondence
and hierarchies.

865
00:34:35,320 --> 00:34:38,719
And also, there's
this data set called

866
00:34:38,719 --> 00:34:40,518
PartNet, where they
want to annotate

867
00:34:40,518 --> 00:34:42,060
not only the parts
and the semantics,

868
00:34:42,060 --> 00:34:43,280
but also how they may move.

869
00:34:43,280 --> 00:34:46,100
So a little bit of mobility
information of different parts.

870
00:34:46,100 --> 00:34:48,239
Like, the laptop, you
can open and close it.

871
00:34:48,239 --> 00:34:50,241
And there are also data
sets for 3D scenes,

872
00:34:50,241 --> 00:34:51,699
so not only just
objects and parts.

873
00:34:51,699 --> 00:34:53,600
But there are also the rooms.

874
00:34:53,600 --> 00:34:57,200
So there have been things
like the scan data sets.

875
00:34:57,200 --> 00:34:59,450
These are people who actually
just go inside your home

876
00:34:59,450 --> 00:35:01,990
or go inside your-- actually,
to our office as well.

877
00:35:01,990 --> 00:35:04,630
They just come in, and then
they just have a 3D scanner.

878
00:35:04,630 --> 00:35:07,130
They scan the home, and then
they have some annotations.

879
00:35:07,130 --> 00:35:11,850
So here and more
recently, again, you

880
00:35:11,850 --> 00:35:14,090
can do that even with
your iPhone right now.

881
00:35:14,090 --> 00:35:17,150
But still, these kind of
data sets are much smaller.

882
00:35:17,150 --> 00:35:20,945
So here this one, the first
version scanner you have 1,500.

883
00:35:20,945 --> 00:35:23,070
I think they they have a
++ for the second version,

884
00:35:23,070 --> 00:35:26,710
which is roughly the same size,
maybe 2,000 or 3,000 rooms.

885
00:35:26,710 --> 00:35:29,170
So the amount of data
you have for 3D data,

886
00:35:29,170 --> 00:35:32,250
for 3D scenes in particular,
is also even much smaller than

887
00:35:32,250 --> 00:35:34,703
the amount of data you
have for 3D objects.

888
00:35:34,703 --> 00:35:36,370
So I think it's not
obvious that how can

889
00:35:36,370 --> 00:35:38,453
we go beyond that constraints,
because if you have

890
00:35:38,453 --> 00:35:40,010
to scan it yourself,
you're always

891
00:35:40,010 --> 00:35:41,410
bounded by how
much time you have

892
00:35:41,410 --> 00:35:45,250
and how much people you have.

893
00:35:45,250 --> 00:35:47,850
Anyway, but there
are attempts being

894
00:35:47,850 --> 00:35:52,010
made in trying to collect data.

895
00:35:52,010 --> 00:35:56,220
And finally, if you want to
apply deep learning on to 3D

896
00:35:56,220 --> 00:35:59,085
vision, so what are the
tasks we care about?

897
00:35:59,085 --> 00:36:00,460
So there are
generative modeling,

898
00:36:00,460 --> 00:36:03,000
just like your generative--
just like what Justin said.

899
00:36:03,000 --> 00:36:05,700
You can generate 2D
images or videos.

900
00:36:05,700 --> 00:36:07,080
You can also generate 3D shapes.

901
00:36:07,080 --> 00:36:08,240
You can generate 3D scenes.

902
00:36:08,240 --> 00:36:09,980
You can make them condition.

903
00:36:09,980 --> 00:36:13,560
The condition can be condition
on language, condition on image.

904
00:36:13,560 --> 00:36:14,600
You have an input image.

905
00:36:14,600 --> 00:36:16,940
And how can you
reconstruct the 3D objects?

906
00:36:16,940 --> 00:36:19,000
And you have to learn
the shape priors.

907
00:36:19,000 --> 00:36:21,320
You have to do shape
generation, completion.

908
00:36:21,320 --> 00:36:23,220
Sometimes, you have
a partial object,

909
00:36:23,220 --> 00:36:25,280
and you want to repair it.

910
00:36:25,280 --> 00:36:26,080
You want to fix it.

911
00:36:26,080 --> 00:36:28,580
So there's geometric
data processing as well.

912
00:36:28,580 --> 00:36:31,425
Other tasks, including
discriminative models.

913
00:36:31,425 --> 00:36:32,800
For example, you
have a 3D shape.

914
00:36:32,800 --> 00:36:35,700
How can you classify what's
the category of objects

915
00:36:35,700 --> 00:36:36,480
it belongs to?

916
00:36:36,480 --> 00:36:38,380
Is it a chair or a table?

917
00:36:38,380 --> 00:36:39,940
And a lot of them
now are actually

918
00:36:39,940 --> 00:36:41,820
done by rendering
them into pixels

919
00:36:41,820 --> 00:36:44,500
because you have very
good image recognition

920
00:36:44,500 --> 00:36:46,000
models, like GPT or something.

921
00:36:46,000 --> 00:36:47,502
So you just take a 3D object.

922
00:36:47,502 --> 00:36:48,960
You can render them
into a picture.

923
00:36:48,960 --> 00:36:51,160
You can upload a picture to
GPT, and they can do it for you.

924
00:36:51,160 --> 00:36:52,820
So that's, in some
sense, one way

925
00:36:52,820 --> 00:36:55,670
of solving these
discriminative problems.

926
00:36:55,670 --> 00:36:57,550
But there are also
more specific things

927
00:36:57,550 --> 00:36:59,430
that it's not very
easy to solve.

928
00:36:59,430 --> 00:37:02,070
For example, you have a
different type of cell.

929
00:37:02,070 --> 00:37:03,570
And you have the 3D scans.

930
00:37:03,570 --> 00:37:05,750
And how can you classify
the cell and all these more

931
00:37:05,750 --> 00:37:08,083
specialized domains where you
don't have that much data?

932
00:37:08,083 --> 00:37:11,950
So how can you solve these
discriminative problems?

933
00:37:11,950 --> 00:37:13,965
And joint modeling
of 2D and 3D data,

934
00:37:13,965 --> 00:37:16,590
which is becoming more and more
important because the two data,

935
00:37:16,590 --> 00:37:17,650
we have so much more.

936
00:37:17,650 --> 00:37:19,188
We have so many
images and videos.

937
00:37:19,188 --> 00:37:21,730
We have very good foundation
models that are trained on them.

938
00:37:21,730 --> 00:37:24,670
So how can we leverage the
priors in our 2D foundation

939
00:37:24,670 --> 00:37:25,190
models?

940
00:37:25,190 --> 00:37:26,607
Like, what an image
look like, how

941
00:37:26,607 --> 00:37:28,950
to make an image look
realistic, how to make a video

942
00:37:28,950 --> 00:37:29,730
look realistic.

943
00:37:29,730 --> 00:37:32,590
How can we use that information
to help our 3D reconstruction

944
00:37:32,590 --> 00:37:34,270
to be more realistic?

945
00:37:34,270 --> 00:37:36,490
So joint modeling
in 2D and 3D data--

946
00:37:36,490 --> 00:37:38,950
because there are so many
large-scale 2D data sets

947
00:37:38,950 --> 00:37:40,450
and very good pretrained models.

948
00:37:40,450 --> 00:37:43,377
And also, there has been a lot
of advances in neural rendering

949
00:37:43,377 --> 00:37:45,710
or differential rendering
methods that basically connect

950
00:37:45,710 --> 00:37:46,927
3D world and 2D world.

951
00:37:46,927 --> 00:37:48,010
Because you have 3D world.

952
00:37:48,010 --> 00:37:48,760
You have 3D model.

953
00:37:48,760 --> 00:37:49,970
You can render them into 2D.

954
00:37:49,970 --> 00:37:52,500
The rendering process can
be made differentiable

955
00:37:52,500 --> 00:37:54,460
or can be approximated
with neural networks.

956
00:37:54,460 --> 00:37:56,680
Then, now you can connect
all these data in different

957
00:37:56,680 --> 00:37:58,763
modalities through
differentiable neural networks,

958
00:37:58,763 --> 00:38:02,920
allows you to bridge the priors
you have in 2D data or 2D

959
00:38:02,920 --> 00:38:06,360
foundation models
into the 3D world.

960
00:38:06,360 --> 00:38:09,113
Yeah, and sometimes, you want
to even do some joint multimodal

961
00:38:09,113 --> 00:38:10,780
beyond visual data,
including text data.

962
00:38:10,780 --> 00:38:12,238
But sometimes, you
have other data.

963
00:38:12,238 --> 00:38:14,660
Let's say, in robotics, you
often have tactile data.

964
00:38:14,660 --> 00:38:16,280
So how to fuse them as well.

965
00:38:16,280 --> 00:38:18,360
And sometimes, for
autonomous driving,

966
00:38:18,360 --> 00:38:20,380
maybe you have LiDAR
data or depth data.

967
00:38:20,380 --> 00:38:22,760
How can you fuse them as well?

968
00:38:22,760 --> 00:38:25,480
So I want to use deep learning
on 3D data to solve all these

969
00:38:25,480 --> 00:38:26,878
different problems.

970
00:38:26,878 --> 00:38:29,170
So we spend all the time
talking about representations.

971
00:38:29,170 --> 00:38:30,640
So how do we begin with?

972
00:38:30,640 --> 00:38:33,898
So as I suggested, people
who are initially doing that

973
00:38:33,898 --> 00:38:35,940
is the computer vision
people who work on pixels.

974
00:38:35,940 --> 00:38:36,900
They work on images.

975
00:38:36,900 --> 00:38:39,420
So naturally, they say, why
don't we start with voxels?

976
00:38:39,420 --> 00:38:42,858
But even before that, they
say, OK, this is an old idea.

977
00:38:42,858 --> 00:38:45,400
And this is the very first idea
that people tried in applying

978
00:38:45,400 --> 00:38:46,900
deep learning to 3D vision.

979
00:38:46,900 --> 00:38:48,643
And now, in some sense,
it's coming back.

980
00:38:48,643 --> 00:38:50,060
But the very first
idea they tried

981
00:38:50,060 --> 00:38:52,080
is, now, let's don't
even worry about voxels.

982
00:38:52,080 --> 00:38:53,700
Let's just say you
have a 3D shape.

983
00:38:53,700 --> 00:38:54,320
It's a mesh.

984
00:38:54,320 --> 00:38:55,700
It's voxel, whatever.

985
00:38:55,700 --> 00:38:59,200
And I want you to learn to
recognize what an object is.

986
00:38:59,200 --> 00:39:00,280
What is the object here?

987
00:39:00,280 --> 00:39:01,600
It's a chair.

988
00:39:01,600 --> 00:39:03,120
But what if the
input is 3D data?

989
00:39:03,120 --> 00:39:04,820
How can we process that?

990
00:39:04,820 --> 00:39:07,320
Before we have a 3D
deep learning method,

991
00:39:07,320 --> 00:39:10,140
what if I just render it into
images because I have very good

992
00:39:10,140 --> 00:39:10,790
image models?

994
00:39:13,620 --> 00:39:14,960
I just take the 3D objects.

995
00:39:14,960 --> 00:39:16,480
I would just put camera
at different places.

996
00:39:16,480 --> 00:39:18,230
I can render all these
images, the objects

997
00:39:18,230 --> 00:39:19,540
from different views.

998
00:39:19,540 --> 00:39:22,120
And then, now this
becomes a 2D problem.

999
00:39:22,120 --> 00:39:24,180
I will just apply a
convolutional neural network

1000
00:39:24,180 --> 00:39:27,560
on each of these views, and I
have some ways to fuse them.

1001
00:39:27,560 --> 00:39:29,580
So I have some
pooling, whatever.

1002
00:39:29,580 --> 00:39:32,500
And then I just do
image classification.

1003
00:39:32,500 --> 00:39:34,860
So this becomes an image
classification problem.

1004
00:39:34,860 --> 00:39:37,722
Well, the only difference being,
now you have multiple views.

1005
00:39:37,722 --> 00:39:40,180
So this is like, in some sense,
one of the very first ideas

1006
00:39:40,180 --> 00:39:41,700
people applied to 3D vision.

1007
00:39:41,700 --> 00:39:43,375
They just used 2D networks.

1008
00:39:43,375 --> 00:39:45,000
And why do you want
to use 2D networks?

1009
00:39:45,000 --> 00:39:47,042
Because back then, they
were pushing on ImageNet.

1010
00:39:47,042 --> 00:39:48,150
And they're very good.

1011
00:39:48,150 --> 00:39:51,535
So ImageNet is much
larger than 3D data sets.

1012
00:39:51,535 --> 00:39:53,410
So any model that are
pretrained on ImageNet,

1013
00:39:53,410 --> 00:39:54,743
they have very good performance.

1014
00:39:54,743 --> 00:39:57,190
So the easiest way to solve
this 3D recognition problem is

1015
00:39:57,190 --> 00:39:59,670
to first render it into 2D.

1016
00:39:59,670 --> 00:40:02,790
Later, people move away from
it because people are like, oh,

1017
00:40:02,790 --> 00:40:04,010
we have more 3D data.

1018
00:40:04,010 --> 00:40:05,550
We should try to do 3D native--

1019
00:40:05,550 --> 00:40:07,390
come up with 3D native methods.

1020
00:40:07,390 --> 00:40:10,310
And people also come up with
ideas about connecting 3D and 2D

1021
00:40:10,310 --> 00:40:11,715
through neural rendering.

1022
00:40:11,715 --> 00:40:13,590
But now I feel like this
trend is coming back

1023
00:40:13,590 --> 00:40:15,815
because all these
image and video models

1024
00:40:15,815 --> 00:40:16,690
are getting so great.

1025
00:40:16,690 --> 00:40:18,790
I don't know if many of
you may have seen the VL3

1026
00:40:18,790 --> 00:40:21,070
or whatever, was
released yesterday?

1027
00:40:21,070 --> 00:40:21,570
Right.

1028
00:40:21,570 --> 00:40:24,470
So if they're so great, maybe
we should just rely a bit more

1029
00:40:24,470 --> 00:40:26,770
on the image and video
foundation models, again,

1030
00:40:26,770 --> 00:40:29,853
because they are just trained
on a thousand times or tens

1031
00:40:29,853 --> 00:40:31,770
of thousands of times,
or even more than that,

1032
00:40:31,770 --> 00:40:34,310
maybe a million times
more data than 3D data.

1033
00:40:34,310 --> 00:40:36,090
So how can we incorporate that?

1034
00:40:36,090 --> 00:40:39,430
But coming back, now this
is the very first method.

1035
00:40:39,430 --> 00:40:43,030
In some sense, people try to
apply deep learning on 3D data

1036
00:40:43,030 --> 00:40:45,270
just by converting them into 2D.

1037
00:40:45,270 --> 00:40:48,122
And they do very well
in shape classification.

1038
00:40:48,122 --> 00:40:50,080
They have shapes, and
you want to classify them

1039
00:40:50,080 --> 00:40:51,163
into different categories.

1040
00:40:51,163 --> 00:40:54,280
And they have very
good performance.

1041
00:40:54,280 --> 00:40:58,440
And yeah, so you can leverage a
lot of literatures on 2D image

1042
00:40:58,440 --> 00:41:00,120
pretrained models.

1043
00:41:00,120 --> 00:41:02,680
But the issue is you
need some projections.

1044
00:41:02,680 --> 00:41:05,140
But sometimes, the
input can be very noisy.

1045
00:41:05,140 --> 00:41:07,243
People are like, what if
my input is too noisy?

1046
00:41:07,243 --> 00:41:09,660
The point clouds, or whatever,
they're just not very good.

1047
00:41:09,660 --> 00:41:11,760
If I render them,
they look kind of bad.

1048
00:41:11,760 --> 00:41:15,680
So is it possible for us to come
up with more 3D native methods?

1049
00:41:15,680 --> 00:41:20,000
So later, people tried a number
of 3D native methods that just

1050
00:41:20,000 --> 00:41:22,040
apply deep learning
directly on 3D data.

1051
00:41:22,040 --> 00:41:24,280
As I said, the
easiest way to do this

1052
00:41:24,280 --> 00:41:27,640
is just to apply your pixel
convolution neural network

1053
00:41:27,640 --> 00:41:30,440
into a voxel, volumetric
convolution neural network.

1054
00:41:30,440 --> 00:41:33,360
So this is actually a
Deep Belief Network,

1055
00:41:33,360 --> 00:41:34,880
which is generative network.

1056
00:41:34,880 --> 00:41:38,060
But still, you have some
3D convolutional filters.

1057
00:41:38,060 --> 00:41:41,280
And this is in
2015 by Princeton.

1058
00:41:41,280 --> 00:41:43,640
And you can see that in
their generative model,

1059
00:41:43,640 --> 00:41:48,330
that can actually synthesize 3D
shapes in the form of 3D voxels

1060
00:41:48,330 --> 00:41:50,370
at relatively lower resolution.

1061
00:41:50,370 --> 00:41:52,430
But this is 10 years ago now.

1062
00:41:52,430 --> 00:41:56,090
So back then, this was
considered pretty impressive.

1063
00:41:56,090 --> 00:41:58,470
And you can do all these
condition generations,

1064
00:41:58,470 --> 00:42:02,750
conditional semantic labels
at bats and a desk and tables.

1065
00:42:02,750 --> 00:42:04,582
You can synthesize
these different shapes.

1066
00:42:04,582 --> 00:42:06,290
And because this is
a generative network,

1067
00:42:06,290 --> 00:42:08,730
you can also use it
for classification.

1068
00:42:08,730 --> 00:42:12,890
So you can do image shape
classification as well.

1069
00:42:12,890 --> 00:42:16,890
And later, something
that actually we did

1070
00:42:16,890 --> 00:42:19,330
is what if we just
apply it the GANs,

1071
00:42:19,330 --> 00:42:20,830
this Generative
Adversarial Network?

1072
00:42:20,830 --> 00:42:22,510
You can use GANs to
generate 2D pixels.

1073
00:42:22,510 --> 00:42:25,090
There's no reason you cannot
use GANs to generate 3D voxels.

1074
00:42:25,090 --> 00:42:27,450
So we just did this
very simple thing,

1075
00:42:27,450 --> 00:42:30,690
and that is applied a GAN to
3D voxels and actually give you

1076
00:42:30,690 --> 00:42:33,830
a pretty good generation
of 3D objects.

1077
00:42:33,830 --> 00:42:36,450
This is eight, nine years ago.

1078
00:42:36,450 --> 00:42:39,410
Yeah, OK.

1079
00:42:39,410 --> 00:42:44,490
And later, with training from
CMU, we also did an extension.

1080
00:42:44,490 --> 00:42:48,130
That is, you can use GANs to
not only generate 3D shapes,

1081
00:42:48,130 --> 00:42:50,330
but also you can
render them into 2D.

1082
00:42:50,330 --> 00:42:53,470
You can project them into 2D
surfaces so that you can get

1083
00:42:53,470 --> 00:42:57,130
the depth map of the 3D
objects you generated.

1084
00:42:57,130 --> 00:42:59,950
And then you can use a
cyclegan to convert this depth

1085
00:42:59,950 --> 00:43:02,273
map into a color image.

1086
00:43:02,273 --> 00:43:04,690
Now you can have adversarial
losses not only on 3D shapes,

1087
00:43:04,690 --> 00:43:05,830
but also on 2D pictures.

1088
00:43:05,830 --> 00:43:08,270
You want 3D shapes to look
realistic so that they can be

1089
00:43:08,270 --> 00:43:12,270
indistinguishable from the
3D object data you have.

1090
00:43:12,270 --> 00:43:14,710
You also want the 2D images
to look realistic so that

1091
00:43:14,710 --> 00:43:18,470
they're indistinguishable
from images of real cars.

1092
00:43:18,470 --> 00:43:20,270
So then you can
do 3D generation,

1093
00:43:20,270 --> 00:43:22,630
as well as 2D generation
at the same time.

1094
00:43:22,630 --> 00:43:27,210
And because you have different
latent vectors for the shapes,

1095
00:43:27,210 --> 00:43:29,330
for the viewpoints,
and for the textures,

1096
00:43:29,330 --> 00:43:31,352
you also have some level
of controllability.

1097
00:43:31,352 --> 00:43:32,810
Like, you can change
the viewpoint.

1098
00:43:32,810 --> 00:43:34,310
You can change the textures.

1099
00:43:34,310 --> 00:43:36,270
You can do interpolation.

1100
00:43:36,270 --> 00:43:39,750
And you can transfer
the texture of one car

1101
00:43:39,750 --> 00:43:42,260
onto the shape of another car.

1102
00:43:42,260 --> 00:43:45,200
This is 2018.

1103
00:43:45,200 --> 00:43:47,480
So people tried
applying deep networks,

1104
00:43:47,480 --> 00:43:48,980
like convolutional
neural networks,

1105
00:43:48,980 --> 00:43:51,920
generative adversarial networks,
on 3D voxels instead of 2D

1106
00:43:51,920 --> 00:43:52,863
pixels.

1107
00:43:52,863 --> 00:43:54,780
And can we do a little
bit better with voxels?

1108
00:43:54,780 --> 00:43:57,200
Because one thing that people
have complained about voxels

1109
00:43:57,200 --> 00:43:59,240
is they're just really slow.

1110
00:43:59,240 --> 00:44:00,420
You have to presample them.

1111
00:44:00,420 --> 00:44:01,920
And there are a lot
of wasted effort

1112
00:44:01,920 --> 00:44:04,880
because a lot of sample points
are just like empty space.

1113
00:44:04,880 --> 00:44:08,125
Or there are inside objects,
and it gives you no information.

1114
00:44:08,125 --> 00:44:09,500
So naturally,
people thought, OK,

1115
00:44:09,500 --> 00:44:10,792
can we actually make it better?

1116
00:44:10,792 --> 00:44:14,120
So there are improvements to
voxels, like octave trees.

1117
00:44:14,120 --> 00:44:16,160
The idea of octave
trees is you still

1118
00:44:16,160 --> 00:44:18,093
have explicit representations.

1119
00:44:18,093 --> 00:44:19,760
Sorry, in some sense,
you can argue it's

1120
00:44:19,760 --> 00:44:20,885
an implicit representation.

1121
00:44:20,885 --> 00:44:24,400
But it's like nonparametric
implicit representations.

1122
00:44:24,400 --> 00:44:26,520
But then, instead
of representing

1123
00:44:26,520 --> 00:44:31,848
every point in space at a
uniform scale, you actually--

1124
00:44:31,848 --> 00:44:34,780
assuming the voxels or--

1125
00:44:34,780 --> 00:44:37,840
yeah, basically, the voxels
can be of different sizes,

1126
00:44:37,840 --> 00:44:40,230
I just divide the divide
space into different regions.

1127
00:44:40,230 --> 00:44:41,490
And I spend a lot more.

1128
00:44:41,490 --> 00:44:43,810
And when I feel like I'm
really close to the surface

1129
00:44:43,810 --> 00:44:47,110
of the objects, I just represent
objects in a much finer scale.

1130
00:44:47,110 --> 00:44:49,890
And when I'm in this empty
space or inside objects,

1131
00:44:49,890 --> 00:44:52,690
where I really don't care too
much about what's going on,

1132
00:44:52,690 --> 00:44:54,990
I can have huge
voxels in some sense.

1133
00:44:54,990 --> 00:44:57,690
So you can recursively
partition the space,

1134
00:44:57,690 --> 00:45:02,070
and you can have different sizes
of voxels at different space.

1135
00:45:02,070 --> 00:45:04,730
And this allows you
to really scale up.

1136
00:45:04,730 --> 00:45:07,870
So you can see that compared
with just directly using voxels,

1137
00:45:07,870 --> 00:45:10,130
this is 2019-ish.

1138
00:45:10,130 --> 00:45:12,890
People say, OK, octave trees
are great because it allows

1139
00:45:12,890 --> 00:45:14,950
me to go from lower resolution.

1140
00:45:14,950 --> 00:45:17,770
That's how much you can
fit into GPU memory.

1141
00:45:17,770 --> 00:45:20,150
You can do 64 by 64 with voxels.

1142
00:45:20,150 --> 00:45:23,956
But with octave
trees, you can do 256.

1143
00:45:23,956 --> 00:45:26,230
And you can even use that
for generation as well.

1144
00:45:26,230 --> 00:45:28,530
You can generate objects also.

1145
00:45:28,530 --> 00:45:31,330
They look like voxels, but
they're higher resolution

1146
00:45:31,330 --> 00:45:34,490
because you're more efficient
in representing the space.

1147
00:45:34,490 --> 00:45:37,260
So these are the very early
attempts in applying deep

1148
00:45:37,260 --> 00:45:38,620
learning to 3D space.

1149
00:45:38,620 --> 00:45:41,406
And you're like, OK, why
don't we just try voxels?

1150
00:45:41,406 --> 00:45:44,300
Then this is the
moment where people

1151
00:45:44,300 --> 00:45:46,640
have got a bit more
interest into like, oh, no,

1152
00:45:46,640 --> 00:45:50,740
now what if you the graphics
people feel like-- you're just

1153
00:45:50,740 --> 00:45:51,700
doing all this wrong.

1154
00:45:51,700 --> 00:45:54,460
Because why do you want
to use this kind of pretty

1155
00:45:54,460 --> 00:45:56,540
inefficient, ugly-looking
representations,

1156
00:45:56,540 --> 00:45:58,067
like voxels or octave trees?

1157
00:45:58,067 --> 00:45:59,900
Now we have all these
good representations--

1158
00:45:59,900 --> 00:46:01,820
point clouds, meshes, splines.

1159
00:46:01,820 --> 00:46:03,880
Why are we not using
these representations?

1160
00:46:03,880 --> 00:46:06,652
But as we said, the challenge
is, points are here and there.

1161
00:46:06,652 --> 00:46:08,860
How can you even apply
convolutional points and stuff

1162
00:46:08,860 --> 00:46:09,360
like that?

1163
00:46:09,360 --> 00:46:10,540
It's just not very obvious.

1164
00:46:10,540 --> 00:46:12,420
But people start
to look into it.

1165
00:46:12,420 --> 00:46:15,860
So naturally, people move into
applying or developing new deep

1166
00:46:15,860 --> 00:46:19,302
learning methods that directly
work on not only just 3D data,

1167
00:46:19,302 --> 00:46:21,260
but also different types
of 3D representations,

1168
00:46:21,260 --> 00:46:23,060
like point clouds.

1169
00:46:23,060 --> 00:46:26,380
So in PointNet, I think this
is an important work also

1170
00:46:26,380 --> 00:46:29,620
from Stanford, from Leo's team.

1171
00:46:29,620 --> 00:46:32,300
What's going on here is they
develop a new type of deep

1172
00:46:32,300 --> 00:46:34,660
network that directly
works with 3D point clouds.

1173
00:46:34,660 --> 00:46:37,520
So it's called PointNet.

1174
00:46:37,520 --> 00:46:42,440
So the idea is, for points, you
have to be permutation invariant

1175
00:46:42,440 --> 00:46:45,840
because if I have point 1 and
point 2 OK, point 1 is here,

1176
00:46:45,840 --> 00:46:47,170
and point 2 is here.

1177
00:46:47,170 --> 00:46:49,300
Now I have a different input.

1178
00:46:49,300 --> 00:46:52,000
I would say point 1
here and point 2 there.

1179
00:46:52,000 --> 00:46:55,080
So then whatever--
your network should

1180
00:46:55,080 --> 00:46:58,880
be invariant to these two types
of input, which means no matter

1181
00:46:58,880 --> 00:47:02,060
how I name, this one is
point 1, that one is point 2,

1182
00:47:02,060 --> 00:47:03,920
or I name this one's
point 1, that one

1183
00:47:03,920 --> 00:47:05,900
is point 2, your output
should be the same,

1184
00:47:05,900 --> 00:47:08,488
because the points
there are unordered.

1185
00:47:08,488 --> 00:47:10,280
There's no guaranteed
ordering in the sense

1186
00:47:10,280 --> 00:47:11,680
that our top left is 1, 1.

1187
00:47:11,680 --> 00:47:14,660
Bottom right is 100, 100.

1188
00:47:14,660 --> 00:47:17,500
So if there's an order, if
the points are unordered,

1189
00:47:17,500 --> 00:47:19,880
you have to be
permutation invariant.

1190
00:47:19,880 --> 00:47:22,560
So how can we do that?

1191
00:47:22,560 --> 00:47:25,400
And second is you have to
be also sampling invariant.

1192
00:47:25,400 --> 00:47:28,200
So sometimes, you
sample, say, 10 points

1193
00:47:28,200 --> 00:47:31,320
on the head of the
bunny or rabbit

1194
00:47:31,320 --> 00:47:33,380
and 5 points on the
tail of the rabbit.

1195
00:47:33,380 --> 00:47:34,970
And sometimes, you
sample 10 points

1196
00:47:34,970 --> 00:47:36,690
on the tail of the
rabbit, only 5 points

1197
00:47:36,690 --> 00:47:38,082
on the head of the rabbit.

1198
00:47:38,082 --> 00:47:39,790
So how can you also
be invariant to that?

1199
00:47:39,790 --> 00:47:42,710
Because there's no guarantee
on how your sample points.

1200
00:47:42,710 --> 00:47:45,730
So there are a bit of
a issue here and there,

1201
00:47:45,730 --> 00:47:50,250
but the one idea they
imply, they used, and I

1202
00:47:50,250 --> 00:47:52,970
think it's basically probably
the most important point,

1203
00:47:52,970 --> 00:47:53,830
is they just--

1204
00:47:53,830 --> 00:47:55,910
I just apply-- it's
also so simple.

1205
00:47:55,910 --> 00:47:57,450
I just apply a
symmetric function

1206
00:47:57,450 --> 00:47:59,190
on the embeddings of the points.

1207
00:47:59,190 --> 00:48:01,930
So basically, for
all the points,

1208
00:48:01,930 --> 00:48:04,270
I first compute some
embeddings for them,

1209
00:48:04,270 --> 00:48:07,010
just like you compute
embeddings for different regions

1210
00:48:07,010 --> 00:48:08,810
or different windows of image.

1211
00:48:08,810 --> 00:48:11,370
And I compute the
features for each point.

1212
00:48:11,370 --> 00:48:13,310
And then I just
have to fuse them.

1213
00:48:13,310 --> 00:48:16,150
But because I want them to
be permutation invariant,

1214
00:48:16,150 --> 00:48:18,890
so I just use a symmetric
function in the sense

1215
00:48:18,890 --> 00:48:21,070
that, for example, it can
be just a max function.

1216
00:48:21,070 --> 00:48:23,410
And I take the maximum, softmax.

1217
00:48:23,410 --> 00:48:24,670
It can also be a sum function.

1218
00:48:24,670 --> 00:48:26,850
I just add them up.

1219
00:48:26,850 --> 00:48:27,990
So that's what's going on.

1220
00:48:27,990 --> 00:48:28,790
This is so simple.

1221
00:48:28,790 --> 00:48:31,583
You have a number of points,
1, 2, 3 or 1, whatever.

1222
00:48:31,583 --> 00:48:33,500
And then you have compute
embeddings for them,

1223
00:48:33,500 --> 00:48:34,640
and they just aggregate them.

1224
00:48:34,640 --> 00:48:36,440
You can compute the
max for each dimension.

1225
00:48:36,440 --> 00:48:38,340
You can sum them up
or stuff like that.

1226
00:48:38,340 --> 00:48:39,560
And yeah.

1227
00:48:39,560 --> 00:48:42,870
And you have this aggregated
embeddings for all the points.

1228
00:48:42,870 --> 00:48:44,620
And then you go through
maybe a few layers

1229
00:48:44,620 --> 00:48:46,760
of fully connected networks
and stuff like that.

1230
00:48:46,760 --> 00:48:50,380
And then you use it
to classify all--

1231
00:48:50,380 --> 00:48:55,380
are these points are really
representing a chair or a table?

1232
00:48:55,380 --> 00:48:57,440
So that's basically
what's going on,

1233
00:48:57,440 --> 00:49:00,200
and it turned out to
be quite powerful.

1234
00:49:00,200 --> 00:49:03,735
And of course, there have
been a lot of the improvements

1235
00:49:03,735 --> 00:49:04,360
on top of that.

1236
00:49:04,360 --> 00:49:06,360
People have been coming
up with new methods that

1237
00:49:06,360 --> 00:49:07,756
improve on PointNet.

1238
00:49:07,756 --> 00:49:09,335
They have PointNet++.

1239
00:49:09,335 --> 00:49:11,460
And there are things people
have been trying to do,

1240
00:49:11,460 --> 00:49:14,940
is graph neural networks because
you can easily translate points

1241
00:49:14,940 --> 00:49:19,440
into nodes of a graph, and then
the neighborhood, the proximity,

1242
00:49:19,440 --> 00:49:22,100
whether points are close
to each other as the edges

1243
00:49:22,100 --> 00:49:23,320
connecting these points.

1244
00:49:23,320 --> 00:49:24,820
So there have been like
graph neural networks

1245
00:49:24,820 --> 00:49:26,362
and all these other
methods that have

1246
00:49:26,362 --> 00:49:30,150
been developed for these
point cloud processing.

1247
00:49:30,150 --> 00:49:34,410
But the original idea in the
PointNet paper is so simple,

1248
00:49:34,410 --> 00:49:36,990
and it turned out to
be also very powerful.

1249
00:49:36,990 --> 00:49:38,510
Something else you
want to consider

1250
00:49:38,510 --> 00:49:40,350
is you also want to measure.

1251
00:49:40,350 --> 00:49:41,410
For pixels, it's easy.

1252
00:49:41,410 --> 00:49:42,650
I have an output image.

1253
00:49:42,650 --> 00:49:44,170
I have the ground truth image.

1254
00:49:44,170 --> 00:49:46,128
I just compute the
differences between the two.

1255
00:49:46,128 --> 00:49:48,070
I have two loss or whatever.

1256
00:49:48,070 --> 00:49:50,950
For points, how would you
compare the output point cloud

1257
00:49:50,950 --> 00:49:52,910
and the input point
cloud, especially if you

1258
00:49:52,910 --> 00:49:54,035
care about generation task?

1259
00:49:54,035 --> 00:49:55,702
If you do a classification,
that's fine.

1260
00:49:55,702 --> 00:49:57,350
You have input point
cloud, and output

1261
00:49:57,350 --> 00:49:59,350
is chair, table, whatever.

1262
00:49:59,350 --> 00:50:00,850
You have a cross-entropy loss.

1263
00:50:00,850 --> 00:50:02,110
That's all you need.

1264
00:50:02,110 --> 00:50:04,670
But if you output, if you're
doing a generation task,

1265
00:50:04,670 --> 00:50:06,270
and you have a voxels,
it's also easy.

1266
00:50:06,270 --> 00:50:08,510
Just do a cross-entropy
loss of the 100

1267
00:50:08,510 --> 00:50:10,630
by 100 by 100 voxel grid.

1268
00:50:10,630 --> 00:50:13,230
But if your output is a point
and 100 points, and how would

1269
00:50:13,230 --> 00:50:16,150
you compare the output point
cloud versus the ground truth

1270
00:50:16,150 --> 00:50:16,810
point cloud?

1271
00:50:16,810 --> 00:50:19,270
You have to also design
distance metrics.

1272
00:50:19,270 --> 00:50:22,992
So the two common distance
metrics that people did use--

1273
00:50:22,992 --> 00:50:24,450
one is called the
Chamfer distance.

1274
00:50:24,450 --> 00:50:26,210
A Chamfer distance is
easy to understand.

1275
00:50:26,210 --> 00:50:27,790
That is you have
two sets of points.

1276
00:50:27,790 --> 00:50:31,370
And for each point on
each side or each set,

1277
00:50:31,370 --> 00:50:33,485
you just basically find
the nearest neighbor.

1278
00:50:33,485 --> 00:50:35,110
So you have a collection
of red points.

1279
00:50:35,110 --> 00:50:36,630
You have a collection
of blue points.

1280
00:50:36,630 --> 00:50:38,150
And for the red point,
for each of red points,

1281
00:50:38,150 --> 00:50:40,590
you just find its nearest
neighbor in the blue set.

1282
00:50:40,590 --> 00:50:41,930
And for each of the
blue points, you just

1283
00:50:41,930 --> 00:50:43,130
find the nearest
neighbor in the red set.

1284
00:50:43,130 --> 00:50:44,850
And you want to
minimize the distance,

1285
00:50:44,850 --> 00:50:46,850
minimize the distance
of each point

1286
00:50:46,850 --> 00:50:49,170
to its nearest neighbor
in the other set.

1287
00:50:49,170 --> 00:50:51,770
And the second idea, loss
function that people may use

1288
00:50:51,770 --> 00:50:53,270
is called Earth Mover distance.

1289
00:50:53,270 --> 00:50:55,770
And here you do a bipartite
matching between the two

1290
00:50:55,770 --> 00:50:58,107
sets of points, and you
have a one-to-one paired

1291
00:50:58,107 --> 00:50:59,690
matching between
these points, and you

1292
00:50:59,690 --> 00:51:02,672
want to minimize the distance
among all these pairs.

1293
00:51:02,672 --> 00:51:04,130
So these are the
two common metrics

1294
00:51:04,130 --> 00:51:05,930
that people use when they're
comparing the distance

1295
00:51:05,930 --> 00:51:07,170
between the point clouds.

1296
00:51:07,170 --> 00:51:08,470
And they can be
made differentiable,

1297
00:51:08,470 --> 00:51:10,345
which means you can now
compute the gradients

1298
00:51:10,345 --> 00:51:12,530
and use it to optimize
your neural network so

1299
00:51:12,530 --> 00:51:14,530
that hopefully, the
output better point

1300
00:51:14,530 --> 00:51:19,330
clouds if you're caring about a
point cloud generation problem.

1301
00:51:19,330 --> 00:51:23,050
So we have moved from
voxels to point clouds.

1302
00:51:23,050 --> 00:51:24,950
And then people were
like, OK, this is great.

1303
00:51:24,950 --> 00:51:28,080
And now I can process the
points, I can output points.

1304
00:51:28,080 --> 00:51:31,900
But we also have other beautiful
partitions like splines.

1305
00:51:31,900 --> 00:51:34,320
They're very good at capturing
the surfaces of objects.

1306
00:51:34,320 --> 00:51:36,340
If you use any kind
of neural network

1307
00:51:36,340 --> 00:51:38,200
to generate voxels or
generate point clouds,

1308
00:51:38,200 --> 00:51:39,880
they always look very ugly.

1309
00:51:39,880 --> 00:51:42,400
So they don't have smooth
surfaces and stuff like that.

1310
00:51:42,400 --> 00:51:43,858
So how can we have
a neural network

1311
00:51:43,858 --> 00:51:46,900
that can output or understand
objects, but also represent

1312
00:51:46,900 --> 00:51:48,940
the beautiful surfaces?

1313
00:51:48,940 --> 00:51:51,772
So people go a bit
forward in thinking

1314
00:51:51,772 --> 00:51:53,980
about how I can integrate
neural networks with things

1315
00:51:53,980 --> 00:51:56,340
like splines or
functions like that.

1316
00:51:56,340 --> 00:51:58,220
And a notable example--

1317
00:51:58,220 --> 00:52:00,400
here is this thing
called AtlasNet.

1318
00:52:00,400 --> 00:52:04,060
So what's going on here is
they try to use deep learning.

1319
00:52:04,060 --> 00:52:06,940
But then, instead of directly
outputting a set of 3D point

1320
00:52:06,940 --> 00:52:10,940
clouds, so I learned a
transformation function.

1321
00:52:10,940 --> 00:52:13,420
I have latent shape
representations.

1322
00:52:13,420 --> 00:52:16,940
And then, if you remember, when
we say you have these parametric

1323
00:52:16,940 --> 00:52:19,680
representation of object shapes,
you're basically transforming,

1324
00:52:19,680 --> 00:52:25,030
let's say, a 2D space of u and v
into a 3D space, like a sphere.

1325
00:52:25,030 --> 00:52:27,353
And for simple things
like sphere, it's easy.

1326
00:52:27,353 --> 00:52:28,270
You can write it down.

1327
00:52:28,270 --> 00:52:32,088
What is that function in
sine and cosine or whatever?

1328
00:52:32,088 --> 00:52:34,630
But for complex objects, it is
very hard to write a function.

1329
00:52:34,630 --> 00:52:36,470
And often, there
is no closed form.

1330
00:52:36,470 --> 00:52:39,158
So the idea here is, OK,
if there is no closed form,

1331
00:52:39,158 --> 00:52:40,950
then why don't we just
use a neural network

1332
00:52:40,950 --> 00:52:42,210
to represent that?

1333
00:52:42,210 --> 00:52:44,390
So here you can see
this neural network,

1334
00:52:44,390 --> 00:52:49,210
which is implemented as MLP,
just learns that function f.

1335
00:52:49,210 --> 00:52:51,750
You can take the two values,
u and v, as the input

1336
00:52:51,750 --> 00:52:53,110
to the function f.

1337
00:52:53,110 --> 00:52:55,510
And the neural networks is
performing the computation

1338
00:52:55,510 --> 00:52:59,470
of the function f and u and v
and output a point in 3D space.

1339
00:52:59,470 --> 00:53:04,070
So it's basically learning how
we are able to transform this 2D

1340
00:53:04,070 --> 00:53:09,070
space into the 3D space, and it
might be too hard to represent

1341
00:53:09,070 --> 00:53:11,920
the entire object using
a single transformation.

1342
00:53:11,920 --> 00:53:13,670
So people thought, OK,
we can use a couple

1343
00:53:13,670 --> 00:53:14,885
of small neural networks.

1344
00:53:14,885 --> 00:53:17,010
So think about it as now
you have a piece of paper.

1345
00:53:17,010 --> 00:53:18,450
You can fold it
in different ways.

1346
00:53:18,450 --> 00:53:20,010
You can fold it multiple times.

1347
00:53:20,010 --> 00:53:21,750
And all these things
get put together

1348
00:53:21,750 --> 00:53:25,920
to form the final
shapes you care about.

1349
00:53:25,920 --> 00:53:29,007
So this is, you can see, the
differences between these two,

1350
00:53:29,007 --> 00:53:30,340
three different representations.

1351
00:53:30,340 --> 00:53:31,480
You have an input image.

1352
00:53:31,480 --> 00:53:34,440
And if you want to represent
it reconstructed using voxels,

1353
00:53:34,440 --> 00:53:36,680
you can see it's
doing something.

1354
00:53:36,680 --> 00:53:40,400
But you're really bounded by
limited resolution voxels.

1355
00:53:40,400 --> 00:53:42,440
And for point clouds,
yeah, you're no longer

1356
00:53:42,440 --> 00:53:43,860
bounded by the resolutions.

1357
00:53:43,860 --> 00:53:46,260
And it gives you maybe
a bit more details.

1358
00:53:46,260 --> 00:53:48,020
But the points are
really unordered.

1359
00:53:48,020 --> 00:53:50,720
You cannot really get any
smooth surfaces out of the point

1360
00:53:50,720 --> 00:53:51,560
clouds.

1361
00:53:51,560 --> 00:53:54,063
And for this thing called
AtlasNet, which is basically

1362
00:53:54,063 --> 00:53:55,480
learning transform
pieces, you can

1363
00:53:55,480 --> 00:53:58,160
see that they have
actually smoother surfaces.

1364
00:53:58,160 --> 00:54:00,160
So using neural
network to represent

1365
00:54:00,160 --> 00:54:02,240
how you can map a
parametric representation

1366
00:54:02,240 --> 00:54:05,320
from a low-dimensional space
to a higher-dimensional space,

1367
00:54:05,320 --> 00:54:07,340
and you learn multiple
of these mappings.

1368
00:54:07,340 --> 00:54:10,080
And when they are combined,
they give you the final output

1369
00:54:10,080 --> 00:54:11,610
geometries conditioned
on 2D images.

1371
00:54:16,200 --> 00:54:30,020
OK, so finally, in some
sense, we can put it this way.

1372
00:54:30,020 --> 00:54:32,660
So what is deep network
doing when they're

1373
00:54:32,660 --> 00:54:34,380
doing ImageNet classification?

1374
00:54:34,380 --> 00:54:36,980
They're basically learning
very complex functions

1375
00:54:36,980 --> 00:54:40,220
that map input images
in the form of pixels

1376
00:54:40,220 --> 00:54:42,040
into a final category label.

1377
00:54:42,040 --> 00:54:45,060
Is it a cat or a dog
or person or whatever?

1378
00:54:45,060 --> 00:54:47,500
That function is really
complex, and output space

1379
00:54:47,500 --> 00:54:48,240
is really small.

1380
00:54:48,240 --> 00:54:50,620
Output space is
1,000 dimensions.

1381
00:54:50,620 --> 00:54:52,200
So it's like, OK,
it's a cat or dog?

1382
00:54:52,200 --> 00:54:53,617
You have 1,000 way
classification.

1383
00:54:53,617 --> 00:54:54,680
Output space is so small.

1384
00:54:54,680 --> 00:54:57,260
Input space is much larger
because you have 500

1385
00:54:57,260 --> 00:54:58,480
by 500 pixels.

1386
00:54:58,480 --> 00:55:00,935
So that's 250,000 or something.

1387
00:55:00,935 --> 00:55:02,060
Input space is much larger.

1388
00:55:02,060 --> 00:55:03,268
Output space is really small.

1389
00:55:03,268 --> 00:55:07,340
The function is
really hard to write.

1390
00:55:07,340 --> 00:55:09,500
Is it possible for me to
write down some formulas

1391
00:55:09,500 --> 00:55:13,540
so that I can classify the
input image by computing

1392
00:55:13,540 --> 00:55:15,640
whatever, some specific
values and output

1393
00:55:15,640 --> 00:55:16,720
if this is a cat or dog?

1394
00:55:16,720 --> 00:55:17,545
I cannot do that.

1395
00:55:17,545 --> 00:55:18,920
The function is
so hard to write.

1396
00:55:18,920 --> 00:55:19,810
There's no closed form.

1397
00:55:19,810 --> 00:55:20,970
That's why I need
a deep network.

1398
00:55:20,970 --> 00:55:22,010
Input space is large.

1399
00:55:22,010 --> 00:55:23,630
Output space is small.

1400
00:55:23,630 --> 00:55:26,190
So if you really think about
deep networks that way and think

1401
00:55:26,190 --> 00:55:29,070
about what they are doing, then
you realize a lot of the things

1402
00:55:29,070 --> 00:55:31,770
that we have been doing with
deep networks on 3D shapes,

1403
00:55:31,770 --> 00:55:36,670
it doesn't seem to map that
mapping to that, I would say,

1404
00:55:36,670 --> 00:55:37,570
equation.

1405
00:55:37,570 --> 00:55:38,997
So it doesn't map that well.

1406
00:55:38,997 --> 00:55:40,830
And what are the
representations that really

1407
00:55:40,830 --> 00:55:42,530
map them-- map it the best?

1408
00:55:42,530 --> 00:55:44,270
What's the optimal
representations

1409
00:55:44,270 --> 00:55:47,390
that seems to really
fit into the paradigm?

1410
00:55:47,390 --> 00:55:50,650
And if we think more carefully,
I think, around 2019,

1411
00:55:50,650 --> 00:55:52,788
then people realize,
oh, yeah, in some sense,

1412
00:55:52,788 --> 00:55:54,330
deep network is an
implicit function.

1413
00:55:54,330 --> 00:55:57,510
And why don't we just use it
to represent implicit function

1414
00:55:57,510 --> 00:55:59,470
for object 3D geometry?

1415
00:55:59,470 --> 00:56:02,110
So instead of
representing the voxels

1416
00:56:02,110 --> 00:56:04,030
because now you're just doing--

1417
00:56:04,030 --> 00:56:07,870
you turn it into pixels, but
then you just scale up into 3D.

1418
00:56:07,870 --> 00:56:09,410
And you apply 3D convolution.

1419
00:56:09,410 --> 00:56:12,310
But fundamentally,
voxel is really about

1420
00:56:12,310 --> 00:56:14,570
whether the thing is inside
and outside the object.

1421
00:56:14,570 --> 00:56:18,240
So instead of just directly
prequerying the space and get

1422
00:56:18,240 --> 00:56:20,383
the voxels and apply a
convolution on top of it,

1423
00:56:20,383 --> 00:56:22,800
what if I just directly using
deep network to perform that

1424
00:56:22,800 --> 00:56:25,840
query for me so that I don't
have to run 3D convolution

1425
00:56:25,840 --> 00:56:26,400
or anything?

1426
00:56:26,400 --> 00:56:29,840
I just query a space in 3D,
and deep network should tell me

1427
00:56:29,840 --> 00:56:34,280
if the output can just be one
dimensional inside and outside,

1428
00:56:34,280 --> 00:56:38,960
whether that point is inside a
3D shape or outside a 3D shape.

1429
00:56:38,960 --> 00:56:41,240
So finally, I think
people move that leap,

1430
00:56:41,240 --> 00:56:44,640
take the leap to go from
specific representations

1431
00:56:44,640 --> 00:56:47,660
on point clouds or splines
into implicit representations,

1432
00:56:47,660 --> 00:56:49,300
but not directly
working on voxels.

1433
00:56:49,300 --> 00:56:51,840
But instead, think
about it as a level set

1434
00:56:51,840 --> 00:56:55,520
or some implicit functions that
use deep network to represent.

1435
00:56:55,520 --> 00:56:56,680
That's the final step.

1436
00:56:56,680 --> 00:56:59,380
Going from this
AtlasNet or whatever,

1437
00:56:59,380 --> 00:57:02,760
you're learning the
transformation from a 2D space

1438
00:57:02,760 --> 00:57:03,860
to 3D space.

1439
00:57:03,860 --> 00:57:06,240
But now I can directly
do an implicit query

1440
00:57:06,240 --> 00:57:07,600
using the deep networks.

1441
00:57:07,600 --> 00:57:10,840
So that goes to deep
implicit functions, which

1442
00:57:10,840 --> 00:57:13,460
is interesting because
around that time, in 2019,

1443
00:57:13,460 --> 00:57:16,290
there are four papers that
are doing almost exactly

1444
00:57:16,290 --> 00:57:17,330
the same thing.

1445
00:57:17,330 --> 00:57:19,913
They all argue that before we
have been using voxels and point

1446
00:57:19,913 --> 00:57:22,455
clouds and meshes, or whatever,
they have their own strengths

1447
00:57:22,455 --> 00:57:23,030
or weakness.

1448
00:57:23,030 --> 00:57:24,490
But really, the
right thing to do

1449
00:57:24,490 --> 00:57:28,710
is I should just send the
query into the deep network.

1450
00:57:28,710 --> 00:57:31,990
So deep network, what it should
do is just take the input,

1451
00:57:31,990 --> 00:57:34,110
let's say x, y,
z-coordinate, and output

1452
00:57:34,110 --> 00:57:37,770
whether that point is inside
and outside the object.

1453
00:57:37,770 --> 00:57:41,090
And that would be, in some
sense, the final, I would say,

1454
00:57:41,090 --> 00:57:41,830
one of the final.

1455
00:57:41,830 --> 00:57:44,810
And that's the kind of idea
that was proposed in 2019.

1456
00:57:44,810 --> 00:57:47,010
And even right now, in
2025, a lot of people

1457
00:57:47,010 --> 00:57:49,170
have been still
using this same idea.

1458
00:57:49,170 --> 00:57:51,530
That is, I would
just use deep network

1459
00:57:51,530 --> 00:57:54,210
to tell me whether a point is
inside or outside an object.

1460
00:57:54,210 --> 00:57:56,850
And not only you can go a
little bit beyond then, just

1461
00:57:56,850 --> 00:57:59,220
like a binary classification
of inside and outside,

1462
00:57:59,220 --> 00:58:01,470
because you can also say,
oh, maybe I care a bit more.

1463
00:58:01,470 --> 00:58:03,553
I say, what would be the
signed distance function?

1464
00:58:03,553 --> 00:58:06,970
How far the point is from
the surface of the object.

1465
00:58:06,970 --> 00:58:09,570
Or what would be the
density values of the point?

1466
00:58:09,570 --> 00:58:11,470
Or later, what
will be the color?

1467
00:58:11,470 --> 00:58:14,030
What will be the radiance
values of the point?

1468
00:58:14,030 --> 00:58:16,550
But here, starting from
2019, people really

1469
00:58:16,550 --> 00:58:19,550
start to apply deep network in
a way that is, in some sense,

1470
00:58:19,550 --> 00:58:20,650
similar to classification.

1471
00:58:20,650 --> 00:58:24,630
That is, I take points in 3D
space and use it as an implicit

1472
00:58:24,630 --> 00:58:27,550
function to query some
properties of the points in 3D

1473
00:58:27,550 --> 00:58:29,270
space.

1474
00:58:29,270 --> 00:58:33,190
And people have tried to use
it as represent a collection

1475
00:58:33,190 --> 00:58:36,305
of implicit functions, not
just how PCs are deforming

1476
00:58:36,305 --> 00:58:38,430
into the 3D space to get
different pieces of papers

1477
00:58:38,430 --> 00:58:42,070
in 3D, but really representing
implicit parts of the objects

1478
00:58:42,070 --> 00:58:43,250
using small neural networks.

1479
00:58:43,250 --> 00:58:45,950
And they can form
a complex shapes.

1480
00:58:45,950 --> 00:58:52,350
And if you can represent objects
in 3D using implicit functions,

1481
00:58:52,350 --> 00:58:54,170
you can do that not
only for geometry.

1482
00:58:54,170 --> 00:58:56,030
You can query not
only the inside

1483
00:58:56,030 --> 00:58:58,290
or whether a point is
inside or outside objects,

1484
00:58:58,290 --> 00:59:01,170
whether how far the point is
from the surface of the object.

1485
00:59:01,170 --> 00:59:03,308
You can also query, what
will be the radiance?

1486
00:59:03,308 --> 00:59:04,850
What will be the
color of the object?

1487
00:59:04,850 --> 00:59:06,490
And then I go directly
one year later.

1488
00:59:06,490 --> 00:59:08,170
This is maybe one
or two years later.

1489
00:59:08,170 --> 00:59:10,390
Now, people come up with
this thing called NeRF So

1490
00:59:10,390 --> 00:59:13,240
where they used differences
here is now say--

1491
00:59:13,240 --> 00:59:14,960
or I should use
deepen our knowledge

1492
00:59:14,960 --> 00:59:17,600
to query, what will be the
signed distance function

1493
00:59:17,600 --> 00:59:21,640
or density of the objects, but
also querying the radiance?

1494
00:59:21,640 --> 00:59:26,380
So here you can see what's going
on is you query NeRF about x, y,

1495
00:59:26,380 --> 00:59:28,090
z-coordinate in the 3D space.

1496
00:59:28,090 --> 00:59:29,840
In addition to that,
because we are trying

1497
00:59:29,840 --> 00:59:31,640
to model the appearance
as well, you also

1498
00:59:31,640 --> 00:59:35,400
query the viewing directions,
the camera viewing directions.

1499
00:59:35,400 --> 00:59:39,200
And the output of the neural
network is not just like 1 or 0,

1500
00:59:39,200 --> 00:59:40,440
inside, outside.

1501
00:59:40,440 --> 00:59:44,080
It is the density values in
addition to the color values

1502
00:59:44,080 --> 00:59:45,720
or the radiance.

1503
00:59:45,720 --> 00:59:50,600
And if you directly train
implicit functions on 3D shapes,

1504
00:59:50,600 --> 00:59:52,600
and then you require
3D supervision.

1505
00:59:52,600 --> 00:59:54,783
So if you have a
collection of 3D objects,

1506
00:59:54,783 --> 00:59:56,200
you can use them
as a supervision.

1507
00:59:56,200 --> 00:59:59,200
That gives you-- OK, you have
the ground truth about whether

1508
00:59:59,200 --> 01:00:01,760
a point is inside or
outside a 3D object.

1509
01:00:01,760 --> 01:00:04,525
But here you want to
train on 2D images.

1510
01:00:04,525 --> 01:00:05,900
That's what's
going on with NeRF.

1511
01:00:05,900 --> 01:00:08,840
So they also put that together
with the neural rendering--

1512
01:00:08,840 --> 01:00:09,940
volume rendering function.

1513
01:00:09,940 --> 01:00:11,773
And they made this
volume rendering function

1514
01:00:11,773 --> 01:00:15,010
differentiable in the sense that
you can have a rendering model,

1515
01:00:15,010 --> 01:00:17,730
you can query all these
different points in 3D space.

1516
01:00:17,730 --> 01:00:20,210
You can get their
colors, and also

1517
01:00:20,210 --> 01:00:23,090
their densities in appearance.

1518
01:00:23,090 --> 01:00:25,210
And then you can
compute how much

1519
01:00:25,210 --> 01:00:27,010
light is blocked along the way.

1520
01:00:27,010 --> 01:00:29,010
So this is basically
volume rendering

1521
01:00:29,010 --> 01:00:30,570
as in computer graphics.

1522
01:00:30,570 --> 01:00:35,850
There's very minimal change
made because you can see,

1523
01:00:35,850 --> 01:00:38,090
even directly from the
volume-rendering equations,

1524
01:00:38,090 --> 01:00:39,890
that everything here is--

1525
01:00:39,890 --> 01:00:41,030
this is an approximation.

1526
01:00:41,030 --> 01:00:42,655
But with approximation,
everything here

1527
01:00:42,655 --> 01:00:43,890
is differentiable.

1528
01:00:43,890 --> 01:00:46,438
So you can compute
how much light--

1529
01:00:46,438 --> 01:00:48,730
if the neural network gives
you the density, which is--

1530
01:00:48,730 --> 01:00:51,610
basically, you can think of it
as opacity of the point in 3D

1531
01:00:51,610 --> 01:00:53,283
space, and it also
gives you the color,

1532
01:00:53,283 --> 01:00:55,450
then you can compute how
much light has been blocked

1533
01:00:55,450 --> 01:01:00,050
by the points that are
sampled ahead of that point.

1534
01:01:00,050 --> 01:01:02,450
And along the way, and
you can compute also

1535
01:01:02,450 --> 01:01:05,090
how much light is
there contributing

1536
01:01:05,090 --> 01:01:09,820
to what I'm going to see in this
ray from any particular point?

1537
01:01:09,820 --> 01:01:11,360
So now you have a few things.

1538
01:01:11,360 --> 01:01:13,693
You have neural networks to
represent implicit functions

1539
01:01:13,693 --> 01:01:16,122
for the colors or radiance
and the densities.

1540
01:01:16,122 --> 01:01:18,080
And then you have this
volume render equations,

1541
01:01:18,080 --> 01:01:21,260
which is made differentiable so
that you can learn directly from

1542
01:01:21,260 --> 01:01:23,083
2D images.

1543
01:01:23,083 --> 01:01:25,000
So these are the two
things that have changed.

1544
01:01:25,000 --> 01:01:27,760
And one is, I no longer
have to train on 3D shapes.

1545
01:01:27,760 --> 01:01:30,640
I can train on 2D images with
this volume rendering equations.

1546
01:01:30,640 --> 01:01:33,940
And the second is, instead
of just looking into geometry

1547
01:01:33,940 --> 01:01:38,540
or density of objects in 3D, I
also look into their radiance

1548
01:01:38,540 --> 01:01:40,120
or appearance in 3D.

1549
01:01:40,120 --> 01:01:43,060
So these two changes lead
to the kind of big jump

1550
01:01:43,060 --> 01:01:46,100
behind, from NeRF, or
from implicit functions

1551
01:01:46,100 --> 01:01:49,540
or deep [? SDF ?] and all
these other methods to NeRF.

1552
01:01:49,540 --> 01:01:52,472
So a lot of people feel like,
oh yeah, NeRF has been great.

1553
01:01:52,472 --> 01:01:53,680
It seems like out of nowhere.

1554
01:01:53,680 --> 01:01:55,740
That's really not the
case because they're

1555
01:01:55,740 --> 01:01:58,380
very much inspired-- if you
look at the articles they later

1556
01:01:58,380 --> 01:02:01,700
wrote themselves,
they are very much

1557
01:02:01,700 --> 01:02:04,640
inspired by all these advances
in deep intrinsic functions.

1558
01:02:04,640 --> 01:02:06,600
Although, they focus
only on geometry.

1559
01:02:06,600 --> 01:02:09,900
But now I do both
geometry and appearance.

1560
01:02:09,900 --> 01:02:13,920
And I do learning from 2D
images instead of 3D shapes.

1561
01:02:13,920 --> 01:02:16,200
So yeah, here are
some results of NeRF.

1562
01:02:16,200 --> 01:02:17,950
This you may have
seen many times.

1564
01:02:24,800 --> 01:02:27,620
OK, so if you remember,
we said, in the past,

1565
01:02:27,620 --> 01:02:29,920
we have been working on
something like generating 3D

1566
01:02:29,920 --> 01:02:33,908
shapes, and then also
generating their 2D appearances.

1567
01:02:33,908 --> 01:02:36,200
Here, at the very beginning,
we used the representation

1568
01:02:36,200 --> 01:02:38,640
that is voxels.

1569
01:02:38,640 --> 01:02:41,253
But now, as we said,
yeah, NeRF is great.

1570
01:02:41,253 --> 01:02:42,920
And if we have implicit
representations,

1571
01:02:42,920 --> 01:02:45,900
there's no need to really
represent it as voxels.

1572
01:02:45,900 --> 01:02:49,480
What if we just replace that
with the radiance fields?

1573
01:02:49,480 --> 01:02:50,973
So we also did that as well.

1574
01:02:50,973 --> 01:02:52,640
So we have a neural
network that capture

1575
01:02:52,640 --> 01:02:54,980
the implicit radiance
fields and densities.

1576
01:02:54,980 --> 01:02:57,067
But it is generative
neural network.

1577
01:02:57,067 --> 01:02:59,400
And then you can even still
apply the same GAN rendering

1578
01:02:59,400 --> 01:03:01,880
framework so that you
can render objects in 3D,

1579
01:03:01,880 --> 01:03:04,100
as well as their 2D pictures.

1580
01:03:04,100 --> 01:03:06,730
And you can also do the same
thing as controllability.

1581
01:03:06,730 --> 01:03:10,750
And you can change
the camera viewpoint.

1582
01:03:10,750 --> 01:03:13,768
You can change object identity,
but you can keep the viewpoint.

1583
01:03:13,768 --> 01:03:15,810
You can do all the things
that you can do before.

1584
01:03:15,810 --> 01:03:18,237
But now with NeRF, you can
learn directly from images.

1585
01:03:18,237 --> 01:03:20,570
So you don't have to restrict
yourself to the categories

1586
01:03:20,570 --> 01:03:23,810
of cars or chairs, where
you have a lot of 3D data,

1587
01:03:23,810 --> 01:03:26,930
because you can learn
directly from images.

1588
01:03:26,930 --> 01:03:29,850
Yeah, so you can see that now,
the output becomes much more

1589
01:03:29,850 --> 01:03:30,510
realistic.

1590
01:03:30,510 --> 01:03:32,850
So this is the thing we
did called [? pigeon ?]

1591
01:03:32,850 --> 01:03:35,910
with Eric Chen as
a first author,

1592
01:03:35,910 --> 01:03:39,110
and also with mostly
people from Gordon's group.

1594
01:03:42,410 --> 01:03:48,010
OK, and finally, NeRF is great,
but then NeRF has this issue

1595
01:03:48,010 --> 01:03:50,890
that is, you have to sample
a lot of points in 3D.

1596
01:03:50,890 --> 01:03:53,890
You're no longer presampling
them and then applying

1597
01:03:53,890 --> 01:03:55,230
a volumetric convolution.

1598
01:03:55,230 --> 01:03:56,650
But still, just
like a level set,

1599
01:03:56,650 --> 01:03:58,025
you have to sample
all the points

1600
01:03:58,025 --> 01:03:59,730
and create neural
network all the time.

1601
01:03:59,730 --> 01:04:02,583
And now you can do
it learning from 2D.

1602
01:04:02,583 --> 01:04:04,000
You can do all
these great things.

1603
01:04:04,000 --> 01:04:06,083
But because you still have
to do all the sampling,

1604
01:04:06,083 --> 01:04:07,380
it is very slow.

1605
01:04:07,380 --> 01:04:09,238
So people thought
a little bit more.

1606
01:04:09,238 --> 01:04:10,780
Again, from the
graphics people, they

1607
01:04:10,780 --> 01:04:13,547
were like, OK, I have this good
idea about points and meshes.

1608
01:04:13,547 --> 01:04:15,880
And the nice thing about them
is they are free in space.

1609
01:04:15,880 --> 01:04:17,200
They're very efficient.

1610
01:04:17,200 --> 01:04:19,200
So is that possible for
us to integrate the two?

1611
01:04:19,200 --> 01:04:20,880
Can I have an implicit
representations?

1612
01:04:20,880 --> 01:04:23,798
But maybe I don't have to
have a fixed sampling grid.

1613
01:04:23,798 --> 01:04:25,340
I don't have to
sample all the times,

1614
01:04:25,340 --> 01:04:26,960
because it takes so much time.

1615
01:04:26,960 --> 01:04:30,020
So maybe I really should
put them together.

1616
01:04:30,020 --> 01:04:34,117
So you can argue that NeRF tried
to parameterize densities--

1617
01:04:34,117 --> 01:04:36,200
sorry, parameterize the
scenes very, very densely,

1618
01:04:36,200 --> 01:04:39,660
you have to sample all
the points density in 3D.

1619
01:04:39,660 --> 01:04:41,980
A lot of points are wasted,
just like in voxels.

1620
01:04:41,980 --> 01:04:44,600
You have all the points that
are representing empty space.

1621
01:04:44,600 --> 01:04:45,660
You don't want that.

1622
01:04:45,660 --> 01:04:48,300
In NeRF, a lot of
sampling, a lot of queries

1623
01:04:48,300 --> 01:04:50,180
are also querying empty space.

1624
01:04:50,180 --> 01:04:52,100
And network may give
you a density of 0

1625
01:04:52,100 --> 01:04:55,220
or something like that, but
it's taking a lot of time.

1626
01:04:55,220 --> 01:04:57,500
So how can we address that?

1627
01:04:57,500 --> 01:05:00,440
What If I just try to
sample things more sparsely?

1628
01:05:00,440 --> 01:05:02,790
I still have this
implicit representations.

1629
01:05:02,790 --> 01:05:06,890
But instead of sampling
empty spaces all the time,

1630
01:05:06,890 --> 01:05:09,830
I only sample at places
where I know there are stuff.

1631
01:05:09,830 --> 01:05:11,070
But how can I know that?

1632
01:05:11,070 --> 01:05:13,590
What if I have a
point representations?

1633
01:05:13,590 --> 01:05:16,890
So this is the idea behind this
thing called Gaussian splats,

1634
01:05:16,890 --> 01:05:18,190
which you may have heard of.

1635
01:05:18,190 --> 01:05:20,730
So it still has the
same implicit functions.

1636
01:05:20,730 --> 01:05:23,470
You're querying neural network
for densities and for appearance

1637
01:05:23,470 --> 01:05:24,870
and stuff like that.

1638
01:05:24,870 --> 01:05:28,170
But instead of creating
neural network all the time,

1639
01:05:28,170 --> 01:05:30,938
I have a point representation of
the 3D Gaussian blobs in the 3D

1640
01:05:30,938 --> 01:05:33,230
space, which I think sometimes
you can think about them

1641
01:05:33,230 --> 01:05:34,350
as point clouds.

1642
01:05:34,350 --> 01:05:36,170
But the points are not
like a single point.

1643
01:05:36,170 --> 01:05:37,370
They're like a blob.

1644
01:05:37,370 --> 01:05:39,750
They're like some regions.

1645
01:05:39,750 --> 01:05:43,050
And because you know
where these blobs are,

1646
01:05:43,050 --> 01:05:46,115
when you're sending out a ray
from your camera to the 3D space

1647
01:05:46,115 --> 01:05:48,490
and sample points, you don't
have to sample all the time.

1648
01:05:48,490 --> 01:05:50,590
You just look at
where these blobs are,

1649
01:05:50,590 --> 01:05:53,790
and then you can know--
based on the radius

1650
01:05:53,790 --> 01:05:55,390
of these different
Gaussians, you

1651
01:05:55,390 --> 01:05:56,870
will only sample
at regions where

1652
01:05:56,870 --> 01:05:58,470
you know there is some stuff.

1653
01:05:58,470 --> 01:06:01,710
So this makes rendering
much more efficient.

1654
01:06:01,710 --> 01:06:05,393
And so here are some of the
reconstruction results using 3D

1655
01:06:05,393 --> 01:06:06,060
Gaussian splats.

1657
01:06:14,330 --> 01:06:17,530
And you can see that
in terms of quality,

1658
01:06:17,530 --> 01:06:19,858
they're actually not that--

1659
01:06:19,858 --> 01:06:20,650
they're comparable.

1660
01:06:20,650 --> 01:06:22,890
I would say they're
comparable to NeRFs.

1661
01:06:22,890 --> 01:06:25,010
This is different
metrics, PSNR, SSIM.

1662
01:06:25,010 --> 01:06:26,430
They're like
rendering qualities.

1663
01:06:26,430 --> 01:06:29,898
And I think the y-axis,
it doesn't start from 0.

1664
01:06:29,898 --> 01:06:31,190
So this is a little misleading.

1665
01:06:31,190 --> 01:06:33,230
But basically, you can see
these numbers are really close.

1666
01:06:33,230 --> 01:06:34,610
So in terms of quality,
rendering quality,

1667
01:06:34,610 --> 01:06:36,850
Gaussian splats and NeRFs
are similar, at least

1668
01:06:36,850 --> 01:06:38,370
when they're first proposed.

1669
01:06:38,370 --> 01:06:40,890
But Gaussian splats are
just much more efficient.

1670
01:06:40,890 --> 01:06:42,610
So this is FPS,
Frame Per Second.

1671
01:06:42,610 --> 01:06:45,110
You can render 150
pictures per second.

1672
01:06:45,110 --> 01:06:49,370
Well, for NeRF, it takes
you maybe 20 seconds

1673
01:06:49,370 --> 01:06:51,050
to render just a single picture.

1674
01:06:51,050 --> 01:06:54,670
So now this thing is now
made 1,000 times faster.

1675
01:06:54,670 --> 01:06:56,170
At least that's
what they argued.

1676
01:06:56,170 --> 01:06:59,340
So because you no longer
waste all your computing power

1677
01:06:59,340 --> 01:07:02,500
on sampling empty space
and query neural networks

1678
01:07:02,500 --> 01:07:06,370
all the time about these points
that are in the empty space.

1680
01:07:11,260 --> 01:07:13,940
OK, yeah, so that is basically
how deep learning has been

1681
01:07:13,940 --> 01:07:17,167
integrated on 3D data in all
these different representations,

1682
01:07:17,167 --> 01:07:19,000
how they got started,
how they have evolved,

1683
01:07:19,000 --> 01:07:21,180
and its connection with
all these different shape

1684
01:07:21,180 --> 01:07:22,060
representations.

1685
01:07:22,060 --> 01:07:23,560
And one thing we
didn't talk about--

1686
01:07:23,560 --> 01:07:25,620
we just use two minutes
to quickly cover it--

1687
01:07:25,620 --> 01:07:28,660
is there has been
also interesting

1688
01:07:28,660 --> 01:07:30,620
things about object
geometry that

1689
01:07:30,620 --> 01:07:32,720
is not only about
the element geometry,

1690
01:07:32,720 --> 01:07:36,360
the specific details about the
parts, but also the structures.

1691
01:07:36,360 --> 01:07:39,605
Because often, there could
be-- the chairs are symmetric.

1692
01:07:39,605 --> 01:07:40,980
So we talk a little
bit about it,

1693
01:07:40,980 --> 01:07:42,980
where there's a
parametric surface.

1694
01:07:42,980 --> 01:07:46,580
And you can parameterize part
of the surface using a sphere

1695
01:07:46,580 --> 01:07:49,107
or stuff like that, using
these closed form equations.

1696
01:07:49,107 --> 01:07:50,940
And that gives you a
little bit of symmetry.

1697
01:07:50,940 --> 01:07:53,100
But there has been also
more systematic studies

1698
01:07:53,100 --> 01:07:56,110
about the regularities or
structures within object

1699
01:07:56,110 --> 01:07:58,430
geometry, including their
repetitions, including

1700
01:07:58,430 --> 01:07:59,290
their symmetries.

1701
01:07:59,290 --> 01:08:01,832
And people also come up with
different representations for it

1702
01:08:01,832 --> 01:08:04,350
as well.

1703
01:08:04,350 --> 01:08:06,730
So how can we really
represent-- in some sense,

1704
01:08:06,730 --> 01:08:09,290
you can argue that point clouds,
meshes, implicit functions,

1705
01:08:09,290 --> 01:08:11,582
they're really representing
the geometric details maybe

1706
01:08:11,582 --> 01:08:13,450
for the individual parts.

1707
01:08:13,450 --> 01:08:15,310
None of them is
directly capturing

1708
01:08:15,310 --> 01:08:18,290
things like regularities,
like symmetry and repetition.

1709
01:08:18,290 --> 01:08:20,189
So how can we capture that?

1710
01:08:20,189 --> 01:08:23,510
A few other attempts that people
have been exploring, mostly

1711
01:08:23,510 --> 01:08:27,270
from the graphics community,
is I can represent objects just

1712
01:08:27,270 --> 01:08:31,229
basically as a collection of
these simple geometric parts,

1713
01:08:31,229 --> 01:08:33,590
like a part set.

1714
01:08:33,590 --> 01:08:36,310
And there has been methods
that apply deep learning on it,

1715
01:08:36,310 --> 01:08:38,790
representing using deep
networks to represent

1716
01:08:38,790 --> 01:08:40,189
different parts
of objects, using

1717
01:08:40,189 --> 01:08:42,750
simple geometric primitives,
and then compose them,

1718
01:08:42,750 --> 01:08:45,649
or using implicit
functions and compose them,

1719
01:08:45,649 --> 01:08:47,470
as we talked about before.

1720
01:08:47,470 --> 01:08:51,547
But there has also been attempts
to do a bit more, so not just

1721
01:08:51,547 --> 01:08:53,630
like representing objects
as a collection of parts

1722
01:08:53,630 --> 01:08:55,297
without considering
their relationships,

1723
01:08:55,297 --> 01:08:57,040
but also modeling
the relationships

1724
01:08:57,040 --> 01:08:58,560
between these parts.

1725
01:08:58,560 --> 01:09:03,319
This is even more so the case
for scenes, let's say, a bed.

1726
01:09:03,319 --> 01:09:04,883
Our bad is usually
next to the wall.

1727
01:09:04,883 --> 01:09:07,300
Chairs is usually next to the
tables, and stuff like that.

1728
01:09:07,300 --> 01:09:09,000
So you not only want
to represent them

1729
01:09:09,000 --> 01:09:12,580
as unrelated collection
of parts or objects.

1730
01:09:12,580 --> 01:09:15,000
You want to capture their
relationships as well.

1731
01:09:15,000 --> 01:09:18,008
In the hierarchies, when
you are constructing,

1732
01:09:18,008 --> 01:09:20,300
when you're building-- you're
doing some constructions,

1733
01:09:20,300 --> 01:09:21,640
your architecture.

1734
01:09:21,640 --> 01:09:22,580
You're an architect.

1735
01:09:22,580 --> 01:09:26,000
You're designing a
building, then, of course,

1736
01:09:26,000 --> 01:09:29,180
you're not just representing
objects or their relationships.

1737
01:09:29,180 --> 01:09:30,580
You have to consider
hierarchies,

1738
01:09:30,580 --> 01:09:31,960
what you build first.

1739
01:09:31,960 --> 01:09:33,520
There's a classroom,
and a classroom

1740
01:09:33,520 --> 01:09:35,580
has-- there's some
tables and chairs in it,

1741
01:09:35,580 --> 01:09:36,740
and chairs has parts.

1742
01:09:36,740 --> 01:09:38,840
There's basically
a level hierarchy

1743
01:09:38,840 --> 01:09:41,752
and how this can be used and
integrated with neural networks,

1744
01:09:41,752 --> 01:09:43,960
as well as you have not only
the hierarchy, but also,

1745
01:09:43,960 --> 01:09:46,180
you can compose hierarchies
and relationships.

1746
01:09:46,180 --> 01:09:48,722
So you have hierarchical
graph, where,

1747
01:09:48,722 --> 01:09:50,680
let's say, for chairs,
you have different level

1748
01:09:50,680 --> 01:09:53,340
hierarchy for bases,
for seats, for backs.

1749
01:09:53,340 --> 01:09:55,520
And the bases may
have different legs.

1750
01:09:55,520 --> 01:09:57,520
But then also, the legs
themselves are related.

1751
01:09:57,520 --> 01:09:59,760
The left leg of the chair and
the right leg of the chair,

1752
01:09:59,760 --> 01:10:01,180
they are supposed
to be symmetric.

1753
01:10:01,180 --> 01:10:03,380
And they should have
the identical shape.

1754
01:10:03,380 --> 01:10:05,780
There are constraints
on where these legs are.

1755
01:10:05,780 --> 01:10:08,180
They have to be really
aligned, otherwise

1756
01:10:08,180 --> 01:10:09,400
the chair is going to fall.

1757
01:10:09,400 --> 01:10:10,817
So there are all
these constraints

1758
01:10:10,817 --> 01:10:12,560
that are pretty useful.

1759
01:10:12,560 --> 01:10:13,960
And how can we represent them?

1760
01:10:13,960 --> 01:10:16,520
And people come up with all
these different representations.

1761
01:10:16,520 --> 01:10:19,060
And for each of
them, there are also

1762
01:10:19,060 --> 01:10:21,620
a lot of neural networks,
deep learning methods designed

1763
01:10:21,620 --> 01:10:24,300
to learn and to capture and
to generate objects that

1764
01:10:24,300 --> 01:10:26,180
satisfy all these constraints.

1765
01:10:26,180 --> 01:10:29,820
For example, you can see,
this is a hierarchical graph,

1766
01:10:29,820 --> 01:10:33,580
encoders and decoders that try
to represent and generate 3D

1767
01:10:33,580 --> 01:10:37,300
chairs that satisfy all these
constraints while maintaining

1768
01:10:37,300 --> 01:10:39,392
their hierarchies.

1769
01:10:39,392 --> 01:10:43,660
I think this is also from
Leonidas's group, from 2019.

1770
01:10:43,660 --> 01:10:45,660
And sometimes, we
can even represent

1771
01:10:45,660 --> 01:10:47,380
shapes using some
form of programs

1772
01:10:47,380 --> 01:10:49,260
because there's
repetitions and for loops

1773
01:10:49,260 --> 01:10:52,070
and how this can be
incorporated into using

1774
01:10:52,070 --> 01:10:54,070
neural networks to
generate programs

1775
01:10:54,070 --> 01:10:57,630
that synthesize object shapes
and synthesize their relations

1776
01:10:57,630 --> 01:10:59,190
between these object parts.

1777
01:10:59,190 --> 01:11:01,230
And that's also an
important topic.

1778
01:11:01,230 --> 01:11:05,390
And most recently, I would
say, let me end this by saying,

1779
01:11:05,390 --> 01:11:08,270
I think there has been a new
trend just in the past one

1780
01:11:08,270 --> 01:11:11,750
year or two that the deep
networks or large language

1781
01:11:11,750 --> 01:11:15,230
models are doing so well, and
they understand things so well

1782
01:11:15,230 --> 01:11:16,930
that people are exploring.

1783
01:11:16,930 --> 01:11:19,990
Is it possible for us to just
use large language models,

1784
01:11:19,990 --> 01:11:22,470
like GPT, to output
these programs

1785
01:11:22,470 --> 01:11:23,910
because they
understand semantics

1786
01:11:23,910 --> 01:11:25,070
and what the chair
should be like?

1787
01:11:25,070 --> 01:11:26,970
What are the constraints
of chairs to satisfy?

1788
01:11:26,970 --> 01:11:29,220
So is it possible for me to
use a large language model

1789
01:11:29,220 --> 01:11:30,330
to output programs?

1790
01:11:30,330 --> 01:11:32,950
But then maybe I can use some
implicit functions, or whatever,

1791
01:11:32,950 --> 01:11:34,430
to capture the
specific geometric

1792
01:11:34,430 --> 01:11:37,570
details of the parts of the
objects, like the chairs.

1793
01:11:37,570 --> 01:11:39,870
So there's some kind of new
emerging trend of research

1794
01:11:39,870 --> 01:11:42,390
that is happening right
now in these days.

1795
01:11:42,390 --> 01:11:43,770
OK, I think that's all I have.

1796
01:11:43,770 --> 01:11:45,580
Thank you.